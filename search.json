[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Problems and Solutions in Applied Statistics (2nd ed)",
    "section": "",
    "text": "Introduction\nThis book contains a collection of problems, and my solutions to them, in applied statistics with R. These come from my courses STAC32, STAC33, and STAD29 at the University of Toronto Scarborough.\nYou will occasionally see question parts beginning with a *; this means that other question parts refer back to this one. (One of my favourite question strategies is to ask how two different approaches lead to the same answer, or more generally to demonstrate that there are different ways to see the same thing.)\nThanks to Dann Sioson for spotting some errors and making some useful suggestions.\nIf you see anything, file an issue on the Github page for now. Likely problems include:\nAs I read through looking for problems like these, I realize that there ought to be a textbook that reflects my way of doing things. There isn’t one (yet), though there are lecture notes. Current versions of these are at:\nA little background:\nSTAC32 is an introduction to R as applied to statistical methods that have (mostly) been learned in previous courses. This course is designed for students who have a second non-mathematical applied statistics course such as this. The idea is that students have already seen a little of regression and analysis of variance (and the things that precede them), and need mainly an introduction of how to run them in R.\nSTAC33 is an introduction to R, and applied statistics in general, for students who have a background in mathematical statistics. The way our courses are structured, these students have a strong mathematical background, but not very much experience in applications, which this course is designed to provide. The material covered is similar to STAC32, with a planned addition of some ideas in bootstrap and practical Bayesian statistics. There are some questions on these here.\nSTAD29 is an overview of a number of advanced statistical methods. I start from regression and proceed to some regression-like methods (logistic regression, survival analysis, log-linear frequency table analysis), then I go a little further with analysis of variance and proceed with MANOVA and repeated measures. I finish with a look at classical multivariate methods such as discriminant analysis, cluster analysis, principal components and factor analysis. I cover a number of methods in no great depth; my aim is to convey an understanding of what these methods are for, how to run them and how to interpret the results. Statistics majors and specialists cannot take this course for credit (they have separate courses covering this material with the proper mathematical background). D29 is intended for students in other disciplines who find themselves wanting to learn more statistics; we have an Applied Statistics Minor program for which C32 and D29 are two of the last courses."
  },
  {
    "objectID": "index.html#packages-used-somewhere-in-this-book",
    "href": "index.html#packages-used-somewhere-in-this-book",
    "title": "Problems and Solutions in Applied Statistics (2nd ed)",
    "section": "Packages used somewhere in this book",
    "text": "Packages used somewhere in this book\nThe bottom lines are below used with the conflicted package: if a function by the name shown is in two or more packages, prefer the one from the package shown.\n\nlibrary(tidyverse)\nlibrary(smmr)\nlibrary(MASS)\nlibrary(nnet)\nlibrary(survival)\nlibrary(survminer)\nlibrary(car)\nlibrary(lme4)\nlibrary(ggbiplot)\nlibrary(ggrepel)\nlibrary(broom)\nlibrary(rpart)\nlibrary(bootstrap)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(tmaptools)\nlibrary(leaflet)\nlibrary(conflicted)\nconflict_prefer(\"summarize\", \"dplyr\")\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"count\", \"dplyr\")\nconflict_prefer(\"arrange\", \"dplyr\")\nconflict_prefer(\"rename\", \"dplyr\")\nconflict_prefer(\"id\", \"dplyr\")\n\nAll of these packages are on CRAN, and may be installed via the usual install.packages, with the exceptions of:\n\nsmmr on Github: install with\n\n\ndevtools::install_github(\"nxskok/smmr\")\n\n\nggbiplot on Github: install with\n\n\ndevtools::install_github(\"vqv/ggbiplot\")\n\n\ncmdstanr, posterior, and bayesplot: install with\n\n\ninstall.packages(\"cmdstanr\", \n                 repos = c(\"https://mc-stan.org/r-packages/\", \n                           getOption(\"repos\")))\ninstall.packages(\"posterior\", \n                 repos = c(\"https://mc-stan.org/r-packages/\", \n                            getOption(\"repos\")))\ninstall.packages(\"bayesplot\", \n                 repos = c(\"https://mc-stan.org/r-packages/\", \n                            getOption(\"repos\")))"
  },
  {
    "objectID": "getting_used.html#using-r-studio-online",
    "href": "getting_used.html#using-r-studio-online",
    "title": "1  Getting used to R and R Studio",
    "section": "1.1 Using R Studio online",
    "text": "1.1 Using R Studio online\n\nPoint your web browser at http://r.datatools.utoronto.ca. Click on the button to the left of “R Studio” (it will show blue), click the orange Log in to Start, and log in using your UTorID and password.\n\nSolution\nThis is about what you should see first, before you click the orange thing:\n\nYou will see a progress bar as things start up, and then you should see something like this:\n\nThis is R Studio, ready to go.\nIf you are already logged in to something else on the same browser that uses your UTorID and password, you may come straight here without needing to log in again.\n\\(\\blacksquare\\)\n\nTake a look around, and create a new Project. Give the new project any name you like.\n\nSolution\nSelect File and New Project to get this:\n\nClick on New Directory (highlighted blue on mine). This will create a new folder to put your new project in, which is usually what you want to do. The idea is that a project is a container for a larger collection of work, such as all your assignments in this course. That brings you to this:\n\nwhere you click on New Project (highlighted on mine), and:\n\nGive your project a name, as I did. Then click Create Project. At this point, R Studio will be restarted in your new project. You can tell which project you are in by looking top right, and you’ll see the name of your project next to the R symbol:\n\n\\(\\blacksquare\\)\n\nOne last piece of testing: find the Console window (which is probably on the left). Click next to the blue &gt;, and type library(tidyverse). Press Enter.\n\nSolution\nIt may think a bit, and then you’ll see something like this:\n\nAside: I used to use a cloud R Studio called rstudio.cloud. If you see or hear any references to that, it means the same thing as R Studio on r.datatools or jupyter. (You can still use rstudio.cloud if you want; it used to be completely free, but now the free tier won’t last you very long; the utoronto.calink is free as long as you are at U of T.) I’m trying to get rid of references to R Studio Cloud as I see them, but I am bound to miss some, and in the lecture videos they are rather hard to find.\nNow we can get down to some actual work.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "getting_used.html#using-r-studio-on-your-own-computer",
    "href": "getting_used.html#using-r-studio-on-your-own-computer",
    "title": "1  Getting used to R and R Studio",
    "section": "1.2 Using R Studio on your own computer",
    "text": "1.2 Using R Studio on your own computer\nThis is not required now, but you may wish to do this now or later so that you are not fighting for resources on the r.datatools server at busy times (eg. when an assignment is due).\nFollow the instructions here to install R Studio on your computer, then start R Studio (which itself starts R).\nOnce you have this working, you can use it for any of the following questions, in almost exactly the same way as the online R (I will explain any differences)."
  },
  {
    "objectID": "getting_used.html#getting-started",
    "href": "getting_used.html#getting-started",
    "title": "1  Getting used to R and R Studio",
    "section": "1.3 Getting started",
    "text": "1.3 Getting started\nThis question is to get you started using R.\n\nStart R Studio on r.datatools (or on your computer), in some project. (If you started up a new project in the previous question and are still logged in, use that; if not, create a new project with File, New Project, and New Directory. Then select New Project and give it a name. Click Create Project. This will give you an empty workspace to start from.)\n\nSolution\nYou ought to see something like this:\n\nThere should be one thing on the left half, and at the top right it’ll say “Environment is empty”.\nExtra: if you want to tweak things, select Tools (at the top of the screen) and from it Global Options, then click Appearance. You can make the text bigger or smaller via Editor Font Size, and choose a different colour scheme by picking one of the Editor Themes (which previews on the right). My favourite is Tomorrow Night Blue. Click Apply or OK when you have found something you like. (I spend a lot of time in R Studio, and I like having a dark background to be easier on my eyes.)\n\\(\\blacksquare\\)\n\nWe’re going to do some stuff in R here, just to get used to it. First, make a Quarto document by selecting File, New File and Quarto Document.\n\nSolution\nIn the first box that pops up, you’ll be invited to give your document a title. Make something up for now.\nThe first time, you might be invited to “install some packages” to make the document thing work.1 Let it do that by clicking Yes. After that, you’ll have this:\n\nA couple of technical notes:\n\nthis should be in the top left pane of your R Studio now, with the Console below it.\nAt the top of the file, between the two lines with three hyphens (minus signs, whatever), is some information about the document, known in the jargon as a YAML block, any of which you can change:\n\nthe title is whatever title you gave your document\nthe formatis what the output is going to be (in this case, HTML like a webpage, which is mostly what we’ll be using)\nthere is a visual editor that looks like Notion or a bit like a Google doc (the default), and also a Source editor which gives you more control, and shows that underlying the document is a thing called R Markdown (which is a code for writing documents).\n\nMy document is called “My awesome title”, but the file in which the document lives is still untitled because I haven’t saved it yet. See right at the top.\n\n\\(\\blacksquare\\)\n\nYou can delete the template code below the YAML block now (that is, everything from the title “Quarto” to the end). Somewhere in the space opened up below the YAML block (it might say “Heading 2”, greyed out), type a /. This, like Notion, gives you a list of things to choose from to insert there. Pressing Enter will insert a “code chunk”, sometimes known as a “code cell”. We are going to use this in a moment.\n\nSolution\nSomething like this:\n\nThe {r} at the top of the code chunk means that the code that will go in there will be R code (you can also have a Python code chunk, among others).\n\\(\\blacksquare\\)\n\nOn the line below the {r}, type these two lines of code into the chunk in the Quarto document:\n\nlibrary(tidyverse)\nmtcars\nSolution\nWhat this will do: get hold of a built-in data set with information about some different models of car, and display it.\n\nIn approximately five seconds, you’ll be demonstrating that for yourself.\n\\(\\blacksquare\\)\n\nRun this command. To do that, look at the top right of your code chunk block (shaded in a slightly different colour). You should see a down arrow and a green “play button”. Click the play button. This will run the code, and show the output below the code chunk.\n\nSolution\nHere’s what I get (yours should be the same):\n\nThis is a rectangular array of rows and columns, with individuals (here, cars) in rows and variables in columns, known as a “dataframe”. When you display a dataframe in an Quarto document, you see 10 rows and as many columns as will fit on the screen. At the bottom, it says how many rows and columns there are altogether (here 32 rows and 11 columns), and which ones are being displayed.\nYou can see more rows by clicking on Next, and if there are more columns, you’ll see a little arrow next to the rightmost column (as here next to am) that you can click on to see more columns. Try it and see. Or if you want to go to a particular collection of rows, click one of the numbers between Previous and Next: 1 is rows 1–10, 2 is rows 11–20, and so on.\nThe column on the left without a header (containing the names of the cars) is called “row names”. These have a funny kind of status, kind of a column and kind of not a column; usually, if we need to use the names, we have to put them in a column first.\nIn future solutions, rather than showing you a screenshot, expect me to show you something like this:\n\nlibrary(tidyverse)\nmtcars\n\n\n\n\n\n  \n\n\n\nThe top bit is the code, the bottom bit the output. In this kind of display, you only see the first ten rows (by default).2\nIf you don’t see the “play button”, make sure that what you have really is a code chunk. (I often accidentally delete one of the special characters above or below the code chunk). If you can’t figure it out, delete this code chunk and make a new one. Sometimes R Studio gets confused.\nOn the code chunk, the other symbols are the settings for this chunk (you have the choice to display or not display the code or the output or to not actually run the code). The second one, the down arrow, runs all the chunks prior to this one (but not this one).\nYour output has its own little buttons (as seen on the screenshot). The first one pops the output out into its own window; the second one shows or hides the output, and the third one deletes the output (so that you have to run the chunk again to get it back). Experiment. You can’t do much damage here.\n\\(\\blacksquare\\)\n\nSomething a little more interesting: summary obtains a summary of whatever you feed it (the five-number summary plus the mean for numerical variables). Obtain this for our data frame. To do this, create a new code chunk below the previous one, type summary(mtcars) into the code chunk, and run it.\n\nSolution\nThis is what you should see:\n\nor the other way:\n\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nFor the gas mileage column mpg, the mean is bigger than the median, and the largest value is unusually large compared with the others, suggesting a distribution that is skewed to the right.\nThere are 11 numeric (quantitative) variables, so we get the five-number summary plus mean for each one. Categorical variables, if we had any here, would be displayed a different way.\n\\(\\blacksquare\\)\n\nLet’s make a histogram of the gas mileage data. Type the code below into another new code chunk, and run it:\n\nggplot(mtcars, aes(x = mpg)) + geom_histogram(bins = 8)\nThe code looks a bit wordy, but we’ll see what all those pieces do later in the course (like, maybe tomorrow).\nSolution\nThis is what you should see:\n\nggplot(mtcars, aes(x = mpg)) + geom_histogram(bins = 8)\n\n\n\n\nThe long right tail supports our guess from before that the distribution is right-skewed.\n\\(\\blacksquare\\)\n\nSome aesthetics: Add some narrative text above and below your code chunks. Above the code chunk is where you say what you are going to do (and maybe why you are doing it), and below is where you say what you conclude from the output you just obtained. I find it looks better if you have a blank line above and below each code chunk.\n\nSolution\nThis is what I wrote (screenshot), with none of the code run yet. My library(tidyverse) line seems to have disappeared, but yours should still be there:\n\n\\(\\blacksquare\\)\n\nSave your Quarto document (the usual way with File and Save). This saves it on the jupyter servers (and not on your computer). This means that when you come back to it later, even from another device, this document will still be available to you. (If you are running R Studio on your own computer, it is much simpler: the Quarto document is on that computer, in the folder associated with the project you created.)\n\nNow click Render. This produces a pretty HTML version of your Quarto document. This will appear in a new tab of your web browser, which you might need to encourage to appear (if you have a pop-up blocker) by clicking a Try Again.\nSolution\nIf there are any errors in the rendering process, these will appear in the Render tab. The error message will tell you where in your document your error was. Find it and correct it.3 Otherwise, you should see your document.\nExtra 1: the rendering process as you did it doesn’t produce that nice display of a dataframe that I had in one of my screenshots. To get that, alter the YAML block to read:\nformat: \n  html:\n     df-print: paged\nThis way, anyone reading your document can actually page through the dataframes you display in the same way that you did, to check that they contain the right things.\nExtra 2: you might prefer to have a preview of your document within R Studio. To make this happen, look for the gear wheel to the right of Render. Click the arrow beside it, and in the drop-down, click on Preview in Viewer Pane. Render again, and you’ll see the rendered version of your document in a Viewer pane on the right. This puts the thing you’re writing and what it will look like side by side.\nExtra 3: you might be annoyed at having to remember to save things. If you are, you can enable auto-saving. To do this, go to Tools and select Global Options. Select Code (on the left) and Saving (at the top). Click on Automatically Save when editor loses focus, to put a check mark in the box on the left of it. Change the pull-down below that to Save and Write Changes. Click OK. Now, as soon as you pause for a couple of seconds, everything unsaved will be saved.\n\\(\\blacksquare\\)\n\nPractice handing in your rendered Quarto document, as if it were an assignment that was worth something. (It is good to get the practice in a low-stakes situation, so that you’ll know what to do next week.)\n\nSolution\nThere are two steps: download the HTML file onto your computer, and then handing it in on Quercus. To download: find the HTML file that you want to download in the Files pane on the right. You might need to click on Files at the top, especially if you had a Viewer open there before:\n\nI called my Quarto document awesomeand the file I was working on was called awesome.qmd(which stands for “Quarto Markdown”). That’s the file I had to render to produce the output. My output file itself is called awesome.html.That’s the file I want to hand in. If you called your file something different when you saved it, that’s the thing to look for: there should be something ending in .qmd and something with the same first part ending in .html.\nClick the checkbox to the left of the HTML file. Now click on More above the bottom-right pane. This pops up a menu from which you choose Export. This will pop up another window called Export Files, where you put the name that the file will have on your computer. (I usually leave the name the same.) Click Download. The file will go to your Downloads folder, or wherever things you download off the web go.\nNow, to hand it in. Open up Quercus at q.utoronto.ca, log in and navigate to this course. Click Assignments. Click (the title of) Assignment 0. There is a big blue Start Assignment button top right. Click it. You’ll get a File Upload at the bottom of the screen. Click Choose File and find the HTML file that you downloaded. Click Open (or equivalent on your system). The name of the file should appear next to Choose File. Click Submit Assignment. You’ll see Submitted at the top right, and below that is a Submission Details window and the file you uploaded.\n\nYou should be in the habit of always checking what you hand in, by downloading it again and looking at it to make sure it’s what you thought you had handed in.\nIf you want to try this again, you can try again as many times as you like, by making a New Attempt. (For the real thing, you can use this if you realize you made a mistake in something you submitted. The graders’ instructions, for the real thing, are to grade the last file submitted, so in that case you need to make sure that the last thing submitted before the due date includes everything that you want graded. Here, though, it doesn’t matter.)\n\\(\\blacksquare\\)\n\nOptional extra. Something more ambitious: make a scatterplot of gas mileage mpg, on the \\(y\\) axis, against horsepower, hp, on the \\(x\\)-axis.\n\nSolution\nThat goes like this. I’ll explain the steps below.\n\nlibrary(tidyverse)\nggplot(mtcars, aes(x=hp, y=mpg)) + geom_point()\n\n\n\n\nThis shows a somewhat downward trend, which is what you’d expect, since a larger hp value means a more powerful engine, which will probably consume more gas and get fewer miles per gallon. As for the code: to make a ggplot plot, as we will shortly see in class, you first need a ggplot statement that says what to plot. The first thing in a ggplot is a data frame (mtcars here), and then the aes says that the plot will have hp on the \\(x\\)-axis and mpg on the \\(y\\)-axis, taken from the data frame that you specified. That’s all of the what-to-plot. The last thing is how to plot it; geom_point() says to plot the data values as points.\nYou might like to add a regression line to the plot. That is a matter of adding this to the end of the plotting command:\n\nggplot(mtcars, aes(x=hp, y=mpg)) + geom_point() + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe line definitely goes downhill. Decide for yourself how well you think a line fits these data.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "getting_used.html#reading-data-from-a-file",
    "href": "getting_used.html#reading-data-from-a-file",
    "title": "1  Getting used to R and R Studio",
    "section": "1.4 Reading data from a file",
    "text": "1.4 Reading data from a file\nIn this question, we read a file from the web and do some descriptive statistics and a graph. This is very like what you will be doing on future assignments, so it’s good to practice it now.\nTake a look at the data file at http://ritsokiguess.site/datafiles/jumping.txt. These are measurements on 30 rats that were randomly made to do different amounts of jumping by group (we’ll see the details later in the course). The control group did no jumping, and the other groups did “low jumping” and “high jumping”. The first column says which jumping group each rat was in, and the second is the rat’s bone density (the experimenters’ supposition was that more jumping should go with higher bone density).\n\nWhat are the two columns of data separated by? (The fancy word is “delimited”).\n\nSolution\nExactly one space. This is true all the way down, as you can check.\n\\(\\blacksquare\\)\n\nMake a new Quarto document. Leave the YAML block, but get rid of the rest of the template document. Start with a code chunk containing library(tidyverse). Run it.\n\nSolution\nYou will get either the same message as before or nothing. (I got nothing because I had already loaded the tidyverse in this session.)\n\\(\\blacksquare\\)\n\nPut the URL of the data file in a variable called my_url. Then use read_delim to read in the file. (See solutions for how.) read_delim reads data files where the data values are always separated by the same single character, here a space. Save the data frame in a variable rats.\n\nSolution\nLike this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/jumping.txt\"\nrats &lt;- read_delim(my_url,\" \")\n\nRows: 30 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): group\ndbl (1): density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe second thing in read_delim is the thing that separates the data values. Often when you use read_delim it’ll be a space.\nHint: to get the file name into my_url, the best way is to right-click on the link, and select Copy Link Address (or equivalent in your browser). That puts in on your clipboard. Then make a code chunk and put this in it (you’ll probably only need to type one quote symbol, because R Studio will supply the other one):\n\nmy_url &lt;- \"\"\n\nthen put the cursor between the two quote symbols and paste. This is better than selecting the URL in my text and then copy-pasting that because odd things happen if it happens to span two lines on your screen. (URLs tend to be rather long, so this is not impossible.)\n\\(\\blacksquare\\)\n\nTake a look at your data frame, by making a new code chunk and putting the data frame’s name in it (as we did with mtcars).\n\nSolution\n\nrats\n\n\n\n  \n\n\n\nThere are 30 rows and two columns, as there should be.\n\\(\\blacksquare\\)\n\nFind the mean bone density for rats that did each amount of jumping.\n\nSolution\nThis is something you’ll see a lot: group_by followed by summarize. Reminder: to get that funny thing with the percent signs (called the “pipe symbol”), type control-shift-M (or equivalent on a Mac):\n\nrats %&gt;% group_by(group) %&gt;%\nsummarize(m = mean(density))\n\n\n\n  \n\n\n\nThe mean bone density is clearly highest for the high jumping group, and not much different between the low-jumping and control groups.\n\\(\\blacksquare\\)\n\nMake a boxplot of bone density for each jumping group.\n\nSolution\nOn a boxplot, the groups go across and the values go up and down, so the right syntax is this:\n\nggplot(rats, aes(x=group, y=density)) + geom_boxplot()\n\n\n\n\nGiven the amount of variability, the control and low-jump groups are very similar (with the control group having a couple of outliers), but the high-jump group seems to have a consistently higher bone density than the others.\nThis is more or less in line with what the experimenters were guessing, but it seems that it has to be high jumping to make a difference.\nYou might recognize that this is the kind of data where we would use analysis of variance, which we will do later on in the course: we are comparing several (here three) groups.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "getting_used.html#reading-files-different-ways",
    "href": "getting_used.html#reading-files-different-ways",
    "title": "1  Getting used to R and R Studio",
    "section": "1.5 Reading files different ways",
    "text": "1.5 Reading files different ways\nThis question is about different ways of reading data files. If you’re working online (using r.datatools or similar), start at the beginning. If you’re using R Studio running on your own computer, start at part (here).\n\nLog in to r.datatools.utoronto.ca. Open up a project (or start a new one), and watch the spinning circles for a few minutes. When that’s done, create a new Quarto Document with File, New File, Quarto Document. Delete the “template” document, but not the top lines with title: and output: in them. Add a code chunk that contains library(tidyverse) and run it.\n\nSolution\nSo far you (with luck) have something that looks like this:\n\nIf you have an error rather than that output, you probably need to install the tidyverse first. Make another code chunk, containing\n\ninstall.packages(\"tidyverse\")\n\nand run it. Wait for it to finish. It may take a while. If it completes successfully (you might see the word DONE at the end), delete that code chunk (you don’t need it any more) and try again with the library(tidyverse) chunk. It should run properly this time.\n\\(\\blacksquare\\)\n\n* The easiest kind of files to read in are ones on the Internet, with a URL address that begins with http or https. I have a small file at link. Click the link to see it, and keep the tab open for the next part of this question. This is a text file with three things on each line, each separated by exactly one space. Read the data file into a data frame, and display your data frame.\n\nSolution\nData values separated by exactly one space is the kind of thing that read_delim reads, so make another code chunk and fill it with this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/testing.txt\"\ntesting &lt;- read_delim(my_url, \" \")\n\nRows: 6 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): g\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntesting\n\n\n\n  \n\n\n\nWhen you run that, you’ll see something like my output. The first part is read_delim telling you what it saw in the file: two columns of (whole) numbers and one column of text. The top line of the file is assumed to contain names, which are used as the names of the columns of your data frame. The bottom part of the output, obtained by putting the name of the data frame on a line by itself in your code chunk, is what the data frame actually looks like. You ought to get into the habit of eyeballing it and checking that it looks like the values in the data file.\nThe things on the left side of the &lt;- symbol (that is meant to look like an arrow pointing left) are variables that you are creating in R. You get to choose the names for them. My habit is to use my_url for URLs of files that I am going to read in, and (usually) to give my data frames names that say something about what they contain, but this is your choice to make.\n\\(\\blacksquare\\)\n\nYou might have data in a file on your own computer. To read data from such a file, you first have to upload it to r.datatools, and then read it from there. To practice this: open a text editor (like Notepad or TextEdit). Go back to the web browser tab containing the data you used in the previous part. Copy the data from there and paste it into the text editor. Save it somewhere on your computer (like the Desktop). Upload that file, read in the data and verify that you get the right thing. (For this last part, see the Solution.)\n\nSolution\nI copied and pasted the data, and saved it in a file called testing.txt on my computer. I’m assuming that you’ve given it a similar name. Then go back to r.datatools. You should have a Files pane bottom right. If you don’t see a pane bottom right at all, press Control-Shift-0 to show all the panes. If you see something bottom right but it’s not Files (for example a plot), click the Files tab, and you should see a list of things that look like files, like this:\n\nClick the Upload button (next to New Folder), click Choose File. Use the file finder to track down the file you saved on your computer, then click OK. The file should be uploaded to the same folder on r.datatools that your project is, and appear in the Files pane bottom right.\nTo read it in, you supply the file name to read_delim thus:\n\ntesting2 &lt;- read_delim(\"testing.txt\", \" \")\n\nRows: 6 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): g\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nand then you look at it in the same way as before:\n\ntesting2\n\n\n\n  \n\n\n\nCheck.\n\\(\\blacksquare\\)\n\nYou might have a spreadsheet on your computer. Create a .csv file from it, and use the ideas of the last part to read it into R Studio.\n\nSolution\nOpen the spreadsheet containing the data you want to read into R. If there are several sheets in the workbook, make sure you’re looking at the right one. Select File, Save As, select “CSV” or “comma-separated values” and give it a name. Save the resulting file somewhere.\nThen follow the same steps as the previous part to upload it to your project on R Studio Cloud. (If you look at the actual file, it will be plain text with the data values having commas between them, as the name suggests. You can open the file in R Studio by clicking on it in the Files pane; it should open top left.)\nThe final step is to read it into an R data frame. This uses read_csv; there are several read_ functions that read in different types of file, and you need to use an appropriate one.\nMy spreadsheet got saved as cars.csv, so:\n\ncars &lt;- read_csv(\"cars.csv\")\n\nRows: 38 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Car, Country\ndbl (4): MPG, Weight, Cylinders, Horsepower\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncars\n\n\n\n  \n\n\n\nSome information about different types of cars.\nYou are now done with this question.\n\\(\\blacksquare\\)\n\n* Start here if you downloaded R and R Studio and they are running on your own computer. Open a web browser and point it at link. Click the link to see it, and keep the tab open for the next part of this question. This is a text file with three things on each line, each separated by exactly one space. Read the data file into a data frame, and display your data frame.\n\nSolution\nData values separated by exactly one space is the kind of thing that read_delim reads, so make another code chunk and fill it with this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/testing.txt\"\ntesting &lt;- read_delim(my_url, \" \")\n\nRows: 6 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): g\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntesting\n\n\n\n  \n\n\n\nWhen you run that, you’ll see something like my output. The first part is read_delim telling you what it saw in the file: two columns of (whole) numbers and one column of text. The top line of the file is assumed to contain names, which are used as the names of the columns of your data frame. The bottom part of the output, obtained by putting the name of the data frame on a line by itself in your code chunk, is what the data frame actually looks like. You ought to get into the habit of eyeballing it and checking that it looks like the values in the data file.\nThe things on the left side of the &lt;- symbol (that is meant to look like an arrow pointing left) are variables that you are creating in R. You get to choose the names for them. My habit is to use my_url for URLs of files that I am going to read in, and (usually) to give my data frames names that say something about what they contain, but this is your choice to make.\n(This part is exactly the same whether you are running R Studio on r.datatools or have R Studio running on your computer. A remote file is obtained in exactly the same way regardless.)\n\\(\\blacksquare\\)\n\nYou might have data in a file on your own computer. To read data from such a file, R has to know where to find it. Each R project lives in a folder, and one way of specifying where a data file is is to give its complete path relative to the folder that R Studio is running its current project in. This is rather complicated, so we will try a simpler way. To set this up, open a text editor (like Notepad or TextEdit). Go back to the web browser tab containing the data you used in the previous part. Copy the data from there and paste it into the text editor. Save it somewhere on your computer (like the Desktop). Follow the steps in the solution below to read the data into R.\n\nSolution\nI copied and pasted the data, and saved it in a file called testing.txt on my computer. I’m assuming that you’ve given it a similar name. Go back to R Studio. Create a new code chunk containing this:\n\nf &lt;- file.choose()\n\nRun this code chunk. You’ll see a file chooser. Find the file you saved on your computer, and click Open (or OK or whatever you see). This saves what R needs to access the file in the variable f. If you want to, you can look at it:\n\nf\n\nand you’ll see what looks like a file path in the appropriate format for your system (Windows, Mac, Linux). To read the data in, you supply the file path to read_delim thus:\n\ntesting2 &lt;- read_delim(f, \" \")\n\nand then you look at it in the same way as before:\n\ntesting2\n\n\n\n  \n\n\n\nCheck.\n\\(\\blacksquare\\)\n\nYou might have a spreadsheet on your computer. Create a .csv file from it, and use the ideas of the last part to read it into R Studio.\n\nSolution\nOpen the spreadsheet containing the data you want to read into R. If there are several sheets in the workbook, make sure you’re looking at the right one. Select File, Save As, select “CSV” or “comma-separated values” and give it a name. Save the resulting file somewhere.\nThen read it into an R data frame. This uses read_csv; there are several read_ functions that read in different types of file, and you need to use an appropriate one. Before that, though, again run\n\nf &lt;- file.choose()\n\nto find the .csv file on your computer, and then\n\ncars &lt;- read_csv(f)\n\nto read it in. My spreadsheet was\n\ncars\n\n\n\n  \n\n\n\nSome information about different types of cars.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "getting_used.html#footnotes",
    "href": "getting_used.html#footnotes",
    "title": "1  Getting used to R and R Studio",
    "section": "",
    "text": "Especially if you are on your own computer.↩︎\nThis document was actually produced by literally running this code, a process known as “rendering”, which we will learn about shortly.↩︎\nA big part of coding is dealing with errors. You will forget things, and it is fine. In the same way that it doesn’t matter how many times you get knocked down, it’s key that you get up again each time: it doesn’t matter how many errors you made, it’s key that you fix them. If you want something to sing along with while you do this, I recommend this.↩︎"
  },
  {
    "objectID": "reading-in.html#orange-juice",
    "href": "reading-in.html#orange-juice",
    "title": "2  Reading in data",
    "section": "2.1 Orange juice",
    "text": "2.1 Orange juice\nThe quality of orange juice produced by a manufacturer (identity unknown) is constantly being monitored. The manufacturer has developed a “sweetness index” for its orange juice, for which a higher value means sweeter juice. Is the sweetness index related to a chemical measure such as the amount of water-soluble pectin (parts per million) in the orange juice? Data were obtained from 24 production runs, and the sweetness and pectin content were measured for each run. The data are in link. Open that link up now. You can click on that link just above to open the file.\n\nThe data values are separated by a space. Use the appropriate Tidyverse function to read the data directly from the course website into a “tibble”.\nTake a look at what got read in. Do you have data for all 24 runs?\nIn your data frame, where did the column (variable) names come from? How did R know where to get them from?"
  },
  {
    "objectID": "reading-in.html#making-soap",
    "href": "reading-in.html#making-soap",
    "title": "2  Reading in data",
    "section": "2.2 Making soap",
    "text": "2.2 Making soap\nA company operates two production lines in a factory for making soap bars. The production lines are labelled A and B. A production line that moves faster may produce more soap, but may possibly also produce more “scrap” (that is, bits of soap that can no longer be made into soap bars and will have to be thrown away).\nThe data are in link.\n\nRead the data into R. Display the data.\nThere should be 27 rows. Are there? What columns are there?"
  },
  {
    "objectID": "reading-in.html#handling-shipments",
    "href": "reading-in.html#handling-shipments",
    "title": "2  Reading in data",
    "section": "2.3 Handling shipments",
    "text": "2.3 Handling shipments\nA company called Global Electronics from time to time imports shipments of a certain large part used as a component in several of its products. The size of the shipment varies each time. Each shipment is sent to one of two warehouses (labelled A and B) for handling. The data in link show the size of each shipment (in thousands of parts) and the direct cost of handling it, in thousands of dollars. Also shown is the warehouse (A or B) that handled each shipment.\n\nRead the data into R and display your data frame.\nDescribe how many rows and columns your data frame has, and what they contain.\n\nMy solutions follow:"
  },
  {
    "objectID": "reading-in.html#orange-juice-1",
    "href": "reading-in.html#orange-juice-1",
    "title": "2  Reading in data",
    "section": "2.4 Orange juice",
    "text": "2.4 Orange juice\nThe quality of orange juice produced by a manufacturer (identity unknown) is constantly being monitored. The manufacturer has developed a “sweetness index” for its orange juice, for which a higher value means sweeter juice. Is the sweetness index related to a chemical measure such as the amount of water-soluble pectin (parts per million) in the orange juice? Data were obtained from 24 production runs, and the sweetness and pectin content were measured for each run. The data are in link. Open that link up now. You can click on that link just above to open the file.\n\nThe data values are separated by a space. Use the appropriate Tidyverse function to read the data directly from the course website into a “tibble”.\n\nSolution\nStart with this (almost always):\n\nlibrary(tidyverse)\n\nThe appropriate function, the data values being separated by a space, will be read_delim. Put the URL as the first thing in read_delim, or (better) define it into a variable first:1\n\nurl &lt;- \"http://ritsokiguess.site/datafiles/ojuice.txt\"\njuice &lt;- read_delim(url, \" \")\n\nRows: 24 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): run, sweetness, pectin\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nread_delim (or read_csv or any of the others) tell you what variables were read in, and also tell you about any “parsing errors” where it couldn’t work out what was what. Here, we have three variables, which is entirely consistent with the three columns of data values in the file.\nread_delim can handle data values separated by any character, not just spaces, but the separating character, known as a “delimiter”, does not have a default, so you have to say what it is, every time.\n\\(\\blacksquare\\)\n\nTake a look at what got read in. Do you have data for all 24 runs?\n\nSolution\nType the name of the data frame in a code chunk (a new one, or add it to the end of the previous one). Because this is actually a “tibble”, which is what read_delim reads in, you’ll only actually see the first 10 lines, but it will tell you how many lines there are altogether, and you can click on the appropriate thing to see the rest of it.\n\njuice\n\n\n\n  \n\n\n\nI appear to have all the data. If you want further convincing, click Next a couple of times to be sure that the runs go down to number 24.\n\\(\\blacksquare\\)\n\nIn your data frame, where did the column (variable) names come from? How did R know where to get them from?\n\nSolution\nThey came from the top line of the data file, so we didn’t have to specify them. This is the default behaviour of all the read_ functions, so we don’t have to ask for it specially.\nExtra: in fact, if the top line of your data file is not variable names, that’s when you have to say something special. The read_ functions have an option col_names which can either be TRUE (the default), which means “read them in from the top line”, FALSE (“they are not there, so make some up”) or a list of column names to use. You might use the last alternative when the column names that are in the file are not the ones you want to use; in that case, you would also say skip=1 to skip the first line. For example, with file a.txt thus:\na b\n1 2\n3 4\n5 6\nyou could read the same data but call the columns x and y thus:\n\nread_delim(\"a.txt\", \" \", col_names = c(\"x\", \"y\"), skip = 1)\n\nRows: 3 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "reading-in.html#making-soap-1",
    "href": "reading-in.html#making-soap-1",
    "title": "2  Reading in data",
    "section": "2.5 Making soap",
    "text": "2.5 Making soap\nA company operates two production lines in a factory for making soap bars. The production lines are labelled A and B. A production line that moves faster may produce more soap, but may possibly also produce more “scrap” (that is, bits of soap that can no longer be made into soap bars and will have to be thrown away).\nThe data are in link.\n\nRead the data into R. Display the data.\n\nSolution\nRead directly from the URL, most easily:\n\nurl &lt;- \"http://ritsokiguess.site/datafiles/soap.txt\"\nsoap &lt;- read_delim(url, \" \")\n\nRows: 27 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): line\ndbl (3): case, scrap, speed\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsoap\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nThere should be 27 rows. Are there? What columns are there?\n\nSolution\nThere are indeed 27 rows, one per observation. The column called case identifies each particular run of a production line (scroll down to see that it gets to 27 as well). Though it is a number, it is an identifier variable and so should not be treated quantitatively. The other columns (variables) are scrap and speed (quantitative) and line (categorical). These indicate which production line was used for each run, the speed it was run at, and the amount of scrap produced.\nThis seems like an odd place to end this question, but later we’ll be using these data to draw some graphs.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "reading-in.html#handling-shipments-1",
    "href": "reading-in.html#handling-shipments-1",
    "title": "2  Reading in data",
    "section": "2.6 Handling shipments",
    "text": "2.6 Handling shipments\nA company called Global Electronics from time to time imports shipments of a certain large part used as a component in several of its products. The size of the shipment varies each time. Each shipment is sent to one of two warehouses (labelled A and B) for handling. The data in link show the size of each shipment (in thousands of parts) and the direct cost of handling it, in thousands of dollars. Also shown is the warehouse (A or B) that handled each shipment.\n\nRead the data into R and display your data frame.\n\nSolution\nIf you open the data file in your web browser, it will probably open as a spreadsheet, which is not really very helpful, since then it is not clear what to do with it. You could, I suppose, save it and upload it to R Studio Cloud, but it requires much less brainpower to open it directly from the URL:\n\nurl &lt;- \"http://ritsokiguess.site/datafiles/global.csv\"\nshipments &lt;- read_csv(url)\n\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): warehouse\ndbl (2): size, cost\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf you display your data frame and it looks like this, you are good (you can give the data frame any name):\n\nshipments\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nDescribe how many rows and columns your data frame has, and what they contain.\n\nSolution\nIt has 10 rows and 3 columns. You need to say this.\nThat is, there were 10 shipments recorded, and for each of them, 3 variables were noted: the size and cost of the shipment, and the warehouse it was handled at.\nWe will also be making some graphs of these data later.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "reading-in.html#footnotes",
    "href": "reading-in.html#footnotes",
    "title": "2  Reading in data",
    "section": "",
    "text": "I say “better” because otherwise the read line gets rather long. This way you read it as “the URL is some long thing that I don’t care about especially, and I what I need to do is to read the data from that URL, separated by spaces.”↩︎"
  },
  {
    "objectID": "data-summaries.html#north-carolina-births",
    "href": "data-summaries.html#north-carolina-births",
    "title": "3  Data exploration",
    "section": "3.1 North Carolina births",
    "text": "3.1 North Carolina births\nThe data in file link are about 500 randomly chosen births of babies in North Carolina. There is a lot of information: not just the weight at birth of the baby, but whether the baby was born prematurely, the ages of the parents, whether the parents are married, how long (in weeks) the pregnancy lasted (this is called the “gestation”) and so on.\n\nRead in the data from the file into R, bearing in mind what type of file it is.\nFrom your output, verify that you have the right number of observations and that you have several variables. Which of your variables correspond to birthweight, prematureness and length of pregnancy? (You might have to make guesses based on the names of the variables.)\nThe theory behind the \\(t\\)-test (which we do later) says that the distribution of birth weights should be (approximately) normally distributed. Obtain a histogram of the birth weights. Does it look approximately normal? Comment briefly. (You’ll have to pick a number of bins for your histogram first. I don’t mind very much what you pick, as long as it’s not obviously too many or too few bins.)"
  },
  {
    "objectID": "data-summaries.html#more-about-the-nc-births",
    "href": "data-summaries.html#more-about-the-nc-births",
    "title": "3  Data exploration",
    "section": "3.2 More about the NC births",
    "text": "3.2 More about the NC births\nThis is an exploration of some extra issues around the North Carolina births data set.\n\nHow short does a pregnancy have to be, for the birth to be classified as “premature”? Deduce this from the data, by drawing a suitable graph or otherwise.\nExplore the relationship between birth weight and length of pregancy (“gestation”) using a suitable graph. What do you see?\nDo a web search to find the standard (North American) definition of a premature birth. Does that correspond to what you saw in the data? Cite the website you used, for example by saying “according to URL, …”, with URL replaced by the address of the website you found."
  },
  {
    "objectID": "data-summaries.html#nenana-alaska",
    "href": "data-summaries.html#nenana-alaska",
    "title": "3  Data exploration",
    "section": "3.3 Nenana, Alaska",
    "text": "3.3 Nenana, Alaska\nNenana, Alaska, is about 50 miles west of Fairbanks. Every spring, there is a contest in Nenana. A wooden tripod is placed on the frozen river, and people try to guess the exact minute when the ice melts enough for the tripod to fall through the ice. The contest started in 1917 as an amusement for railway workers, and has taken place every year since. Now, hundreds of thousands of people enter their guesses on the Internet and the prize for the winner can be as much as $300,000.\nBecause so much money is at stake, and because the exact same tripod is placed at the exact same spot on the ice every year, the data are consistent and accurate. The data are in link.\n\nRead the data into R. Note that the values are separated by tabs rather than spaces, so you’ll need an appropriate read_ to read it in.\nFind a way of displaying how many rows and columns your data frame has, and some of the values. Describe the first and last of the variables that you appear to have.\nDates and times are awkward to handle with software. (We see more ways later in the course.) The column JulianDate expresses the time that the tripod fell through the ice as a fractional number of days since December 31. This enables the time (as a fraction of the way through the day) to be recorded as well, the whole thing being an ordinary number. Make a histogram of the Julian dates. Comment briefly on its shape.\nPlot JulianDate against Year on a scatterplot. What recent trends, if any, do you see? Comment briefly."
  },
  {
    "objectID": "data-summaries.html#computerized-accounting",
    "href": "data-summaries.html#computerized-accounting",
    "title": "3  Data exploration",
    "section": "3.4 Computerized accounting",
    "text": "3.4 Computerized accounting\nBeginning accounting students need to learn to learn to audit in a computerized environment. A sample of beginning accounting students took each of two tests: the Computer Attitude Scale (CAS) and the Computer Anxiety Rating Scale (CARS). A higher score in each indicates greater anxiety around computers. The test scores are scaled to be between 0 and 5. Also noted was each student’s gender. The data are in http://ritsokiguess.site/datafiles/compatt.txt. The data values are separated by spaces.\n\nRead the data into R. Do you have what you expected? Explain briefly.\nHow many males and females were there in the sample?\nDo the CAS scores tend to be higher for females or for males? Draw a suitable graph to help you decide, and come to a conclusion.\nFind the median CAS scores for each gender. Does this support what you saw on your plot? Explain briefly.\nFind the mean and standard deviation of both CAS and CARS scores (for all the students combined, ie. not separated by gender) without naming those columns explicitly."
  },
  {
    "objectID": "data-summaries.html#test-scores-in-two-classes",
    "href": "data-summaries.html#test-scores-in-two-classes",
    "title": "3  Data exploration",
    "section": "3.5 Test scores in two classes",
    "text": "3.5 Test scores in two classes\nOpen R Studio. Create a new Text File by selecting File, New File and Text File. You should see a new empty, untitled window appear at the top left. In that window, type or copy the data below (which are scores on a test for students in two different classes):\n\nclass score\nken 78\nken 62\nken 59\nken 69\nken 81\nthomas 83\nthomas 77\nthomas 63\nthomas 61\nthomas 79\nthomas 72\n\nSave the file, using a filename of your choice (with, perhaps, extension .txt). Or, if you prefer, use the one at link.\n\nRead the data into a data frame called marks, using read_delim, and list the data frame (by typing its name) to confirm that you read the data values properly. Note that the top line of the data file contains the names of the variables, as it ought to.\n* Obtain side-by-side boxplots of the scores for each class.\nDo the two classes appear to have similar or different scores, on average? Explain briefly.\nObtain a boxplot of all the scores together, regardless of which class they came from.\nCompute the median score (of all the scores together). Does this seem about right, looking at the boxplot? Explain briefly."
  },
  {
    "objectID": "data-summaries.html#unprecedented-rainfall",
    "href": "data-summaries.html#unprecedented-rainfall",
    "title": "3  Data exploration",
    "section": "3.6 Unprecedented rainfall",
    "text": "3.6 Unprecedented rainfall\nIn 1997, a company in Davis, California, had problems with odour in its wastewater facility. According to a company official, the problems were caused by “unprecedented weather conditions” and “because rainfall was at 170 to 180 percent of its normal level, the water in the holding ponds took longer to exit for irrigation, giving it more time to develop an odour.”\nAnnual rainfall data for the Davis area is here. Note that clicking on the link will display the file, and right-clicking on the link will give you some options, one of which is Copy Link Address, which you can then paste into your R Notebook.\nThe rainfall is measured in inches.\n\nRead in and display (some of) the data.\nSummarize the data frame.\nMake a suitable plot of the rainfall values. (We are not, for the time being, concerned about the years.)\nHow would you describe the shape of the distribution of rainfall values?\nIn the quote at the beginning of the question, where do you think the assertion that the 1997 rainfall was “at 170 to 180 percent of its normal level” came from? Explain briefly.\nDo you think the official’s calculation was reasonable? Explain briefly. (Note that this is not the same as asking whether the official’s calculation was correct. This is an important distinction for you to make.)\nDo you think that the official was right to use the word “unprecedented” to describe the 1997 rainfall? Justify your answer briefly."
  },
  {
    "objectID": "data-summaries.html#learning-algebra",
    "href": "data-summaries.html#learning-algebra",
    "title": "3  Data exploration",
    "section": "3.7 Learning algebra",
    "text": "3.7 Learning algebra\nAt a high school in New Jersey, teachers were interested in what might help students to learn algebra. One idea was laptops as a learning aid, to see whether having access to one helped with algebra scores. (This was some time ago.) The 20 students in one class were given laptops to use in school and at home, while the 27 students in another class were not given laptops. For all of these students, the final exam score in algebra was recorded. The data are in http://ritsokiguess.site/datafiles/algebra.txt, with two columns, one indicating whether the student received a laptop or not, and the other giving their score on the algebra final exam.\n\nRead in and display (some of) the data. Do you have (i) the correct number of observations, and (ii) the correct type of columns? Explain briefly.\nMake a suitable graph of these data.\nComment briefly on your graph, thinking about what the teachers would like to know.\nWork out the median and inter-quartile range for the students who did and who did not have laptops, and compare with the boxplot. (In R, the inter-quartile range is IQR in uppercase.)\n\nMy solutions follow:"
  },
  {
    "objectID": "data-summaries.html#north-carolina-births-1",
    "href": "data-summaries.html#north-carolina-births-1",
    "title": "3  Data exploration",
    "section": "3.8 North Carolina births",
    "text": "3.8 North Carolina births\nThe data in file link are about 500 randomly chosen births of babies in North Carolina. There is a lot of information: not just the weight at birth of the baby, but whether the baby was born prematurely, the ages of the parents, whether the parents are married, how long (in weeks) the pregnancy lasted (this is called the “gestation”) and so on.\n\nRead in the data from the file into R, bearing in mind what type of file it is.\n\nSolution\nThis is a .csv file (it came from a spreadsheet), so it needs reading in accordingly. Work directly from the URL:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/ncbirths2.csv\"\nbw &lt;- read_csv(myurl)\n\nRows: 500 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): father_age, mother_age, weeks_gestation, pre_natal_visits, marital...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis shows you which variables the data set has (some of the names got a bit mangled), and it shows you that they are all integers except for the birth weight (a decimal number).\nThe easiest way to find out how many rows and columns there are is simply to list the data frame:\n\nbw\n\n\n\n  \n\n\n\nor you can take a “glimpse” of it, which is good if you have a lot of columns:\n\nglimpse(bw)\n\nRows: 500\nColumns: 10\n$ father_age           &lt;dbl&gt; 27, 35, 34, NA, 35, 32, 33, 38, 28, NA, 28, 34, N…\n$ mother_age           &lt;dbl&gt; 26, 33, 22, 16, 33, 24, 33, 35, 29, 19, 26, 31, 1…\n$ weeks_gestation      &lt;dbl&gt; 38, 40, 37, 38, 39, 36, 38, 38, 40, 34, 39, 39, 3…\n$ pre_natal_visits     &lt;dbl&gt; 14, 11, 10, 9, 12, 12, 15, 16, 5, 10, 15, 15, 0, …\n$ marital_status       &lt;dbl&gt; 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2…\n$ mother_weight_gained &lt;dbl&gt; 32, 23, 50, NA, 15, 12, 60, 2, 20, NA, 45, 22, 20…\n$ low_birthweight      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ weight_pounds        &lt;dbl&gt; 6.8750, 6.8125, 7.2500, 8.8125, 8.8125, 5.8125, 6…\n$ premie               &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0…\n$ few_visits           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0…\n\n\nEither of these displays show that there are 500 rows (observations, here births) and 10 columns (variables), and they both show what the variables are called. So they’re both good as an answer to the question.\nExtra: As is rather too often the way, the original data weren’t like this, and I had to do some tidying first. Here’s the original:\n\nmy_old_url &lt;- \"http://ritsokiguess.site/datafiles/ncbirths_original.csv\"\nbw0 &lt;- read_csv(my_old_url)\n\nRows: 500 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): Father Age, Mother Age, Weeks Gestation, Pre-natal Visits, Marital...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbw0\n\n\n\n  \n\n\n\nWhat you’ll notice is that the variables have spaces in their names, which would require special handling later. The glimpse output shows you what to do about those spaces in variable names:\n\nglimpse(bw0)\n\nRows: 500\nColumns: 10\n$ `Father Age`           &lt;dbl&gt; 27, 35, 34, NA, 35, 32, 33, 38, 28, NA, 28, 34,…\n$ `Mother Age`           &lt;dbl&gt; 26, 33, 22, 16, 33, 24, 33, 35, 29, 19, 26, 31,…\n$ `Weeks Gestation`      &lt;dbl&gt; 38, 40, 37, 38, 39, 36, 38, 38, 40, 34, 39, 39,…\n$ `Pre-natal Visits`     &lt;dbl&gt; 14, 11, 10, 9, 12, 12, 15, 16, 5, 10, 15, 15, 0…\n$ `Marital Status`       &lt;dbl&gt; 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,…\n$ `Mother Weight Gained` &lt;dbl&gt; 32, 23, 50, NA, 15, 12, 60, 2, 20, NA, 45, 22, …\n$ `Low Birthweight?`     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ `Weight (pounds)`      &lt;dbl&gt; 6.8750, 6.8125, 7.2500, 8.8125, 8.8125, 5.8125,…\n$ `Premie?`              &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,…\n$ `Few Visits?`          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,…\n\n\nWhat you have to do is to surround the variable name with “backticks”. (On my keyboard, that’s on the key to the left of number 1, where the squiggle is, that looks like a backwards apostrophe. Probably next to Esc, depending on the layout of your keyboard.) For example, to get the mean mother’s age, you have to do this:\n\nbw0 %&gt;% summarize(mom_mean = mean(`Mother Age`))\n\n\n\n  \n\n\n\nAlthough almost all of the variables are stored as integers, the ones that have a question mark in their name are actually “logical”, true or false, with 1 denoting true and 0 false. We could convert them later if we want to. A question mark is not a traditional character to put in a variable name either, so we have to surround these variables with backticks too.\nIn fact, all the variables have “illegal” names in one way or another: they contain spaces, or question marks, or brackets. So they all need backticks, which, as you can imagine, is rather awkward. The Capital Letters at the start of each word are also rather annoying to type every time.\nPeople who collect data are not always the people who analyze it, so there is not always a lot of thought given to column names in spreadsheets.\nSo how did I get you a dataset with much more sane variable names? Well, I used the janitor package, which has a function in it called clean_names. This is what it does:\n\nlibrary(janitor)\nbw0 %&gt;% clean_names() %&gt;% glimpse()\n\nRows: 500\nColumns: 10\n$ father_age           &lt;dbl&gt; 27, 35, 34, NA, 35, 32, 33, 38, 28, NA, 28, 34, N…\n$ mother_age           &lt;dbl&gt; 26, 33, 22, 16, 33, 24, 33, 35, 29, 19, 26, 31, 1…\n$ weeks_gestation      &lt;dbl&gt; 38, 40, 37, 38, 39, 36, 38, 38, 40, 34, 39, 39, 3…\n$ pre_natal_visits     &lt;dbl&gt; 14, 11, 10, 9, 12, 12, 15, 16, 5, 10, 15, 15, 0, …\n$ marital_status       &lt;dbl&gt; 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2…\n$ mother_weight_gained &lt;dbl&gt; 32, 23, 50, NA, 15, 12, 60, 2, 20, NA, 45, 22, 20…\n$ low_birthweight      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ weight_pounds        &lt;dbl&gt; 6.8750, 6.8125, 7.2500, 8.8125, 8.8125, 5.8125, 6…\n$ premie               &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0…\n$ few_visits           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0…\n\n\nAll the spaces have been replaced by underscores, the question marks and brackets have been removed, and all the uppercase letters have been made lowercase. The spaces have been replaced by underscores because an underscore is a perfectly legal thing to have in a variable name. I saved this dataset into the file you read in.\n\\(\\blacksquare\\)\n\nFrom your output, verify that you have the right number of observations and that you have several variables. Which of your variables correspond to birthweight, prematureness and length of pregnancy? (You might have to make guesses based on the names of the variables.)\n\nSolution\nAs a reminder:\n\nglimpse(bw)\n\nRows: 500\nColumns: 10\n$ father_age           &lt;dbl&gt; 27, 35, 34, NA, 35, 32, 33, 38, 28, NA, 28, 34, N…\n$ mother_age           &lt;dbl&gt; 26, 33, 22, 16, 33, 24, 33, 35, 29, 19, 26, 31, 1…\n$ weeks_gestation      &lt;dbl&gt; 38, 40, 37, 38, 39, 36, 38, 38, 40, 34, 39, 39, 3…\n$ pre_natal_visits     &lt;dbl&gt; 14, 11, 10, 9, 12, 12, 15, 16, 5, 10, 15, 15, 0, …\n$ marital_status       &lt;dbl&gt; 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2…\n$ mother_weight_gained &lt;dbl&gt; 32, 23, 50, NA, 15, 12, 60, 2, 20, NA, 45, 22, 20…\n$ low_birthweight      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ weight_pounds        &lt;dbl&gt; 6.8750, 6.8125, 7.2500, 8.8125, 8.8125, 5.8125, 6…\n$ premie               &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0…\n$ few_visits           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0…\n\n\nI do indeed have 500 observations (rows) on 10 variables (columns; “several”). (If you don’t have several variables, check to see that you didn’t use read_delim or something by mistake.) After that, you see all the variables by name, with what type of values they have,1 and the first few of the values.2\nThe variable weight_pounds is the birthweight (in pounds), premie is 1 for a premature baby and 0 for a full-term baby, and weeks_gestation is the number of weeks the pregnancy lasted.\n\\(\\blacksquare\\)\n\nThe theory behind the \\(t\\)-test (which we do later) says that the birth weights should be (approximately) normally distributed. Obtain a histogram of the birth weights. Does it look approximately normal? Comment briefly. (You’ll have to pick a number of bins for your histogram first. I don’t mind very much what you pick, as long as it’s not obviously too many or too few bins.)\n\nSolution\nYou’ll have seen that I often start with 10 bins, or maybe not quite that many if I don’t have much data, and this is a decent general principle. That would give\n\nggplot(bw, aes(x = weight_pounds)) + geom_histogram(bins = 10)\n\n\n\n\nwhich is perfectly acceptable with 500 observations. You can try something a bit more or a bit less, and see how you like it in comparison. What you are looking for is a nice clear picture of shape. If you have too few bins, you’ll lose the shape:\n\nggplot(bw, aes(x = weight_pounds)) + geom_histogram(bins = 4)\n\n\n\n\n(is that leftmost bin an indication of skewness or some observations that happen to be smallish?)\nAnd if you have too many, the shape will be there, but it will be hard to make out in all the noise, with frequencies going up and down:\n\nggplot(bw, aes(x = weight_pounds)) + geom_histogram(bins = 30)\n\n\n\n\nI generally am fairly relaxed about the number of bins you use, as long as it’s not clearly too few or too many. You might have done exercises in the past that illustrate that the choice of number of bins (or the class intervals where you move from one bin to the next, which is another issue that I won’t explore here) can make an appreciable difference to how a histogram looks.\nExtra: I had some thoughts about this issue that I put in a blog post, that you might like to read: link. The nice thing about Sturges’ rule, mentioned there, is that you can almost get a number of bins for your histogram in your head (as long as you know the powers of 2, that is). What you do is to start with your sample size, here \\(n=500\\). You find the next power of 2 above that, which is here \\(512=2^9\\). You then take that power and add 1, to get 10 bins. If you don’t like that, you can get R to calculate it for you:\n\nnclass.Sturges(bw$weight_pounds)\n\n[1] 10\n\n\nThe place where Sturges’ rule comes from is an assumption of normal data (actually a binomial approximation to the normal, backwards though that sounds). If you have less than 30 observations, you’ll get fewer than 6 bins, which won’t do much of a job of showing the shape. Rob Hyndman wrote a critical note about Sturges’ rule in which he asserts that it is just plain wrong (if you have taken B57, this note is very readable).\nSo what to use instead? Well, judgment is still better than something automatic, but if you want a place to start from, something with a better foundation than Sturges is the Freedman-Diaconis rule. This, in its original formulation, gives a bin width rather than a number of bins:\n\\[\nw=2(IQR)n^{-1/3}\n\\]\nThe nice thing about this is that it uses the interquartile range, so it won’t be distorted by outliers. geom_histogram can take a bin width, so we can use it as follows:\n\nw &lt;- 2 * IQR(bw$weight_pounds) * 500^(-1 / 3)\nw\n\n[1] 0.4094743\n\nggplot(bw, aes(x = weight_pounds)) + geom_histogram(binwidth = w)\n\n\n\n\nR also has\n\nnc &lt;- nclass.FD(bw$weight_pounds)\nnc\n\n[1] 26\n\n\nwhich turns the Freedman-Diaconis rule into a number of bins rather than a binwidth; using that gives the same histogram as we got with binwidth.\nIn my opinion, Freedman-Diaconis tends to give too many bins (here there are 26 rather than the 10 of Sturges). But I put it out there for you to make your own call.\nAnother way to go is a “density plot”. This is a smoothed-out version of a histogram that is not obviously frequencies in bins, but which does have a theoretical basis. It goes something like this:\n\nggplot(bw, aes(x = weight_pounds)) + geom_density()\n\n\n\n\ngeom_density has an optional parameter that controls how smooth or wiggly the picture is, but the default is usually good.\nAlright, before we got distracted, we were assessing normality. What about that?\nIt is mostly normal-looking, but I am suspicious about those very low birth weights, the ones below about 4 pounds. There are a few too many of those, as I see it.\nIf you think this is approximately normal, you need to make some comment along the lines of “the shape is approximately symmetric with no outliers”. I think my first answer is better, but this answer is worth something, since it is a not completely unreasonable interpretation of the histogram.\nI have been making the distinction between a histogram (for one quantitative variable) and side-by-side boxplots (for one quantitative variable divided into groups by one categorical variable). When you learned the boxplot, you probably learned it in the context of one quantitative variable. You can draw a boxplot for that, too, but the ggplot boxplot has an x as well as a y. What you do to make a single boxplot is to set the x equal 1, which produces a weird \\(x\\)-axis (that you ignore):\n\nggplot(bw, aes(x = 1, y = weight_pounds)) + geom_boxplot()\n\n\n\n\nThe high weight is actually an outlier, but look at all those outliers at the bottom!3\nI think the reason for those extra very low values is that they are the premature births (that can result in very small babies). Which leads to the additional question coming up later.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#more-about-the-nc-births-1",
    "href": "data-summaries.html#more-about-the-nc-births-1",
    "title": "3  Data exploration",
    "section": "3.9 More about the NC births",
    "text": "3.9 More about the NC births\nThis is an exploration of some extra issues around the North Carolina births data set.\n\nHow short does a pregnancy have to be, for the birth to be classified as “premature”? Deduce this from the data, by drawing a suitable graph or otherwise.\n\nSolution\nTo figure it out from the data, we can see how weeks_gestation depends on premie. Some possibilities are boxplots or a scatterplot. Either of the first two graphs would get full credit (for the graphing part: you still have to do the explanation) if this were being marked:\n\nggplot(bw,aes(x=factor(premie), y=weeks_gestation)) + geom_boxplot()\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThe warning is because the prematurity of one of the babies is not known. Or\n\nggplot(bw,aes(x=premie, y=weeks_gestation)) + geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe same warning again, for the same reason.\nNotice how the graphs are similar in syntax, because the what-to-plot is the same (apart from the factor thing) and we just make a small change in how-to-plot-it. In the boxplot, the thing on the \\(x\\)-scale needs to be categorical, and premie is actually a number, so we’d better make it into a factor, which is R’s version of a categorical variable. premie is actually a categorical variable (“premature” or “not premature”) masquerading as a quantitative one (1 or 0). It is an “indicator variable”, if you’re familiar with that term.\nIt looks as if the breakpoint is 37 weeks: a pregnancy at least that long is considered normal, but a shorter one ends with a premature birth. Both plots show the same thing: the premie=1 births all go with short pregnancies, shorter than 37 weeks. This is completely clear cut.\nAnother way to attack this is to use summarize, finding the max and min:\n\nbw %&gt;% summarize( n=n(),\nmin=min(weeks_gestation),\nmax=max(weeks_gestation))\n\n\n\n  \n\n\n\nonly this is for all the babies, premature or not.4 So we want it by prematurity, which means a group_by first:\n\nbw %&gt;% group_by(premie) %&gt;%\nsummarize( n=n(),\nmin=min(weeks_gestation),\nmax=max(weeks_gestation))\n\n\n\n  \n\n\n\ngroup_by with a number works, even though using the number in premie in a boxplot didn’t. group_by just uses the distinct values, whether they are numbers, text or factor levels.\nAny of these graphs or summaries will help you answer the question, in the same way. The ultimate issue here is “something that will get the job done”: it doesn’t matter so much what.\nExtra: In R, NA means “missing”. When you try to compute something containing a missing value, the answer is usually missing (since you don’t know what the missing value is). That’s why the first summarize gave us missing values: there was one missing weeks of gestation in with all the ones for which we had values, so the max and min had to be missing as well. In the second summarize, the one by whether a baby was born prematurely or not, we learn a bit more about that missing premie: evidently its weeks of gestation was missing as well, since the min and max of that were missing.5\nHere’s that baby. I’m doing a bit of fiddling to show all the columns (as rows, since there’s only one actual row). Don’t worry about the second line of code below; we will investigate that later in the course. Its job here is to show the values nicely:\n\nbw %&gt;% \n  filter(is.na(premie)) %&gt;% \n  pivot_longer(everything(), names_to=\"name\", values_to=\"value\")\n\n\n\n  \n\n\n\nThe only thing that was missing was its weeks of gestation, but that prevented anyone from figuring out whether it was premature or not.\n\\(\\blacksquare\\)\n\nExplore the relationship between birth weight and length of pregancy (“gestation”) using a suitable graph. What do you see?\n\nSolution\nThis needs to be a scatterplot because these are both quantitative variables:\n\nggplot(bw,aes(x=weeks_gestation, y=weight_pounds)) + geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nYou see a rather clear upward trend. Those very underweight babies came from very short pregnancies, but the vast majority of pregnancies were of more or less normal length (40 weeks is normal) and resulted in babies of more or less normal birth weight.\nExtra: I want to illustrate something else: how about colouring the births that were premature? Piece of cake with ggplot:\n\nggplot(bw,aes(x=weeks_gestation, y=weight_pounds, colour = premie)) + \n  geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThat was rather silly because ggplot treated prematureness as a continuous variable, and plotted the values on a dark blue-light blue scale. This is the same issue as on the boxplot above, and has the same solution:\n\nggplot(bw,aes(x=weeks_gestation, y=weight_pounds, \n              colour = factor(premie))) + geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nBetter.\nWith the normal-length pregnancies (red), there seems to be no relationship between length of pregnancy and birth weight, just a random variation. But with the premature births, a shorter pregnancy typically goes with a lower birth weight. This would be why the birth weights for the premature births were more variable.\n\\(\\blacksquare\\)\n\nDo a web search to find the standard (North American) definition of a premature birth. Does that correspond to what you saw in the data? Cite the website you used, for example by saying “according to URL, …”, with URL replaced by the address of the website you found.\n\nSolution\nThe website http://www.mayoclinic.org/diseases-conditions/premature-birth/basics/definition/con-20020050 says that “a premature birth is one that occurs before the start of the 37th week of pregnancy”, which is exactly what we found. (Note that I am citing the webpage on which I found this, and I even made it into a link so that you can check it.) The Mayo Clinic is a famous hospital system with locations in several US states, so I think we can trust what its website says.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#nenana-alaska-1",
    "href": "data-summaries.html#nenana-alaska-1",
    "title": "3  Data exploration",
    "section": "3.10 Nenana, Alaska",
    "text": "3.10 Nenana, Alaska\nNenana, Alaska, is about 50 miles west of Fairbanks. Every spring, there is a contest in Nenana. A wooden tripod is placed on the frozen river, and people try to guess the exact minute when the ice melts enough for the tripod to fall through the ice. The contest started in 1917 as an amusement for railway workers, and has taken place every year since. Now, hundreds of thousands of people enter their guesses on the Internet and the prize for the winner can be as much as $300,000.\nBecause so much money is at stake, and because the exact same tripod is placed at the exact same spot on the ice every year, the data are consistent and accurate. The data are in link.\n\nRead the data into R. Note that the values are separated by tabs rather than spaces, so you’ll need an appropriate read_ to read it in.\n\nSolution\nThese are “tab-separated values”, so read_tsv is the thing, as for the Australian athletes:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/nenana.txt\"\nnenana &lt;- read_tsv(myurl)\n\nRows: 87 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Date&Time\ndbl (2): Year, JulianDate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nUse whatever name you like for the data frame. One that is different from any of the column headers is smart; then it is clear whether you mean the whole data frame or one of its columns. ice or melt or anything like that would also be good.\nI haven’t asked you to display or check the data (that’s coming up), but if you look at it and find that it didn’t work, you’ll know to come back and try this part again. R usually gets it right or gives you an error.\nIf you look at the data, they do appear to be separated by spaces, but the text version of the date and time also have spaces in them, so things might go astray if you try and read the values in without recognizing that the actual separator is a tab:\n\nx &lt;- read_delim(myurl, \" \")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 87 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): Year   JulianDate  Date&Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOuch! A hint as to what went wrong comes from looking at the read-in data frame:\n\nx\n\n\n\n  \n\n\n\nThose t symbols mean “tab character”, which is our hint that the values were separated by tabs rather than spaces.\nMore detail (if you can bear to see it) is here:\n\nproblems(x)\n\n\n\n  \n\n\n\nThe first line of the data file (with the variable names in it) had no spaces, only tabs, so read_delim thinks there is one column with a very long name, but in the actual data, there are five space-separated columns. The text date-times are of the form “April 30 at 11:30 AM”, which, if you think it’s all separated by spaces, is actually 5 things: April, 30, at and so on. These are the only things that are separated by spaces, so, from that point of view, there are five columns.\n\\(\\blacksquare\\)\n\nFind a way of displaying how many rows and columns your data frame has, and some of the values. Describe the first and last of the variables that you appear to have.\n\nSolution\nThe easiest is just to display the tibble:\n\nnenana\n\n\n\n  \n\n\n\nAlternatively, you can take a glimpse of it:\n\nglimpse(nenana)\n\nRows: 87\nColumns: 3\n$ Year        &lt;dbl&gt; 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926…\n$ JulianDate  &lt;dbl&gt; 120.4795, 131.3983, 123.6066, 132.4490, 131.2795, 132.5559…\n$ `Date&Time` &lt;chr&gt; \"April 30 at 11:30 AM\", \"May 11 at 9:33 AM\", \"May 3 at 2:3…\n\n\nThere are 87 years, and 3 columns (variables). The first column is year, and the last column is the date and time that the tripod fell into the river, written as a piece of text. I explain the second column in a moment.\n\\(\\blacksquare\\)\n\nDates and times are awkward to handle with software. (We see more ways later in the course.) The column JulianDate expresses the time that the tripod fell through the ice as a fractional number of days since December 31. This enables the time (as a fraction of the way through the day) to be recorded as well, the whole thing being an ordinary number. Make a histogram of the Julian dates. Comment briefly on its shape.\n\nSolution\nWith a ggplot histogram, we need a number of bins first. I can do Sturges’ rule in my head: the next power of 2 up from 87 (our \\(n\\)) is 128, which is \\(2^7\\), so the base 2 log of 87 rounds up to 7. That plus one is 8, so we need 8 bins. For you, any not-insane number of bins will do, or any not-insane bin width, if you want to go that way:\n\nggplot(nenana, aes(x = JulianDate)) + geom_histogram(bins = 8)\n\n\n\n\nNote that you need to type JulianDate exactly as it appears, capital letters and all. R is case-sensitive.\nThis histogram looks more or less symmetric (and, indeed, normal). I really don’t think you can justify an answer other than “symmetric” here. (Or “approximately normal”: that’s good too.) If your histogram is different, say so. I think that “hole” in the middle is not especially important.\nWe haven’t done normal quantile plots yet, but looking ahead:\n\nggplot(nenana, aes(sample = JulianDate)) + stat_qq() + stat_qq_line()\n\n\n\n\nThat hugs the line pretty well, so I would call it close to normally-distributed. It bulges away from the line because there are more values just below 120 than you would expect for a normal. This corresponds to the histogram bar centred just below 120 being taller than you would have expected.6\nExtra: looking way ahead (to almost the end of the R stuff), this is how you handle the dates and times:\n\nlibrary(lubridate)\nnenana %&gt;%\n  mutate(longdt = str_c(Year, \" \", `Date&Time`)) %&gt;%\n  mutate(datetime = ymd_hm(longdt, tz = \"America/Anchorage\"))\n\n\n\n  \n\n\n\nI am not doing any further analysis with these, so just displaying them is good.\nI have to do a preliminary step to get the date-times with their year in one place. str_c glues pieces of text together: in this case, the year, a space, and then the rest of the Date&Time. I stored this in longdt. The second mutate is the business end of it: ymd_hm takes a piece of text containing a year, month (by name or number), day, hours, minutes in that order, and extracts those things from it, storing the whole thing as an R date-time. Note that the AM/PM was handled properly. The benefit of doing that is we can extract anything from the dates, such as the month or day of week, or take differences between the dates. Or even check that the Julian dates were calculated correctly (the lubridate function is called yday for “day of year”):\n\nnenana %&gt;%\n  mutate(longdt = str_c(Year, \" \", `Date&Time`)) %&gt;%\n  mutate(datetime = ymd_hm(longdt, tz = \"America/Anchorage\")) %&gt;%\n  mutate(jd = yday(datetime)) -&gt;\nnenana2\nnenana2 %&gt;% select(JulianDate, jd, datetime)\n\n\n\n  \n\n\n\nThe Julian days as calculated are the same. Note that these are not rounded; the Julian day begins at midnight and lasts until the next midnight. Thus Julian day 132 is May 12 (in a non-leap year like 1922) and the reason that the Julian date given in the file for that year would round to 133 is that it is after noon (1:20pm as you see).\n\\(\\blacksquare\\)\n\nPlot JulianDate against Year on a scatterplot. What recent trends, if any, do you see? Comment briefly.\n\nSolution\ngeom_point:\n\nggplot(nenana, aes(x = Year, y = JulianDate)) + geom_point()\n\n\n\n\nThis is actually a small-but-real downward trend, especially since about 1960, but the large amount of variability makes it hard to see, so I’m good with either “no trend” or “weak downward trend” or anything roughly like that. There is definitely not much trend before 1960, but most of the really early break-ups (less than about 118) have been since about 1990.\nYou can even add to the ggplot, by putting a smooth trend on it:\n\nggplot(nenana, aes(x = Year, y = JulianDate)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis is R’s version of a trend that is not constrained to be linear (so that it “lets the data speak for itself”).\nNow there is something obvious to see: after about 1960, there is a clear downward trend: the ice is breaking up earlier on average every year. Even though there is a lot of variability, the overall trend, viewed this way, is clear.\nWhat does this mean, in practice? This notion of the ice melting earlier than it used to is consistent all over the Arctic, and is one more indication of climate change. Precisely, it is an indication that climate change is happening, but we would have to delve further to make any statements about the cause of that climate change.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#computerized-accounting-1",
    "href": "data-summaries.html#computerized-accounting-1",
    "title": "3  Data exploration",
    "section": "3.11 Computerized accounting",
    "text": "3.11 Computerized accounting\nBeginning accounting students need to learn to learn to audit in a computerized environment. A sample of beginning accounting students took each of two tests: the Computer Attitude Scale (CAS) and the Computer Anxiety Rating Scale (CARS). A higher score in each indicates greater anxiety around computers. The test scores are scaled to be between 0 and 5. Also noted was each student’s gender. The data are in http://ritsokiguess.site/datafiles/compatt.txt. The data values are separated by spaces.\n\nRead the data into R. Do you have what you expected? Explain briefly.\n\nSolution\nRead in and display the data. This, I think, is the easiest way.\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/compatt.txt\"\nanxiety=read_delim(my_url,\" \")\n\nRows: 35 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): gender\ndbl (2): CAS, CARS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nanxiety\n\n\n\n  \n\n\n\nThere is a total of 35 students with a CAS score, a CARS score and a gender recorded for each. This is in line with what I was expecting. (You can also note that the genders appear to be a mixture of males and females.)\n\\(\\blacksquare\\)\n\nHow many males and females were there in the sample?\n\nSolution\nMost easily count:\n\nanxiety %&gt;% count(gender)\n\n\n\n  \n\n\n\nThis also works (and is therefore good):\n\nanxiety %&gt;% group_by(gender) %&gt;% summarize(count=n())\n\n\n\n  \n\n\n\nI want you to use R to do the counting (that is, don’t just go through the whole data set and count the males and females yourself). This is because you might have thousands of data values and you need to learn how to get R to count them for you.\n15 females and 20 males, which you should say. I made a point of not saying that it is enough to get the output with the answers on it, so you need to tell me what the answer is.\n\\(\\blacksquare\\)\n\nDo the CAS scores tend to be higher for females or for males? Draw a suitable graph to help you decide, and come to a conclusion.\n\nSolution\nGender is categorical and CAS score is quantitative, so a boxplot would appear to be the thing:\n\nggplot(anxiety,aes(x=gender,y=CAS))+geom_boxplot()\n\n\n\n\nThe median for males is slightly higher, so male accountants are more anxious around computers than female accountants are.\nIf you wish, side-by-side (or, better, above-and-below) histograms would also work:\n\nggplot(anxiety,aes(x=CAS))+geom_histogram(bins=6)+\nfacet_wrap(~gender,ncol=1)\n\n\n\n\nIf you go this way, you have to make a call about where the centres of the histograms are. I guess the male one is slightly further to the right, but it’s not so easy to tell. (Make a call.)\n\\(\\blacksquare\\)\n\nFind the median CAS scores for each gender. Does this support what you saw on your plot? Explain briefly.\n\nSolution\nGroup-by and summarize:\n\nanxiety %&gt;% group_by(gender) %&gt;% summarize(med=median(CAS))\n\n\n\n  \n\n\n\nThe median is a bit higher for males, which is what I got on my boxplot (and is apparently the same thing as is on the histograms, but it’s harder to be sure there).\n\\(\\blacksquare\\)\n\nFind the mean and standard deviation of both CAS and CARS scores (for all the students combined, ie. not separated by gender) without naming those columns explicitly.\n\nSolution\nWithout naming them explicitly means using some other way to pick them out of the data frame, summarize with across.\nThe basic across comes from asking yourself what the names of those columns have in common: they start with C and the gender column doesn’t:\n\nanxiety %&gt;% summarize(across(starts_with(\"C\"), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nAnother way is to ask what property these two columns have in common: they are the only two numeric (quantitative) columns. This means using an across with a where inside it, thus:\n\nanxiety %&gt;% summarize(across(where(is.numeric), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nRead the first one as “across all the columnns whose names start with S, find the mean and SD of them.” The second one is a little clunkier: “acrosss all the columns for which is.numeric is true, find the mean and SD of them”. A shorter way for the second one is “across all the numeric (quantitative) columns, find their mean and SD”, but then you have to remember exactly how to code that. The reason for the list is that we are calculating two statistics for each column that we find. I am using a “named list” so that the mean gets labelled with an m on the end of the column name, and the SD gets an s on the end.\nEither of these is good, or anything equivalent (like noting that the two anxiety scales both ends\\_with S):\n\nanxiety %&gt;% summarize(across(ends_with(\"S\"), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nBecause I didn’t say otherwise, you should tell me what the means and SDs are, rounding off suitably: the CAS scores have mean 2.82 and SD 0.48, and the CARS scores have mean 2.77 and SD 0.67.\nYet another way to do it is to select the columns you want first (which you can do by number so as not to name them), and then find the mean and SD of all of them:\n\nanxiety %&gt;% select(2:3) %&gt;% \n    summarize(across(everything(), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nThis doesn’t work:\n\nsummary(anxiety)\n\n    gender               CAS             CARS      \n Length:35          Min.   :1.800   Min.   :1.000  \n Class :character   1st Qu.:2.575   1st Qu.:2.445  \n Mode  :character   Median :2.800   Median :2.790  \n                    Mean   :2.816   Mean   :2.771  \n                    3rd Qu.:3.150   3rd Qu.:3.290  \n                    Max.   :3.750   Max.   :4.000  \n\n\nbecause, although it gets the means, it does not get the standard deviations. (I added the SD to the original question to make you find a way other than this.)\nIn summary, find a way to get those answers without naming those columns in your code, and I’m good.\nIn case you were wondering about how to do this separately by gender, well, put the group\\_by in like you did before:\n\nanxiety %&gt;% group_by(gender) %&gt;%\nsummarize(across(where(is.numeric), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nor\n\nanxiety %&gt;% group_by(gender) %&gt;%\nsummarize(across(starts_with(\"C\"), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nThe male means are slightly higher on both tests, but the male standard deviations are a little smaller. You might be wondering whether the test scores are related. They are both quantitative, so the obvious way to find out is a scatterplot:\n\nggplot(anxiety,aes(x=CAS,y=CARS))+geom_point()\n\n\n\n\nThe two variables can be on either axis, since there is no obvious response or explanatory variable. A higher score on one scale goes with a higher score on the other, suggesting that the two scales are measuring the same thing.\nThis plot mixes up the males and females, so you might like to distinguish them, which goes like this:\n\nggplot(anxiety,aes(x=CAS,y=CARS,colour=gender))+geom_point()\n\n\n\n\nThere is a slight (but only slight) tendency for the males to be up and to the right, and for the females to be down and to the left. This is about what you would expect, given that the male means are slightly bigger on both scores, but the difference in means is not that big compared to the SD.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#test-scores-in-two-classes-1",
    "href": "data-summaries.html#test-scores-in-two-classes-1",
    "title": "3  Data exploration",
    "section": "3.12 Test scores in two classes",
    "text": "3.12 Test scores in two classes\nOpen R Studio. Create a new Text File by selecting File, New File and Text File. You should see a new empty, untitled window appear at the top left. In that window, type or copy the data below (which are scores on a test for students in two different classes):\n\nclass score\nken 78\nken 62\nken 59\nken 69\nken 81\nthomas 83\nthomas 77\nthomas 63\nthomas 61\nthomas 79\nthomas 72\n\nSave the file, using a filename of your choice (with, perhaps, extension .txt). Or, if you prefer, use the one at link.\n\nRead the data into a data frame called marks, using read_delim, and list the data frame (by typing its name) to confirm that you read the data values properly. Note that the top line of the data file contains the names of the variables, as it ought to.\n\nSolution\nI was lazy and used the one on the web, the values being separated (“delimited”) by exactly one space:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/marks.txt\"\nmarks &lt;- read_delim(my_url, \" \")\n\nRows: 11 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): class\ndbl (1): score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmarks\n\n\n\n  \n\n\n\nIf you copied and pasted, or typed in, the data values yourself, use the local file name (such as marks.txt) in place of the URL.\nExtra: in the old days, when we used read.table (which actually also works here), we needed to also say header=T to note that the top line of the data file was variable names. With read_delim, that’s the default, and if the top line is not variable names, that’s when you have to say so. If I cheat, by skipping the first line and saying that I then have no column names, I get:\n\nread_delim(my_url, \" \", col_names = F, skip = 1)\n\nRows: 11 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): X1\ndbl (1): X2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n  \n\n\n\nColumn names are supplied (X1 and X2). I could also supply my own column names, in which case the file needs not to have any, so I need the skip again:\n\nread_delim(my_url, \" \", col_names = c(\"instructor\", \"mark\"), skip = 1)\n\nRows: 11 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): instructor\ndbl (1): mark\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\n* Obtain side-by-side boxplots of the scores for each class.\n\nSolution\n\nlibrary(tidyverse)\nggplot(marks, aes(x = class, y = score)) + geom_boxplot()\n\n\n\n\nRemember: on a regular boxplot,7 the groups go across (\\(x\\)), the variable measured goes up (\\(y\\)).\nExtra: this might work:\n\nggplot(marks, aes(x = class, y = score)) + geom_boxplot() +\n  coord_flip()\n\n\n\n\nIt does. That was a guess. So if you want sideways boxplots, this is how you can get them. Long group names sometimes fit better on the \\(y\\)-axis, in which case flipping the axes will help. (The x and y happen before the coordinate-flip, so they are the same as above, not the same way they come out.)\n\\(\\blacksquare\\)\n\nDo the two classes appear to have similar or different scores, on average? Explain briefly.\n\nSolution\nThe median for Thomas’s class appears to be quite a bit higher than for Ken’s class (the difference is actually about 6 marks). It’s up to you whether you think this is a big difference or not: I want you to have an opinion, but I don’t mind so much what that opinion is. Having said that the medians are quite a bit different, note that the boxes overlap substantially, so that the distributions of scores are pretty similar (or, the quartiles of scores are similar, or, the IQR of scores is similar for the two groups). If you say that, it’s good, but I’m not insisting that you do.\n\\(\\blacksquare\\)\n\nObtain a boxplot of all the scores together, regardless of which class they came from.\n\nSolution\nReplace your \\(x\\)-coordinate by some kind of dummy thing like 1 (factor(1) also works):\n\nggplot(marks, aes(x = 1, y = score)) + geom_boxplot()\n\n\n\n\nThe \\(x\\)-axis is kind of dopey, so you just ignore it. It is possible to remove it, but that is more work than it’s worth, and I didn’t get rid of the ticks below:\n\nggplot(marks, aes(x = 1, y = score)) + geom_boxplot() +\n  theme(\n    axis.text.x = element_blank(),\n    axis.title.x = element_blank()\n  )\n\n\n\n\n\\(\\blacksquare\\)\n\nCompute the median score (of all the scores together). Does this seem about right, looking at the boxplot? Explain briefly.\n\nSolution\nThree ways to get the median score. I like the first one best:\n\nmarks %&gt;% summarize(med = median(score))\n\n\n\n  \n\n\nwith(marks, median(score))\n\n[1] 72\n\nmedian(marks$score)\n\n[1] 72\n\n\nsummarize is the tidyverse “verb” that does what you want here. (The same idea gets the mean score for each class, below.)\nThe other ways use the basic function median. To make that work, you need to say that the variable score whose median you want lives in the data frame marks. These are two ways to do that.\nExtra: if you wanted median by group, this is the approved (tidyverse) way:\n\nmarks %&gt;%\n  group_by(class) %&gt;%\n  summarize(med = median(score))\n\n\n\n  \n\n\n\nTo get something by group, the extra step is group_by, and then whatever you do after that is done for each group.\nYou can now go back and compare these medians with the ones on the boxplots in (here). They should be the same. Or you can even do this:\n\nmarks %&gt;%\n  group_by(class) %&gt;%\n  summarize(\n    q1 = quantile(score, 0.25),\n    med = median(score),\n    q3 = quantile(score, 0.75)\n  )\n\n\n\n  \n\n\n\nYou can calculate as many summaries as you like. These ones should match up with the top and bottom of the boxes on the boxplots. The only restriction is that the things on the right side of the equals should return a single number. If you have a function like quantile without anything extra that returns more than one number:\n\nquantile(marks$score)\n\n  0%  25%  50%  75% 100% \n59.0 62.5 72.0 78.5 83.0 \n\n\nyou’re in trouble. Only read on if you really want to know how to handle this. Here’s step 1:\n\nmarks %&gt;%\n  nest_by(class)\n\n\n\n  \n\n\n\nThis is kind of a funky group_by. The things in the data column are the whole rest of the data frame: there were 5 students in Ken’s class and 6 in Thomas’s, and they each had a score, so 5 or 6 rows and 1 column. The column data is known in the trade as a “list-column”.\nNow, for each of those mini-data-frames, we want to calculate the quantiles of score. This is rowwise: for each of our mini-data-frames data, calculate the five-number summary of the column called score in it:\n\nmarks %&gt;%\n  nest_by(class) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(quantile(data$score)))\n\n\n\n  \n\n\n\nI have to be a little bit careful about which data frame I want the score to come from: the ones hidden in data, which are the things we’re for-eaching over.\nThis obtains a new list-column called qq, with the five-number summary for each group.8\nNow we want to display the quantiles. This is the easiest way:\n\nmarks %&gt;%\n  nest_by(class) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(quantile(data$score))) %&gt;% \n  unnest(qq)\n\n\n\n  \n\n\n\nThe unnest turns the list-column back into actual data, so we get the five quantiles for each class.\nThe only thing this doesn’t do is to show us which quantile is which (we know, of course, that the first one is the minimum, the last one is the max and the quartiles and median are in between). It would be nice to see which is which, though. A trick to do that is to use enframe, thus:\n\nquantile(marks$score) %&gt;% enframe()\n\n\n\n  \n\n\n\nor thus:\n\nenframe(quantile(marks$score))\n\n\n\n  \n\n\n\nI don’t normally like the second way with all the brackets, but we’ll be using it later.\nThe idea here is that the output from a quantile is a vector, but one with “names”, namely the percentiles themselves. enframe makes a two-column data frame with the names and the values. (You can change the names of the columns it creates, but here we’ll keep track of which is which.)\nSo we have a two-column data frame with a column saying which quantile is which. So let’s rewrite our code to use this:\n\nmarks %&gt;%\n  nest_by(class) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(enframe(quantile(data$score)))) \n\n\n\n  \n\n\n\nNote that the qq data frames in the list-column now themselves have two columns.\nAnd finally unnest qq:\n\nmarks %&gt;%\n  nest_by(class) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(enframe(quantile(data$score)))) %&gt;% \n  unnest(qq)\n\n\n\n  \n\n\n\nSuccess! Or even:\n\nmarks %&gt;%\n  nest_by(class) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(enframe(quantile(data$score)))) %&gt;% \n  unnest(qq) %&gt;% \n  mutate(qn = parse_number(name)) %&gt;%\n  select(-name) %&gt;%\n  pivot_wider(names_from = qn, values_from = value)\n\n\n\n  \n\n\n\nThis deliberately untidies the final answer to make it nicer to look at. (The lines before that create a numeric quantile, so that it sorts into the right order, and then get rid of the original quantile percents. Investigate what happens if you do a similar pivot_wider without doing that.)"
  },
  {
    "objectID": "data-summaries.html#unprecedented-rainfall-1",
    "href": "data-summaries.html#unprecedented-rainfall-1",
    "title": "3  Data exploration",
    "section": "3.13 Unprecedented rainfall",
    "text": "3.13 Unprecedented rainfall\nIn 1997, a company in Davis, California, had problems with odour in its wastewater facility. According to a company official, the problems were caused by “unprecedented weather conditions” and “because rainfall was at 170 to 180 percent of its normal level, the water in the holding ponds took longer to exit for irrigation, giving it more time to develop an odour.”\nAnnual rainfall data for the Davis area is here. Note that clicking on the link will display the file, and right-clicking on the link will give you some options, one of which is Copy Link Address, which you can then paste into your R Notebook.\nThe rainfall is measured in inches.\n\nRead in and display (some of) the data.\n\nSolution\nLook at the data file, and see that the values are separated by a single space, so will do it. Read straight from the URL; the hint above tells you how to copy it, which would even work if the link spans two lines.\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/rainfall.txt\"\nrain &lt;- read_delim(my_url, \" \")\n\nRows: 47 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): Year, Rainfall\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nrain\n\n\n\n  \n\n\n\nNote for later that the and the have Capital Letters. You can call the data frame whatever you like, but I think something descriptive is better than eg. .\nExtra: this works because there is exactly one space between the year and the rainfall amount. But the year is always four digits, so the columns line up, and there is a space all the way down between the year and the rainfall. That means that this will also work:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/rainfall.txt\"\nrain &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Year = col_double(),\n  Rainfall = col_double()\n)\n\nrain\n\n\n\n  \n\n\n\nThis is therefore also good.\nIt also looks as if it could be tab-separated values, since the rainfall column always starts in the same place, but if you try it, you’ll find that it doesn’t work:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/rainfall.txt\"\nrain_nogood &lt;- read_tsv(my_url)\n\nRows: 47 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Year Rainfall\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nrain_nogood\n\n\n\n  \n\n\n\nThis looks as if it worked, but it didn’t, because there is only one column, of years and rainfalls smooshed together as text, and if you try to do anything else with them later it won’t work.\nHence those values that might have been tabs actually were not. There’s no way to be sure about this; you have to try something and see what works. An indication, though: if you have more than one space, and the things in the later columns are left-justified, that could be tab-separated; if the things in the later columns are right-justified, so that they finish in the same place but don’t start in the same place, that is probably aligned columns.\n\\(\\blacksquare\\)\n\nSummarize the data frame.\n\nSolution\nI almost gave the game away: this is summary.\n\nsummary(rain)\n\n      Year         Rainfall    \n Min.   :1951   Min.   : 6.14  \n 1st Qu.:1962   1st Qu.:12.30  \n Median :1974   Median :16.72  \n Mean   :1974   Mean   :18.69  \n 3rd Qu.:1986   3rd Qu.:25.21  \n Max.   :1997   Max.   :37.42  \n\n\nThe summary of the years may not be very helpful, but the summary of the annual rainfall values might be. It’s not clear yet why I asked you to do this, but it will become clearer later.\n\\(\\blacksquare\\)\n\nMake a suitable plot of the rainfall values. (We are not, for the time being, concerned about the years.)\n\nSolution\nThis is one quantitative variable, so a histogram is your first thought. This means picking a number of bins. Not too many, since you want a picture of the shape:\n\nggplot(rain, aes(x=Rainfall)) + geom_histogram(bins=8)\n\n\n\n\nIf you picked fewer bins, you’ll get a different picture:\n\nggplot(rain, aes(x=Rainfall)) + geom_histogram(bins=6)\n\n\n\n\nThe choice of the number of bins depends on what you think the story about shape is that you want to tell (see next part). You will probably need to try some different numbers of bins to see which one you like best. You can say something about what you tried, for example “I also tried 8 bins, but I like the histogram with 6 bins better.”\n\\(\\blacksquare\\)\n\nHow would you describe the shape of the distribution of rainfall values?\n\nSolution\nThis will depend on the histogram you drew in the previous part. If it looks like the first one, the best answer is “bimodal”: that is, it has two peaks with a gap between them. If it looks like the second one, you have an easier time; this is ordinary right-skewness.\n\\(\\blacksquare\\)\n\nIn the quote at the beginning of the question, where do you think the assertion that the 1997 rainfall was “at 170 to 180 percent of its normal level” came from? Explain briefly.\n\nSolution\nFirst we need the 1997 rainfall. Go back and find it in the data. I am borrowing an idea from later in the course (because I am lazy):\n\nrain %&gt;% filter(Year==1997)\n\n\n\n  \n\n\n\n29.7 inches.\nNow, what would be a “normal level” of rainfall? Some kind of average, like a mean or a median, maybe. But we have those, from our summary that we made earlier, repeated here for (my) convenience:\n\nsummary(rain)\n\n      Year         Rainfall    \n Min.   :1951   Min.   : 6.14  \n 1st Qu.:1962   1st Qu.:12.30  \n Median :1974   Median :16.72  \n Mean   :1974   Mean   :18.69  \n 3rd Qu.:1986   3rd Qu.:25.21  \n Max.   :1997   Max.   :37.42  \n\n\nThe mean is 18.69 and the median is 16.72 inches.\nSo divide the 1997 rainfall by each of the summaries, and see what happens, using your calculator, or using R as a calculator:\n\n29.7/18.69\n\n[1] 1.589085\n\n29.7/16.72\n\n[1] 1.776316\n\n\nThe 1997 rainfall was about 178 percent of the normal level if the normal level was the median.\n\\(\\blacksquare\\)\n\nDo you think the official’s calculation was reasonable? Explain briefly. (Note that this is not the same as asking whether the official’s calculation was correct. This is an important distinction for you to make.)\n\nSolution\nThere are several approaches to take. Argue for yours.\nIf you came to the conclusion that the distribution was right-skewed, you can say that the sensible “normal level” is the median, and therefore the official did the right thing. Using the mean would have been the wrong thing.\nIf you thought the distribution was bimodal, you can go a couple of ways: (i) it makes no sense to use any measure of location for “normal” (in fact, the mean rainfall is almost in that low-frequency bar, and so is not really a “normal level” at all). Or, (ii) it looks as if the years split into two kinds: low-rainfall years with around 15 inches, and high-rainfall years with more than 25 inches. Evidently 1997 was a high-rainfall year, but 29.7 inches was not especially high for a high-rainfall year, so the official’s statement was an exaggeration. (I think (ii) is more insightful than (i), so ought to get more points.)\nYou could even also take a more conspiratorial approach and say that the official was trying to make 1997 look like a freak year, and picked the measure of location that made 1997 look more unusual.\n“Normal level” here has nothing to do with a normal distribution; for this to make sense, the official would have needed to say something like “normal shape”. This is why language skills are also important for a statistician to have.\n\\(\\blacksquare\\)\n\nDo you think that the official was right to use the word “unprecedented” to describe the 1997 rainfall? Justify your answer briefly.\n\nSolution\n“Unprecedented” means “never seen before” or “never having happened or existed in the past”.9 That came out of my head; this link has a very similar “never before known or experienced”).\nIf you look back at your histogram, there are several years that had over about 30 inches of rain: five or six, depending on your histogram. One of them was 1997, but there were others too, so 1997 was in no way “unprecedented”.\nAnother approach that you have seen is to View your dataframe:\n\nView(rain)\n\nThat will come out as a separate tab in your R Studio and you can look at it (yourself; it won’t appear in the Preview). You can look at the 1997 rainfall (29.69 inches) and count how many were bigger than that, 4 of them. Or, save yourself some effort10 and sort the rainfall values in descending order (with the biggest one first), by clicking on the little arrows next to Rainfall (twice). Mine looks like this:\n\nLater, we learn how to sort in code, which goes like this (to sort highest first):\n\nrain %&gt;% arrange(desc(Rainfall))\n\n\n\n  \n\n\n\nA more sophisticated way that we learn later:\n\nrain %&gt;% summarize(max=max(Rainfall))\n\n\n\n  \n\n\n\nThis is greater than the rainfall for 1997, ruling out “unprecedented”.\n1997 was only the fifth highest rainfall, and two of the higher ones were also in the 1990s. Definitely not “unprecedented”. The official needs to get a new dictionary!\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#learning-algebra-1",
    "href": "data-summaries.html#learning-algebra-1",
    "title": "3  Data exploration",
    "section": "3.14 Learning algebra",
    "text": "3.14 Learning algebra\nAt a high school in New Jersey, teachers were interested in what might help students to learn algebra. One idea was laptops as a learning aid, to see whether having access to one helped with algebra scores. (This was some time ago.) The 20 students in one class were given laptops to use in school and at home, while the 27 students in another class were not given laptops. For all of these students, the final exam score in algebra was recorded. The data are in http://ritsokiguess.site/datafiles/algebra.txt, with two columns, one indicating whether the student received a laptop or not, and the other giving their score on the algebra final exam.\n\nRead in and display (some of) the data. Do you have (i) the correct number of observations, and (ii) the correct type of columns? Explain briefly.\n\nSolution\nTake a look at the data file first: the data values are aligned in columns with variable numbers of spaces between, so read_table is the thing. Read directly from the URL, rather than trying to copy the data from the website:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/algebra.txt\"\nalgebra &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  laptop = col_character(),\n  score = col_double()\n)\n\nalgebra\n\n\n\n  \n\n\n\nThere were \\(20+27=47\\) students altogether in the two classes, and we do indeed have 47 rows, one per student. So we have the right number of rows. This is two independent samples; each student was in only one of the two classes, either the class whose students got laptops or not. The values in the laptop column are text (see the chr at the top), and the values in the score column are numbers (dbl at the top). Alternatively, you can look at the R Console output in which you see that laptop is col_character() (text) and score is col_double() (numerical, strictly a decimal number).\nExtra 1: read.table also works but it is wrong in this course (because it is not what I taught you in class).\nExtra 2: with more than one space between the values, read_delim will not work. Or, perhaps more confusing, it will appear to work and then fail later, which means that you need to pay attention:\n\nd &lt;- read_delim(my_url, \" \")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 47 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): laptop, score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nd\n\n\n\n  \n\n\n\nThis looks all right, but look carefully: the laptop column is correctly text, but the score column, which should be numbers (dbl), is actually text as well. An easier way to see this is to look at the output from the console, which is the descriptions of the columns: they are both col_character or text, while score should be numbers. You might be able to see exactly what went wrong: with more than one space separating the values, the remaining spaces went into score, which then becomes a piece of text with some spaces at the front and then numbers.\nThis will actually work for a while, as you go through the question, but will come back to bite you the moment you need score to be numerical (eg. when you try to draw a boxplot), because it is actually not numerical at all.\nExtra 3: this is the standard R way to lay out this kind of data, with all the outcome values in one column and a second (categorical) column saying which group each observation was in. In other places you might see two separate columns of scores, one for the students with laptops and one for the students without, as below (you won’t understand the code below now, but you will by the end of the course):\n\nalgebra %&gt;% \nmutate(row = c(1:20, 1:27)) %&gt;% \npivot_wider(names_from = laptop, values_from = score)\n\n\n\n  \n\n\n\nA column of yes and a column of no. The classes were of different sizes, so the yes column, with only 20 observations, has some NA (“missing”) observations at the end (scroll down to see them) to enable the dataframe to keep a rectangular shape.\nWe will learn later what to call these layouts of data: “longer” and “wider” (respectively), and how to convert between them. R usually likes “longer” data, as in the data file, but you will often see data sets displayed wider because it takes up less space.\n\\(\\blacksquare\\)\n\nMake a suitable graph of these data.\n\nSolution\nThe teachers were hoping to see how the laptop-yes and the laptop-no groups compared in terms of algebra scores, so side-by-side boxplots would be helpful. More simply, we have one quantitative and one categorical variable, which is a boxplot according to the table in the notes:\n\nggplot(algebra, aes(x = laptop, y = score)) + geom_boxplot()\n\n\n\n\nExtra: as you will note below, the median score for the students with laptops is a little higher for the students who had laptops. This is easy to see on a boxplot because that is what a boxplot does. (That was what Tukey, who we will meet later, designed the boxplot to do.)\nAnother plot you might have drawn is a histogram for each group, side by side, or, as they come out here, above and below. This works using facets:\n\nggplot(algebra, aes(x = score)) + \ngeom_histogram(bins = 10) +\nfacet_wrap(~laptop, ncol = 1)\n\n\n\n\nLooking at those, can you really say that the median is slightly higher for the yes group? I really don’t think you can. Certainly it is clear from the histograms that the spread for the yes group is less, but comparing the medians is much more easily done from the boxplot. The teachers were interested in whether the laptops were associated with higher scores on average, so the kind of comparison that the boxplot affords is clearly preferred here.\nIf you are interested in the code: you imagine you’re going to make a histogram of scores regardless of group, and then at the end you facet by your grouping variable. I added the ncol = 1 to make the plots come out in one column (that is, one above the other). If you don’t do this, they come out left and right, which makes the distributions even harder to compare.\n\\(\\blacksquare\\)\n\nComment briefly on your graph, thinking about what the teachers would like to know.\n\nSolution\nThere are three things to say something about, the first two of which would probably interest the teachers:\n\ncomparison of centre: the median score for the group that had laptops is (slightly) higher than for the group that did not.\ncomparison of spread: the scores for the group that had laptops are less spread out (have smaller interquartile range) than for the group that did not.\nassessment of shape: both groups have low outliers, or are skewed to the left in shape.\n\nSome comments from me:\n\nboxplots say nothing about mean and standard deviation, so don’t mention those here. You should say something about the measures of centre (median) and spread (IQR) that they do use.\nI think of skewness as a property of a whole distribution, but outlierness as a property of individual observations. So, when you’re looking at this one, think about where the evidence about shape is coming from: is it coming from those one or two low values that are different from the rest (which would be outliers), or is it coming from the whole distribution (would you get the same story if those maybe-outliers are taken away)? My take is that if you take the outliers away, both distributions are close to symmetric, and therefore what you see here is outliers rather than skewness. If you see something different, make the case for it.\n\nOne reason to suspect skewness or something like it is that test scores have an upper limit (100) that some of the scores got close to, and no effective lower limit (the lower limit is 0 but no-one got very close to that). In this sort of situation, you’d expect the scores to be skewed away from the limit: that is, to the left. Or to have low outliers rather than high ones.\n\\(\\blacksquare\\)\n\nWork out the median and inter-quartile range for the students who did and who did not have laptops, and compare with the boxplot. (In R, the inter-quartile range is IQR in uppercase.)\n\nSolution\nThis is easy to make way harder than it needs to be: group_by and summarize will do it. Put the two summaries in one summarize:\n\nalgebra %&gt;% \ngroup_by(laptop) %&gt;% \nsummarize(med = median(score), iqr = IQR(score))\n\n\n\n  \n\n\n\nThen relate these to the information on the boxplot: the centre line of the box is the median. For the no group this is just above 80, so 81 makes sense; for the yes group this is not quite halfway between 80 and 90, so 84 makes sense.\nThe inter-quartile range is the height of the box for each group. Estimate the top and bottom of the two boxes from the boxplot scale, and subtract. For the no group this is something like \\(88-68\\) which is 20, and for the yes group it is something like \\(93-80\\) which is indeed 13.\nExtra: I didn’t ask you here about whether the difference was likely meaningful. The focus here was on getting the graph and summaries. If I had done so, you would then need to consider things like whether a three-point difference in medians could have been chance, and whether we really had random allocation of students to groups.\nTo take the second point first: these are students who chose to take two different classes, rather than being randomly allocated to classes as would be the case in a true experiment. What we have is really in between an experiment and an observational study; yes, there was a treatment (laptop or not) that was (we hope) randomly allocated to one class and not the other, but the classes could have been different for any number of other reasons that had nothing to do with having laptops or not, such as time of day, teacher, approach to material, previous ability at algebra, etc.\nSo even if we are willing to believe that the students were as-if randomized to laptop or not, the question remains as to whether that three-point difference in medians is reproducible or indicative of a real difference or not. This is the kind of thing we would try a two-sample \\(t\\)-test with. In this case, we might doubt whether it will come out significant (because of the small difference in medians and presumably means, compared to the amount of variability present), and, even then, there is the question of whether we should be doing a \\(t\\)-test at all, given the outliers.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#footnotes",
    "href": "data-summaries.html#footnotes",
    "title": "3  Data exploration",
    "section": "",
    "text": "these are mostly int, that is, integer.↩︎\nOther possible variable types are num for (real, decimal) numbers such as birth weight, chr for text, and Factor (with the number of levels) for factors/categorical variables. We don’t have any of the last two here. There is also lgl for logical, things that were actually recorded as TRUE or FALSE. We have some variables that are actually logical ones, but they are recorded as integer values.↩︎\nWhen Tukey, a name we will see again, invented the boxplot in the 1950s, 500 observations would have been considered a big data set. He designed the boxplot to produce a sensible number of outliers for the typical size of data set of his day, but a boxplot of a large data set tends to have a lot of outliers that are probably not really outliers at all.↩︎\nI explain the missing values below.↩︎\nIf there had been a weeks of gestation, we could have figured out whether it was premature or not, according to whether the weeks of gestation was less than 37.↩︎\nThat is to say, the principal deviation from normality is not the hole on the histogram, the bar centred around 123 being too short, but that the bar centred just below 120 is too tall.↩︎\nBoxplots can also go across the page, but for us, they don’t.↩︎\nIt’s actually a coincidence that the five-number summary and Ken’s class both have five values in them.↩︎\nSearching for “define” followed by a word is a good way to find out exactly what that word means, if you are not sure, but you should at least say where you got the definition from if you had to look it up.↩︎\nWhen you have a computer at your disposal, it’s worth taking a few minutes to figure out how to use it to make your life easier.↩︎"
  },
  {
    "objectID": "one-sample-inference.html#hunter-gatherers-in-australia",
    "href": "one-sample-inference.html#hunter-gatherers-in-australia",
    "title": "4  One-sample inference",
    "section": "4.1 Hunter-gatherers in Australia",
    "text": "4.1 Hunter-gatherers in Australia\nA hunter-gatherer society is one where people get their food by hunting, fishing or foraging rather than by agriculture or by raising animals. Such societies tend to move from place to place. Anthropologists have studied hunter-gatherer societies in forest ecosystems across the world. The average population density of these societies is 7.38 people per 100 km\\(^2\\). Hunter-gatherer societies on different continents might have different population densities, possibly because of large-scale ecological constraints (such as resource availability), or because of other factors, possibly social and/or historic, determining population density.\nSome hunter-gatherer societies in Australia were studied, and the population density per 100 km\\(^2\\) recorded for each. The data are in http://ritsokiguess.site/datafiles/hg.txt.\n\nRead the data into R. Do you have the correct variables? How many hunter-gatherer societies in Australia were studied? Explain briefly.\nThe question of interest is whether these Australian hunter-gatherer societies are like the rest of the world in terms of mean population density. State suitable null and alternative hypotheses. Define any symbols you use: that is, if you use a symbol, you also have to say what it means.\nTest your hypotheses using a suitable test. What do you conclude, in the context of the data?\nDo you have any doubts about the validity of your test? Explain briefly, using a suitable graph to support your explanation."
  },
  {
    "objectID": "one-sample-inference.html#buses-to-boulder",
    "href": "one-sample-inference.html#buses-to-boulder",
    "title": "4  One-sample inference",
    "section": "4.2 Buses to Boulder",
    "text": "4.2 Buses to Boulder\nA bus line operates a route from Denver to Boulder (these places are in Colorado). The schedule says that the journey time should be 60 minutes. 11 randomly chosen journey times were recorded, and these are in the file link, with journey times shown in minutes.\n\nRead the data into R, and display the data frame that you read in.\nRun a suitable test to see whether there is evidence that the mean journey time differs from 60 minutes. What do you conclude? (I want a conclusion that says something about journey times of buses.)\nGive a 95% confidence interval for the mean journey time. (No R code is needed here.)\nDo you draw consistent conclusions from your test and confidence interval? Explain briefly.\nDraw a boxplot of the journey times. Do you see a reason to doubt the test that you did above?"
  },
  {
    "objectID": "one-sample-inference.html#length-of-gestation-in-north-carolina",
    "href": "one-sample-inference.html#length-of-gestation-in-north-carolina",
    "title": "4  One-sample inference",
    "section": "4.3 Length of gestation in North Carolina",
    "text": "4.3 Length of gestation in North Carolina\nThe data in file link are about 500 randomly chosen births of babies in North Carolina. There is a lot of information: not just the weight at birth of the baby, but whether the baby was born prematurely, the ages of the parents, whether the parents are married, how long (in weeks) the pregnancy lasted (this is called the “gestation”) and so on. We have seen these data before.\n\nRead in the data from the file into R, bearing in mind what type of file it is.\nFind a 95% confidence interval for the mean birth weight of all babies born in North Carolina (of which these babies are a sample). At the end, you should state what the confidence interval is. Giving some output is necessary, but not enough by itself.\nBirth weights of babies born in the United States have a mean of 7.3 pounds. Is there any evidence that babies born in North Carolina are less heavy on average? State appropriate hypotheses, do your test, obtain a P-value and state your conclusion, in terms of the original data.\nThe theory behind the \\(t\\)-test says that the distribution of birth weights should be (approximately) normally distributed. Obtain a histogram of the birth weights. Does it look approximately normal? Comment briefly. (You’ll have to pick a number of bins for your histogram first. I don’t mind very much what you pick, as long as it’s not obviously too many or too few bins.)"
  },
  {
    "objectID": "one-sample-inference.html#inferring-ice-break-up-in-nenana",
    "href": "one-sample-inference.html#inferring-ice-break-up-in-nenana",
    "title": "4  One-sample inference",
    "section": "4.4 Inferring ice break-up in Nenana",
    "text": "4.4 Inferring ice break-up in Nenana\nNenana, Alaska, is about 50 miles west of Fairbanks. Every spring, there is a contest in Nenana. A wooden tripod is placed on the frozen river, and people try to guess the exact minute when the ice melts enough for the tripod to fall through the ice. The contest started in 1917 as an amusement for railway workers, and has taken place every year since. Now, hundreds of thousands of people enter their guesses on the Internet and the prize for the winner can be as much as $300,000.\nBecause so much money is at stake, and because the exact same tripod is placed at the exact same spot on the ice every year, the data are consistent and accurate. The data are in link.\nYes, we saw these data before.\n\nRead the data into R, as before, or use the data frame that you read in before. Note that the values are separated by tabs rather than spaces, so you’ll need an appropriate read_ to read it in.\nObtain a 90% confidence interval for the mean JulianDate. What interval do you get? Looking back at your histogram, do you have any doubts about the validity of what you have just done?\nAn old-timer in Nenana strokes his grey beard and says “When I were young, I remember the tripod used to fall into the water around May 10”. In a non-leap year, May 10 is Julian day 130. Test the null hypothesis that the mean JulianDay is 130, against the alternative that it is less. What do you conclude? What practical implication does that have (assuming that the old-timer has a good memory)?\nPlot JulianDate against Year on a scatterplot. What recent trends, if any, do you see? Comment briefly. (You did this before, but I have some extra comments on the graph this time, so feel free to just read this part.)"
  },
  {
    "objectID": "one-sample-inference.html#diameters-of-trees",
    "href": "one-sample-inference.html#diameters-of-trees",
    "title": "4  One-sample inference",
    "section": "4.5 Diameters of trees",
    "text": "4.5 Diameters of trees\nThe Wade Tract in Thomas County, Georgia, is an old-growth forest of longleaf pine trees. It has survived in a relatively undisturbed state since before settlements of the area by Europeans. For each tree in the tract, researchers measured the diameter at breast height. This is a standard measure in forestry: it is defined as the diameter of the tree at 4.5 feet above the ground.1 They are interested in the mean diameter at breast height of the trees in this tract. These values are in http://ritsokiguess.site/datafiles/treediameter.csv. The diameters are measured in centimetres. The easiest way to get the URL is to right-click on the blue text and select Copy URL. (If you copy and paste the actual text you might end up with extra spaces, especially if the printed URL goes over two lines.)\n\nRead in and display (some of) the data.\nMake a suitable plot of your dataframe.\nObtain a 95% confidence interval for the mean diameter.\nBased on what you have seen so far, would you expect to reject a null hypothesis that the population mean diameter (of all longleaf pines like these) is 35 cm? Explain briefly. Then, carry out the test (against a two-sided alternative) and explain briefly whether you were right.\nWould you expect 35 cm to be in a 99% confidence interval for the mean diameter? Explain briefly, and then see if you were right."
  },
  {
    "objectID": "one-sample-inference.html#one-sample-cholesterol",
    "href": "one-sample-inference.html#one-sample-cholesterol",
    "title": "4  One-sample inference",
    "section": "4.6 One-sample cholesterol",
    "text": "4.6 One-sample cholesterol\nThe data set here contains cholesterol measurements for heart attack patients (at several different times) as well as for a group of control patients. We will focus on the control patients in this question.\n\nRead in and display (some of) the data.\nMake a suitable plot of the cholesterol levels of the control patients, and comment briefly on the shape of the distribution.\nIt is recommended that people in good health, such as the Control patients here, keep their cholesterol level below 200. Is there evidence that the mean cholesterol level of the population of people of which the Control patients are a sample is less than 200? Show that you understand the process, and state your conclusion in the context of the data.\nWhat values could the population mean cholesterol level take? You might need to get some more output to determine this.\nExplain briefly why you would be reasonably happy to trust the \\(t\\) procedures in this question. (There are two points you need to make.)\n\nMy solutions follow:"
  },
  {
    "objectID": "one-sample-inference.html#hunter-gatherers-in-australia-1",
    "href": "one-sample-inference.html#hunter-gatherers-in-australia-1",
    "title": "4  One-sample inference",
    "section": "4.7 Hunter-gatherers in Australia",
    "text": "4.7 Hunter-gatherers in Australia\nA hunter-gatherer society is one where people get their food by hunting, fishing or foraging rather than by agriculture or by raising animals. Such societies tend to move from place to place. Anthropologists have studied hunter-gatherer societies in forest ecosystems across the world. The average population density of these societies is 7.38 people per 100 km\\(^2\\). Hunter-gatherer societies on different continents might have different population densities, possibly because of large-scale ecological constraints (such as resource availability), or because of other factors, possibly social and/or historic, determining population density.\nSome hunter-gatherer societies in Australia were studied, and the population density per 100 km\\(^2\\) recorded for each. The data are in http://ritsokiguess.site/datafiles/hg.txt.\n\nRead the data into R. Do you have the correct variables? How many hunter-gatherer societies in Australia were studied? Explain briefly.\n\nSolution\nThe data values are separated by (single) spaces, so read_delim is the thing:\n\nurl=\"http://ritsokiguess.site/datafiles/hg.txt\"\nsocieties=read_delim(url,\" \")\n\nRows: 13 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): name\ndbl (1): density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nI like to put the URL in a variable first, because if I don’t, the read_delim line can be rather long. But if you want to do it in one step, that’s fine, as long as it’s clear that you are doing the right thing.\nLet’s look at the data frame:\n\nsocieties\n\n\n\n  \n\n\n\nI have the name of each society and its population density, as promised (so that is correct). There were 13 societies that were studied. For me, they were all displayed. For you, you’ll probably see only the first ten, and you’ll have to click Next to see the last three.\n\\(\\blacksquare\\)\n\nThe question of interest is whether these Australian hunter-gatherer societies are like the rest of the world in terms of mean population density. State suitable null and alternative hypotheses. Define any symbols you use: that is, if you use a symbol, you also have to say what it means.\n\nSolution\nThe mean for the world as a whole (“average”, as stated earlier) is 7.38. Let \\(\\mu\\) denote the population mean for Australia (of which these societies are a sample). Then our hypotheses are: \\[ H_0: \\mu=7.38\\] and \\[ H_a: \\mu \\ne 7.38.\\] There is no reason for a one-sided alternative here, since all we are interested in is whether Australia is different from the rest of the world. Expect to lose a point if you use the symbol \\(\\mu\\) without saying what it means.\n\\(\\blacksquare\\)\n\nTest your hypotheses using a suitable test. What do you conclude, in the context of the data?\n\nSolution\nA \\(t\\)-test, since we are testing a mean:\n\nt.test(societies$density,mu=7.38)\n\n\n    One Sample t-test\n\ndata:  societies$density\nt = 3.8627, df = 12, p-value = 0.002257\nalternative hypothesis: true mean is not equal to 7.38\n95 percent confidence interval:\n 15.59244 36.84449\nsample estimates:\nmean of x \n 26.21846 \n\n\nThe P-value is 0.0023, less than the usual \\(\\alpha\\) of 0.05, so we reject the null hypothesis and conclude that the mean population density is not equal to 7.38. That is to say, Australia is different from the rest of the world in this sense.\nAs you know, “reject the null hypothesis” is only part of the answer, so gets only part of the marks.\n\\(\\blacksquare\\)\n\nDo you have any doubts about the validity of your test? Explain briefly, using a suitable graph to support your explanation.\n\nSolution\nThe assumption behind the \\(t\\)-test is that the data are approximately normal. We can assess that in several ways, but the simplest (which is perfectly acceptable at this point) is a histogram. You’ll need to pick a suitable number of bins. This one comes from Sturges’ rule:\n\nggplot(societies,aes(x=density))+geom_histogram(bins=5)\n\n\n\n\nYour conclusion might depend on how many bins you chose for your histogram. Here’s 8 bins (which is really too many with only 13 observations, but it actually shows the shape well):\n\nggplot(societies,aes(x=density))+geom_histogram(bins=8)\n\n\n\n\nor you can get a number of bins from one of the built-in functions, such as:\n\nmybins=nclass.FD(societies$density)\nmybins\n\n[1] 3\n\n\nThis one is small. The interquartile range is large and \\(n\\) is small, so the binwidth will be large and therefore the number of bins will be small.\nOther choices: a one-group boxplot:\n\nggplot(societies,aes(x=1,y=density))+geom_boxplot()\n\n\n\n\nThis isn’t the best for assessing normality as such, but it will tell you about lack of symmetry and outliers, which are the most important threats to the \\(t\\)-test, so it’s fine here. Or, a normal quantile plot:\n\nggplot(societies,aes(sample=density))+\nstat_qq()+stat_qq_line()\n\n\n\n\nThis is actually the best way to assess normality, but I’m not expecting you to use this plot here, because we may not have gotten to it in class yet. (If you have read ahead and successfully use the plot, it’s fine.)\nAfter you have drawn your chosen plot (you need one plot), you need to say something about normality and thus whether you have any doubts about the validity of your \\(t\\)-test. This will depend on the graph you drew: if you think your graph is symmetric and outlier-free, you should have no doubts about your \\(t\\)-test; if you think it has something wrong with it, you should say what it is and express your doubts. My guess is that you will think this distribution is skewed to the right. Most of my plots are saying that.2\nOn the website where I got these data, they were using the data as an example for another test, precisely because they thought the distribution was right-skewed. Later on, we’ll learn about the sign test for the median, which I think is actually a better test here.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#buses-to-boulder-1",
    "href": "one-sample-inference.html#buses-to-boulder-1",
    "title": "4  One-sample inference",
    "section": "4.8 Buses to Boulder",
    "text": "4.8 Buses to Boulder\nA bus line operates a route from Denver to Boulder (these places are in Colorado). The schedule says that the journey time should be 60 minutes. 11 randomly chosen journey times were recorded, and these are in the file link, with journey times shown in minutes.\n\nRead the data into R, and display the data frame that you read in.\n\nSolution\nSince you can read the data directly from the URL, do that (if you are online) rather than having to copy and paste and save, and then find the file you saved. Also, there is only one column, so you can pretend that there were multiple columns, separated by whatever you like. It’s least typing to pretend that they were separated by commas like a .csv file:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/buses.txt\"\njourney.times &lt;- read_csv(my_url)\n\nRows: 11 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): minutes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\njourney.times\n\n\n\n  \n\n\n\nUsing read_delim with any delimiter (such as \" \") will also work, and is thus also good.\nVariable names in R can have a dot (or an underscore, but not a space) in them. I have grown accustomed to using dots to separate words. This works in R but not other languages, but is seen by some as old-fashioned, with underscores being the modern way.3 You can also use what is called “camel case” by starting each “word” after the first with an uppercase letter like this:\n\njourneyTimes &lt;- read_csv(my_url)\n\nYou have to get the capitalization and punctuation right when you use your variables, no matter what they’re called. In any of the cases above, there is no variable called journeytimes. As Jenny Bryan (in link) puts it, boldface in original: Implicit contract with the computer / scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Get better at typing.\n\\(\\blacksquare\\)\n\nRun a suitable test to see whether there is evidence that the mean journey time differs from 60 minutes. What do you conclude? (I want a conclusion that says something about journey times of buses.)\n\nSolution\nt.test doesn’t take a data= to say which data frame to use. Wrap it in a with:\n\nwith(journey.times, t.test(minutes, mu = 60))\n\n\n    One Sample t-test\n\ndata:  minutes\nt = 1.382, df = 10, p-value = 0.1971\nalternative hypothesis: true mean is not equal to 60\n95 percent confidence interval:\n 57.71775 69.73680\nsample estimates:\nmean of x \n 63.72727 \n\n\nWe are testing that the mean journey time is 60 minutes, against the two-sided alternative (default) that the mean is not equal to 60 minutes. The P-value, 0.1971, is a lot bigger than the usual \\(\\alpha\\) of 0.05, so we cannot reject the null hypothesis. That is, there is no evidence that the mean journey time differs from 60 minutes.\nAs you remember, we have not proved that the mean journey time is 60 minutes, which is what “accepting the null hypothesis” would be. We have only failed to reject it, in a shoulder-shrugging kind of way: “the mean journey time could be 60 minutes”. The other acceptable word is “retain”; when you say “we retain the null hypothesis”, you imply something like “we act as if the mean is 60 minutes, at least until we find something better.”\n\\(\\blacksquare\\)\n\nGive a 95% confidence interval for the mean journey time. (No R code is needed here.)\n\nSolution\nJust read it off from the output: 57.72 to 69.74 minutes.\n\\(\\blacksquare\\)\n\nDo you draw consistent conclusions from your test and confidence interval? Explain briefly.\n\nSolution\nThe test said that we should not reject a mean of 60 minutes. The confidence interval says that 60 minutes is inside the interval of plausible values for the population mean, which is another way of saying the same thing. (If we had rejected 60 as a mean, 60 would have been outside the confidence interval.)\n\\(\\blacksquare\\)\n\nDraw a boxplot of the journey times. Do you see a reason to doubt the test that you did above?\n\nSolution\nThe grouping variable is a “nothing” as in the Ken and Thomas question (part (d)):\n\nggplot(journey.times, aes(x = 1, y = minutes)) + geom_boxplot()\n\n\n\n\nThe assumption behind the \\(t\\)-test is that the population from which the data come has a normal distribution: ie. symmetric with no outliers. A small sample (here we have 11 values) even from a normal distribution might look quite non-normal (as in Assignment 0 from last week), so I am not hugely concerned by this boxplot. However, it’s perfectly all right to say that this distribution is skewed, and therefore we should doubt the \\(t\\)-test, because the upper whisker is longer than the lower one. In fact, the topmost value is very nearly an outlier:4\n\nggplot(journey.times, aes(x = minutes)) + geom_histogram(bins = 5)\n\n\n\n\nand there might be skewness as well, so maybe I should have been concerned.\nI would be looking for some intelligent comment on the boxplot: what it looks like vs. what it ought to look like. I don’t so much mind what that comment is, as long as it’s intelligent enough.\nPerhaps I should draw a normal quantile plot:\n\nggplot(journey.times, aes(sample = minutes)) + stat_qq() + stat_qq_line()\n\n\n\n\nThe normal quantile plot is saying that the problem is actually at the bottom of the distribution: the lowest value is not low enough, but the highest value is actually not too high. So this one seems to be on the edge between OK and being right-skewed (too bunched up at the bottom). My take is that with this small sample this is not too bad. But you are free to disagree.\nIf you don’t like the normality, you’d use a sign test and test that the median is not 60 minutes, which you would (at my guess) utterly fail to reject:\n\nlibrary(smmr)\nsign_test(journey.times, minutes, 60)\n\n$above_below\nbelow above \n    4     7 \n\n$p_values\n  alternative   p_value\n1       lower 0.8867187\n2       upper 0.2744141\n3   two-sided 0.5488281\n\nci_median(journey.times, minutes)\n\n[1] 54.00195 71.99023\n\n\nand so we do. The median could easily be 60 minutes.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#length-of-gestation-in-north-carolina-1",
    "href": "one-sample-inference.html#length-of-gestation-in-north-carolina-1",
    "title": "4  One-sample inference",
    "section": "4.9 Length of gestation in North Carolina",
    "text": "4.9 Length of gestation in North Carolina\nThe data in file link are about 500 randomly chosen births of babies in North Carolina. There is a lot of information: not just the weight at birth of the baby, but whether the baby was born prematurely, the ages of the parents, whether the parents are married, how long (in weeks) the pregnancy lasted (this is called the “gestation”) and so on. We have seen these data before.\n\nRead in the data from the file into R, bearing in mind what type of file it is.\n\nSolution\nThis is a .csv file (it came from a spreadsheet), so it needs reading in accordingly. Work directly from the URL (rather than downloading the file):\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/ncbirths2.csv\"\nbw &lt;- read_csv(myurl)\n\nRows: 500 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): father_age, mother_age, weeks_gestation, pre_natal_visits, marital...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\\(\\blacksquare\\)\n\nFind a 95% confidence interval for the mean birth weight of all babies born in North Carolina (of which these babies are a sample). At the end, you should state what the confidence interval is. Giving some output is necessary, but not enough by itself.\n\nSolution\nThis:\n\nt.test(bw$weight_pounds)\n\n\n    One Sample t-test\n\ndata:  bw$weight_pounds\nt = 104.94, df = 499, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.936407 7.201093\nsample estimates:\nmean of x \n  7.06875 \n\n\nor (the same, but remember to match your brackets):\n\nwith(bw, t.test(weight_pounds))\n\n\n    One Sample t-test\n\ndata:  weight_pounds\nt = 104.94, df = 499, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.936407 7.201093\nsample estimates:\nmean of x \n  7.06875 \n\n\nThe confidence interval goes from 6.94 to 7.20 pounds.\nThere is an annoyance about t.test. Sometimes you can use data= with it, and sometimes not. When we do a two-sample \\(t\\)-test later, there is a “model formula” with a squiggle in it, and there we can use data=, but here not, so you have to use the dollar sign or the with to say which data frame to get things from. The distinction seems to be that if you are using a model formula, you can use data=, and if not, not.\nThis is one of those things that is a consequence of R’s history. The original t.test was without the model formula and thus without the data=, but the model formula got “retro-fitted” to it later. Since the model formula comes from things like regression, where data= is legit, that had to be retro-fitted as well. Or, at least, that’s my understanding.\n\\(\\blacksquare\\)\n\nBirth weights of babies born in the United States have a mean of 7.3 pounds. Is there any evidence that babies born in North Carolina are less heavy on average? State appropriate hypotheses, do your test, obtain a P-value and state your conclusion, in terms of the original data.\n\nSolution\nLet \\(\\mu\\) be the population mean (the mean weight of all babies born in North Carolina). Null hypothesis is \\(H_0: \\mu=7.3\\) pounds, and the alternative is that the mean is less: \\(H_a: \\mu&lt;7.3\\) pounds.\nNote that I defined \\(\\mu\\) first before I used it.\nThis is a one-sided alternative, which we need to feed into t.test:\n\nt.test(bw$weight_pounds, mu = 7.3, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  bw$weight_pounds\nt = -3.4331, df = 499, p-value = 0.0003232\nalternative hypothesis: true mean is less than 7.3\n95 percent confidence interval:\n     -Inf 7.179752\nsample estimates:\nmean of x \n  7.06875 \n\n\nOr with with. If you see what I mean.\nThe P-value is 0.0003, which is less than any \\(\\alpha\\) we might have chosen: we reject the null hypothesis in favour of the alternative, and thus we conclude that the mean birth weight of babies in North Carolina is indeed less than 7.3 pounds.\n“Reject the null hypothesis” is not a complete answer. You need to say something about what rejecting the null hypothesis means in this case: that is, you must make a statement about birth weights of babies.\n\\(\\blacksquare\\)\n\nThe theory behind the \\(t\\)-test says that the distribution of birth weights should be (approximately) normally distributed. Obtain a histogram of the birth weights. Does it look approximately normal? Comment briefly. (You’ll have to pick a number of bins for your histogram first. I don’t mind very much what you pick, as long as it’s not obviously too many or too few bins.)\n\nSolution\nWe did this before (and discussed the number of bins before), so I’ll just reproduce my 10-bin histogram (which is what I preferred, but this is a matter of taste):\n\nggplot(bw, aes(x = weight_pounds)) + geom_histogram(bins = 10)\n\n\n\n\nSo, we were assessing normality. What about that?\nIt is mostly normal-looking, but I am suspicious about those very low birth weights, the ones below about 4 pounds. There are too many of those, as I see it.\nIf you think this is approximately normal, you need to make some comment along the lines of “the shape is approximately symmetric with no outliers”. I think my first answer is better, but this answer is worth something, since it is a not completely unreasonable interpretation of the histogram.\nA normal quantile plot is better for assessing normality than a histogram is, but I won’t make you do one until we have seen the idea in class. Here’s the normal quantile plot for these data:\n\nggplot(bw, aes(sample = weight_pounds)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis is rather striking: the lowest birthweights (the ones below 5 pounds or so) are way too low for a normal distribution to apply. The top end is fine (except perhaps for that one very heavy baby), but there are too many low birthweights for a normal distribution to be believable. Note how much clearer this story is than on the histogram.\nHaving said that, the \\(t\\)-test, especially with a sample size as big as this (500), behaves very well when the data are somewhat non-normal (because it takes advantage of the Central Limit Theorem: that is, it’s the sampling distribution of the sample mean whose shape matters). So, even though the data are definitely not normal, I wouldn’t be too worried about our test.\nThis perhaps gives some insight as to why Freedman-Diaconis said we should use so many bins for our histogram. We have a lot of low-end outliers, so that the IQR is actually small compared to the overall spread of the data (as measured, say, by the SD or the range) and so FD thinks we need a lot of bins to describe the shape. Sturges is based on data being approximately normal, so it will tend to produce a small number of bins for data that have outliers.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#inferring-ice-break-up-in-nenana-1",
    "href": "one-sample-inference.html#inferring-ice-break-up-in-nenana-1",
    "title": "4  One-sample inference",
    "section": "4.10 Inferring ice break-up in Nenana",
    "text": "4.10 Inferring ice break-up in Nenana\nNenana, Alaska, is about 50 miles west of Fairbanks. Every spring, there is a contest in Nenana. A wooden tripod is placed on the frozen river, and people try to guess the exact minute when the ice melts enough for the tripod to fall through the ice. The contest started in 1917 as an amusement for railway workers, and has taken place every year since. Now, hundreds of thousands of people enter their guesses on the Internet and the prize for the winner can be as much as $300,000.\nBecause so much money is at stake, and because the exact same tripod is placed at the exact same spot on the ice every year, the data are consistent and accurate. The data are in link.\nYes, we saw these data before.\n\nRead the data into R, as before, or use the data frame that you read in before. Note that the values are separated by tabs rather than spaces, so you’ll need an appropriate read_ to read it in.\n\nSolution\nThese are “tab-separated values”, so read_tsv is the thing, as for the Australian athletes:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/nenana.txt\"\nnenana &lt;- read_tsv(myurl)\n\nRows: 87 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Date&Time\ndbl (2): Year, JulianDate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nUse whatever name you like for the data frame. One that is different from any of the column headers is smart; then it is clear whether you mean the whole data frame or one of its columns. ice or melt or anything like that would also be good.\n\\(\\blacksquare\\)\n\nObtain a 90% confidence interval for the mean JulianDate. What interval do you get? Looking back at your histogram, do you have any doubts about the validity of what you have just done?\n\nSolution\nThis is a matter of using t.test and pulling out the interval. Since we are looking for a non-standard interval, we have to remember conf.level as the way to get the confidence level that we want. I’m going with with this time, though the dollar-sign thing is equally as good:\n\nwith(nenana, t.test(JulianDate, conf.level = 0.90))\n\n\n    One Sample t-test\n\ndata:  JulianDate\nt = 197.41, df = 86, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 124.4869 126.6018\nsample estimates:\nmean of x \n 125.5443 \n\n\nBetween 124.5 and 126.6 days into the year. Converting that into something we can understand (because I want to), there are \\(31+28+31+30=120\\) days in January through April (in a non-leap year), so this says that the mean breakup date is between about May 4 and May 6.\nThe \\(t\\)-test is based on an assumption of data coming from a normal distribution. The histogram we made earlier looks pretty much normal, so there are no doubts about normality and thus no doubts about the validity of what we have done, on the evidence we have seen so far. (I have some doubts on different grounds, based on another of the plots we did earlier, which I’ll explain later, but all I’m expecting you to do is to look at the histogram and say “Yep, that’s normal enough”. Bear in mind that the sample size is 87, which is large enough for the Central Limit Theorem to be pretty helpful, so that we don’t need the data to be more than “approximately normal” for the sampling distribution of the sample mean to be very close to \\(t\\) with the right df.)\n\\(\\blacksquare\\)\n\nAn old-timer in Nenana strokes his grey beard and says “When I were young, I remember the tripod used to fall into the water around May 10”. In a non-leap year, May 10 is Julian day 130. Test the null hypothesis that the mean JulianDay is 130, against the alternative that it is less. What do you conclude? What practical implication does that have (assuming that the old-timer has a good memory)?\n\nSolution\nThe test is t.test again, but this time we have to specify a null mean and a direction of alternative:\n\nwith(nenana, t.test(JulianDate, mu = 130, alternative = \"less\"))\n\n\n    One Sample t-test\n\ndata:  JulianDate\nt = -7.0063, df = 86, p-value = 2.575e-10\nalternative hypothesis: true mean is less than 130\n95 percent confidence interval:\n     -Inf 126.6018\nsample estimates:\nmean of x \n 125.5443 \n\n\nFor a test, look first at the P-value, which is 0.0000000002575: that is to say, the P-value is very small, definitely smaller than 0.05 (or any other \\(\\alpha\\) you might have chosen). So we reject the null hypothesis, and conclude that the mean JulianDate is actually less than 130.\nNow, this is the date on which the ice breaks up on average, and we have concluded that it is earlier than it used to be, since we are assuming the old-timer’s memory is correct.\nThis is evidence in favour of global warming; a small piece of evidence, to be sure, but the ice is melting earlier than it used to all over the Arctic, so it’s not just in Nenana that it is happening. You don’t need to get to the “global warming” part, but I do want you to observe that the ice is breaking up earlier than it used to.\n\\(\\blacksquare\\)\n\nPlot JulianDate against Year on a scatterplot. What recent trends, if any, do you see? Comment briefly. (You did this before, but I have some extra comments on the graph this time, so feel free to just read this part.)\n\nSolution\nI liked the ggplot with a smooth trend on it:\n\nggplot(nenana, aes(x = Year, y = JulianDate)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThere was something obvious to see: after about 1960, there is a clear downward trend: the ice is breaking up earlier on average every year. Even though there is a lot of variability, the overall trend, viewed this way, is clear (and consistent with the test we did earlier). Note that the old-timer’s value of 130 is the kind of JulianDate we would typically observe around 1920, which would make the old-timer over 90 years old.\nAll right, why did I say I had some doubts earlier? Well, because of this downward trend, the mean is not actually the same all the way through, so it doesn’t make all that much sense to estimate it, which is what we were doing earlier by doing a confidence interval or a hypothesis test. What would actually make more sense is to estimate the mean JulianDate for a particular year. This could be done by a regression: predict JulianDate from Year, and then get a “confidence interval for the mean response” (as you would have seen in B27 or will see in C67). The trend isn’t really linear, but is not that far off. I can modify the previous picture to give you an idea. Putting in method=\"lm\" fits a line; as we see later, lm does regressions in R:\n\nggplot(nenana, aes(x = Year, y = JulianDate)) + geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCompare the confidence interval for the mean JulianDate in 1920: 126 to 131 (the shaded area on the graph), with 2000: 121 to 125. A change of about 5 days over 80 years. And with the recent trend that we saw above, it’s probably changing faster than that now. Sobering indeed.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#diameters-of-trees-1",
    "href": "one-sample-inference.html#diameters-of-trees-1",
    "title": "4  One-sample inference",
    "section": "4.11 Diameters of trees",
    "text": "4.11 Diameters of trees\nThe Wade Tract in Thomas County, Georgia, is an old-growth forest of longleaf pine trees. It has survived in a relatively undisturbed state since before settlements of the area by Europeans. For each tree in the tract, researchers measured the diameter at breast height. This is a standard measure in forestry: it is defined as the diameter of the tree at 4.5 feet above the ground.5 They are interested in the mean diameter at breast height of the trees in this tract. These values are in http://ritsokiguess.site/datafiles/treediameter.csv. The diameters are measured in centimetres. The easiest way to get the URL is to right-click on the blue text and select Copy URL. (If you copy and paste the actual text you might end up with extra spaces, especially if the printed URL goes over two lines.)\n\nRead in and display (some of) the data.\n\nSolution\nThe obvious way is this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/treediameter.csv\"\ntrees &lt;- read_csv(my_url)\n\nRows: 40 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): diameter\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntrees\n\n\n\n  \n\n\n\nCall the data frame what you like, though it is better to use a name that tells you what the dataframe contains (rather than something like mydata).\nExtra 1: there is only one column, so you can pretend the columns are separated by anything at all. Thus you could use this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/treediameter.csv\"\ntrees &lt;- read_delim(my_url, \" \")\n\nRows: 40 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (1): diameter\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntrees\n\n\n\n  \n\n\n\nor even this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/treediameter.csv\"\ntrees &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  diameter = col_double()\n)\n\ntrees\n\n\n\n  \n\n\n\nExtra 2: you might be wondering how they measure the diameter without doing something like drilling a hole through the tree. They don’t actually measure the diameter at all. What they measure is the circumference of the tree, which is easy enough to do with a tape measure. Longleaf pines are usually near circular, so you get the diameter by taking the circumference and dividing by \\(\\pi\\). This City of Portland website shows you how it’s done.\n\\(\\blacksquare\\)\n\nMake a suitable plot of your dataframe.\n\nSolution\nOne quantitative variable, so a histogram. Choose a sensible number of bins. There are 40 observations, so a number of bins up to about 10 is good. Sturges’ rule says 6 since \\(2^6=64\\):\n\nggplot(trees, aes(x=diameter)) + geom_histogram(bins=6)\n\n\n\n\nExtra 1: comments come later, but you might care to note (if only for yourself) that the distribution is a little skewed to the right, or, perhaps better, has no left tail at all. You might even observe that diameters cannot be less than 0 (they are measurements), and so you might expect a skew away from the limit.\nAfter you’ve looked at the \\(t\\) procedures for these data, we’ll get back to the shape.\nExtra 2: later we look at a more precise tool for assessing normality, the normal quantile plot, which looks like this:\n\nggplot(trees, aes(sample=diameter)) + stat_qq() + stat_qq_line()\n\n\n\n\nIf the data come from a normal distribution, the points should follow the straight line, at least approximately. Here, most of the points do, except for the points on the left, which veer away upwards from the line: that is, the highest values, on the right, are about right for a normal distribution, but the lowest values, on the left, don’t go down low enough.6 Thus, the problem with normality is not the long tail on the right, but the short one on the left. It is hard to get this kind of insight from the histogram, but at the moment, it’s the best we have.\nThe big problems, for things like \\(t\\)-tests that depend on means, is stuff like outliers, or long tails, with extreme values that might distort the mean. Having short tails, as the left tail here, will make the distribution look non-normal but won’t cause any problems for the \\(t\\)-tests.\n\\(\\blacksquare\\)\n\nObtain a 95% confidence interval for the mean diameter.\n\nSolution\nThis is t.test, but with conf.level to get the interval (and then you ignore the P-value):\n\nwith(trees, t.test(diameter))\n\n\n    One Sample t-test\n\ndata:  diameter\nt = 9.748, df = 39, p-value = 5.245e-12\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.6274 32.9526\nsample estimates:\nmean of x \n    27.29 \n\n\nThe mean diameter of a longleaf pine (like the ones in this tract) is between 21.6 and 33.0 centimetres.\nIf you prefer, do it this way:\n\nt.test(trees$diameter)\n\n\n    One Sample t-test\n\ndata:  trees$diameter\nt = 9.748, df = 39, p-value = 5.245e-12\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.6274 32.9526\nsample estimates:\nmean of x \n    27.29 \n\n\nYou need to state the answer and round it off suitably. The actual diameters in the data have one decimal place, so you can give the same accuracy for the CI, or at most two decimals (so 21.63 to 32.95 cm would also be OK).7 Giving an answer with more decimals is something you cannot possibly justify. Worse even than giving too many decimals is not writing out the interval at all. Never make your reader find something in output. If they want it, tell them what it is.\nThus, here (if this were being graded), one mark for the output, one more for saying what the interval is, and the third if you give the interval with a sensible number of decimals.\n\\(\\blacksquare\\)\n\nBased on what you have seen so far, would you expect to reject a null hypothesis that the population mean diameter (of all longleaf pines like these) is 35 cm? Explain briefly. Then, carry out the test (against a two-sided alternative) and explain briefly whether you were right.\n\nSolution\nThe logic is that “plausible” values for the population mean, ones you believe, are inside the interval, and implausible ones that you don’t believe are outside. Remember that the interval is your best answer to “what is the population mean”, and 35 is outside the interval so you don’t think the population mean is 35, and thus you would reject it.\nAre we right? Take out the conf.level and put in a mu:\n\nwith(trees, t.test(diameter, mu = 35))\n\n\n    One Sample t-test\n\ndata:  diameter\nt = -2.754, df = 39, p-value = 0.008895\nalternative hypothesis: true mean is not equal to 35\n95 percent confidence interval:\n 21.6274 32.9526\nsample estimates:\nmean of x \n    27.29 \n\n\nThe P-value is less than our \\(\\alpha\\) of 0.05, so we would indeed reject a mean of 35 cm (in favour of the mean being different from 35).\n\\(\\blacksquare\\)\n\nWould you expect 35 cm to be in a 99% confidence interval for the mean diameter? Explain briefly, and then see if you were right.\n\nSolution\nThe P-value is less than 0.01 (as well as being less than 0.05), so, in the same way that 35 was outside the 95% interval, it should be outside the 99% CI also. Maybe not by much, though, since the P-value is only just less than 0.01:\n\nwith(trees, t.test(diameter, conf.level = 0.99))\n\n\n    One Sample t-test\n\ndata:  diameter\nt = 9.748, df = 39, p-value = 5.245e-12\nalternative hypothesis: true mean is not equal to 0\n99 percent confidence interval:\n 19.70909 34.87091\nsample estimates:\nmean of x \n    27.29 \n\n\nIndeed so, outside, but only just.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#one-sample-cholesterol-1",
    "href": "one-sample-inference.html#one-sample-cholesterol-1",
    "title": "4  One-sample inference",
    "section": "4.12 One-sample cholesterol",
    "text": "4.12 One-sample cholesterol\nThe data set here contains cholesterol measurements for heart attack patients (at several different times) as well as for a group of control patients. We will focus on the control patients in this question.\n\nRead in and display (some of) the data.\n\nSolution\nThis is (as you might guess) a .csv, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/cholest.csv\"\ncholest &lt;- read_csv(my_url)\n\nRows: 30 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): 2-Day, 4-Day, 14-Day, control\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncholest\n\n\n\n  \n\n\n\nNote for yourself that there are 30 observations (and some missing ones), and a column called that is the one we’ll be working with.\nExtra: the 2-day, 4-day and 14-day columns need to be referred to with funny “backticks” around their names, because a column name cannot contain a - or start with a number. This is not a problem here, since we won’t be using those columns, but if we wanted to, this would not work:\n\ncholest %&gt;% summarize(xbar = mean(2-Day))\n\nError in `summarize()`:\nℹ In argument: `xbar = mean(2 - Day)`.\nCaused by error in `h()`:\n! error in evaluating the argument 'x' in selecting a method for function 'mean': object 'Day' not found\n\n\nbecause it is looking for a column called Day, which doesn’t exist. The meaning of 2-Day is “take the column called Day and subtract it from 2”. To make this work, we have to supply the backticks ourselves:\n\ncholest %&gt;% summarize(xbar = mean(`2-Day`, na.rm = TRUE))\n\n\n\n  \n\n\n\nThis column also has missing values (at the bottom), so here I’ve asked to remove the missing values8 before working out the mean. Otherwise the mean is, unhelpfully, missing as well.\nYou might imagine that dealing with column names like this would get annoying. There is a package called janitor that has a function called clean_names to save you the trouble. Install it first, then load it:\n\nlibrary(janitor)\n\nand then pipe your dataframe into clean_names and see what happens:\n\ncholest %&gt;% clean_names() -&gt; cholest1\ncholest1\n\n\n\n  \n\n\n\nThese are all legit column names; the - has been replaced by an underscore, and each of the first three column names has gained an x on the front so that it no longer starts with a number. This then works:\n\ncholest1 %&gt;% summarize(xbar = mean(x2_day, na.rm = TRUE))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake a suitable plot of the cholesterol levels of the control patients, and comment briefly on the shape of the distribution.\n\nSolution\nThere is one quantitative variable, so a histogram, as ever:\n\nggplot(cholest, aes(x=control)) + geom_histogram(bins=6)\n\n\n\n\nPick a number of bins that shows the shape reasonably well. Too many or too few won’t. (Sturges’ rule says 6, since there are 30 observations and \\(2^5=32\\).) Seven bins also works, but by the time you get to 8 bins or more, you are starting to lose a clear picture of the shape. Four bins is, likewise, about as low as you can go before getting too crude a picture.\nChoosing one of these numbers of bins will make it clear that the distribution is somewhat skewed to the right.\n\\(\\blacksquare\\)\n\nIt is recommended that people in good health, such as the Control patients here, keep their cholesterol level below 200. Is there evidence that the mean cholesterol level of the population of people of which the Control patients are a sample is less than 200? Show that you understand the process, and state your conclusion in the context of the data.\n\nSolution\nThe word “evidence” means to do a hypothesis test and get a P-value. Choose an \\(\\alpha\\) first, such as 0.05.\nTesting a mean implies a one-sample \\(t\\)-test. We are trying to prove that the mean is less than 200, so that’s our alternative: \\(H_a: \\mu &lt; 200\\), and therefore the null is that the mean is equal to 200: \\(H_0: \\mu = 200\\). (You might think it makes more logical sense to have \\(H_0: \\mu \\ge 200\\), which is also fine. As long as the null hypothesis has an equals in it in a logical place, you are good.)\n\nwith(cholest, t.test(control, mu=200, alternative = \"less\"))\n\n\n    One Sample t-test\n\ndata:  control\nt = -1.6866, df = 29, p-value = 0.05121\nalternative hypothesis: true mean is less than 200\n95 percent confidence interval:\n     -Inf 200.0512\nsample estimates:\nmean of x \n 193.1333 \n\n\nThis is also good:\n\nt.test(cholest$control, mu=200, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  cholest$control\nt = -1.6866, df = 29, p-value = 0.05121\nalternative hypothesis: true mean is less than 200\n95 percent confidence interval:\n     -Inf 200.0512\nsample estimates:\nmean of x \n 193.1333 \n\n\nI like the first version better because a lot of what we do later involves giving a data frame, and then working with things in that data frame. This is more like that.\nThis test is one-sided because we are looking for evidence of less; if the mean is actually more than 200, we don’t care about that. For a one-sided test, R requires you to say which side you are testing.\nThe P-value is not (quite) less than 0.05, so we cannot quite reject the null. Therefore, there is no evidence that the mean cholesterol level (of the people of which the control group are a sample) is less than 200. Or, this mean is not significantly less than 200. Or, we conclude that this mean is equal to 200. Or, we conclude that this mean could be 200. Any of those.\nIf you chose a different \\(\\alpha\\), draw the right conclusion for the \\(\\alpha\\) you chose. For example, with \\(\\alpha=0.10\\), we do have evidence that the mean is less than 200. Being consistent is more important than getting the same answer as me.\nWriting out all the steps correctly shows that you understand the process. Anything less doesn’t.\n\\(\\blacksquare\\)\n\nWhat values could the population mean cholesterol level take? You might need to get some more output to determine this.\n\nSolution\nThis is not quoting the sample mean, giving that as your answer, and then stopping. The sample mean should, we hope, be somewhere the population mean, but it is almost certainly not the same as the population mean, because there is variability due to random sampling. (This is perhaps the most important thing in all of Statistics: recognizing that variability exists and dealing with it.)\nWith that in mind, the question means to get a range of values that the population mean could be: that is to say, a confidence interval. The one that came out of the previous output is one-sided, to go with the one-sided test, but confidence intervals for us are two-sided, so we have to run the test again, but two-sided, to get it. To do that, take out the “alternative”, thus (you can also take out the null mean, since a confidence interval has no null hypothesis):\n\nwith(cholest, t.test(control))\n\n\n    One Sample t-test\n\ndata:  control\nt = 47.436, df = 29, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 184.8064 201.4603\nsample estimates:\nmean of x \n 193.1333 \n\n\nWith 95% confidence, the population mean cholesterol level is between 184.8 and 201.5.\nYou need to state the interval, and you also need to round off the decimal places to something sensible. This is because in your statistical life, you are providing results to someone else in a manner that they can read and understand. They do not have time to go searching in some output, or to fish through some excessive number of decimal places. If that’s what you give them, they will ask you to rewrite your report, wasting everybody’s time when you could have done it right the first time.\nHow many decimal places is a good number? Look back at your data. In this case, the cholesterol values are whole numbers (zero decimal places). A confidence interval is talking about a mean. In this case, we have a sample size of 30, which is between 10 and 100, so we can justify one extra decimal place beyond the data, here one decimal altogether, or two at the absolute outside. (Two is more justifiable if the sample size is bigger than 100.) See, for example, this, in particular the piece at the bottom.\n\\(\\blacksquare\\)\n\nExplain briefly why you would be reasonably happy to trust the \\(t\\) procedures in this question. (There are two points you need to make.)\n\nSolution\nThe first thing is to look back at the graph you made earlier. This was skewed to the right (“moderately” or “somewhat” or however you described it). This would seem to say that the \\(t\\) procedures were not very trustworthy, since the population distribution doesn’t look very normal in shape.\nHowever, the second thing is to look at the sample size. We have the central limit theorem, which says (for us) that the larger the sample is, the less the normality matters, when it comes to estimating the mean. Here, the sample size is 30, which, for the central limit theorem, is large enough to overcome moderate non-normality in the data.\nMy take, which I was trying to guide you towards, is that our non-normality was not too bad, and so our sample size is large enough to trust the \\(t\\) procedures we used.\nExtra 1: What matters is the tradeoff between sample size and the extent of the non-normality. If your data is less normal, you need a larger sample size to overcome it. Even a sample size of 500 might not be enough if your distribution is very skewed, or if you have extreme outliers.\nThe place \\(n=30\\) comes from is back from the days when we only ever used printed tables. In most textbooks, if you printed the \\(t\\)-table on one page in a decent-sized font, you’d get to about 29 df before running out of space. Then they would say “\\(\\infty\\) df” and put the normal-distribution \\(z\\) numbers in. If the df you needed was bigger than what you had in the table, you used this last line: that is, you called the sample “large”. Try it in your stats textbooks: I bet the df go up to 30, then you get a few more, then the \\(z\\) numbers.\nExtra 2: By now you are probably thinking that this is very subjective, and so it is. What actually matters is the shape of the thing called the sampling distribution of the sample mean. That is to say, what kind of sample means you might get in repeated samples from your population. The problem is that you don’t know what the population looks like.9 But we can fake it up, in a couple of ways: we can play what-if and pretend we know what the population looks like (to get some understanding for “populations like that”), or we can use a technique called the “bootstrap” that will tell us what kind of sample means we might get from the population that our sample came from (this seems like magic and, indeed, is).\nThe moral of the story is that the central limit theorem is more powerful than you think.\nTo illustrate my first idea, let’s pretend the population looks like this, with a flat top:\n\n\n\n\n\nOnly values between 0 and 1 are possible, and each of those is equally likely. Not very normal in shape. So let’s take some random samples of size three, not in any sense a large sample, from this “uniform” population, and see what kind of sample means we get. This technique is called simulation: rather than working out the answer by math, we’re letting the computer approximate the answer for us. Here’s one simulated sample:\n\nu &lt;- runif(3)\nu\n\n[1] 0.9475841 0.1245953 0.2277288\n\nmean(u)\n\n[1] 0.4333027\n\n\nand here’s the same thing 1000 times, including a histogram of the sample means:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(runif(3))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12)\n\n\n\n\nThis is our computer-generated assessment of what the sampling distribution of the sample mean looks like. Isn’t this looking like a normal distribution?\nLet’s take a moment to realize what this is saying. If the population looks like the flat-topped uniform distribution, the central limit theorem kicks in for a sample of size three, and thus if your population looks like this, \\(t\\) procedures will be perfectly good for \\(n=3\\) or bigger, even though the population isn’t normal.\nThus, when you’re thinking about whether to use a \\(t\\)-test or something else (that we’ll learn about later), the distribution shape matters, but so does the sample size.\nI should say a little about my code. I’m not expecting you to figure out details now (we see the ideas properly in simulating power of tests), but in words, one line at a time:\n\ngenerate 1000 (“many”) samples each of 3 observations from a uniform distribution\nfor each sample, work out the mean of it\nturn those sample means into a data frame with a column called value\nmake a histogram of those.\n\nNow, the central limit theorem doesn’t always work as nicely as this, but maybe a sample size of 30 is large enough to overcome the skewness that we had:\n\nggplot(cholest, aes(x=control)) + geom_histogram(bins=6)\n\n\n\n\nThat brings us to my second idea above.\nThe sample that we had is in some sense an “estimate of the population”. To think about the sampling distribution of the sample mean, we need more estimates of the population. How might we get those? The curious answer is to sample from the sample. This is the idea behind the bootstrap. (This is what Lecture 3c is about.) The name comes from the expression “pulling yourself up by your own bootstraps”, meaning “to begin an enterprise or recover from a setback without any outside help” (from here), something that should be difficult or impossible. How is it possible to understand a sampling distribution with only one sample?\nWe have to be a bit careful. Taking a sample from the sample would give us the original sample back. So, instead, we sample with replacement, so that each bootstrap sample is different:\n\nsort(cholest$control)\n\n [1] 160 162 164 166 170 176 178 178 182 182 182 182 182 184 186 188 196 198 198\n[20] 198 200 200 204 206 212 218 230 232 238 242\n\nsort(sample(cholest$control, replace=TRUE))\n\n [1] 164 166 166 166 166 176 178 178 182 182 182 182 182 188 198 198 198 200 200\n[20] 200 200 204 206 206 218 218 230 232 232 242\n\n\nA bootstrap sample contains repeats of the original data values, and misses some of the others. Here, the original data had values 160 and 162 that are missing in the bootstrap sample; the original data had one value 166, but the bootstrap sample has four! I sorted the data and the bootstrap sample to make this clearer; you will not need to sort. This is a perfectly good bootstrap sample:\n\nsample(cholest$control, replace = TRUE)\n\n [1] 242 232 198 160 242 182 182 182 198 162 212 198 242 204 242 242 170 198 182\n[20] 206 232 170 218 188 166 178 164 160 218 196\n\n\nSo now we know what to do: take lots of bootstrap samples, work out the mean of each, plot the means, and see how normal it looks. The only new idea here is the sampling with replacement:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(cholest$control, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12)\n\n\n\n\nThat looks pretty normal, not obviously skewed, and so the \\(t\\) procedures we used will be reliable enough.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#footnotes",
    "href": "one-sample-inference.html#footnotes",
    "title": "4  One-sample inference",
    "section": "",
    "text": "The height of a typical human breast off the ground. Men have a breast too, you know.↩︎\nThe normal quantile plot is rather interesting: it says that the uppermost values are approximately normal, but the smallest eight or so values are too bunched up to be normal. That is, normality fails not because of the long tail on the right, but the bunching on the left. Still right-skewed, though.↩︎\nIn some languages, a dot is used to concatenate bits of text, or as a way of calling a method on an object. But in R, a dot has no special meaning, and is used in function names like t.test. Or p.value.↩︎\nWhether you think it is or not may depend on how many bins you have on your histogram. With 5 bins it looks like an outlier, but with 6 it does not. Try it and see.↩︎\nThe height of a typical human breast off the ground. Men have a breast too, you know.↩︎\nThey cannot go down far enough, because they can’t go below zero.↩︎\nOne more decimal place than the data is the maximum you give in a CI.↩︎\nIn R, missing values are labelled NA, and rm is Unix/C shorthand for remove.↩︎\nIf you did, all your problems would be over.↩︎"
  },
  {
    "objectID": "two-sample-inference.html#children-and-electronic-devices",
    "href": "two-sample-inference.html#children-and-electronic-devices",
    "title": "5  Two-sample inference",
    "section": "5.1 Children and electronic devices",
    "text": "5.1 Children and electronic devices\nDo children (aged 8–17) spend more time on electronic devices now than they did 10 years ago? Samples of 15 children aged 8–17 were taken in each of two years, 1999 and 2009, and the children (with their parents’ help) were asked to keep a diary of the number of hours they spent using electronic devices on a certain day. The data are in the file http://ritsokiguess.site/datafiles/pluggedin.txt.\n\nRead in the data and verify that you have 30 rows of data from two different years.\nDraw side-by-side boxplots of the number of hours for each year. year is a numeric variable that we want to treat as a factor, so we need to make it into a factor.\nTest whether the mean number of hours has increased since 1999. Which test did R do?\nObtain a 99% confidence interval for the difference in means."
  },
  {
    "objectID": "two-sample-inference.html#parking-close-to-the-curb",
    "href": "two-sample-inference.html#parking-close-to-the-curb",
    "title": "5  Two-sample inference",
    "section": "5.2 Parking close to the curb",
    "text": "5.2 Parking close to the curb\nIn 2009, the Toronto Star commissioned a survey to address the issue of who is better at parking a car: men or women. The researchers recorded 93 drivers who were parallel-parking their car in downtown Toronto, and for each driver, recorded the distance between the car and the curb, in inches, when the driver was finished parking their car. The data are in an Excel spreadsheet, link. Click on the link. The data will probably download automatically. Check the folder on your computer where things get downloaded.1 If the spreadsheet is just displayed and not downloaded, save it somewhere on your computer.\n\nThere are two sheets in this spreadsheet workbook. They are of the same data in two different formats. Take a look at Sheet 1 and Sheet 2. Describe the format of the data in each case. Which will be the most suitable data layout, bearing in mind that one of the first things we do is to make side-by-side boxplots of parking distances for males and females? Explain briefly.\nRead your preferred sheet directly into R, without using a .csv file. (There is a clue in the lecture notes, in the section about reading in files.) If you get stuck, make a .csv file and read that in.\nObtain side-by-side boxplots of parking distances for males and females. Does one gender seem to be better at parking than the other? Explain briefly.\nExplain briefly why this is two independent samples rather than matched pairs.\nRun a suitable \\(t\\)-test for comparing parking distances for males and females. What do you conclude, in the context of the data?\nWhy might you have some doubts about the \\(t\\)-test that you just did? Explain briefly.\nThe Toronto Star in its report said that females are more accurate at parking their cars. Why do you think they concluded that, and do you think they were right to do so? Explain briefly."
  },
  {
    "objectID": "two-sample-inference.html#bell-peppers-and-too-much-water",
    "href": "two-sample-inference.html#bell-peppers-and-too-much-water",
    "title": "5  Two-sample inference",
    "section": "5.3 Bell peppers and too much water",
    "text": "5.3 Bell peppers and too much water\nA pathogen called Phytophthora capsici causes bell peppers to wilt and die. It is thought that too much water aids in the spread of the pathogen. Two fields are under study, labelled a and b. The first step in the research project is to compare the mean soil water content of the two fields. There is a suspicion that field a will have a higher water content than field b. The data are in the file link.\n\nRead the file in using read_csv, and list the resulting data frame.\nMake side-by-side boxplots of the water content values for the two fields. How do the fields seem to compare?\nDo a two-sample \\(t\\)-test to test whether there is evidence that the mean water content in field a is higher than that of field b. What do you conclude? Explain briefly. (You’ll need to figure out a way of doing a one-sided test, or how to adapt the results from a two-sided test.)\nIs the result of your test consistent with the boxplot, or not? Explain briefly."
  },
  {
    "objectID": "two-sample-inference.html#exercise-and-anxiety-and-bullying-mice",
    "href": "two-sample-inference.html#exercise-and-anxiety-and-bullying-mice",
    "title": "5  Two-sample inference",
    "section": "5.4 Exercise and anxiety and bullying mice",
    "text": "5.4 Exercise and anxiety and bullying mice\nDoes exercise help to reduce anxiety? To assess this, some researchers randomly assigned mice to either an enriched environment where there was an exercise wheel available, or a standard environment with no exercise options. After three weeks in the specified environment, for five minutes a day for two weeks, the mice were each exposed to a “mouse bully” — a mouse who was very strong, aggressive, and territorial. One measure of mouse anxiety is amount of time hiding in a dark compartment, with mice who are more anxious spending more time in darkness. The amount of time spent in darkness is recorded for each of the mice.\nThe data can be found at link.\n\nRead the data into R, and display your data frame. Count the number of mice in each group.\nDraw side-by-side boxplots of time spent in darkness for each group of mice.\nDo the boxplots support the hypothesis about exercise and anxiety? Explain briefly.\nCarry out a \\(t\\)-test for comparing the mean time spent in darkness for the mice in the two groups. Think carefully about the details of the \\(t\\)-test (and what you need evidence in favour of).\nWhat do you conclude, in terms of anxiety and exercise (at least for mice)? Explain briefly.\nDoes anything in the previous parts suggest any problems with the analysis you just did? Explain briefly."
  },
  {
    "objectID": "two-sample-inference.html#diet-and-growth-in-boys",
    "href": "two-sample-inference.html#diet-and-growth-in-boys",
    "title": "5  Two-sample inference",
    "section": "5.5 Diet and growth in boys",
    "text": "5.5 Diet and growth in boys\nA dietician is studying the effect of different diets on children’s growth. In part of the study, the dietician is investigating two religious sects, labelled a and b in our data set. Both sects are vegetarian; the difference between them is that people in Sect A only eat vegetables from below the ground, and Sect B only eats vegetables from above the ground. The height and weight of the boys2 are measured at regular intervals. The data in link are the heights of the boys at age 12.\n\nRead in the data and find out how many observations you have and which variables.\nObtain side-by-side boxplots of the heights for boys from each sect. Does it look as if the heights of the boys in each sect are different? Comment briefly.\nLooking at your boxplots, do you see any problems with doing a two-sample \\(t\\)-test? Explain briefly.\nRun a \\(t\\)-test to determine whether the mean heights differ significantly. What do you conclude? Explain briefly. (Run the \\(t\\)-test even if your previous work suggests that it is not the right thing to do.)"
  },
  {
    "objectID": "two-sample-inference.html#handspans-of-males-and-females",
    "href": "two-sample-inference.html#handspans-of-males-and-females",
    "title": "5  Two-sample inference",
    "section": "5.6 Handspans of males and females",
    "text": "5.6 Handspans of males and females\nTake your right hand, and stretch the fingers out as far as you can. The distance between the tip of your thumb and the tip of your little (pinky) finger is your handspan. The students in a Statistics class at Penn State measured their handspans and also whether they identified as male or female. The data are at http://ritsokiguess.site/datafiles/handspan.txt, with handspans measured in inches. Thinking of these as a random sample of all possible students, is it true that males have a larger mean handspan than females? This is what we will explore.\n\nRead in and display (some of) the data.\nMake a suitable graph of the two columns.\nRun a suitable two-sample \\(t\\)-test to address the question of interest. What do you conclude, in the context of the data?\nObtain a 90% confidence interval for the difference in mean handspan between males and females. Do you need to run any more code? Explain briefly.\nExplain briefly why you might have some concerns about the validity of the \\(t\\)-tests you ran in this question. Or, if you don’t have any concerns, explain briefly why that is."
  },
  {
    "objectID": "two-sample-inference.html#the-anchoring-effect-australia-vs-us",
    "href": "two-sample-inference.html#the-anchoring-effect-australia-vs-us",
    "title": "5  Two-sample inference",
    "section": "5.7 The anchoring effect: Australia vs US",
    "text": "5.7 The anchoring effect: Australia vs US\nTwo groups of students (in a class at a American university) were asked what they thought the population of Canada was. (The correct answer at the time was just over 30 million.) Some of the students, before having to answer this, were told that the population of the United States was about 270 million. The other students in the class were told that the population of Australia was about 18 million. The data are in http://ritsokiguess.site/datafiles/anchoring.csv. The first column contains the country whose population the student was told, and the second contains the student’s guess at the population of Canada.\nYou might wonder how being told the population of an unrelated country would have any impact on a student’s guess at the population of Canada. Psychology says it does: it’s called the anchoring effect, and the idea is that the number mentioned first acts as an “anchor”: a person’s guess will be closer to the anchor than it would have been otherwise. In this case, that would mean that the guesses for the students given the US as an anchor will be higher than for the students given Australia as an anchor. We are interested in seeing whether there is evidence for that here.\n\nRead in and display (some of) the data.\nDraw a suitable graph of these data.\nExplain briefly why a Welch \\(t\\)-test would be better than a pooled \\(t\\)-test in this case.\nRun a suitable Welch \\(t\\)-test and display the output.\nWhat do you conclude from your test, in the context of the data?\n\nMy solutions follow:"
  },
  {
    "objectID": "two-sample-inference.html#children-and-electronic-devices-1",
    "href": "two-sample-inference.html#children-and-electronic-devices-1",
    "title": "5  Two-sample inference",
    "section": "5.8 Children and electronic devices",
    "text": "5.8 Children and electronic devices\nDo children (aged 8–17) spend more time on electronic devices now than they did 10 years ago? Samples of 15 children aged 8–17 were taken in each of two years, 1999 and 2009, and the children (with their parents’ help) were asked to keep a diary of the number of hours they spent using electronic devices on a certain day. The data are in the file http://ritsokiguess.site/datafiles/pluggedin.txt.\n\nRead in the data and verify that you have 30 rows of data from two different years.\n\nSolution\nI see this:\n\nmyurl=\"http://ritsokiguess.site/datafiles/pluggedin.txt\"\nplugged=read_delim(myurl,\" \")\n\nRows: 30 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): year, hours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nplugged\n\n\n\n  \n\n\n\nI see only the first ten rows (with an indication that there are 20 more, so 30 altogether). In your notebook, it’ll look a bit different: again, you’ll see the first 10 rows, but you’ll see exactly how many rows and columns there are, and there will be buttons “Next” and “Previous” to see earlier and later rows, and a little right-arrow to see more columns to the right (to which is added a little left-arrow if there are previous columns to scroll back to). If you want to check for yourself that there are 30 rows, you can click Next a couple of times to get down to row 30, and then see that the Next button cannot be clicked again, and therefore that 30 rows is how many there are.\nOr, you can summarize the years by counting how many there are of each:\n\nplugged %&gt;% count(year)\n\n\n\n  \n\n\n\nor the more verbose form of the same thing:\n\nplugged %&gt;% group_by(year) %&gt;% summarize(rows=n())\n\n\n\n  \n\n\n\nAny of those says that it looks good. 30 rows, 1999 and 2009, 15 measurements for each.\n\\(\\blacksquare\\)\n\nDraw side-by-side boxplots of the number of hours for each year. year is a numeric variable that we want to treat as a factor, so we need to make it into a factor.\n\nSolution\n\nggplot(plugged,aes(x=factor(year),y=hours))+geom_boxplot()\n\n\n\n\nThe fct_inorder trick will also work, since the years are in the data in the order we want them to be displayed. (An option in case you have seen this.)\nThe median for 2009 is noticeably higher, and there is no skewness or outliers worth worrying about.\nThe measurements for the two years have a very similar spread, so there would be no problem running the pooled test here.\nYou might be bothered by the factor(year) on the \\(x\\)-axis. To get around that, you can define year-as-factor first, using mutate, then feed your new column into the boxplot. That goes like this. There is a wrinkle that I explain afterwards:\n\nplugged %&gt;% mutate(the_year=factor(year)) %&gt;%\nggplot(aes(x=the_year, y=hours))+geom_boxplot()\n\n\n\n\nYou could even redefine year to be the factor version of itself (if you don’t need the year-as-number anywhere else). The wrinkle I mentioned above is that in the ggplot you do not name the data frame first; the data frame used is the (nameless) data frame that came out of the previous step, not plugged but plugged with a new column the_year.\nNote how the \\(x\\)-axis now has the name of the new variable.\nIf you forget to make year into a factor, this happens:\n\nggplot(plugged, aes(x = year, y = hours)) + geom_boxplot()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n\nYou get one boxplot, for all the hours, without distinguishing by year, and a warning message that tries (and fails) to read our mind: yes, we have a continuous, quantitative x, but geom_boxplot doesn’t take a group. Or maybe it does. Try it and see:\n\nggplot(plugged,aes(x = year, y = hours, group = year)) + geom_boxplot()\n\n\n\n\nThe \\(x\\)-axis treats the year as a number, which looks a little odd, but adding a group correctly gets us two boxplots side by side, so this is also a good way to do it. So maybe the error message did read our mind after all.\n\\(\\blacksquare\\)\n\nTest whether the mean number of hours has increased since 1999. Which test did R do?\n\nSolution\nThe hard part to remember is how you specify a one-sided test in R; it’s alternative=\"less\" (rather than “greater”) because 1999 is “before” 2009:\n\nt.test(hours~year,data=plugged,alternative=\"less\")  \n\n\n    Welch Two Sample t-test\n\ndata:  hours by year\nt = -3.3323, df = 24.861, p-value = 0.001348\nalternative hypothesis: true difference in means between group 1999 and group 2009 is less than 0\n95 percent confidence interval:\n       -Inf -0.8121415\nsample estimates:\nmean in group 1999 mean in group 2009 \n          5.933333           7.600000 \n\n\nThe P-value is 0.0013. R does the Welch-Satterthwaite test by default (the unequal-variances one). Since we didn’t change that, that’s what we got. (The pooled test is below.)\nThis is the cleanest way to do it, because this version of t.test, with a “model formula” (the thing with the squiggle) allows a data= to say which data frame to get things from. The other ways, using (for example) with, also work:\n\nwith(plugged,t.test(hours~year,alternative=\"less\"))\n\n\n    Welch Two Sample t-test\n\ndata:  hours by year\nt = -3.3323, df = 24.861, p-value = 0.001348\nalternative hypothesis: true difference in means between group 1999 and group 2009 is less than 0\n95 percent confidence interval:\n       -Inf -0.8121415\nsample estimates:\nmean in group 1999 mean in group 2009 \n          5.933333           7.600000 \n\n\nThis also works, but is ugly:\n\nt.test(plugged$hours~plugged$year,alternative=\"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  plugged$hours by plugged$year\nt = -3.3323, df = 24.861, p-value = 0.001348\nalternative hypothesis: true difference in means between group 1999 and group 2009 is less than 0\n95 percent confidence interval:\n       -Inf -0.8121415\nsample estimates:\nmean in group 1999 mean in group 2009 \n          5.933333           7.600000 \n\n\nUgly because you’ve just typed the name of the data frame and the dollar sign twice for no reason. As a general principle, if you as a programmer are repeating yourself, you should stop and ask yourself how you can avoid the repeat.\nIf you want the pooled test in R, you have to ask for it:\n\nt.test(hours~year, alternative = \"less\", data = plugged, var.equal = TRUE)    \n\n\n    Two Sample t-test\n\ndata:  hours by year\nt = -3.3323, df = 28, p-value = 0.001216\nalternative hypothesis: true difference in means between group 1999 and group 2009 is less than 0\n95 percent confidence interval:\n       -Inf -0.8158312\nsample estimates:\nmean in group 1999 mean in group 2009 \n          5.933333           7.600000 \n\n\nAs is often the case, the P-values for the pooled and Welch-Satterthwaite tests are very similar, so from that point of view it doesn’t matter much which one you use. If you remember back to the boxplots, the number of hours had about the same spread for the two years, so if you used the pooled test instead of the Welch-Satterthwaite test, that would have been just fine.\nThere is a school of thought that says we should learn the Welch-Satterthwaite test and use that always. This is because W-S (i) works when the populations from which the groups are sampled have different SDs and (ii) is pretty good even when those SDs are the same.\nThe pooled test can go badly wrong if the groups have very different SDs. The story is this: if the larger sample is from the population with the larger SD, the probability of a type I error will be smaller than \\(\\alpha\\), and if the larger sample is from the population with the smaller SD, the probability of a type I error will be larger than \\(\\alpha\\). This is why you see S-W in STAB22. You see the pooled test in STAB57 because the logic of its derivation is so much clearer, not because it’s really the better test in practice. The theory says that if your data are normal in both groups with the same variance, then the pooled test is best, but it says nothing about the quality of the pooled test if any of that goes wrong. The usual approach to assessing things like this is via simulation, as we do for estimating power (later): generate some random data eg. from normal distributions with the same means, SDs 10 and 20 and sample sizes 15 and 30, run the pooled \\(t\\)-test, see if you reject, then repeat lots of times and see whether you reject about 5% of the time. Then do the same thing again with the sample sizes switched around. Or, do the same thing with Welch-Satterthwaite.\n\\(\\blacksquare\\)\n\nObtain a 99% confidence interval for the difference in means.\n\nSolution\nTake off the thing that made it one-sided, and put in a thing that gets the right CI:\n\nt.test(hours~year,data=plugged,conf.level=0.99)  \n\n\n    Welch Two Sample t-test\n\ndata:  hours by year\nt = -3.3323, df = 24.861, p-value = 0.002696\nalternative hypothesis: true difference in means between group 1999 and group 2009 is not equal to 0\n99 percent confidence interval:\n -3.0614628 -0.2718705\nsample estimates:\nmean in group 1999 mean in group 2009 \n          5.933333           7.600000 \n\n\n\\(-3.06\\) to \\(-0.27\\). The interval contains only negative values, which is consistent with our having rejected a null hypothesis of no difference in means.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#parking-close-to-the-curb-1",
    "href": "two-sample-inference.html#parking-close-to-the-curb-1",
    "title": "5  Two-sample inference",
    "section": "5.9 Parking close to the curb",
    "text": "5.9 Parking close to the curb\nIn 2009, the Toronto Star commissioned a survey to address the issue of who is better at parking a car: men or women. The researchers recorded 93 drivers who were parallel-parking their car in downtown Toronto, and for each driver, recorded the distance between the car and the curb, in inches, when the driver was finished parking their car. The data are in an Excel spreadsheet, link. Click on the link. The data will probably download automatically. Check the folder on your computer where things get downloaded.3 If the spreadsheet is just displayed and not downloaded, save it somewhere on your computer.\n\nThere are two sheets in this spreadsheet workbook. They are of the same data in two different formats. Take a look at Sheet 1 and Sheet 2. Describe the format of the data in each case. Which will be the most suitable data layout, bearing in mind that one of the first things we do is to make side-by-side boxplots of parking distances for males and females? Explain briefly.\n\nSolution\nThe data in Sheet 1 has one column of parking distances for males, and another for females. This is often how you see data of this sort laid out. Sheet 2 has one column of parking distances, all combined together, and a second column indicating the gender of the driver whose distance is in the first column. If you look back at the kind of data we’ve used to make side-by-side boxplots, it’s always been in the format of Sheet 2: one column containing all the values of the variable we’re interested in, with a second column indicating which group each observation belongs to (“group” here being “gender of driver”). So we need to use the data in Sheet 2, because the data in Sheet 1 are not easy to handle with R. The layout of Sheet 2 is the way R likes to do most things: so-called “long format” with a lot of rows and not many columns. This is true for descriptive stuff: side-by-side boxplots or histograms or means by group, as well as modelling such as (here) a two-sample \\(t\\)-test, or (in other circumstances, as with several groups) a one-way analysis of variance. Hadley Wickham, the guy behind the tidyverse, likes to talk about “tidy data” (like Sheet 2), with each column containing a variable, and “untidy data” (like Sheet 1), where the two columns are the same thing (distances), but under different circumstances (genders). As we’ll see later, it is possible to convert from one format to the other. Usually you want to make untidy data tidy (the function for this is called pivot_longer).\n\\(\\blacksquare\\)\n\nRead your preferred sheet directly into R, without using a .csv file. (There is a clue in the lecture notes, in the section about reading in files.) If you get stuck, make a .csv file and read that in.\n\nSolution\nThe direct way is to use the package readxl. This has a read_excel that works the same way as any of the other read_ functions. You’ll have to make sure that you read in sheet 2, since that’s the one you want. There is some setup first. There are a couple of ways you can do that:\n\nDownload the spreadsheet to your computer, and upload it to your project on R Studio Cloud (or, if you are running R Studio on your computer, use something like file.choose to get the file from wherever it got downloaded to).\nUse the function download.file to get the file from the URL and store it in your project folder directly. This also works in R Studio Cloud, and completely by-passes the download-upload steps that you would have to do otherwise. (I am grateful to Rose Gao for this idea.) Here is how you can use download.file here:\n\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/parking.xlsx\"\nlocal &lt;- \"parking.xlsx\"\ndownload.file(my_url, local, mode = \"wb\")\n\nWhen you’ve gotten the spreadsheet into your project folder via one of those two ways, you go ahead and do this:\n\nlibrary(readxl)\nparking &lt;- read_excel(\"parking.xlsx\", sheet = 2)\nparking\n\n\n\n  \n\n\n\nYou have to do it this way, using the version of the spreadsheet on your computer, since read_excel won’t take a URL, or if it does, I can’t make it work.4 I put the spreadsheet in R Studio’s current folder, so I could read it in by name, or you can do the f &lt;- file.choose() thing, find it, then read it in. The sheet= thing can take either a number (as here: the second sheet in the workbook), or a name (whatever name the sheet has on its tab in the workbook).\nExtra: Rose actually came up with a better idea, which I will show you and explain:\n\ntf &lt;- tempfile()\ndownload.file(my_url, tf, mode = \"wb\")\np &lt;- read_excel(tf, sheet = 2)\n\nWhat tempfile() does is to create a temporary file to hold the spreadsheet that you are about to download. After downloading the spreadsheet to the temporary file, you then use read_excel to read from the temporary file into the data frame.\nThe advantage of this approach is that the temporary file disappears as soon as you close R, and so you don’t have a copy of the spreadsheet lying around that you don’t need (once you have created the dataframe that I called parking, anyway).\nIf you are wondering about that mode thing on download.file: files are of two different types, “text” (like the text of an email, that you can open and look at in something like Notepad), and “binary” that you can’t look at directly, but for which you need special software like Word or Excel to decode it for you.5\nThe first character in mode is either w for “write a new file”, which is what we want here, or a for “append”, which would mean adding to the end of a file that already exists. Thus mode=\"wb\" means “create a new binary file”. End of Extra.\nIf you can’t make any of this work, then do it in two steps: save the appropriate sheet as a .csv file, and then read the .csv file using read_csv. If you experiment, you’ll find that saving a spreadsheet workbook as .csv only saves the sheet you’re looking at, so make sure you are looking at sheet 2 before you Save As .csv. I did that, and called my saved .csv parking2.csv (because it was from sheet 2, but you can use any name you like). Then I read this into R thus:\n\nparking2 &lt;- read_csv(\"parking2.csv\")\n\nRows: 93 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): gender\ndbl (1): distance\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nparking2\n\n\n\n  \n\n\n\nThe read-in data frame parking has 93 rows (\\(47+46=93\\) drivers) and two columns: the distance from the curb that the driver ended up at, and the gender of the driver. This is as the spreadsheet Sheet 2 was, and the first few distances match the ones in the spreadsheet.\nIf I were grading this, you’d get some credit for the .csv route, but I really wanted you to figure out how to read the Excel spreadsheet directly, so that’s what would be worth full marks.\nYou might want to check that you have some males and some females, and how many of each, which you could do this way:\n\nparking %&gt;% count(gender)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nObtain side-by-side boxplots of parking distances for males and females. Does one gender seem to be better at parking than the other? Explain briefly.\n\nSolution\nWith the right data set, this is a piece of cake:\n\nggplot(parking, aes(x = gender, y = distance)) + geom_boxplot()\n\n\n\n\nThe outcome variable is distance from the curb, so smaller should be better (more accurate parking). With that in mind, the median for females is a little smaller than for males (about 8.5 vs. about 10), so it seems that on average females are more accurate parkers than males are. The difference is small, however (and so you might be wondering at this point whether it’s a statistically significant difference — don’t worry, that’s coming up).\nBefore I leave this one, I want to show you something else: above-and-below histograms, as another way of comparing males and females (two or more groups, in general). First, we make a histogram of all the distances, without distinguishing by gender:\n\nggplot(parking, aes(x = distance)) + geom_histogram(bins = 8)\n\n\n\n\nThat big outlier is the very inaccurate male driver.\nNow, how do we get a separate histogram for each gender? In ggplot, separate plots for each “something” are called facets, and the way to get facets arranged as you want them is called facet_grid.6 Let me show you the code first, and then explain how it works:\n\nggplot(parking, aes(x = distance)) +\n  geom_histogram(bins = 7) +\n  facet_grid(gender ~ .)\n\n\n\n\nfacet_grid takes a “model formula” with a squiggle, with \\(y\\) on the left and \\(x\\) on the right. We want to compare our two histograms, one for males and one for females, and I think the best way to compare histograms is to have one on top of the other. Note that the same distance scale is used for both histograms, so that it is a fair comparison. The above-and-below is accomplished by having gender as the \\(y\\) in the arrangement of the facets, so it goes before the squiggle. We don’t have any \\(x\\) in the arrangement of the facets, and we tell ggplot this by putting a dot where the \\(x\\) would be.7\nYou can also use facet_wrap for this, but you have to be more careful since you don’t have any control over how the histograms come out (you probably get them side by side, which is not so helpful for comparing distributions). You can make it work by using ncol=1 to arrange all the histograms in one column:\n\nggplot(parking, aes(x = distance)) +\n  geom_histogram(bins = 7) +\n  facet_wrap(~gender, ncol = 1)\n\n\n\n\nThe centres of both histograms are somewhere around 10, so it’s hard to see any real difference between males and females here. Maybe this is further evidence that the small difference we saw between the boxplots is really not worth getting excited about.\nYou might be concerned about how you know what to put with the squiggle-thing in facet_grid and facet_wrap. The answer is that facet_wrap only has something to the right of the squiggle (which ggplot then decides how to arrange), but facet_grid must have something on both sides of the squiggle (how to arrange in the \\(y\\) direction on the left, how to arrange in the \\(x\\) direction on the right), and if you don’t have anything else to put there, you put a dot. Here’s my facet_grid code from above, again:\n\nggplot(parking, aes(x = distance)) +\n  geom_histogram(bins = 7) +\n  facet_grid(gender ~ .)\n\nWe wanted gender to go up and down, and we had nothing to go left and right, hence the dot. Contrast that with my facet_wrap code:8\n\nggplot(parking, aes(x = distance)) +\n  geom_histogram(bins = 7) +\n  facet_wrap(~gender)\n\nThis says “make a separate facet for each gender”, but it doesn’t say anything about how to arrange them. The choice of bins for my histogram(s) came from Sturges’ rule: with \\(n\\) being the number of observations, you use \\(k\\) bins where \\(k=\\log_2(n)+1\\), rounded up. If we were to make a histogram of all the parking distances combined together, we would have \\(n=47+48=95\\) observations, so we should use this many bins:\n\nsturges &lt;- log(95, 2) + 1\nsturges\n\n[1] 7.569856\n\n\nRound this up to 8. (The second thing in log is the base of the logs, if you specify it, otherwise it defaults to \\(e\\) and gives you “natural” logs.) I seem to have the powers of 2 in my head, so I can do it mentally by saying “the next power of 2 is 128, which is \\(2^7\\), so I need \\(7+1=8\\) bins.”\nOr:\n\nwith(parking, nclass.Sturges(distance))\n\n[1] 8\n\n\nSturges’ rule tends to produce not enough bins if \\(n\\) is small, so be prepared to increase it a bit if you don’t have much data. I think that gives a fairly bare-bones picture of the shape: skewed to the right with outlier.\nThe other rule we saw was Freedman-Diaconis:\n\nwith(parking, nclass.FD(distance))\n\n[1] 14\n\n\nand that leads to this histogram:\n\nggplot(parking, aes(x = distance)) + geom_histogram(bins = 14)\n\n\n\n\nThat gives rather more detail (a lot more bars: the binwidth in the Sturges-rule histogram is about 7, or twice what you see here), but in this case the overall story is about the same.\nIn the case of faceted histograms, you would want to apply a rule that uses the number of observations in each histogram. The facets might have quite different numbers of observations, but you can only use one binwidth (or bins), so you may have to compromise. For example, using Sturges’ rule based on 47 observations (the number of males; the number of females is one more):\n\nlog(47, 2) + 1\n\n[1] 6.554589\n\n\nand so each facet should have that many bins, rounded up. That’s where I got my 7 for the facetted histogram from. This one doesn’t work immediately with nclass.Sturges, because we do not have one column whose length is the number of observations we want: we have one column of distances that are males and females mixed up. To do that, filter one of the genders first:\n\nparking %&gt;%\n  filter(gender == \"female\") %&gt;%\n  with(., nclass.Sturges(distance))\n\n[1] 7\n\n\nI used the “dot” trick again, which you can read as “it”: “from parking, take only the rows for the females, and with it, give me the number of bins for a histogram by Sturges’ rule.”\n\\(\\blacksquare\\)\n\nExplain briefly why this is two independent samples rather than matched pairs.\n\nSolution\nThere is no way to pair any male with a corresponding female, because they are unrelated people. You might also notice that there are not even the same number of males and females, so there can be no way of pairing them up without leaving one over. (In general, if the two samples are paired, there must be the same number of observations in each; if there are different numbers in each, as here, they cannot be paired.) If you want that more mathematically, let \\(n_1\\) and \\(n_2\\) be the two sample sizes; then:\n\\[\n\\mbox{Paired} \\Longrightarrow n_1=n_2\n\\]\nfrom which it follows logically (the “contrapositive”) that\n\\[\nn_1 \\ne n_2 \\Longrightarrow \\mbox{not paired}\n\\]\nYou’ll note from the logic that if the two sample sizes are the same, that tells you nothing about whether it’s paired or independent samples: it could be either, and in that case you have to look at the description of the data to decide between them.\nHere, anything that gets at why the males and females cannot be paired up is good.\n\\(\\blacksquare\\)\n\nRun a suitable \\(t\\)-test for comparing parking distances for males and females. What do you conclude, in the context of the data?\n\nSolution\nA two-sample \\(t\\)-test. I think either the Welch or the pooled one can be justified (and I would expect them to give similar answers). You can do the Welch one either without comment or by asserting that the boxplots show different spreads; if you are going to do the pooled one, you need to say that the spreads are “about equal”, by comparing the heights of the boxes on the boxplots:\n\nt.test(distance ~ gender, data = parking)\n\n\n    Welch Two Sample t-test\n\ndata:  distance by gender\nt = -1.3238, df = 79.446, p-value = 0.1894\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -4.5884103  0.9228228\nsample estimates:\nmean in group female   mean in group male \n            9.308511            11.141304 \n\n\nThis is the Welch-Satterthwaite version of the test, the one that does not assume equal SDs in the two groups. The P-value of 0.1894 is not small, so there is no evidence of any difference in parking accuracy between males and females.\nOr, this being the pooled one:\n\nt.test(distance ~ gender, data = parking, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  distance by gender\nt = -1.329, df = 91, p-value = 0.1872\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -4.5722381  0.9066506\nsample estimates:\nmean in group female   mean in group male \n            9.308511            11.141304 \n\n\nYou might have thought, looking at the boxplots, that the groups had about the same SD (based, for example, on noting that the two boxes were about the same height, so the IQRs were about the same). In that case, you might run a pooled \\(t\\)-test, which here gives an almost identical P-value of 0.1872, and the exact same conclusion.\n\\(\\blacksquare\\)\n\nWhy might you have some doubts about the \\(t\\)-test that you just did? Explain briefly.\n\nSolution\nThe two-sample \\(t\\)-test is based on an assumption of normally-distributed data within each group. If you go back and look at the boxplots, you’ll see either (depending on your point of view) that both groups are right-skewed, or that both groups have outliers, neither of which fits a normal distribution. The outlier in the male group is particularly egregious.9 So I think we are entitled to question whether a two-sample \\(t\\)-test is the right thing to do. Having said that, we should go back and remember that the \\(t\\)-tests are “robust to departures from normality” (since we are working with the Central Limit Theorem here), and therefore that this test might be quite good even though the data are not normal, because the sample sizes of 40-plus are large (by the standards of what typically makes the Central Limit Theorem work for us). So it may not be as bad as it seems. A common competitor for the two-sample \\(t\\)-test is the Mann-Whitney test. This doesn’t assume normality, but it does assume symmetric distributions, which it’s not clear that we have here. I like a test called Mood’s Median Test, which is kind of the two-sample equivalent of the sign test (which we will also see later). It goes like this: Work out the overall median of all the distances, regardless of gender:\n\nparking %&gt;% summarize(med = median(distance))\n\n\n\n  \n\n\n\nThe overall median is 9.\nCount up how many distances of each gender were above or below the overall median. (Strictly, I’m supposed to throw away any values that are exactly equal to the overall median, but I won’t here for clarity of exposition.)\n\ntab &lt;- with(parking, table(gender, distance &lt; 9))\ntab\n\n        \ngender   FALSE TRUE\n  female    23   24\n  male      27   19\n\n\nFor example, 19 of the male drivers had a distance (strictly) less than 9. Both genders are pretty close to 50–50 above and below the overall median, which suggests that the males and females have about the same median. This can be tested (it’s a chi-squared test for independence, if you know that):\n\nchisq.test(tab, correct = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  tab\nX-squared = 0.89075, df = 1, p-value = 0.3453\n\n\nThis is even less significant (P-value 0.3453) than the two-sample \\(t\\)-test, and so is consistent with our conclusion from before that there is actually no difference between males and females in terms of average parking distance. The Mood’s median test is believable because it is not affected by outliers or distribution shape.\n\\(\\blacksquare\\)\n\nThe Toronto Star in its report said that females are more accurate at parking their cars. Why do you think they concluded that, and do you think they were right to do so? Explain briefly.\n\nSolution\nThe conclusion from the boxplots was that the female median distance was less than the males, slightly, in this sample. That is probably what the Star seized on. Were they right? Well, that was why we did the test of significance. We were trying to see whether this observed difference between males and females was “real” (would hold up if you looked at “all” male and female drivers) or “reproducible” (you would expect to see it again if you did another study like this one). The large, non-significant P-values in all our tests tell us that the difference observed here was nothing more than chance. So it was not reasonable to conclude that females generally are more accurate at parallel-parking than males are.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#bell-peppers-and-too-much-water-1",
    "href": "two-sample-inference.html#bell-peppers-and-too-much-water-1",
    "title": "5  Two-sample inference",
    "section": "5.10 Bell peppers and too much water",
    "text": "5.10 Bell peppers and too much water\nA pathogen called Phytophthora capsici causes bell peppers to wilt and die. It is thought that too much water aids in the spread of the pathogen. Two fields are under study, labelled a and b. The first step in the research project is to compare the mean soil water content of the two fields. There is a suspicion that field a will have a higher water content than field b. The data are in the file link.\n\nRead the file in using read_csv, and list the resulting data frame.\n\nSolution\nReading directly from the URL is easiest:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/bellpepper.csv\"\npepper &lt;- read_csv(my_url)\n\nRows: 30 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): field\ndbl (1): water\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npepper\n\n\n\n  \n\n\n\nIf you like, find out how many observations you have from each field, thus:\n\npepper %&gt;% count(field)\n\n\n\n  \n\n\n\nFourteen and sixteen.\n\\(\\blacksquare\\)\n\nMake side-by-side boxplots of the water content values for the two fields. How do the fields seem to compare?\n\nSolution\nThis kind of thing:\n\nggplot(pepper, aes(x = field, y = water)) + geom_boxplot()\n\n\n\n\nThis one is rather interesting: the distribution of water contents for field a is generally higher than that for field b, but the median for a is actually lower.\nThe other reasonable plot is a facetted histogram, something like this:\n\nggplot(pepper, aes(x = water)) + geom_histogram(bins = 6) +\n  facet_grid(field ~ .)\n\n\n\n\nThe distribution of water content in field b is actually bimodal, which is probably the explanation of the funny thing with the median. What actually seems to be happening (at least for these data) is that the water content in field B is either about the same as field A, or a lot less (nothing in between). I can borrow an idea from earlier to find the five-number summaries for each field:\n\npepper %&gt;%\n  nest(-field) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(enframe(quantile(data$water))))%&gt;%\n  unnest(qq) %&gt;%\n  select(-data) %&gt;% \n  pivot_wider(names_from=name, values_from=value)\n\nWarning: Supplying `...` without names was deprecated in tidyr 1.0.0.\nℹ Please specify a name for each selection.\nℹ Did you want `data = -field`?\n\n\n\n\n  \n\n\n\nThis is a weird one: all the quantiles are greater for field A except for the median.\n\\(\\blacksquare\\)\n\nDo a two-sample \\(t\\)-test to test whether there is evidence that the mean water content in field a is higher than that of field b. What do you conclude? Explain briefly. (You’ll need to figure out a way of doing a one-sided test, or how to adapt the results from a two-sided test.)\n\nSolution\n\nt.test(water ~ field, alternative = \"greater\", data = pepper)\n\n\n    Welch Two Sample t-test\n\ndata:  water by field\nt = 2.0059, df = 27.495, p-value = 0.0274\nalternative hypothesis: true difference in means between group a and group b is greater than 0\n95 percent confidence interval:\n 0.2664399       Inf\nsample estimates:\nmean in group a mean in group b \n       12.52857        10.76875 \n\n\nNote the use of alternative to specify that the first group mean (that of field a) is bigger than the second, field b, under the alternative hypothesis.\nThe P-value, 0.0274, is less than 0.05, so we reject the null (equal means) in favour of the a mean being bigger than the b mean: field a really does have a higher mean water content.\nAnother way to tackle this is to do a two-sided test and adapt the P-value:\n\nt.test(water ~ field, data = pepper)\n\n\n    Welch Two Sample t-test\n\ndata:  water by field\nt = 2.0059, df = 27.495, p-value = 0.0548\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.03878411  3.55842696\nsample estimates:\nmean in group a mean in group b \n       12.52857        10.76875 \n\n\nThis time we do not go straight to the P-value. First we check that we are on the correct side, which we are since the sample mean for field a is bigger than for field b. Then we are entitled to take the two-sided P-value 0.0548 and halve it to get the same 0.0274 that we did before.\n\\(\\blacksquare\\)\n\nIs the result of your test consistent with the boxplot, or not? Explain briefly.\n\nSolution\nThe test said that field a had a greater mean water content. Looking at the boxplot, this is consistent with where the boxes sit (a’s box is higher up than b’s). However, it is not consistent with the medians, where b’s median is actually bigger. You have two possible right answers here: comparing the boxes with the test result (they agree) or comparing the medians with the test result (they disagree). Either is good. If you like, you could also take the angle that the two boxes overlap a fair bit, so it is surprising that the test came out significant. (The resolution of this one is that we have 30 measurements altogether, 14 and 16 in the two groups, so the sample size is not tiny. With smaller samples, having overlapping boxes would probably lead to a non-significant difference.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#exercise-and-anxiety-and-bullying-mice-1",
    "href": "two-sample-inference.html#exercise-and-anxiety-and-bullying-mice-1",
    "title": "5  Two-sample inference",
    "section": "5.11 Exercise and anxiety and bullying mice",
    "text": "5.11 Exercise and anxiety and bullying mice\nDoes exercise help to reduce anxiety? To assess this, some researchers randomly assigned mice to either an enriched environment where there was an exercise wheel available, or a standard environment with no exercise options. After three weeks in the specified environment, for five minutes a day for two weeks, the mice were each exposed to a “mouse bully” — a mouse who was very strong, aggressive, and territorial. One measure of mouse anxiety is amount of time hiding in a dark compartment, with mice who are more anxious spending more time in darkness. The amount of time spent in darkness is recorded for each of the mice.\nThe data can be found at link.\n\nRead the data into R, and display your data frame. Count the number of mice in each group.\n\nSolution\nThese are aligned columns with spaces in between, so we need read_table:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/stressedmice.txt\"\nmice &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Time = col_double(),\n  Environment = col_character()\n)\n\nmice\n\n\n\n  \n\n\n\nYou can call the data frame whatever you like.\nIf you must, you can physically count the number of mice in each group, but you ought to get in the habit of coding this kind of thing:\n\nmice %&gt;% count(Environment)\n\n\n\n  \n\n\n\nSeven in each.\n\\(\\blacksquare\\)\n\nDraw side-by-side boxplots of time spent in darkness for each group of mice.\n\nSolution\nThis:\n\nggplot(mice, aes(x = Environment, y = Time)) + geom_boxplot()\n\n\n\n\nYou did remember to put capital letters on the variable names, didn’t you?\n\\(\\blacksquare\\)\n\nDo the boxplots support the hypothesis about exercise and anxiety? Explain briefly.\n\nSolution\nThe hypothesis about exercise and anxiety is that mice who exercise more should be less anxious. How does that play out in this study? Well, mice in the enriched environment at least have the opportunity to exercise, which the mice in the standard environment do not, and anxiety is measured by the amount of time spent in darkness (more equals more anxious). So we’d expect the mice in the standard environment to spend more time in darkness, if that hypothesis is correct. That’s exactly what the boxplots show, with very little doubt.10 Your answer needs to make two points: (i) what you would expect to see, if the hypothesis about anxiety and exercise is true, and (ii) whether you actually did see it. You can do this either way around: for example, you can say what you see in the boxplot, and then make the case that this does support the idea of more exercise corresponding with less anxiety.\n\\(\\blacksquare\\)\n\nCarry out a \\(t\\)-test for comparing the mean time spent in darkness for the mice in the two groups. Think carefully about the details of the \\(t\\)-test (and what you need evidence in favour of).\n\nSolution\nWe are trying to prove that exercise goes with less anxiety, so a one-sided test is called for. The other thing to think about is how R organizes the groups for Environment: in alphabetical order. Thus Enriched is first (like on the boxplot). We’re trying to prove that the mean Time is less for Enriched than for Standard, so we need alternative=\"less\":\n\nwith(mice, t.test(Time ~ Environment, alternative = \"less\"))\n\n\n    Welch Two Sample t-test\n\ndata:  Time by Environment\nt = -6.7966, df = 9.1146, p-value = 3.734e-05\nalternative hypothesis: true difference in means between group Enriched and group Standard is less than 0\n95 percent confidence interval:\n      -Inf -151.2498\nsample estimates:\nmean in group Enriched mean in group Standard \n              231.7143               438.7143 \n\n\nA common clue that you have the wrong alternative hypothesis is a P-value coming out close to 1, which is what you would have gotten from something like this:\n\nwith(mice, t.test(Time ~ Environment, alternative = \"greater\"))\n\n\n    Welch Two Sample t-test\n\ndata:  Time by Environment\nt = -6.7966, df = 9.1146, p-value = 1\nalternative hypothesis: true difference in means between group Enriched and group Standard is greater than 0\n95 percent confidence interval:\n -262.7502       Inf\nsample estimates:\nmean in group Enriched mean in group Standard \n              231.7143               438.7143 \n\n\nHere, we looked at the pictures and expected to find a difference, so we expected to find a P-value close to 0 rather than close to 1.\n\\(\\blacksquare\\)\n\nWhat do you conclude, in terms of anxiety and exercise (at least for mice)? Explain briefly.\n\nSolution\nThe P-value (from the previous part) is 0.000037, which is way less than 0.05 (or 0.01 or whatever \\(\\alpha\\) you chose). So the null hypothesis (equal means) is resoundingly rejected in favour of the one-sided alternative that the mean anxiety (as measured by time spent in darkness) is less for the mice who (can) exercise. You need to end up by doing a one-sided test. An alternative to what I did is to do a two-sided test in the previous part. Then you can fix it up by recognizing that the means are the right way around for the research hypothesis (the mean time in darkness is way less for Enriched), and then dividing the two-sided P-value by 2. But you need to do the “correct side” thing: just halving the two-sided P-value is not enough, because the sample mean for Enriched might have been more than for Standard.\n\\(\\blacksquare\\)\n\nDoes anything in the previous parts suggest any problems with the analysis you just did? Explain briefly.\n\nSolution\nLook at the side-by-side boxplots. The strict assumptions hiding behind the \\(t\\)-tests are that the data in each group come from normal distributions (equal standard deviations are not required). Are the data symmetric? Are there any outliers? Well, I see a high outlier in the Enriched group, so I have some doubts about the normality. On the other hand, I only have seven observations in each group, so there is no guarantee even if the populations from which they come are normal that the samples will be. So maybe things are not so bad. This is one of those situations where you make a case and defend it. I don’t mind so much which case you make, as long as you can defend it. Thus, something like either of these two is good:\n\nI see an outlier in the Enriched group. The data within each group are supposed to be normally distributed, and the Enriched group is not. So I see a problem.\nI see an outlier in the Enriched group. But the sample sizes are small, and an apparent outlier could arise by chance. So I do not see a problem.\n\nExtra: another way to think about this is normal quantile plots to assess normality within each group. This uses the facetting trick to get a separate normal quantile plot for each Environment:\n\nggplot(mice, aes(sample = Time)) + stat_qq() + stat_qq_line() +\n  facet_wrap(~Environment, scales = \"free\")\n\n\n\n\nFor the Enriched group, the upper-end outlier shows up. In a way this plot is no more illuminating than the boxplot, because you still have to make a call about whether this is “too big”. Bear in mind also that these facetted normal quantile plots, with two groups, come out tall and skinny, so vertical deviations from the line are exaggerated. On this plot, the lowest value also looks too low.\nFor the Standard group, there are no problems with normality at all.\nWhat happens if we change the shape of the plots?\n\nggplot(mice, aes(sample = Time)) + stat_qq() + stat_qq_line() +\n  facet_wrap(~Environment, scales = \"free\", ncol = 1)\n\n\n\n\nThis makes the plots come out in one column, that is, short and squat. I prefer these. I’d still call the highest value in Enriched an outlier, but the lowest value now looks pretty close to what you’d expect.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#diet-and-growth-in-boys-1",
    "href": "two-sample-inference.html#diet-and-growth-in-boys-1",
    "title": "5  Two-sample inference",
    "section": "5.12 Diet and growth in boys",
    "text": "5.12 Diet and growth in boys\nA dietician is studying the effect of different diets on children’s growth. In part of the study, the dietician is investigating two religious sects, labelled a and b in our data set. Both sects are vegetarian; the difference between them is that people in Sect A only eat vegetables from below the ground, and Sect B only eats vegetables from above the ground. The height and weight of the boys11 are measured at regular intervals. The data in link are the heights of the boys at age 12.\n\nRead in the data and find out how many observations you have and which variables.\n\nSolution\nThe data values are separated by one space, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/kids-diet.txt\"\ndiet &lt;- read_delim(my_url, \" \")\n\nRows: 21 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): sect\ndbl (1): height\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndiet\n\n\n\n  \n\n\n\n21 observations on two variables, sect and height. (You should state this; it is not enough to make the reader figure it out for themselves.)\nThe heights are evidently in centimetres.\nYou can call the data frame whatever you like.\n\\(\\blacksquare\\)\n\nObtain side-by-side boxplots of the heights for boys from each sect. Does it look as if the heights of the boys in each sect are different? Comment briefly.\n\nSolution\nThe boxplot is the kind of thing we’ve seen before:\n\nggplot(diet, aes(x = sect, y = height)) + geom_boxplot()\n\n\n\n\nIt looks to me as if the boys in Sect B are taller on average.\n\\(\\blacksquare\\)\n\nLooking at your boxplots, do you see any problems with doing a two-sample \\(t\\)-test? Explain briefly.\n\nSolution\nThe assumption is that the data in each group are “approximately normal”. Boxplots don’t tell you about normality specifically, but they tell you whether there are any outliers (none here) and something about the shape (via the lengths of the whiskers). I’d say the Sect A values are as symmetric as we could hope for. For Sect B, you can say either that they’re skewed to the left (and that therefore we have a problem), or that the heights are close enough to symmetric (and that therefore we don’t). For me, either is good. As ever, normal quantile plots can offer more insight. With data in this form, the two samples are mixed up, but using facets is the way to go. Philosophically, we draw a normal quantile plot of all the heights, and then say at the end that we would actually like a separate plot for each sect:\n\ndiet %&gt;%\n  ggplot(aes(sample = height)) + stat_qq() + stat_qq_line() +\n  facet_wrap(~sect, ncol = 1)\n\n\n\n\nI decided that I wanted short squat plots rather than tall skinny ones.\nWith the sizes of the samples, I really don’t see any problems here. Most of the evidence for the left skewness in Sect B is actually coming from that largest value being too small. Sect A is as good as you could wish for. Having extreme values being not extreme enough is not a problem, since it won’t be distorting the mean.\nThe other way of doing this is to use filter to pull out the rows you want and then feed that into the plot:\n\nsecta &lt;- filter(diet, sect == \"a\") %&gt;%\n  ggplot(aes(sample = sect)) + stat_qq() + stat_qq_line()\n\nand the same for sect B. This is the usual ggplot-in-pipeline thing where you don’t have a named data frame in the ggplot because it will use whatever came out of the previous step of the pipeline.\n\\(\\blacksquare\\)\n\nRun a \\(t\\)-test to determine whether the mean heights differ significantly. What do you conclude? Explain briefly. (Run the \\(t\\)-test even if your previous work suggests that it is not the right thing to do.)\n\nSolution\nThe wording states that a two-sided test is correct, which is the default, so you don’t need anything special:\n\nt.test(height ~ sect, data = diet)\n\n\n    Welch Two Sample t-test\n\ndata:  height by sect\nt = -1.7393, df = 14.629, p-value = 0.103\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -12.007505   1.229728\nsample estimates:\nmean in group a mean in group b \n       144.8333        150.2222 \n\n\nThis is a two-sample test, so it takes a data=.\nOur null hypothesis is that the two sects have equal mean height. The P-value of 0.103 is larger than 0.05, so we do not reject that null hypothesis. That is, there is no evidence that the sects differ in mean height. (That is, our earlier thought that the boys in Sect B were taller is explainable by chance.)\nYou must end up with a statement about mean heights, and when you do a test, you must state the conclusion in the context of the problem, whether I ask you to or not. “Don’t reject the null hypothesis” is a step on the way to an answer, not an answer in itself. If you think it’s an answer in itself, you won’t be of much use to the world as a statistician.\nYou might have been thinking that Mood’s median test was the thing, if you were worried about that skewness in Sect B. My guess is that the \\(t\\)-test is all right, so it will be the better test (and give the smaller P-value) here, but if you want to do it, you could do it this way:\n\nlibrary(smmr)\nmedian_test(diet, height, sect)\n\n$table\n     above\ngroup above below\n    a     4     7\n    b     6     3\n\n$test\n       what     value\n1 statistic 1.8181818\n2        df 1.0000000\n3   P-value 0.1775299\n\n\nMy suspicion (that I wrote before doing the test) is correct: there is even less evidence of a difference in median height between the sects. The table shows that both sects are pretty close to 50–50 above and below the overall median, and with sample sizes this small, they are certainly not significantly different from an even split. The small frequencies bring a warning about the chi-squared approximation possibly not working (that smmr suppresses). We had one like this elsewhere, but there the result was very significant, and this one is very non-significant. However, the implication is the same: even if the P-value is not very accurate (because the expected frequencies for sect B are both 4.5), the conclusion is unlikely to be wrong because the P-value is so far from 0.05.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#handspans-of-males-and-females-1",
    "href": "two-sample-inference.html#handspans-of-males-and-females-1",
    "title": "5  Two-sample inference",
    "section": "5.13 Handspans of males and females",
    "text": "5.13 Handspans of males and females\nTake your right hand, and stretch the fingers out as far as you can. The distance between the tip of your thumb and the tip of your little (pinky) finger is your handspan. The students in a Statistics class at Penn State measured their handspans and also whether they identified as male or female. The data are at http://ritsokiguess.site/datafiles/handspan.txt, with handspans measured in inches. Thinking of these as a random sample of all possible students, is it true that males have a larger mean handspan than females? This is what we will explore.\n\nRead in and display (some of) the data.\n\nSolution\nThis is a delimited (by spaces) file, so:\n\nlibrary(tidyverse)\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/handspan.txt\"\nspan &lt;- read_delim(my_url, \" \")\n\nRows: 190 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): sex\ndbl (1): handspan\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspan\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake a suitable graph of the two columns.\n\nSolution\nOne quantitative variable and one categorical one, so a boxplot:\n\nggplot(span, aes(x=sex, y=handspan)) + geom_boxplot()\n\n\n\n\n\\(\\blacksquare\\)\n\nRun a suitable two-sample \\(t\\)-test to address the question of interest. What do you conclude, in the context of the data?\n\nSolution\nWe are trying to show that males have a larger mean handspan, so we need an alternative. To see which: there are two sexes, F and M in that order, and we are trying to show that F is less than M:\n\nt.test(handspan~sex, data=span, alternative=\"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  handspan by sex\nt = -10.871, df = 187.92, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group F and group M is less than 0\n95 percent confidence interval:\n      -Inf -2.154173\nsample estimates:\nmean in group F mean in group M \n       20.01699        22.55747 \n\n\nThe P-value is very small, so there is no doubt that males have larger average handspans than females.\n\\(\\blacksquare\\)\n\nObtain a 90% confidence interval for the difference in mean handspan between males and females. Do you need to run any more code? Explain briefly.\n\nSolution\nA confidence interval is two-sided, so we have to re-run the test without the to make it two-sided. Note also that we need a 90% interval, which is different from the default 95%, so we have to ask for that too:\n\nt.test(handspan~sex, data=span, conf.level=0.90)\n\n\n    Welch Two Sample t-test\n\ndata:  handspan by sex\nt = -10.871, df = 187.92, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n90 percent confidence interval:\n -2.926789 -2.154173\nsample estimates:\nmean in group F mean in group M \n       20.01699        22.55747 \n\n\nThe interval is \\(-2.93\\) to \\(-2.15\\), which you should say. It would be even better to say that males have a mean handspan between 2.15 and 2.93 inches larger than that of females. You also need to round off your answer: the data are given to 0 or 1 decimals, so your interval should be given to 1 or 2 decimals (since the confidence interval is for a mean).\nOn a question like this, the grader is looking for three things:\n\ngetting the output\nsaying what the interval is\nrounding it to a suitable number of decimals.\n\nThus, getting the output alone is only one out of three things.\n\\(\\blacksquare\\)\n\nExplain briefly why you might have some concerns about the validity of the \\(t\\)-tests you ran in this question. Or, if you don’t have any concerns, explain briefly why that is.\n\nSolution\nThe major assumption here is that the male and female handspans have (approximate) normal distributions. The boxplots we drew earlier both had low-end outliers, so the normality is questionable.\nAlso, say something about the sample sizes and whether or not you think they are large enough to be helpful.\nHow big are our sample sizes?\n\nspan %&gt;% count(sex)\n\n\n\n  \n\n\n\nMy suspicion is that we are saved by two things: the sample sizes are large enough for the central limit theorem to help us, and in any case, the conclusion is so clear that the assumptions can afford to be off by a bit.\nExtra: one way to think about whether we should be concerned about the lack of normality is to use the bootstrap to see what the sampling distribution of the sample mean might look like for males and for females. (This is the stuff in Lecture 5a.) The way this works is to sample from each distribution with replacement, work out the mean of each sample, then repeat many times, once for the females and once for the males.\nTo start with the females, the first thing to do is to grab only the rows containing the females. This, using an idea from Lecture 5a that we see again properly later, is filter:\n\nspan %&gt;% filter(sex==\"F\") -&gt; females\nfemales\n\n\n\n  \n\n\n\nThere are 103 females. From these we need to take a “large” number of bootstrap samples to get a sense of how the mean handspan of the females varies:\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(the_sample = list(sample(females$handspan, replace = TRUE))) %&gt;% \n  mutate(the_mean = mean(the_sample)) -&gt; d\n\nThen we make a histogram of the bootstrap sampling distribution of the sample mean for the females:\n\nggplot(d, aes(x = the_mean)) + geom_histogram(bins = 10)\n\n\n\n\nI don’t know what you think of this. There are a few more extreme values than I would like, and it looks otherwise a bit left-skewed to me. But maybe I am worrying too much.\nThe males one works exactly the same way:\n\nspan %&gt;% filter(sex==\"M\") -&gt; males\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(the_sample = list(sample(males$handspan, replace = TRUE))) %&gt;% \n  mutate(the_mean = mean(the_sample)) -&gt; d\n\n\nggplot(d, aes(x = the_mean)) + geom_histogram(bins = 10)\n\n\n\n\nThere is a similar story here. I think these are good enough overall, and so I am happy with the two-sample \\(t\\)-test, but it is not as clear-cut as I was expecting.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#the-anchoring-effect-australia-vs-us-1",
    "href": "two-sample-inference.html#the-anchoring-effect-australia-vs-us-1",
    "title": "5  Two-sample inference",
    "section": "5.14 The anchoring effect: Australia vs US",
    "text": "5.14 The anchoring effect: Australia vs US\nTwo groups of students (in a class at a American university) were asked what they thought the population of Canada was. (The correct answer at the time was just over 30 million.) Some of the students, before having to answer this, were told that the population of the United States was about 270 million. The other students in the class were told that the population of Australia was about 18 million. The data are in http://ritsokiguess.site/datafiles/anchoring.csv. The first column contains the country whose population the student was told, and the second contains the student’s guess at the population of Canada.\nYou might wonder how being told the population of an unrelated country would have any impact on a student’s guess at the population of Canada. Psychology says it does: it’s called the anchoring effect, and the idea is that the number mentioned first acts as an “anchor”: a person’s guess will be closer to the anchor than it would have been otherwise. In this case, that would mean that the guesses for the students given the US as an anchor will be higher than for the students given Australia as an anchor. We are interested in seeing whether there is evidence for that here.\n\nRead in and display (some of) the data.\n\nSolution\nI made it as easy as I could:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/anchoring.csv\"\ncanada &lt;- read_csv(my_url)\n\nRows: 21 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): anchor\ndbl (1): estimate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncanada\n\n\n\n  \n\n\n\nYou might need to scroll down to see that both “anchor” countries are indeed represented.\n\\(\\blacksquare\\)\n\nDraw a suitable graph of these data.\n\nSolution\nOne categorical variable and one quantitative one, so a boxplot:\n\nggplot(canada, aes(x = anchor, y = estimate)) + geom_boxplot()\n\n\n\n\n\\(\\blacksquare\\)\n\nExplain briefly why a Welch \\(t\\)-test would be better than a pooled \\(t\\)-test in this case.\n\nSolution\nThe decision between these two tests lies in whether you think the two groups have equal spread (variance, strictly). Here, the spread for the US group is much larger than for the Australia group, even taking into account the big outlier in the latter group. Since the spreads are different, we should do a Welch \\(t\\)-test rather than a pooled one.\nMake sure you answer the question I asked, not the one you think I should have asked.\nThere is a separate question about whether the groups are close enough to normal, but I wasn’t asking about that here. I was asking: given that we have decided to do some kind of \\(t\\)-test, why is the Welch one better than the pooled one? I am not asking whether we should be doing any kind of \\(t\\)-test at all; if I had, you could then reasonably talk about the outlier in the Australia group, and other possible skewness in its distribution, but that’s not what I asked about.\n\\(\\blacksquare\\)\n\nRun a suitable Welch \\(t\\)-test and display the output.\n\nSolution\nThe word “suitable” is a hint that you may have to think a bit about how you run the test. If the anchoring effect is real, the mean of the guesses for the students told the population of the US will be higher on average than for those told the population of Australia, so we want a one-sided alternative. Australia is before the US alphabetically, so the alternative has to be less:\n\nt.test(estimate~anchor, data = canada, alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  estimate by anchor\nt = -3.0261, df = 10.558, p-value = 0.006019\nalternative hypothesis: true difference in means between group australia and group US is less than 0\n95 percent confidence interval:\n      -Inf -26.63839\nsample estimates:\nmean in group australia        mean in group US \n               22.45455                88.35000 \n\n\nNote that the Welch test is the default, so you don’t have to do anything special to get it. Your output will tell you that a Welch test is what you have. It’s if you want a pooled test that you have to ask for it specifically (with var.equal = TRUE).\nIf you get a P-value close to 1, this is often an indication that you have the alternative the wrong way around.\n\\(\\blacksquare\\)\n\nWhat do you conclude from your test, in the context of the data?\n\nSolution\nThe P-value is definitely less than 0.05, so we reject the null hypothesis (which says that the mean guess is the same regardless of the anchor the student was given). So we have evidence that the mean guess is higher for the students who were given the US population first.\nExtra 1: this is perhaps the place to think about what effect that outlier in the australia group might have had. Since it is a high outlier, its effect will be to make the the australia mean higher than it would have been otherwise, and therefore to make the two group means closer together. Despite this, the difference still came out strongly significant, so that we can be even more sure than the P-value says that there is a real difference between the means of estimates of the population of Canada. (To say it differently, if the outlier had not been there, the difference in means would have been even bigger and thus even more significant.)\nExtra 2: if you are still worried about doing a two-sample \\(t\\)-test here, you might consider looking at the bootstrapped sampling distribution of the sample mean of the australia group:\n\ncanada %&gt;% filter(anchor == \"australia\") -&gt; oz\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(the_sample = list(sample(oz$estimate, replace = TRUE))) %&gt;% \n  mutate(the_mean = mean(the_sample)) %&gt;% \n  ggplot(aes(x = the_mean)) + geom_histogram(bins=10)\n\n\n\n\nThis is indeed skewed to the right (though, with 11 observations, not nearly so non-normal as the original data), and so the P-value we got from the \\(t\\)-test may not be reliable. But, as discussed in Extra 1, the “correct” P-value is, if anything, even smaller than the one we got, and so the conclusion we drew earlier (that there is a significant anchoring effect) is not going to change.\nExtra 3: looking even further ahead, there is a test that definitely does apply here, called Mood’s Median Test. You won’t have installed the package yet, so this won’t work for you just yet (read ahead if you want to learn more), but here’s how it goes:\n\nlibrary(smmr)\nmedian_test(canada, estimate, anchor)\n\n$table\n           above\ngroup       above below\n  australia     2     5\n  US            7     1\n\n$test\n       what      value\n1 statistic 5.40178571\n2        df 1.00000000\n3   P-value 0.02011616\n\n\nThis does (as it is written) a two-sided test, because it can also be used for comparing more than two groups. Since we want a one-sided test here, you can (i) check that we are on the correct side (we are)12 (ii) halve the P-value to get 0.010.\nThis is a P-value you can trust. It is not smaller than the \\(t\\)-test one, perhaps because this test is less powerful than the \\(t\\)-test in most cases.13\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#footnotes",
    "href": "two-sample-inference.html#footnotes",
    "title": "5  Two-sample inference",
    "section": "",
    "text": "Mine is rather prosaically called Downloads.↩︎\nThis was not sexism, but a recognition that boys and girls will be of different heights for reasons unrelated to diet. Doing it this way makes the analysis simpler.↩︎\nMine is rather prosaically called Downloads.↩︎\nLet me know if you have more success than I did.↩︎\nA Word or Excel document has all kinds of formatting information hidden in the file as well as the text that you see on the screen.↩︎\nI wrote this question a long time ago, back when I thought that facet_grid was the only way to do facets. Now, I would use facet_wrap. See the discussion about facet_wrap near the bottom.↩︎\nYou might have a second categorical variable by which you want to arrange the facets left and right, and that would go where the dot is.↩︎\nI took out the ncol since that confuses the explanation here.↩︎\nGoogle defines this as meaning “outstandingly bad, shocking”.↩︎\nThis means that I would expect to reject a null hypothesis of equal means, but I get ahead of myself.↩︎\nThis was not sexism, but a recognition that boys and girls will be of different heights for reasons unrelated to diet. Doing it this way makes the analysis simpler.↩︎\nThe test works by comparing the data values in each group to the overall median. The students who were given Australia as an anchor mostly guessed below the overall median, and the students given the US as an anchor mostly guessed above.↩︎\nIt uses the data less efficiently than the t-test; it just counts the number of values above and below the overall median in each group, rather than using the actual numbers to compute means.↩︎"
  },
  {
    "objectID": "power.html#simulating-power",
    "href": "power.html#simulating-power",
    "title": "6  Power and sample size",
    "section": "6.1 Simulating power",
    "text": "6.1 Simulating power\nThis question investigates power by simulation.\n\nUse rnorm to generate 10 random values from a normal distribution with mean 20 and SD 2. Do your values look reasonable? Explain briefly. (You don’t need to draw a graph.)\nEstimate by simulation the power of a \\(t\\)-test to reject a null hypothesis of 20 when the true mean is also 20, the population SD is 2, and the sample size is 10, against a (default) two-sided alternative. Remember the steps: (i) generate a lot of random samples from the true distribution, (ii) run the \\(t\\)-test with the required null mean, (iii) pull out the P-values, (iv) count how many of them are 0.05 or less.\nIn the simulation you just did, was the null hypothesis true or false? Do you want to reject the null hypothesis or not? Explain briefly why the simulation results you got were (or were not) about what you would expect.\nBy copying, pasting and editing your code from the previous part, estimate the power of the test of \\(H_0: \\mu=20\\) (against a two-sided alternative) when the true population mean is 22 (rather than 20).\nUse R to calculate this power exactly (without simulation). Compare the exact result with your simulation."
  },
  {
    "objectID": "power.html#calculating-power-and-sample-size-for-estimating-mean",
    "href": "power.html#calculating-power-and-sample-size-for-estimating-mean",
    "title": "6  Power and sample size",
    "section": "6.2 Calculating power and sample size for estimating mean",
    "text": "6.2 Calculating power and sample size for estimating mean\nWe are planning a study to estimate a population mean. The population standard deviation is believed to be 20, and the population distribution is believed to be approximately normal. We will be testing the null hypothesis that the population mean is 100. Suppose the population mean is actually 110, and we want to determine how likely we are to (correctly) reject the null hypothesis in this case, using a two-sided (but one-sample) test with \\(\\alpha=0.05\\).\n\nWe will take a sample of size \\(n=30\\). Calculate the power of this test.\nFind the sample size necessary to obtain a power of at least 0.80 under these conditions. What sample size do you need? Explain briefly how your answer is consistent with (a)."
  },
  {
    "objectID": "power.html#simulating-power-for-proportions",
    "href": "power.html#simulating-power-for-proportions",
    "title": "6  Power and sample size",
    "section": "6.3 Simulating power for proportions",
    "text": "6.3 Simulating power for proportions\nIn opinion surveys (and other places), we are testing for a proportion \\(p\\) (for example, the proportion of people agreeing with some statement). Often, we want to know whether the proportion is “really” greater than 0.5.1\nThat would entail testing a null \\(H_0: p=0.5\\) against an alternative \\(H_a: p&gt;0.5\\). This is usually done by calculating the test statistic \\[ z = { \\hat{p} - 0.5 \\over \\sqrt{0.25/n}},\\] where \\(\\hat{p}\\) is the observed proportion in the sample, and getting a P-value from the upper tail of a standard normal distribution. (The 0.25 is \\(p(1-p)\\) where \\(p=0.5\\).) This is what prop.test does, as we investigate shortly.\n\nUse rbinom to generate a random value from a binomial distribution with \\(n=100\\) and \\(p=0.6\\). There are three inputs to rbinom: the first one should be the number 1, and the second and third are the \\(n\\) and \\(p\\) of the binomial distribution.\nUsing the random binomial that you generated just above, use prop.test to test whether it could reasonably have come from a binomial population with \\(n=100\\) and \\(p=0.5\\), or whether \\(p\\) is actually bigger than 0.5. (Of course, you know it actually did not come from a population with \\(p=0.5\\).) prop.test has, for us, four inputs, thus:\n\n\nthe observed number of successes\nthe n of the binomial distribution\nthe null-hypothesis p of the binomial distribution\nthe alternative hypothesis, here “greater”\n\n\nRun prop.test again, just as you did before, but this time save the result, and extract the piece of it called p.value. Is that the P-value from your test?\nEstimate the power of a test of \\(H_0: p=0.5\\) against \\(H_a: p&gt;0.5\\) when \\(n=500\\) and \\(p=0.56\\), using \\(\\alpha=0.05\\). There are three steps:\n\n\ngenerate random samples from binomial distributions with \\(n=500\\) and \\(p=0.56\\), repeated “many” times (something like 1000 or 10,000 is good)\nrun prop.test on each of those random samples\nextract the P-value for each test and save the results (in a column called, perhaps, pvals).\n\nSo I lied: the fourth and final step is to count how many of those P-values are 0.05 or less."
  },
  {
    "objectID": "power.html#designing-a-study-to-have-enough-power",
    "href": "power.html#designing-a-study-to-have-enough-power",
    "title": "6  Power and sample size",
    "section": "6.4 Designing a study to have enough power",
    "text": "6.4 Designing a study to have enough power\nYou are designing a study to test the null hypothesis that a population mean is 0 against the alternative hypothesis that it is greater than 0. Assume that the population SD is \\(\\sigma=15\\). It is important to detect the alternative \\(\\mu=2\\); that is, we want to design the study so that most of the time the null hypothesis would be (correctly) rejected if in fact \\(\\mu=2\\). A one-sample \\(t\\)-test will be used, and the data values are assumed to have a normal distribution.\n\nUse simulation to estimate the power of this test when the sample size is 100. Use \\(\\alpha=0.05\\).\nAgain by using simulation, estimate how large a sample size would be needed to obtain a power of 0.80. Show and briefly explain your process."
  },
  {
    "objectID": "power.html#power-and-alpha-in-a-skewed-population",
    "href": "power.html#power-and-alpha-in-a-skewed-population",
    "title": "6  Power and sample size",
    "section": "6.5 Power and \\(\\alpha\\) in a skewed population",
    "text": "6.5 Power and \\(\\alpha\\) in a skewed population\nA population of a large number of values v is at http://ritsokiguess.site/datafiles/pop.csv, in a CSV file.\n\nRead in the population and display some of the values.\nObtain a suitable plot of your population. What do you notice?\nIf you take a sample of 10 observations from this population and run a \\(t\\)-test, how likely are you to (correctly) reject the null hypothesis \\(H_0: \\mu = 4\\), against the alternative \\(H_a: \\mu &gt; 4\\)? Investigate by simulation.\nTry again with a sample size of 50 (leaving everything else the same). Explain briefly why the results so far are as you’d expect.\nAgain by simulation, estimate the probability that the null hypothesis \\(H_0: \\mu=5\\) will be rejected when a sample of size 10 is taken from this population, in favour of the alternative \\(H_a: \\mu &gt; 5\\). Explain briefly why the answer is not what you would have expected, and why that happened here. (Hint: what is the population mean?)\n\nMy solutions follow:"
  },
  {
    "objectID": "power.html#simulating-power-1",
    "href": "power.html#simulating-power-1",
    "title": "6  Power and sample size",
    "section": "6.6 Simulating power",
    "text": "6.6 Simulating power\nThis question investigates power by simulation.\n\nUse rnorm to generate 10 random values from a normal distribution with mean 20 and SD 2. Do your values look reasonable? Explain briefly. (You don’t need to draw a graph.)\n\nSolution\nrnorm with the number of values first, then the mean, then the SD:\n\nx=rnorm(10,20,2)\nx\n\n [1] 21.59476 18.64044 21.83231 18.76556 18.64861 21.81889 21.62614 20.18249\n [9] 16.91266 20.63490\n\n\n95% of the sampled values should be within 2 SDs of the mean, that is, between 16 and 24 (or 99.7% should be within 3 SDs of the mean, between 14 and 26). None of my values are even outside the interval 16 to 24, though yours may be different.\nI saved mine in a variable and then displayed them, which you don’t need to do. I did because there’s another way of assessing them for reasonableness: turn the sample into \\(z\\)-scores and see whether the values you get look like \\(z\\)-scores (that is, most of them are between \\(-2\\) and 2, for example):\n\n(x-20)/2\n\n [1]  0.79738130 -0.67977910  0.91615386 -0.61722168 -0.67569291  0.90944266\n [7]  0.81307163  0.09124563 -1.54367207  0.31744905\n\n\nThese ones look very much like \\(z\\)-scores. This, if you think about it, is really the flip-side of 68–95–99.7, so it’s another way of implementing the same idea.\nYou might also think of finding the sample mean and SD, and demonstrating that they are close to the right answers. Mine are:\n\nmean(x)\n\n[1] 20.06568\n\nsd(x)\n\n[1] 1.731305\n\n\nThe sample SD is more variable than the sample mean, so it can get further away from the population SD than the sample mean does from the population mean.\nThe downside to this idea is that it doesn’t get at assessing the normality, which looking at \\(z\\)-scores or equivalent does. Maybe coupling the above with a boxplot would have helped, had I not said “no graphs”, since then you’d (hopefully) see no outliers and a roughly symmetric shape.\nThis is old-fashioned “base R” technology; you could do it with a data frame like this:\n\nd &lt;- tibble(x=rnorm(10,20,2))\nd\n\n\n\n  \n\n\nd %&gt;% summarize(m=mean(x), s=sd(x))\n\n\n\n  \n\n\n\nThese are different random numbers, but are about equally what you’d expect. (These ones are a bit less variable than you’d expect, but with only ten values, don’t expect perfection.)\nSome discussion about the kind of values you should get, and whether or not you get them, is what is called for here. I want you to say something convincing about how the values you get come from a normal distribution with mean 20 and SD 2. “Close to 20” is not the whole answer here, because that doesn’t get at “how close to 20?”: that is, it talks about the mean but not about the SD.\n\\(\\blacksquare\\)\n\nEstimate by simulation the power of a \\(t\\)-test to reject a null hypothesis of 20 when the true mean is also 20, the population SD is 2, and the sample size is 10, against a (default) two-sided alternative. Remember the steps: (i) generate a lot of random samples from the true distribution, (ii) run the \\(t\\)-test with the required null mean, (iii) pull out the P-values, (iv) count how many of them are 0.05 or less.\n\nSolution\nOnce you get the hang of these, they all look almost the same. This one is easier than some because we don’t have to do anything special to get a two-sided alternative hypothesis. The initial setup is to make a dataframe with a column called something like sim to label the simulations, and then a rowwise to generate one random sample, \\(t\\)-test and P-value for each simulation:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %&gt;% \n  mutate(t_test = list(t.test(norm_sample, mu = 20))) %&gt;% \n  mutate(pval = t_test$p.value) %&gt;% \n  count(pval &lt;= 0.05)\n\n\n\n  \n\n\n\nThe power is about 4.2%. This seems depressingly small, but see the next part. (Are you confused about something in this one? You have a right to be.)\n\\(\\blacksquare\\)\n\nIn the simulation you just did, was the null hypothesis true or false? Do you want to reject the null hypothesis or not? Explain briefly why the simulation results you got were (or were not) about what you would expect.\n\nSolution\nThe null mean and the true mean were both 20: that is, the null hypothesis was correct, and rejecting it would be a mistake, to be precise a type I error. We were doing the test at \\(\\alpha=0.05\\) (by comparing our collection of simulated P-values with 0.05), so we should be making a type I error 5% of the time. This is entirely in line with the 4.2% of (wrong) rejections that I had. Your estimation is likely to be different from mine, but you should be rejecting about 5% of the time. If your result is very different from 5%, that’s an invitation to go back and check your code. On the other hand, if it is about 5%, that ought to give you confidence to go on and use the same ideas for the next part.\n\\(\\blacksquare\\)\n\nBy copying, pasting and editing your code from the previous part, estimate the power of the test of \\(H_0: \\mu=20\\) (against a two-sided alternative) when the true population mean is 22 (rather than 20).\n\nSolution\nHere’s the code we just used:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %&gt;% \n  mutate(t_test = list(t.test(norm_sample, mu = 20))) %&gt;% \n  mutate(pval = t_test$p.value) %&gt;% \n  count(pval &lt;= 0.05)\n\nOne of those 20s needs to become 22. Not the one in the t.test, since the hypotheses have not changed. So we need to change the 20 in the rnorm line to 22, since that’s where we’re generating data from the true distribution. The rest of it stays the same:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %&gt;% \n  mutate(t_test = list(t.test(norm_sample, mu = 22))) %&gt;% \n  mutate(pval = t_test$p.value) %&gt;% \n  count(pval &lt;= 0.05)\n\n\n\n  \n\n\n\nThis time, we want to reject, since the null hypothesis is false. So look at the TRUE count: the power is about \\(80\\%\\). We are very likely to correctly reject a null of 20 when the mean is actually 22.\nExtra: another way to reason that the power should be fairly large is to think about what kind of sample you are likely to get from the true distribution: one with a mean around 22 and an SD around 2. Thus the \\(t\\)-statistic should be somewhere around this (we have a sample size of 10):\n\nt_stat=(22-20)/(2/sqrt(10))\nt_stat\n\n[1] 3.162278\n\n\nand the two-sided P-value should be about\n\n2*(1-pt(t_stat,10-1))\n\n[1] 0.01150799\n\n\nOf course, with your actual data, you will sometimes be less lucky than this (a sample mean nearer 20 or a larger sample SD), but sometimes you will be luckier. But the suggestion is that most of the time, the P-value will be pretty small and you will end up correctly rejecting.\nThe quantity t_stat above, 3.16, is known to some people as an “effect size”, and summarizes how far apart the null and true means are, relative to the amount of variability present (in the sampling distribution of the sample mean). As effect sizes go, this one is pretty large.\n\\(\\blacksquare\\)\n\nUse R to calculate this power exactly (without simulation). Compare the exact result with your simulation.\n\nSolution\nThis is power.t.test. The quantity delta is the difference between true and null means:\n\npower.t.test(n=10,delta=22-20,sd=2,type=\"one.sample\",alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 10\n          delta = 2\n             sd = 2\n      sig.level = 0.05\n          power = 0.8030962\n    alternative = two.sided\n\n\nThis, 0.803, is very close to the value I got from my simulation. Which makes me think I did them both right. This is not a watertight proof, though: for example, I might have made a mistake and gotten lucky somewhere. But it does at least give me confidence.\nExtra: when you estimate power by simulation, what you are doing is rejecting or not with a certain probability (which is the same for all simulations). So the number of times you actually do reject has a binomial distribution with \\(n\\) equal to the number of simulated P-values you got (1000 in my case; you could do more) and a \\(p\\) that the simulation is trying to estimate. This is inference for a proportion, exactly what prop.test does.\nRecall that prop.test has as input:\n\na number of “successes” (rejections of the null in our case)\nthe number of trials (simulated tests)\nthe null-hypothesis value of p (optional if you only want a CI)\n(optional) a confidence level conf.level.\n\nIn part (b), we knew that the probability of (incorrectly) rejecting should have been 0.05 and we rejected 42 times out of 1000:\n\nprop.test(42,1000,0.05)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  42 out of 1000, null probability 0.05\nX-squared = 1.1842, df = 1, p-value = 0.2765\nalternative hypothesis: true p is not equal to 0.05\n95 percent confidence interval:\n 0.03079269 0.05685194\nsample estimates:\n    p \n0.042 \n\n\nLooking at the P-value, we definitely fail to reject that the probability of (incorrectly) rejecting is the 0.05 that it should be. Ouch. That’s true, but unnecessarily confusing. Look at the confidence interval instead, 0.031 to 0.057. The right answer is 0.05, which is inside that interval, so good.\nIn part (c), we didn’t know what the power was going to be (not until we calculated it with power.t.test, anyway), so we go straight for a confidence interval; the default 95% confidence level is fine. We (correctly) rejected 798 times out of 1000:\n\nprop.test(798,1000)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  798 out of 1000, null probability 0.5\nX-squared = 354.02, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.7714759 0.8221976\nsample estimates:\n    p \n0.798 \n\n\nI left out the 3rd input since we’re not doing a test, and ignore the P-value that comes out. (The default null proportion is 0.5, which often makes sense, but not here.)\nAccording to the confidence interval, the estimated power is between 0.771 and 0.822. This interval definitely includes what we now know is the right answer of 0.803.\nThis might be an accurate enough assessment of the power for you, but if not, you can do more simulations, say 10,000:\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %&gt;% \n  mutate(t_test = list(t.test(norm_sample, mu = 22))) %&gt;% \n  mutate(pval = t_test$p.value) %&gt;% \n  count(pval &lt;= 0.05)\n\n\n\n  \n\n\n\nI copied and pasted my code again, which means that I’m dangerously close to turning it into a function, but anyway.\nThe confidence interval for the power is then\n\nprop.test(7996,10000)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  7996 out of 10000, null probability 0.5\nX-squared = 3589.2, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.7915892 0.8073793\nsample estimates:\n     p \n0.7996 \n\n\nthat is, from 0.792 to 0.807, which once again includes the right answer of 0.803. The first interval, based on 1,000 simulations, has length 0.051, while this interval has length 0.015. The first interval is more than three times as long as the second, which is about what you’d expect since the first one is based on 10 times fewer simulations, and thus ought to be a factor of \\(\\sqrt{10}\\simeq 3.16\\) times longer.\nThis means that you can estimate power as accurately as you like by doing a large enough (possibly very large) number of simulations. Provided, that is, that you are prepared to wait a possibly long time for it to finish working!\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "power.html#calculating-power-and-sample-size-for-estimating-mean-1",
    "href": "power.html#calculating-power-and-sample-size-for-estimating-mean-1",
    "title": "6  Power and sample size",
    "section": "6.7 Calculating power and sample size for estimating mean",
    "text": "6.7 Calculating power and sample size for estimating mean\nWe are planning a study to estimate a population mean. The population standard deviation is believed to be 20, and the population distribution is believed to be approximately normal. We will be testing the null hypothesis that the population mean is 100. Suppose the population mean is actually 110, and we want to determine how likely we are to (correctly) reject the null hypothesis in this case, using a two-sided (but one-sample) test with \\(\\alpha=0.05\\).\n\nWe will take a sample of size \\(n=30\\). Calculate the power of this test.\n\nSolution\npower.t.test. Fill in: sample size n, difference in means delta (\\(10=110-100\\)), population SD sd, type of test type (one.sample) and kind of alternative hypothesis alternative (two.sided). Leave out power since that’s what we want:\n\npower.t.test(n=30,delta=10,sd=20,type=\"one.sample\",alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 30\n          delta = 10\n             sd = 20\n      sig.level = 0.05\n          power = 0.7539627\n    alternative = two.sided\n\n\nI meant “calculate” exactly rather than “estimate” (by simulation). Though if you want to, you can do that as well, thus:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(samples = list(rnorm(30, 110, 20))) %&gt;% \n  mutate(ttest = list(t.test(samples, mu= 100))) %&gt;% \n  mutate(pvals = ttest$p.value) %&gt;% \n  count(pvals&lt;=0.05)\n\n\n\n  \n\n\n\nThat came out alarmingly close to the exact answer.\n\\(\\blacksquare\\)\n\nFind the sample size necessary to obtain a power of at least 0.80 under these conditions. What sample size do you need? Explain briefly how your answer is consistent with (a).\n\nSolution\nAgain, the implication is “by calculation”. This time, in power.t.test, put in 0.80 for power and leave out n. The order of things doesn’t matter (since I have named everything that’s going into power.t.test):\n\npower.t.test(delta=10,power=0.80,sd=20,type=\"one.sample\",alternative=\"two.sided\")  \n\n\n     One-sample t test power calculation \n\n              n = 33.3672\n          delta = 10\n             sd = 20\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nTo get sample size for power at least 0.80, we have to round 33.36 up to the next whole number, ie. \\(n=34\\) is needed. (A sample of size 33 wouldn’t quite have enough power.)\nThis answer is consistent with (a) because a sample size of 30 gave a power a bit less than 0.80, and so to increase the power by a little (0.75 to 0.80), we had to increase the sample size by a little (30 to 34).\nExtra: estimating sample sizes by simulation is tricky, because the sample size has to be input to the simulation. That means your only strategy is to try different sample sizes until you find one that gives the right power.\nIn this case, we know that a sample of size 30 doesn’t give quite enough power, so we have to up the sample size a bit. How about we try 40? I copied and pasted my code from above and changed 30 to 40:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(samples = list(rnorm(40, 110, 20))) %&gt;% \n  mutate(ttest = list(t.test(samples, mu= 100))) %&gt;% \n  mutate(pvals = ttest$p.value) %&gt;% \n  count(pvals&lt;=0.05)\n\n\n\n  \n\n\n\nNow the power is a bit too big, so we don’t need a sample size quite as big as 40. So probably our next guess would be 35. But before we copy and paste again, we should be thinking about making a function of it first, with the sample size as input. Copy-paste once more and edit:\n\nsim_power=function(n) {\n  tibble(sim = 1:1000) %&gt;% \n    rowwise() %&gt;% \n    mutate(samples = list(rnorm(n, 110, 20))) %&gt;% \n    mutate(ttest = list(t.test(samples, mu= 100))) %&gt;% \n    mutate(pvals = ttest$p.value) %&gt;% \n    ungroup() %&gt;% \n    count(pvals&lt;=0.05)\n}\n\nIn the grand scheme of things, we might want to have the null and true means, population SD and \\(\\alpha\\) be inputs to the function as well, so that we have a more general tool, but this will do for now.\nLet’s run it with a sample size of 35:\n\nsim_power(35)\n\n\n\n  \n\n\n\nand I’m going to call that good. (Because there is randomness in the estimation of the power, don’t expect to get too close to the right answer. This one came out a fair bit less than the right answer; the power for \\(n=35\\) should be a bit more than 0.80.)\nNow that you have the software to do it, you can see that figuring out a sample size like this, at least roughly, won’t take very long: each one of these simulations takes maybe seconds to run, and all you have to do is copy and paste the previous one, and edit it to contain the new sample size before running it again. You’re making the computer work hard while you lazily sip your coffee, but there’s no harm in that: programmer’s brain cells are more valuable than computer CPU cycles, and you might as well save your brain cells for when you really need them.\nYou might even think about automating this further. The easiest way, now that we have the function, is something like this:\n\ntibble(ns = seq(20, 50, 5)) %&gt;% \n  rowwise() %&gt;% \n  mutate(power_tab = list(sim_power(ns))) %&gt;% \n  unnest(power_tab) %&gt;% \n  pivot_wider(names_from = `pvals &lt;= 0.05`, values_from = n)\n\n\n\n  \n\n\n\nThe business end of this is happening in the first three lines. I wasn’t thinking of this when I originally wrote sim_power to return a dataframe, so there is a bit more fiddling after the simulations are done: I have to unnest to see what the list-column power_tab actually contains, and because of the layout of the output from unnesting sim_power (long format), it looks better if I pivot it wider, so that I can just cast my eye down the TRUE column and see the power increasing as the sample size increases.\nYou might also think of something like bisection to find the sample size that has power 0.8, but it starts getting tricky because of the randomness; just by chance, it may be that sometimes the simulated power goes down as the sample size goes up. With 1000 simulations each time, it seems that the power ought to hit 80% with a sample size between 30 and 35."
  },
  {
    "objectID": "power.html#simulating-power-for-proportions-1",
    "href": "power.html#simulating-power-for-proportions-1",
    "title": "6  Power and sample size",
    "section": "6.8 Simulating power for proportions",
    "text": "6.8 Simulating power for proportions\nIn opinion surveys (and other places), we are testing for a proportion \\(p\\) (for example, the proportion of people agreeing with some statement). Often, we want to know whether the proportion is “really” greater than 0.5.2\nThat would entail testing a null \\(H_0: p=0.5\\) against an alternative \\(H_a: p&gt;0.5\\). This is usually done by calculating the test statistic \\[ z = { \\hat{p} - 0.5 \\over \\sqrt{0.25/n}},\\] where \\(\\hat{p}\\) is the observed proportion in the sample, and getting a P-value from the upper tail of a standard normal distribution. (The 0.25 is \\(p(1-p)\\) where \\(p=0.5\\).) This is what prop.test does, as we investigate shortly.\n\nUse rbinom to generate a random value from a binomial distribution with \\(n=100\\) and \\(p=0.6\\). There are three inputs to rbinom: the first one should be the number 1, and the second and third are the \\(n\\) and \\(p\\) of the binomial distribution.\n\nSolution\nI am doing some preparatory work that you don’t need to do:\n\nset.seed(457299)\n\nBy setting the “seed” for the random number generator, I guarantee that I will get the same answers every time I run my code below (and therefore I can talk about my answers without worrying that they will change). Up to you whether you do this. You can “seed” the random number generator with any number you like. A lot of people use 1. Mahinda seems to like 123. Mine is an old phone number.\nAnd so to work:\n\nrbinom(1, 100, 0.6)\n\n[1] 60\n\n\nI got exactly 60% successes this time. You probably won’t get exactly 60, but you should get somewhere close. (If you use my random number seed and use the random number generator exactly the same way I did, you should get the same values I did.)\nFor fun, you can see what happens if you change the 1:\n\nrbinom(3, 100, 0.6)\n\n[1] 58 57 55\n\n\nThree random binomials, that happened to come out just below 60. We’re going to leave the first input as 1, though, and let rowwise handle “lots of sampled values” later.\n\\(\\blacksquare\\)\n\nUsing the random binomial that you generated just above, use prop.test to test whether it could reasonably have come from a binomial population with \\(n=100\\) and \\(p=0.5\\), or whether \\(p\\) is actually bigger than 0.5. (Of course, you know it actually did not come from a population with \\(p=0.5\\).) prop.test has, for us, four inputs, thus:\n\n\nthe observed number of successes\nthe n of the binomial distribution\nthe null-hypothesis p of the binomial distribution\nthe alternative hypothesis, here “greater”\n\nSolution\nI got exactly 60 successes, so I do this:\n\nprop.test(60, 100, 0.5, alternative = \"greater\")\n\n\n    1-sample proportions test with continuity correction\n\ndata:  60 out of 100, null probability 0.5\nX-squared = 3.61, df = 1, p-value = 0.02872\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.5127842 1.0000000\nsample estimates:\n  p \n0.6 \n\n\nThe P-value should at least be fairly small, since 60 is a bit bigger than 50. (Think about tossing a coin 100 times; would 60 heads make you doubt the coin’s fairness? The above says it should.)\n\\(\\blacksquare\\)\n\nRun prop.test again, just as you did before, but this time save the result, and extract the piece of it called p.value. Is that the P-value from your test?\n\nSolution\nCopying and pasting:\n\np_test &lt;- prop.test(60, 100, 0.5, alternative = \"greater\")\np_test$p.value\n\n[1] 0.02871656\n\n\nYep, the same.\n\\(\\blacksquare\\)\n\nEstimate the power of a test of \\(H_0: p=0.5\\) against \\(H_a: p&gt;0.5\\) when \\(n=500\\) and \\(p=0.56\\), using \\(\\alpha=0.05\\). There are three steps:\n\n\ngenerate random samples from binomial distributions with \\(n=500\\) and \\(p=0.56\\), repeated “many” times (something like 1000 or 10,000 is good)\nrun prop.test on each of those random samples\nextract the P-value for each test and save the results (in a column called, perhaps, pvals).\n\nSo I lied: the fourth and final step is to count how many of those P-values are 0.05 or less.\nSolution\nThe first part of the first step is to create a column called something like sim that labels each simulated sample, and to make sure that everything happens rowwise. After that, you follow the procedure:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = rbinom(1, 500, 0.56)) %&gt;% \n  mutate(test = list(prop.test(sample, 500, 0.5, alternative = \"greater\"))) %&gt;% \n  mutate(pvals = test$p.value) %&gt;% \n  count(pvals &lt;= 0.05)\n\n\n\n  \n\n\n\nThe previous parts, using rbinom and prop.test, were meant to provide you with the ingredients for this part. The first step is to use rbinom. The first input is 1 since we only want one random binomial each time (the rowwise will handle the fact that you actually want lots of them; you only want one per row since you are working rowwise). The second step runs prop.test; the first input to that is each one of the numbers of successes from the first step. The last part is to pull out all the P-values and make a table of them, just like the example in lecture.\nThe estimated power is about 85%. That is, if \\(p\\) is actually 0.56 and we have a sample of size 500, we have a good chance of (correctly) rejecting that \\(p=0.5\\).\nExtra: It turns out that SAS can work out this power by calculation (using proc power). SAS says our power is also about 85%, as our simulation said. I was actually pleased that my simulation came out so close to the right answer.\nIn contrast to power.t.test, SAS’s proc power handles power analyses for a lot of things, including analysis of variance, correlation and (multiple) regression. What these have in common is some normal-based theory that allows you (under assumptions of sufficiently normal-shaped populations) to calculate the exact answer (that is, the distribution of the test statistic when the alternative hypothesis is true). The case we looked at is one of those because of the normal approximation to the binomial: once \\(n\\) gets big, particularly if \\(p\\) is somewhere near 0.5, the binomial is very well approximated by a normal with the right mean and SD.\nThe moral of this story is that when you have a decently large sample, \\(n=500\\) in this case, \\(p\\) doesn’t have to get very far away from 0.5 before you can correctly reject 0.5. Bear in mind that sample sizes for estimating proportions need to be larger than those for estimating means, so \\(n=500\\) is large without being huge. The practical upshot is that if you design a survey and give it to 500 (or more) randomly chosen people, the proportion of people in favour doesn’t have to be much above 50% for you to correctly infer that it is above 50%, most of the time.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "power.html#designing-a-study-to-have-enough-power-1",
    "href": "power.html#designing-a-study-to-have-enough-power-1",
    "title": "6  Power and sample size",
    "section": "6.9 Designing a study to have enough power",
    "text": "6.9 Designing a study to have enough power\nYou are designing a study to test the null hypothesis that a population mean is 0 against the alternative hypothesis that it is greater than 0. Assume that the population SD is \\(\\sigma=15\\). It is important to detect the alternative \\(\\mu=2\\); that is, we want to design the study so that most of the time the null hypothesis would be (correctly) rejected if in fact \\(\\mu=2\\). A one-sample \\(t\\)-test will be used, and the data values are assumed to have a normal distribution.\n\nUse simulation to estimate the power of this test when the sample size is 100. Use \\(\\alpha=0.05\\).\n\nSolution\nUse at least 1000 simulations (more, if you’re willing to wait for it). In rnorm, the sample size is first, then the (true) population mean, then the (assumed) population SD:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(100, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThe power is (estimated as) a disappointing 0.361. Your answer won’t (most likely) be the same as this, but it should be somewhere close. I would like to see you demonstrate that you know what power is, for example “if the population mean is actually 2, the null hypothesis \\(H_0: \\mu = 0\\), which is wrong, will only be rejected about 36% of the time”.3\nThe test we are doing is one-sided, so you need the alternative in there. If you omit it, you’ll have the answer to a different problem:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(100, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThis is the probability that you reject \\(H_0: \\mu=0\\) in favour of \\(H_a: \\mu \\ne 0\\). This is smaller, because the test is “wasting effort” allowing the possibility of rejecting when the sample mean is far enough less than zero, when most of the time the samples drawn from the true distribution have mean greater than zero. (If you get a sample mean of 2.5, say, the P-value for a one-sided test will be smaller than for a two-sided one.)\nExtra 1:\nThis low power of 0.361 is because the population SD is large relative to the kind of difference from the null that we are hoping to find. To get a sense of how big the power might be, imagine you draw a “typical” sample from the true population: it will have a sample mean of 2 and a sample SD of 15, so that \\(t\\) will be about\n\n(2-0)/(15/sqrt(100))\n\n[1] 1.333333\n\n\nYou won’t reject with this (\\(t\\) would have to be bigger than 2), so in the cases where you do reject, you’ll have to be more lucky: you’ll need a sample mean bigger than 2, or a sample SD smaller than 15. So the power won’t be very big, less than 0.5, because about half the time you’ll get a test statistic less than 1.33 and about half the time more, and not all of those will lead to rejection.\nExtra 2:\nThis is exactly the situation where power.t.test works, so we can get the exact answer (you need all the pieces):\n\npower.t.test(n=100, delta=2-0, sd=15, type=\"one.sample\", \nalternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 100\n          delta = 2\n             sd = 15\n      sig.level = 0.05\n          power = 0.3742438\n    alternative = one.sided\n\n\nYour answer, from 1000 simulations, should be within about 3 percentage points of that. (Mine was only about 1 percentage point off.)\n\\(\\blacksquare\\)\n\nAgain by using simulation, estimate how large a sample size would be needed to obtain a power of 0.80. Show and briefly explain your process.\n\nSolution\nThe point of this one is the process as well as the final answer, so you need to show and justify what you are doing. Showing only a final answer does not show that you know how to do it. The whole point of this one is to make mistakes and fix them!\nThe simulation approach does not immediately give you a sample size for fixed power, so what you have to do is to try different sample sizes until you get one that gives a power close enough to 0.80. You have to decide what “close enough” means for you, given that the simulations have randomness in them. I’m going to use 10,000 simulations for each of my attempts, in the hope of getting a more accurate answer.\nFirst off, for a sample size of 100, the power was too small, so the answer had better be bigger than 100. I’ll try 200. For these, copy and paste the code, changing the sample size each time:\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(200, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nA sample size of 200 isn’t big enough yet. I’ll double again to 400:\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(400, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nGetting closer. 400 is too big, but closer than 200. 350?\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(350, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nClose! I reckon you could call that good (see below), or try again with a sample size a bit less than 350:\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(345, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\n340 is definitely too small:\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(340, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThis is actually not as close as I was expecting. I think we are getting close to simulation accuracy for this number of simulations. If we do 10,000 simulations of an event with probability 0.8 (correctly rejecting this null), below are the kind of results we might get.4 This is the middle 95% of that distribution.\n\nqbinom(c(0.025,0.975), 10000, 0.8)\n\n[1] 7921 8078\n\n\nAnything between those limits is the kind of thing we might get by chance, so simulation doesn’t let us distinguish between 347 and 350 as the correct sample size. Unless we do more than 10,000 simulations, of course!\nIf you stuck with 1000 simulations each time, these are the corresponding limits:\n\nqbinom(c(0.025,0.975), 1000, 0.8)\n\n[1] 775 824\n\n\nand any sample sizes that produce an estimated power between these are as accurate as you’ll get. (Here you see the advantage of doing more simulations.)\nIf you’ve been using 10,000 simulations each time like me, you’ll have noticed that these actually take a noticeable time to run. This is why coders always have a coffee or something else to sip on while their code runs; coders, like us, need to see the output to decide what to do next. Or you could install the beepr package, and get some kind of sound when your simulation finishes, so that you’ll know to get off Twitter5 and see what happened. There are also packages that will send you a text message or will send a notification to all your devices.\nWhat I want to see from you here is some kind of trial and error that proceeds logically, sensibly increasing or decreasing the sample size at each trial, until you have gotten reasonably close to power 0.8.\nExtra: once again we can figure out the correct answer:\n\npower.t.test(power = 0.80, delta=2-0, sd=15, type=\"one.sample\", \nalternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 349.1256\n          delta = 2\n             sd = 15\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\n\nThis does not answer the question, though, since you need to do it by simulation with trial and error. If you want to do it this way, do it at the end as a check on your work; if the answer you get this way is very different from the simulation results, that’s an invitation to check what you did.\n350 actually is the correct answer. But you will need to try different sample sizes until you get close enough to a power of 0.8; simply doing it for \\(n=350\\) is not enough, because how did you know to try 350 and not some other sample size?\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "power.html#power-and-alpha-in-a-skewed-population-1",
    "href": "power.html#power-and-alpha-in-a-skewed-population-1",
    "title": "6  Power and sample size",
    "section": "6.10 Power and \\(\\alpha\\) in a skewed population",
    "text": "6.10 Power and \\(\\alpha\\) in a skewed population\nA population of a large number of values v is at http://ritsokiguess.site/datafiles/pop.csv, in a CSV file.\n\nRead in the population and display some of the values.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/pop.csv\"\npop &lt;- read_csv(my_url)\n\nRows: 10000 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): v\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npop\n\n\n\n  \n\n\n\n10,000 values. A large population. (From these few values, v seems to be positive but rather variable.)\n\\(\\blacksquare\\)\n\nObtain a suitable plot of your population. What do you notice?\n\nSolution\nOne quantitative variable, so a histogram. The population is large, so you can use more bins than usual. Sturges’ rule says 14 bins (the logarithm below is base 2, or, the next power of 2 above 10,000 is 16,384 which is \\(2^{14}\\)):\n\nlog(10000, 2)\n\n[1] 13.28771\n\n2^14\n\n[1] 16384\n\n\n\nggplot(pop, aes(x=v)) + geom_histogram(bins=14)\n\n\n\n\nPick a number of bins: the default 30 bins is pretty much always too many. Any number of bins that shows this shape is good as an answer, but you also need to display some thinking about how many bins to use, either starting with a rule as I did, or experimenting with different numbers of bins. Rules are not hard and fast; it so happened that I liked the picture that 14 bins gave, so I stopped there. Thirty bins, the default, is actually not bad here:\n\nggplot(pop, aes(x=v)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nbut if you do this, you need to say something that indicates some conscious thought, such as saying “this number of bins gives a good picture of the shape of the distribution”, which I am OK with. Have a reason for doing what you do.\nThis is skewed to the right, or has a long right tail. This is a better description than “outliers”: there are indeed some very large values (almost invisible on the histogram), but to say that is to imply that the rest of the distribution apart from the outliers has a regular shape, not something you can say here.6\nExtra: The issue that’s coming up is whether this is normally-distributed, which of course it is not. This is a normal quantile plot. (Idea: if the points follow the line, at least approximately, the variable is normally distributed; if not, not.):\n\nggplot(pop, aes(sample=v)) + stat_qq() + stat_qq_line()\n\n\n\n\nThat is your archetypal skewed-to-right. The two highest values are not a lot higher than the rest, again supporting a curved shape overall (skewness) on this plot, rather than the problem being outliers. (The highest values are consistent with the shape of the curve, rather than being unusually high compared to the curve.)\n\\(\\blacksquare\\)\n\nIf you take a sample of 10 observations from this population and run a \\(t\\)-test, how likely are you to (correctly) reject the null hypothesis \\(H_0: \\mu = 4\\), against the alternative \\(H_a: \\mu &gt; 4\\)? Investigate by simulation.\n\nSolution\nAs you noted, this is a one-sided alternative, so make sure your code does the right thing. Take a lot of random samples, run the \\(t\\)-test on each one, grab the P-value each time, count the number of P-values less or equal to your \\(\\alpha\\). This is not a bootstrap, so the sampling needs to be without replacement, and you need to say how big the sample is:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(pop$v, 10))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 4, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThe estimated power is only about 0.19.\nAs to the code, well, the samples and the \\(t\\)-test both consist of more than one thing, so in the mutates that create them, don’t forget the list around the outside, which will create a list-column.\nHere, and elsewhere in this question, use at least 1000 simulations. More will give you more accurate results, but you’ll have to wait longer for it to run. Your choice.\nAs a final remark, you can not do this one by algebra, as you might have done in other courses, because you do not know the functional form of the population distribution. The power calculations you may have done before as calculations typically assume a normal population, because if you don’t, the algebra gets too messy too fast. (You’d need to know the distribution of the test statistic under the alternative hypothesis, which in cases beyond the normal is not usually known.)\n\\(\\blacksquare\\)\n\nTry again with a sample size of 50 (leaving everything else the same). Explain briefly why the results so far are as you’d expect.\n\nSolution\nFor the code, this is copy-paste-edit. Just change the sample size:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(pop$v, 50))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 4, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThe power is now much bigger, around 73%. This is as expected because with a larger sample size we should be more likely to reject a false null hypothesis.\nThe reason for this is that the mean of a bigger sample should be closer to the population mean, because of the Law of Large Numbers, and thus further away from the incorrect null hypothesis and more likely far enough away to reject it. In this case, as you will see shortly, the population mean is 5, and so, with a bigger sample, the sample mean will almost certainly be closer to 5 and further away from 4.\nI have a feeling you could formalize this kind of argument with Chebyshev’s inequality, which would apply to any kind of population.7 I think I’d have to write it down to get it right, though.\n\\(\\blacksquare\\)\n\nAgain by simulation, estimate the probability that the null hypothesis \\(H_0: \\mu=5\\) will be rejected when a sample of size 10 is taken from this population, in favour of the alternative \\(H_a: \\mu &gt; 5\\). Explain briefly why the answer is not what you would have expected, and why that happened here. (Hint: what is the population mean?)\n\nSolution\nTaking the hint first:\n\npop %&gt;% \nsummarize(m = mean(v))\n\n\n\n  \n\n\n\n(I’m hoping that some light dawns at this point), and copy-paste-edit your simulation code again, this time changing the null mean to 5:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(pop$v, 10))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 5, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThe “power” is estimated to be 0.020. (Again, your value won’t be exactly this, most likely, but it should be somewhere close.)\nSo what were we expecting? This time, the null hypothesis, that the population mean is 5, is actually true. So rejecting it is now a type I error, and the probability of that should be \\(\\alpha\\), which was 0.05 here. In our simulation, though, the estimated probability is quite a bit less than 0.05. (Your result will probably differ from mine, but it is not likely to be bigger than 0.05).\nTo think about why that happened, remember that this is a very skewed population, and the sample size of 10 is not big, so this is not really the situation in which we should be using a \\(t\\)-test. The consequence of doing so anyway, which is what we investigated, is that the actual \\(\\alpha\\) of our test is not 0.05, but something smaller: the test is not properly calibrated.\nIf you do this again for a sample of size 50, you’ll find that the simulation tells you that \\(\\alpha\\) is closer to 0.05, but still less. The population is skewed enough that the Central Limit Theorem still hasn’t kicked in yet, and so we still cannot trust the \\(t\\)-test to give us a sensible P-value.\nExtra: a lot more discussion on what is happening here:\nThis test is what is known in the jargon as “conservative”. To a statistician, this means that the probability of making a type I error is smaller than it should be. That is in some sense safe, in that if you reject, you can be pretty sure that this rejection is correct, but it makes it a lot harder than it should to reject in the first place, and thus you can fail to declare a discovery when you have really made one (but the test didn’t say so).\nI did some investigation to see what was going on. First, I ran the simulation again, but this time keeping the mean and SD of each sample, as well as the \\(t\\)-statistic, but not actually doing the \\(t\\)-test:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(pop$v, 10))) %&gt;% \n  mutate(xbar = mean(my_sample),\n         s = sd(my_sample),\n         t_stat = (xbar - 5) / (s / sqrt(10))) -&gt; mean_sd\nmean_sd\n\n\n\n  \n\n\n\nAs for coding, I made a dataframe with a column sim that numbers the individual samples, made sure I said that I wanted to work rowwise, generated a random sample from the population in each row of size 10, and found its mean, SD and the calculated-by-me \\(t\\)-statistic.8\nAfter that, I played around with several things, but I found something interesting when I plotted the sample mean and SD against each other:\n\nggplot(mean_sd, aes(x=xbar, y=s)) + geom_point() + geom_smooth(se=F)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nWhen the sample mean is bigger, so is the sample standard deviation!\nThis actually does make sense, if you stop to think about it. A sample with a large mean will have some of those values from the long right tail in it, and having those values will also make the sample more spread out. The same does not happen at the low end: if the mean is small, all the sample values must be close together and the SD will be small also.9\nIt wasn’t clear to me what that would do to the \\(t\\)-statistic. A larger sample mean would make the top of the test statistic bigger, but a larger sample mean would also go with a larger sample SD, and so the bottom of the test statistic would be bigger as well. That’s why I included this in the simulation too:\n\nggplot(mean_sd, aes(x=t_stat)) + geom_histogram(bins=12)\n\n\n\n\nWell, well. Skewed to the left.\nThis too makes sense with a bit of thought. A small sample mean will also have a small sample SD, so the test statistic could be more negative. But a large sample mean will have a large sample SD, so the test statistic won’t get so positive. Hence, in our simulation, the test statistic won’t get large enough to reject with as often as it should. Thus, the type I error probability that is too small.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "power.html#footnotes",
    "href": "power.html#footnotes",
    "title": "6  Power and sample size",
    "section": "",
    "text": "That would mean assessing whether an observed proportion could be greater than 0.5 just by chance, or whether it is bigger enough than 0.5 to reject chance as a plausible explanation.↩︎\nThat would mean assessing whether an observed proportion could be greater than 0.5 just by chance, or whether it is bigger enough than 0.5 to reject chance as a plausible explanation.↩︎\nThis is why I called my result disappointing. I would like to reject a lot more of the time than this, but, given that the truth was not very far away from the null given the (large) population SD, I can’t. See Extra 1.↩︎\nIf the power is really 0.8, the number of simulated tests that end up rejecting has a binomial distribution with n of 10000 and p of 0.80.↩︎\nOr Reddit or Quora or whatever your favourite time-killer is.↩︎\nThe question to ask yourself is whether the shape comes from the entire distribution, as it does here (skewness), or whether it comes from a few unusual observations (outliers).↩︎\nIt has to have a standard deviation, though, but our population seems well-enough behaved to have a standard deviation.↩︎\nThis is not bootstrapping, but generating ordinary random samples from a presumed-known population, so there is no replace = TRUE here.↩︎\nYou might have something lurking in your mind about the sample mean and sample SD/variance being independent, which they clearly are not here. That is true if the samples come from a normal distribution, and from that comes independence of the top and bottom of the \\(t\\)-statistic. But here is an example of how everything fails once you go away from normality, and how you have to rely on the central limit theorem, or large sample sizes more generally, for most of your theory to be any good.↩︎"
  },
  {
    "objectID": "sign.html#running-a-maze",
    "href": "sign.html#running-a-maze",
    "title": "7  The sign test",
    "section": "7.1 Running a maze",
    "text": "7.1 Running a maze\nA researcher is trying to design a maze that can be run by rats in about 60 seconds. One particular maze was run by a sample of 21 rats, with the times shown in link.\n\nRead the data into R. What (if anything) are the data values delimited by?\nRun a sign test, doing it yourself as we did in class: count the number of values above and below 60, take the smaller of those, and find the probability of a value of that or smaller still on a binomial distribution with \\(n=21\\) and \\(p=0.5\\) (we have 21 data points), doubling the answer because the test is two-sided.\nInstall my package smmr, if you haven’t already. To do this, you first need to install the package devtools (if you haven’t already), by going to the console and typing\n\n\ninstall.packages(\"devtools\")\n\nWhen that’s all done, install smmr thus:\n\nlibrary(devtools)\ninstall_github(\"nxskok/smmr\")\n\nThat all needs to be done only once. Then, each R Studio session where you want to use smmr needs this:\n\nlibrary(smmr)\n\nAs usual, only the library thing only needs to be done every time.\nWhen you have smmr installed, use sign_test from that package to re-run your sign test. Do you get the same P-value?\n\nPackage smmr also has a function pval_sign, which has the same input as sign_test, but with the null median first. Run it on your data and see what it gives.\nObtain a 95% confidence interval for the median based on these data. Do this two ways. First, use the trial and error way from class (either the try-lots-of-values way or the bisection way; either is good). Second, use ci_median from smmr. The latter takes as input a data frame, a column name (unquoted) and optionally a conf.level that defaults to 0.95."
  },
  {
    "objectID": "sign.html#chocolate-chips",
    "href": "sign.html#chocolate-chips",
    "title": "7  The sign test",
    "section": "7.2 Chocolate chips",
    "text": "7.2 Chocolate chips\nA famous cookie manufacturer claims that their bags of chocolate chip cookies contain “more than 1100 chocolate chips on average”. A diligent group of students buys 16 bags of these cookies and counts the number of chocolate chips in each bag. The results are in http://ritsokiguess.site/datafiles/chips.txt.\n\nRead in and display (some of) the data.\nBuild your own sign test in R for testing that the median is 1100 chocolate chips, against the alternative that it is greater. (Do this as in class: count the appropriate thing, compare it with an appropriate binomial distribution, and obtain a P-value.\nUse my R package smmr to reproduce your sign test above, and verify that you get consistent results. (See the maze-design question for instructions on installing this, if you haven’t yet.)\nUse smmr to obtain a 95% confidence interval for the median number of chocolate chips per bag of cookies."
  },
  {
    "objectID": "sign.html#the-power-of-the-sign-test",
    "href": "sign.html#the-power-of-the-sign-test",
    "title": "7  The sign test",
    "section": "7.3 The power of the sign test",
    "text": "7.3 The power of the sign test\nI’ve mentioned several times that the sign test has less power than the \\(t\\)-test. Let’s investigate this with a specific example.\nLet’s suppose we are testing \\(H_0: \\mu=40\\) against \\(H_a: \\mu \\ne 40\\), where \\(\\mu\\) is the population mean (and median, as we shall see). Our population actually has a normal distribution with mean 50 and SD 15, so that the null hypothesis is wrong and we want to reject it most of the time. On the other hand, the population actually is normally-distributed and so the \\(t\\)-test is the right one to use.\n(This is an old question, so I tackle the simulated power differently than I did it in class this time. But see if you can follow what I do here.)\n\nUse power.t.test to find the probability that a \\(t\\)-test correctly rejects the null hypothesis using a sample size of \\(n=10\\).\nWhat code in R would draw a random sample of size 10 from the true population distribution and save the sample in a variable?\nWhat code would count how many of the sampled values are less than 40 and how many are greater (or equal)?\nIt turns out the sign test would reject \\(H_0: M=40\\) against \\(H_a: M \\ne 40\\) at \\(\\alpha=0.05\\) if the smaller of the numbers in the last part is 1 or less. (\\(M\\) is the population median.) Add to your pipeline to obtain TRUE if you should reject the null for your data and FALSE otherwise.\nSimulate the above process 1000 times: draw a random sample from a normal distribution of size 10 with mean 50 and SD 15, count the number of values below 40, reject if the minimum of those is 0, 1, 9, or 10, then count the number of rejections out of 1000.\nWhich is more powerful in this case, the sign test or the \\(t\\)-test? How do you know?"
  },
  {
    "objectID": "sign.html#ben-roethlisberger",
    "href": "sign.html#ben-roethlisberger",
    "title": "7  The sign test",
    "section": "7.4 Ben Roethlisberger",
    "text": "7.4 Ben Roethlisberger\nBen Roethlisberger plays (American) football for the Pittsburgh Steelers. He plays as a quarterback, which means that his job is to throw (pass) the ball so that one of his teammates can catch it. Each time he makes a pass that is caught, this is called a “completion”, and the team coaches are interested in his average number of completions per game (this average could be the mean or the median).\nIn 2010, Roethlisberger was suspended for the first four games of the season, and there was concern that this might affect his performance (in terms of the number of passes completed in the games after he returned). The Pittsburgh Steelers did not play in week 5 of the 2010 season; the season is 17 weeks long (one game per week) and each team has one week in which they do not play.\nThe data are here. There are four columns: the year (always 2010), the week number of the season that the game was played in, the name of the opposing team, and the number of completed passes by Roethlisberger in the game.\n\nRead in and display (some of) the data. Do you have what you were expecting?\nMake a suitable graph of the number of completed passes, and explain briefly why you would have some doubts about using \\(t\\)-procedures in this situation.\nRun a sign test to compare Roethlisberger’s performance in 2010 with his previous average of 22.5 completions per game. What do you conclude?\nWhy might you have expected your sign test to come out significant, even without looking at the P-value? Explain briefly.\nObtain a 90% confidence interval for the median number of completed passes (over “all possible games played by 2010 Ben Roethlisberger”).\nFind a 90% confidence interval for the mean number of passes completed, and explain briefly why it differs from the one for the median in the way that it does."
  },
  {
    "objectID": "sign.html#six-ounces-of-protein",
    "href": "sign.html#six-ounces-of-protein",
    "title": "7  The sign test",
    "section": "7.5 Six ounces of protein",
    "text": "7.5 Six ounces of protein\nA company produces prepackaged diet meals. These meals are advertised as containing “6 ounces of protein per package”. A consumer organization is concerned that this is not accurate. The organization takes a random sample of 20 of these meals, and measures the protein content of each one. The data are in http://ritsokiguess.site/datafiles/protein.txt as one column.\n\nRead in and display (some of) the data.\nMake a suitable graph of your data.\nWhy might a sign test be better than a \\(t\\)-test for assessing the average amount of protein per package? Explain briefly. (“Average” here means any measure of centre.)\nRun a suitable sign test for these data. What do you conclude?\nIn your sign test, how could you have deduced that the P-value was going to be small even without looking at any of the P-values themselves? Explain briefly.\nObtain a 90% confidence interval for the population median protein content. What does this tell you about the reason for the rejection or non-rejection of the null hypothesis above?\n\nMy solutions follow:"
  },
  {
    "objectID": "sign.html#running-a-maze-1",
    "href": "sign.html#running-a-maze-1",
    "title": "7  The sign test",
    "section": "7.6 Running a maze",
    "text": "7.6 Running a maze\nA researcher is trying to design a maze that can be run by rats in about 60 seconds. One particular maze was run by a sample of 21 rats, with the times shown in link.\n\nRead the data into R. What (if anything) are the data values delimited by?\n\nSolution\nTake a look at the data file first. There is only one column of data, so you can treat it as being delimited by anything you like: a space, or a comma (the file can also be treated as a .csv), etc.:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/maze.txt\"\ntimes &lt;- read_delim(myurl, \" \")\n\nRows: 21 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntimes\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nRun a sign test, doing it yourself as we did in class: count the number of values above and below 60, take the smaller of those, and find the probability of a value of that or smaller still on a binomial distribution with \\(n=21\\) and \\(p=0.5\\) (we have 21 data points), doubling the answer because the test is two-sided.\n\nSolution\nCount how many values are above and below 60:\n\ntimes %&gt;% count(time &gt; 60)\n\n\n\n  \n\n\n\n5 above and 16 below. Then find out how likely it is that a binomial with \\(n=21, p=0.5\\) would produce 5 or fewer successes:\n\np &lt;- sum(dbinom(0:5, 21, 0.5))\np\n\n[1] 0.01330185\n\n\nor if you prefer count upwards from 16:\n\nsum(dbinom(16:21, 21, 0.5))\n\n[1] 0.01330185\n\n\nand double it to get a two-sided P-value:\n\n2 * p\n\n[1] 0.0266037\n\n\nWe’ll compare this with smmr in a moment.\n\\(\\blacksquare\\)\n\nInstall my package smmr, if you haven’t already. To do this, you first need to install the package devtools (if you haven’t already), by going to the console and typing\n\n\ninstall.packages(\"devtools\")\n\nWhen that’s all done, install smmr thus:\n\nlibrary(devtools)\ninstall_github(\"nxskok/smmr\")\n\nThat all needs to be done only once. Then, each R Studio session where you want to use smmr needs this:\n\nlibrary(smmr)\n\nAs usual, only the library thing only needs to be done every time.\nWhen you have smmr installed, use sign_test from that package to re-run your sign test. Do you get the same P-value?\nSolution\nThe sign test function takes a data frame, an (unquoted) column name from that data frame of data to test the median of, and a null median (which defaults to 0 if you omit it):\n\nlibrary(smmr)\nsign_test(times, time, 60)\n\n$above_below\nbelow above \n   16     5 \n\n$p_values\n  alternative    p_value\n1       lower 0.01330185\n2       upper 0.99640131\n3   two-sided 0.02660370\n\n\nThis shows you two things: a count of the values below and above the null median, and then the P-values according to the various alternative hypotheses you might have.\nIn our case, we see again the 16 maze-running times below 60 seconds and 5 above (one of which was a long way above, but we don’t care about that here). We were testing whether the median was different from 60, so we look at the two-sided P-value of 0.0266, which is exactly what we had before.\nIf sign_test doesn’t work for you (perhaps because it needs a function enquo that you don’t have), there is an alternative function sign_test0 that doesn’t use it. It requires as input a column of values (extracted from the data frame) and a null median, thus:\n\nwith(times, sign_test0(time, 60))\n\n$above_below\nbelow above \n   16     5 \n\n$p_values\n  alternative    p_value\n1       lower 0.01330185\n2       upper 0.99640131\n3   two-sided 0.02660370\n\n\nThe output should be, and here is, identical.\n\\(\\blacksquare\\)\n\nPackage smmr also has a function pval_sign, which has the same input as sign_test, but with the null median first. Run it on your data and see what it gives.\n\nSolution\nTry it and see:\n\npval_sign(60, times, time)\n\n[1] 0.0266037\n\n\nThe two-sided P-value, and that is all. We’ll be using this in a minute.\nAlternatively, there is also this, which needs a null median and a column as input:\n\nwith(times, pval_sign0(60, time))\n\n[1] 0.0266037\n\n\n\\(\\blacksquare\\)\n\nObtain a 95% confidence interval for the median based on these data. Do this two ways. First, use the trial and error way from class (either the try-lots-of-values way or the bisection way; either is good). Second, use ci_median from smmr. The latter takes as input a data frame, a column name (unquoted) and optionally a conf.level that defaults to 0.95.\n\nSolution\nThe reason for showing you pval_sign in the previous part is that this is a building block for the confidence interval. What we do is to try various null medians and find out which ones give P-values less than 0.05 (outside the interval) and which ones bigger (inside). We know that the value 60 is outside the 95% CI, and the sample median is close to 50 (which we expect to be inside), so sensible values to try for the upper end of the interval would be between 50 and 60:\n\npval_sign(58, times, time)\n\n[1] 0.0266037\n\npval_sign(55, times, time)\n\n[1] 0.6636238\n\n\nSo, 55 is inside the interval and 58 is outside. I could investigate further in similar fashion, but I thought I would try a whole bunch of null medians all at once. That goes like this, rowwise because pval_sign expects one null-hypothesis median, not several all at once:\n\ntibble(meds = seq(55, 58, 0.25)) %&gt;% \n  rowwise() %&gt;% \n  mutate(pvals = pval_sign(meds, times, time))\n\n\n\n  \n\n\n\nSo values for the median all the way up to and including 57.5 are in the confidence interval.\nNow for the other end of the interval. I’m going to do this a different way: more efficient, but less transparent. The first thing I need is a pair of values for the median: one inside the interval and one outside. Let’s try 40 and 50:\n\npval_sign(40, times, time)\n\n[1] 0.00719738\n\npval_sign(50, times, time)\n\n[1] 1\n\n\nOK, so 40 is outside and 50 is inside. So what do I guess for the next value to try? I could do something clever like assuming that the relationship between hypothesized median and P-value is linear, and then guessing where that line crosses 0.05. But I’m going to assume nothing about the relationship except that it goes uphill, and therefore crosses 0.05 somewhere. So my next guess is halfway between the two values I tried before:\n\npval_sign(45, times, time)\n\n[1] 0.07835388\n\n\nSo, 45 is inside the interval, and my (slightly) improved guess at the bottom end of the interval is that it’s between 40 and 45. So next, I try halfway between those:\n\npval_sign(42.5, times, time)\n\n[1] 0.0266037\n\n\n42.5 is outside, so the bottom end of the interval is between 42.5 and 45.\nWhat we are doing is narrowing down where the interval’s bottom end is. We started by knowing it to within 10, and now we know it to within 2.5. So if we keep going, we’ll know it as accurately as we wish.\nThis is called a “bisection” method, because at each step, we’re dividing our interval by 2.\nThere is one piece of decision-making at each step: if the P-value for the median you try is greater than 0.05, that becomes the top end of your interval (as when we tried 45); if it is less, it becomes the bottom end (when we tried 42.5).\nThis all begs to be automated into a loop. It’s not a for-type loop, because we don’t know how many times we’ll be going around. It’s a while loop: keep going while something is true. Here’s how it goes:\n\nlo &lt;- 40\nhi &lt;- 50\nwhile (abs(hi - lo) &gt; 0.1) {\n  try &lt;- (hi + lo) / 2\n  ptry &lt;- pval_sign(try, times, time)\n  print(c(try, ptry))\n  if (ptry &lt; 0.05) {\n    lo &lt;- try\n  } else {\n    hi &lt;- try\n  }\n}\n\n[1] 45.00000000  0.07835388\n[1] 42.5000000  0.0266037\n[1] 43.7500000  0.0266037\n[1] 44.37500000  0.07835388\n[1] 44.0625000  0.0266037\n[1] 44.2187500  0.0266037\n[1] 44.2968750  0.0266037\n\nlo\n\n[1] 44.29688\n\npval_sign(lo, times, time)\n\n[1] 0.0266037\n\nhi\n\n[1] 44.375\n\npval_sign(hi, times, time)\n\n[1] 0.07835388\n\n\nThe loop stopped because 44.297 and 44.375 are less than 0.1 apart. The first of those is outside the interval and the second is inside. So the bottom end of our interval is 44.375, to this accuracy. If you want it more accurately, change 0.1 in the while line to something smaller (but then you’ll be waiting longer for the answer).\nI put the print statement in the loop so that you could see what values were being tried, and what P-values they were producing. What happens with these is that the P-value jumps at each data value, so you won’t get a P-value exactly 0.05; you’ll get one above and one below.\nLikewise, you can use the function with a zero on its name and feed it a column rather than a data frame and a column name:\n\ntibble(meds =  seq(55, 58, 0.25)) %&gt;% \n  rowwise() %&gt;% \n  mutate(pvals =  with(times, pval_sign0(meds, time)))\n\n\n\n  \n\n\n\nOr adapt the idea I had above for bisection. All that was a lot of work, but I wanted you to see it all once, so that you know where the confidence interval is coming from. smmr also has a function ci_median that does all of the above without you having to do it. As I first wrote it, it was using the trial and error thing with rowwise, but I chose to rewrite it with the bisection idea, because I thought that would be more accurate.\n\nci_median(times, time)\n\n[1] 44.30747 57.59766\n\n\nThis is a more accurate interval than we got above. (The while loop for the bisection keeps going until the two guesses at the appropriate end of the interval are less than 0.01 apart, by default.)1\nIf you want some other confidence level, you add conf.level on the end, as you would for t.test:\n\nci_median(times, time, conf.level = 0.75)\n\n[1] 46.20444 55.49473\n\n\nA 75% CI, just for fun. This is a shorter interval than the 95% one, as it should be.\nLikewise there is a ci_median0 that takes a column and an optional confidence level:\n\nwith(times, ci_median0(time))\n\n[1] 44.30747 57.59766\n\nwith(times, ci_median0(time, conf.level = 0.75))\n\n[1] 46.20444 55.49473\n\n\nwith the same results. Try ci_median first, and if it doesn’t work, try ci_median0.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "sign.html#chocolate-chips-1",
    "href": "sign.html#chocolate-chips-1",
    "title": "7  The sign test",
    "section": "7.7 Chocolate chips",
    "text": "7.7 Chocolate chips\nA famous cookie manufacturer claims that their bags of chocolate chip cookies contain “more than 1100 chocolate chips on average”. A diligent group of students buys 16 bags of these cookies and counts the number of chocolate chips in each bag. The results are in http://ritsokiguess.site/datafiles/chips.txt.\n\nRead in and display (some of) the data.\n\nSolution\nI’ll pretend it’s a .csv this time, just for fun. Give the data frame a name different from chips, so that you don’t get confused:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/chips.txt\"\nbags &lt;- read_csv(my_url)\n\nRows: 16 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): chips\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbags\n\n\n\n  \n\n\n\nThat looks sensible.\n\\(\\blacksquare\\)\n\nBuild your own sign test in R for testing that the median is 1100 chocolate chips, against the alternative that it is greater. (Do this as in class: count the appropriate thing, compare it with an appropriate binomial distribution, and obtain a P-value.\n\nSolution\nThe null median is 1100, so we count the number of values above and below:\n\nbags %&gt;% count(chips&lt;1100)\n\n\n\n  \n\n\n\nThe un-standard thing there is that we can put a logical condition directly into the count. If you don’t think of that, you can also do this, which creates a new variable less that is TRUE or FALSE for each bag appropriately:\n\nbags %&gt;% mutate(less=(chips&lt;1100)) %&gt;% count(less)\n\n\n\n  \n\n\n\nor the more verbose\n\nbags %&gt;% mutate(less=(chips&lt;1100)) %&gt;%\ngroup_by(less) %&gt;% summarize(howmany=n())\n\n\n\n  \n\n\n\nJust one value below, with all the rest above. Getting the right P-value, properly, requires some careful thought (but you will probably get the right answer anyway). If the alternative hypothesis is true, and the median is actually bigger than 1100 (say, 1200), you would expect half the data values to be bigger than 1200 and half smaller. So more than half the data values would be bigger than 1100, and fewer than half of them would be less than 1100. So, if we are going to reject the null (as it looks as if we will), that small number of values below 1100 is what we want.\nThe P-value is the probability of a value 1 or less in a binomial distribution with \\(n=16, p=0.5\\):\n\nsum(dbinom(0:1,16,0.5))\n\n[1] 0.0002593994\n\n\nOr, equivalently, count up from 15:\n\nsum(dbinom(15:16,16,0.5))\n\n[1] 0.0002593994\n\n\nThis is correctly one-sided, so we don’t have to do anything with it.\n\\(\\blacksquare\\)\n\nUse my R package smmr to reproduce your sign test above, and verify that you get consistent results. (See the maze-design question for instructions on installing this, if you haven’t yet.)\n\nSolution\nThis will mean reading the output carefully:\n\nlibrary(smmr)\nsign_test(bags,chips,1100)\n\n$above_below\nbelow above \n    1    15 \n\n$p_values\n  alternative      p_value\n1       lower 0.9999847412\n2       upper 0.0002593994\n3   two-sided 0.0005187988\n\n\nThis time, we’re doing a one-sided test, specifically an upper-tail test, since we are looking for evidence that the median is greater than 1100. The results are exactly what we got “by hand”: 15 values above and one below, and a P-value (look along the upper line) of 0.00026. The two-sided P-value of 0.00052 rounds to the same 0.0005 as SAS got.\nAlternatively, you can do this:\n\nsign_test0(bags$chips,1100)\n\n$above_below\nbelow above \n    1    15 \n\n$p_values\n  alternative      p_value\n1       lower 0.9999847412\n2       upper 0.0002593994\n3   two-sided 0.0005187988\n\n\nwith the same result (but only go this way if you need to).\n\\(\\blacksquare\\)\n\nUse smmr to obtain a 95% confidence interval for the median number of chocolate chips per bag of cookies.\n\nSolution\nOnce everything is in place, this is simplicity itself:\n\nci_median(bags,chips)\n\n[1] 1135.003 1324.996\n\n\n1135 to 1325. I would round these off to whole numbers, since the data values are all whole numbers. These values are all above 1100, which supports the conclusion we got above that the median is above 1100. This is as it should be, because the CI is “all those medians that would not be rejected by the sign test”.\nOr,\n\nci_median0(bags$chips)\n\n[1] 1135.003 1324.996\n\n\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "sign.html#the-power-of-the-sign-test-1",
    "href": "sign.html#the-power-of-the-sign-test-1",
    "title": "7  The sign test",
    "section": "7.8 The power of the sign test",
    "text": "7.8 The power of the sign test\nI’ve mentioned several times that the sign test has less power than the \\(t\\)-test. Let’s investigate this with a specific example.\nLet’s suppose we are testing \\(H_0: \\mu=40\\) against \\(H_a: \\mu \\ne 40\\), where \\(\\mu\\) is the population mean (and median, as we shall see). Our population actually has a normal distribution with mean 50 and SD 15, so that the null hypothesis is wrong and we want to reject it most of the time. On the other hand, the population actually is normally-distributed and so the \\(t\\)-test is the right one to use.\n(This is an old question, so I tackle the simulated power differently than I did it in class this time. But see if you can follow what I do here.)\n\nUse power.t.test to find the probability that a \\(t\\)-test correctly rejects the null hypothesis using a sample size of \\(n=10\\).\n\nSolution\n\npower.t.test(delta=50-40,n=10,sd=15,type=\"one.sample\",alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 10\n          delta = 10\n             sd = 15\n      sig.level = 0.05\n          power = 0.4691805\n    alternative = two.sided\n\n\nThe power is 0.469. Not great, but we’ll see how this stacks up against the sign test.\n\\(\\blacksquare\\)\n\nWhat code in R would draw a random sample of size 10 from the true population distribution and save the sample in a variable?\n\nSolution\nThe data actually have a normal distribution with mean 50 and SD 15, so we use rnorm with this mean and SD, obtaining 10 values:\n\nx=rnorm(10,50,15)  \nx\n\n [1] 30.17079 64.00529 49.19231 68.83447 59.37662 57.36909 53.65464 66.93529\n [9] 50.22649 45.52094\n\n\n\\(\\blacksquare\\)\n\nWhat code would count how many of the sampled values are less than 40 and how many are greater (or equal)?\n\nSolution\nThe way we know this is to put x into a data frame first:\n\ntibble(x) %&gt;% count(x&lt;40)\n\n\n\n  \n\n\n\n2 values less (and 8 greater-or-equal).\n\\(\\blacksquare\\)\n\nIt turns out the sign test would reject \\(H_0: M=40\\) against \\(H_a: M \\ne 40\\) at \\(\\alpha=0.05\\) if the smaller of the numbers in the last part is 1 or less. (\\(M\\) is the population median.) Add to your pipeline to obtain TRUE if you should reject the null for your data and FALSE otherwise.\n\nSolution\nThis is actually easier than you might think. The output from count is a data frame with a column called n, whose minimum value you want. I add to my pipeline:\n\ntibble(x) %&gt;% count(x&lt;40) %&gt;%\nsummarize(the_min=min(n)) %&gt;%\nmutate(is_rejected=(the_min&lt;=1))\n\n\n\n  \n\n\n\nThis will fail sometimes. If all 10 of your sample values are greater than 40, which they might turn out to be, you’ll get a table with only one line, FALSE and 10; the minimum of the n values is 10 (since there is only one), and it will falsely say that you should not reject. The fix is\n\ntibble(x) %&gt;% count(x&lt;40) %&gt;%\nsummarize(the_min=min(n)) %&gt;%\nmutate(is_rejected=(the_min&lt;=1 | the_min==10))\n\n\n\n  \n\n\n\nThe above is almost the right thing, but not quite: we only want that value that I called is_rejected, rather than the whole data frame, so a pull will grab it:\n\ntibble(x) %&gt;% count(x&lt;40) %&gt;%\nsummarize(the_min=min(n)) %&gt;%\nmutate(is_rejected=(the_min&lt;=1 | the_min==10)) %&gt;%\npull(is_rejected)\n\n[1] TRUE\n\n\nYou might be wondering where the “1 or less” came from. Getting a P-value for the sign test involves the binomial distribution: if the null is correct, each data value is independently either above or below 40, with probability 0.5 of each, so the number of values below 40 (say) is binomial with \\(n=10\\) and \\(p=0.5\\). The P-value for 1 observed value below 40 and the rest above is\n\n2*pbinom(1,10,0.5)  \n\n[1] 0.02148438\n\n\nwhich is less than 0.05; the P-value for 2 values below 40 and the rest above is\n\n2*pbinom(2,10,0.5)    \n\n[1] 0.109375\n\n\nwhich is bigger than 0.05.\nYou might have encountered the term “critical region” for a test. This is the values of your test statistic that you would reject the null hypothesis for. In this case, the critical region is 1 and 0 observations below 40, along with 1 and 0 observations above 40.\nWhen you’re thinking about power, I think it’s easiest to think in terms of the critical region (rather than directly in terms of P-values) since you have a certain \\(\\alpha\\) in mind all the way through, 0.05 in the power examples that I’ve done. The steps are then:\n\nWork out the critical region for your test, the values of the test statistic (or sample mean or sample count) that would lead to rejecting the null hypothesis.\nUnder your particular alternative hypothesis, find the probability of falling into your critical region.\n\nWhen I say “work out”, I mean either calculating (along the lines of STAB57), or simulating, as we have done here.\n\\(\\blacksquare\\)\n\nSimulate the above process 1000 times: draw a random sample from a normal distribution of size 10 with mean 50 and SD 15, count the number of values below 40, reject if the minimum of those is 0, 1, 9, or 10, then count the number of rejections out of 1000.\n\nSolution\nSet up a dataframe with a column (called, maybe, sim) that counts the number of simulations you are doing, and then use rowwise to take a random sample in each row and extract what you need from it.\nI start with setting the random number seed, so it comes out the same each time. That way, if I rerun the code, my answers are the same (and I don’t have to change my discussion of them.)\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rnorm(10, 50, 15)))\n\n\n\n  \n\n\n\nEach sample has 10 values in it, not just one, so you need the list around the rnorm. Note that sample is labelled as a list-column.\nNow we have to count how many of the sample values are less than 40:\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rnorm(10, 50, 15))) %&gt;% \n  mutate(less = list(sample&lt;40)) %&gt;% \n  mutate(counted = sum(less)) \n\n\n\n  \n\n\n\nThis is a bit of a programmer’s trick. In R, less contains a vector of 10 TRUE or FALSE values, according to whether the corresponding value in sample is less than 40 or not. In R (and many other programming languages), the numeric value of TRUE is 1 and of FALSE is 0, so you count how many TRUE values there are by adding them up. To verify that this worked, we should unnest sample and less:\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rnorm(10, 50, 15))) %&gt;% \n  mutate(less = list(sample&lt;40)) %&gt;% \n  mutate(counted = sum(less)) %&gt;% \n  unnest(c(sample, less))\n\n\n\n  \n\n\n\nIn the first sample, 38.8, 39.5, and 33.8 are less than 40, correctly identified so in less, and the counted column shows that the first sample did indeed have 3 values less than 40. You can check a few of the others as well, enough to convince yourself that this is working.\nNext, the sign test will reject if there are 0, 1, 9 or 10 values less than 40 (you might be guessing that the last two will be pretty unlikely), so make a column called reject that encapsulates that, and then count how many times you rejected in your simulations. I don’t need my unnest any more; that was just to check that everything was working so far:\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rnorm(10, 50, 15))) %&gt;% \n  mutate(less = list(sample&lt;40)) %&gt;% \n  mutate(counted = sum(less)) %&gt;% \n  mutate(reject = (counted&lt;=1 | counted &gt;= 9)) %&gt;% \n  count(reject)\n\n\n\n  \n\n\n\nMy simulated power is 0.243\nThis is all liable to go wrong the first few times, so make sure that each line works before you go on to the next, as I did. While you’re debugging, try it with a small number of random samples like 5. (It is smart to have a variable called nsim which you set to a small number like 5 when you are testing, and than to 1000 when you run the real thing, so that the first line of the pipeline is then tibble(sim = 1:nsim).)\nIf you were handing something like this in, I would only want to see your code for the final pipeline that does everything, though you could and should have some words that describe what you did.\nI’m now thinking a better way to do this is to write a function that takes a sample (in a vector) and returns a TRUE or FALSE according to whether or not a median of 40 would be rejected for that sample:\n\nis_reject=function(x) {\n  tibble(x=x) %&gt;%\n    mutate(counted = (x &lt; 40)) %&gt;% \n    summarize(below = sum(counted)) %&gt;% \n    summarize(is_rejected = (below&lt;=1 | below&gt;=9)) %&gt;% \n    pull(is_rejected)\n}\nis_reject(c(35, 45, 55))\n\n[1] TRUE\n\nis_reject(c(35, 38, 45, 55))\n\n[1] FALSE\n\n\nNow, we have to use that:\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rnorm(10, 50, 15))) %&gt;% \n  mutate(reject = is_reject(sample)) %&gt;% \n  count(reject)\n\n\n\n  \n\n\n\nThis is a bit cleaner because the process of deciding whether each sample leads to a rejection of the median being 40 has been “outsourced” to the function, and the pipeline with the rowwise is a lot cleaner: take a sample, decide whether that sample leads to rejection, and count up the rejections.\n\\(\\blacksquare\\)\n\nWhich is more powerful in this case, the sign test or the \\(t\\)-test? How do you know?\n\nSolution\nThe power of the sign test is estimated as 0.243, which is quite a bit less than the power of the \\(t\\)-test, which we found back in (a) to be 0.469. So the \\(t\\)-test, in this situation where it is valid, is the right test to use: it is (i) valid and (ii) more powerful. So the \\(t\\)-test is more powerful. One way to think about how much more powerful is to ask “how much smaller of a sample size would be needed for the \\(t\\)-test to have the same power as this sign test?” The power of my sign test was 0.243, so in power.t.test we set power equal to that and omit the sample size n:\n\npower.t.test(delta=50-40,power=0.243,sd=15,type=\"one.sample\",alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 5.599293\n          delta = 10\n             sd = 15\n      sig.level = 0.05\n          power = 0.243\n    alternative = two.sided\n\n\nA sample of size 6 gives the same power for the \\(t\\)-test that a sample of size 10 does for the sign test. The ratio of these two sample sizes is called the relative efficiency of the two tests: in this case, the \\(t\\)-test is \\(10/6=1.67\\) times more efficient. The data that you have are being used “more efficiently” by the \\(t\\)-test. It is possible to derive2\nthe limiting relative efficiency of the \\(t\\) test relative to the sign test when the data are actually normal, as the sample size gets larger. This turns out not to depend on how far wrong the null is (as long as it is the same for both the \\(t\\)-test and the sign test). This “asymptotic relative efficiency” is \\(\\pi/2=1.57\\). Our relative efficiency for power 0.243, namely 1.67, was pretty close to this, even though our sample sizes 10 and 6 are not especially close to infinity. This says that, if your data are actually from a normal distribution, you do a lot better to use the \\(t\\)-test than the sign test, because the sign test is wasteful of data (it only uses above/below rather than the actual values).\nExtra: if your data are not from a normal distribution, then the story can be very different. Of course you knew I would investigate this. There is a distribution called the “Laplace” or “double exponential” distribution, that has very long tails.3 The distribution is not in base R, but there is a package called smoothmest that contains a function rdoublex to generate random values from this distribution. So we’re going to do a simulation investigation of the power of the sign test for Laplace data, by the same simulation technique that we did above. Like the normal, the Laplace distribution is symmetric, so its mean and median are the same (which makes our life easier).4\nLet’s test the hypothesis that the median is zero. We’ll suppose that the true median is 0.5 (this is called mu in rdoublex). The first problem we run into is that we can’t use power.t.test because they assume normal data, which we are far from having. So we have to do two simulations: one to simulate the power of the \\(t\\) test, and one to simulate the power of the sign test.\nTo simulate the \\(t\\) test, we first have to generate some Laplace data with the true mean of 0.5. We’ll use a sample size of 50 throughout these simulations.\n\nlibrary(smoothmest)\nrl &lt;- rdoublex(50,mu=0.5)\nrl\n\n [1] -0.33323285  0.70569291 -1.22513053  0.68517708  0.87221482  0.49250051\n [7]  0.26700527  1.90236874  0.53288312  1.37374732  0.72743434  0.46634071\n[13]  0.43581431 -0.01545866  0.18594908 -0.40403202 -0.13540289  0.83862694\n[19] -0.23360644 -0.74050354  2.92089551 -2.72173880  0.51571185  1.23636045\n[25]  0.82921382  1.72456334  0.07903058  0.74789589  0.90487190  2.52310082\n[31]  3.13629814  0.81851434  0.74615575 -0.26068744  2.70683355  1.46981530\n[37]  1.45646489  1.20232517  6.65249860 -0.51575026 -0.07606399  2.11338640\n[43] -1.20427995  1.70986104 -1.66466321  0.55346854  0.33908531  0.72100677\n[49]  0.92025176  0.98922656\n\n\nThis seems to have some unusual values, far away from zero:\n\ntibble(rl) %&gt;%\nggplot(aes(sample=rl))+\nstat_qq()+stat_qq_line()\n\n\n\n\nYou see the long tails compared to the normal.\nNow, we feed these values into t.test and see whether we reject a null median of zero (at \\(\\alpha=0.05\\)):\n\ntt &lt;- t.test(rl)  \ntt\n\n\n    One Sample t-test\n\ndata:  rl\nt = 3.72, df = 49, p-value = 0.0005131\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.3399906 1.1388911\nsample estimates:\nmean of x \n0.7394408 \n\n\nOr we can just pull out the P-value and even compare it to 0.05:\n\npval &lt;- tt$p.value  \npval\n\n[1] 0.0005130841\n\nis.reject &lt;- (pval&lt;=0.05)\nis.reject\n\n[1] TRUE\n\n\nThis one has a small P-value and so the null median of 0 should be (correctly) rejected.\nWe’ll use these ideas to simulate the power of the \\(t\\)-test for these data, testing a mean of 0. This uses the same ideas as for any power simulation; the difference here is the true distribution:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rdoublex(50, mu = 0.5))) %&gt;% \n  mutate(t_test = list(t.test(sample, mu = 0))) %&gt;% \n  mutate(t_pval = t_test$p.value) %&gt;% \n  count(t_pval &lt;= 0.05)\n\n\n\n  \n\n\n\nAnd now we simulate the sign test. Since what we want is a P-value from a vector, the easiest way to do this is to use pval_sign0 from smmr, which returns exactly the two-sided P-value that we want, so that the procedure is a step simpler:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rdoublex(50, mu = 0.5))) %&gt;% \n  mutate(sign_pval = pval_sign0(0, sample)) %&gt;% \n  count(sign_pval &lt;= 0.05)\n\n\n\n  \n\n\n\nFor data from this Laplace distribution, the power of this \\(t\\)-test is 0.696, but the power of the sign test on the same data is 0.761, bigger. For Laplace-distributed data, the sign test is more powerful than the \\(t\\)-test.\nThis is not to say that you will ever run into data that comes from the Laplace distribution. But the moral of the story is that the sign test can be more powerful than the \\(t\\)-test, under the right circumstances (and the above simulation is the “proof” of that statement). So a blanket statement like “the sign test is not very powerful” needs to be qualified a bit: when your data come from a sufficiently long-tailed distribution, the sign test can be more powerful relative to the \\(t\\)-test than you would think.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "sign.html#ben-roethlisberger-1",
    "href": "sign.html#ben-roethlisberger-1",
    "title": "7  The sign test",
    "section": "7.9 Ben Roethlisberger",
    "text": "7.9 Ben Roethlisberger\nBen Roethlisberger plays (American) football for the Pittsburgh Steelers. He plays as a quarterback, which means that his job is to throw (pass) the ball so that one of his teammates can catch it. Each time he makes a pass that is caught, this is called a “completion”, and the team coaches are interested in his average number of completions per game (this average could be the mean or the median).\nIn 2010, Roethlisberger was suspended for the first four games of the season, and there was concern that this might affect his performance (in terms of the number of passes completed in the games after he returned). The Pittsburgh Steelers did not play in week 5 of the 2010 season; the season is 17 weeks long (one game per week) and each team has one week in which they do not play.\nThe data are here. There are four columns: the year (always 2010), the week number of the season that the game was played in, the name of the opposing team, and the number of completed passes by Roethlisberger in the game.\n\nRead in and display (some of) the data. Do you have what you were expecting?\n\nSolution\nReading in is the usual, noting that this is a .csv:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/roethlisberger.csv\"\nben &lt;- read_csv(my_url)\n\nRows: 12 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): opponent\ndbl (3): season, week, completed\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nben\n\n\n\n  \n\n\n\nSince “Roethlisberger” is a lot to type every time, I called the dataframe by his first name.\nI am showing all 12 rows here; you are probably seeing only 10, and will have to scroll down to see the last two.\nI have the four variables promised, and I also have a sensible number of rows. In particular, there is no data for weeks 1–4 (the suspension) and for week 5 (in which the team did not play), but there is a number of passes completed for all the other weeks of the season up to week 17. (If Roethlisberger had not played in any other games, you can expect that I would have told you about it.)\nExtra: I did some processing to get the data to this point. I wanted to ask you about the 2010 season, and that meant having the 2009 data to compare it with. So I went here, scrolled down to Schedule and Game Results, and clicked on each of the Boxscores to get the player stats by game. Then I made a note of the opponent and the number of passes completed, and did the same for 2010. I put them in a file I called r1.txt, in aligned columns, and read that in. (An alternative would have been to make a spreadsheet and save that as a .csv, but I already had R Studio open.) Thus:\n\nr0 &lt;- read_table(\"r1.txt\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  season = col_double(),\n  week = col_double(),\n  opponent = col_character(),\n  completed = col_double()\n)\n\nr0\n\n\n\n  \n\n\n\nI was curious about the season medians (for reasons you see later), thus:\n\nr0 %&gt;% group_by(season) %&gt;% summarise(med = median(completed))\n\n\n\n  \n\n\n\nYou will realize that my asserted average for “previous seasons” is close to the median for 2009. Here is where I have to admit that I cheated. It actually is the median for 2009, except that there are some games in 2010 where Roethlisberger had 22 completed passes and I didn’t want to mess the sign test up (I talk more about this later). So I made it 22.5, which is a possible value for the median of an even number of whole-number values.\nAnyway, the last thing to do is to grab only the rows for 2010 and save them for you. This uses filter to select only the rows for which something is true:\n\nlibrary(smmr)\nr0 %&gt;% filter(season==2010) -&gt; r1\nwrite_csv(r1, \"roethlisberger.csv\")\n\n\\(\\blacksquare\\)\n\nMake a suitable graph of the number of completed passes, and explain briefly why you would have some doubts about using \\(t\\)-procedures in this situation.\n\nSolution\nDon’t be tempted to think too hard about the choice of graph (though I talk more about this below). One quantitative variable, so a histogram again. There are only 12 observations, so 5 bins is about as high as you should go:\n\nggplot(ben, aes(x=completed)) + geom_histogram(bins=5)\n\n\n\n\nThis one shows an outlier: there is one number of completed passes that is noticeably higher than the rest. A normal distribution doesn’t have outliers, and so this, coupled with a small sample in which normality is important, means that we should not be using a \\(t\\)-test or confidence interval.\nIf you chose a different number of bins, you might get a different look. Here’s 4 bins:\n\nggplot(ben, aes(x=completed)) + geom_histogram(bins=4)\n\n\n\n\nThat looks more like right-skewness, but the conclusion is the same.\nExtra: if you have read, for example, Problem 6.1 in PASIAS, you’ll have seen that another possibility is a one-group boxplot. This might have been the context in which you first saw the boxplot, maybe at about the time you first saw the five-number summary, but I don’t talk about that so much in this course because ggplot boxplots have both an x and a y, and it makes more sense to think about using boxplots to compare groups. But, you can certainly get R to make you a one-sample boxplot. What you do is to set the grouping variable to a “dummy” thing like the number 1:\n\nggplot(ben, aes(x=1, y=completed)) + geom_boxplot()\n\n\n\n\nand then you ignore the \\(x\\)-axis.\nThis really shows off the outlier; it is actually much bigger than the other observations. It didn’t show up so much on the histograms because of where the bin boundaries happened to come. On the four-bin histogram, the highest value 30 was in the 27.5–32.5 bin, and the second-highest value 23 was at the bottom of the 22.5–27.5 bin. So the highest and second-highest values looked closer together than they actually were.\nIf you have been reading ahead, you might also be thinking about a normal quantile plot. That is for specifically assessing normality, and here this is something that interests us, because a \\(t\\)-test will be doubtful if the normality fails:\n\nggplot(ben, aes(sample=completed)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis again shows off the outlier at the high end. It is a reasonable choice of plot here because normality is of specific interest to us.\nA note: you are absolutely not required to read ahead to future lectures. Each assignment can be done using the material in the indicated lectures only. If you want to use something from future lectures, go ahead, but make sure you are using it appropriately.\nDon’t be tempted to plot the number of completed passes against something like week number:\n\nggplot(ben, aes(x=week, y=completed)) + geom_point()\n\n\n\n\nThat is quite interesting (a mostly increasing trend over weeks, with the outlier performance in week 10), but it doesn’t tell us what we want to know here: namely, is a \\(t\\)-test any good?\n\\(\\blacksquare\\)\n\nRun a sign test to compare Roethlisberger’s performance in 2010 with his previous average of 22.5 completions per game. What do you conclude?\n\nSolution\nUse smmr, dataframe, column, null median:\n\nsign_test(ben, completed, 22.5)\n\n$above_below\nbelow above \n   10     2 \n\n$p_values\n  alternative    p_value\n1       lower 0.01928711\n2       upper 0.99682617\n3   two-sided 0.03857422\n\n\nI am looking for any change, so for me, a two-sided test is appropriate. If you think this is one-sided, make a case for your side, and then go ahead.\nMy P-value is 0.039, so I can reject the null hypothesis (that the median number of passes completed is 22.5) and conclude that it has changed in 2010.\n(You might hypothesize that this is the result of a decrease in confidence, that he is either throwing fewer passes, or the ones that he is throwing are harder to catch. If you know about football, you might suspect that Roethlisberger was actually passing too much, including in situations where he should have handing off to the running back, instead of reading the game appropriately.)\nExtra: I said above that I cheated and made the null median 22.5 instead of 22. What happens if we make the null median 22?\n\nsign_test(ben, completed, 22)\n\n$above_below\nbelow above \n    8     2 \n\n$p_values\n  alternative   p_value\n1       lower 0.0546875\n2       upper 0.9892578\n3   two-sided 0.1093750\n\n\nFor one thing, the result is no longer significant. But looking at the table of values above and below reveals something odd: there are only ten values. What happened to the other two? What happened is that two of the data values were exactly equal to 22, so they are neither above nor below. In the sign test, they are thrown away, so that we are left with 8 values below 22 and 2 above.\nI didn’t want to make you wonder what happened, so I made the null median 22.5.\n\\(\\blacksquare\\)\n\nWhy might you have expected your sign test to come out significant, even without looking at the P-value? Explain briefly.\n\nSolution\nThe other ingredient to the sign test is how many data values are above and below the null median. You can look at the output from sign_test (the first part), or count them yourself:\n\nben %&gt;% count(completed&lt;22.5)\n\n\n\n  \n\n\n\nYou can put a logical condition (something that can be true or false) into count, or you can create a new column using ifelse (which I think I showed you somewhere):\n\nben %&gt;% mutate(side = ifelse(completed&lt;22.5, \"below\", \"above\")) %&gt;% \ncount(side)\n\n\n\n  \n\n\n\nWhichever way you do it, there seem to be a lot more values below than above, very different from a 50–50 split. Even with only 12 observations, this turns out to be enough to be significant. (If you tossed a fair coin 12 times, would you be surprised to get only 2 heads or 2 tails?)\n\\(\\blacksquare\\)\n\nObtain a 90% confidence interval for the median number of completed passes (over “all possible games played by 2010 Ben Roethlisberger”).\n\nSolution\nThis is ci_median, but with conf.level since you are not using the default level of 95%:\n\nci_median(ben, completed, conf.level = 0.90)\n\n[1] 17.00244 21.99878\n\n\n17 to 22 completed passes.\nExtra: the P-value of the sign test only changes (as the null median changes) when you get to a data point; otherwise, the number of values above and below will stay the same, and the P-value will stay the same. The data values here were all whole numbers, so the limits of the confidence interval are also whole numbers (to the accuracy of the bisection), so the interval really should be rounded off.\n\\(\\blacksquare\\)\n\nFind a 90% confidence interval for the mean number of passes completed, and explain briefly why it differs from the one for the median in the way that it does.\n\nSolution\nAll right, get the interval for the mean first:\n\nwith(ben, t.test(completed, conf.level = 0.90))\n\n\n    One Sample t-test\n\ndata:  completed\nt = 17.033, df = 11, p-value = 2.971e-09\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 17.89124 22.10876\nsample estimates:\nmean of x \n       20 \n\n\nThe 95% confidence interval for the mean goes from 17.9 to 22.1 (completions per game).\nThis is higher at both ends than the interval for the median, though possibly not as much as I expected. This is because the mean is made higher by the outlier (compared to the median), and so the CI procedure comes to the conclusion that the mean is higher.\nExtra: this is one of those cases where the bootstrap might shed some light on the sampling distribution of the sample mean:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(ben$completed, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 10)\n\n\n\n\nThis is noticeably skewed to the right (it goes further up from the peak than down), which is why the CI for the mean went up a bit higher than the one for the median.\nFinally, bootstrapping the median is not something you’d want to do, since the sign test doesn’t depend on anything being normally-distributed. This is a good thing, since bootstrapping the sample median is weird:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(ben$completed, replace = TRUE))) %&gt;% \n  mutate(my_median = median(my_sample)) %&gt;% \n  ggplot(aes(x = my_median)) + geom_histogram(bins = 30)\n\n\n\n\nThe “holes” in the distribution comes about because there are not all that many different possible sample medians when you sample with replacement. For one thing, the values are all whole numbers, so the median can only be something or something and a half. Even then, the bar heights look kind of irregular.\nI used a large number of bins to emphasize this, but even a more reasonable number looks strange:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(ben$completed, replace = TRUE))) %&gt;% \n  mutate(my_median = median(my_sample)) %&gt;% \n  ggplot(aes(x = my_median)) + geom_histogram(bins = 10)\n\n\n\n\nA sample median of 19 or 20 is more likely than one of 19.5.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "sign.html#six-ounces-of-protein-1",
    "href": "sign.html#six-ounces-of-protein-1",
    "title": "7  The sign test",
    "section": "7.10 Six ounces of protein",
    "text": "7.10 Six ounces of protein\nA company produces prepackaged diet meals. These meals are advertised as containing “6 ounces of protein per package”. A consumer organization is concerned that this is not accurate. The organization takes a random sample of 20 of these meals, and measures the protein content of each one. The data are in http://ritsokiguess.site/datafiles/protein.txt as one column.\n\nRead in and display (some of) the data.\n\nSolution\nThe usual. This is one column only, so you can pretend the columns are separated by anything at all and it will still work:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/protein.txt\"\nmeals &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  protein = col_double()\n)\n\nmeals %&gt;% arrange(protein)\n\n\n\n  \n\n\n\nGet it to work via one of the methods you’ve seen in this class (ie., not read.table); I don’t mind how you manage it.\n\\(\\blacksquare\\)\n\nMake a suitable graph of your data.\n\nSolution\nOne quantitative variable, so a histogram with a sufficiently small number of bins:\n\nggplot(meals, aes(x=protein)) + geom_histogram(bins = 5)\n\n\n\n\n\\(\\blacksquare\\)\n\nWhy might a sign test be better than a \\(t\\)-test for assessing the average amount of protein per package? Explain briefly. (“Average” here means any measure of centre.)\n\nSolution\nThe shape of the above distribution is skewed to the left, and not symmetric like a normal distribution. (If you say “the histogram is not normal”, make sure you also say how you know.) This means that the median would be a better measure of “average” (that is, centre) than the mean is, because the mean would be pulled downwards by the long tail, and the median would not. To complete the story, the sign test is a test of the median, so the sign test would be better than the \\(t\\)-test, which is a test of the mean.\nThe other thing you might consider is the sample size, 20, which might be large enough to overcome this amount of skewness, but then again it might not be. So you could say that we should be cautious and run the sign test here instead.\n\\(\\blacksquare\\)\n\nRun a suitable sign test for these data. What do you conclude?\n\nSolution\nFirst, if you have not already done so, install smmr following the instructions in the lecture notes. (This one is not just install.packages.) Then, make sure you have a library(smmr) somewhere above where you are going to use something from it. Once that is in place, remember what we were interested in: was the median protein content 6 ounces, or is there evidence that it is something different? (The “not accurate” in the question says that the median could be higher or lower, either of which would be a problem, and so we need a two-sided alternative.) Thus, the null median is 6 and we need a two-sided test, which goes this way:\n\nsign_test(meals, protein, 6)\n\n$above_below\nbelow above \n   15     5 \n\n$p_values\n  alternative    p_value\n1       lower 0.02069473\n2       upper 0.99409103\n3   two-sided 0.04138947\n\n\nThe P-value, 0.0414, is less than 0.05, so we reject the null hypothesis and conclude that the median is different from 6 ounces. The advertisement by the company is not accurate.\nMake sure you give the actual P-value you are comparing with 0.05, since otherwise your answer is incomplete. That is, you need to say more than just “the P-value is less than 0.05”; there are three P-values here, and only one of them is the right one.\nExtra: we already decided that a \\(t\\)-test is not the best here, but I am curious as to how different its P-value is:\n\nwith(meals, t.test(protein, mu=6))\n\n\n    One Sample t-test\n\ndata:  protein\nt = -4.2312, df = 19, p-value = 0.000452\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 4.946263 5.643737\nsample estimates:\nmean of x \n    5.295 \n\n\nThe conclusion is the same, but the P-value is a lot smaller. I don’t think it should really be this small; this is probably because the mean is pulled down by the left skew and so really ought not to look so far below 6. I am inclined to think that if the \\(t\\)-test were correct, its P-value ought to be between this and the one from the sign test, because the \\(t\\)-test uses the actual data values, and the sign test uses the data less efficiently (only considering whether each one is above or below the null median).\n\\(\\blacksquare\\)\n\nIn your sign test, how could you have deduced that the P-value was going to be small even without looking at any of the P-values themselves? Explain briefly.\n\nSolution\nLook at the other part of the output, the count of values above and below the null median. (You might have to click on “R Console” to see it.) If the null hypothesis was correct and the median was really 6, you’d expect to see about half the data values above 6 and about half below. But that is not what happened: there were 15 values below and only 5 above. Such an uneven split is rather unlikely if the null hypothesis was correct. So we would guess that our P-value would be small, as indeed it is.\n\\(\\blacksquare\\)\n\nObtain a 90% confidence interval for the population median protein content. What does this tell you about the reason for the rejection or non-rejection of the null hypothesis above?\n\nSolution\nThis is ci_median, but you need to be paying attention: it’s not the default 95% confidence level, so you have to specify that as well:\n\nci_median(meals, protein, conf.level = 0.90)\n\n[1] 4.905273 5.793750\n\n\nThe interval goes from 4.91 to 5.79. (The data values have one decimal place, so you could justify two decimals in the CI for the median, but anything beyond that is noise and you shouldn’t give it in your answer.5)\nThis interval is entirely below 6 (the null median), so evidently the reason that we rejected 6 as the population median is that the actual population median is less than 6.\nExtra: the CI for the median is not that different from the one for the mean, which suggests that maybe the \\(t\\)-test was not so bad after all. If you want to investigate further, you can try finding a bootstrapped sampling distribution of the sample mean, and see how non-normal it looks:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(meals$protein, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) -&gt; d\nggplot(d, aes(x = my_mean)) + geom_histogram(bins = 10)\n\n\n\n\nThat is pretty close to normal. So the \\(t\\)-test would in actual fact have been fine. To confirm, a normal quantile plot of the bootstrapped sampling distribution:\n\nggplot(d, aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nA tiny bit skewed to the left.\nBut I didn’t ask you to do this, because I wanted to give you a chance to do a sign test for what seemed like a good reason.\nExtra 2: I mentioned in an note that the endpoints of the CI for the median are actually data points, only we didn’t see it because of the accuracy to which ci_median was working. You can control this accuracy by an extra input tol. Let’s do something silly here:\n\nci_median(meals, protein, conf.level = 0.90, tol = 0.00000001)\n\n[1] 4.900001 5.799999\n\n\nThis takes a bit longer to run, since it has to get the answer more accurately, but now you can see how the interval goes from “just over 4.9” to “just under 5.8”, and it actually makes the most sense to give the interval as “4.9 to 5.8” without giving any more decimals.\nExtra 3: the reason for the confidence interval endpoints to be data values is that the interval comes from inverting the test: that is, finding the values of the population median that would not be rejected by a sign test run on our data. Recall how the sign test works: it is based on a count of how many data values are above and below the hypothesized population median. These counts are only going to change as the hypothesized median changes if you hit a data point, since that’s the only way you can change how many values are above and below.6 Thus, the only places where changing the null median changes whether or not a value for it is inside or outside the confidence interval are at data values, and thus the ends of the interval must be at (or, perhaps more strictly, just above or below) data values.\nThis is a peculiarity of using the sign test to get a CI for the median. If, say, you were to invert the \\(t\\)-test to get a confidence interval for the mean, you wouldn’t see that. (This is in fact exactly what you do to get a confidence interval for the mean, but this is not the way it is usually introduced.) The reason that the CI for the mean (based on the \\(t\\)-test) is different from the one for the median (based on the sign test) is that if you change the null hypothesis in the \\(t\\)-test, however slightly, you change the P-value (maybe only slightly, but you change it). So the CI for the mean, based on the \\(t\\)-test, is not required to have data points at its ends, and indeed usually does not. The difference is in the kind of distribution the test statistic has; the \\(t\\)-distribution is continuous, while the sign test statistic (a count of the number of values above or below something) is discrete. It’s the discreteness that causes the problems.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "sign.html#footnotes",
    "href": "sign.html#footnotes",
    "title": "7  The sign test",
    "section": "",
    "text": "You can change this by adding something like tol=1e-4 to the end of your ci-median.↩︎\nMeaning, I forget how to do it. But it has something to do with looking at alternatives that are very close to the null.↩︎\nIf you’ve ever run into the exponential distribution, you’ll recall that this is right skewed with a very long tail. The Laplace distribution looks like two of these glued back to back.↩︎\nThis is about the only way in which the normal and Laplace distributions are alike.↩︎\nThere is actually slightly more to it here: the ends of this confidence interval for the median are always data values, because of the way it is constructed, so the actual end points really ought to be given to the same number of decimals as the data, here 4.9 to 5.8. The output given is not exactly 4.9 and 5.8 because of inaccuracy in the bisection.↩︎\nThere is a technicality about what happens when the null median is exactly equal to a data value; see PASIAS for more discussion on this.↩︎"
  },
  {
    "objectID": "mood-median.html#sugar-in-breakfast-cereals",
    "href": "mood-median.html#sugar-in-breakfast-cereals",
    "title": "8  Mood median test",
    "section": "8.1 Sugar in breakfast cereals",
    "text": "8.1 Sugar in breakfast cereals\nThe data for this question are in http://ritsokiguess.site/datafiles/cereal-sugar.txt. The story here is whether breakfast cereals marketed to children have a lot of sugar in them; in particular, whether they have more sugar on average than cereals marketed to adults.\n\nRead in the data (to R) and display the data set. Do you have a variable that distinguishes the children’s cereals from the adults’ cereals, and another that contains the amount of sugar?\nCalculate the mean sugar content for each group of cereals (the adults’ ones and the children’s ones). Do they look similar or different?\nMake side-by-side boxplots of the sugar contents of the two types of cereal. What do you see that is out of the ordinary?\nExplain briefly why you would not trust a two-sample \\(t\\)-test with these data. (That is, say what the problem is, and why it’s a problem.)\nRun a suitable test to see whether the “typical” amount of sugar differs between adult’s and children’s cereals. Justify the test that you run. (You can use the version of your test that lives in a package, if that is easier for you.) What do you conclude, in the context of the data?"
  },
  {
    "objectID": "mood-median.html#fear-of-math",
    "href": "mood-median.html#fear-of-math",
    "title": "8  Mood median test",
    "section": "8.2 Fear of math",
    "text": "8.2 Fear of math\nTwo new short courses have been proposed for helping students who suffer from severe math phobia. The courses are labelled A and B. Ten students were randomly allocated to one of these two courses, and each student’s score on a math phobia test was recorded after they completed their course. The math phobia test produces whole-number scores between 0 and 10, with a higher score indicating a greater fear of mathematics. The data can be found in link. We start with R for this question.\n\nRead in the data and check, however you like, that you have 10 observations, 5 from each course.\nDo a two-sample \\(t\\)-test to assess whether there is a difference in mean phobia scores after the students have taken the two courses. What do you conclude? (You have no 1 reason to suppose that a particular one of the tests will produce a higher mean than the other, so do a two-sided test.)\nDraw boxplots of the math phobia scores for each group (one line of code). What is the most striking thing that you notice?\nExplain briefly why a \\(t\\)-test would not be good for these data. (There are two things that you need to say.)\nRun a suitable test to compare the “typical” scores for the two courses. (You can use the version from a package rather than building your own.) What do you conclude?"
  },
  {
    "objectID": "mood-median.html#medical-instructions",
    "href": "mood-median.html#medical-instructions",
    "title": "8  Mood median test",
    "section": "8.3 Medical instructions",
    "text": "8.3 Medical instructions\nDo people understand medical instructions better at certain times of the day? In a study, students in a grade 12 class are randomly divided into two groups, A and B. All students see a video describing how to use an infant forehead thermometer. The students in Group A see the video at 8:30 am, while the students in Group B see the same video at 3:00 pm (on the same day). The next day, all the students are given a test on the material in the video (graded out of 100). The observed scores are in link (values separated by spaces).\n\nRead the data into R and display the (first ten) values.\nObtain a suitable plot that will enable you to assess the assumptions for a two-sample \\(t\\)-test.\nWhy might you have doubts about using a two-sample \\(t\\)-test here?\nRun Mood’s median test as in class (without using smmr). What do you conclude, in the context of the data? What recommendation would you make about the time of day to see the video? (You might get a warning about “chisquared approximation being incorrect”, which you can ignore here.)\nRun Mood’s median test on these data using my smmr package, and verify that you get the same answer."
  },
  {
    "objectID": "mood-median.html#handspans-revisited",
    "href": "mood-median.html#handspans-revisited",
    "title": "8  Mood median test",
    "section": "8.4 Handspans revisited",
    "text": "8.4 Handspans revisited\nTake your right hand, and stretch the fingers out as far as you can. The distance between the tip of your thumb and the tip of your little (pinky) finger is your handspan. The students in a Statistics class at Penn State measured their handspans and also whether they identified as male or female. The data are at http://ritsokiguess.site/datafiles/handspan.txt, with handspans measured in inches. We want to see whether male students have a larger mean handspan than female students.\n\nRead in and display (some of) the data.\nMake a suitable (facetted) normal quantile plot of the data. (Bear in mind what is supposed to have a normal distribution.)\nDiscuss briefly whether you might prefer to use Mood’s median test to compare the handspans of the male and female students, compared to a two-sample \\(t\\)-test.\nRun Mood’s median test. What do you conclude from the test, in the context of the data?\n\nMy solutions follow:"
  },
  {
    "objectID": "mood-median.html#sugar-in-breakfast-cereals-1",
    "href": "mood-median.html#sugar-in-breakfast-cereals-1",
    "title": "8  Mood median test",
    "section": "8.5 Sugar in breakfast cereals",
    "text": "8.5 Sugar in breakfast cereals\nThe data for this question are in http://ritsokiguess.site/datafiles/cereal-sugar.txt. The story here is whether breakfast cereals marketed to children have a lot of sugar in them; in particular, whether they have more sugar on average than cereals marketed to adults.\n\nRead in the data (to R) and display the data set. Do you have a variable that distinguishes the children’s cereals from the adults’ cereals, and another that contains the amount of sugar?\n\nSolution\n\nmy_url=\"http://ritsokiguess.site/datafiles/cereal-sugar.txt\"\ncereals=read_delim(my_url,\" \")\n\nRows: 40 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): who\ndbl (1): sugar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncereals\n\n\n\n  \n\n\n\nThe variable who is a categorical variable saying who the cereal is intended for, and the variable sugar says how much sugar each cereal has.\n\\(\\blacksquare\\)\n\nCalculate the mean sugar content for each group of cereals (the adults’ ones and the children’s ones). Do they look similar or different?\n\nSolution\ngroup_by and summarize:\n\ncereals %&gt;% group_by(who) %&gt;%\nsummarize(sugar_mean=mean(sugar))\n\n\n\n  \n\n\n\nThese means look very different, though it would be better to look at a boxplot (coming up in a moment).\n\\(\\blacksquare\\)\n\nMake side-by-side boxplots of the sugar contents of the two types of cereal. What do you see that is out of the ordinary?\n\nSolution\nThe usual:\n\nggplot(cereals,aes(x=who,y=sugar))+geom_boxplot()\n\n\n\n\nI see outliers: two high ones on the adults’ cereals, and one high and one low on the children’s cereals.\nMy thought above about the means being very different is definitely supported by the medians being very different on the boxplots. We should have no trouble declaring that the “typical” amounts of sugar in the adults’ and children’s cereals are different.\n\\(\\blacksquare\\)\n\nExplain briefly why you would not trust a two-sample \\(t\\)-test with these data. (That is, say what the problem is, and why it’s a problem.)\n\nSolution\nThe problem is the outliers (which is rather a giveaway), but the reason it’s a problem is that the two-sample \\(t\\)-test assumes (approximately) normal data, and a normal distribution doesn’t have outliers. Not only do you need to note the outliers, but you also need to say why the outliers cause a problem in this case. Anything less than that is not a complete answer.\n\\(\\blacksquare\\)\n\nRun a suitable test to see whether the “typical” amount of sugar differs between adult’s and children’s cereals. Justify the test that you run. (You can use the version of your test that lives in a package, if that is easier for you.) What do you conclude, in the context of the data?\n\nSolution\nHaving ruled out the two-sample \\(t\\)-test, we are left with Mood’s median test. I didn’t need you to build it yourself, so you can use package smmr to run it with:\n\nlibrary(smmr)\nmedian_test(cereals,sugar,who)\n\n$table\n          above\ngroup      above below\n  adults       2    19\n  children    18     1\n\n$test\n       what        value\n1 statistic 2.897243e+01\n2        df 1.000000e+00\n3   P-value 7.341573e-08\n\n\nWe conclude that there is a difference between the median amounts of sugar between the two groups of cereals, the P-value of 0.00000007 being extremely small.\nWhy did it come out so small? Because the amount of sugar was smaller than the overall median for almost all the adult cereals, and larger than the overall median for almost all the children’s ones. That is, the children’s cereals really do have more sugar.\nMood’s median test doesn’t come with a confidence interval (for the difference in population medians), because whether or not a certain difference in medians is rejected depends on what those medians actually are, and the idea of the duality of the test and CI doesn’t carry over as we would like.\nMy daughter likes chocolate Cheerios, but she also likes Shredded Wheat and Bran Flakes. Go figure. (Her current favourite is Raisin Bran, even though she doesn’t like raisins by themselves.)\nMood’s median test is the test we should trust, but you might be curious about how the \\(t\\)-test stacks up here:\n\nt.test(sugar~who,data=cereals)\n\n\n    Welch Two Sample t-test\n\ndata:  sugar by who\nt = -11.002, df = 37.968, p-value = 2.278e-13\nalternative hypothesis: true difference in means between group adults and group children is not equal to 0\n95 percent confidence interval:\n -42.28180 -29.13925\nsample estimates:\n  mean in group adults mean in group children \n              10.90000               46.61053 \n\n\nThe P-value is even smaller, and we have the advantage of getting a confidence interval for the difference in means: from about 30 to about 40 units less sugar in the adult cereals. Whatever the units were.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mood-median.html#fear-of-math-1",
    "href": "mood-median.html#fear-of-math-1",
    "title": "8  Mood median test",
    "section": "8.6 Fear of math",
    "text": "8.6 Fear of math\nTwo new short courses have been proposed for helping students who suffer from severe math phobia. The courses are labelled A and B. Ten students were randomly allocated to one of these two courses, and each student’s score on a math phobia test was recorded after they completed their course. The math phobia test produces whole-number scores between 0 and 10, with a higher score indicating a greater fear of mathematics. The data can be found in link. We start with R for this question.\n\nRead in the data and check, however you like, that you have 10 observations, 5 from each course.\n\nSolution\nThis doesn’t need much comment:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/mathphobia.txt\"\nmath &lt;- read_delim(my_url, \" \")\n\nRows: 10 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): course\ndbl (1): phobia\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmath\n\n\n\n  \n\n\n\nThis will do, counting the a and b. Or, to save yourself that trouble:\n\nmath %&gt;% count(course)\n\n\n\n  \n\n\n\nFive each. The story is to get the computer to do the grunt work for you, if you can make it do so. Other ways:\n\nmath %&gt;% group_by(course) %&gt;% summarize(count = n())\n\n\n\n  \n\n\n\nand this:\n\nwith(math, table(course))\n\ncourse\na b \n5 5 \n\n\ngiving the same answer. Lots of ways.\nExtra: there is an experimental design issue here. You might have noticed that each student did only one of the courses. Couldn’t students do both, in a matched-pairs kind of way? Well, it’s a bit like the kids learning to read in that if the first of the courses reduces a student’s anxiety, the second course won’t appear to do much good (even if it actually would have been helpful had the student done that one first). This is the same idea as the kids learning to read: once you’ve learned to read, you’ve learned to read, and learning to read a second way won’t help much. The place where matched pairs scores is when you can “wipe out” the effect of one treatment before a subject gets the other one. We have an example of kids throwing baseballs and softballs that is like that: if you throw one kind of ball, that won’t affect how far you can throw the other kind.\n\\(\\blacksquare\\)\n\nDo a two-sample \\(t\\)-test to assess whether there is a difference in mean phobia scores after the students have taken the two courses. What do you conclude? (You have no 2 reason to suppose that a particular one of the tests will produce a higher mean than the other, so do a two-sided test.)\n\nSolution\nA two-sided test is the default, so there is not much to do here:\n\nt.test(phobia ~ course, data = math)\n\n\n    Welch Two Sample t-test\n\ndata:  phobia by course\nt = 0.83666, df = 4.4199, p-value = 0.4456\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -3.076889  5.876889\nsample estimates:\nmean in group a mean in group b \n            6.8             5.4 \n\n\nThe P-value of 0.4456 is nowhere near less than 0.05, so there is no evidence at all that the\nmean math phobia scores are different between the two courses.\n\\(\\blacksquare\\)\n\nDraw boxplots of the math phobia scores for each group (one line of code). What is the most striking thing that you notice?\n\nSolution\n\nggplot(math, aes(x = course, y = phobia)) + geom_boxplot()\n\n\n\n\nBoxplot a is just weird. The bar across the middle is actually at the top, and it has no bottom. (Noting something sensible like this is enough.) Boxplot b is hugely spread out.3\nBy way of explanation: the course a scores have a number of values equal so that the 3rd quartile and the median are the name, and also that the first quartile and the minimum value are the same:\n\ntmp &lt;- math %&gt;% filter(course == \"a\")\ntmp %&gt;% count(phobia)\n\n\n\n  \n\n\nsummary(tmp)\n\n    course              phobia   \n Length:5           Min.   :6.0  \n Class :character   1st Qu.:6.0  \n Mode  :character   Median :7.0  \n                    Mean   :6.8  \n                    3rd Qu.:7.0  \n                    Max.   :8.0  \n\n\nThe phobia scores from course A are two 6’s, two 7’s and an 8. The median and third quartile are both 7, and the first quartile is the same as the lowest value, 6.\nTechnique note: I wanted to do two things with the phobia scores from course A: count up how many of each score, and show you what the five-number summary looks like. One pipe won’t do this (the pipe “branches”), so I saved what I needed to use, before it branched, into a data frame tmp and then used tmp twice. Pipes are powerful, but not all-powerful.\n\\(\\blacksquare\\)\n\nExplain briefly why a \\(t\\)-test would not be good for these data. (There are two things that you need to say.)\n\nSolution\nThe easiest way to structure this is to ask yourself first what the \\(t\\)-test needs, and second whether you have it. The \\(t\\)-test assumes (approximately) normal data. The boxplot for group a doesn’t even look symmetric, and the one for group b has an oddly asymmetric box. So I think the normality is in question here, and therefore another test would be better. (This is perhaps a bit glib of an answer, since there are only 5 values in each group, and so they can certainly look non-normal even if they actually are normal, but these values are all integers, so it is perhaps wise to be cautious.) We have the machinery to assess the normality for these, in one shot:\n\nggplot(math, aes(sample = phobia)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~course, ncol = 1, scales = \"free\")\n\n\n\n\nI don’t know what you make of those, but they both look pretty straight to me (and there are only five observations, so it’s hard to judge). Course b maybe has a “hole” in it (three large values and two small ones). Maybe. I dunno. What I would really be worried about is outliers, and at least we don’t have those. I mentioned in class that the \\(t\\)-tests are robust to non-normality. I ought to have expanded on that a bit: what really makes the \\(t\\)-test still behave itself with non-normality is when you have large samples, that is, when the Central Limit Theorem has had a chance to take hold. (That’s what drives the normality not really being necessary in most cases.) But, even with small samples, exact normality doesn’t matter so much. Here, we have two tiny samples, and so we have to insist a bit more, but only a bit more, on a more-or-less normal shape in each group. (It’s kind of a double jeopardy in that the situation where normality matters most, namely with small samples, is where it’s the hardest to judge, because samples of size 5 even from a normal distribution can look very non-normal.) But, the biggest threats to the \\(t\\)-test are big-time skewness and outliers, and we are not suffering too badly from those.\n\\(\\blacksquare\\)\n\nRun a suitable test to compare the “typical” scores for the two courses. (You can use the version from a package rather than building your own.) What do you conclude?\n\nSolution\nThis is an invite to use smmr:\n\nlibrary(smmr)\nmedian_test(math, phobia, course)\n\n$table\n     above\ngroup above below\n    a     1     2\n    b     2     2\n\n$test\n       what     value\n1 statistic 0.1944444\n2        df 1.0000000\n3   P-value 0.6592430\n\n\nWe are nowhere near rejecting equal medians; in fact, both courses are very close to 50–50 above and below the overall median.\nIf you look at the frequency table, you might be confused by something: there were 10 observations, but there are only \\(1+2+2+2=7\\) in the table. This is because three of the observations were equal to the overall median, and had to be thrown away:\n\nmath %&gt;% summarize(med = median(phobia))\n\n\n\n  \n\n\nmath %&gt;% count(phobia)\n\n\n\n  \n\n\n\nThe overall median was 7. Because the actual data were really discrete (the phobia scores could only be whole numbers), we risked losing a lot of our data when we did this test (and we didn’t have much to begin with). The other thing to say is that with small sample sizes, the frequencies in the table have to be very lopsided for you to have a chance of rejecting the null. Something like this is what you’d need:\n\nx &lt;- c(1, 1, 2, 6, 6, 6, 7, 8, 9, 10)\ng &lt;- c(1, 1, 1, 1, 1, 2, 2, 2, 2, 2)\nd &lt;- tibble(x, g)\nmedian_test(d, x, g)\n\n$table\n     above\ngroup above below\n    1     0     3\n    2     4     0\n\n$test\n       what       value\n1 statistic 7.000000000\n2        df 1.000000000\n3   P-value 0.008150972\n\n\nI faked it up so that we had 10 observations, three of which were equal to the overall median. Of the rest, all the small ones were in group 1 and all the large ones were in group 2. This is lopsided enough to reject with, though, because of the small frequencies, there actually was a warning about “chi-squared approximation may be inaccurate”.4\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mood-median.html#medical-instructions-1",
    "href": "mood-median.html#medical-instructions-1",
    "title": "8  Mood median test",
    "section": "8.7 Medical instructions",
    "text": "8.7 Medical instructions\nDo people understand medical instructions better at certain times of the day? In a study, students in a grade 12 class are randomly divided into two groups, A and B. All students see a video describing how to use an infant forehead thermometer. The students in Group A see the video at 8:30 am, while the students in Group B see the same video at 3:00 pm (on the same day). The next day, all the students are given a test on the material in the video (graded out of 100). The observed scores are in link (values separated by spaces).\n\nRead the data into R and display the (first ten) values.\n\nSolution\nSeparated by spaces, so read_delim:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/forehead.txt\"\ninstr &lt;- read_delim(my_url, \" \")\n\nRows: 18 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): group\ndbl (1): score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ninstr\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nObtain a suitable plot that will enable you to assess the assumptions for a two-sample \\(t\\)-test.\n\nSolution\nWe need the values in each group to be approximately normally distributed. Side-by-side boxplots will do it:\n\nggplot(instr, aes(x = group, y = score)) + geom_boxplot()\n\n\n\n\nor, if you like, separate (facetted) normal quantile plots, which I would do this way:\n\nggplot(instr, aes(sample = score)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~group, ncol = 1)\n\n\n\n\n\\(\\blacksquare\\)\n\nWhy might you have doubts about using a two-sample \\(t\\)-test here?\n\nSolution\nWe are looking for non-normality in at least one of the groups. Here, both groups have an outlier at the low end that would be expected to pull the mean downward. I don’t think there is left-skewness here, since there is no particular evidence of the high-end values being bunched up: the problem in both cases with normality is at the low end. One way or another, I’m expecting you to have noticed the outliers. Extra: last year, when I first drew the normal quantile plots, there was no stat_qq_line, so you had to imagine where the line went if you did it this way. Without the line, these plots look somewhat curved, which would have pointed to left-skewness, but now we see that the lowest observation is too low, and maybe the second-lowest one as well, while the other observations are just fine.\n\\(\\blacksquare\\)\n\nRun Mood’s median test as in class (without using smmr). What do you conclude, in the context of the data? What recommendation would you make about the time of day to see the video? (You might get a warning about “chisquared approximation being incorrect”, which you can ignore here.)\n\nSolution\nThe overall median first:\n\ninstr %&gt;% summarize(med = median(score))\n\n\n\n  \n\n\n\n87.5, which is not equal to any of the data values (they are all integers). This will avoid any issues with values-equal-to-median later.\nThen, create and save a table of the value by group and above/below median. You can count either above or below (it comes out equivalently either way):\n\ntab &lt;- with(instr, table(group, score &gt; 87.5))\ntab\n\n     \ngroup FALSE TRUE\n    A     2    8\n    B     7    1\n\n\nThen, chi-squared test for independence (the null) or association of some kind (the alternative). The correct=F is saying not to do Yates’s correction, so that it would come out the same if you were doing it by hand (“observed minus expected, squared, divided by expected” and all that stuff).\n\nchisq.test(tab, correct = F)\n\nWarning in chisq.test(tab, correct = F): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tab\nX-squared = 8.1, df = 1, p-value = 0.004427\n\n\nThe P-value is 0.0044, which is (much) smaller than 0.05, and therefore you can reject independence and conclude association: that is, whether a student scores above or below the median depends on which group they are in, or, that the median scores are different for the two groups.\nThe warning is because the expected frequencies are on the small side (if you have done this kind of problem by hand, you might remember something about “expected frequencies less than 5”. This is that.) Here, the P-value is so small that we can afford to have it be inaccurate by a bit and still not affect the conclusion, so I think we are safe.\nAs for which group is better, well, the easiest way is to go back to your boxplots and see that the median for group A (8:30 am) is substantially higher than for group B (3:00pm). But you can also see it from your frequency table, if you displayed it:\n\ntab\n\n     \ngroup FALSE TRUE\n    A     2    8\n    B     7    1\n\n\nMost of the people in the 8:30 am group scored above the median, and most of the people in the 3:00 pm group scored below the median. So the scores at 8:30 am were better overall.\nAs I write this, it is just after 3:00 pm and I am about to make myself a pot of tea!\nExtra: about that correct=F thing. There was a point of view for a long time that when you are dealing with a \\(2 \\times 2\\) table, you can get better P-values by, before squaring “observed minus expected”, taking 0.5 away from the absolute value of the difference. This is called Yates’s correction. It is in the same spirit as the “continuity correction” that you might have encountered in the normal approximation to the binomial, where in the binomial you have to have a whole number of successes, but the normal allows fractional values as well. In about the 1960s, the usefulness of Yates’s correction was shot down, for general contingency tables. There is, however, one case where it is useful, and that is the case where the row totals and column totals are fixed.\nWhat do I mean by that? Well, first let’s look at a case where the totals are not all fixed. Consider a survey in which you want to see whether males and females agree or disagree on some burning issue of the day. You collect random samples of, say, 500 males and 500 females, and you count how many of them say Yes or No to your statement.5 You might get results like this:\n\nYes  No  Total\nMales    197 303   500\nFemales  343 157   500\nTotal    540 460  1000\n\nIn this table, the row totals must be 500, because you asked this many males and this many females, and each one must have answered something. The column totals, however, are not fixed: you didn’t know, ahead of time, that 540 people would answer “yes”. That was just the way the data turned out, and if you did another survey with the same design, you’d probably get a different number of people saying “yes”.\nFor another example, let’s go back to Fisher (yes, that Fisher). A “lady” of his acquaintance claimed to be able, by drinking a cup of tea with milk and sugar in it, whether the milk or the sugar had been added first. Fisher, or, more likely, his housekeeper, prepared 8 cups of tea, 4 with milk first and 4 with sugar first. The lady knew that four of the cups had milk first, and her job was to say which four. The results might have been like this:\n\nActual \nMilk first  sugar first  Total\nLady   Milk first        3            1         4\nsays   sugar first       1            3         4\nTotal             4            4         8\n\nThis time, all of the row totals and all of the column totals must be 4, regardless of what the lady thinks. Even if she thinks 5 of the cups of tea actually had milk first, she is going to pick 4 of them to say that they have milk first, since she knows there are only 4. In this case, all of the row and column totals are fixed at 4, and the right analysis is called Fisher’s Exact Test, based on the hypergeometric distribution. In a \\(2\\times 2\\) table like this one, there is only one “degree of freedom”, since as soon as you specify one of the frequencies, say the number of cups where the lady said milk first and they actually were milk first, you can work out the others. But, leaving that aside, the usual chi-squared analysis is a perfectly good approximation, especially if the frequencies are large, and especially if you use Yates’s correction.\nIt is clear that Fisher must have been English, since he was able to get a publication out of drinking tea.\nHow does that apply to Mood’s median test? Well, let’s remind ourselves of the table we had:\n\ntab\n\n     \ngroup FALSE TRUE\n    A     2    8\n    B     7    1\n\n\nWe know how many students were in each group: 10 in group A and 8 in B. So the row totals are fixed. What about the columns? These are whether each observation was above or below the overall median. There were 18 observations altogether, so there must be 9 above and 9 below.6 So the column totals are fixed as well. All totals fixed, so we should be using Yates’s correction. I didn’t, because I wanted to keep things simple, but I should have done.\nR’s chisq.test by default always uses Yates’s correction, and if you don’t want it, you have to say correct=F. Which is why I have been doing so all through.\n\\(\\blacksquare\\)\n\nRun Mood’s median test on these data using my smmr package, and verify that you get the same answer.\n\nSolution\nNot much to it, since the data is already read in:\n\nlibrary(smmr)\nmedian_test(instr, score, group)\n\n$table\n     above\ngroup above below\n    A     8     2\n    B     1     7\n\n$test\n       what       value\n1 statistic 8.100000000\n2        df 1.000000000\n3   P-value 0.004426526\n\n\nIdentical, test statistic, degrees of freedom and P-value. The table of frequencies is also the same, just with columns rearranged. (In smmr I counted the number of values below the overall median, whereas in my build-it-yourself I counted the number of values above.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mood-median.html#handspans-revisited-1",
    "href": "mood-median.html#handspans-revisited-1",
    "title": "8  Mood median test",
    "section": "8.8 Handspans revisited",
    "text": "8.8 Handspans revisited\nTake your right hand, and stretch the fingers out as far as you can. The distance between the tip of your thumb and the tip of your little (pinky) finger is your handspan. The students in a Statistics class at Penn State measured their handspans and also whether they identified as male or female. The data are at http://ritsokiguess.site/datafiles/handspan.txt, with handspans measured in inches. We want to see whether male students have a larger mean handspan than female students.\n\nRead in and display (some of) the data.\n\nSolution\nDelimited by a single space, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/handspan.txt\"\nspan &lt;- read_delim(my_url, \" \")\n\nRows: 190 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): sex\ndbl (1): handspan\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspan\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake a suitable (facetted) normal quantile plot of the data. (Bear in mind what is supposed to have a normal distribution.)\n\nSolution\nHere, we need each group to be approximately normal, so make normal quantile plots of handspan, facetted by sex:\n\nggplot(span, aes(sample=handspan)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~sex)\n\n\n\n\n\\(\\blacksquare\\)\n\nDiscuss briefly whether you might prefer to use Mood’s median test to compare the handspans of the male and female students, compared to a two-sample \\(t\\)-test.\n\nSolution\nA two-sample \\(t\\)-test assumes that each of the two samples comes from a (approximately) normal distribution (“the data are normal” is not precise enough). The female values, on the left, definitely have some outliers at the low end (or a long lower tail), so these are definitely not normal. The male values (on the right) are slightly skewed to the left, or there are some mild outliers at the low end, or, if you prefer, these are approximately normal. (You need discussion of each of the males and females, or of why looking at one group is enough.) Because the males are not close enough to normal (or, because neither group is close enough to normal), we would prefer to use Mood’s median test. (Say this.) You do yourself a favour by making it clear that you know that both groups have to be normal enough; if one is good but the other is not, that is not enough.\nThe other relevant issue is sample size. The best answer discusses that as well, even though you have a lot to think about already. This data set has 190 observations in it, so the samples must be pretty big:\n\nspan %&gt;% count(sex)\n\n\n\n  \n\n\n\nWith these sample sizes, we can expect a lot of help from the central limit theorem. The apparent outliers in the males won’t be a problem, and maybe we could even get away with those outliers in the females.\nExtra: you could also think about bootstrapped sampling distributions of the sample mean here. The one we are most concerned about is the females; if it turns out that they are all right, then the males must be all right too, since the plot for them is showing less non-normality (or, without the double negative, is closer to being normal). So let’s do the females:\n\nspan %&gt;% \n  filter(sex == \"F\") -&gt; females \ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(females$handspan, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nMy take is that the sampling distribution of the sample mean for the females is normal enough, therefore the one for the males is also normal enough, therefore the two-sample \\(t\\)-test is actually fine.\nThe reason that this one is close to normal is different from the other one, though. In the other question, we had milder non-normality but a smaller sample; in this one, the data distribution is less normal, but we had a much larger sample size to compensate.\n\\(\\blacksquare\\)\n\nRun Mood’s median test. What do you conclude from the test, in the context of the data?\n\nSolution\n\nlibrary(smmr)\nmedian_test(span, handspan, sex)\n\n$table\n     above\ngroup above below\n    F    17    82\n    M    65    11\n\n$test\n       what       value\n1 statistic 8.06725e+01\n2        df 1.00000e+00\n3   P-value 2.66404e-19\n\n\nThe P-value of \\(2.66 \\times 10^{-19}\\) is extremely small, so we can conclude that males and females have different median handspans. Remember that we are now comparing medians, and that this test is two-sided.\nYou can stop here, or you can go on and note that most of the males have a handspan bigger than the median, and most of the females have a handspan smaller than the median, so that males have on average a larger handspan. But you have to make the case that males have a larger handspan; you cannot just assert this from the P-value.\nA more formal way to do this is to make the same observation as above, then note that this is “on the correct side” (for males to have a larger handspan), and thus that you can halve the P-value, and conclude that males’ handspans are indeed larger in terms of median.\nExtra: you are probably expecting a confidence interval now for the difference in medians. I haven’t talked about that in lecture, because the ideas are a bit trickier than they were for the confidence interval for the sign test. The sign test could be used for testing any median, so we could try a bunch of medians and see whether each one was rejected or not. The problem with Mood’s median test is that it only tests that the medians are the same. If you could easily test that the difference in medians was 3, say, you would know whether 3 was inside or outside the confidence interval for the difference in medians.\nWhat were the actual sample medians, anyway?\n\nspan %&gt;% group_by(sex) %&gt;% \nsummarize(med = median(handspan))\n\n\n\n  \n\n\n\nHere’s an idea: if we shift all the female handspans up by 2.5 inches, the medians would be the same:\n\nspan %&gt;% mutate(x = ifelse(sex==\"F\", handspan+2.5, handspan)) -&gt; d\nd\n\n\n\n  \n\n\n\nDataframe d has a new column x that is the handspan plus 2.5 inches for females, and the unchanged handspan for males. So the median of x should be the same for males and females:\n\nd %&gt;% group_by(sex) %&gt;% \nsummarize(med_x = median(x))\n\n\n\n  \n\n\n\nand also the medians of x cannot possibly be significantly different:\n\nmedian_test(d, x, sex)\n\n$table\n     above\ngroup above below\n    F    46    36\n    M    41    35\n\n$test\n       what      value\n1 statistic 0.07369901\n2        df 1.00000000\n3   P-value 0.78602526\n\n\nQuite a lot of the values of x are exactly equal to the overall median (and are discarded), so the P-value is not exactly 1 as you would expect. But it is definitely not significant, and so a difference of 2.5 inches smaller for females is going to be in a confidence interval for the difference in medians.\nThe strategy now is to try shifting the female handspans by different amounts, run Mood’s median test for each one, and see which shifts are not rejected. These are the ones for which that difference in medians would be in the confidence interval. Before we get to that, though, I want to simplify the procedure we have, so that it is easier to run it lots of times. First, let’s get just the P-value out of the median test:\n\nd.1 &lt;- median_test(d, x, sex)\nd.1 %&gt;% pluck(\"test\", \"value\", 3)\n\n[1] 0.7860253\n\n\nThat’s the P-value. pluck pulls individual things out of bigger things. The variable I called d.1 has two things in it. The one called table has the numbers of data values above and below the overall median; the one called test has the test statistic and P-value in it. test is a dataframe; inside that is a column called what and a column called value with the number we want in it, and we want the third thing in that (the other two are the chi-squared test statistic and its degrees of freedom). Hence the pluck statement got the right thing.\nLet’s think strategy: we want to shift the female handspans by a bunch of different amounts, run the test on each one, and get the P-value each time. When you’re running a big for-each like this, you want the thing you do each time to be as simple as possible. So let’s write a function that takes the shift as input, works out the new x, runs the test, and returns the P-value. We have all the ingredients, so it’s a matter of putting them together:\n\nshift_pval &lt;- function(shift) {\n  span %&gt;% mutate(x = ifelse(sex == \"F\", handspan + shift, handspan)) -&gt; d\n  d.1 &lt;- median_test(d, x, sex)\n  d.1 %&gt;% pluck(\"test\", \"value\", 3)\n}\n\nIn the function, the shift is input. The first line computes the handspans shifted by the input amount, whatever it is; the second line runs the median test on the shifted data; the last line pulls out, and returns, the P-value.\nI am being a little sloppy here (but R is letting me get away with it): the function is also using a dataframe called span, which is the one we read in from the file earlier. That was not input to the function, so, if you have experience with other programming languages, you might be wondering whether that is “in the scope” of inside the function: that is, whether R will know about it. R does; anything the function needs that is not part of the input, it will take from your workspace. This is, you might imagine, dangerous; if the input to your function is called, say, x, you might easily have an x lying around in your workspace from some other analysis that has nothing to do with the x you want as the input to your function. The safe way to do it, and what I should have done, is to have span be input to my function as well. However, that clutters up the discussion below, so we’ll leave things as I did them here.\nLet’s test this on a shift of 2.5 inches, and on the original data (a shift of zero):\n\nshift_pval(2.5)\n\n[1] 0.7860253\n\nshift_pval(0)\n\n[1] 2.66404e-19\n\n\nThose are the same P-values we got before, so good.\nNow, let’s get a bunch of shifts, say from 0 to 5 in steps of 0.5:\n\ntibble(shift = seq(0, 5, 0.5))\n\n\n\n  \n\n\n\nwork out the P-value for each one, rowwise:\n\ntibble(shift = seq(0, 5, 0.5)) %&gt;%\n  rowwise() %&gt;% \n  mutate(p_value = shift_pval(shift))\n\n\n\n  \n\n\n\nand finally decide whether each shift is inside or outside the CI (because I am too lazy to figure out the scientific notation):\n\ntibble(shift = seq(0, 5, 0.5)) %&gt;%\n  rowwise() %&gt;% \n  mutate(p_value = shift_pval(shift)) %&gt;% \n  mutate(where = ifelse(p_value&lt;0.05, \"outside\", \"inside\"))\n\n\n\n  \n\n\n\nThe confidence interval goes from 2 inches to 2.5 inches on this scale. I checked and it goes up to 3 really, except that 3 itself is outside the interval. So let’s call it 2 to 3 inches. This means that the median female handspan is between 2 and 3 inches smaller than the median male handspan, because we had to shift the female handspans up by that much to make them not significantly different.\nYou, of course, would do just the last pipeline; I showed you the steps so you could see what was going on.\nThe final observation is that this interval is a long way from containing zero, because the P-value was so tiny.\nLet’s see how the \\(t\\)-interval looks in comparison (two-sided, because we want the confidence interval):\n\nt.test(handspan~sex, data = span)\n\n\n    Welch Two Sample t-test\n\ndata:  handspan by sex\nt = -10.871, df = 187.92, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -3.001496 -2.079466\nsample estimates:\nmean in group F mean in group M \n       20.01699        22.55747 \n\n\nAlmost exactly the same (except that F is before M). So it made no difference at all whether we did a \\(t\\)-test or a Mood’s median test, as the bootstrapped sampling distribution suggested.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mood-median.html#footnotes",
    "href": "mood-median.html#footnotes",
    "title": "8  Mood median test",
    "section": "",
    "text": "That is, before looking at the data. This is Latin. It’s also the place that the Bayesian “prior distribution” comes from. The “posterior distribution” comes from the Latin a posteriori, which means afterwards, that is, after you have looked at the data.↩︎\nThat is, before looking at the data. This is Latin. It’s also the place that the Bayesian “prior distribution” comes from. The “posterior distribution” comes from the Latin a posteriori, which means afterwards, that is, after you have looked at the data.↩︎\nThe two groups have very different spreads, but that is not a problem as long as we remember to do the Welch-Satterthwaite test that does not assume equal spreads. This is the default in R, so we are good, at least with that.↩︎\nThere was, in the chisq.test inside median_test, but in smmr I didn’t pass that warning back to the outside world.↩︎\nTo simplify things, we’ll assume that everyone gave a Yes or a No answer, though you could add a column like “No answer” if you wanted to make it more realistic.↩︎\nExcept in the case of the previous problem, where there were multiple observations equal to the overall median. Which we ignore for the moment.↩︎"
  },
  {
    "objectID": "matched-pairs-sign.html#measuring-body-fat",
    "href": "matched-pairs-sign.html#measuring-body-fat",
    "title": "9  Matched pairs t and sign test",
    "section": "9.1 Measuring body fat",
    "text": "9.1 Measuring body fat\nAthletes are concerned with measuring their body fat percentage. Two different methods are available: one using ultrasound, and the other using X-ray technology. We are interested in whether there is a difference in the mean body fat percentage as measured by these two methods, and if so, how big that difference is. Data on 16 athletes are at link.\n\nExplain briefly why a matched pairs analysis is more suitable for these data than a two-independent-samples analysis (using a two-sample \\(t\\)-test). You might find that looking at the data (clicking on the link) helps you figure this out.\nRead in the data and check that you have a sensible number of rows and columns.\nCarry out a suitable test to determine whether the means are the same or different. (At this point, obtain the R output including a P-value.)\nWhat do you conclude from the test?\nObtain a 95% confidence interval for the population mean difference. How is the interval consistent with your test?\nCalculate the differences, and make a normal quantile plot of them. Is there any evidence that normality of differences fails? Explain briefly."
  },
  {
    "objectID": "matched-pairs-sign.html#throwing-baseballs-and-softballs",
    "href": "matched-pairs-sign.html#throwing-baseballs-and-softballs",
    "title": "9  Matched pairs t and sign test",
    "section": "9.2 Throwing baseballs and softballs",
    "text": "9.2 Throwing baseballs and softballs\nCan students throw a baseball farther than a softball? A statistics class, containing 24 students, went out to a football field to try to answer this question. Each student warmed up and then threw each type of ball as far as they could. The order of ball types was randomized: some students threw the baseball first, and some threw the softball first. (A softball is bigger than a baseball, so we might expect that a softball would be harder to throw a long way than a baseball.) The data are in http://ritsokiguess.site/datafiles/throw.txt in three columns: the first is a number identifying the student, the second is the distance thrown with the baseball (in yards) and the third is the distance thrown with the softball (also in yards).\n\nRead the data into R. You’ll need to supply some names to the columns.\nCalculate a column of differences, baseball minus softball, in the data frame.\nCarry out a sign test in R, testing the null hypothesis that the median difference is zero, against the alternative that it is greater than zero. Obtain a P-value. Your option whether you use smmr or not."
  },
  {
    "objectID": "matched-pairs-sign.html#throwing-baseballs-and-softballs-again",
    "href": "matched-pairs-sign.html#throwing-baseballs-and-softballs-again",
    "title": "9  Matched pairs t and sign test",
    "section": "9.3 Throwing baseballs and softballs, again",
    "text": "9.3 Throwing baseballs and softballs, again\nPreviously, you carried out a sign test to determine whether students could throw a baseball farther than a softball. This time, we will calculate a confidence interval for the median difference baseball minus softball, using the results of sign tests.\n\nRead the data into R from link, giving appropriate names to the columns, and add a column of differences.\nUse smmr to find a 95% confidence interval for the median difference.\nWhat function in smmr will run a two-sided sign test and return only the P-value? Check that it works by testing whether the median difference for your data is zero or different from zero.\nBased on your P-value, do you think 0 is inside the confidence interval or not? Explain briefly.\nObtain a 95% confidence interval for the population median difference, baseball minus softball, using a trial-and-error procedure that determines whether a number of possible medians are inside or outside the CI."
  },
  {
    "objectID": "matched-pairs-sign.html#changes-in-salary",
    "href": "matched-pairs-sign.html#changes-in-salary",
    "title": "9  Matched pairs t and sign test",
    "section": "9.4 Changes in salary",
    "text": "9.4 Changes in salary\nA company is growing and would like to attract more employees. The company would like to advertise that salaries there are increasing. To do this, the company randomly samples 20 employees that have been working there since January 2016, and for each of these employees, records their salary in January 2016 and January 2017. The data, with salaries in thousands of dollars, are in link.\n\nRead the data into R and demonstrate that you have two salaries for each of 20 employees.\nTo compare the salaries, explain briefly why a matched-pairs test would be better than a two-sample test.\nMake a suitable graph to assess the assumptions for a matched-pairs \\(t\\)-test. What does your graph tell you?\nCarry out a suitable matched-pairs \\(t\\)-test on these data. (If you thought in the previous part that this was the wrong thing to do, do it anyway for the purposes of this assignment.) What do you conclude?\nThe company would like to estimate\nhow much salaries are increasing, on average. Obtain some output that will enable the company to assess this, and tell the CEO which piece of the output they should look at."
  },
  {
    "objectID": "matched-pairs-sign.html#body-fat-revisited",
    "href": "matched-pairs-sign.html#body-fat-revisited",
    "title": "9  Matched pairs t and sign test",
    "section": "9.5 Body fat revisited",
    "text": "9.5 Body fat revisited\nAthletes are concerned with measuring their body fat percentage. Two different methods are available: one using ultrasound, and the other using X-ray technology. We are interested in whether there is a difference in the mean body fat percentage as measured by these two methods, and if so, how big that difference is. Data on 16 athletes are at link.\nWe saw this data set before.\n\nRead in the data again.\nCalculate the differences, and make a normal quantile plot of them. Is there any evidence that normality of differences fails? Explain briefly.\nPreviously, we did a matched-pairs \\(t\\)-test for these data. In the light of your normal quantile plot, do you think that was a good idea? Explain briefly.\nUse the sign test appropriately to compare the two methods for measuring body fat. (Use smmr if you wish.) What do you conclude, as ever in the context of the data?"
  },
  {
    "objectID": "matched-pairs-sign.html#the-dentist-and-blood-pressure",
    "href": "matched-pairs-sign.html#the-dentist-and-blood-pressure",
    "title": "9  Matched pairs t and sign test",
    "section": "9.6 The dentist and blood pressure",
    "text": "9.6 The dentist and blood pressure\nGoing to the dentist is scary for a lot of people. One way in which this might show up is that people might have higher blood pressure on average before their dentist’s appointment than an hour after the appointment is done. Ten randomly-chosen individuals have their (systolic) blood pressure measured while they are in a dentist’s waiting room, and then again one hour after their appointment is finished.\nThe data are in http://ritsokiguess.site/datafiles/blood_pressure1.csv.\n\nRead in and display the data.\nWhat kind of experimental design is this? How do you know? Explain briefly.\nRun a suitable \\(t\\)-test on these data. What do you conclude, in the context of the data?\nRun a suitable sign test on these data. What do you conclude, in the context of the data?\nDraw a suitable normal quantile plot of these data, one that will enable you to decide between the tests you ran in the previous two parts.\nDiscuss briefly which of your two tests is the more appropriate one to run."
  },
  {
    "objectID": "matched-pairs-sign.html#french-teachers",
    "href": "matched-pairs-sign.html#french-teachers",
    "title": "9  Matched pairs t and sign test",
    "section": "9.7 French teachers",
    "text": "9.7 French teachers\nTwenty high-school French teachers attended a summer institute to improve their French skills. At the beginning of their session, each teacher took a listening test (to test their understanding of spoken French). After 4 weeks of immersion in French, each teacher took a similar listening test again. (The actual French spoken in the two tests was different, so simply taking the first test should not improve the score in the second one; the tests were otherwise similar.) The maximum score on each test was 36, and a higher score is better. The data are here. (Right-click on the link, select “copy link address”, and then paste that URL into R Studio.) The data values are separated by tabs.\nThe data file has three columns:\n\nan identification for each teacher\nthe teacher’s score in the first test\nthe teacher’s score in the second test\n\n\nRead in and display (some of) the data.\nExplain briefly why this is a matched-pairs study.\nRun a suitable matched-pairs \\(t\\)-test to see whether the teachers’ scores have on average improved over the four weeks.\nWhat do you conclude from your test, in the context of the data?\nHow much is the teachers’ listening skill improving, on average? Give a suitable interval to support your answer.\nMake a suitable plot to assess any assumptions for this test.\nDo you trust the result of your matched-pairs \\(t\\)-test? Explain briefly.\nRun a suitable sign test, and obtain a suitable (95%) confidence interval. Comment briefly on your results.\nComment briefly on the comparison between your inferences for the mean and the median.\n\nMy solutions follow:"
  },
  {
    "objectID": "matched-pairs-sign.html#measuring-body-fat-1",
    "href": "matched-pairs-sign.html#measuring-body-fat-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.8 Measuring body fat",
    "text": "9.8 Measuring body fat\nAthletes are concerned with measuring their body fat percentage. Two different methods are available: one using ultrasound, and the other using X-ray technology. We are interested in whether there is a difference in the mean body fat percentage as measured by these two methods, and if so, how big that difference is. Data on 16 athletes are at link.\n\nExplain briefly why a matched pairs analysis is more suitable for these data than a two-independent-samples analysis (using a two-sample \\(t\\)-test). You might find that looking at the data (clicking on the link) helps you figure this out.\n\nSolution\nThe data file looks like this:\n\nathlete xray ultrasound\n1 5.00 4.75\n2 7 3.75\n3 9.25 9\n4 12 11.75\n5 17.25 17\n6 29.5 27.5\n7 5.5 6.5\n8 6 6.75\n9 8 8.75\n10 8.5 9.5\n11 9.25 9.5\n12 11 12\n13 12 12.25\n14 14 15.5\n15 17 18\n16 18 18.25\n\nThe data are two measurements for each of the 16 athletes: that is, each athlete had their body fat percentage measured using both of the two methods. Extra: a two-sample \\(t\\) approach would be reasonable if one set of 16 athletes had been measured by X-ray and another different set of 16 athletes had been measured by ultrasound. (That is, if there had been 32 athletes in total, with each one randomly assigned to one of the measurement methods.) But that’s not what happened. It is easy to measure one athlete’s body fat percentage using both of the two methods, so a matched pairs design is easy to implement (as well as being better). If you use two independent samples (each athlete doing only one measurement method), you introduce an extra source of variability: athletes differ one from another in body fat, as well as differing possibly by measurement method. If you use a matched-pairs design, you remove the athlete-to-athlete differences, leaving only the differences due to measurement method.\n\\(\\blacksquare\\)\n\nRead in the data and check that you have a sensible number of rows and columns.\n\nSolution\nThis kind of thing. Since you looked at the data (didn’t you?), you’ll know that the values are separated by single spaces:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/bodyfat.txt\"\nbodyfat &lt;- read_delim(myurl, \" \")\n\nRows: 16 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): athlete, xray, ultrasound\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbodyfat\n\n\n\n  \n\n\n\n16 rows (athletes) and 3 columns, one for each measurement method and one labelling the athletes. All good.\nSince 16 is not too much bigger than 10, I got the whole data frame here. (At least, I think that’s the reason I got more than 10 rows.) In an R Notebook, you’ll see the first ten rows as normal, with a button to click to see the other six.\n\\(\\blacksquare\\)\n\nCarry out a suitable test to determine whether the means are the same or different. (At this point, obtain the R output including a P-value.)\n\nSolution\nFeed the two columns into t.test along with paired=T. This is a two-sided test, so we don’t have to take any special steps for that. Note that we’re back to the “old-fashioned” version of t.test that does not allow data=, so we have to go the with way:\n\nwith(bodyfat, t.test(xray, ultrasound, paired = TRUE))\n\n\n    Paired t-test\n\ndata:  xray and ultrasound\nt = -0.30801, df = 15, p-value = 0.7623\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.7425068  0.5550068\nsample estimates:\nmean difference \n       -0.09375 \n\n\n\\(\\blacksquare\\)\n\nWhat do you conclude from the test?\n\nSolution\nThe P-value of 0.7623 is not at all small, so there is no way we can reject the null hypothesis.1 There is no evidence of a difference in means; we can act as if the two methods produce the same mean body fat percentage. That is to say, on this evidence we can use either method, whichever one is cheaper or more convenient.\n\\(\\blacksquare\\)\n\nObtain a 95% confidence interval for the population mean difference. How is the interval consistent with your test?\n\nSolution\nYou don’t even need to do any more coding: the test was two-sided, so just pick the confidence interval off the output above: \\(-0.74\\) to 0.56. The interval includes both positive and negative values (or, 0 is inside the interval), so the difference could go either way. This is entirely consistent with not being able to reject the null.\n\\(\\blacksquare\\)\n\nCalculate the differences, and make a normal quantile plot of them. Is there any evidence that normality of differences fails? Explain briefly.\n\nSolution\nThe smoothest2 way to do this is to use a pipeline: use a mutate to create the column of differences, and then pipe that into ggplot, omitting the data frame that would normally go first (the input data frame here is the new one with the differences in it, which doesn’t have a name). I’ll make a normal quantile plot in a moment, but if you haven’t seen that yet, the plot to make is a histogram:\n\nbodyfat %&gt;%\n  mutate(diff = xray - ultrasound) %&gt;%\n  ggplot(aes(x = diff)) + geom_histogram(bins = 6)\n\n\n\n\nI don’t know whether you’d call that “approximately normal” or not. We are in kind of a double-bind with this one: the sample size is small, so normality matters, but with a small sample, the data might not look very normal. It’s kind of skewed right, but most of the evidence for the skewness is contained in those two observations with difference 2 and above, which is pretty flimsy evidence for anything. (In the normal quantile plot below, the suggestion is that those two observations really are a bit too large. It’s easier to tell there.)\nBelow, I’m repeating the calculation of the differences, which is inefficient. If I’m going to draw two graphs of the differences, the right way is to calculate the differences and save the data frame, then use that new data frame twice. But you’re probably only going to draw either the histogram or the normal quantile plot, not both, so you can use the appropriate one of my two bits of code. The normal quantile plot:\n\nbodyfat %&gt;%\n  mutate(diff = xray - ultrasound) %&gt;%\n  ggplot(aes(sample = diff)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis is showing a little evidence of skewness or outliers (depending on your point of view: either is good). The lowest and highest values are both too high, and the pattern of points on the plot is kind of curved (which would be evidence of skewness). Or you could say that the two highest values are too high, with the other values being more or less in line (that would be evidence of outliers at the upper end). I like outliers better than skewness, since those bottom-end points are not far off the line. I would also accept “no substantial problems”, if you can make the case that those two highest points are not too far off the line. With only 16 observations as we have here, even truly normal data would stray off the line a bit.\nAs ever, your explanation is more important than your conclusion. Can you justify what you think?\nIf you took your differences the other way around, as ultrasound minus xray, your plot will also be the other way around, with the “outliers” at the bottom. That’s good too.\nWhere this is going (which I didn’t ask you) is whether or not we trust the result of the matched pairs test. I would say that the test is so far from being significant, and the failure of normality is not gross, that it is hard to imagine any alternative test coming up with a significant result. So I would be happy to trust this paired \\(t\\)-test.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#throwing-baseballs-and-softballs-1",
    "href": "matched-pairs-sign.html#throwing-baseballs-and-softballs-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.9 Throwing baseballs and softballs",
    "text": "9.9 Throwing baseballs and softballs\nCan students throw a baseball farther than a softball? A statistics class, containing 24 students, went out to a football field to try to answer this question. Each student warmed up and then threw each type of ball as far as they could. The order of ball types was randomized: some students threw the baseball first, and some threw the softball first. (A softball is bigger than a baseball, so we might expect that a softball would be harder to throw a long way than a baseball.) The data are in http://ritsokiguess.site/datafiles/throw.txt in three columns: the first is a number identifying the student, the second is the distance thrown with the baseball (in yards) and the third is the distance thrown with the softball (also in yards).\n\nRead the data into R. You’ll need to supply some names to the columns.\n\nSolution\nThis kind of thing:\n\nmyurl=\"http://ritsokiguess.site/datafiles/throw.txt\"\nthrows=read_delim(myurl,\" \",col_names=c(\"student\",\"baseball\",\"softball\"))\n\nRows: 24 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): student, baseball, softball\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nthrows\n\n\n\n  \n\n\n\nThis is one of those times where we have to tell R what names to give the columns. Or you can put col_names=F and leave the columns called X1, X2, X3 or whatever they end up as.\n\\(\\blacksquare\\)\n\nCalculate a column of differences, baseball minus softball, in the data frame.\n\nSolution\nAdd it to the data frame using mutate. Use the right-arrow assignment to create what I called throws2 below, or put something like throws2 &lt;- on the beginning of the line. Your choice.\n\nthrows %&gt;% mutate(diff=baseball-softball) -&gt;\n  throws2\n\n\\(\\blacksquare\\)\n\nCarry out a sign test in R, testing the null hypothesis that the median difference is zero, against the alternative that it is greater than zero. Obtain a P-value. Your option whether you use smmr or not.\n\nSolution\nI think using smmr is way easier, so I’ll do that first. There is even a shortcut in that the null median defaults to zero, which is exactly what we want here:\n\nlibrary(smmr)\nsign_test(throws2,diff)\n\n$above_below\nbelow above \n    2    21 \n\n$p_values\n  alternative      p_value\n1       lower 9.999971e-01\n2       upper 3.302097e-05\n3   two-sided 6.604195e-05\n\n\nWe want, this time, the upper-tailed one-sided test, since we want to prove that students can throw a baseball a longer distance than a softball. Thus the P-value we want is 0.000033.\nTo build it yourself, you know the steps by now. First step is to count how many differences are greater and less than zero:\n\ntable(throws2$diff&gt;0)\n\n\nFALSE  TRUE \n    3    21 \n\n\nor\n\ntable(throws2$diff&lt;0)\n\n\nFALSE  TRUE \n   22     2 \n\n\nor, since we have things in a data frame,\n\nthrows2 %&gt;% count(diff&gt;0)\n\n\n\n  \n\n\n\nor count those less than zero. I’d take any of those.\nNote that these are not all the same. One of the differences is in fact exactly zero. The technically right thing to do with the zero difference is to throw it away (leaving 23 differences with 2 negative and 21 positive). I would take that, or 2 or 3 negative differences out of 24 (depending on whether you count “greater than zero” or “less than zero”). We hope that this won’t make a material difference to the P-value; it’ll make some difference, but won’t (we hope) change the conclusion about whether to reject.\nSecond step is to get a P-value for whichever one of those you got, from the appropriate binomial distribution.\nThe P-value is the probability of getting 21 (or 22) positive differences out of 24 (or 23) or more, since this is the end of the distribution we should be at if the alternative hypothesis is correct. Thus any of these will get you a defensible P-value:\n\nsum(dbinom(21:23,23,0.5))\n\n[1] 3.302097e-05\n\nsum(dbinom(22:24,24,0.5))\n\n[1] 1.7941e-05\n\nsum(dbinom(21:24,24,0.5))\n\n[1] 0.0001385808\n\nsum(dbinom(0:2,23,0.5))\n\n[1] 3.302097e-05\n\nsum(dbinom(0:2,24,0.5))\n\n[1] 1.7941e-05\n\nsum(dbinom(0:3,24,0.5))\n\n[1] 0.0001385808\n\n\nThe first and fourth of those are the same as smmr (throwing away the exactly-median value).\nAs we hoped, there is no material difference here: there is no doubt with any of these possibilities that we will reject a median difference of zero in favour of a median difference greater than zero.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#throwing-baseballs-and-softballs-again-1",
    "href": "matched-pairs-sign.html#throwing-baseballs-and-softballs-again-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.10 Throwing baseballs and softballs, again",
    "text": "9.10 Throwing baseballs and softballs, again\nPreviously, you carried out a sign test to determine whether students could throw a baseball farther than a softball. This time, we will calculate a confidence interval for the median difference baseball minus softball, using the results of sign tests.\n\nRead the data into R from link, giving appropriate names to the columns, and add a column of differences.\n\nSolution\nI did it this way, combining the reading of the data with the calculation of the differences in one pipe:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/throw.txt\"\nthrows &lt;- read_delim(myurl, \" \", col_names = c(\"student\", \"baseball\", \"softball\")) %&gt;%\n  mutate(diff = baseball - softball)\n\nRows: 24 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): student, baseball, softball\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nthrows\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nUse smmr to find a 95% confidence interval for the median difference.\n\nSolution\nci_median, with 95% being the default confidence level:\n\nci_median(throws, diff)\n\n[1] 2.002930 8.999023\n\n\n2 to 9. The ends of a CI for the median will be data values, which are all whole numbers, so round off that 8.999.\n\\(\\blacksquare\\)\n\nWhat function in smmr will run a two-sided sign test and return only the P-value? Check that it works by testing whether the median difference for your data is zero or different from zero.\n\nSolution\nThe rest of the way, we are trying to reproduce that confidence interval by finding it ourselves. The function is called pval_sign. If you haven’t run into it before, in R Studio click on Packages, find smmr, and click on its name. This will bring up package help, which includes a list of all the functions in the package, along with a brief description of what each one does. (Clicking on a function name brings up the help for that function.) Let’s check that it works properly by repeating the previous sign_test and verifying that pval_sign gives the same thing:\n\nsign_test(throws, diff, 0)\n\n$above_below\nbelow above \n    2    21 \n\n$p_values\n  alternative      p_value\n1       lower 9.999971e-01\n2       upper 3.302097e-05\n3   two-sided 6.604195e-05\n\npval_sign(0, throws, diff)\n\n[1] 6.604195e-05\n\n\nThe P-values are the same (for the two-sided test) and both small, so the median difference is not zero.\n\\(\\blacksquare\\)\n\nBased on your P-value, do you think 0 is inside the confidence interval or not? Explain briefly.\n\nSolution\nAbsolutely not. The median difference is definitely not zero, so zero cannot be in the confidence interval. Our suspicion, from the one-sided test from earlier, is that the differences were mostly positive (people could throw a baseball farther than a softball, in most cases). So the confidence interval ought to contain only positive values. I ask this because it drives what happens below.\n\\(\\blacksquare\\)\n\nObtain a 95% confidence interval for the population median difference, baseball minus softball, using a trial-and-error procedure that determines whether a number of possible medians are inside or outside the CI.\n\nSolution\nI’ve given you a fair bit of freedom to tackle this as you wish. Anything that makes sense is good: whatever mixture of mindlessness, guesswork and cleverness that you want to employ. The most mindless way to try some values one at a time and see what you get, eg.:\n\npval_sign(1, throws, diff)\n\n[1] 0.001489639\n\npval_sign(5, throws, diff)\n\n[1] 1.168188\n\n\nSo median 1 is outside and median 5 is inside the 95% interval. Keep trying values until you’ve figured out where the lower and upper ends of the interval are: where the P-values cross from below 0.05 to above, or vice versa.\nSomething more intelligent is to make a long list of potential medians, and get the P-value for each of them, eg.:\n\nd &lt;- tibble(my.med = seq(0, 20, 2))\nd %&gt;% rowwise() %&gt;% \n  mutate(pvals = pval_sign(my.med, throws, diff))\n\n\n\n  \n\n\n\n2 is just inside the interval, 8 is also inside, and 10 is outside. Some closer investigation:\n\nd &lt;- tibble(my.med = seq(0, 2, 0.5))\nd %&gt;% rowwise() %&gt;% \n  mutate(pvals = pval_sign(my.med, throws, diff))\n\n\n\n  \n\n\n\nThe bottom end of the interval actually is 2, since 2 is inside and 1.5 is outside.\n\nd &lt;- tibble(my.med = seq(8, 10, 0.5))\nd %&gt;% rowwise() %&gt;% \n  mutate(pvals = pval_sign(my.med, throws, diff))\n\n\n\n  \n\n\n\nThe top end is 9, 9 being inside and 9.5 outside.\nSince the data values are all whole numbers, I think this is accurate enough. The most sophisticated way is the “bisection” idea we saw before. We already have a kickoff for this, since we found, mindlessly, that 1 is outside the interval on the low end and 5 is inside, so the lower limit has to be between 1 and 5. Let’s try halfway between, ie. 3:\n\npval_sign(3, throws, diff)\n\n[1] 0.3833103\n\n\nInside, so lower limit is between 1 and 3. This can be automated, thus:\n\nlo &lt;- 1\nhi &lt;- 3\nwhile (abs(hi - lo) &gt; 0.1) {\n  try &lt;- (lo + hi) / 2\n  ptry &lt;- pval_sign(try, throws, diff)\n  if (ptry &gt; 0.05) {\n    hi &lt;- try\n  } else {\n    lo &lt;- try\n  }\n}\nc(lo, hi)\n\n[1] 1.9375 2.0000\n\n\nThe difficult bit is to decide whether the value try becomes the new lo or the new hi. If the P-value for the median of try is greater than 0.05, try is inside the interval, and it becomes the new hi; otherwise it’s outside and becomes the new lo. Whatever the values are, lo is always outside the interval and hi is always inside, and they move closer and closer to each other.\nAt the other end of the interval, lo is inside and hi is outside, so there is a little switching around within the loop. For starting values, you can be fairly mindless: for example, we know that 5 is inside and something big like 20 must be outside:\n\nlo &lt;- 5\nhi &lt;- 20\nwhile (abs(hi - lo) &gt; 0.1) {\n  try &lt;- (lo + hi) / 2\n  ptry &lt;- pval_sign(try, throws, diff)\n  if (ptry &gt; 0.05) {\n    lo &lt;- try\n  } else {\n    hi &lt;- try\n  }\n}\nc(lo, hi)\n\n[1] 8.984375 9.042969\n\n\nThe interval goes from 2 to (as calculated here) about 9. (This is apparently the same as ci_median in smmr got.) ci_median uses the bisection method with a smaller “tolerance” than we did, so its answer is more accurate. It looks as if the interval goes from 2 to 9: that is, students can throw a baseball on average between 2 and 9 feet further than they can throw a softball.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#changes-in-salary-1",
    "href": "matched-pairs-sign.html#changes-in-salary-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.11 Changes in salary",
    "text": "9.11 Changes in salary\nA company is growing and would like to attract more employees. The company would like to advertise that salaries there are increasing. To do this, the company randomly samples 20 employees that have been working there since January 2016, and for each of these employees, records their salary in January 2016 and January 2017. The data, with salaries in thousands of dollars, are in link.\n\nRead the data into R and demonstrate that you have two salaries for each of 20 employees.\n\nSolution\nLooking at the file, we see that the values are separated by exactly one space:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/salaryinc.txt\"\nsalaries &lt;- read_delim(my_url, \" \")\n\nRows: 20 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): employee\ndbl (2): jan2016, jan2017\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsalaries\n\n\n\n  \n\n\n\nThere are 20 employees (rows), and two columns of salaries: for each employee in the data set, their salary in January 2016 and in January 2017 (thus, two salaries for each employee).\n\\(\\blacksquare\\)\n\nTo compare the salaries, explain briefly why a matched-pairs test would be better than a two-sample test.\n\nSolution\nA matched-pairs test would be better because we have two observations (salaries) for each subject (employee). A two-sample test would be appropriate if we had two separate sets of employees, one set with their salaries recorded in 2016 and the other with their salaries recorded in 2017. That is not what we have here. You can go after this either way: why a matched-pairs approach is appropriate, or why a two-sample approach is not (or a bit of both).\n\\(\\blacksquare\\)\n\nMake a suitable graph to assess the assumptions for a matched-pairs \\(t\\)-test. What does your graph tell you?\n\nSolution\nThis requires thought first before you do any coding (and this is the reason for this one being four points). What has to be at least approximately normally distributed is the set of differences, salary at one time point minus the salary at the other, for each employee. The individual salaries don’t have to be normally distributed at all. We don’t have the differences here, so we have to calculate them first. The smoothest way is to make a pipeline:\n\nsalaries %&gt;%\n  mutate(diff = jan2017 - jan2016) %&gt;%\n  ggplot(aes(sample = diff)) + stat_qq() + stat_qq_line()\n\n\n\n\nA couple of coding notes: (i) you can take the differences 2016 minus 2017 if you like (then they will tend to be negative), (ii) ggplot used in a pipeline like this does not have a data frame first (the data frame used is the nameless output from the mutate, with the differences in it).\nAlso, there’s no problem doing the mutate, saving that, and then feeding the saved data frame into ggplot. If you find that clearer, go for it.\nAs for what I see: I think those points get a bit far from the line at the high and low ends: the high values are too high and the low values are too low, which is to say that we have outliers at both ends, or the distribution has long tails (either way of saying it is good).\nThe important conclusion here is whether these differences are normal enough to trust a matched pairs \\(t\\)-test here. We have a sample of size 20, so the central limit theorem will help us some, but you can reasonably say that these tails are too long and that we should not trust a matched-pairs \\(t\\)-test.\nI actually wanted you to practice doing a matched-pairs \\(t\\)-test anyway, hence my comment in the next part, but the result is probably not so trustworthy.\n\\(\\blacksquare\\)\n\nCarry out a suitable matched-pairs \\(t\\)-test on these data. (If you thought in the previous part that this was the wrong thing to do, do it anyway for the purposes of this assignment.) What do you conclude?\n\nSolution\nThe company is trying to prove that salaries are increasing over time, so we need a one-sided alternative. Following through the procedure, even though you may not trust it much:\n\nwith(salaries, t.test(jan2016, jan2017, alternative = \"less\", paired = TRUE))\n\n\n    Paired t-test\n\ndata:  jan2016 and jan2017\nt = -10.092, df = 19, p-value = 2.271e-09\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n      -Inf -5.125252\nsample estimates:\nmean difference \n         -6.185 \n\n\nYou could also have the years the other way around, in which case the alternative has to be the other way around as well:\n\nwith(salaries, t.test(jan2017, jan2016, alternative = \"greater\", paired = TRUE))\n\n\n    Paired t-test\n\ndata:  jan2017 and jan2016\nt = 10.092, df = 19, p-value = 2.271e-09\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 5.125252      Inf\nsample estimates:\nmean difference \n          6.185 \n\n\nOr, if you saved the data frame with the differences in it, do a one-sample test on those, again making sure that you get the alternative right. I didn’t save it, so I’m calculating the differences again:\n\nsalaries %&gt;%\n  mutate(diff = jan2017 - jan2016) %&gt;%\n  with(., t.test(diff, mu = 0, alternative = \"greater\"))\n\n\n    One Sample t-test\n\ndata:  diff\nt = 10.092, df = 19, p-value = 2.271e-09\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 5.125252      Inf\nsample estimates:\nmean of x \n    6.185 \n\n\nWhichever way you do it, the P-value is the same \\(2.271 \\times 10^{-9}\\), which is a whole lot less than 0.05, so there is no doubt at all that salaries are increasing.\n(Your intuition ought to have expected something like this, because everyone’s 2017 salary appears to be greater than their 2016 salary.)\nExtra: you might be feeling that we ought to be doing a matched-pairs sign test, which you could do this way:\n\nlibrary(smmr)\nsalaries %&gt;%\n  mutate(diff = jan2017 - jan2016) %&gt;%\n  sign_test(diff, 0)\n\n$above_below\nbelow above \n    0    20 \n\n$p_values\n  alternative      p_value\n1       lower 1.000000e+00\n2       upper 9.536743e-07\n3   two-sided 1.907349e-06\n\n\nand then take the “upper” P-value, which is in the same ballpark as the one from the \\(t\\)-test. So the salaries really are increasing, whether you believe the \\(t\\)-test or not. And note that every single employee’s salary increased.\n(Again, the “missing” data frame in sign_test is the nameless one with the differences in it.)\n\\(\\blacksquare\\)\n\nThe company would like to estimate how much salaries are increasing, on average. Obtain some output that will enable the company to assess this, and tell the CEO which piece of the output they should look at.\n\nSolution\nA confidence interval. 95% is fine. As before, we have to run t.test again because we ran a one-sided test and a confidence interval for us is two-sided:\n\nwith(salaries, t.test(jan2017, jan2016, paired = TRUE))\n\n\n    Paired t-test\n\ndata:  jan2017 and jan2016\nt = 10.092, df = 19, p-value = 4.542e-09\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 4.902231 7.467769\nsample estimates:\nmean difference \n          6.185 \n\n\nBetween about $5,000 and about $7,500. This is what to tell the CEO.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#body-fat-revisited-1",
    "href": "matched-pairs-sign.html#body-fat-revisited-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.12 Body fat revisited",
    "text": "9.12 Body fat revisited\nAthletes are concerned with measuring their body fat percentage. Two different methods are available: one using ultrasound, and the other using X-ray technology. We are interested in whether there is a difference in the mean body fat percentage as measured by these two methods, and if so, how big that difference is. Data on 16 athletes are at link.\nWe saw this data set before.\n\nRead in the data again.\n\nSolution\nThis kind of thing. Since you looked at the data (didn’t you?), you’ll know that the values are separated by single spaces:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/bodyfat.txt\"\nbodyfat &lt;- read_delim(myurl, \" \")\n\nRows: 16 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): athlete, xray, ultrasound\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbodyfat\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nCalculate the differences, and make a normal quantile plot of them. Is there any evidence that normality of differences fails? Explain briefly.\n\nSolution\nThis is a good place to look ahead. We’ll need the differences in two places, most likely: first for the normal quantile plot, and second for the matched-pairs sign test. So we should calculate and save them first:\n\nbodyfat %&gt;% mutate(diff = xray - ultrasound) -&gt; bodyfat2\n\nI seem to be using a 2 on the end to name my dataframe-with-differences, but you can use whatever name you like.\nThen, not forgetting to use the data frame that we just made:\n\nggplot(bodyfat2, aes(sample = diff)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis is showing a little evidence of skewness or outliers (depending on your point of view: either is good). The lowest and highest values are both too high, and the pattern of points on the plot is kind of curved (which would be evidence of skewness). Or you could say that the two highest values are too high, with the other values being more or less in line (that would be evidence of outliers at the upper end). I like outliers better than skewness, since those bottom-end points are not far off the line. I would also accept “no substantial problems”, if you can make the case that those two highest points are not too far off the line. With only 16 observations as we have here, even truly normal data would stray off the line a bit.\nAs ever, your explanation is more important than your conclusion. Can you justify what you think?\nIf you took your differences the other way around, as ultrasound minus xray, your plot will also be the other way around, with the “outliers” at the bottom. That’s good too.\n\\(\\blacksquare\\)\n\nPreviously, we did a matched-pairs \\(t\\)-test for these data. In the light of your normal quantile plot, do you think that was a good idea? Explain briefly.\n\nSolution\nWe are looking for the differences to be approximately normal, bearing in mind that we have a sample of size 16, which is not that large. Say what you think here; the points, if I were giving any here, would be for the way in which you support it. The comment I made before when we did a matched-pairs \\(t\\)-test was that the P-value was so large and non-significant that it was hard to imagine any other test giving a significant result. Another way of saying that is that I considered these differences to be “normal enough”, given the circumstances. You might very well take a different view. You could say that these differences are clearly not normal, and that the sample size of 16 is not large enough to get any substantial help from the Central Limit Theorem. From that point of view, running the \\(t\\)-test is clearly not advisable.\n\\(\\blacksquare\\)\n\nUse the sign test appropriately to compare the two methods for measuring body fat. (Use smmr if you wish.) What do you conclude, as ever in the context of the data?\n\nSolution\nThat means using a sign test to test the null hypothesis that the median difference is zero, against the alternative that it is not zero. (I don’t see anything here to indicate that we are looking only for positive or only for negative differences, so I think two-sided is right. You need some reason to do a one-sided test, and there isn’t one here.)\nRemembering again to use the data frame that has the differences in it:\n\nsign_test(bodyfat2, diff, 0)\n\n$above_below\nbelow above \n   10     6 \n\n$p_values\n  alternative   p_value\n1       lower 0.2272491\n2       upper 0.8949432\n3   two-sided 0.4544983\n\n\nThe two-sided P-value is 0.4545, so we are nowhere near rejecting the null hypothesis that the median difference is zero. There is no evidence that the two methods for measuring body fat show any difference on average.\nThe table of aboves and belows says that there were 6 positive differences and 10 negative ones. This is not far from an even split, so the lack of significance is entirely what we would expect.\nExtra: this is the same conclusion that we drew the last time we looked at these data (with a matched-pairs \\(t\\)-test). That supports what I said then, which is that the \\(t\\)-test was so far from being significant, that it could be very wrong without changing the conclusion. That is what seems to have happened.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#the-dentist-and-blood-pressure-1",
    "href": "matched-pairs-sign.html#the-dentist-and-blood-pressure-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.13 The dentist and blood pressure",
    "text": "9.13 The dentist and blood pressure\nGoing to the dentist is scary for a lot of people. One way in which this might show up is that people might have higher blood pressure on average before their dentist’s appointment than an hour after the appointment is done. Ten randomly-chosen individuals have their (systolic) blood pressure measured while they are in a dentist’s waiting room, and then again one hour after their appointment is finished.\nThe data are in http://ritsokiguess.site/datafiles/blood_pressure1.csv.\n\nRead in and display the data.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/blood_pressure1.csv\"\nblood_pressure &lt;- read_csv(my_url)\n\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): person\ndbl (2): before, after\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nblood_pressure\n\n\n\n  \n\n\n\nAside: A blood pressure is usually given as two numbers, like ``120 over 80’’. The first number, which is the one shown in our data, is called the systolic blood pressure. It is the pressure in the arteries when the heart is pumping. The second is called the diastolic blood pressure, and it is the pressure in the arteries when the heart is resting.\n\\(\\blacksquare\\)\n\nWhat kind of experimental design is this? How do you know? Explain briefly.\n\nSolution\nThis is a matched pairs design. We know this because we have two measurements on each person, or the same people were measured before and after seeing the dentist. (The thing that it is not is one group of people measured before seeing the dentist, and a different group of people measured afterwards, so a two-sample test is not the right thing.)\n\\(\\blacksquare\\)\n\nRun a suitable \\(t\\)-test on these data. What do you conclude, in the context of the data?\n\nSolution\nA matched-pairs \\(t\\)-test, then. Remember, we want to see whether blood pressure is lower afterwards (that is, before is greater than after), so this needs to be one-sided:\n\nwith(blood_pressure, t.test(before, after, alternative = \"greater\", paired = TRUE))\n\n\n    Paired t-test\n\ndata:  before and after\nt = 2.9945, df = 9, p-value = 0.007545\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 2.210659      Inf\nsample estimates:\nmean difference \n            5.7 \n\n\nThere are some variations possible here: before and after could be switched (in which case alternative must be reversed also).\nOr, you can do a one-sample \\(t\\) on the differences, with the right alternative corresponding to the way you took differences. If you are looking ahead, you might realize that working out the differences now and adding them to the dataframe will be a good idea:\n\nblood_pressure %&gt;% \nmutate(difference = before - after) -&gt; blood_pressure\nblood_pressure\n\n\n\n  \n\n\n\nI took the differences this way around since I was expecting, if anything, the before numbers to be bigger than the after ones. And then:\n\nwith(blood_pressure, t.test(difference, mu = 0, alternative = \"greater\"))\n\n\n    One Sample t-test\n\ndata:  difference\nt = 2.9945, df = 9, p-value = 0.007545\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 2.210659      Inf\nsample estimates:\nmean of x \n      5.7 \n\n\nIf you did the differences the other way around, your alternative will need to be the other way around also.\nThe P-value (either way) is 0.008,3 so we have evidence that the mean blood pressure before is greater than the mean blood pressure after.\n\\(\\blacksquare\\)\n\nRun a suitable sign test on these data. What do you conclude, in the context of the data?\n\nSolution\nA sign test on the differences. By this point, you will realize that you will need to have obtained the differences. Get them here if you did not already get them:\n\nsign_test(blood_pressure, difference, 0)\n\n$above_below\nbelow above \n    2     8 \n\n$p_values\n  alternative   p_value\n1       lower 0.9892578\n2       upper 0.0546875\n3   two-sided 0.1093750\n\n\nThis one gives us all three P-values. The way around I found the differences, the one we want is “upper”, 0.055. There is not quite evidence that median blood pressure before is higher.\n\\(\\blacksquare\\)\n\nDraw a suitable normal quantile plot of these data, one that will enable you to decide between the tests you ran in the previous two parts.\n\nSolution\nThe differences are supposed to be approximately normal if a matched-pairs \\(t\\)-test is the thing:\n\nggplot(blood_pressure, aes(sample=difference)) + stat_qq() + stat_qq_line()\n\n\n\n\n\\(\\blacksquare\\)\n\nDiscuss briefly which of your two tests is the more appropriate one to run.\n\nSolution\nMake a call about whether the differences are normal enough. You have a couple of angles you can take:\n\nthe lowest two values are too low, so we have two outliers at the low end\nthe lowest and highest values are too extreme, so that we have a long-tailed distribution\n\nEither of these would suggest a non-normal distribution, which I think you have to conclude from this plot.\nThe best answer also considers the sample size: there are only 10 differences, a small sample size, and so we will not get much help from the Central Limit Theorem (the sample size is likely not enough4 to overcome those two outliers or the long tails). Thus, we should not trust the \\(t\\)-test and should prefer the sign test.\nExtra: you might be disappointed to go through this and come to the conclusion that there was not a decrease in blood pressure between before and after.\nWhat has happened, I think, is that we have only a small sample (10 people), and having 8 positive differences and 2 negative ones is not quite unbalanced enough (with such a small sample) to rule out chance: that is to say, a median difference of zero. The \\(t\\)-test accounted for the size of the differences, and if you believed the normality was satisfactory, you could demonstrate a difference between before and after. But if you didn’t like the normality, you were out of luck: the only test you have is an apparently not very powerful one.\nIf you wanted to, you could bootstrap the sampling distribution of the sample mean and see how normal it looks:\n\ntibble(sim =  1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(the_sample = list(sample(blood_pressure$difference, replace = TRUE))) %&gt;% \n  mutate(the_mean = mean(the_sample)) %&gt;% \n  ggplot(aes(sample = the_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\n(Code note: you can do anything with the result of a simulation, and you can use anything that might need to be normal as input to a normal quantile plot. Now that we have the normal quantile plot as a tool, we can use it wherever it might be helpful.)\nThis is actually not nearly as bad as I was expecting. Even a sample size of 10 is providing some help. The bootstrapped sampling distribution is somewhat left-skewed, which is not a surprise given the two low outliers. However, it is rather close to normal, suggesting that the \\(t\\)-test is not as bad as we thought.\n(I did 10,000 simulations because I was having trouble seeing how non-normal it was. With this many, I can be pretty sure that this distribution is somewhat left-skewed.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#french-teachers-1",
    "href": "matched-pairs-sign.html#french-teachers-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.14 French teachers",
    "text": "9.14 French teachers\nTwenty high-school French teachers attended a summer institute to improve their French skills. At the beginning of their session, each teacher took a listening test (to test their understanding of spoken French). After 4 weeks of immersion in French, each teacher took a similar listening test again. (The actual French spoken in the two tests was different, so simply taking the first test should not improve the score in the second one; the tests were otherwise similar.) The maximum score on each test was 36, and a higher score is better. The data are here. (Right-click on the link, select “copy link address”, and then paste that URL into R Studio.) The data values are separated by tabs.\nThe data file has three columns:\n\nan identification for each teacher\nthe teacher’s score in the first test\nthe teacher’s score in the second test\n\n\nRead in and display (some of) the data.\n\nSolution\nSeparated by tabs means read_tsv:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/frenchtest.txt\"\nfrench &lt;- read_tsv(my_url)\n\nRows: 20 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (3): id, pre, post\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfrench\n\n\n\n  \n\n\n\nAs promised. The score on the first test is called pre and on the second is called post.\n\\(\\blacksquare\\)\n\nExplain briefly why this is a matched-pairs study.\n\nSolution\nThere are two measurements for each teacher, or, the 20 pre measurements and the 20 post measurements are paired up, namely, the ones that come from the same teacher. Or, if it were two independent samples, our 40 measurements would come from 40 different teachers, but there are only 20 teachers, so the 40 measurements must be paired up.\n\\(\\blacksquare\\)\n\nRun a suitable matched-pairs \\(t\\)-test to see whether the teachers’ scores have on average improved over the four weeks.\n\nSolution\nSeeing whether the scores have improved implies a one-sided test that post is bigger than pre. There are three ways you might do that, any of which is good. Remember that if you are running a test with paired = TRUE, the alternative is relative to the column that is input first, not the first one in alphabetical order or anything like that:\n(i):\n\nwith(french, t.test(pre, post, paired = TRUE, alternative = \"less\"))\n\n\n    Paired t-test\n\ndata:  pre and post\nt = -3.8649, df = 19, p-value = 0.0005216\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n      -Inf -1.381502\nsample estimates:\nmean difference \n           -2.5 \n\n\n(ii):\n\nwith(french, t.test(post, pre, paired = T, alternative = \"greater\"))\n\n\n    Paired t-test\n\ndata:  post and pre\nt = 3.8649, df = 19, p-value = 0.0005216\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 1.381502      Inf\nsample estimates:\nmean difference \n            2.5 \n\n\nYour choice between these two might be influenced by whether you think pre comes first, or whether you think it’s easier to decide how post compares to pre. It’s all down to what seems natural to you.\n\nworking out the differences and testing those (but look ahead in the question to see whether you need the differences for anything else: you do):\n\n\nfrench %&gt;% mutate(gain = post - pre) -&gt; french1\nwith(french1, t.test(gain, mu=0, alternative = \"greater\"))\n\n\n    One Sample t-test\n\ndata:  gain\nt = 3.8649, df = 19, p-value = 0.0005216\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 1.381502      Inf\nsample estimates:\nmean of x \n      2.5 \n\n\nThis last is an ordinary one-sample test, which saves you having to learn anything new, but requires you to calculate the differences first. You will need the differences for a plot anyway, so this may not be as much extra work as it appears. The right thing to do here is to save the data frame with the differences in it, so that you don’t need to calculate them again later.\nA fourth alternative is to calculate the differences as pre minus post, and then switch the alternative around (since if going to the French institute helps, the differences this way will be mostly negative):\n\nfrench %&gt;% mutate(gain = pre - post) -&gt; french2\nwith(french2, t.test(gain, mu=0, alternative = \"less\"))\n\n\n    One Sample t-test\n\ndata:  gain\nt = -3.8649, df = 19, p-value = 0.0005216\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n      -Inf -1.381502\nsample estimates:\nmean of x \n     -2.5 \n\n\n\\(\\blacksquare\\)\n\nWhat do you conclude from your test, in the context of the data?\n\nSolution\nThe P-value of 0.0005 is much less than 0.05, so we reject the null hypothesis that the mean scores before and after are the same, in favour of the alternative that the mean score afterwards is higher. That is to say, the four-week program is helping the teachers improve their understanding of spoken French.\n\\(\\blacksquare\\)\n\nHow much is the teachers’ listening skill improving, on average? Give a suitable interval to support your answer.\n\nSolution\nA 95% (or other level) confidence interval for the mean difference. A one-sided test doesn’t give that, so you need to do the test again without the alternative (to make it two-sided), via any of the methods above, such as:\n\nwith(french, t.test(post, pre, paired = TRUE))\n\n\n    Paired t-test\n\ndata:  post and pre\nt = 3.8649, df = 19, p-value = 0.001043\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.146117 3.853883\nsample estimates:\nmean difference \n            2.5 \n\n\nThis says that, with 95% confidence, the mean test score afterwards is between about 1.1 and 3.9 points higher than before. So that’s how much listening skill is improving on average. Give the suitably rounded interval; the test scores are whole numbers, and there are 20 differences making up the mean, so one decimal is the most you should give.\nIf you did it the first way:\n\nwith(french, t.test(pre, post, paired = TRUE))\n\n\n    Paired t-test\n\ndata:  pre and post\nt = -3.8649, df = 19, p-value = 0.001043\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.853883 -1.146117\nsample estimates:\nmean difference \n           -2.5 \n\n\nyou have given yourself a bit of work to do, because this is before minus after, so you have to strip off the minus signs and switch the numbers around. Giving the answer with the minus signs is wrong, because I didn’t ask about before minus after. Disentangle it, though, and you’re good.\n\\(\\blacksquare\\)\n\nMake a suitable plot to assess any assumptions for this test.\n\nSolution\nThe key assumption here is that the differences are approximately normally distributed.\nFirst calculate and save the differences (since you will need them later for a sign test; otherwise you would have to find them again). If you found the differences to make your \\(t\\)-test, use the ones you saved there.\n\nfrench %&gt;% mutate(gain = post - pre) -&gt; french1\n\nAssess that with a histogram (with suitable number of bins):\n\nggplot(french1, aes(x=gain)) + geom_histogram(bins=6)\n\n\n\n\nor, better, a normal quantile plot (since the normality is our immediate concern):\n\nggplot(french1, aes(sample=gain)) + stat_qq() + stat_qq_line()\n\n\n\n\n(note that the horizontal lines of points are because the test scores were whole numbers, therefore the differences between them are whole numbers also, and some of the teachers had the same difference in scores as others.)\n\\(\\blacksquare\\)\n\nDo you trust the result of your matched-pairs \\(t\\)-test? Explain briefly.\n\nSolution\nThere are about three considerations here:\n\nthe plot shows an outlier at the low end, but no other real problems.\nthe sample size is 20, so we should get some help from the Central Limit Theorem.\nthe P-value was really small.\n\nI expect you to mention the first two of those. Make a call about whether you think that outlier is too much of a problem, given the sample size. You could, I think, go either way with this one.\nThe third of my points says that even if the distribution of differences is not normal enough, and so the P-value is off by a bit, it would take a lot to change it enough to stop it being significant. So I don’t think we need to worry, for myself.\nExtra:\nWe can assess the \\(t\\)-test by obtaining a bootstrap distribution of the sample mean, by sampling from the differences with replacement:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(french1$gain, replace = T))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nThe bootstrapped sampling distribution of the sample mean difference is about as normal as you could reasonably wish for, so there was no need to worry. Only a very few of the most extreme samples were at all off the line.\nA histogram would be almost as good, but now that you know about the normal quantile plot, the time to use it is when you are specifically interested in normality, as you are here. (If you were interested in shape generally, then a histogram or, if appropriate, a boxplot, would also work.)\nThe code: the first line takes 1000 bootstrap samples, and the second finds the mean of each one. Instead of saving the sample means, since I was only going to be using them once, I made them into a dataframe, and then made a normal quantile plot of them. The enframe creates a dataframe with a column called value with the means in it, which I use in the plot.\n\\(\\blacksquare\\)\n\nRun a suitable sign test, and obtain a suitable (95%) confidence interval. Comment briefly on your results.\n\nSolution\nThis works with the differences, that you calculated for the plot, so use the data frame that you saved them in:\n\nsign_test(french1, gain, 0)\n\n$above_below\nbelow above \n    1    16 \n\n$p_values\n  alternative      p_value\n1       lower 0.9999923706\n2       upper 0.0001373291\n3   two-sided 0.0002746582\n\nci_median(french1, gain)\n\n[1] 1.007812 3.000000\n\n\nThe P-value is 0.00014, again very small, saying that the median difference is greater than zero, that is, that the test scores after are greater than the test scores before on average. The confidence interval is from 1 to 3 points, indicating that this is how much test scores are increasing on average.\nA technique thing: the first time you are going through this, you probably got to this point and realized that you were calculating the differences for the second (or third) time. This is the place to stop and think that you don’t really need to do that, and to go back to the plot you did and save the differences after you have calculated them. Then you edit the code here to use the differences you got before and saved. It doesn’t matter whether you see this the first time you do it or not, but it does matter that you see it before you hand it in. It’s like editing an essay; you need to go back through work that you will be handing in and make sure you did it the best way you could.\n\\(\\blacksquare\\)\n\nComment briefly on the comparison between your inferences for the mean and the median.\n\nSolution\nThe upper-tail P-value is 0.0001, in the same ballpark as the \\(t\\)-test (0.0005). The 95% confidence interval for the median difference is from 1 to 3,5 again much like the \\(t\\)-interval (1.1 to 3.9).\nThis suggests that it doesn’t matter much which test we do, and therefore that the \\(t\\)-test ought to be better because it uses the data better.6 This is more evidence that the outlier didn’t have that big of an effect.\nExtra: choosing a test on the basis of its P-value is wrong, because as soon as you introduce a choice on that basis, your P-value looks lower than it should; a P-value is based on you doing one test and only that one test. It is reasonable to note, as I did, that the two P-values are about the same and then choose between the tests on some other basis, such as that the \\(t\\)-test uses the data better.7\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#footnotes",
    "href": "matched-pairs-sign.html#footnotes",
    "title": "9  Matched pairs t and sign test",
    "section": "",
    "text": "My hat stays on my head.↩︎\nI learned yesterday that the Welsh word for “ironing” is smwddio, which seems weird until you say it out loud: it sounds like “smoothio”.↩︎\nGive the P-value, and round it off to about this accuracy so that your reader can see easily (i) how it compares to 0.05, and (ii) about how big it is. More than two decimal places is too many.↩︎\nBut see the Extra.↩︎\nI think I mentioned elsewhere that the P-value of the sign test, as it depends on the null median for a fixed data set, only changes at a data point. Therefore, the ends of a CI for the median must be data points.↩︎\nIt uses the actual data values, not just whether each one is positive or negative.↩︎\nIf the P-values had come out very different, that would be telling you that it matters which one you use, and you would need to go back and look at your plot to decide. Often, this happens when there is something wrong with the \\(t\\)-test, but not necessarily.↩︎"
  },
  {
    "objectID": "normal-quantile.html#lengths-of-heliconia-flowers",
    "href": "normal-quantile.html#lengths-of-heliconia-flowers",
    "title": "10  Normal quantile plots",
    "section": "10.1 Lengths of heliconia flowers",
    "text": "10.1 Lengths of heliconia flowers\nThe tropical flower Heliconia is fertilized by hummingbirds, a different species for each variety of Heliconia. Over time, the lengths of the flowers and the form of the hummingbirds’ beaks have evolved to match each other. The length of the Heliconia flower is therefore an important measurement. Does it have a normal distribution for each variety?\nThe data set at http://ritsokiguess.site/datafiles/heliconia.csv contains the lengths (in millimetres) of samples of flowers from each of three varieties of Heliconia: bihai, caribaea red, and caribaea yellow.\n\nRead the data into R. There are different numbers of length measurements for each variety. How does this show up in the data frame? (Look at all the rows, not just the first ten.)\nMake a normal quantile plot for the variety bihai.\nMake a normal quantile plot for the variety Caribaea red (note that the variable name in the data frame has an underscore in it).\nMake a normal quantile plot for the variety Caribaea yellow (this also has an underscore in it).\nWhich of the three varieties is closest to having a normal distribution? Explain (very) briefly.\nFor each of the two other varieties, apart from the one you mentioned in the last part, describe briefly how their distributions fail to be normal."
  },
  {
    "objectID": "normal-quantile.html#ferritin-and-normality",
    "href": "normal-quantile.html#ferritin-and-normality",
    "title": "10  Normal quantile plots",
    "section": "10.2 Ferritin and normality",
    "text": "10.2 Ferritin and normality\nIn the lecture notes, we looked at some data on different athletes from the Australian Institute of Sport. This data set can be found at http://ritsokiguess.site/datafiles/ais.txt. Recall that the values are separated by tabs. In this question, we will assess one of the variables in the data set for normality.\n\nRead the data set into R.\nOne of the variables, Ferr, is a measurement of Ferritin for each athlete. Obtain a normal quantile plot of the Ferritin values, for all the athletes together. What do you conclude about the shape of the distribution? Explain briefly.\nIt is possible that the shape you found in the previous part is because the athletes from all the different sports were mixed together. Use ggplot to obtain one normal quantile plot for each sport, collected together on one plot.\nLooking at the plots in the previous part, would you say that the Ferritin values for each sport individually have a more normal shape than they do for all the sports together? Explain briefly.\n\nMy solutions follow:"
  },
  {
    "objectID": "normal-quantile.html#lengths-of-heliconia-flowers-1",
    "href": "normal-quantile.html#lengths-of-heliconia-flowers-1",
    "title": "10  Normal quantile plots",
    "section": "10.3 Lengths of heliconia flowers",
    "text": "10.3 Lengths of heliconia flowers\nThe tropical flower Heliconia is fertilized by hummingbirds, a different species for each variety of Heliconia. Over time, the lengths of the flowers and the form of the hummingbirds’ beaks have evolved to match each other. The length of the Heliconia flower is therefore an important measurement. Does it have a normal distribution for each variety?\nThe data set at http://ritsokiguess.site/datafiles/heliconia.csv contains the lengths (in millimetres) of samples of flowers from each of three varieties of Heliconia: bihai, caribaea red, and caribaea yellow.\n\nRead the data into R. There are different numbers of length measurements for each variety. How does this show up in the data frame? (Look at all the rows, not just the first ten.)\n\nSolution\nThe usual read_csv:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/heliconia.csv\"\nheliconia &lt;- read_csv(my_url)\n\nRows: 23 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): bihai, caribaea_red, caribaea_yellow\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nI suggested to look at all the rows. Here’s why:\n\nheliconia \n\n\n\n  \n\n\n\nThe varieties with fewer values have missings (NAs) attached to the end. This is because all the columns in a data frame have to have the same number of values. (The missings won’t impact what we do below — we get a warning but not an error, and the plots are the same as they would be without the missings — but you might be aesthetically offended by them, in which case you can read what I do later on.)\n\\(\\blacksquare\\)\n\nMake a normal quantile plot for the variety bihai.\n\nSolution\nThere’s a certain amount of repetitiveness here (that we work around later):\n\nggplot(heliconia,aes(sample=bihai))+stat_qq()+stat_qq_line()\n\nWarning: Removed 7 rows containing non-finite values (`stat_qq()`).\n\n\nWarning: Removed 7 rows containing non-finite values (`stat_qq_line()`).\n\n\n\n\n\nI’m saving the comments until we’ve seen all three.\n\\(\\blacksquare\\)\n\nMake a normal quantile plot for the variety Caribaea red (note that the variable name in the data frame has an underscore in it).\n\nSolution\nSame idea again:\n\nggplot(heliconia,aes(sample=caribaea_red))+stat_qq()+stat_qq_line()\n\n\n\n\n\\(\\blacksquare\\)\n\nMake a normal quantile plot for the variety Caribaea yellow (this also has an underscore in it).\n\nSolution\nAnd, one more time:\n\nggplot(heliconia,aes(sample=caribaea_yellow))+stat_qq()+stat_qq_line()\n\nWarning: Removed 8 rows containing non-finite values (`stat_qq()`).\n\n\nWarning: Removed 8 rows containing non-finite values (`stat_qq_line()`).\n\n\n\n\n\nI did a lot of copying and pasting there.\n\\(\\blacksquare\\)\n\nWhich of the three varieties is closest to having a normal distribution? Explain (very) briefly.\n\nSolution\nLook at the three plots, and see which one stays closest to the line. To my mind, this is clearly the last one, Caribaea yellow. So your answer ought to be “Caribaea yellow, because the points are closest to the line”. This, I would say, is acceptably close to normal, so using a \\(t\\)-test here would be fine. The answer “the last one” is not quite complete, because I asked you which variety, so your answer needs to name a variety.\n\\(\\blacksquare\\)\n\nFor each of the two other varieties, apart from the one you mentioned in the last part, describe briefly how their distributions fail to be normal.\n\nSolution\nLet’s look at bihai first. I see this one as an almost classic curve: the points are above the line, then below, then above again. If you look at the data scale (\\(y\\)-axis), the points are too bunched up to be normal at the bottom, and too spread out at the top: that is, skewed to the right. You might also (reasonably) take the view that the points at the bottom are close to the line (not sure about the very smallest one, though), but the points at the top are farther away, so that what we have here is two outliers at the top. I’m OK with that. It’s often difficult to distinguish between skewness and outliers (at the end of the long tail). What you conclude can often depend on how you look. We also had to look at the second plot, caribaea red. This is a rather strange one: the points veer away from the line at the ends, but look carefully: it is not outliers at both ends, but rather the points are too bunched up to be normal at both ends: that is, the distribution has short tails compared to the normal. It is something more like a uniform distribution, which has no tails at all, than a normal distribution, which won’t have outliers but it does have some kind of tails. So, “short tails”.\nExtra: that’s all you needed, but I mentioned above that you might have been offended aesthetically by those missing values that were not really missing. Let’s see if we can do this aesthetically. As you might expect, it uses several of the tools from the “tidyverse”. First, tidy the data. The three columns of the data frame are all lengths, just lengths of different things, which need to be labelled. This is pivot_longer from tidyr:\n\nheliconia %&gt;% \n  pivot_longer(everything(), names_to=\"variety\", values_to=\"length\", values_drop_na = T) -&gt; heliconia.long\nheliconia.long  \n\n\n\n  \n\n\n\nThis is now aesthetic as well as tidy: all those NA lines have gone (you can check that there are now \\(16+23+15=54\\) rows of actual data, as there should be). This was accomplished by the last thing in the pivot_longer: “in the values (that is, the lengths), drop any missing values.”\nNow, how to get a normal quantile plot for each variety? This is facet_wrap on the end of the ggplot again.\n\nggplot(heliconia.long,aes(sample=length))+\nstat_qq()+stat_qq_line()+\nfacet_wrap(~variety,scale=\"free\")\n\n\n\n\nThese are a bit elongated vertically. The scale=\"free\" allows a different vertical scale for each plot (otherwise there would be one vertical scale for all three plots); I decided that was best here since the typical lengths for the three varieties are different. Caribaea yellow is more or less straight, bihai has outliers (and may also be curved), caribaea red has that peculiar S-bend shape.\nI didn’t really like the vertical elongation. I’d rather have the plots be almost square, which they would be if we put them in three cells of a \\(2 \\times 2\\) grid. facet_wrap has nrow and ncol which you can use one or both of to make this happen. This creates an array of plots with two columns and as many rows as needed:\n\nggplot(heliconia.long,aes(sample=length))+\nstat_qq()+stat_qq_line()+\nfacet_wrap(~variety,scale=\"free\",ncol=2)\n\n\n\n\nI think the squarer plots make it easier to see the shape of these: curved, S-bend, straightish. Almost the same code will get a histogram for each variety, which I’ll also make squarish:\n\nggplot(heliconia.long,aes(x=length))+\ngeom_histogram(bins=5)+facet_wrap(~variety,scale=\"free\",ncol=2)\n\n\n\n\nbihai has those two outliers, caribaea red has no tails to speak of (or you might say “it’s bimodal”, which would be another explanation of the pattern on the normal quantile plot1) and caribaea yellow is shoulder-shruggingly normal (I looked at that and said, “well, I guess it’s normal”.) After you’ve looked at the normal quantile plots, you see what a crude tool a histogram is for assessing normality.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "normal-quantile.html#ferritin-and-normality-1",
    "href": "normal-quantile.html#ferritin-and-normality-1",
    "title": "10  Normal quantile plots",
    "section": "10.4 Ferritin and normality",
    "text": "10.4 Ferritin and normality\nIn the lecture notes, we looked at some data on different athletes from the Australian Institute of Sport. This data set can be found at http://ritsokiguess.site/datafiles/ais.txt. Recall that the values are separated by tabs. In this question, we will assess one of the variables in the data set for normality.\n\nRead the data set into R.\n\nSolution\nread_tsv is the right thing:\n\nmy_url=\"http://ritsokiguess.site/datafiles/ais.txt\"\nathletes=read_tsv(my_url)\n\nRows: 202 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nathletes\n\n\n\n  \n\n\n\nI listed the data to check that I had it right, but I didn’t ask you to. (If you didn’t have it right, that will show up soon enough.)\n\\(\\blacksquare\\)\n\nOne of the variables, Ferr, is a measurement of Ferritin for each athlete. Obtain a normal quantile plot of the Ferritin values, for all the athletes together. What do you conclude about the shape of the distribution? Explain briefly.\n\nSolution\nAs you would expect:\n\nggplot(athletes, aes(sample=Ferr))+\nstat_qq()+stat_qq_line()\n\n\n\n\nThis is almost a classic right skew: the values are too bunched up at the bottom and too spread out at the top. The curved shape should be making you think “skewed” and then you can work out which way it’s skewed.\n\\(\\blacksquare\\)\n\nIt is possible that the shape you found in the previous part is because the athletes from all the different sports were mixed together. Use ggplot to obtain one normal quantile plot for each sport, collected together on one plot.\n\nSolution\nYour previous plot had all the sports mixed together. To that you add something that will put each sport in its own facet:\n\nggplot(athletes,aes(sample=Ferr))+stat_qq()+stat_qq_line()+\nfacet_wrap(~Sport)\n\n\n\n\n\\(\\blacksquare\\)\n\nLooking at the plots in the previous part, would you say that the Ferritin values for each sport individually have a more normal shape than they do for all the sports together? Explain briefly.\n\nSolution\nThere are a couple of ways you can go, and as ever I’m looking mostly for consistency of argument. The two major directions you can go are (i) most of these plots are still curved the same way as the previous one, and (ii) they are mostly straighter than they were before. Possible lines of argument include that pretty much all of these plots are right-skewed still, with the same upward-opening curve. Pretty much the only one that doesn’t is Gymnastics, for which there are only four observations, so you can’t really tell. So, by this argument, Ferritin just does have a right-skewed distribution, and breaking things out by sport doesn’t make much difference to that. Or, you could go another way and say that the plot of all the data together was very curved, and these plots are much less curved, that is to say, much less skewed. Some of them, such as basketball and netball, are almost straight, and they are almost normally distributed. Some of the distributions, such as track sprinting (TSprnt), are definitely still right-skewed, but not as seriously so as before. Decide what you think and then discuss how you see it.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "normal-quantile.html#footnotes",
    "href": "normal-quantile.html#footnotes",
    "title": "10  Normal quantile plots",
    "section": "",
    "text": "If you have studied a thing called kurtosis, the fourth moment about the mean, you’ll know that this measures both tail length and peakedness, so a short-tailed distribution also has a strong peak. Or, maybe, in this case, two strong peaks.↩︎"
  },
  {
    "objectID": "analysis-of-variance.html#movie-ratings-and-lengths",
    "href": "analysis-of-variance.html#movie-ratings-and-lengths",
    "title": "11  Analysis of variance",
    "section": "11.1 Movie ratings and lengths",
    "text": "11.1 Movie ratings and lengths\nBefore a movie is shown in theatres, it receives a “rating” that says what kind of material it contains. link explains the categories, from G (suitable for children) to R (anyone under 17 must be accompanied by parent/guardian). In 2011, two students collected data on the length (in minutes) and the rating category, for 15 movies of each rating category, randomly chosen from all the movies released that year. The data are at link.\n\nRead the data into R, and display (some of) what you read in.\nCount how many movies there are of each rating.\nCarry out an ANOVA and a Tukey analysis (if warranted).\nMake a graph to assess whether this ANOVA is trustworthy. Discuss your graph and its implications briefly."
  },
  {
    "objectID": "analysis-of-variance.html#deer-and-how-much-they-eat",
    "href": "analysis-of-variance.html#deer-and-how-much-they-eat",
    "title": "11  Analysis of variance",
    "section": "11.2 Deer and how much they eat",
    "text": "11.2 Deer and how much they eat\nDo adult deer eat different amounts of food at different times of the year? The data in link are the weights of food (in kilograms) consumed by randomly selected adult deer observed at different times of the year (in February, May, August and November). We will assume that these were different deer observed in the different months. (If the same animals had been observed at different times, we would have been in the domain of “repeated measures”, which would require a different analysis, beyond the scope of this course.)\n\nRead the data into R, and calculate numbers of observations and the median amounts of food eaten each month.\nMake side-by-side boxplots of the amount of food eaten each month. Comment briefly on what you see.\nRun a Mood’s median test as in lecture (ie. not using smmr). What do you conclude, in the context of the data?\nRun a Mood’s median test using smmr, and compare the results with the previous part.\nHow is it that Mood’s median test does not completely answer the question you really want to answer? How might you get an answer to the question you really want answered? Explain briefly, and obtain the answer you really want, discussing your results briefly."
  },
  {
    "objectID": "analysis-of-variance.html#movie-ratings-again",
    "href": "analysis-of-variance.html#movie-ratings-again",
    "title": "11  Analysis of variance",
    "section": "11.3 Movie ratings again",
    "text": "11.3 Movie ratings again\nThis question again uses the movie rating data at link.\n\nRead the data into R and obtain the number of movies of each rating and the median length of movies of each rating.\nObtain a suitable graph that assesses the assumptions for ANOVA. Why do you think it is not reasonable to run ANOVA here? Explain briefly.\nRun a Mood’s median test (use smmr if you like). What do you conclude, in the context of the data?"
  },
  {
    "objectID": "analysis-of-variance.html#atomic-weight-of-carbon",
    "href": "analysis-of-variance.html#atomic-weight-of-carbon",
    "title": "11  Analysis of variance",
    "section": "11.4 Atomic weight of carbon",
    "text": "11.4 Atomic weight of carbon\nThe atomic weight of the chemical element carbon is 12. Two methods of measuring the atomic weight of samples of carbon were compared. The results are shown in link. The methods are labelled 1 and 2. The first task is to find out whether the two methods have different “typical” measures (mean or median, as appropriate) of the atomic weight of carbon.\nFor this question, compose a report in R Markdown. (R Markdown is what you use in an R Notebook, but you can also have a separate R Markdown document from which you can produce HTML, Word etc. output.) See part (a) for how to get this started.\nYour report should read like an actual report, not just the answers to some questions that I set you. To help with that, write some text that links the parts of the report together smoothly, so that it reads as a coherent whole. The grader had 3 discretionary marks to award for the overall quality of your writing. The scale for this was:\n\n3 points: excellent writing. The report flows smoothly, is easy to read, and contains everything it should (and nothing it shouldn’t).\n2 points: satisfactory writing. Not the easiest to read, but says what it should, and it looks at least somewhat like a report rather than a string of answers to questions.\n1 point: writing that is hard to read or to understand. If you get this (or 0), you should consider what you need to do to improve when you write your project.\n0 points: you answered the questions, but you did almost nothing to make it read like a report.\n\n\nCreate a new R Markdown document. To do this, in R Studio, select File, New File, R Markdown. Type the report title and your name in the boxes, and leave the output on the default HTML. Click OK.\nWrite an introduction that explains the purpose of this study and the data collected in your own words.\nBegin an appropriately-titled new section in your report, read the data into R and display the results.\nMake an appropriate plot to compare the measurements obtained by the two methods. You might need to do something about the two methods being given as numbers even though they are really only identifiers. (If you do, your report ought to say what you did and why.)\nComment briefly on what you see in your plot.\nCarry out the most appropriate \\(t\\)-test. (You might like to begin another new section in your report here.)\nDo the most appropriate test you know that does not assume normally-distributed data.\nDiscuss the results of your tests and what they say about the two methods for measuring the atomic weight of carbon. If it seems appropriate, put the discussion into a section called Conclusions."
  },
  {
    "objectID": "analysis-of-variance.html#can-caffeine-improve-your-performance-on-a-test",
    "href": "analysis-of-variance.html#can-caffeine-improve-your-performance-on-a-test",
    "title": "11  Analysis of variance",
    "section": "11.5 Can caffeine improve your performance on a test?",
    "text": "11.5 Can caffeine improve your performance on a test?\nDoes caffeine help students do better on a certain test? To find out, 36 students were randomly allocated to three groups (12 in each group). Each student received a fixed number of cups of coffee while they were studying, but the students didn’t know whether they were receiving all full-strength coffee (“high”), all decaf coffee (“low”) or a 50-50 mixture of the two (“moderate”). For each subject, their group was recorded as well as their score on the test. The data are in link, as a .csv file.\n\nRead in and examine the data. How are the values laid out?\nExplain briefly how the data are not “tidy”.\nUse a suitable tool from the tidyverse to create one column of test scores and and one column of group labels. Call your column of group labels amount. Is it a factor?\nObtain side-by-side boxplots of test scores by amount of caffeine.\nDoes caffeine amount seem to have an effect? If so, what kind of effect?\nRun a suitable analysis of variance to determine whether the mean test score is equal or unequal for the three groups. What do you conclude?\nWhy is it a good idea to run Tukey’s method here?\nRun Tukey’s method. What do you conclude?"
  },
  {
    "objectID": "analysis-of-variance.html#reggae-music",
    "href": "analysis-of-variance.html#reggae-music",
    "title": "11  Analysis of variance",
    "section": "11.6 Reggae music",
    "text": "11.6 Reggae music\nReggae is a music genre that originated in Jamaica in the late 1960s. One of the most famous reggae bands was Bob Marley and the Wailers. In a survey, 729 students were asked to rate reggae music on a scale from 1, “don’t like it at all” to 6, “like it a lot”. We will treat the ratings as quantitative. Each student was also asked to classify their home town as one of “big city”, “suburban”, “small town”, “rural”. Does a student’s opinion of reggae depend on the kind of home town they come from? The data are in http://ritsokiguess.site/datafiles/reggae.csv.\n\nRead in and display (some of) the data.\nHow many students are from each different size of town?\nMake a suitable graph of the two variables in this data frame.\nDiscuss briefly why you might prefer to run Mood’s median test to compare ratings among home towns.\nSuppose that somebody wanted to run Welch ANOVA on these data. What would be a reasonable argument to support that?\nRun Mood’s median test and display the output.\nExplain briefly why running pairwise median tests is a good idea, run them, and display the results.\nSummarize, as concisely as possible, how the home towns differ in terms of their students’ ratings of reggae music."
  },
  {
    "objectID": "analysis-of-variance.html#watching-tv-and-education",
    "href": "analysis-of-variance.html#watching-tv-and-education",
    "title": "11  Analysis of variance",
    "section": "11.7 Watching TV and education",
    "text": "11.7 Watching TV and education\nThe General Social Survey is a large survey of a large number of people. One of the questions on the survey is “how many hours of TV do you watch in a typical day?” Another is “what is your highest level of education attained”, on this scale:\n\nHSorLess: completed no more than high h school\nCollege: completed some form of college, either a community college (like Centennial) or a four-year university (like UTSC)\nGraduate: completed a graduate degree such as an MSc.\n\nDo people with more education tend to watch more TV? We will be exploring this. The data are in http://ritsokiguess.site/datafiles/gss_tv.csv.\n\nRead in and display (some of) the data.\nFor each level of education, obtain the number of observations, the mean and the median of the number of hours of TV watched.\nWhat does your answer to the previous part tell you about the shapes of the distributions of the numbers of hours of TV watched? Explain briefly.\nObtain a suitable graph of your data frame.\nDoes your plot indicate that your guess about the distribution shape was correct? Explain briefly.\nRun a suitable test to compare the average number of hours of TV watched for people with each amount of education. (“Average” could be mean or median, whichever you think is appropriate.)\nWhat do you conclude from your test, in the context of the data?\nWhy might you now want to run some kind of follow-up test? Run the appropriate thing and explain briefly what you conclude from it, in the context of the data."
  },
  {
    "objectID": "analysis-of-variance.html#death-of-poets",
    "href": "analysis-of-variance.html#death-of-poets",
    "title": "11  Analysis of variance",
    "section": "11.8 Death of poets",
    "text": "11.8 Death of poets\nSome people believe that poets, especially female poets, die younger than other types of writer. William Butler Yeats1 wrote:\n\nShe is the Gaelic2 muse, for she gives inspiration to those she persecutes. The Gaelic poets die young, for she is restless, and will not let them remain long on earth.\n\nA literature student wanted to investigate this, and so collected a sample of 123 female writers (of three different types), and noted the age at death of each writer.\nThe data are in http://ritsokiguess.site/datafiles/writers.csv.\n\nRead in and display (some of) the data.\nMake a suitable plot of the ages and types of writing.\nObtain a summary table showing, for each type of writing, the number of writers of that type, along with the mean, median and standard deviation of their ages at death.\nRun a complete analysis, starting with an ordinary (not Welch) analysis of variance, that ends with a conclusion in the context of the data and an assessment of assumptions."
  },
  {
    "objectID": "analysis-of-variance.html#religion-and-studying",
    "href": "analysis-of-variance.html#religion-and-studying",
    "title": "11  Analysis of variance",
    "section": "11.9 Religion and studying",
    "text": "11.9 Religion and studying\nMany students at a certain university were asked about the importance of religion in their lives (categorized as “not”, “fairly”, or “very” important), and also about the number of hours they spent studying per week. (This was part of a much larger survey.) We want to see whether there is any kind of relationship between these two variables. The data are in here.\n\nRead in and display (some of) the data.\nObtain the number of observations and the mean and standard deviation of study hours for each level of importance.\nComment briefly on how the groups compare in terms of study hours.\nMake a suitable graph of this data set.\nThe statistician in this study decided that the data were sufficiently normal in shape given the (very large) sample sizes, but was concerned about unequal spreads among the three groups. Given this, run a suitable analysis and display the output. (This includes a suitable follow-up test, if warranted.)\nWhat do you conclude from your analysis of the previous part, in the context of the data?\n\nMy solutions follow:"
  },
  {
    "objectID": "analysis-of-variance.html#movie-ratings-and-lengths-1",
    "href": "analysis-of-variance.html#movie-ratings-and-lengths-1",
    "title": "11  Analysis of variance",
    "section": "11.10 Movie ratings and lengths",
    "text": "11.10 Movie ratings and lengths\nBefore a movie is shown in theatres, it receives a “rating” that says what kind of material it contains. link explains the categories, from G (suitable for children) to R (anyone under 17 must be accompanied by parent/guardian). In 2011, two students collected data on the length (in minutes) and the rating category, for 15 movies of each rating category, randomly chosen from all the movies released that year. The data are at link.\n\nRead the data into R, and display (some of) what you read in.\n\nSolution\nread_csv:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/movie-lengths.csv\"\nmovies &lt;- read_csv(my_url)\n\nRows: 60 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): rating\ndbl (1): length\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmovies\n\n\n\n  \n\n\n\nSomething that looks like a length in minutes, and a rating.\n\\(\\blacksquare\\)\n\nCount how many movies there are of each rating.\n\nSolution\n\nmovies %&gt;% count(rating)\n\n\n\n  \n\n\n\nFifteen of each rating. (It’s common to have the same number of observations in each group, but not necessary for a one-way ANOVA.)\n\\(\\blacksquare\\)\n\nCarry out an ANOVA and a Tukey analysis (if warranted).\n\nSolution\nANOVA first:\n\nlength.1 &lt;- aov(length ~ rating, data = movies)\nsummary(length.1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nrating       3  14624    4875   11.72 4.59e-06 ***\nResiduals   56  23295     416                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis P-value is 0.00000459, which is way less than 0.05.\nHaving rejected the null (which said “all means equal”), we now need to do Tukey, thus:\n\nTukeyHSD(length.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = length ~ rating, data = movies)\n\n$rating\n               diff        lwr       upr     p adj\nPG-G      26.333333   6.613562 46.053104 0.0044541\nPG-13-G   42.800000  23.080229 62.519771 0.0000023\nR-G       30.600000  10.880229 50.319771 0.0007379\nPG-13-PG  16.466667  -3.253104 36.186438 0.1327466\nR-PG       4.266667 -15.453104 23.986438 0.9397550\nR-PG-13  -12.200000 -31.919771  7.519771 0.3660019\n\n\nCast your eye down the p adj column and look for the ones that are significant, here the first three. These are all comparisons with the G (“general”) movies, which are shorter on average than the others (which are not significantly different from each other).\nIf you like, you can make a table of means to verify that:\n\nmovies %&gt;%\n  group_by(rating) %&gt;%\n  summarize(mean = mean(length))\n\n\n\n  \n\n\n\nWhen we do this problem in SAS, you’ll see the Tukey get handled a different way, one that you might find more appealing.\n\\(\\blacksquare\\)\n\nMake a graph to assess whether this ANOVA is trustworthy. Discuss your graph and its implications briefly.\n\nSolution\nThe obvious graph is a boxplot:\n\nggplot(movies, aes(x = rating, y = length)) + geom_boxplot()\n\n\n\n\nFor ANOVA, we are looking for approximately normal distributions within each group and approximately equal spreads. Without the outliers, I would be more or less happy with that, but the G movies have a low outlier that would pull the mean down and the PG and PG-13 movies have outliers that would pull the mean up. So a comparison of means might make the differences look more significant than they should. Having said that, you could also say that the ANOVA is very significant, so even considering the effect of the outliers, the differences between G and the others are still likely to be significant.\nExtra: the way to go if you don’t trust the ANOVA is (as for the two-sample \\(t\\)) the Mood’s median test. This applies to any number of groups, and works in the same way as before:\n\nlibrary(smmr)\nmedian_test(movies, length, rating)\n\n$table\n       above\ngroup   above below\n  G         2    13\n  PG        7     7\n  PG-13    12     3\n  R         8     6\n\n$test\n       what        value\n1 statistic 13.752380952\n2        df  3.000000000\n3   P-value  0.003262334\n\n\nStill significant, though not quite as small a P-value as before (which echoes our thoughts about what the outliers might do to the means). If you look at the table above the test results, you see that the G movies are mostly shorter than the overall median, but now the PG-13 movies are mostly longer. So the picture is a little different.\nMood’s median test does not naturally come with something like Tukey. What you can do is to do all the pairwise Mood’s median tests, between each pair of groups, and then adjust to allow for your having done several tests at once. I thought this was generally useful enough that I put it into smmr under the name pairwise_median_test:\n\npairwise_median_test(movies, length, rating)\n\n\n\n  \n\n\n\nYou can ignore those (adjusted) P-values rather stupidly bigger than 1. These are not significant.\nThere are two significant differences in median length: between G movies and the two flavours of PG movies. The G movies are significantly shorter (as you can tell from the boxplot), but the difference between G and R movies is no longer significant (a change from the regular ANOVA).\nYou may be puzzled by something in the boxplot: how is it that the G movies are significantly shorter than the PG movies, but not significantly shorter than the R movies, when the difference in medians between G and R movies is bigger? In Tukey, if the difference in means is bigger, the P-value is smaller.3 The resolution to this puzzle, such as it is, is that Mood’s median test is not directly comparing the medians of the groups (despite its name); it’s counting values above and below a joint median, which might be a different story.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#deer-and-how-much-they-eat-1",
    "href": "analysis-of-variance.html#deer-and-how-much-they-eat-1",
    "title": "11  Analysis of variance",
    "section": "11.11 Deer and how much they eat",
    "text": "11.11 Deer and how much they eat\nDo adult deer eat different amounts of food at different times of the year? The data in link are the weights of food (in kilograms) consumed by randomly selected adult deer observed at different times of the year (in February, May, August and November). We will assume that these were different deer observed in the different months. (If the same animals had been observed at different times, we would have been in the domain of “repeated measures”, which would require a different analysis, beyond the scope of this course.)\n\nRead the data into R, and calculate numbers of observations and the median amounts of food eaten each month.\n\nSolution\nThe usual stuff for data values separated by (single) spaces:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/deer.txt\"\ndeer &lt;- read_delim(myurl, \" \")\n\nRows: 22 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): month\ndbl (1): food\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nand then, recalling that n() is the handy way of getting the number of observations in each group:\n\ndeer %&gt;%\n  group_by(month) %&gt;%\n  summarize(n = n(), med = median(food))\n\n\n\n  \n\n\n\nWhen you want the number of observations plus some other summaries, as here, the group-by and summarize idea is the way, using n() to get the number of observations in each group. count counts the number of observations per group when you only have grouping variables.\nThe medians differ a bit, but it’s hard to judge without a sense of spread, which the boxplots (next) provide. November is a bit higher and May a bit lower.\n\\(\\blacksquare\\)\n\nMake side-by-side boxplots of the amount of food eaten each month. Comment briefly on what you see.\n\nSolution\n\nggplot(deer, aes(x = month, y = food)) + geom_boxplot()\n\n\n\n\nThis offers the suggestion that maybe November will be significantly higher than the rest and May significantly lower, or at least they will be significantly different from each other.\nThis is perhaps getting ahead of the game: we should be thinking about spread and shape. Bear in mind that there are only 5 or 6 observations in each group, so you won’t be able to say much about normality. In any case, we are going to be doing a Mood’s median test, so any lack of normality doesn’t matter (eg. perhaps that 4.4 observation in August). Given the small sample sizes, I actually think the spreads are quite similar.\nAnother way of looking at the data, especially with these small sample sizes, is a “dot plot”: instead of making a boxplot for each month, we plot the actual points for each month as if we were making a scatterplot:\n\nggplot(deer, aes(x = month, y = food)) + geom_point()\n\n\n\n\nWait a minute. There were five deer in February and six in August. Where did they go?\nThe problem is overplotting: more than one of the deer plotted in the same place on the plot, because the amounts of food eaten were only given to one decimal place and there were some duplicated values. One way to solve this is to randomly move the points around so that no two of them plot in the same place. This is called jittering, and is done like this:\n\nggplot(deer, aes(x = month, y = food)) + geom_jitter(width = 0, height = 0.05)\n\n\n\n\nNow you see all the deer, and you can see that two pairs of points in August and one pair of points in February are close enough on the jittered plot that they would have been the same to one decimal place.\nI wanted to keep the points above the months they belong to, so I only allowed vertical jitter (that’s the width and height in the geom_jitter; the width is zero so there is no horizontal jittering). If you like, you can colour the months; it’s up to you whether you think that’s making the plot easier to read, or is overkill (see my point on the facetted plots on the 2017 midterm).\nThis way you see the whole distribution for each month. Normally it’s nicer to see the summary made by the boxplots, but here there are not very many points. The value of 4.4 in August does look quite a bit lower than the rest, but the other months look believably normal given the small sample sizes. I don’t know about equal spreads (November looks more spread out), but normality looks believable. Maybe this is the kind of situation in which Welch’s ANOVA is a good idea. (If you believe that the normality-with-unequal-spreads is a reasonable assumption to make, then the Welch ANOVA will be more powerful than the Mood’s median test, and so should be preferred.)\n\\(\\blacksquare\\)\n\nRun a Mood’s median test as in lecture (ie. not using smmr). What do you conclude, in the context of the data?\n\nSolution\nTo give you some practice with the mechanics, first find the overall median:\n\ndeer %&gt;% summarize(med = median(food))\n\n\n\n  \n\n\n\nor\n\nmedian(deer$food)\n\n[1] 4.7\n\n\nI like the first way because it’s the same idea as we did before, just not differentiating by month. I think there are some observations exactly equal to the median, which will mess things up later:\n\ndeer %&gt;% filter(food == 4.7)\n\n\n\n  \n\n\n\nThere are, two in February and two in August.\nNext, make (and save) a table of the observations within each month that are above and below this median:\n\ntab1 &lt;- with(deer, table(month, food &lt; 4.7))\ntab1\n\n     \nmonth FALSE TRUE\n  Aug     4    2\n  Feb     5    0\n  May     0    6\n  Nov     5    0\n\n\nor\n\ntab2 &lt;- with(deer, table(month, food &gt; 4.7))\ntab2\n\n     \nmonth FALSE TRUE\n  Aug     4    2\n  Feb     2    3\n  May     6    0\n  Nov     0    5\n\n\nEither of these is good, but note that they are different. Two of the February observations (the ones that were exactly 4.7) have “switched sides”, and (look carefully) two of the August ones also. Hence the test results will be different, and smmr (later) will give different results again:\n\nchisq.test(tab1, correct = F)\n\nWarning in chisq.test(tab1, correct = F): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tab1\nX-squared = 16.238, df = 3, p-value = 0.001013\n\nchisq.test(tab2, correct = F)\n\nWarning in chisq.test(tab2, correct = F): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tab2\nX-squared = 11.782, df = 3, p-value = 0.008168\n\n\nThe warnings are because of the small frequencies. If you’ve done these by hand before (which you will have if you took PSYC08), you’ll remember that thing about “expected frequencies less than 5”. This is that. It means “don’t take those P-values too seriously.”\nThe P-values are different, but they are both clearly significant, so the median amounts of food eaten in the different months are not all the same. (This is the same “there are differences” that you get from an ANOVA, which you would follow up with Tukey.) Despite the injunction not to take the P-values too seriously, I think these are small enough that they could be off by a bit without affecting the conclusion.\nThe first table came out with a smaller P-value because it looked more extreme: all of the February measurements were taken as higher than the overall median (since we were counting “strictly less” and “the rest”). In the second table, the February measurements look more evenly split, so the overall P-value is not quite so small.\nYou can make a guess as to what smmr will come out with (next), since it throws away any data values exactly equal to the median.\n\\(\\blacksquare\\)\n\nRun a Mood’s median test using smmr, and compare the results with the previous part.\n\nSolution\nOff we go:\n\nlibrary(smmr)\nmedian_test(deer, food, month)\n\n$table\n     above\ngroup above below\n  Aug     2     2\n  Feb     3     0\n  May     0     6\n  Nov     5     0\n\n$test\n       what        value\n1 statistic 13.950000000\n2        df  3.000000000\n3   P-value  0.002974007\n\n\nThe P-value came out in between the other two, but the conclusion is the same all three ways: the months are not all the same in terms of median food eaten. The researchers can then go ahead and try to figure out why the animals eat different amounts in the different months.\nYou might be wondering how you could get rid of the equal-to-median values in the build-it-yourself way. This is filter from dplyr, which you use first:\n\ndeer2 &lt;- deer %&gt;% filter(food != 4.7)\ntab3 &lt;- with(deer2, table(month, food &lt; 4.7))\ntab3\n\n     \nmonth FALSE TRUE\n  Aug     2    2\n  Feb     3    0\n  May     0    6\n  Nov     5    0\n\nchisq.test(tab3)\n\nWarning in chisq.test(tab3): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tab3\nX-squared = 13.95, df = 3, p-value = 0.002974\n\n\nwhich is exactly what smmr does, so the answer is identical.4 How would an ANOVA come out here? My guess is, very similarly:\n\ndeer.1 &lt;- aov(food ~ month, data = deer)\nsummary(deer.1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmonth        3 2.3065  0.7688   22.08 2.94e-06 ***\nResiduals   18 0.6267  0.0348                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(deer.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = food ~ month, data = deer)\n\n$month\n              diff         lwr        upr     p adj\nFeb-Aug  0.1533333 -0.16599282  0.4726595 0.5405724\nMay-Aug -0.3333333 -0.63779887 -0.0288678 0.0290758\nNov-Aug  0.5733333  0.25400718  0.8926595 0.0004209\nMay-Feb -0.4866667 -0.80599282 -0.1673405 0.0021859\nNov-Feb  0.4200000  0.08647471  0.7535253 0.0109631\nNov-May  0.9066667  0.58734052  1.2259928 0.0000013\n\n\nThe conclusion is the same, but the P-value on the \\(F\\)-test is much smaller. I think this is because the \\(F\\)-test uses the actual values, rather than just whether they are bigger or smaller than 4.7. The Tukey says that all the months are different in terms of (now) mean, except for February and August, which were those two very similar ones on the boxplot.\n\\(\\blacksquare\\)\n\nHow is it that Mood’s median test does not completely answer the question you really want to answer? How might you get an answer to the question you really want answered? Explain briefly, and obtain the answer you really want, discussing your results briefly.\n\nSolution\nThat’s rather a lot, so let’s take those things one at a time.5\nMood’s median test is really like the \\(F\\)-test in ANOVA: it’s testing the null hypothesis that the groups (months) all have the same median (of food eaten), against the alternative that the null is not true. We rejected this null, but we don’t know which months differ significantly from which. To resolve this in ANOVA, we do Tukey (or Games-Howell if we did the Welch ANOVA). The corresponding thing here is to do all the possible two-group Mood tests on all the pairs of groups, and, after adjusting for doing (here) six tests at once, look at the adjusted P-values to see how the months differ in terms of food eaten.\nThis is accomplished in smmr via pairwise_median_test, thus:\n\npairwise_median_test(deer, food, month)\n\n\n\n  \n\n\n\nThis compares each month with each other month. Looking at the last column, there are only three significant differences: August-November, February-May and May-November. Going back to the table of medians we made in (a), November is significantly higher (in terms of median food eaten) than August and May (but not February), and February is significantly higher than May. The other differences are not big enough to be significant.\nExtra: Pairwise median tests done this way are not likely to be very sensitive (that is, powerful), for a couple of reasons: (i) the usual one that the median tests don’t use the data very efficiently, and (ii) the way I go from the unadjusted to the adjusted P-values is via Bonferroni (here, multiply the P-values by 6), which is known to be safe but conservative. This is why the Tukey produced more significant differences among the months than the pairwise median tests did.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#movie-ratings-again-1",
    "href": "analysis-of-variance.html#movie-ratings-again-1",
    "title": "11  Analysis of variance",
    "section": "11.12 Movie ratings again",
    "text": "11.12 Movie ratings again\nThis question again uses the movie rating data at link.\n\nRead the data into R and obtain the number of movies of each rating and the median length of movies of each rating.\n\nSolution\nReading in is as in the other question using these data (just copy your code, or mine).\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/movie-lengths.csv\"\nmovies &lt;- read_csv(my_url)\n\nRows: 60 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): rating\ndbl (1): length\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmovies\n\n\n\n  \n\n\n\nNow, the actual for-credit part, which is a group_by and summarize:\n\nmovies %&gt;%\n  group_by(rating) %&gt;%\n  summarize(count = n(), med = median(length))\n\n\n\n  \n\n\n\nThe G movies have a smaller median than the others, but also the PG-13 movies seem to be longer on average (not what we found before).\n\\(\\blacksquare\\)\n\nObtain a suitable graph that assesses the assumptions for ANOVA. Why do you think it is not reasonable to run ANOVA here? Explain briefly.\n\nSolution\nThe graph would seem to be a boxplot, side by side for each group:\n\nggplot(movies, aes(x = rating, y = length)) + geom_boxplot()\n\n\n\n\nWe are looking for approximate normal distributions with approximately equal spreads, which I don’t think we have: there are outliers, at the low end for G movies, and at the high end for PG and PG-13 movies. Also, you might observe that the distribution of lengths for R movies is skewed to the right. (Noting either the outliers or skewness as a reason for not believing normality is enough, since all we need is one way that normality fails.)\nI think the spreads (as measured by the interquartile ranges) are acceptably similar, but since we have rejected normality, it is a bit late for that.\nSo I think it is far from reasonable to run an ANOVA here. In my opinion 15 observations in each group is not enough to gain much from the Central Limit Theorem either.\nExtra: since part of the assumption for ANOVA is (approximate) normality, it would also be entirely reasonable to make normal quantile plots, one for each movie type, facetted. Remember the process: you pretend that you are making a normal quantile plot for all the data together, regardless of group, and then at the last minute, you throw in a facet_wrap. I’ve written the code out on three lines, so that you can see the pieces: the “what to plot”, then the normal quantile plot part, then the facetting:\n\nggplot(movies, aes(sample = length)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~rating)\n\n\n\n\nSince there are four movie ratings, facet_wrap has arranged them into a \\(2\\times 2\\) grid, which satisfyingly means that each normal quantile plot is more or less square and thus easy to interpret.\nThe principal problem unveiled by these plots is outliers. It looks as if the G movies have one low outlier, the PG movies have two high outliers, the PG-13 movies have one or maybe three high outliers (depending on how you count them), and the R movies have none. Another way to look at the last two is you could call them curved, with too much bunching up at the bottom and (on PG-13) too much spread-out-ness at the top, indicating right-skewed distributions. The distribution of lengths of the R-rated movies is too bunched up at the bottom, but as you would expect for a normal at the top. The R movies show the right-skewedness in an odd way: usually this skewness shows up by having too many high values, but this time it’s having too few low values.\nThe assumption for ANOVA is that all four of these are at least approximately normal (with the same spread). We found problems with the normality on at least three of them, so we definitely have doubts about trusting ANOVA here.\nI could have used scales=free here to get a separate \\(y\\)-axis for each plot, but since the \\(y\\)-axis is movie length each time, and all four groups would be expected to have at least roughly similar movie lengths, I left it as it was. (The other advantage of leaving the scales the same is that you can compare spread by comparing the slopes of the lines on these graphs; since the lines connect the observed and theoretical quartiles, a steeper slope means a larger IQR. Here, the R line is steepest and the PG line is flattest. Compare this with the spreads of the boxplots.)\nExtra extra: if you want, you can compare the normal quantile plots with the boxplots to see whether you get the same conclusion from both. For the G movies, the low outlier shows up both ways, and the rest of the distribution is at least more or less normal. For the PG movies, I’d say the distribution is basically normal except for the highest two values (on both plots). For the PG-13 movies, only the highest value shows up as an outlier, but the next two apparent outliers on the normal quantile plot are at the upper end of the long upper whisker, so the boxplot is saying “right-skewed with one upper outlier” rather than “three upper outliers”. The distribution of the R movies is skewed right, with the bunching at the bottom showing up as the very small lower whisker.\nThe boxplots and the normal quantile plots are basically telling the same story in each case, but they are doing it in a slightly different way.\n\\(\\blacksquare\\)\n\nRun a Mood’s median test (use smmr if you like). What do you conclude, in the context of the data?\n\nSolution\nThe smart way is to use smmr, since it is much easier:\n\nlibrary(smmr)\nmedian_test(movies, length, rating)\n\n$table\n       above\ngroup   above below\n  G         2    13\n  PG        7     7\n  PG-13    12     3\n  R         8     6\n\n$test\n       what        value\n1 statistic 13.752380952\n2        df  3.000000000\n3   P-value  0.003262334\n\n\nThe movies do not all have the same median length, or at least one of the rating types has movies of different median length from the others. Or something equivalent to that. It’s the same conclusion as for ANOVA, only with medians instead of means.\nYou can speculate about why the test came out significant. My guess is that the G movies are shorter than average, and that the PG-13 movies are longer than average. (We had the first conclusion before, but not the second. This is where medians are different from means.)\nThe easiest way to see which movie types really differ in length from which is to do all the pairwise median tests, which is in smmr thus:\n\npairwise_median_test(movies, length, rating)\n\n\n\n  \n\n\n\nThe inputs for this are the same ones in the same order as for median_test. (A design decision on my part, since otherwise I would never have been able to remember how to run these!) Only the first two of these are significant (look in the last column). We can remind ourselves of the sample medians:\n\nmovies %&gt;%\n  group_by(rating) %&gt;%\n  summarize(count = n(), med = median(length))\n\n\n\n  \n\n\n\nThe G movies are significantly shorter than the PG and PG-13 movies, but not quite significantly different from the R movies. This is a little odd, since the difference in sample medians between G and PG, significant, is less than for G and R (not significant). There are several Extras here, which you can skip if you don’t care about the background. First, we can do the median test by hand: This has about four steps: (i) find the median of all the data, (ii) make a table tabulating the number of values above and below the overall median for each group, (iii) test the table for association, (iv) draw a conclusion. Thus (i):\n\nmedian(movies$length)\n\n[1] 100\n\n\nor\n\nmovies %&gt;% summarize(med = median(length))\n\n\n\n  \n\n\n\nor store it in a variable, and then (ii):\n\ntab1 &lt;- with(movies, table(length &lt; 100, rating))\ntab1\n\n       rating\n         G PG PG-13  R\n  FALSE  2  8    12  9\n  TRUE  13  7     3  6\n\n\nor\n\ntab2 &lt;- with(movies, table(length &gt; 100, rating))\ntab2\n\n       rating\n         G PG PG-13  R\n  FALSE 13  8     3  7\n  TRUE   2  7    12  8\n\n\nThese differ because there are evidently some movies of length exactly 100 minutes, and it matters whether you count \\(&lt;\\) and \\(\\ge\\) (as in tab1) or \\(&gt;\\) and \\(le\\) (tab2). Either is good.\nWas I right about movies of length exactly 100 minutes?\n\nmovies %&gt;% filter(length == 100)\n\n\n\n  \n\n\n\nOne PG and one R. It makes a difference to the R movies, but if you look carefully, it makes a difference to the PG movies as well, because the False and True switch roles between tab1 and tab2 (compare the G movies, for instance). You need to store your table in a variable because it has to get passed on to chisq.test below, (iii):\n\nchisq.test(tab1, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  tab1\nX-squared = 14.082, df = 3, p-value = 0.002795\n\n\nor\n\nchisq.test(tab2, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  tab2\nX-squared = 13.548, df = 3, p-value = 0.003589\n\n\nEither is correct, or, actually, without the correct=FALSE.6\nThe conclusion (iv) is the same either way: the null of no association is clearly rejected (with a P-value of 0.0028 or 0.0036 as appropriate), and therefore whether a movie is longer or shorter than median length depends on what rating it has: that is, the median lengths do differ among the ratings. The same conclusion, in other words, as the \\(F\\)-test gave, though with not quite such a small P-value.\nSecond, you might be curious about how we might do something like Tukey having found some significant differences (that is, what’s lurking in the background of pairwise_median_test).\nLet’s first suppose we are comparing G and PG movies. We need to pull out just those, and then compare them using smmr. Because the first input to median_test is a data frame, it fits neatly into a pipe (with the data frame omitted):\n\nmovies %&gt;%\n  filter(rating == \"G\" | rating == \"PG\") %&gt;%\n  median_test(length, rating)\n\n$table\n     above\ngroup above below\n   G      4    11\n   PG    10     3\n\n$test\n       what       value\n1 statistic 7.035897436\n2        df 1.000000000\n3   P-value 0.007989183\n\n\nWe’re going to be doing this about six times — \\({4 \\choose 2}=6\\) choices of two rating groups to compare out of the four — so we should have a function to do it. I think the input to the function should be a data frame that has a column called rating, and two names of ratings to compare:\n\ncomp2 &lt;- function(rat_1, rat_2, d) {\n  d %&gt;%\n    filter(rating == rat_1 | rating == rat_2) %&gt;%\n    median_test(length, rating)\n}\n\nThe way I wrote this function is that you have to specify the movie ratings in quotes. It is possible to write it in such a way that you input them without quotes, tidyverse style, but that gets into “non-standard evaluation” and enquo() and !!, which (i) I have to look up every time I want to do it, and (ii) I am feeling that the effort involved in explaining it to you is going to exceed the benefit you will gain from it. I mastered it enough to make it work in smmr (note that you specify column names without quotes there). There are tutorials on this kind of thing if you’re interested.\nAnyway, testing:\n\ncomp2(\"G\", \"PG\", movies)\n\n$table\n     above\ngroup above below\n   G      4    11\n   PG    10     3\n\n$test\n       what       value\n1 statistic 7.035897436\n2        df 1.000000000\n3   P-value 0.007989183\n\n\nThat works, but I really only want to pick out the P-value, which is in the list item test in the column value, the third entry. So let’s rewrite the function to return just that:\n\ncomp2 &lt;- function(rat_1, rat_2, d) {\n  d %&gt;%\n    filter(rating == rat_1 | rating == rat_2) %&gt;%\n    median_test(length, rating) %&gt;%\n    pluck(\"test\", \"value\", 3)\n}\ncomp2(\"G\", \"PG\", movies)\n\n[1] 0.007989183\n\n\nGosh.\nWhat median_test returns is an R list that has two things in it, one called table and one called test. The thing called test is a data frame with a column called value that contains the P-values. The third of these is the two-sided P-value that we want.\nYou might not have seen pluck before. This is a way of getting things out of complicated data structures. This one takes the output from median_test and from it grabs the piece called test. This is a data frame. Next, we want the column called value, and from that we want the third row. These are specified one after the other to pluck and it pulls out the right thing.\nSo now our function returns just the P-value.\nI have to say that it took me several goes and some playing around in R Studio to sort this one out. Once I thought I understood pluck, I wondered why my function was not returning a value. And then I realized that I was saving the value inside the function and not returning it. Ooops. The nice thing about pluck is that I can put it on the end of the pipeline and and it will pull out (and return) whatever I want it to.\nLet’s grab a hold of the different rating groups we have:\n\nthe_ratings &lt;- unique(movies$rating)\nthe_ratings\n\n[1] \"G\"     \"PG-13\" \"PG\"    \"R\"    \n\n\nThe Pythonisti among you will know how to finish this off: do a loop-inside-a-loop over the rating groups, and get the P-value for each pair. You can do that in R, if you must. It’s not pretty at all, but it works:\n\nii &lt;- character(0)\njj &lt;- character(0)\npp &lt;- numeric(0)\nfor (i in the_ratings) {\n  for (j in the_ratings) {\n    pval &lt;- comp2(i, j, movies)\n    ii &lt;- c(ii, i)\n    jj &lt;- c(jj, j)\n    pp &lt;- c(pp, pval)\n  }\n}\ntibble(ii, jj, pp)\n\n\n\n  \n\n\n\nThis is a lot of fiddling about, since you have to initialize three vectors, and then update them every time through the loop. It’s hard to read, because the actual business part of the loop is the calculation of the P-value, and that’s almost hidden by all the book-keeping. (It’s also slow and inefficient, though the slowness doesn’t matter too much here since it’s not a very big problem.)\nLet’s try another way:\n\ncrossing(first = the_ratings, second = the_ratings)\n\n\n\n  \n\n\n\nThis does “all possible combinations” of one rating with another. We don’t actually need all of that; we just need the ones where the first one is (alphabetically) strictly less than the second one. This is because we’re never comparing a rating with itself, and each pair of ratings appears twice, once in alphabetical order, and once the other way around. The ones we need are these:\n\ncrossing(first = the_ratings, second = the_ratings) %&gt;%\n  filter(first &lt; second)\n\n\n\n  \n\n\n\nA technique thing to note: instead of asking “how do I pick out the distinct pairs of ratings?”, I use two simpler tools: first I make all the combinations of pairs of ratings, and then out of those, pick the ones that are alphabetically in ascending order, which we know how to do.\nNow we want to call our function comp2 for each of the things in first and each of the things in second, and make a new column called pval that contains exactly that. comp2 expects single movie ratings for each of its inputs, not a vector of each, so the way to go about this is rowwise:\n\ncrossing(first = the_ratings, second = the_ratings) %&gt;%\n  filter(first &lt; second) %&gt;%\n  rowwise() %&gt;% \n  mutate(pval = comp2(first, second, movies))\n\n\n\n  \n\n\n\nOne more thing: we’re doing 6 tests at once here, so we’re giving ourselves 6 chances to reject a null (all medians equal) that might have been true. So the true probability of a type I error is no longer 0.05 but something bigger.\nThe easiest way around that is to do a so-called Bonferroni adjustment: instead of rejecting if the P-value is less than 0.05, we only reject if it is less than \\(0.05/6\\), since we are doing 6 tests. This is a fiddly calculation to do by hand, but it’s easy to build in another mutate, thus:7\n\ncrossing(first = the_ratings, second = the_ratings) %&gt;%\n  filter(first &lt; second) %&gt;%\n  rowwise() %&gt;% \n  mutate(pval = comp2(first, second, movies)) %&gt;% \n  mutate(reject = (pval &lt; 0.05 / 6))\n\n\n\n  \n\n\n\nAnd not a loop in sight.\nThis is how I coded it in pairwise_median_test. If you want to check it, it’s on Github: link. The function median_test_pair is the same as comp2 above.\nSo the only significant differences are now G compared to PG and PG-13. There is not a significant difference in median movie length between G and R, though it is a close call. We thought the PG-13 movies might have a significantly different median from other rating groups beyond G, but they turn out not to have. (The third and fourth comparisons would have been significant had we not made the Bonferroni adjustment to compensate for doing six tests at once; with that adjustment, we only reject if the P-value is less than \\(0.05/6=0.0083\\), and so 0.0106 is not quite small enough to reject with.)\nListing the rating groups sorted by median would give you an idea of how far different the medians have to be to be significantly different:\n\nmedians &lt;- movies %&gt;%\n  group_by(rating) %&gt;%\n  summarize(med = median(length)) %&gt;%\n  arrange(desc(med))\nmedians\n\n\n\n  \n\n\n\nSomething rather interesting has happened: even though the comparison of G and PG (18 apart) is significant, the comparison of G and R (21 apart) is not significant. This seems very odd, but it happens because the Mood median test is not actually literally comparing the sample medians, but only assessing the splits of values above and below the median of the combined sample. A subtlety, rather than an error, I’d say.\nHere’s something extremely flashy to finish with:\n\ncrossing(first = the_ratings, second = the_ratings) %&gt;%\n  filter(first &lt; second) %&gt;%\n  rowwise() %&gt;% \n  mutate(pval = comp2(first, second, movies)) %&gt;% \n  mutate(reject = (pval &lt; 0.05 / 6)) %&gt;% \n  left_join(medians, by = c(\"first\" = \"rating\")) %&gt;%\n  left_join(medians, by = c(\"second\" = \"rating\"))\n\n\n\n  \n\n\n\nThe additional two lines look up the medians of the rating groups in first, then second, so that you can see the actual medians of the groups being compared each time. You see that medians different by 30 are definitely different, ones differing by 15 or less are definitely not different, and ones differing by about 20 could go either way.\nI think that’s quite enough of that.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#atomic-weight-of-carbon-1",
    "href": "analysis-of-variance.html#atomic-weight-of-carbon-1",
    "title": "11  Analysis of variance",
    "section": "11.13 Atomic weight of carbon",
    "text": "11.13 Atomic weight of carbon\nThe atomic weight of the chemical element carbon is 12. Two methods of measuring the atomic weight of samples of carbon were compared. The results are shown in link. The methods are labelled 1 and 2. The first task is to find out whether the two methods have different “typical” measures (mean or median, as appropriate) of the atomic weight of carbon.\nFor this question, compose a report in R Markdown. (R Markdown is what you use in an R Notebook, but you can also have a separate R Markdown document from which you can produce HTML, Word etc. output.) See part (a) for how to get this started.\nYour report should read like an actual report, not just the answers to some questions that I set you. To help with that, write some text that links the parts of the report together smoothly, so that it reads as a coherent whole. The grader had 3 discretionary marks to award for the overall quality of your writing. The scale for this was:\n\n3 points: excellent writing. The report flows smoothly, is easy to read, and contains everything it should (and nothing it shouldn’t).\n2 points: satisfactory writing. Not the easiest to read, but says what it should, and it looks at least somewhat like a report rather than a string of answers to questions.\n1 point: writing that is hard to read or to understand. If you get this (or 0), you should consider what you need to do to improve when you write your project.\n0 points: you answered the questions, but you did almost nothing to make it read like a report.\n\n\nCreate a new R Markdown document. To do this, in R Studio, select File, New File, R Markdown. Type the report title and your name in the boxes, and leave the output on the default HTML. Click OK.\n\nSolution\nYou’ll see the title and your name in a section at the top of the document, and below that you’ll see a template document, as you would for an R Notebook. The difference is that where you are used to seeing Preview, it now says “knit”, but this has the same effect of producing the formatted version of your report.\n\\(\\blacksquare\\)\n\nWrite an introduction that explains the purpose of this study and the data collected in your own words.\n\nSolution\nSomething like this:\n\nThis study is intended to compare two different methods (labelled 1 and 2) for measuring the atomic weight of carbon (which is known in actual fact to be 12). Fifteen samples of carbon were used; ten of these were assessed using method 1 and the remaining five using method 2. The primary interest in this particular study is to see whether there is a difference in the mean or median atomic weight as measured by the two methods.\n\nBefore that, start a new section like this: ## Introduction. Also, get used to expressing your understanding in your words, not mine. Using my words, in my courses, is likely to be worth very little.\n\\(\\blacksquare\\)\n\nBegin an appropriately-titled new section in your report, read the data into R and display the results.\n\nSolution\nValues separated by spaces:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/carbon.txt\"\ncarbon &lt;- read_delim(my_url, \" \")\n\nRows: 15 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): method, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncarbon\n\n\n\n  \n\n\n\nI would expect you to include, without being told to include it, some text in your report indicating that you have sensible data: two methods labelled 1 and 2 as promised, and a bunch8 of atomic weights close to the nominal figure of 12.\n\\(\\blacksquare\\)\n\nMake an appropriate plot to compare the measurements obtained by the two methods. You might need to do something about the two methods being given as numbers even though they are really only identifiers. (If you do, your report ought to say what you did and why.)\n\nSolution\nThe appropriate plot, with a categorical method and quantitative weight, is something like a boxplot. If you’re not careful, method will get treated as a quantitative variable, which you don’t want; the easiest way around that, for a boxplot at least, is to turn it into a factor like this:\n\nggplot(carbon, aes(x = factor(method), y = weight)) + geom_boxplot()\n\n\n\n\nIf you insist, you could do a faceted histogram (above and below, for preference):\n\nggplot(carbon, aes(x = weight)) + geom_histogram(bins = 5) +\n  facet_wrap(~method, ncol = 1)\n\n\n\n\nThere are really not enough data values for a histogram to be of much help, so I don’t like this as much.\nIf you are thinking ahead (we are going to be doing a \\(t\\)-test), then you’ll realize that normality is the kind of thing we’re looking for, in which case normal quantile plots would be the thing. However, we might have to be rather forgiving for method 2 since there are only 5 observations:\n\nggplot(carbon, aes(sample = weight)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~method)\n\n\n\n\nI don’t mind these coming out side by side, though I would rather have them squarer.\nI would say, boxplots are the best, normal quantile plots are also acceptable, but expect to lose something for histograms because they offer only a rather crude comparison in this case.\n\\(\\blacksquare\\)\n\nComment briefly on what you see in your plot.\n\nSolution\nIn boxplots, if that’s what you drew, there are several things that deserve comment: the medians, the spreads and the shapes. The median for method 1 is a little bit lower than for method 2 (the means are probably more different, given the shapes of the boxes). The spread for method 2 is a lot bigger. (Looking forward, that suggests a Welch-Satterthwaite rather than a pooled test.) As for shape, the method 2 measurements seem more or less symmetric (the whiskers are equal anyway, even if the position of the median in the box isn’t), but the method 1 measurements have a low outlier. The histograms are hard to compare. Try to say something about centre and spread and shape. I think the method 2 histogram has a slightly higher centre and definitely bigger spread. On my histogram for method 1, the distribution looks skewed left. If you did normal quantile plots, say something sensible about normality for each of the two methods. For method 1, I would say the low value is an outlier and the rest of the values look pretty straight. Up to you whether you think there is a curve on the plot (which would indicate skewness, but then that highest value is too high: it would be bunched up with the other values below 12.01 if there were really skewness). For method 2, it’s really hard to say anything since there are only five values. Given where the line goes, there isn’t much you can say to doubt normality. Perhaps the best you can say here is that in a sample of size 5, it’s difficult to assess normality at all.\n\\(\\blacksquare\\)\n\nCarry out the most appropriate \\(t\\)-test. (You might like to begin another new section in your report here.)\n\nSolution\nThis would be the Welch-Satterthwaite version of the two-sample \\(t\\)-test, since the two groups do appear to have different spreads:\n\nt.test(weight ~ method, data = carbon)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by method\nt = -1.817, df = 5.4808, p-value = 0.1238\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.027777288  0.004417288\nsample estimates:\nmean in group 1 mean in group 2 \n       12.00260        12.01428 \n\n\nImagining that this is a report that would go to your boss, you ought to defend your choice of the Welch-Satterthwaite test (as I did above), and not just do the default \\(t\\)-test without comment.\nIf, in your discussion above, you thought the spreads were equal enough, then you should do the pooled \\(t\\)-test here, which goes like this:\n\nt.test(weight ~ method, data = carbon, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  weight by method\nt = -2.1616, df = 13, p-value = 0.04989\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -2.335341e-02 -6.588810e-06\nsample estimates:\nmean in group 1 mean in group 2 \n       12.00260        12.01428 \n\n\nThe point here is that you should do the right test based on your conclusion. Being consistent is the most important thing. (In this case, note that the P-values are very different. We’ll get to that shortly.)\nIf we were doing this in SAS, as we see later, we’d get a test at the bottom of the output that compares the two variances. I feel that it’s just as good to eyeball the spreads and make a call about whether they are “reasonably close”. Or even, to always do the Welch-Satterthwaite test on the basis that it is pretty good even if the two populations have the same variance. (If this last point of view is one that you share, you ought to say something about that when you do your \\(t\\)-test.)\nExtra: I guess this is a good place to say something about tests for comparing variances, given that you might be pondering that. There are several that I can think of, that R can do, of which I mention two.\nThe first is the \\(F\\)-test for variances that you might have learned in B57 (that is the basis for the ANOVA \\(F\\)-test):\n\nvar.test(weight ~ method, data = carbon)\n\n\n    F test to compare two variances\n\ndata:  weight by method\nF = 0.35768, num df = 9, denom df = 4, p-value = 0.1845\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.04016811 1.68758230\nsample estimates:\nratio of variances \n         0.3576842 \n\n\nThis, unfortunately, is rather dependent on the data in the two groups being approximately normal. Since we are talking variances rather than means, there is no Central Limit Theorem to rescue us for large samples (quite aside from the fact that these samples are not large). Since the ANOVA \\(F\\)-test is based on the same theory, this is why normality is also more important in ANOVA than it is in a \\(t\\)-test.\nThe second is Levene’s test. This doesn’t depend on normality (at least, not nearly so much), so I like it better in general:\n\nlibrary(car)\nleveneTest(weight ~ factor(method), data = carbon)\n\n\n\n  \n\n\n\nLevene’s test takes a different approach: first the absolute differences from the group medians are calculated, and then an ANOVA is run on the absolute differences. If, say, one of the groups has a larger spread than the other(s), its absolute differences from the median will tend to be bigger.9 As for what we conclude here, well, neither of the variance tests show any significance at all, so from that point of view there is no evidence against using the pooled \\(t\\)-test. Having said that, the samples are small, and so it would be difficult to prove that the two methods have different variance, even if they actually did.10\nThings are never as clear-cut as you would like. In the end, it all comes down to making a call and defending it.\n\\(\\blacksquare\\)\n\nDo the most appropriate test you know that does not assume normally-distributed data.\n\nSolution\nThat would be Mood’s median test. Since I didn’t say anything about building it yourself, feel free to use smmr:\n\nlibrary(smmr)\nmedian_test(carbon, weight, method)\n\n$table\n     above\ngroup above below\n    1     3     6\n    2     4     1\n\n$test\n       what      value\n1 statistic 2.80000000\n2        df 1.00000000\n3   P-value 0.09426431\n\n\nAs an aside, if you have run into a non-parametric test such as Mann-Whitney or Kruskal-Wallis that applies in this situation, be careful about using it here, because they have additional assumptions that you may not want to trust. Mann-Whitney started life as a test for “equal distributions”.11 This means that the null is equal location and equal spread, and if you reject the null, one of those has failed. But here, we suspect that equal spread will fail, so that the Mann-Whitney test may end up rejecting whether or not the medians are different, so it won’t answer the question you want an answer to. Mood’s median test doesn’t have that problem; all it’s saying if the null is true is that the medians are equal; the spreads could be anything at all.\nThe same kind of issues apply to the signed-rank test vs. the sign test. In the case of the signed-rank test, the extra assumption is of a symmetric distribution — to my mind, if you don’t believe normality, you probably don’t have much confidence in symmetry either. That’s why I like the sign test and Mood’s median test: in the situation where you don’t want to be dealing with assumptions, these tests don’t make you worry about that.\nAnother comment that you don’t need to make is based on the not-quite-significance of the Mood test. The P-value is less than 0.10 but not less than 0.05, so it doesn’t quite reach significance by the usual standard. But if you look up at the table, the frequencies seem rather unbalanced: 6 out of the remaining 9 weights in group 1 are below the overall median, but 4 out of 5 weights in group 2 are above. This seems as if it ought to be significant, but bear in mind that the sample sizes are small, and thus Mood’s median test needs very unbalanced frequencies, which we don’t quite have here.\n\\(\\blacksquare\\)\n\nDiscuss the results of your tests and what they say about the two methods for measuring the atomic weight of carbon. If it seems appropriate, put the discussion into a section called Conclusions.\n\nSolution\nBegin by pulling out the P-values for your preferred test(s) and say what they mean. The P-value for the Welch-Satterthwaite \\(t\\)-test is 0.1238, which indicates no difference in mean atomic weights between the two methods. The Mood median test gives a similarly non-significant 0.0943, indicating no difference in the median weights. If you think both tests are plausible, then give both P-values and do a compare-and-contrast with them; if you think that one of the tests is clearly preferable, then say so (and why) and focus on that test’s results.\nIf you thought the pooled test was the right one, then you’ll have a bit more discussion to do, since its P-value is 0.0499, and at \\(\\alpha=0.05\\) this test disagrees with the others. If you are comparing this test with the Mood test, you ought to make some kind of reasoned recommendation about which test to believe.\nAs ever, be consistent in your reasoning.\nExtra: this dataset, where I found it, was actually being used to illustrate a case where the pooled and the Welch-Satterthwaite tests disagreed. The authors of the original paper that used this dataset (a 1987 paper by Best and Rayner;12 the data come from 1924!) point out that the pooled \\(t\\)-test can be especially misleading when the smaller sample is also the one with the larger variance. This is what happened here.\nIn the Best and Rayner paper, the Mood (or the Mann-Whitney) test was not being considered, but I think it’s good practice to draw a picture and make a call about which test is appropriate.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#can-caffeine-improve-your-performance-on-a-test-1",
    "href": "analysis-of-variance.html#can-caffeine-improve-your-performance-on-a-test-1",
    "title": "11  Analysis of variance",
    "section": "11.14 Can caffeine improve your performance on a test?",
    "text": "11.14 Can caffeine improve your performance on a test?\nDoes caffeine help students do better on a certain test? To find out, 36 students were randomly allocated to three groups (12 in each group). Each student received a fixed number of cups of coffee while they were studying, but the students didn’t know whether they were receiving all full-strength coffee (“high”), all decaf coffee (“low”) or a 50-50 mixture of the two (“moderate”). For each subject, their group was recorded as well as their score on the test. The data are in link, as a .csv file.\n\nRead in and examine the data. How are the values laid out?\n\nSolution\nread_csv because it’s a .csv file:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/caffeine.csv\"\ncaffeine.untidy &lt;- read_csv(my_url)\n\nRows: 12 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Sub, High, Moderate, None\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncaffeine.untidy\n\n\n\n  \n\n\n\nThe first column is the number of the subject (actually within each group, since each student only tried one amount of caffeine). Then follow the test scores for the students in each group, one group per column.\nI gave the data frame a kind of dumb name, since (looking ahead) I could see that I would need a less-dumb name for the tidied-up data, and it seemed sensible to keep caffeine for that.\n\\(\\blacksquare\\)\n\nExplain briefly how the data are not “tidy”.\n\nSolution\nThe last three columns are all scores on the test: that is, they all measure the same thing, so they should all be in the same column. Or, there should be a column of scores, and a separate column naming the groups. Or, there were 36 observations in the data, so there should be 36 rows. You always have a variety of ways to answer these, any of which will do.\n\\(\\blacksquare\\)\n\nUse a suitable tool from the tidyverse to create one column of test scores and and one column of group labels. Call your column of group labels amount. Is it a factor?\n\nSolution\nWe are combining several columns into one, so this is pivot_longer:\n\ncaffeine.untidy %&gt;% \n  pivot_longer(-Sub, names_to = \"amount\", values_to = \"score\") -&gt; caffeine\n\nI didn’t ask you to list the resulting data frame, but it is smart to at least look for yourself, to make sure pivot_longer has done what you expected.\n\ncaffeine\n\n\n\n  \n\n\n\nA column of amounts of caffeine, and a column of test scores. This is what we expected. There should be 12 each of the amounts, which you can check if you like:\n\ncaffeine %&gt;% count(amount)\n\n\n\n  \n\n\n\nIndeed.\nNote that amount is text, not a factor. Does this matter? We’ll see.\nThis is entirely the kind of situation where you need pivot_longer, so get used to seeing where it will be useful.\n\\(\\blacksquare\\)\n\nObtain side-by-side boxplots of test scores by amount of caffeine.\n\nSolution\n\nggplot(caffeine, aes(x = amount, y = score)) + geom_boxplot()\n\n\n\n\nNote that this is much more difficult if you don’t have a tidy data frame. (Try it and see.)\n\\(\\blacksquare\\)\n\nDoes caffeine amount seem to have an effect? If so, what kind of effect?\n\nSolution\nOn average, exam scores seem to be higher when the amount of caffeine is higher (with the effect being particularly pronounced for High caffeine). If you want to, you can also say the the effect of caffeine seems to be small, relative to the amount of variability there is (there is a lot). The point is that you say something supported by the boxplot.\n\\(\\blacksquare\\)\n\nRun a suitable analysis of variance to determine whether the mean test score is equal or unequal for the three groups. What do you conclude?\n\nSolution\nSomething like this:\n\ncaff.1 &lt;- aov(score ~ amount, data = caffeine)\nsummary(caff.1)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \namount       2  477.7  238.86   3.986 0.0281 *\nResiduals   33 1977.5   59.92                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe P-value on the \\(F\\)-test is less than 0.05, so we reject the null hypothesis (which says that all the groups have equal means) in favour of the alternative: the group means are not all the same (one or more of them is different from the others).\nNotice that the boxplot and the aov are quite happy for amount to be text rather than a factor (they actually do want a factor, but if the input is text, they’ll create one).\n\\(\\blacksquare\\)\n\nWhy is it a good idea to run Tukey’s method here?\n\nSolution\nThe analysis of variance \\(F\\)-test is significant, so that the groups are not all the same. Tukey’s method will tell us which group(s) differ(s) from the others. There are three groups, so there are differences to find that we don’t know about yet.\n\\(\\blacksquare\\)\n\nRun Tukey’s method. What do you conclude?\n\nSolution\nThis kind of thing:\n\ncaff.3 &lt;- TukeyHSD(caff.1)\ncaff.3\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = score ~ amount, data = caffeine)\n\n$amount\n                   diff       lwr       upr     p adj\nModerate-High -4.750000 -12.50468  3.004679 0.3025693\nNone-High     -8.916667 -16.67135 -1.161987 0.0213422\nNone-Moderate -4.166667 -11.92135  3.588013 0.3952176\n\n\nThe high-caffeine group definitely has a higher mean test score than the no-caffeine group. (The Moderate group is not significantly different from either of the other groups.) Both the comparisons involving Moderate could go either way (the interval for the difference in means includes zero). The None-High comparison, however, is away from zero, so this is the significant one. As is usual, we are pretty sure that the difference in means (this way around) is negative, but we are not at all clear about how big it is, because the confidence interval is rather long.13\nExtra: the normality and equal spreads assumptions look perfectly good, given the boxplots, and I don’t think there’s any reason to consider any other test. You might like to assess that with normal quantile plots:\n\nggplot(caffeine, aes(sample=score)) + stat_qq() +\n  stat_qq_line() + facet_wrap(~amount, ncol=2)\n\n\n\n\nThere’s nothing to worry about there normality-wise. If anything, there’s a little evidence of short tails (in the None group especially), but you’ll recall that short tails don’t affect the mean and thus pose no problems for the ANOVA. Those three lines also have pretty much the same slope, indicating very similar spreads. Regular ANOVA is the best test here. (Running eg. Mood’s median test would be a mistake here, because it doesn’t use the data as efficiently (counting only aboves and belows) as the ANOVA does, and so the ANOVA will give a better picture of what differs from what.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#reggae-music-1",
    "href": "analysis-of-variance.html#reggae-music-1",
    "title": "11  Analysis of variance",
    "section": "11.15 Reggae music",
    "text": "11.15 Reggae music\nReggae is a music genre that originated in Jamaica in the late 1960s. One of the most famous reggae bands was Bob Marley and the Wailers. In a survey, 729 students were asked to rate reggae music on a scale from 1, “don’t like it at all” to 6, “like it a lot”. We will treat the ratings as quantitative. Each student was also asked to classify their home town as one of “big city”, “suburban”, “small town”, “rural”. Does a student’s opinion of reggae depend on the kind of home town they come from? The data are in http://ritsokiguess.site/datafiles/reggae.csv.\n\nRead in and display (some of) the data.\n\nSolution\nThis is (evidently) a .csv, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/reggae.csv\"\nreggae &lt;- read_csv(my_url)\n\nRows: 729 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): home\ndbl (1): rating\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nreggae\n\n\n\n  \n\n\n\nThe students shown are all from big cities, but there are others, as you can check by scrolling down.\n\\(\\blacksquare\\)\n\nHow many students are from each different size of town?\n\nSolution\nThis is the usual kind of application of count:\n\nreggae %&gt;% count(home)\n\n\n\n  \n\n\n\nAnother, equally good, way (you can ignore the warning):\n\nreggae %&gt;% group_by(home) %&gt;% \nsummarize(n=n())\n\n\n\n  \n\n\n\nMost of the students in this data set are from suburbia.\n\\(\\blacksquare\\)\n\nMake a suitable graph of the two variables in this data frame.\n\nSolution\nOne quantitative, one categorical: a boxplot, as ever:\n\nggplot(reggae, aes(x=home, y=rating)) + geom_boxplot()\n\n\n\n\nExtra 1: the last three boxplots really are identical, because the medians, means, quartiles and extreme values are all equal. However, the data values are not all the same, as you see below.\nExtra 2: I said that the ratings should be treated as quantitative, to guide you towards this plot. You could otherwise have taken the point of view that the ratings were (ordered) categorical, in which case the right graph would have been a grouped bar chart, as below. There is a question about which variable should be x and which should be fill. I am taking the point of view that we want to compare ratings within each category of home, which I think makes sense here (see discussion below), which breaks my “rule” that the categorical variable with fewer categories should be x.14\n\nggplot(reggae, aes(x=home, fill=factor(rating))) + geom_bar(position = \"dodge\")\n\n\n\n\n\\(\\blacksquare\\)\n\nDiscuss briefly why you might prefer to run Mood’s median test to compare ratings among home towns.\n\nSolution\nThe issue here is whether all of the rating distributions (within each category of home) are sufficiently close to normal in shape. The “big city” group is clearly skewed to the left. This is enough to make us favour Mood’s median test over ANOVA.\nA part-marks answer is to note that the big-city group has smaller spread than the other groups (as measured by the IQR). This is answering the wrong question, though. Remember the process: first we assess normality. If that fails, we use Mood’s median test. Then, with normality OK, we assess equal spreads. If that fails, we use Welch ANOVA, and if both normality and equal spreads pass, we use regular ANOVA.\n\\(\\blacksquare\\)\n\nSuppose that somebody wanted to run Welch ANOVA on these data. What would be a reasonable argument to support that?\n\nSolution\nThe argument would have to be that normality is all right, given the sample sizes. We found earlier that there are between 89 and 368 students in each group. These are large samples, and might be enough to overcome the non-normality we see.\nThe only real concern I have is with the big city group. This is the least normal, and also the smallest sample. The other groups seem to have the kind of non-normality that will easily be taken care of by the sample sizes we have.\nExtra: the issue is really about the sampling distribution of the mean within each group. Does that look normal enough? This could be assessed by looking at each group, one at a time, and taking bootstrap samples. Here’s the big-city group:\n\nreggae %&gt;% filter(home==\"big city\") -&gt; bigs\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(bigs$rating, replace = T))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12)\n\n\n\n\nNot too much wrong with that. This shows that the sample size is indeed big enough to cope with the skewness.\nYou can do any of the others the same way.\nIf you’re feeling bold, you can get hold of all three bootstrapped sampling distributions at once, like this:\n\nreggae %&gt;% \n  nest_by(home) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$rating, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12) +\n  facet_wrap(~home, scales = \"free\")\n\n\n\n\nAll of these distributions look very much normal, so there is no cause for concern anywhere.\nThis was rather a lot of code, so let me take you through it. The first thing is that we want to treat the different students’ homes separately, so the first step is this:\n\nreggae %&gt;% \n  nest_by(home) \n\n\n\n  \n\n\n\nThis subdivides the students’ reggae ratings according to where their home is. The things in data are data frames containing a column rating for in each case the students who had the home shown.\nNormally, we would start by making a dataframe with a column called sim that labels the 1000 or so simulations. This time, we want four sets of simulations, one for each home, which we can set up this way:\n\nreggae %&gt;% \n  nest_by(home) %&gt;% \n  mutate(sim = list(1:1000)) \n\n\n\n  \n\n\n\nThe definition of sim happens by group, or rowwise, by home (however you want to look at it). Next, we need to spread out those sim values so that we’ll have one row per bootstrap sample:\n\nreggae %&gt;% \n  nest_by(home) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) \n\n\n\n  \n\n\n\n\\(4 \\times 1000 = 4000\\) rows. Note that the data column now contains multiple copies of all the ratings for the students with that home, which seems wasteful, but it makes our life easier because what we want is a bootstrap sample from the right set of students, namely the rating column from the dataframe data in each row. Thus, from here out, everything is the same as we have done before: work rowwise, get a bootstrap sample , find its mean, plot it. The one thing we need to be careful of is to make a separate histogram for each home, since each of the four distributions need to look normal. I used different scales for each one, since they are centred in different places; this has the side benefit of simplifying the choice of the number of bins. (See what happens if you omit the scales = \"free\".)\nIn any case, all is absolutely fine. We’ll see how this plays out below.\n\\(\\blacksquare\\)\n\nRun Mood’s median test and display the output.\n\nSolution\nData frame, quantitative column, categorical column:\n\nmedian_test(reggae, rating, home)\n\n$table\n            above\ngroup        above below\n  big city      51    21\n  rural         25    49\n  small town    64    89\n  suburban     120   187\n\n$test\n       what        value\n1 statistic 2.733683e+01\n2        df 3.000000e+00\n3   P-value 5.003693e-06\n\n\n\\(\\blacksquare\\)\n\nExplain briefly why running pairwise median tests is a good idea, run them, and display the results.\n\nSolution\nThe Mood’s median test is significant, with a P-value of 0.000005, so the median ratings are not all the same. We want to find out how they differ.\n(The table of aboves and belows, and for that matter the boxplot earlier, suggest that big-city will be different from the rest, but it is not clear whether there will be any other significant differences.)\n\npairwise_median_test(reggae, rating, home)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nSummarize, as concisely as possible, how the home towns differ in terms of their students’ ratings of reggae music.\n\nSolution\nThe students from big cities like reggae more than students from other places. The other kinds of hometown do not differ significantly.\nExtra 1: Given the previous discussion, you might be wondering how Welch ANOVA (and maybe even regular ANOVA) compare. Let’s find out:\n\noneway.test(rating~home,data=reggae)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  rating and home\nF = 16.518, num df = 3.00, denom df = 257.07, p-value = 7.606e-10\n\n\nand\n\ngamesHowellTest(rating~factor(home),data=reggae)\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: rating by factor(home)\n\n\n           big city rural small town\nrural      1.1e-07  -     -         \nsmall town 2.9e-06  0.74  -         \nsuburban   4.9e-09  0.91  0.94      \n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nThe conclusions are identical with Mood’s median test, and the P-values are not that different, either.\nThis makes me wonder how an ordinary ANOVA with Tukey would have come out:\n\nreggae %&gt;% \naov(rating~home, data=.) %&gt;% \nTukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = rating ~ home, data = .)\n\n$home\n                           diff        lwr        upr     p adj\nrural-big city      -1.20681180 -1.8311850 -0.5824386 0.0000048\nsmall town-big city -1.00510725 -1.5570075 -0.4532070 0.0000194\nsuburban-big city   -1.09404006 -1.5952598 -0.5928203 0.0000002\nsmall town-rural     0.20170455 -0.3366662  0.7400753 0.7695442\nsuburban-rural       0.11277174 -0.3735106  0.5990540 0.9329253\nsuburban-small town -0.08893281 -0.4778062  0.2999406 0.9354431\n\n\nAgain, almost identical.\nExtra 2: some Bob Marley and the Wailers for you:\n\nfrom 1980\nfrom 1973\n\nReggae music at its finest.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#watching-tv-and-education-1",
    "href": "analysis-of-variance.html#watching-tv-and-education-1",
    "title": "11  Analysis of variance",
    "section": "11.16 Watching TV and education",
    "text": "11.16 Watching TV and education\nThe General Social Survey is a large survey of a large number of people. One of the questions on the survey is “how many hours of TV do you watch in a typical day?” Another is “what is your highest level of education attained”, on this scale:\n\nHSorLess: completed no more than high h school\nCollege: completed some form of college, either a community college (like Centennial) or a four-year university (like UTSC)\nGraduate: completed a graduate degree such as an MSc.\n\nDo people with more education tend to watch more TV? We will be exploring this. The data are in http://ritsokiguess.site/datafiles/gss_tv.csv.\n\nRead in and display (some of) the data.\n\nSolution\nExactly the usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/gss_tv.csv\"\ngss &lt;- read_csv(my_url)\n\nRows: 905 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): degree\ndbl (1): tvhours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngss\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nFor each level of education, obtain the number of observations, the mean and the median of the number of hours of TV watched.\n\nSolution\ngroup_by and summarize, using n() to get the number of observations (rather than count because you want some numerical summaries as well):\n\ngss %&gt;% group_by(degree) %&gt;% \nsummarise(n=n(), mean=mean(tvhours), med=median(tvhours))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nWhat does your answer to the previous part tell you about the shapes of the distributions of the numbers of hours of TV watched? Explain briefly.\n\nSolution\nIn each of the three groups, the mean is greater than the median, so I think the distributions are skewed to the right. Alternatively, you could say that you expect to see some outliers at the upper end.\n\\(\\blacksquare\\)\n\nObtain a suitable graph of your data frame.\n\nSolution\nOne quantitative variable and one categorical one, so a boxplot. (I hope you are getting the hang of this by now.)\n\nggplot(gss, aes(x=degree, y=tvhours)) + geom_boxplot()\n\n\n\n\n\\(\\blacksquare\\)\n\nDoes your plot indicate that your guess about the distribution shape was correct? Explain briefly.\n\nSolution\nI guessed before that the distributions would be right-skewed, and they indeed are, with the long upper tails. Or, if you suspected upper outliers, they are here as well.\nSay what you guessed before, and how your graph confirms it (or doesn’t, if it doesn’t.)\n\\(\\blacksquare\\)\n\nRun a suitable test to compare the average number of hours of TV watched for people with each amount of education. (“Average” could be mean or median, whichever you think is appropriate.)\n\nSolution\nFrom the boxplot, the distributions are definitely not all normal; in fact, none of them are. So we should use Mood’s median test, thus:\n\nmedian_test(gss, tvhours, degree)\n\n$table\n          above\ngroup      above below\n  College     67    70\n  Graduate    18    36\n  HSorLess   355   126\n\n$test\n       what        value\n1 statistic 5.608269e+01\n2        df 2.000000e+00\n3   P-value 6.634351e-13\n\n\n\\(\\blacksquare\\)\n\nWhat do you conclude from your test, in the context of the data?\n\nSolution\nThe P-value of \\(6.6\\times 10^{-13}\\) is extremely small, so we conclude that not all of the education groups watch the same median amount of TV. Or, there are differences in the median amount of TV watched among the three groups.\nAn answer of “the education groups are different” is wrong, because you don’t know that they are all different. It might be that some of them are different and some of them are the same. The next part gets into that.\n\\(\\blacksquare\\)\n\nWhy might you now want to run some kind of follow-up test? Run the appropriate thing and explain briefly what you conclude from it, in the context of the data.\n\nSolution\nThe overall Mood test is significant, so there are some differences between the education groups, but we don’t know where they are. Pairwise median tests will reveal where any differences are:\n\npairwise_median_test(gss, tvhours, degree)\n\n\n\n  \n\n\n\nThe people whose education is high school or less are significantly different from the other two education levels. The boxplot reveals that this is because they watch more TV on average. The college and graduate groups are not significantly different (in median TV watching).\nExtra 1:\nYou might have been surprised that the College and Graduate medians were not significantly different. After all, they look quite different on the boxplot. Indeed, the P-value for comparing just those two groups is 0.0512, only just over 0.05. But remember that we are doing three tests at once, so the Bonferroni adjustment is to multiply the P-values by 3, so this P-value is “really” some way from being significant. I thought I would investigate this in more detail:\n\ngss %&gt;% filter(degree != \"HSorLess\") %&gt;% \nmedian_test(tvhours, degree)\n\n$table\n          above\ngroup      above below\n  College     67    70\n  Graduate    18    36\n\n$test\n       what     value\n1 statistic 3.8027625\n2        df 1.0000000\n3   P-value 0.0511681\n\n\nThe College group are about 50-50 above and below the overall median, but the Graduate group are two-thirds below. This suggests that the Graduate group watches less TV, and with these sample sizes I would have expected a smaller P-value. But it didn’t come out that way.\nYou might also be concerned that there are in total more values below the grand median (106) than above (only 85). This must mean that there are a lot of data values equal to the grand median:\n\ngss %&gt;% filter(degree != \"HSorLess\") -&gt; gss1\ngss1 %&gt;% summarize(med=median(tvhours))\n\n\n\n  \n\n\n\nand\n\ngss1 %&gt;% count(tvhours)\n\n\n\n  \n\n\n\nEverybody gave a whole number of hours, and there are not too many different ones; in addition, a lot of them are equal to the grand median of 2.\nExtra 2:\nRegular ANOVA and Welch ANOVA should be non-starters here because of the non-normality, but you might be curious about how they would perform:\n\ngss.1 &lt;- aov(tvhours~degree, data=gss)\nsummary(gss.1)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndegree        2    267  133.30   25.18 2.27e-11 ***\nResiduals   902   4774    5.29                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(gss.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = tvhours ~ degree, data = gss)\n\n$degree\n                        diff        lwr       upr     p adj\nGraduate-College  -0.4238095 -1.1763372 0.3287181 0.3831942\nHSorLess-College   1.0598958  0.6181202 1.5016715 0.0000001\nHSorLess-Graduate  1.4837054  0.8037882 2.1636225 0.0000011\n\n\nand\n\noneway.test(tvhours~degree, data=gss)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  tvhours and degree\nF = 37.899, num df = 2.00, denom df = 206.22, p-value = 9.608e-15\n\ngamesHowellTest(tvhours~factor(degree), data=gss)\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: tvhours by factor(degree)\n\n\n         College Graduate\nGraduate 0.12    -       \nHSorLess 2.4e-10 1.7e-10 \n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nThe conclusions are actually identical to our Mood test, and the P-values are actually not all that much different. Which makes me wonder just how bad the sampling distributions of the sample means are. Bootstrap to the rescue:\n\ngss %&gt;% \n  nest_by(degree) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$tvhours, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12) +\n  facet_wrap(~degree, scales = \"free\")\n\n\n\n\nCoding this made my head hurt, but building it one line at a time, I pretty much got it right first time. In words:\n\n“compress” the dataframe to get one row per degree and a list-column called data with the number of hours of TV watched for each person with that degree\ngenerate 1000 sims for each degree (to guide the taking of bootstrap samples shortly)\norganize into one row per sim\nthen take bootstrap samples as normal and work out the mean of each one\nmake histograms for each degree, using a different scale for each one. (This has the advantage that the normal number of bins will work for all the histograms.)\n\nIf you are not sure about what happened, run it one line at a time and see what the results look like after each one.\nAnyway, even though the data was very much not normal, these sampling distributions are very normal-looking, suggesting that something like Welch ANOVA would have been not nearly as bad as you would have guessed. This is evidently because of the big sample sizes. (This also explains why the two other flavours of ANOVA gave results very similar to Mood’s median test.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#death-of-poets-1",
    "href": "analysis-of-variance.html#death-of-poets-1",
    "title": "11  Analysis of variance",
    "section": "11.17 Death of poets",
    "text": "11.17 Death of poets\nSome people believe that poets, especially female poets, die younger than other types of writer. William Butler Yeats15 wrote:\n\nShe is the Gaelic16 muse, for she gives inspiration to those she persecutes. The Gaelic poets die young, for she is restless, and will not let them remain long on earth.\n\nA literature student wanted to investigate this, and so collected a sample of 123 female writers (of three different types), and noted the age at death of each writer.\nThe data are in http://ritsokiguess.site/datafiles/writers.csv.\n\nRead in and display (some of) the data.\n\nSolution\nThe usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/writers.csv\"\nwriters &lt;- read_csv(my_url)\n\nRows: 123 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Type\ndbl (2): Type1, Age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwriters\n\n\n\n  \n\n\n\nThere are indeed 123 writers. The second column shows the principal type of writing each writer did, and the third column shows their age at death. The first column is a numerical code for the type of writing, which we ignore (since we can handle the text writing type).\n\\(\\blacksquare\\)\n\nMake a suitable plot of the ages and types of writing.\n\nSolution\nAs usual, one quantitative and one categorical, so a boxplot:\n\nggplot(writers, aes(x=Type, y=Age)) + geom_boxplot()\n\n\n\n\nAt this point, a boxplot is best, since right now you are mostly after a general sense of what is going on, rather than assessing normality in particular (that will come later).\n\\(\\blacksquare\\)\n\nObtain a summary table showing, for each type of writing, the number of writers of that type, along with the mean, median and standard deviation of their ages at death.\n\nSolution\nThe customary group_by and summarize:\n\nwriters %&gt;% group_by(Type) %&gt;% \nsummarize(n=n(), mean=mean(Age), med=median(Age), sd=sd(Age))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nRun a complete analysis, starting with an ordinary (not Welch) analysis of variance, that ends with a conclusion in the context of the data and an assessment of assumptions.\n\nSolution\nI’ve left this fairly open-ended, to see how well you know what needs to be included and what it means. There is a lot of room here for explanatory text to show that you know what you are doing. One output followed by another without any explanatory text suggests that you are just copying what I did without any idea about why you are doing it.\nThe place to start is the ordinary (not Welch) ANOVA. You may not think that this is the best thing to do (you’ll have a chance to talk about that later), but I wanted to make sure that you practiced the procedure:\n\nwriters.1 &lt;- aov(Age~Type, data=writers)\nsummary(writers.1)\n\n             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nType          2   2744  1372.1   6.563 0.00197 **\nResiduals   120  25088   209.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis says that the mean ages at death of the three groups of writers are not all the same, or that there are differences among those writers (in terms of mean age at death). “The mean ages of the types of writer are different” is not accurate enough, because it comes too close to saying that all three groups are different, which is more than you can say right now.\nThe \\(F\\)-test is significant, meaning that there are some differences among17 the means, and Tukey’s method will enable us to see which ones differ:\n\nTukeyHSD(writers.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Age ~ Type, data = writers)\n\n$Type\n                        diff       lwr       upr     p adj\nNovels-Nonfiction  -5.427239 -13.59016  2.735681 0.2591656\nPoems-Nonfiction  -13.687500 -22.95326 -4.421736 0.0018438\nPoems-Novels       -8.260261 -15.63375 -0.886772 0.0240459\n\n\nThere is a significant difference in mean age at death between the poets and both the other types of writer. The novelists and the nonfiction writers do not differ significantly in mean age at death.\nWe know from the boxplots (or the summary table) that this significant difference was because the poets died younger on average, which is exactly what the literature student was trying to find out. Thus, female poets really do die younger on average than female writers of other types. It is best to bring this point out, since this is the reason we (or the literature student) were doing this analysis in the first place. See Extra 1 for more.\nSo now we need to assess the assumptions on which the ANOVA depends.\nThe assumption we made is that the ages at death of the authors of each different type had approximately a normal distribution (given the sample sizes) with approximately equal spread. The boxplots definitely look skewed to the left (well, not the poets so much, but the others definitely). So now consider the sample sizes: 24, 67, and 32 for the three groups (respectively), and make a call about whether you think the normality is good enough. You are certainly entitled to declare the two outliers on the nonfiction writers to be too extreme given a sample size of only 24. Recall that once one sample fails normality, that’s all you need.\nNow, since you specifically want normality, you could reasonably look at normal quantile plots instead of the boxplots. Don’t just get normal quantile plots, though; say something about why you want them instead of the boxplots you drew earlier:\n\nggplot(writers, aes(sample = Age)) +\nstat_qq() + stat_qq_line() + \nfacet_wrap(~Type)\n\n\n\n\nI see that the Nonfiction writers have two outliers at the low end (and are otherwise not bad); the writers of Novels don’t go up high enough (it’s almost as if there is some magic that stops them living beyond 90!); the writers of Poems have a short-tailed distribution. You’ll remember that short tails are not a problem, since the mean is still descriptive of such a distribution; it’s long tails or outliers or skewness that you need to be worried about. The outliers in the Nonfiction writers are the biggest concern.\nAre you concerned that these outliers are a problem, given the sample size? There are only 24 nonfiction writers (from your table of means earlier), so the Central Limit Theorem will help a bit. Make a call about whether these outliers are a big enough problem. You can go either way on this, as long as you raise the relevant issues.\nAnother approach you might take is to look at the P-values. The one in the \\(F\\)-test is really small, and so is one of the ones in the Tukey. So even if you think the analysis is a bit off, those conclusions are not likely to change. The 0.02 P-value in the Tukey, however, is another story. This could become non-significant in actual fact if the P-value is not to be trusted.\nYet another approach (looking at the bootstrapped sampling distributions of the sample means) is in Extra 3. This gets more than a little hairy with three groups, especially doing it the way I do.\nIf you think that the normality is not good enough, it’s a good idea to suggest that we might do a Mood’s Median Test instead, and you could even do it (followed up with pairwise median tests). If you think that normality is all right, you might then look at the spreads. I think you ought to conclude that these are close enough to equal (the SDs from the summary table or the heights of the boxes on the boxplots), and so there is no need to do a Welch ANOVA. (Disagree if you like, but be prepared to make the case.)\nI have several Extras:\nExtra 1: having come to that tidy conclusion, we really ought to back off a bit. These writers were (we assume) a random sample of some population, but they were actually mostly Americans, with a few Canadian and Mexican writers. So this appears to be true at least for North American writers. But this is (or might be) a different thing to the Yeats quote about female Gaelic poets.\nThere is a more prosaic reason. It is harder (in most places, but especially North America) to get poetry published than it is to find a market for other types of writing. (A would-be novelist, say, can be a journalist or write for magazines to pay the bills while they try to find a publisher for their novel.) Thus a poet is living a more precarious existence, and that might bring about health problems.\nExtra 2: with the non-normality in mind, maybe Mood’s median test is the thing:\n\nmedian_test(writers, Age, Type)\n\n$table\n            above\ngroup        above below\n  Nonfiction    17     6\n  Novels        33    30\n  Poems         10    22\n\n$test\n       what       value\n1 statistic 9.872664561\n2        df 2.000000000\n3   P-value 0.007180888\n\n\nThe P-value here is a bit bigger than for the \\(F\\)-test, but it is still clearly significant. Hence, we do the pairwise median tests to find out which medians differ:\n\npairwise_median_test(writers, Age, Type)\n\n\n\n  \n\n\n\nThe conclusion here is exactly the same as for the ANOVA. The P-values have moved around a bit, though: the first one is a little closer to significance (remember, look at the last column since we are doing three tests at once) and the last one is now only just significant.\n\nwriters %&gt;% group_by(Type) %&gt;% \nsummarize(n=n(), mean=mean(Age), med=median(Age), sd=sd(Age))\n\n\n\n  \n\n\n\nIn both of these two cases (Nonfiction-Novels and Novels-Poems), the medians are closer together than the means are. That would explain why the Novels-Poems P-value would increase, but not why the Nonfiction-Novels one would decrease.\nI would have no objection in general to your running a Mood’s Median Test on these data, but the point of this problem was to give you practice with aov.\nExtra 3: the other way to assess if the normality is OK given the sample sizes is to obtain bootstrap sampling distributions of the sample means for each Type. The sample size for the novelists is 67, so I would expect the skewness there to be fine, but the two outliers among the Nonfiction writers may be cause for concern, since there are only 24 of those altogether.\nLet’s see if we can do all three at once (I like living on the edge). I take things one step at a time, building up a pipeline as I go. Here’s how it starts:\n\nwriters %&gt;% nest_by(Type)\n\n\n\n  \n\n\n\nThe thing data is a so-called list-column. The dataframes we have mostly seen so far are like spreadsheets, in that each “cell” or “entry” in a dataframe has something like a number or a piece of text in it (or, occasionally, a thing that is True or False, or a date). Tibble-type dataframes are more flexible than that, however: each cell of a dataframe could contain anything.\nIn this one, the three things in the column data are each dataframes,18 containing the column called Age from the original dataframe. These are the ages at death of the writers of that particular Type. These are the things we want bootstrap samples of.\nI’m not at all sure how this is going to go, so let’s shoot for just 5 bootstrap samples to start with. If we can get it working, we can scale up the number of samples later, but having a smaller number of samples is easier to look at:\n\nwriters %&gt;% nest_by(Type) %&gt;% \n  mutate(sim = list(1:5))\n\n\n\n  \n\n\n\nLet me break off at this point to say that we want 1000 bootstrap samples for the writers of each type, so this is the kind of thing we need to start with. nest_by has an implied rowwise, so we get three lots of values in sim; the list is needed since each one is five values rather than just one. The next stage is to unnest these, and then do another rowwise to work with all the (more) rows of the dataframe we now have. After that, the process should look more or less familiar:\n\nwriters %&gt;% nest_by(Type) %&gt;% \n  mutate(sim = list(1:5)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$Age, replace = TRUE)))\n\n\n\n  \n\n\n\nThat seems to be about the right thing; the bootstrap samples appear to be the right size, considering how many writers of each type our dataset had. From here, work out the mean of each sample:\n\nwriters %&gt;% nest_by(Type) %&gt;% \n  mutate(sim = list(1:5)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$Age, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample))\n\n\n\n  \n\n\n\nand then you could plot those means. This seems to be working, so let’s scale up to 1000 simulations, and make normal quantile plots of the bootstrapped sampling distributions, one for each Type of writer:\n\nwriters %&gt;% nest_by(Type) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$Age, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + \n  stat_qq_line() + facet_wrap(~Type, scales = \"free\")\n\n\n\n\nThese three normal quantile plots are all acceptable, to my mind, although the Nonfiction one, with the two outliers and the smallest sample size, is still a tiny bit skewed to the left. Apart from that, the three sampling distributions of the sample means are close to normal, so our aov is much better than you might have thought from looking at the boxplots. That’s the result of having large enough samples to get help from the Central Limit Theorem.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#religion-and-studying-1",
    "href": "analysis-of-variance.html#religion-and-studying-1",
    "title": "11  Analysis of variance",
    "section": "11.18 Religion and studying",
    "text": "11.18 Religion and studying\nMany students at a certain university were asked about the importance of religion in their lives (categorized as “not”, “fairly”, or “very” important), and also about the number of hours they spent studying per week. (This was part of a much larger survey.) We want to see whether there is any kind of relationship between these two variables. The data are in here.\n\nRead in and display (some of) the data.\n\nSolution\nThe usual. This is a straightforward one:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/student_relig.csv\"\nstudent &lt;- read_csv(my_url)\n\nRows: 686 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ReligImp\ndbl (1): StudyHrs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstudent\n\n\n\n  \n\n\n\n686 students, with columns obviously named for religious importance and study hours.\nExtra:\nI said this came from a bigger survey, actually this one:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/student0405.csv\"\nstudent0 &lt;- read_csv(my_url)\n\nRows: 690 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Sex, ReligImp, Seat\ndbl (4): GPA, MissClass, PartyDays, StudyHrs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstudent0\n\n\n\n  \n\n\n\nThere are four extra rows here. Why? Let’s look at a summary of the dataframe:\n\nsummary(student0) \n\n     Sex                 GPA          ReligImp           MissClass     \n Length:690         Min.   :1.500   Length:690         Min.   :0.0000  \n Class :character   1st Qu.:2.930   Class :character   1st Qu.:0.0000  \n Mode  :character   Median :3.200   Mode  :character   Median :1.0000  \n                    Mean   :3.179                      Mean   :0.9064  \n                    3rd Qu.:3.515                      3rd Qu.:1.0000  \n                    Max.   :4.000                      Max.   :6.0000  \n                    NA's   :3                          NA's   :1       \n     Seat             PartyDays         StudyHrs    \n Length:690         Min.   : 0.000   Min.   : 0.00  \n Class :character   1st Qu.: 3.000   1st Qu.: 6.25  \n Mode  :character   Median : 7.000   Median :10.00  \n                    Mean   : 7.501   Mean   :13.16  \n                    3rd Qu.:11.000   3rd Qu.:16.00  \n                    Max.   :31.000   Max.   :70.00  \n                                     NA's   :4      \n\n\nYou get information about each variable. For the text variables, you don’t learn much, only how many there are. (See later for more on this.) For each of the four quantitative variables, you see some stats about each one, along with a count of missing values. The study hours variable is evidently skewed to the right (mean bigger than median), which we will have to think about later.\nR also has a “factor” variable type, which is the “official” way to handle categorical variables in R. Sometimes it matters, but most of the time leaving categorical variables as text is just fine. summary handles these differently. My second line of code below says “for each variable that is text, make it into a factor”:\n\nstudent0 %&gt;% \nmutate(across(where(is.character), ~factor(.))) %&gt;% \nsummary()\n\n     Sex           GPA          ReligImp     MissClass          Seat    \n Female:382   Min.   :1.500   Fairly:319   Min.   :0.0000   Back  :134  \n Male  :308   1st Qu.:2.930   Not   :222   1st Qu.:0.0000   Front :151  \n              Median :3.200   Very  :149   Median :1.0000   Middle:404  \n              Mean   :3.179                Mean   :0.9064   NA's  :  1  \n              3rd Qu.:3.515                3rd Qu.:1.0000               \n              Max.   :4.000                Max.   :6.0000               \n              NA's   :3                    NA's   :1                    \n   PartyDays         StudyHrs    \n Min.   : 0.000   Min.   : 0.00  \n 1st Qu.: 3.000   1st Qu.: 6.25  \n Median : 7.000   Median :10.00  \n Mean   : 7.501   Mean   :13.16  \n 3rd Qu.:11.000   3rd Qu.:16.00  \n Max.   :31.000   Max.   :70.00  \n                  NA's   :4      \n\n\nFor factors, you also get how many observations there are in each category, and the number of missing values, which we didn’t get before. However, ReligImp does not have any missing values.\nI said there were four missing values for study hours, that is, four students who left that blank on their survey. We want to get rid of those students (that is, remove those whole rows), and, to simplify things for you, let’s keep only the study hours and importance of religion columns. That goes like this:\n\nstudent0 %&gt;% drop_na(StudyHrs) %&gt;% \nselect(ReligImp, StudyHrs)\n\n\n\n  \n\n\n\nThen I saved that for you. 686 rows instead of 690, having removed the four rows with missing StudyHrs.\nAnother (better, but more complicated) option is to use the package pointblank, which produces much more detailed data validation reports. You would start that by piping your data into scan_data() to get a (very) detailed report of missingness and data values, and then you can check your data for particular problems, such as missing values, or values bigger or smaller than they should be, for the variables you care about. See here for more.\n\\(\\blacksquare\\)\n\nObtain the number of observations and the mean and standard deviation of study hours for each level of importance.\n\nSolution\ngroup_by and summarize (spelling the latter with s or z as you prefer):\n\nstudent %&gt;% group_by(ReligImp) %&gt;% \nsummarize(n=n(), mean_sh=mean(StudyHrs), sd_sh=sd(StudyHrs))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nComment briefly on how the groups compare in terms of study hours.\n\nSolution\nThe students who think religion is very important have a higher mean number of study hours. The other two groups seem similar.\nAs far as the SDs are concerned, make a call. You could say that the very-important group also has a (slightly) larger SD, or you could say that the SDs are all very similar.\nI would actually favour the second one, but this is going to be a question about Welch ANOVA, so go whichever way you like.\n\\(\\blacksquare\\)\n\nMake a suitable graph of this data set.\n\nSolution\nThis kind of data is one quantitative and one categorical variable, so once again a boxplot:\n\nggplot(student, aes(x=ReligImp, y=StudyHrs)) + geom_boxplot()\n\n\n\n\n\\(\\blacksquare\\)\n\nThe statistician in this study decided that the data were sufficiently normal in shape given the (very large) sample sizes, but was concerned about unequal spreads among the three groups. Given this, run a suitable analysis and display the output. (This includes a suitable follow-up test, if warranted.)\n\nSolution\nNormal-enough data (in the statistician’s estimation) and unequal spreads means a Welch ANOVA:\n\noneway.test(StudyHrs~ReligImp, data=student)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  StudyHrs and ReligImp\nF = 7.9259, num df = 2.0, denom df = 350.4, p-value = 0.0004299\n\ngamesHowellTest(StudyHrs~factor(ReligImp), data=student)\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: StudyHrs by factor(ReligImp)\n\n\n     Fairly  Not    \nNot  0.26035 -      \nVery 0.00906 0.00026\n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nGames-Howell is the suitable follow-up here, to go with the Welch ANOVA. It is warranted because the Welch ANOVA was significant.\nMake sure you have installed and loaded PMCMRplus before trying the second half of this.\nExtra: for large data sets, boxplots make it look as if the outlier problem is bad, because a boxplot of a large amount of data will almost certainly contain some outliers (according to Tukey’s definition). Tukey envisaged a boxplot as something you could draw by hand for a smallish data set, and couldn’t foresee something like R and the kind of data we might be able to deal with. To show you the kind of thing I mean, let’s draw some random samples of varying sizes from normal distributions, which should not have outliers, and see how their boxplots look:\n\ntibble(n=c(10, 30, 100, 300)) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(n))) %&gt;% \n  unnest(my_sample) %&gt;% \n  ggplot(aes(x = factor(n), y = my_sample)) + geom_boxplot()\n\n\n\n\nAs the sample size gets bigger, the number of outliers gets bigger, and the whiskers get longer. All this means is that in a larger sample, you are more likely to see a small number of values that are further out, and that is not necessarily a reason for concern. Here, the outliers are only one value out of 100 and two out of 300, but they have what looks like an outsize influence on the plot. In the boxplot for our data, the distributions were a bit skewed, but the outliers may not have been as much of a problem as they looked.\n\\(\\blacksquare\\)\n\nWhat do you conclude from your analysis of the previous part, in the context of the data?\n\nSolution\nThe Welch ANOVA was significant, so the religious-importance groups are not all the same in terms of mean study hours, and we need to figure out which groups differ from which. (Or say this in the previous part if you wish.)\nThe students for whom religion was very important had a significantly different mean number of study hours than the other students; the Fairly and Not groups were not significantly different from each other. Looking back at the means (or the boxplots), the significance was because the Very group studied for more hours than the other groups. It seems that religion has to be very important to a student to positively affect how much they study.\nExtra: you might have been concerned that the study hours within the groups were not nearly normal enough to trust the Welch ANOVA. But the groups were large, so there is a lot of help from the Central Limit Theorem. Enough? Well, that is hard to judge.\nMy take on this is to bootstrap the sampling distribution of the sample mean for each group. If that looks normal, then we ought to be able to trust the \\(F\\)-test (regular or Welch, as appropriate). The code is complicated (I’ll explain the ideas below):\n\nstudent %&gt;% \n  nest_by(ReligImp) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$StudyHrs, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line() +\n  facet_wrap(~ReligImp, scales = \"free\")\n\n\n\n\nTo truly understand what’s going on, you probably need to run this code one line at a time.\nAnyway, these normal quantile plots are very normal. This says that the sampling distributions of the sample means are very much normal in shape, which means that the sample sizes are definitely large enough to overcome the apparently bad skewness that we saw on the boxplots. In other words, using a regular or Welch ANOVA will be perfectly good; there is no need to reach for Mood’s median test here, despite what you might think from looking at the boxplots, because the sample sizes are so large.\nThe code, line by line:\n\ncreate mini-data-frames called data, containing one column called StudyHrs, for each ReligImp group\nset up for 1000 bootstrap samples for each group, and (next line) arrange for one row per bootstrap sample\nwork rowwise\ngenerate the bootstrap samples\nwork out the mean of each bootstrap sample\nplot normal quantile plots of them, using different facets for each group.\n\nFinally, you might have wondered whether we needed to do Welch:\n\nstudent.1 &lt;- aov(StudyHrs~ReligImp, data=student)\nsummary(student.1)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nReligImp      2   1721   860.7   9.768 6.57e-05 ***\nResiduals   683  60184    88.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(student.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = StudyHrs ~ ReligImp, data = student)\n\n$ReligImp\n                 diff        lwr       upr     p adj\nNot-Fairly  -1.195917 -3.1267811 0.7349462 0.3135501\nVery-Fairly  3.143047  0.9468809 5.3392122 0.0023566\nVery-Not     4.338964  1.9991894 6.6787385 0.0000454\n\n\nIt didn’t make much difference, and the conclusions are identical. So I think either way would have been defensible.\nThe value of doing Tukey is that we get confidence intervals for the difference of means between each group, and this gives us an “effect size”: the students for whom religion was very important studied on average three or four hours per week more than the other students, and you can look at the confidence intervals to see how much uncertainty there is in those estimates. Students vary a lot in how much they study, but the sample sizes are large, so the intervals are not that long.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#footnotes",
    "href": "analysis-of-variance.html#footnotes",
    "title": "11  Analysis of variance",
    "section": "",
    "text": "An Irish, that is to say, Gaelic, poet (see below), but a male one.↩︎\nGaelic is a language of Scotland and Ireland, and the culture of the people who speak it.↩︎\nActually, this doesn’t always work if the sample sizes in each group are different. If you’re comparing two small groups, it takes a very large difference in means to get a small P-value. But in this case the sample sizes are all the same.↩︎\nThe computer scientists among you will note that I should not use equals or not-equals to compare a decimal floating-point number, since decimal numbers are not represented exactly in the computer. R, however, is ahead of us here, since when you try to do “food not equal to 4.7”, it tests whether food is more than a small distance away from 4.7, which is the right way to do it. In R, therefore, code like my food !=  4.7 does exactly what I want, but in a language like C, it does not, and you have to be more careful: abs(food-4.7)&gt;1e-8, or something like that. The small number 1e-8 (\\(10^{-8}\\)) is typically equal to machine epsilon, the smallest number on a computer that is distinguishable from zero.↩︎\nMost of these parts are old from assignment questions that I actually asked a previous class to do, but not this part. I added it later.↩︎\nSee discussion elsewhere about Yates’ Correction and fixed margins.↩︎\nIn the pairwise median test in smmr, I did this backwards: rather than changing the alpha that you compare each P-value with from 0.05 to 0.05/6, I flip it around so that you adjust the P-values by multiplying them by 6, and then comparing the adjusted P-values with the usual 0.05. It comes to the same place in the end, except that this way you can get adjusted P-values that are greater than 1, which makes no sense. You read those as being definitely not significant.↩︎\nIt’s probably better in a report to use language a bit more formal than a bunch. Something like a number would be better.↩︎\nThe use of absolute differences, and the median, downplays the influence of outliers. The assumption here is that the absolute differences from the medians are approximately normal, which seems a less big assumption than assuming the actual data are approximately normal.↩︎\nThis is coming back to the power of something like Levene’s test; the power of any test is not going to be very big if the sample sizes are small.↩︎\nThe test goes back to the 1940s.↩︎\nBest, D. J., and J. C. W. Rayner. “Welch’s Approximate Solution for the Behrens–Fisher Problem.” Technometrics 29, no. 2 (May 1, 1987): 205–10. doi:10.1080/00401706.1987.10488211. The data set is near the end.↩︎\nWe’d need a lot more students to make it narrower, but this is not surprising since students vary in a lot of other ways that were not measured here.↩︎\nPerhaps a better word here would be principle, to convey the idea that you can do something else if it works better for your purposes.↩︎\nAn Irish, that is to say, Gaelic, poet (see below), but a male one.↩︎\nGaelic is a language of Scotland and Ireland, and the culture of the people who speak it.↩︎\nThere might be differences between two things, but among three or more.↩︎\nLike those Russian dolls.↩︎"
  },
  {
    "objectID": "reports.html#atomic-weight-of-carbon",
    "href": "reports.html#atomic-weight-of-carbon",
    "title": "12  Writing reports",
    "section": "12.1 Atomic weight of carbon",
    "text": "12.1 Atomic weight of carbon\nThe atomic weight of the chemical element carbon is 12. Two methods of measuring the atomic weight of samples of carbon were compared. The results are shown in link. The methods are labelled 1 and 2. The first task is to find out whether the two methods have different “typical” measures (mean or median, as appropriate) of the atomic weight of carbon.\nFor this question, compose a report in R Markdown. (R Markdown is what you use in an R Notebook, but you can also have a separate R Markdown document from which you can produce HTML, Word etc. output.) See part (a) for how to get this started.\nYour report should read like an actual report, not just the answers to some questions that I set you. To help with that, write some text that links the parts of the report together smoothly, so that it reads as a coherent whole. The grader had 3 discretionary marks to award for the overall quality of your writing. The scale for this was:\n\n3 points: excellent writing. The report flows smoothly, is easy to read, and contains everything it should (and nothing it shouldn’t).\n2 points: satisfactory writing. Not the easiest to read, but says what it should, and it looks at least somewhat like a report rather than a string of answers to questions.\n1 point: writing that is hard to read or to understand. If you get this (or 0), you should consider what you need to do to improve when you write your project.\n0 points: you answered the questions, but you did almost nothing to make it read like a report.\n\n\nCreate a new R Markdown document. To do this, in R Studio, select File, New File, R Markdown. Type the report title and your name in the boxes, and leave the output on the default HTML. Click OK.\nWrite an introduction that explains the purpose of this study and the data collected in your own words.\nBegin an appropriately-titled new section in your report, read the data into R and display the results.\nMake an appropriate plot to compare the measurements obtained by the two methods. You might need to do something about the two methods being given as numbers even though they are really only identifiers. (If you do, your report ought to say what you did and why.)\nComment briefly on what you see in your plot.\nCarry out the most appropriate \\(t\\)-test. (You might like to begin another new section in your report here.)\nDo the most appropriate test you know that does not assume normally-distributed data.\nDiscuss the results of your tests and what they say about the two methods for measuring the atomic weight of carbon. If it seems appropriate, put the discussion into a section called Conclusions."
  },
  {
    "objectID": "reports.html#sparrowhawks",
    "href": "reports.html#sparrowhawks",
    "title": "12  Writing reports",
    "section": "12.2 Sparrowhawks",
    "text": "12.2 Sparrowhawks\n(This is a report-writing question, but it also uses some regression techniques from later in the course.)\nOne of nature’s patterns is the relationship between the percentage of adult birds in a colony that return from the previous year, and the number of new adults that join the colony. Data for 13 colonies of sparrowhawks can be found at link. The columns are the percentage of adults returning from the previous year, and the number of new adults that join the colony.\n\nCreate a new R Markdown report, give it a suitable title, and ask for HTML output. Answer the questions that follow in your report. At any stage, you can Knit HTML to see how the report looks so far.\nRead in the data and display the first few values. Add some text saying how many rows of data there are.\nCreate a new section entitled “Exploratory analysis”, and create a scatterplot for predicting number of new adults from the percentage of returning adults. Describe what you see, adding some suitable text to your report.\nObtain the correlation between the two variables. Is this consistent with the scatterplot? Explain briefly. (The R function you need is cor. You can feed it a data frame.)\nObtain the regression line for predicting the number of new adults from the percentage of returning adults.\nWhat are the intercept and slope of your regression line? Is the slope significant? What does that mean, in the context of the data?\nCreate a scatterplot of the data with the regression line on it.\nFor short-lived birds, the association between these two variables is positive: changes in weather and food supply cause the populations of new and returning birds to increase together. For long-lived territorial birds, however, the association is negative because returning birds claim their territories in the colony and do not leave room for new recruits. Which type of species is the sparrowhawk? Add a short Conclusions section to your report with discussion of this issue."
  },
  {
    "objectID": "reports.html#learning-to-code",
    "href": "reports.html#learning-to-code",
    "title": "12  Writing reports",
    "section": "12.3 Learning to code",
    "text": "12.3 Learning to code\nA programming course, Comp Sci 101, can be taken either in-person, by attending a class at fixed days and times, or online, by doing sessions that can be taken at times the student chooses. The course coordinator wants to know whether students taking the course in these two different ways learn a different amount, as measured by their scores on the final exam for the course. This example comes from the before-times, so the final exam was taken in person by all students. The final exam was out of 45 marks. A total of 18 students took part in the study. Each student was allowed to choose the section they preferred. The data are in http://ritsokiguess.site/datafiles/proggo.csv.\nWrite a report of a complete and appropriate analysis of these data. Your report should include a description of the data in your own words, any necessary pre-processing steps, appropriate graphs, statistical analysis, assessment of assumptions for your preferred analysis, and a statement of conclusions. Imagine that your report will be read by the department Chair, who does not know about this study, and who still remembers some of their first-year Statistics course.\n(My example report is in a later chapter, the one called Learning to Code.)"
  },
  {
    "objectID": "reports.html#treating-dandruff",
    "href": "reports.html#treating-dandruff",
    "title": "12  Writing reports",
    "section": "12.4 Treating dandruff",
    "text": "12.4 Treating dandruff\nAccording to the Mayo Clinic, dandruff is “a common condition that causes the skin on the scalp to flake. It isn’t contagious or serious. But it can be embarrassing and difficult to treat.” Shampoos often claim to be effective in treating dandruff. In a study, four shampoos were compared:\n\nPyrI: 1% pyrithione zinc shampoo\nPyrII: the same as PyrI but with instructions to shampoo two times at each wash. The labels for these are Pyr with a Roman numeral I or II attached.\nKeto: 2% ketoconazole shampoo\nPlacebo: a placebo shampoo\n\nEach subject was randomly assigned to a shampoo. After six weeks of treatment, eight sections of the scalp were examined for each subject. Each section of the scalp was given a score that measured the amount of flaking on a scale of 0-10, less flaking being better. The response variable, called Flaking, was the sum of these eight scores, and is a whole number for each subject.\nThe data are in http://ritsokiguess.site/datafiles/dandruff.txt, with the data values separated by tabs.\nYour task is to write a report on your analysis of this data set, and to make a recommendation for the best shampoo(s) out of the four studied here. The target audience for your report is the principal investigator of the study described above, who knows a lot about shampoo, but not so much about statistics. (They took a course some time ago that covered the material you’ve seen in this course so far, at about the level of STAB22 or STA 220.) Some things you might want to consider, in no particular order (you need to think about where and if to include these things):\n\nan Introduction, written in your own words as much as possible\na Conclusion that summarizes what you found\na suitable and complete piece of statistical inference\na numerical summary of the data\ngraph(s) of the data\nan assessment of the assumptions of your analysis\ncitation of external sources\nanything else that you can make the case for including\n\nIn R Markdown (the text of an R Notebook), you can use ## to make a heading (you can experiment with more or fewer # symbols).\nYour aim is to produce a report, suitable for the intended audience, with all the important elements and no irrelevant ones, that is well-written and easy to follow. There is credit for good writing. For this report, you should include your code in with your report. (In a real report, you would probably show the output and not the code, but we are interested in your code here as well.)\n(My example report is in a later chapter.)\nMy solutions follow. The example reports on coding and treating dandruff are separate chapters."
  },
  {
    "objectID": "reports.html#atomic-weight-of-carbon-1",
    "href": "reports.html#atomic-weight-of-carbon-1",
    "title": "12  Writing reports",
    "section": "12.5 Atomic weight of carbon",
    "text": "12.5 Atomic weight of carbon\nThe atomic weight of the chemical element carbon is 12. Two methods of measuring the atomic weight of samples of carbon were compared. The results are shown in link. The methods are labelled 1 and 2. The first task is to find out whether the two methods have different “typical” measures (mean or median, as appropriate) of the atomic weight of carbon.\nFor this question, compose a report in R Markdown. (R Markdown is what you use in an R Notebook, but you can also have a separate R Markdown document from which you can produce HTML, Word etc. output.) See part (a) for how to get this started.\nYour report should read like an actual report, not just the answers to some questions that I set you. To help with that, write some text that links the parts of the report together smoothly, so that it reads as a coherent whole. The grader had 3 discretionary marks to award for the overall quality of your writing. The scale for this was:\n\n3 points: excellent writing. The report flows smoothly, is easy to read, and contains everything it should (and nothing it shouldn’t).\n2 points: satisfactory writing. Not the easiest to read, but says what it should, and it looks at least somewhat like a report rather than a string of answers to questions.\n1 point: writing that is hard to read or to understand. If you get this (or 0), you should consider what you need to do to improve when you write your project.\n0 points: you answered the questions, but you did almost nothing to make it read like a report.\n\n\nCreate a new R Markdown document. To do this, in R Studio, select File, New File, R Markdown. Type the report title and your name in the boxes, and leave the output on the default HTML. Click OK.\n\nSolution\nYou’ll see the title and your name in a section at the top of the document, and below that you’ll see a template document, as you would for an R Notebook. The difference is that where you are used to seeing Preview, it now says “knit”, but this has the same effect of producing the formatted version of your report.\n\\(\\blacksquare\\)\n\nWrite an introduction that explains the purpose of this study and the data collected in your own words.\n\nSolution\nSomething like this:\n\nThis study is intended to compare two different methods (labelled 1 and 2) for measuring the atomic weight of carbon (which is known in actual fact to be 12). Fifteen samples of carbon were used; ten of these were assessed using method 1 and the remaining five using method 2. The primary interest in this particular study is to see whether there is a difference in the mean or median atomic weight as measured by the two methods.\n\nBefore that, start a new section like this: ## Introduction. Also, get used to expressing your understanding in your words, not mine. Using my words, in my courses, is likely to be worth very little.\n\\(\\blacksquare\\)\n\nBegin an appropriately-titled new section in your report, read the data into R and display the results.\n\nSolution\nValues separated by spaces:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/carbon.txt\"\ncarbon &lt;- read_delim(my_url, \" \")\n\nRows: 15 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): method, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncarbon\n\n\n\n  \n\n\n\nI would expect you to include, without being told to include it, some text in your report indicating that you have sensible data: two methods labelled 1 and 2 as promised, and a bunch1 of atomic weights close to the nominal figure of 12.\n\\(\\blacksquare\\)\n\nMake an appropriate plot to compare the measurements obtained by the two methods. You might need to do something about the two methods being given as numbers even though they are really only identifiers. (If you do, your report ought to say what you did and why.)\n\nSolution\nThe appropriate plot, with a categorical method and quantitative weight, is something like a boxplot. If you’re not careful, method will get treated as a quantitative variable, which you don’t want; the easiest way around that, for a boxplot at least, is to turn it into a factor like this:\n\nggplot(carbon, aes(x = factor(method), y = weight)) + geom_boxplot()\n\n\n\n\nIf you insist, you could do a faceted histogram (above and below, for preference):\n\nggplot(carbon, aes(x = weight)) + geom_histogram(bins = 5) +\n  facet_wrap(~method, ncol = 1)\n\n\n\n\nThere are really not enough data values for a histogram to be of much help, so I don’t like this as much.\nIf you are thinking ahead (we are going to be doing a \\(t\\)-test), then you’ll realize that normality is the kind of thing we’re looking for, in which case normal quantile plots would be the thing. However, we might have to be rather forgiving for method 2 since there are only 5 observations:\n\nggplot(carbon, aes(sample = weight)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~method)\n\n\n\n\nI don’t mind these coming out side by side, though I would rather have them squarer.\nI would say, boxplots are the best, normal quantile plots are also acceptable, but expect to lose something for histograms because they offer only a rather crude comparison in this case.\n\\(\\blacksquare\\)\n\nComment briefly on what you see in your plot.\n\nSolution\nIn boxplots, if that’s what you drew, there are several things that deserve comment: the medians, the spreads and the shapes. The median for method 1 is a little bit lower than for method 2 (the means are probably more different, given the shapes of the boxes). The spread for method 2 is a lot bigger. (Looking forward, that suggests a Welch-Satterthwaite rather than a pooled test.) As for shape, the method 2 measurements seem more or less symmetric (the whiskers are equal anyway, even if the position of the median in the box isn’t), but the method 1 measurements have a low outlier. The histograms are hard to compare. Try to say something about centre and spread and shape. I think the method 2 histogram has a slightly higher centre and definitely bigger spread. On my histogram for method 1, the distribution looks skewed left. If you did normal quantile plots, say something sensible about normality for each of the two methods. For method 1, I would say the low value is an outlier and the rest of the values look pretty straight. Up to you whether you think there is a curve on the plot (which would indicate skewness, but then that highest value is too high: it would be bunched up with the other values below 12.01 if there were really skewness). For method 2, it’s really hard to say anything since there are only five values. Given where the line goes, there isn’t much you can say to doubt normality. Perhaps the best you can say here is that in a sample of size 5, it’s difficult to assess normality at all.\n\\(\\blacksquare\\)\n\nCarry out the most appropriate \\(t\\)-test. (You might like to begin another new section in your report here.)\n\nSolution\nThis would be the Welch-Satterthwaite version of the two-sample \\(t\\)-test, since the two groups do appear to have different spreads:\n\nt.test(weight ~ method, data = carbon)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by method\nt = -1.817, df = 5.4808, p-value = 0.1238\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.027777288  0.004417288\nsample estimates:\nmean in group 1 mean in group 2 \n       12.00260        12.01428 \n\n\nImagining that this is a report that would go to your boss, you ought to defend your choice of the Welch-Satterthwaite test (as I did above), and not just do the default \\(t\\)-test without comment.\nIf, in your discussion above, you thought the spreads were equal enough, then you should do the pooled \\(t\\)-test here, which goes like this:\n\nt.test(weight ~ method, data = carbon, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  weight by method\nt = -2.1616, df = 13, p-value = 0.04989\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -2.335341e-02 -6.588810e-06\nsample estimates:\nmean in group 1 mean in group 2 \n       12.00260        12.01428 \n\n\nThe point here is that you should do the right test based on your conclusion. Being consistent is the most important thing. (In this case, note that the P-values are very different. We’ll get to that shortly.)\nIf we were doing this in SAS, as we see later, we’d get a test at the bottom of the output that compares the two variances. I feel that it’s just as good to eyeball the spreads and make a call about whether they are “reasonably close”. Or even, to always do the Welch-Satterthwaite test on the basis that it is pretty good even if the two populations have the same variance. (If this last point of view is one that you share, you ought to say something about that when you do your \\(t\\)-test.)\nExtra: I guess this is a good place to say something about tests for comparing variances, given that you might be pondering that. There are several that I can think of, that R can do, of which I mention two.\nThe first is the \\(F\\)-test for variances that you might have learned in B57 (that is the basis for the ANOVA \\(F\\)-test):\n\nvar.test(weight ~ method, data = carbon)\n\n\n    F test to compare two variances\n\ndata:  weight by method\nF = 0.35768, num df = 9, denom df = 4, p-value = 0.1845\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.04016811 1.68758230\nsample estimates:\nratio of variances \n         0.3576842 \n\n\nThis, unfortunately, is rather dependent on the data in the two groups being approximately normal. Since we are talking variances rather than means, there is no Central Limit Theorem to rescue us for large samples (quite aside from the fact that these samples are not large). Since the ANOVA \\(F\\)-test is based on the same theory, this is why normality is also more important in ANOVA than it is in a \\(t\\)-test.\nThe second is Levene’s test. This doesn’t depend on normality (at least, not nearly so much), so I like it better in general:\n\nlibrary(car)\nleveneTest(weight ~ factor(method), data = carbon)\n\n\n\n  \n\n\n\nLevene’s test takes a different approach: first the absolute differences from the group medians are calculated, and then an ANOVA is run on the absolute differences. If, say, one of the groups has a larger spread than the other(s), its absolute differences from the median will tend to be bigger.2 As for what we conclude here, well, neither of the variance tests show any significance at all, so from that point of view there is no evidence against using the pooled \\(t\\)-test. Having said that, the samples are small, and so it would be difficult to prove that the two methods have different variance, even if they actually did.3\nThings are never as clear-cut as you would like. In the end, it all comes down to making a call and defending it.\n\\(\\blacksquare\\)\n\nDo the most appropriate test you know that does not assume normally-distributed data.\n\nSolution\nThat would be Mood’s median test. Since I didn’t say anything about building it yourself, feel free to use smmr:\n\nlibrary(smmr)\nmedian_test(carbon, weight, method)\n\n$table\n     above\ngroup above below\n    1     3     6\n    2     4     1\n\n$test\n       what      value\n1 statistic 2.80000000\n2        df 1.00000000\n3   P-value 0.09426431\n\n\nAs an aside, if you have run into a non-parametric test such as Mann-Whitney or Kruskal-Wallis that applies in this situation, be careful about using it here, because they have additional assumptions that you may not want to trust. Mann-Whitney started life as a test for “equal distributions”.4 This means that the null is equal location and equal spread, and if you reject the null, one of those has failed. But here, we suspect that equal spread will fail, so that the Mann-Whitney test may end up rejecting whether or not the medians are different, so it won’t answer the question you want an answer to. Mood’s median test doesn’t have that problem; all it’s saying if the null is true is that the medians are equal; the spreads could be anything at all.\nThe same kind of issues apply to the signed-rank test vs. the sign test. In the case of the signed-rank test, the extra assumption is of a symmetric distribution — to my mind, if you don’t believe normality, you probably don’t have much confidence in symmetry either. That’s why I like the sign test and Mood’s median test: in the situation where you don’t want to be dealing with assumptions, these tests don’t make you worry about that.\nAnother comment that you don’t need to make is based on the not-quite-significance of the Mood test. The P-value is less than 0.10 but not less than 0.05, so it doesn’t quite reach significance by the usual standard. But if you look up at the table, the frequencies seem rather unbalanced: 6 out of the remaining 9 weights in group 1 are below the overall median, but 4 out of 5 weights in group 2 are above. This seems as if it ought to be significant, but bear in mind that the sample sizes are small, and thus Mood’s median test needs very unbalanced frequencies, which we don’t quite have here.\n\\(\\blacksquare\\)\n\nDiscuss the results of your tests and what they say about the two methods for measuring the atomic weight of carbon. If it seems appropriate, put the discussion into a section called Conclusions.\n\nSolution\nBegin by pulling out the P-values for your preferred test(s) and say what they mean. The P-value for the Welch-Satterthwaite \\(t\\)-test is 0.1238, which indicates no difference in mean atomic weights between the two methods. The Mood median test gives a similarly non-significant 0.0943, indicating no difference in the median weights. If you think both tests are plausible, then give both P-values and do a compare-and-contrast with them; if you think that one of the tests is clearly preferable, then say so (and why) and focus on that test’s results.\nIf you thought the pooled test was the right one, then you’ll have a bit more discussion to do, since its P-value is 0.0499, and at \\(\\alpha=0.05\\) this test disagrees with the others. If you are comparing this test with the Mood test, you ought to make some kind of reasoned recommendation about which test to believe.\nAs ever, be consistent in your reasoning.\nExtra: this dataset, where I found it, was actually being used to illustrate a case where the pooled and the Welch-Satterthwaite tests disagreed. The authors of the original paper that used this dataset (a 1987 paper by Best and Rayner;5 the data come from 1924!) point out that the pooled \\(t\\)-test can be especially misleading when the smaller sample is also the one with the larger variance. This is what happened here.\nIn the Best and Rayner paper, the Mood (or the Mann-Whitney) test was not being considered, but I think it’s good practice to draw a picture and make a call about which test is appropriate.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "reports.html#sparrowhawks-1",
    "href": "reports.html#sparrowhawks-1",
    "title": "12  Writing reports",
    "section": "12.6 Sparrowhawks",
    "text": "12.6 Sparrowhawks\n(This is a report-writing question, but it also uses some regression techniques from later in the course.)\nOne of nature’s patterns is the relationship between the percentage of adult birds in a colony that return from the previous year, and the number of new adults that join the colony. Data for 13 colonies of sparrowhawks can be found at link. The columns are the percentage of adults returning from the previous year, and the number of new adults that join the colony.\n\nCreate a new R Markdown report, give it a suitable title, and ask for HTML output. Answer the questions that follow in your report. At any stage, you can Knit HTML to see how the report looks so far.\n\nSolution\n(Note: this is the previous version of Quarto, called R Markdown. The two are fairly similar.)\nIn R Studio, select File, New File, R Markdown. Fill in the Title, Author and leave the Default Output Format at HTML. You’ll see a template report with the document info at the top. This is my document info:\n\nThis is known in the jargon as a “YAML block”.6 Below that is the template R Markdown document, which you can delete now or later.\n\\(\\blacksquare\\)\n\nRead in the data and display the first few values. Add some text saying how many rows of data there are.\n\nSolution\nRead the data into a data frame. In your report, add some text like “we read in the data”, perhaps after a section heading like “The data”. Then add a code chunk by selecting Chunks and Insert Chunk, or by pressing control-alt-I. So far you have something like this.\n\nInside the code chunk, that is, in the bit between the backtick characters, put R code, just as you would type it at the Console or put in an R notebook. In this case, that would be the following code, minus the message that comes out of read_delim:\n\nlibrary(tidyverse)\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/sparrowhawk.txt\"\nsparrowhawks &lt;- read_delim(my_url, \" \")\n\nRows: 13 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): returning, newadults\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsparrowhawks\n\nFor you, it looks like this:\n\nWe don’t know how many rows of data there are yet, so I’ve left a “placeholder” for it, when we figure it out. The file is annoyingly called sparrowhawk.txt, singular. Sorry about that. If you knit this (click on “Knit HTML” next to the ball of wool, or press control-shift-K), it should run, and you’ll see a viewer pop up with the HTML output. Now you can see how many rows there are, and you can go back and edit the R Markdown and put in 13 in place of the x’s, and knit again. You might be worried about how hard R is working with all this knitting. Don’t worry about that. R can take it. Mine looked like this:\n\nThere is a better way of adding values that come from the output, which I mention here in case you are interested (if you are not, feel free to skip this). What you do is to make what is called an “inline code chunk”. Where you want a number to appear in the text, you have some R Markdown that looks like this:\n\nThe piece inside the backticks is the letter r, a space, and then one line of R code. The one line of code will be run, and all of the stuff within the backticks will be replaced in the output by the result of running the R code, in this case the number 13. Typically, you are extracting a number from the data, like the number of rows or a mean of something. If it’s a decimal number, it will come out with a lot of decimal places unless you explicitly round it. OK, let me try it: the data frame has 13 rows altogether. I didn’t type that number; it was calculated from the data frame. Woo hoo!\n\\(\\blacksquare\\)\n\nCreate a new section entitled “Exploratory analysis”, and create a scatterplot for predicting number of new adults from the percentage of returning adults. Describe what you see, adding some suitable text to your report.\n\nSolution\nThe R code you add should look like this, with the results shown (when you knit the report again):\n\nlibrary(tidyverse)\nggplot(sparrowhawks, aes(x = returning, y = newadults)) +\n  geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\nThe piece of report that I added looks like this:\n\nNote (i) that you have to do nothing special to get the plot to appear, and (ii) that I put “smaller” in italics, and you see how.\n\\(\\blacksquare\\)\n\nObtain the correlation between the two variables. Is this consistent with the scatterplot? Explain briefly. (The R function you need is cor. You can feed it a data frame.)\n\nSolution\nThe appropriate R code is this, in another code chunk:\n\nwith(sparrowhawks, cor(newadults, returning))\n\n[1] -0.7484673\n\n\nOr you can ask for the correlations of the whole data frame:\n\ncor(sparrowhawks)\n\n           returning  newadults\nreturning  1.0000000 -0.7484673\nnewadults -0.7484673  1.0000000\n\n\nThis latter is a “correlation matrix” with a correlation between each column and each other column. Obviously the correlation between a column and itself is 1, and that is not the one we want.\nI added this to the report (still in the Exploratory Analysis section, since it seems to belong there):\n\n\\(\\blacksquare\\)\n\nObtain the regression line for predicting the number of new adults from the percentage of returning adults.\n\nSolution\nThis R code, in another code chunk:\n\nnewadults.1 &lt;- lm(newadults ~ returning, data = sparrowhawks)\nsummary(newadults.1)\n\n\nCall:\nlm(formula = newadults ~ returning, data = sparrowhawks)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8687 -1.2532  0.0508  2.0508  5.3071 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 31.93426    4.83762   6.601 3.86e-05 ***\nreturning   -0.30402    0.08122  -3.743  0.00325 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.667 on 11 degrees of freedom\nMultiple R-squared:  0.5602,    Adjusted R-squared:  0.5202 \nF-statistic: 14.01 on 1 and 11 DF,  p-value: 0.003248\n\n\n\\(\\blacksquare\\)\n\nWhat are the intercept and slope of your regression line? Is the slope significant? What does that mean, in the context of the data?\n\nSolution\nSee the output in the previous part. That’s what we need to talk about. I added this to the report. I thought we deserved a new section here:\n\n\\(\\blacksquare\\)\n\nCreate a scatterplot of the data with the regression line on it.\n\nSolution\nThis code. Using geom_smooth with method=\"lm\" will add the regression line to the plot:\n\nggplot(sparrowhawks, aes(x = returning, y = newadults)) +\n  geom_point() + geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nI added a bit of text to the report, which I will show in a moment.\n\\(\\blacksquare\\)\n\nFor short-lived birds, the association between these two variables is positive: changes in weather and food supply cause the populations of new and returning birds to increase together. For long-lived territorial birds, however, the association is negative because returning birds claim their territories in the colony and do not leave room for new recruits. Which type of species is the sparrowhawk? Add a short Conclusions section to your report with discussion of this issue.\n\nSolution\nMy addition to the report looks like this:\n\nI think that rounds off the report nicely.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "reports.html#footnotes",
    "href": "reports.html#footnotes",
    "title": "12  Writing reports",
    "section": "",
    "text": "It’s probably better in a report to use language a bit more formal than a bunch. Something like a number would be better.↩︎\nThe use of absolute differences, and the median, downplays the influence of outliers. The assumption here is that the absolute differences from the medians are approximately normal, which seems a less big assumption than assuming the actual data are approximately normal.↩︎\nThis is coming back to the power of something like Levene’s test; the power of any test is not going to be very big if the sample sizes are small.↩︎\nThe test goes back to the 1940s.↩︎\nBest, D. J., and J. C. W. Rayner. “Welch’s Approximate Solution for the Behrens–Fisher Problem.” Technometrics 29, no. 2 (May 1, 1987): 205–10. doi:10.1080/00401706.1987.10488211. The data set is near the end.↩︎\nYAML stands for Yet Another Markup Language, but we’re only using it in this course as the top bit of an R Markdown document.↩︎"
  },
  {
    "objectID": "coding.html#introduction",
    "href": "coding.html#introduction",
    "title": "13  Learning to code",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nDo students learn programming more or less effectively from an online course, completed on their own time, compared with a regular in-person lecture course that meets at the same times every week? In Comp Sci 101, a study was carried out in which 18 students, 9 each in the online and in-person sections, were assessed for learning by means of the course final exam (out of 45 marks). We compare the mean final exam scores for the students in the two sections."
  },
  {
    "objectID": "coding.html#data-and-pre-processing",
    "href": "coding.html#data-and-pre-processing",
    "title": "13  Learning to code",
    "section": "13.2 Data and pre-processing",
    "text": "13.2 Data and pre-processing\nWe begin by reading in the data:\n\nlibrary(tidyverse)\n\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/proggo.csv\"\nprog0 &lt;- read_csv(my_url)\n\nRows: 9 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): online, classroom\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprog0\n\n\n\n  \n\n\n\nThe nine students in each section are in separate columns. This is not an appropriate layout for analysis because each row contains data for two separate, unrelated students, and we need to have each student’s score in its own row. (See note 1.) This means making the data longer (see note 2):\n\nprog0 %&gt;% pivot_longer(everything(), \n  names_to = \"instruction\", \n  values_to = \"mark\") -&gt; prog\nprog\n\n\n\n  \n\n\n\nNow everything is arranged as we need, and we can proceed to analysis."
  },
  {
    "objectID": "coding.html#analysis",
    "href": "coding.html#analysis",
    "title": "13  Learning to code",
    "section": "13.3 Analysis",
    "text": "13.3 Analysis\nWe begin by visualizing the data. With one quantitative variable mark and one categorical variable instruction, a boxplot will give us an overall picture (see note 3):\n\nggplot(prog, aes(x= instruction, y = mark)) + geom_boxplot()\n\n\n\n\nIt looks as if the average (median) mark is somewhat higher for the students in the online section. In addition, both distributions look reasonably symmetric with no outliers, and they appear to have similar spread. (see note 4.)\nWith that in mind, it seems sensible to compare the mean final exam marks in the two groups by a pooled two-sample \\(t\\)-test, in order to see whether the apparent difference in performance is any more than chance. The aim of the course coordinator was to see whether there was any difference between the two sections, without having any prior idea about which teaching method would be better, so a two-sided test is appropriate: (see note 5.)\n\nt.test(mark~instruction, data = prog, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  mark by instruction\nt = -1.6495, df = 16, p-value = 0.1185\nalternative hypothesis: true difference in means between group classroom and group online is not equal to 0\n95 percent confidence interval:\n -8.379039  1.045706\nsample estimates:\nmean in group classroom    mean in group online \n               31.55556                35.22222 \n\n\nThe P-value of 0.1185 is not smaller than 0.05, so there is no evidence of any difference between the mean final exam marks of the students in the two sections. The difference between the two groups shown on the boxplot is the kind of thing that could be observed by chance if the students were performing equally well under the two methods of instruction. (See notes 6 and 7.)\nThis \\(t\\)-test comes with the assumption that the data in each group come from a normal distribution, at least approximately. With two small samples, this will not be easy to assess, but we can at least look for any gross violations, using a normal quantile plot for each group:\n\nggplot(prog, aes(sample = mark)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~instruction)\n\n\n\n\nThe largest observation in the classroom group and the largest two observations in the online group are a little bigger than expected, but they were not big enough to be flagged as outliers on the boxplots, so I conclude that the normality is good enough to justify the \\(t\\)-test that we did. (See note 8.)"
  },
  {
    "objectID": "coding.html#conclusions-see-note-9",
    "href": "coding.html#conclusions-see-note-9",
    "title": "13  Learning to code",
    "section": "13.4 Conclusions (see note 9)",
    "text": "13.4 Conclusions (see note 9)\nWe found that there is no significant difference in the performance of the students learning in the classroom compared to those learning online. Before leaping to generalize to other classes, however, we should note two limitations of the study (see note 10):\n\nthe sample sizes were very small; if we had observed this size of difference between the two groups in larger samples, we might have been able to show that the difference was significant.\nwe have no information about how the students were allocated to the groups, and it seems likely that the students were allowed to choose their own method of instruction. If the students had been randomly allocated to instruction method, we could have been more confident that any differences observed were due to the instruction method, rather than also having something to do with the relative ability of the students who chose each instruction method.\n\nWe feel that it would be worth running another study of this type, but with larger sample sizes and randomly allocating students to instruction types. This latter, however, risks running into ethical difficulties, since students will normally wish to choose the section they are in.\nThus ends the report.\nNotes:\n\nSay something about the kind of data layout you have, and whether it’s what you want.\nYou can do a \\(t\\)-test without rearranging the data (the method is that of the “all possible \\(t\\)-tests” discussion in the ANOVA section), but if you try to draw plots with the data laid out that way, you will at best be repeating yourself a lot and at worst get stuck (if you try to make side-by-side boxplots). The right format of data should give you no surprises!\n\nI split the pivot-longer onto several lines so that it wouldn’t run off the right side of the page. Recall that R doesn’t mind if you have it on one line or several, as long as it can tell that the current line is incomplete, which it must be until it sees the closing parenthesis on the pivot_longer.\n\nA normal quantile plot is also justifiable here, but I think it would take more justification, because you would have to sell your reader on the need for normality this early in the story. A standard plot like a boxplot needs no such justification; it just describes centre, spread and shape, exactly the kind of thing your first look at the data should be telling you about.\nSay something about what the graph is telling you. It makes sense to do as I did and look forward to the kind of inference you are going to try. If you do a normal quantile plot here, you can formally assess the assumptions before you do the test, which you might argue makes more sense, rather than doing a second plot and assessing the assumptions later as I do.\nYou might be able to justify a one-sided test here, along the lines of “is the online instruction significantly worse?”, but in any case, you need to justify the one-sided or two-sided test that you do.\nConclusion in the context of the data, as ever. Writing “we fail to reject the null hypothesis” and then stopping invites your reader to ask “so what?”.\nYou might have chosen to do a different test, but your choice needs to be properly justified. I think the pooled \\(t\\)-test is the best choice here. If you thought those marks were not normal enough, then you need to do Mood’s median test, explaining why. That comes out like this:\n\n\nmedian_test(prog, mark, instruction)\n\n$table\n           above\ngroup       above below\n  classroom     3     6\n  online        6     3\n\n$test\n       what     value\n1 statistic 2.0000000\n2        df 1.0000000\n3   P-value 0.1572992\n\n\nThe (two-sided) P-value is a little bigger than the one for the pooled \\(t\\)-test, but the conclusion is the same. (If you think the test should be one-sided, justify dividing your P-value by 2, if you can. In the case that your one-sided alternative was that classroom teaching was better, you cannot reject the null in favour of that, because the online students actually have better marks in these samples. In that case, halving the P-value would not be justifiable.)\nIf you thought that the normality was all right, but the equal spreads was not, you will need to justify the unequal spreads. Perhaps the best way here is to say that you are unsure whether the spreads are close enough to equal, so you are doing the Welch test to be safe. That looks like this:\n\nt.test(mark~instruction, data = prog)\n\n\n    Welch Two Sample t-test\n\ndata:  mark by instruction\nt = -1.6495, df = 15.844, p-value = 0.1187\nalternative hypothesis: true difference in means between group classroom and group online is not equal to 0\n95 percent confidence interval:\n -8.382820  1.049486\nsample estimates:\nmean in group classroom    mean in group online \n               31.55556                35.22222 \n\n\nAs you see, the P-values of the pooled and Welch \\(t\\)-tests are almost identical (which is what I was guessing), but to do the Welch test without comment reveals that you are not thinking about which of the two tests is more appropriate here. In the real world, you might get away with it (for these data, the conclusions are the same), but in this course I need to see that your thought process is correct.\nA final observation in this note: all three tests give you similar P-values and the same conclusion, so that in the end it didn’t really matter which one of them you did. When that happens, it’s usually (as here) a sign that a \\(t\\)-test will be best, because it makes the best use of the data (and thus you will get the most power in your test). I cannot stop you doing all three tests behind the scenes and using this to help decide, but strictly speaking your P-value will not be what you say it is, because these tests (really, any tests) are designed so that the test you choose is the only one you do. In any case, your report should include only the one test you thought was most appropriate. Your reader does not have the time or patience for a detailed comparison of the three tests.\n\nYou might have combined the assessment of assumptions with your first plot, particularly if you chose a normal quantile plot instead of a boxplot there. As long as you have assessed normality (and equal spreads, if you are happy with the normality) somewhere, this is fine. In particular, if you ended up doing a Mood median test, you have presumably already decided that the normality was not good enough, and (I hope) you already discussed that somewhere.\n\nAgain, bear in mind who is (in the setup that we have) reading your report: someone who might remember about \\(t\\)-tests and normality. Ideas like the bootstrap are far too advanced to go into a report like this. If you must include something like that, you need to put it in an appendix, not the main body of the report, so that the person reading your report can get the main ideas without having to wade through that.\nThe rest of this note is an Extra:\nHere is a slightly different way to approach the bootstrap in this case, that takes care of assessing the normality of both groups at once.\nThe first step is nest_by, which does two things, one of which is invisible:\n\nprog %&gt;% nest_by(instruction)\n\n\n\n  \n\n\n\nThis (visibly) creates a list-column containing two mini dataframes data. They are the original data with all the data except instruction in each one: that is, the other column mark. The top one is the nine marks for the students in the classroom section, and the bottom one is the nine marks for the students in the online section. The invisible thing is that nest_by includes a rowwise, so that what we do after this is one for each row, one at a time.\nWhat we do next is to generate a lot of bootstrap samples, for each group. First, a vector of simulation numbers, one for each method of instruction. This only needs to be said once because nest_by is like group_by:\n\nprog %&gt;% nest_by(instruction) %&gt;% \n  mutate(sim = list(1:10))\n\n\n\n  \n\n\n\nThen we need to unnest those sims, so that we can put a bootstrap sample next to each one:\n\nprog %&gt;% nest_by(instruction) %&gt;% \n  mutate(sim = list(1:10)) %&gt;% \n  unnest(sim)\n\n\n\n  \n\n\n\nNow we draw a bootstrap sample from the data to the left of each sim (the ones next to classroom are the in-person ones, and the ones next to online are the online ones, so we will be taking bootstrap samples of the right thing). This here is where the rowwise goes:\n\nprog %&gt;% nest_by(instruction) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(the_sample = list(sample(data$mark, replace = TRUE))) %&gt;% \n  mutate(the_mean = mean(the_sample)) %&gt;% \n  ggplot(aes(sample = the_mean)) + stat_qq() +\n    stat_qq_line() + facet_wrap(~instruction)\n\n\n\n\nThese are close to their lines, which tells me indeed that the two-sample \\(t\\)-test I did was perfectly reasonable.\nAs I say, though, this is very much for you, and not for the report, given who would be reading it. If you were doing a presentation on this, the bootstrap stuff is something you would keep in reserve in case someone asks about the appropriateness of the \\(t\\)-test, and then you could talk about it, but otherwise you wouldn’t mention it at all.\n\nYou definitely need some conclusions. If the department chair is busy (quite likely), this is the only part of the entire report they may be able to read.\nThis is the place to put limitations, and recommendations for next time. The sample size one is (I hope) obvious, but you can also get at the other one by asking yourself how the students were assigned to instruction methods. The classical comparative study assigns them at random; that way, you know that the two groups of students are at least supposed to be about equal on ability (and anything else) before you start. But if the students choose their own groups, it might be that (say) the weaker students choose the classroom instruction, and in that case the reason the online group came out looking better might be that they were stronger students: it could have nothing to do with the effectiveness of the learning environment at all.\n\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dandruff.html#introduction",
    "href": "dandruff.html#introduction",
    "title": "14  Treating dandruff",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nShampoos are often claimed to be effective at treating dandruff. In a study, the dandruff-treating properties of four shampoos were compared. These four shampoos were, as referred to in the dataset:\n\nPyrI: 1% pyrithione zinc shampoo\nPyrII: as PyrI but with instructions to shampoo two times at each wash.1\nKeto: 2% ketoconazole shampoo\nPlacebo: a placebo shampoo\n\nEach of the experimental subjects was randomly given one of the shampoos. After using their shampoo for six weeks, eight sections of the subject’s scalp were examined for each subject. Each section of the scalp was given a score that measured the amount of flaking on a scale of 0-10. The response variable, called Flaking, was the sum of these eight scores, and is a whole number for each subject. A smaller value of Flaking indicates less dandruff.2\nOur aim is to see which shampoo or shampoos are most effective at treating dandruff, that is, have the smallest value of Flaking on average."
  },
  {
    "objectID": "dandruff.html#exploratory-analysis",
    "href": "dandruff.html#exploratory-analysis",
    "title": "14  Treating dandruff",
    "section": "14.2 Exploratory analysis",
    "text": "14.2 Exploratory analysis\nWe begin by reading in the data:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/dandruff.txt\"\ndandruff &lt;- read_tsv(my_url)\n\nRows: 355 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Treatment\ndbl (3): OBS, GroupNum, Flaking\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndandruff\n\n\n\n  \n\n\n\n355 subjects took part in the study altogether. The shampoo used is indicated in the Treatment column. The remaining columns OBS and GroupNum will not be used in this analysis.\nNumerical summaries of the data are as shown:\n\ndandruff %&gt;% group_by(Treatment) %&gt;% \nsummarise(n=n(), fl_mean=mean(Flaking), fl_sd=sd(Flaking))\n\n\n\n  \n\n\n\nThere are approximately 100 observations in each group, apart from the Placebo group, which had only 28. The mean number of flakes is much higher for the Placebo group than for the others, which seem similar. The group standard deviations are fairly similar.\nWith a categorical Treatment and a quantitative Flakes, a suitable graph is a side-by-side boxplot:\n\nggplot(dandruff, aes(x=Treatment, y=Flaking)) + geom_boxplot() \n\n\n\n\nOnce again, we see that the flaking for the Placebo shampoo is much higher than for the others. There are outliers in the PyrI group, but given that the data values are all whole numbers, they are not far different from the rest of the data. Considering these outliers, the spreads of the groups all look fairly similar and the distributions appear more or less symmetric.3"
  },
  {
    "objectID": "dandruff.html#analysis-of-variance",
    "href": "dandruff.html#analysis-of-variance",
    "title": "14  Treating dandruff",
    "section": "14.3 Analysis of Variance",
    "text": "14.3 Analysis of Variance\nFor comparing four groups, we need some kind of analysis of variance. Having seen that the Flaking values within the four groups are more or less normal with more or less equal spreads, we run a standard ANOVA:\n\ndandruff.1 &lt;- aov(Flaking~Treatment, data=dandruff)\nsummary(dandruff.1)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTreatment     3   4151  1383.8   967.8 &lt;2e-16 ***\nResiduals   351    502     1.4                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWith an extremely small P-value, we conclude that the four shampoos do not all have the same mean value of Flaking.\nTo find out which ones are different from which, we use Tukey’s method:\n\nTukeyHSD(dandruff.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Flaking ~ Treatment, data = dandruff)\n\n$Treatment\n                     diff         lwr         upr     p adj\nPlacebo-Keto   13.3645553  12.7086918  14.0204187 0.0000000\nPyrI-Keto       1.3645553   0.9462828   1.7828278 0.0000000\nPyrII-Keto      1.1735330   0.7524710   1.5945950 0.0000000\nPyrI-Placebo  -12.0000000 -12.6521823 -11.3478177 0.0000000\nPyrII-Placebo -12.1910223 -12.8449971 -11.5370475 0.0000000\nPyrII-PyrI     -0.1910223  -0.6063270   0.2242825 0.6352706\n\n\nAll of the shampoo treatments are significantly different from each other except for the two pyrithione ones. To see which shampoos are best and worst, we remind ourselves of the treatment means:\n\ndandruff %&gt;% group_by(Treatment) %&gt;% \nsummarise(n=n(), fl_mean=mean(Flaking), fl_sd=sd(Flaking))\n\n\n\n  \n\n\n\nThe placebo shampoo has significantly more flaking than all the others, and the ketoconazole4 shampoo has significantly less flaking than all the others. From this analysis, therefore, we would recommend the ketoconazole shampoo over all the others."
  },
  {
    "objectID": "dandruff.html#assessment-of-assumptions",
    "href": "dandruff.html#assessment-of-assumptions",
    "title": "14  Treating dandruff",
    "section": "14.4 Assessment of Assumptions",
    "text": "14.4 Assessment of Assumptions\nThe analysis of variance done above requires that the observations within each treatment group (shampoo) be approximately normally distributed, given the sample sizes, with approximately equal spreads. To assess this, we look at normal quantile plots5 for each shampoo:6\n\nggplot(dandruff, aes(sample=Flaking)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~Treatment, scales = \"free\")\n\n\n\n\nGiven that the data are whole numbers, the distributions each appear close to the lines, indicating that the distributions are close to normal in shape.7 The distribution of the PyrI values is slightly long-tailed, but with over 100 observations in that group, this shape is not enough to invalidate the normality assumption.8\nHaving concluded that the normality is sufficient, we need to assess the equality of spreads. Referring back to our summary table:\n\ndandruff %&gt;% group_by(Treatment) %&gt;% \nsummarise(n=n(), fl_mean=mean(Flaking), fl_sd=sd(Flaking))\n\n\n\n  \n\n\n\nwe note that the spreads are not greatly different, and so the equal-spread assumption appears to be satisfied.9\nIn summary, the assumptions for the analysis of variance we did seem to be reasonably well satisfied, and we can have some confidence in our conclusions."
  },
  {
    "objectID": "dandruff.html#conclusions",
    "href": "dandruff.html#conclusions",
    "title": "14  Treating dandruff",
    "section": "14.5 Conclusions",
    "text": "14.5 Conclusions\nWe found that the ketoconazole shampoo produced the smallest mean flaking, and its mean was significantly smaller than that of all the other shampoos. This shampoo can be recommended over the others. There was no significant difference between the two pyrithione treatments; shampooing twice had no benefit over shampooing once.\nThe difference in means between the ketoconazole and the two pyrithione shampoos was only about 1.2. This difference was significant because of the large sample sizes, but it is a separate question as to whether a difference of this size is of practical importance. If it is not, any of the shampoos except for the placebo can be recommended."
  },
  {
    "objectID": "dandruff.html#end",
    "href": "dandruff.html#end",
    "title": "14  Treating dandruff",
    "section": "14.6 End",
    "text": "14.6 End\nThat is the end of my report. You, of course, don’t need the word “end” or any of the footnotes I had. These were to draw your attention to other things that don’t necessarily belong in the report, but I would like you to be aware of them. When you reach the end of your report, you can just stop.10\nSome extra, bigger, thoughts (there are quite a few of these. I hope I don’t repeat things I also say in the “sidenotes”):\nExtra 1: The placebo group is much smaller than the others, but all the groups are pretty big by ANOVA standards. Apparently what happened is that originally the three “real” treatments had 112 subjects each, but the placebo had 28 (ie., a quarter of the subjects that the other groups had), and a few subjects dropped out. There’s no problem, in one-way ANOVAs of this kind, with having groups of unequal sizes; the \\(F\\)-test is fine, and as long as you use a suitable extension of Tukey that deals with unequal sample sizes, you are OK there too. TukeyHSD, according to the help file, “incorporates an adjustment for sample size that produces sensible intervals for mildly unbalanced designs”. In this case, we might be holding our breath a bit, depending on what “mildly unbalanced” actually means. Usually in this kind of study, you have the groups about the same size, because proving that the smallest group differs from any of the others is more difficult. I guess these researchers were pretty confident that the placebo shampoo would be clearly worse than the others! (Additional: TukeyHSD uses the so-called Tukey-Kramer test when sample sizes within groups are unequal. My understanding is that this is good no matter what the sample sizes are.)\nExtra 2: The mean for Placebo is quite a lot bigger than for the other groups, so a plot with different scales for each facet is best. Otherwise you get this kind of thing, which is much harder to read:\n\nggplot(dandruff, aes(sample=Flaking)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~Treatment)\n\n\n\n\nThe points fill less than half their facets, which makes the plots harder to understand. This also makes it look as if the distributions are more normal, because the vertical scale has been compressed. Having a better basis for assessing the normality is a good idea, given that the purpose of the plot is assessing the normality! Hence, using scales = \"free\" is best.\nExtra 3: You might have been wondering why the boxplots, which are the usual thing in these circumstances, look worse than the normal quantile plots.\nLet’s revisit them and see what happened:\n\nggplot(dandruff, aes(x=Treatment, y=Flaking)) + geom_boxplot() \n\n\n\n\nThe Placebo group has the largest IQR, and the PyrI group appears to have two outliers. We need to bear in mind, though, that the data values are whole numbers and there might be repeats; also, what looks like an outlier here might not look quite so much like one when we see all the data.\nWhat you can do is to add a geom_point on to the plot to plot the observations as points:\n\nggplot(dandruff, aes(x=Treatment, y=Flaking)) + \ngeom_boxplot() + geom_point()\n\n\n\n\nBut there are a lot more data points than this! What happened is that a point at, say, 20 is all the observations in that group that were 20, of which there might be a lot, but we cannot see how many, because they are printed on top of each other. To see all the observations, we can jitter them: that is, plot them all not in the same place. In this case, we have the whole width of the boxplot boxes to use; we could also jitter vertically, but I decided not to do that here. There is a geom_jitter that does exactly this:11\n\nggplot(dandruff, aes(x=Treatment, y=Flaking)) + geom_boxplot() + \ngeom_jitter(height = 0)\n\n\n\n\nThe plot is rather messy,12 but now you see everything. The height=0 means not to do any vertical jittering: just spread the points left and right.13 Where the points are exactly on the \\(x\\)-scale is now irrelevant; this is just a device to spread the points out so that you can see them all.\nI left the vertical alone so that you can still see the actual data values. Even though the highest and lowest values in PyrI were shown as outliers on the original boxplot, you can see that they are really not. When the data values are discrete (separated) like this, an apparent outlier may be only one bigger or smaller than the next value, and thus not really an outlier at all.\nTo try the vertical jittering too, use the defaults on geom_jitter:\n\nggplot(dandruff, aes(x=Treatment, y=Flaking)) + geom_boxplot() + geom_jitter()\n\n\n\n\nThis maybe spreads the points out better, so you can be more certain that you’re seeing them all, but you lose the clear picture of the data values being whole numbers.\nWhen Tukey popularized the boxplot, his idea was that it would be drawn by hand with relatively small samples, and when you draw boxplots for large samples, you can get an apparently large number of outliers, that are not in retrospect quite as extreme as they may look at first. This may also have happened here. Tukey, however, did not invent the boxplot; credit for that goes to Mary Eleanor Spear with her “range plot”.14\nExtra 4: More discussion of normality. The assumptions for a standard ANOVA (that is to say, not a Welch ANOVA) are normally-distributed data within each treatment group, with equal spreads. What that means in practice is that you want normal enough data given the sample sizes, and approximately equal spreads. My normal quantile plots are a long way back, so let’s get them again:\n\nggplot(dandruff, aes(sample=Flaking)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~Treatment, scales = \"free\")\n\n\n\n\nThe data values are all whole numbers, so we get those horizontal stripes of Flaking values that are all the same. As long as these more or less hug the line, we are all right. The PyrII values certainly do. In my top row, Keto and Placebo are not quite so good, but they have short tails compared to the normal, so there will be no problem using the means for these groups, as ANOVA does. The only one that is problematic at all is PyrI. That has slightly long tails compared to a normal. (You could, I suppose, call those highest and lowest values “outliers”, but I don’t think they are far enough away from the rest of the data to justify that.) Are these long tails a problem? That depends on how many observations we have:\n\ndandruff %&gt;% group_by(Treatment) %&gt;% \nsummarise(n=n(), mean_flaking=mean(Flaking), sd_flaking=sd(Flaking))\n\n\n\n  \n\n\n\nThere are 112 of them. Easily enough to overcome those long tails. So, to my mind, normality is no problem.\nAside: you might be wondering whether you can make nicer-looking tables in your reports. There are several ways. The gt package is the most comprehensive one I know, and has links to a large number of others (at the bottom of its webpage). The simplest one I know of is kable in the knitr package. You may well have that package already installed, but you’ll need to load it, preferably at the beginning of your report:\n\nlibrary(knitr)\ndandruff %&gt;% group_by(Treatment) %&gt;% \nsummarise(n=n(), mean_flaking=mean(Flaking), sd_flaking=sd(Flaking)) -&gt; summary\nkable(summary)\n\n\n\n\nTreatment\nn\nmean_flaking\nsd_flaking\n\n\n\n\nKeto\n106\n16.02830\n0.9305149\n\n\nPlacebo\n28\n29.39286\n1.5948827\n\n\nPyrI\n112\n17.39286\n1.1418110\n\n\nPyrII\n109\n17.20183\n1.3524999\n\n\n\n\n\nEnd of aside.\nBefore I got distracted, we were talking about whether the distribution of PyrI was normal enough, given the sample size. Another way of thinking about this is to look at the bootstrapped sampling distribution of the sample mean for this group. I set the random number seed so that the results will be the same even if I run this again:\n\nset.seed(457299)\n\n\ndandruff %&gt;% filter(Treatment == \"PyrI\") -&gt; pyri\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(pyri$Flaking, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nOh yes, no problem with the normality there. (The discreteness of the population implies that each sample mean is some number of one-hundred-and-twelfths, so that the sampling distribution is also discrete, just less discrete than the data distribution. This is the reason for the little stair-steps in the plot.) In addition, the fact that the least normal distribution is normal enough means that the other distributions must also be OK. If you wanted to be careful, you would assess the smallest Placebo group as well, though that if anything is short-tailed and so would not be a problem anyway.\nThe other question is whether those spreads are equal enough. The easiest way is to look back at your summary table (that I reproduced above), cast your eye down the SD column, and make a call about whether they are equal enough. The large sample sizes don’t help here, although see the end of the question for more discussion. I would call these “not grossly unequal” and call standard ANOVA good, but you are also entitled to call them different enough, and then you need to say that in your opinion we should have done a Welch ANOVA. Or, if you got your normal quantile plots before you did your ANOVA, you could actually do a Welch ANOVA.\nI am not a fan of doing one test to see whether you can do another test,15 but if you really want to, you can use something like Levene’s test to test the null hypothesis that all the groups have the same variance.16 Levene’s test lives in the package car that you might have to install first:\n\nlibrary(car)\nleveneTest(Flaking~Treatment, data = dandruff)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\n\n\n  \n\n\n\nEqual variances are resoundingly rejected here; the samples here have variances that are less equal than they would be if the populations all had the same variances. But that is really asking the wrong question: the one that matters is “does the inequality of variances that we saw here matter when it comes to doing the ANOVA?”. With samples as big as we had, the variances could be declared unequal even if they were actually quite similar. This is another (different) angle on statistical significance (rather similar variances can be significantly different with large samples) vs. practical importance (does the fact that our sample variances are as different as they are matter to the ANOVA?). I do the Welch ANOVA in Extra 6, and you will see there whether it comes out much different than the regular ANOVA. See also Extra 7.\nIf your normal quantile plots looked like this:\n\nggplot(dandruff, aes(sample=Flaking)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~Treatment)\n\n\n\n\nwith the same scales, you can use the slopes of the lines to judge equal spreads: either equal enough, or the Placebo line is a bit steeper than the others. If you did scales = \"free\", you cannot do this, because you have essentially standardized your data before making the normal quantile plots.\nIt is hugely important to distinguish the null hypothesis (all the means are the same) from the assumptions behind the test (how you know that the P-value obtained from testing your null hypothesis can be trusted). These are separate things, and getting them straight is a vital part of being a good statistician. You might say that this is part of somebody else knowing how they, as someone hiring a statistician, can trust you.\nExtra 5: Several ways to say what you conclude from the ANOVA:\n\nThe null hypothesis, which says that all the shampoos have the same mean amount of flaking, is rejected. (Or say it in two sentences: what the null hypothesis is, and then what you’re doing with it.)\nNot all the shampoos have the same mean amount of flaking.\nThere are shampoos that differ in mean amount of flaking.\n\nSome wrong or incomplete ways to say it:\n\nWe reject the null hypothesis. (Meaning what, about the data?)\nwe reject the null hypothesis that the means are different. (You have confused the null with the conclusion, and come out with something that is backwards.)\nthe mean flaking for the treatments is different (this says that they are all different, but you don’t know that yet.)\n\nExtra 6: You might have been wondering how Welch’s ANOVA would have played out, given that the placebo group measurements looked more variable than the others. Wonder no more:\n\noneway.test(Flaking~Treatment, data=dandruff)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  Flaking and Treatment\nF = 595.03, num df = 3.00, denom df = 105.91, p-value &lt; 2.2e-16\n\ngamesHowellTest(Flaking~factor(Treatment), data=dandruff)\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: Flaking by factor(Treatment)\n\n\n        Keto    Placebo PyrI\nPlacebo 9.2e-14 -       -   \nPyrI    8.7e-14 &lt; 2e-16 -   \nPyrII   2.1e-11 &lt; 2e-16 0.67\n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nThe results are almost exactly the same: the conclusions are identical, and the P-values are even pretty much the same. The place where it would make a difference is when you are close to the boundary between rejecting and not. Here, our Tukey and Games-Howell P-values were all either close to 0 or about 0.6, whichever way we did it. So it didn’t matter which one we did; you could justify using either. The regular ANOVA might have been a better choice for your report, though, because this is something your audience could reasonably be expected to have heard of. The Welch ANOVA deserves to be as well-used as the Welch two-sample \\(t\\)-test, but it doesn’t often appear in Statistics courses. (This course is an exception, of course!)\nExtra 7: the general principle when you are not sure of the choice between two tests is to run them both. If the conclusions agree, as they do here, then it doesn’t matter which one you run. If they disagree, then it matters, and you need to think more carefully about which test is the more appropriate one. (Usually, this is the test with the fewer assumptions, but not always.)\nAnother way to go is to do a simulation (of the ordinary ANOVA). Generate some data that are like what you actually have, and then in your simulation see whether your \\(\\alpha\\) is near to 0.05. Since we are talking about \\(\\alpha\\) here, the simulated data needs to have the same mean in every group, so that the null hypothesis is true, but SDs and sample sizes like the ones in the data (and of a normal shape). Let me build up the process. Let’s start by making a dataframe that contains the sample sizes, means and SDs for the data we want to generate. The treatment names don’t matter:\n\nsim_from &lt;- tribble(\n~trt, ~n, ~mean, ~sd,\n\"A\", 106, 0, 0.93,\n\"B\", 28, 0, 1.59,\n\"C\", 112, 0, 1.14,\n\"D\", 109, 0, 1.35\n)\nsim_from\n\n\n\n  \n\n\n\nStarting from here, we want to set up drawing a lot of random samples from a normal distribution with the mean and SD shown, and the sample size shown. The way I like to do it17 is to set up a list-column called sim that will index the simulations. I’m going to pretend I’m doing just three simulations, while I get my head around this, and then up it later after I have things working:\n\nsim_from %&gt;% \n  mutate(sim = list(1:3))\n\n\n\n  \n\n\n\nThen I unnest sim so that I have a place to draw each sample:\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim)\n\n\n\n  \n\n\n\nNow, working rowwise, I can draw a random sample from each normal distribution. The inputs to rnorm are the sample size, the mean, and the SD, in that order.18\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd)))\n\n\n\n  \n\n\n\nThis is looking similar in procedure to a bootstrap sampling distribution. If we were doing that, we would now make a new column containing something like the mean of each of those samples (and then make a picture of what we had). But this is different: we want to combine all of the random samples for each of the four treatments for one of the simulations, run an ANOVA on it, and get hold of the P-value. So we need to unnest those samples, and then combine them together properly. That goes something like this:\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim)\n\n\n\n  \n\n\n\nWhat this has done is to create three mini-dataframes in the list-column data that have our generated random y and a column called treatment. What we want to do is to run the ordinary ANOVA on each of those dataframes in data, and, in a minute, get hold of the P-value. I think I need another rowwise first, because I want to work with the rows of the new dataframe:\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_anova = list(aov(y ~ trt, data = data)))\n\n\n\n  \n\n\n\nI know the P-value is in there somewhere, but I can’t remember how to get hold of it. The easiest way is to load broom and pass the models into tidy, then take a look at that:\n\nlibrary(broom)\n\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_anova = list(aov(y ~ trt, data = data))) %&gt;% \n  mutate(my_tidy = list(tidy(my_anova))) %&gt;% \n  unnest(my_tidy)\n\n\n\n  \n\n\n\nAlmost there. The rows that have P-values in them are the ones that have trt (the explanatory variable) in the term, so:\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_anova = list(aov(y ~ trt, data = data))) %&gt;% \n  mutate(my_tidy = list(tidy(my_anova))) %&gt;% \n  unnest(my_tidy) %&gt;% \n  filter(term == \"trt\") %&gt;% \n  select(sim, p.value)\n\n\n\n  \n\n\n\nThree simulated samples, each time one from each of the four treatments, and three P-values. So this works, and the remaining thing is to change the number of simulations from 3 to 1000 and run it again, saving the result:\n\nsim_from %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_anova = list(aov(y ~ trt, data = data))) %&gt;% \n  mutate(my_tidy = list(tidy(my_anova))) %&gt;% \n  unnest(my_tidy) %&gt;% \n  filter(term == \"trt\") %&gt;% \n  select(sim, p.value) -&gt; sim_pval\nsim_pval\n\n\n\n  \n\n\n\nNow, the reason we were doing this was to see whether regular ANOVA worked properly on data from populations with different SDs. We know that the null hypothesis is true here (because all the true treatment means were equal to 0), so the probability of making a type I error by rejecting the null (that all the means are the same) should be 0.05. How close is it?\n\nsim_pval %&gt;% \n  count(p.value &lt;= 0.05)\n\n\n\n  \n\n\n\n\\(91/1000 = 0.091\\). We are too likely to falsely reject the null. The regular ANOVA does not behave properly for data like ours.\nThat looks rather high. Is the proportion of times I am rejecting significantly different from 0.05? Testing null hypotheses about (single) proportions is done using prop.test. This uses the normal approximation to the binomial, with continuity correction:\n\nprop.test(91, 1000, p = 0.05)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  91 out of 1000, null probability 0.05\nX-squared = 34.532, df = 1, p-value = 4.194e-09\nalternative hypothesis: true p is not equal to 0.05\n95 percent confidence interval:\n 0.07425035 0.11096724\nsample estimates:\n    p \n0.091 \n\n\nAh, now, that’s interesting. A supposed \\(\\alpha = 0.05\\) test is actually rejecting around 9% of the time, which is significantly different from 0.05. This surprises me. So the ANOVA is actually not all that accurate.19\nSo now let’s do the same simulation for the Welch ANOVA to see whether it’s better:\n\nsim_from %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_anova = list(oneway.test(y ~ trt, data = data))) %&gt;%\n  mutate(pval = my_anova$p.value) -&gt; sim_pval2\nsim_pval2\n\n\n\n  \n\n\n\nThis one is a bit easier because oneway.test has a thing called p.value that you can just pull out. No need to use tidy here.\nHow many of those P-values are less than 0.05?\n\nsim_pval2 %&gt;% \n  count(pval &lt;= 0.05)\n\n\n\n  \n\n\n\nThat couldn’t be much closer to the mark:\n\nprop.test(49, 1000, p = 0.05)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  49 out of 1000, null probability 0.05\nX-squared = 0.0052632, df = 1, p-value = 0.9422\nalternative hypothesis: true p is not equal to 0.05\n95 percent confidence interval:\n 0.03682698 0.06475244\nsample estimates:\n    p \n0.049 \n\n\nThis one is on the money.20 The proportion of times our simulation falsely rejects is not significantly different from 0.05. So this investigation says that the Welch ANOVA is much more trustworthy for data resembling what we observed.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dandruff.html#footnotes",
    "href": "dandruff.html#footnotes",
    "title": "14  Treating dandruff",
    "section": "",
    "text": "The piece in the problem statement about why these two labels were used is clarification for you and doesn’t belong in the report. If you leave it in, you need to at least paraphrase it; simply copying it without having a reason to do so shows that you are not thinking.↩︎\nI’m using a fair few of my own words from the question. This is OK if you think they are clear, but the aim is to write a report that sounds like you rather than me.↩︎\nOffer supported opinions of your own here, which don’t need to be the same as mine. Alternatively, you can get the graph and numerical summaries first and comment on them both at once.↩︎\nUse the full name of the shampoo if you are making a conclusion about it.↩︎\nI’ve used scales = \"free\" to get the plots to fill their boxes, for the best assessment of normality. The downside of doing it this way is that you cannot use the slopes of the lines to compare spreads. I think this way is still better, though, because the mean for placebo is so much bigger than the others that if you use the same scale for each plot, you’ll be wasting a lot of plot real estate that you could use to get a better picture of the normality.↩︎\nIt’s also good, arguably clearer, to use this as your exploratory plot. This enables you to get to a discussion about normality earlier and you might decide in that case that you don’t even need this discussion. You can do the assessment of assumptions first, and then do the corresponding analysis, or you can pick an apparently reasonable analysis and then critique it afterwards. Either way is logical here. In other cases it might be different; for example, in a regression, you might need to fit a model first and improve it after, since it may not be so clear what a good model might be off the top.↩︎\nThis is a way to write it if you suspect your reader won’t remember what a normal quantile plot is, and by writing it this way you won’t insult their intelligence if they do remember after all. The other side benefit of writing it this way is that it shows your understanding as well.↩︎\nIf you have any doubts about sufficient normality, you need to make sure you have also considered the relevant sample size, but if you are already happy with the normality, there is no need. The placebo group, for example, is the smallest, but its shape is if anything short-tailed, so its non-normality will be no problem no matter how small the sample is.↩︎\nI’d rather assess equality of spreads by eyeballing them than by doing a test, but if you really want to, you could use Levene’s test, illustrated elsewhere in PASIAS and in Extra 4 for these data. It works for any number of groups, not just two.↩︎\nI wanted to make it clear where my report ended and where the additional chat began.↩︎\nI explain the height=0 below the plot.↩︎\nIt looks to me as if the boxplot has been attacked by mosquitoes.↩︎\nThe default jittering is up to a maximum of not quite halfway to the next value. Here that means that each observation is nearest to the box it belongs with.↩︎\nI learned this today.↩︎\nBecause the true alpha for the combined procedure in which the test you do second depends on the result of the first test is no longer 0.05; you need to think about what that true alpha is. It might not be too bad here, because regular ANOVA and Welch ANOVA tend to come out similar unless the sample variances are very different, in the same way that the Welch and pooled two-sample tests do. But it is not something to take for granted.↩︎\nThere are other tests you could use here. I like Levene’s test because it works best when the samples are not normal, but the normality is OK here, so this is not so much of an issue. Of course, the time you want to be assessing equality of variances is when you have already seen sufficient normality, but that might have been because the samples were large rather than that they were especially normal in shape themselves.↩︎\nAt this moment, subject to change, etc etc.↩︎\nOr you can remember or look up the names to the inputs, and then you can have them in any order you like. But I know which order they come, which is good enough for me.↩︎\nWhen you get a simulation result that is not what you were expecting, there are two options to explore: either it really is different from your expectation, or there is something wrong with your code. I think my code is OK here, but do let me know if you see a problem with it.↩︎\nYou could redo this with 10,000 simulations to convince yourself further.↩︎"
  },
  {
    "objectID": "tidying-data.html#baseball-and-softball-spaghetti",
    "href": "tidying-data.html#baseball-and-softball-spaghetti",
    "title": "15  Tidying data",
    "section": "15.1 Baseball and softball spaghetti",
    "text": "15.1 Baseball and softball spaghetti\nOn a previous assignment, we found that students could throw a baseball further than they could throw a softball. In this question, we will make a graph called a “spaghetti plot” to illustrate this graphically. (The issue previously was that the data were matched pairs: the same students threw both balls.)\nThis seems to work most naturally by building a pipe, a line or two at a time. See if you can do it that way. (If you can’t make it work, use lots of temporary data frames, one to hold the result of each part.)\n\nRead in the data again from link. The variables had no names, so supply some, as you did before.\nCreate a new column that is the students turned into a factor, adding it to your data frame. The reason for this will become clear later.\nCollect together all the throwing distances into one column, making a second column that says which ball was thrown.\nUsing your new data frame, make a “scatterplot” of throwing distance against type of ball.\nAdd two things to your plot: something that will distinguish the students by colour (this works best if the thing distinguished by colour is a factor),1 and something that will join the two points for the same student by a line.\nThe legend is not very informative. Remove it from the plot, using guides.\nWhat do you see on the final spaghetti plot? What does that tell you about the relative distances a student can throw a baseball vs. a softball? Explain briefly, blah blah blah."
  },
  {
    "objectID": "tidying-data.html#ethanol-and-sleep-time-in-rats",
    "href": "tidying-data.html#ethanol-and-sleep-time-in-rats",
    "title": "15  Tidying data",
    "section": "15.2 Ethanol and sleep time in rats",
    "text": "15.2 Ethanol and sleep time in rats\nA biologist wished to study the effects of ethanol on sleep time in rats. A sample of 20 rats (all the same age) was selected, and each rat was given an injection having a particular concentration (0, 1, 2 or 4 grams per kilogram of body weight) of ethanol. These are labelled e0, e1, e2, e4. The “0” treatment was a control group. The rapid eye movement (REM) sleep time was then recorded for each rat. The data are in link.\n\nRead the data in from the file. Check that you have four rows of observations and five columns of sleep times.\nUnfortunately, the data are in the wrong format. All the sleep times for each treatment group are on one row, and we should have one column containing all the sleep times, and the corresponding row should show which treatment group that sleep time came from. Transform this data frame into one that you could use for modelling or making graphs.\nUsing your new data frame, make side-by-side boxplots of sleep time by treatment group.\nIn your boxplots, how does the median sleep time appear to depend on treatment group?\nThere is an assumption about spread that the analysis of variance needs in order to be reliable. Do your boxplots indicate that this assumption is satisfied for these data, bearing in mind that you have only five observations per group?\nRun an analysis of variance to see whether sleep time differs significantly among treatment groups. What do you conclude?\nWould it be a good idea to run Tukey’s method here? Explain briefly why or why not, and if you think it would be a good idea, run it.\nWhat do you conclude from Tukey’s method? (This is liable to be a bit complicated.) Is there a treatment that is clearly best, in terms of the sleep time being largest?"
  },
  {
    "objectID": "tidying-data.html#growth-of-tomatoes",
    "href": "tidying-data.html#growth-of-tomatoes",
    "title": "15  Tidying data",
    "section": "15.3 Growth of tomatoes",
    "text": "15.3 Growth of tomatoes\nA biology graduate student exposed each of 32 tomato plants to one of four different colours of light (8 plants to each colour). The growth rate of each plant, in millimetres per week, was recorded. The data are in link.\n\nRead the data into R and confirm that you have 8 rows and 5 columns of data.\nRe-arrange the data so that you have one column containing all the growth rates, and another column saying which colour light each plant was exposed to. (The aim here is to produce something suitable for feeding into aov later.)\nSave the data in the new format to a text file. This is most easily done using write_csv, which is the opposite of read_csv. It requires two things: a data frame, and the name of a file to save in, which should have a .csv extension.\nMake a suitable boxplot, and use it to assess the assumptions for ANOVA. What do you conclude? Explain briefly.\nRun (regular) ANOVA on these data. What do you conclude? (Optional extra: if you think that some other variant of ANOVA would be better, run that as well and compare the results.)\nIf warranted, run a suitable follow-up. (If not warranted, explain briefly why not.)"
  },
  {
    "objectID": "tidying-data.html#pain-relief-in-migraine-headaches-again",
    "href": "tidying-data.html#pain-relief-in-migraine-headaches-again",
    "title": "15  Tidying data",
    "section": "15.4 Pain relief in migraine headaches (again)",
    "text": "15.4 Pain relief in migraine headaches (again)\nThe data in link are from a study of pain relief in migraine headaches. Specifically, 27 subjects were randomly assigned to receive one of three pain relieving drugs, labelled A, B and C. Each subject reported the number of hours of pain relief they obtained (that is, the number of hours between taking the drug and the migraine symptoms returning). A higher value is therefore better. Can we make some recommendation about which drug is best for the population of migraine sufferers?\n\nRead in and display the data. Take a look at the data file first, and see if you can say why read_table will work and read_delim will not.\nWhat is it about the experimental design that makes a one-way analysis of variance plausible for data like this?\nWhat is wrong with the current format of the data as far as doing a one-way ANOVA analysis is concerned? (This is related to the idea of whether or not the data are “tidy”.)\n“Tidy” the data to produce a data frame suitable for your analysis.\nGo ahead and run your one-way ANOVA (and Tukey if necessary). Assume for this that the pain relief hours in each group are sufficiently close to normally distributed with sufficiently equal spreads.\nWhat recommendation would you make about the best drug or drugs? Explain briefly."
  },
  {
    "objectID": "tidying-data.html#location-species-and-disease-in-plants",
    "href": "tidying-data.html#location-species-and-disease-in-plants",
    "title": "15  Tidying data",
    "section": "15.5 Location, species and disease in plants",
    "text": "15.5 Location, species and disease in plants\nThe table below is a “contingency table”, showing frequencies of diseased and undiseased plants of two different species in two different locations:\n\nSpecies     Disease present         Disease absent\nLocation X Location Y  Location X Location Y\nA            44         12          38        10\nB            28         22          20        18\n\nThe data were saved as link. In that file, the columns are coded by two letters: a p or an a to denote presence or absence of disease, and an x or a y to denote location X or Y. The data are separated by multiple spaces and aligned with the variable names.\n\nRead in and display the data.\nExplain briefly how these data are not “tidy”.\nUse a suitable tidyr tool to get all the things that are the same into a single column. (You’ll need to make up a temporary name for the other new column that you create.) Show your result.\nExplain briefly how the data frame you just created is still not “tidy” yet.\nUse one more tidyr tool to make these data tidy, and show your result.\nLet’s see if we can re-construct the original contingency table (or something equivalent to it). Use the function xtabs. This requires first a model formula with the frequency variable on the left of the squiggle, and the other variables separated by plus signs on the right. Second it requires a data frame, with data=. Feed your data frame from the previous part into xtabs. Save the result in a variable and display the result.\nTake the output from the last part and feed it into the function ftable. How has the output been changed? Which do you like better? Explain briefly."
  },
  {
    "objectID": "tidying-data.html#mating-songs-in-crickets",
    "href": "tidying-data.html#mating-songs-in-crickets",
    "title": "15  Tidying data",
    "section": "15.6 Mating songs in crickets",
    "text": "15.6 Mating songs in crickets\nMale tree crickets produce “mating songs” by rubbing their wings together to produce a chirping sound. It is hypothesized that female tree crickets identify males of the correct species by how fast (in chirps per second) the male’s mating song is. This is called the “pulse rate”. Some data for two species of crickets are in link. The columns, which are unlabelled, are temperature and pulse rate (respectively) for Oecanthus exclamationis (first two columns) and Oecanthus niveus (third and fourth columns). The columns are separated by tabs. There are some missing values in the first two columns because fewer exclamationis crickets than niveus crickets were measured. The research question is whether males of the different species have different average pulse rates. It is also of interest to see whether temperature has an effect, and if so, what. Before we get to that, however, we have some data organization to do.\n\nRead in the data, allowing for the fact that you have no column names. You’ll see that the columns have names X1 through X4. This is OK.\nTidy these untidy data, going as directly as you can to something tidy. (Some later parts show you how it used to be done.) Begin by: (i) adding a column of row numbers, (ii) rename-ing the columns to species name, an underscore, and the variable contents (keeping pulserate as one word), and then use pivot_longer. Note that the column names encode two things.\nIf you found (b) a bit much to take in, the rest of the way we take a rather more leisurely approach towards the tidying.\n\nThese data are rather far from being tidy. There need to be three variables, temperature, pulse rate and species, and there are \\(14+17=31\\) observations altogether. This one is tricky in that there are temperature and pulse rate for each of two levels of a factor, so I’ll suggest combining the temperature and chirp rate together into one thing for each species, then pivoting them longer (“combining”), then pivoting them wider again (“splitting”). Create new columns, named for each species, that contain the temperature and pulse rate for that species in that order, united together. For the rest of this question, start from the data frame you read in, and build a pipe, one or two steps at a time, to save creating a lot of temporary data frames.\n\nThe two columns exclamationis and niveus that you just created are both temperature-pulse rate combos, but for different species. Collect them together into one column, labelled by species. (This is a straight tidyr pivot_longer, even though the columns contain something odd-looking.)\nNow split up the temperature-pulse combos at the underscore, into two separate columns. This is separate. When specifying what to separate by, you can use a number (“split after this many characters”) or a piece of text, in quotes (“when you see this text, split at it”).\nAlmost there. Temperature and pulse rate are still text (because unite turned them into text), but they should be numbers. Create new variables that are numerical versions of temperature and pulse rate (using as.numeric). Check that you have no extraneous variables (and, if necessary, get rid of the ones you don’t want). (Species is also text and really ought to be a factor, but having it as text doesn’t seem to cause any problems.) You can, if you like, use parse_number instead of as.numeric. They should both work. The distinction I prefer to make is that parse_number is good for text with a number in it (that we want to pull the number out of), while as.numeric is for turning something that looks like a number but isn’t one into a genuine number.2"
  },
  {
    "objectID": "tidying-data.html#number-1-songs",
    "href": "tidying-data.html#number-1-songs",
    "title": "15  Tidying data",
    "section": "15.7 Number 1 songs",
    "text": "15.7 Number 1 songs\nThe data file link contains a lot of information about songs popular in 2000. This dataset is untidy. Our ultimate aim is to answer “which song occupied the #1 position for the largest number of weeks?”. To do that, we will build a pipe that starts from the data frame read in from the URL above, and finishes with an answer to the question. I will take you through this step by step. Each part will involve adding something to the pipe you built previously (possibly after removing a line or two that you used to display the previous result).\n\nRead the data and display what you have.\nThe columns x1st.week through x76th.week contain the rank of each song in the Billboard chart in that week, with week 1 being the first week that the song appeared in the chart. Convert all these columns into two: an indication of week, called week, and of rank, called rank. Most songs appeared in the Billboard chart for a lot less than 76 weeks, so there are missing values, which you want to remove. (I say “indication of week” since this will probably be text at the moment). Display your new data frame. Do you have fewer columns? Why do you have a lot more rows? Explain briefly.\nBoth your week and rank columns are (probably) text. Create new columns that contain just the numeric values, and display just your new columns, again adding onto the end of your pipe. If it so happens that rank is already a number, leave it as it is.\nThe meaning of your week-number column is that it refers to the number of weeks after the song first appeared in the Billboard chart. That is, if a song’s first appearance (in date.entered) is July 24, then week 1 is July 24, week 2 is July 31, week 3 is August 7, and so on. Create a column current by adding the appropriate number of days, based on your week number, to date.entered. Display date.entered, your week number, and current to show that you have calculated the right thing. Note that you can add a number of days onto a date and you will get another date.\nReaching the #1 rank on the Billboard chart is one of the highest accolades in the popular music world. List all the songs that reached rank 1. For these songs, list the artist (as given in the data set), the song title, and the date(s) for which the song was ranked number 1. Arrange the songs in date order of being ranked #1. Display all the songs (I found 55 of them).\nUse R to find out which song held the #1 rank for the largest number of weeks. For this, you can assume that the song titles are all unique (if it’s the same song title, it’s the same song), but the artists might not be (for example, Madonna might have had two different songs reach the #1 rank). The information you need is in the output you obtained for the previous part, so it’s a matter of adding some code to the end of that. The last mark was for displaying only the song that was ranked #1 for the largest number of weeks, or for otherwise making it easy to see which song it was."
  },
  {
    "objectID": "tidying-data.html#bikes-on-college",
    "href": "tidying-data.html#bikes-on-college",
    "title": "15  Tidying data",
    "section": "15.8 Bikes on College",
    "text": "15.8 Bikes on College\nThe City of Toronto collects all kinds of data on aspects of life in the city. See link. One collection of data is records of the number of cyclists on certain downtown streets. The data in link are a record of the cyclists on College Street on the block west from Huron to Spadina on September 24, 2010. In the spreadsheet, each row relates to one cyclist. The first column is the time the cyclist was observed (to the nearest 15 minutes). After that, there are four pairs of columns. The observer filled in (exactly) one X in each pair of columns, according to whether (i) the cyclist was male or female, (ii) was or was not wearing a helmet, (iii) was or was not carrying a passenger on the bike, (iv) was or was not riding on the sidewalk. We want to create a tidy data frame that has the time in each row, and has columns containing appropriate values, often TRUE or FALSE, for each of the four variables measured.\nI will lead you through the process, which will involve developing a (long) pipe, one step at a time.\n\nTake a look at the spreadsheet (using Excel or similar: this may open when you click the link). Are there any obvious header rows? Is there any extra material before the data start? Explain briefly.\nRead the data into an R data frame. Read without headers, and instruct R how many lines to skip over using skip= and a suitable number. When this is working, display the first few lines of your data frame. Note that your columns have names X1 through X9.\nWhat do you notice about the times in your first column? What do you think those “missing” times should be?\nFind something from the tidyverse that will fill3 in those missing values with the right thing. Start a pipe from the data frame you read in, that updates the appropriate column with the filled-in times.\nR’s ifelse function works like =IF in Excel. You use it to create values for a new variable, for example in a mutate. The first input to it is a logical condition (something that is either true or false); the second is the value your new variable should take if the condition is true, and the third is the value of your new variable if the condition is false. Create a new column gender in your data frame that is “male” or “female” depending on the value of your X2 column, using mutate. (You can assume that exactly one of the second and third columns has an X in it.) Add your code to the end of your pipe and display (the first 10 rows of) the result.\nCreate variables helmet, passenger and sidewalk in your data frame that are TRUE if the “Yes” column contains X and FALSE otherwise. This will use mutate again, but you don’t need ifelse: just set the variable equal to the appropriate logical condition. As before, the best way to create these variables is to test the appropriate things for missingness. Note that you can create as many new variables as you like in one mutate. Show the first few lines of your new data frame. (Add your code onto the end of the pipe you made above.)\nFinally (for the data manipulation), get rid of all the original columns, keeping only the new ones that you created. Save the results in a data frame and display its first few rows.\nThe next few parts are a quick-fire analysis of the data set. They can all be solved using count. How many male and how many female cyclists were observed in total?\nHow many male and female cyclists were not wearing helmets?\nHow many cyclists were riding on the sidewalk and carrying a passenger?\nWhat was the busiest 15-minute period of the day, and how many cyclists were there then?"
  },
  {
    "objectID": "tidying-data.html#feeling-the-heat",
    "href": "tidying-data.html#feeling-the-heat",
    "title": "15  Tidying data",
    "section": "15.9 Feeling the heat",
    "text": "15.9 Feeling the heat\nIn summer, the city of Toronto issues Heat Alerts for “high heat or humidity that is expected to last two or more days”. The precise definitions are shown at link. During a heat alert, the city opens Cooling Centres and may extend the hours of operation of city swimming pools, among other things. All the heat alert days from 2001 to 2016 are listed at link.\nThe word “warning” is sometimes used in place of “alert” in these data. They mean the same thing.4\n\nRead the data into R, and display the data frame. Note that there are four columns:\n\n\na numerical id (numbered upwards from the first Heat Alert in 2001; some of the numbers are missing)\nthe date of the heat alert, in year-month-day format with 4-digit years.\na text code for the type of heat alert\ntext describing the kind of heat alert. This can be quite long.\n\n\nIn your data frame, are the dates stored as genuine dates or as text? How can you tell?\nWhich different heat alert codes do you have, and how many of each?\nUse the text in your dataset (or look back at the original data file) to describe briefly in your own words what the various codes represent.\nHow many (regular and extended) heat alert events are there altogether? A heat alert event is a stretch of consecutive days, on all of which there is a heat alert or extended heat alert. Hints: (i) you can answer this from output you already have; (ii) how can you tell when a heat alert event starts?\nWe are going to investigate how many heat alert days there were in each year. To do that, we have to extract the year from each of our dates.\nCount the number of heat alert days for each year, by tabulating the year variable. Looking at this table, would you say that there have been more heat alert days in recent years? Explain (very) briefly."
  },
  {
    "objectID": "tidying-data.html#isoflavones",
    "href": "tidying-data.html#isoflavones",
    "title": "15  Tidying data",
    "section": "15.10 Isoflavones",
    "text": "15.10 Isoflavones\nThe plant called kudzu was imported to the US South from Japan. It is rich in isoflavones, which are believed to be beneficial for bones. In a study, rats were randomly assigned to one of three diets: one with a low dose of isoflavones from kudzu, one with a high dose, and a control diet with no extra isoflavones. At the end of the study, each rat’s bone density was measured, in milligrams per square centimetre. The data as recorded are shown in http://ritsokiguess.site/isoflavones.txt.5 There are 15 observations for each treatment, and hence 45 altogether.\nHere are some code ideas you might need to use later, all part of the tidyverse. You may need to find out how they work.\n\ncol_names (in the read_ functions)\nconvert (in various tidyverse functions)\nfill\nna_if\nrename\nseparate_rows\nskip (in the read_ functions)\nvalues_drop_na (in the pivot_ functions)\n\nIf you use any of these, cite the webpage(s) or other source(s) where you learned about them.\n\nTake a look at the data file. Describe briefly what you see.\nRead in the data, using read_table, and get it into a tidy form, suitable for making a graph. This means finishing with (at least) a column of treatments with a suitable name (the treatments will be text) and a column of bone density values (numbers), one for each rat. You can have other columns as well; there is no obligation to get rid of them. Describe your process clearly enough that someone new to this data set would be able to understand what you have done and reproduce it on another similar dataset. Before you begin, think about whether or not you want to keep the column headers that are in the data file or not. (It can be done either way, but one way is easier than the other.)\nThe statistician on this study is thinking about running an ordinary analysis of variance to compare the bone mineral density for the different treatments. Obtain a plot from your tidy dataframe that will help her decide whether that is a good idea.\nBased on your graph, and any additional graphs you wish to draw, what analysis would you recommend for this dataset? Explain briefly. (Don’t do the analysis.)"
  },
  {
    "objectID": "tidying-data.html#jockos-garage",
    "href": "tidying-data.html#jockos-garage",
    "title": "15  Tidying data",
    "section": "15.11 Jocko’s Garage",
    "text": "15.11 Jocko’s Garage\nInsurance adjusters are concerned that Jocko’s Garage is giving estimates for repairing car damage that are too high. To see whether this is indeed the case, ten cars that had been in collisions were taken to both Jocko’s Garage and another garage, and the two estimates for repair were recorded. The data as recorded are here.\n\nTake a look at the data file (eg. by using your web browser). How are the data laid out? Do there appear to be column headers?\nRead in and display the data file, bearing in mind what you just concluded about it. What names did the columns acquire?\nMake this data set tidy. That is, you need to end up with columns containing the repair cost estimates at each of the two garages and also identifying the cars, with each observation on one row. Describe your thought process. (It needs to be possible for the reader to follow your description and understand why it works.) Save your tidy dataframe.\nMake a suitable graph to assess the comparison of interest, and say briefly what your graph tells you.\nCarry out a test to make an appropriate comparison of the mean estimates. What do you conclude, in the context of the data?"
  },
  {
    "objectID": "tidying-data.html#tidying-electricity-consumption",
    "href": "tidying-data.html#tidying-electricity-consumption",
    "title": "15  Tidying data",
    "section": "15.12 Tidying electricity consumption",
    "text": "15.12 Tidying electricity consumption\nHow does the consumption of electricity depend on temperature? To find out, a short-term study was carried out by a utility company based in a certain area. For a period of two years, the average monthly temperature was recorded (in degrees Fahrenheit), the mean daily demand for electricity per household (in kilowatt hours), and the cost per kilowatt hour of electricity for that year (8 cents for the first year and 10 cents for the second, which it will be easiest to treat as categorical).\nThe data were laid out in an odd way, as shown in http://ritsokiguess.site/datafiles/utils.txt, in aligned columns: the twelve months of temperature were laid out on two lines for the first year, then the twelve months of consumption for the first year on the next two lines, and then four more lines for the second year laid out the same way. Thus the temperature of 31 in the first line goes with the consumption of 55 in the third line, and the last measurements for that year are the 78 at the end of the second line (temperature) and 73 at the end of the fourth line (consumption). Lines 5 through 8 of the data file are the same thing for the second year (when electricity was more expensive).\nThe data seem to have been laid out in order of temperature, rather than in order of months, which I would have thought would make more sense. But this is what we have.\n\nRead in and display the data file, bearing in mind that it has no column names.\nArrange these data tidily, so that there is a column of price (per kilowatt hour), a column of temperatures, and a column of consumptions. Describe your process, including why you got list-columns (if you did) and what you did about them (if necessary).\nMake a suitable graph of temperature, consumption and price in your tidy dataframe. Add smooth trends if appropriate. If you were unable to get the data tidy, use my tidy version here. (If you need the other file, right-click on “here” and Copy Link Address.)\nWhat patterns or trends do you see on your graph? Do they make practical sense? There are two things I would like you to comment on."
  },
  {
    "objectID": "tidying-data.html#tidy-blood-pressure",
    "href": "tidying-data.html#tidy-blood-pressure",
    "title": "15  Tidying data",
    "section": "15.13 Tidy blood pressure",
    "text": "15.13 Tidy blood pressure\nGoing to the dentist is scary for a lot of people. One way in which this might show up is that people might have higher blood pressure on average before their dentist’s appointment than an hour after the appointment is done. Ten randomly-chosen individuals have their (systolic6) blood pressure measured while they are in a dentist’s waiting room, and then again one hour after their appointment is finished.\nYou might have seen a tidy version of this data set before.\nThe data as I originally received it is in http://ritsokiguess.site/datafiles/blood_pressure2.csv.\n\nRead in and display the data as originally received.\nDescribe briefly how the data you read in is not tidy, bearing in mind how the data were collected and how they would be analysed.\nProduce a tidy dataframe from the one you read in from the file. (How many rows should you have?)\nWhat kind of test might you run on these data? Explain briefly.\nDraw a suitable graph of these data.\n\nMy solutions follow:"
  },
  {
    "objectID": "tidying-data.html#baseball-and-softball-spaghetti-1",
    "href": "tidying-data.html#baseball-and-softball-spaghetti-1",
    "title": "15  Tidying data",
    "section": "15.14 Baseball and softball spaghetti",
    "text": "15.14 Baseball and softball spaghetti\nOn a previous assignment, we found that students could throw a baseball further than they could throw a softball. In this question, we will make a graph called a “spaghetti plot” to illustrate this graphically. (The issue previously was that the data were matched pairs: the same students threw both balls.)\nThis seems to work most naturally by building a pipe, a line or two at a time. See if you can do it that way. (If you can’t make it work, use lots of temporary data frames, one to hold the result of each part.)\n\nRead in the data again from link. The variables had no names, so supply some, as you did before.\n\nSolution\nLiteral copy and paste:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/throw.txt\"\nthrows &lt;- read_delim(myurl, \" \", col_names = c(\"student\", \"baseball\", \"softball\"))\n\nRows: 24 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): student, baseball, softball\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nthrows\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nCreate a new column that is the students turned into a factor, adding it to your data frame. The reason for this will become clear later.\n\nSolution\nFeed student into factor, creating a new column with mutate:\n\nthrows %&gt;% mutate(fs = factor(student))\n\n\n\n  \n\n\n\nThis doesn’t look any different from the original student numbers, but note the variable type at the top of the column.\n\\(\\blacksquare\\)\n\nCollect together all the throwing distances into one column, making a second column that says which ball was thrown.\n\nSolution\nUse pivot_longer. It goes like this:\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\")\n\n\n\n  \n\n\n\nThe names_to is the name of a new categorical column whose values will be what is currently column names, and the values_to names a new quantitative (usually) column that will hold the values in those columns that you are making longer.\nIf you want to show off a little, you can use a select-helper, noting that the columns you want to make longer all end in “ball”:\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(ends_with(\"ball\"), names_to=\"ball\", values_to=\"distance\")\n\n\n\n  \n\n\n\nThe same result. Use whichever you like.\n\\(\\blacksquare\\)\n\nUsing your new data frame, make a “scatterplot” of throwing distance against type of ball.\n\nSolution\nThe obvious thing. No data frame in the ggplot because it’s the data frame that came out of the previous part of the pipeline (that doesn’t have a name):\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %&gt;% \n  ggplot(aes(x = ball, y = distance)) + geom_point()\n\n\n\n\nThis is an odd type of scatterplot because the \\(x\\)-axis is actually a categorical variable. It’s really what would be called something like a dotplot. We’ll be using this as raw material for the plot we actually want.\nWhat this plot is missing is an indication of which student threw which ball. As it stands now, it could be an inferior version of a boxplot of distances thrown for each ball (which would imply that they are two independent sets of students, something that is not true).\n\\(\\blacksquare\\)\n\nAdd two things to your plot: something that will distinguish the students by colour (this works best if the thing distinguished by colour is a factor),7 and something that will join the two points for the same student by a line.\n\nSolution\nA colour and a group in the aes, and a geom_line:\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %&gt;% \n  ggplot(aes(x = ball, y = distance, group = fs, colour = fs)) +\n  geom_point() + geom_line()\n\n\n\n\nYou can see what happens if you use the student as a number:\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %&gt;% \n  ggplot(aes(x = ball, y = distance, group = student, colour = student)) +\n  geom_point() + geom_line()\n\n\n\n\nNow the student numbers are distinguished as a shade of blue (on an implied continuous scale: even a nonsensical fractional student number like 17.5 would be a shade of blue). This is not actually so bad here, because all we are trying to do is to distinguish the students sufficiently from each other so that we can see where the spaghetti strands go. But I like the multi-coloured one better.\n\\(\\blacksquare\\)\n\nThe legend is not very informative. Remove it from the plot, using guides.\n\nSolution\nYou may not have seen this before. Here’s what to do: Find what’s at the top of the legend that you want to remove. Here that is fs. Find where fs appears in your aes. It actually appears in two places: in group and colour. I think the legend we want to get rid of is actually the colour one, so we do this:\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %&gt;% \n  ggplot(aes(x = ball, y = distance, group = fs, colour = fs)) +\n  geom_point() + geom_line() +\n  guides(colour = \"none\")\n\n\n\n\nThat seems to have done it.\n\\(\\blacksquare\\)\n\nWhat do you see on the final spaghetti plot? What does that tell you about the relative distances a student can throw a baseball vs. a softball? Explain briefly, blah blah blah.\n\nSolution\nMost of the spaghetti strands go downhill from baseball to softball, or at least very few of them go uphill. That tells us that most students can throw a baseball further than a softball. That was the same impression that the matched-pairs \\(t\\)-test gave us. But the spaghetti plot tells us something else. If you look carefully, you see that most of the big drops are for students who could throw a baseball a long way. These students also threw a softball further than the other students, but not by as much. Most of the spaghetti strands in the bottom half of the plot go more or less straight across. This indicates that students who cannot throw a baseball very far will throw a softball about the same distance as they threw the baseball. There is an argument you could make here that the difference between distances thrown is a proportional one, something like “a student typically throws a baseball 20% further than a softball”. That could be assessed by comparing not the distances themselves, but the logs of the distances: in other words, making a log transformation of all the distances. (Distances have a lower limit of zero, so you might expect observed distances to be skewed to the right, which is another argument for making some kind of transformation.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#ethanol-and-sleep-time-in-rats-1",
    "href": "tidying-data.html#ethanol-and-sleep-time-in-rats-1",
    "title": "15  Tidying data",
    "section": "15.15 Ethanol and sleep time in rats",
    "text": "15.15 Ethanol and sleep time in rats\nA biologist wished to study the effects of ethanol on sleep time in rats. A sample of 20 rats (all the same age) was selected, and each rat was given an injection having a particular concentration (0, 1, 2 or 4 grams per kilogram of body weight) of ethanol. These are labelled e0, e1, e2, e4. The “0” treatment was a control group. The rapid eye movement (REM) sleep time was then recorded for each rat. The data are in link.\n\nRead the data in from the file. Check that you have four rows of observations and five columns of sleep times.\n\nSolution\nSeparated by single spaces:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ratsleep.txt\"\nsleep1 &lt;- read_delim(my_url, \" \")\n\nRows: 4 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): treatment\ndbl (5): obs1, obs2, obs3, obs4, obs5\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsleep1\n\n\n\n  \n\n\n\nThere are six columns, but one of them labels the groups, and there are correctly five columns of sleep times.\nI used a “temporary” name for my data frame, because I’m going to be doing some processing on it in a minute, and I want to reserve the name sleep for my processed data frame.\n\\(\\blacksquare\\)\n\nUnfortunately, the data are in the wrong format. All the sleep times for each treatment group are on one row, and we should have one column containing all the sleep times, and the corresponding row should show which treatment group that sleep time came from. Transform this data frame into one that you could use for modelling or making graphs.\n\nSolution\nWe will want one column of sleep times, with an additional categorical column saying what observation each sleep time was within its group (or, you might say, we don’t really care about that much, but that’s what we are going to get).\nThe columns obs1 through obs5 are different in that they are different observation numbers (“replicates”, in the jargon). I’ll call that rep. What makes them the same is that they are all sleep times. Columns obs1 through obs5 are the ones we want to combine, thus. Here is where I use the name sleep: I save the result of the pivot_longer into a data frame sleep. Note that I also used the brackets-around-the-outside to display what I had, so that I didn’t have to do a separate display. This is a handy way of saving and displaying in one shot:\n\n(sleep1 %&gt;% \n  pivot_longer(-treatment, names_to=\"rep\", values_to=\"sleeptime\") -&gt; sleep)\n\n\n\n  \n\n\n\nTypically in this kind of work, you have a lot of columns that need to be made longer, and a much smaller number of columns that need to be repeated as necessary. You can either specify all the columns to make longer, or you can specify “not” the other columns. Above, my first input to pivot_longer was “everything but treatment”, but you could also do it like this:\n\nsleep1 %&gt;% \n  pivot_longer(obs1:obs5, names_to=\"rep\", values_to=\"sleeptime\") \n\n\n\n  \n\n\n\nor like this:\n\nsleep1 %&gt;% \n  pivot_longer(starts_with(\"obs\"), names_to=\"rep\", values_to=\"sleeptime\") \n\n\n\n  \n\n\n\nThis one was a little unusual in that usually with these you have the treatments in the columns and the replicates in the rows. It doesn’t matter, though: pivot_longer handles both cases.\nWe have 20 rows of 3 columns. I got all the rows, but you will probably get an output with ten rows as usual, and will need to click Next to see the last ten rows. The initial display will say how many rows (20) and columns (3) you have.\nThe column rep is not very interesting: it just says which observation each one was within its group.8 The interesting things are treatment and sleeptime, which are the two variables we’ll need for our analysis of variance.\n\\(\\blacksquare\\)\n\nUsing your new data frame, make side-by-side boxplots of sleep time by treatment group.\n\nSolution\n\nggplot(sleep, aes(x = treatment, y = sleeptime)) + geom_boxplot()\n\n\n\n\n\\(\\blacksquare\\)\n\nIn your boxplots, how does the median sleep time appear to depend on treatment group?\n\nSolution\nIt appears to decrease as the dose of ethanol increases, and pretty substantially so (in that the differences ought to be significant, but that’s coming up).\n\\(\\blacksquare\\)\n\nThere is an assumption about spread that the analysis of variance needs in order to be reliable. Do your boxplots indicate that this assumption is satisfied for these data, bearing in mind that you have only five observations per group?\n\nSolution\nThe assumption is that the population SDs of each group are all equal. Now, the boxplots show IQRs, which are kind of a surrogate for SD, and because we only have five observations per group to base the IQRs on, the sample IQRs might vary a bit. So we should look at the heights of the boxes on the boxplot, and see whether they are grossly unequal. They appear to be to be of very similar heights, all things considered, so I am happy.\nIf you want the SDs themselves:\n\nsleep %&gt;%\n  group_by(treatment) %&gt;%\n  summarize(stddev = sd(sleeptime))\n\n\n\n  \n\n\n\nThose are very similar, given only 5 observations per group. No problems here.\n\\(\\blacksquare\\)\n\nRun an analysis of variance to see whether sleep time differs significantly among treatment groups. What do you conclude?\n\nSolution\nI use aov here, because I might be following up with Tukey in a minute:\n\nsleep.1 &lt;- aov(sleeptime ~ treatment, data = sleep)\nsummary(sleep.1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntreatment    3   5882    1961   21.09 8.32e-06 ***\nResiduals   16   1487      93                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis is a very small P-value, so my conclusion is that the mean sleep times are not all the same for the treatment groups. Further than that I am not entitled to say (yet).\nThe technique here is to save the output from aov in something, look at that (via summary), and then that same something gets fed into TukeyHSD later.\n\\(\\blacksquare\\)\n\nWould it be a good idea to run Tukey’s method here? Explain briefly why or why not, and if you think it would be a good idea, run it.\n\nSolution\nTukey’s method is useful when (i) we have run an analysis of variance and got a significant result and (ii) when we want to know which groups differ significantly from which. Both (i) and (ii) are true here. So:\n\nTukeyHSD(sleep.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = sleeptime ~ treatment, data = sleep)\n\n$treatment\n        diff       lwr         upr     p adj\ne1-e0 -17.74 -35.18636  -0.2936428 0.0455781\ne2-e0 -31.36 -48.80636 -13.9136428 0.0005142\ne4-e0 -46.52 -63.96636 -29.0736428 0.0000056\ne2-e1 -13.62 -31.06636   3.8263572 0.1563545\ne4-e1 -28.78 -46.22636 -11.3336428 0.0011925\ne4-e2 -15.16 -32.60636   2.2863572 0.1005398\n\n\n\\(\\blacksquare\\)\n\nWhat do you conclude from Tukey’s method? (This is liable to be a bit complicated.) Is there a treatment that is clearly best, in terms of the sleep time being largest?\n\nSolution\nAll the differences are significant except treatment e2 vs. e1 and e4. All the differences involving the control group e0 are significant, and if you look back at the boxplots in (c), you’ll see that the control group e0 had the highest mean sleep time. So the control group is best (from this point of view), or another way of saying it is that any dose of ethanol is significantly reducing mean sleep time. The other comparisons are a bit confusing, because the 1-4 difference is significant, but neither of the differences involving 2 are. That is, 1 is better than 4, but 2 is not significantly worse than 1 nor better than 4. This seems like it should be a logical impossibility, but the story is that we don’t have enough data to decide where 2 fits relative to 1 or 4. If we had 10 or 20 observations per group, we might be able to conclude that 2 is in between 1 and 4 as the boxplots suggest.\nExtra: I didn’t ask about normality here, but like the equal-spreads assumption I’d say there’s nothing controversial about it with these data. With normality good and equal spreads good, aov plus Tukey is the analysis of choice.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#growth-of-tomatoes-1",
    "href": "tidying-data.html#growth-of-tomatoes-1",
    "title": "15  Tidying data",
    "section": "15.16 Growth of tomatoes",
    "text": "15.16 Growth of tomatoes\nA biology graduate student exposed each of 32 tomato plants to one of four different colours of light (8 plants to each colour). The growth rate of each plant, in millimetres per week, was recorded. The data are in link.\n\nRead the data into R and confirm that you have 8 rows and 5 columns of data.\n\nSolution\nThis kind of thing:\n\nmy_url=\"http://ritsokiguess.site/datafiles/tomatoes.txt\"\ntoms1=read_delim(my_url,\" \")\n\nRows: 8 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (5): plant, blue, red, yellow, green\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntoms1\n\n\n\n  \n\n\n\nI do indeed have 8 rows and 5 columns.\nWith only 8 rows, listing the data like this is good.\n\\(\\blacksquare\\)\n\nRe-arrange the data so that you have one column containing all the growth rates, and another column saying which colour light each plant was exposed to. (The aim here is to produce something suitable for feeding into aov later.)\n\nSolution\nThis is a job for pivot_longer:\n\ntoms1 %&gt;% \n   pivot_longer(-plant, names_to=\"colour\", values_to=\"growthrate\") -&gt; toms2\ntoms2\n\n\n\n  \n\n\n\nI chose to specify “everything but plant number”, since there are several colour columns with different names.\nSince the column plant was never mentioned, this gets repeated as necessary, so now it denotes “plant within colour group”, which in this case is not very useful. (Where you have matched pairs, or repeated measures in general, you do want to keep track of which individual is which. But this is not repeated measures because plant number 1 in the blue group and plant number 1 in the red group are different plants.)\nThere were 8 rows originally and 4 different colours, so there should be, and are, \\(8 \\times 4=32\\) rows in the made-longer data set.\n\\(\\blacksquare\\)\n\nSave the data in the new format to a text file. This is most easily done using write_csv, which is the opposite of read_csv. It requires two things: a data frame, and the name of a file to save in, which should have a .csv extension.\n\nSolution\nThe code is easy enough:\n\nwrite_csv(toms2,\"tomatoes2.csv\")\n\nIf no error, it worked. That’s all you need.\nTo verify (for my satisfaction) that it was saved correctly:\n\ncat tomatoes2.csv \n\nplant,colour,growthrate\n1,blue,5.34\n1,red,13.67\n1,yellow,4.61\n1,green,2.72\n2,blue,7.45\n2,red,13.04\n2,yellow,6.63\n2,green,1.08\n3,blue,7.15\n3,red,10.16\n3,yellow,5.29\n3,green,3.97\n4,blue,5.53\n4,red,13.12\n4,yellow,5.29\n4,green,2.66\n5,blue,6.34\n5,red,11.06\n5,yellow,4.76\n5,green,3.69\n6,blue,7.16\n6,red,11.43\n6,yellow,5.57\n6,green,1.96\n7,blue,7.77\n7,red,13.98\n7,yellow,6.57\n7,green,3.38\n8,blue,5.09\n8,red,13.49\n8,yellow,5.25\n8,green,1.87\n\n\nOn my system, that will list the contents of the file. Or you can just open it in R Studio (if you saved it the way I did, it’ll be in the same folder, and you can find it in the Files pane.)\n\\(\\blacksquare\\)\n\nMake a suitable boxplot, and use it to assess the assumptions for ANOVA. What do you conclude? Explain briefly.\n\nSolution\nNothing terribly surprising here. My data frame is called toms2, for some reason:\n\nggplot(toms2,aes(x=colour, y=growthrate))+geom_boxplot()\n\n\n\n\nThere are no outliers, but there is a little skewness (compare the whiskers, not the placement of the median within the box, because what matters with skewness is the tails, not the middle of the distribution; it’s problems in the tails that make the mean unsuitable as a measure of centre). The Red group looks the most skewed. Also, the Yellow group has smaller spread than the others (we assume that the population variances within each group are equal). The thing to bear in mind here, though, is that there are only eight observations per group, so the distributions could appear to have unequal variances or some non-normality by chance.\nMy take is that these data, all things considered, are just about OK for ANOVA. Another option would be to do Welch’s ANOVA as well and compare with the regular ANOVA: if they give more or less the same P-value, that’s a sign that I didn’t need to worry.\nExtra: some people like to run a formal test on the variances to test them for equality. My favourite (for reasons explained elsewhere) is the Levene test, if you insist on going this way. It lives in package car, and does not take a data=, so you need to do the with thing:\n\nlibrary(car)\nwith(toms2,leveneTest(growthrate,colour))\n\nWarning in leveneTest.default(growthrate, colour): colour coerced to factor.\n\n\n\n\n  \n\n\n\nThe warning is because colour was actually text, but the test did the right thing by turning it into a factor, so that’s OK.\nThere is no way we can reject equal variances in the four groups. The \\(F\\)-statistic is less than 1, in fact, which says that if the four groups have the same population variances, the sample variances will be more different than the ones we observed on average, and so there is no way that these sample variances indicate different population variances. (This is because of 8 observations only per group; if there had been 80 observations per group, it would have been a different story.) Decide for yourself whether you’re surprised by this.\nWith that in mind, I think the regular ANOVA will be perfectly good, and we would expect that and the Welch ANOVA to give very similar results.\n\\(\\blacksquare\\)\n\nRun (regular) ANOVA on these data. What do you conclude? (Optional extra: if you think that some other variant of ANOVA would be better, run that as well and compare the results.)\n\nSolution\naov, bearing in mind that Tukey is likely to follow:\n\ntoms.1=aov(growthrate~colour,data=toms2)\nsummary(toms.1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncolour       3  410.5  136.82   118.2 5.28e-16 ***\nResiduals   28   32.4    1.16                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis is a tiny P-value, so the mean growth rate for the different colours is definitely not the same for all colours. Or, if you like, one or more of the colours has a different mean growth rate than the others.\nThis, remember, is as far as we go right now.\nExtra: if you thought that normality was OK but not equal spreads, then Welch ANOVA is the way to go:\n\ntoms.2=oneway.test(growthrate~colour,data=toms2)\ntoms.2\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  growthrate and colour\nF = 81.079, num df = 3.000, denom df = 15.227, p-value = 1.377e-09\n\n\nThe P-value is not quite as small as for the regular ANOVA, but it is still very small, and the conclusion is the same.\nIf you had doubts about the normality (that were sufficiently great, even given the small sample sizes), then go with Mood’s median test for multiple groups:\n\nlibrary(smmr)\nmedian_test(toms2,growthrate,colour)\n\n$table\n        above\ngroup    above below\n  blue       5     3\n  green      0     8\n  red        8     0\n  yellow     3     5\n\n$test\n       what        value\n1 statistic 1.700000e+01\n2        df 3.000000e+00\n3   P-value 7.067424e-04\n\n\nThe P-value is again extremely small (though not quite as small as for the other two tests, for the usual reason that Mood’s median test doesn’t use the data very efficiently: it doesn’t use how far above or below the overall median the data values are.)\nThe story here, as ever, is consistency: whatever you thought was wrong, looking at the boxplots, needs to guide the test you do:\n\nif you are not happy with normality, go with median_test from smmr (Mood’s median test).\nif you are happy with normality and equal variances, go with aov.\nif you are happy with normality but not equal variances, go with oneway.test (Welch ANOVA).\n\nSo the first thing to think about is normality, and if you are OK with normality, then think about equal spreads. Bear in mind that you need to be willing to tolerate a certain amount of non-normality and inequality in the spreads, given that your data are only samples from their populations. (Don’t expect perfection, in short.)\n\\(\\blacksquare\\)\n\nIf warranted, run a suitable follow-up. (If not warranted, explain briefly why not.)\n\nSolution\nWhichever flavour of ANOVA you ran (regular ANOVA, Welch ANOVA, Mood’s median test), you got the same conclusion for these data: that the average growth rates were not all the same for the four colours. That, as you’ll remember, is as far as you go. To find out which colours differ from which in terms of growth rate, you need to run some kind of multiple-comparisons follow-up, the right one for the analysis you did. Looking at the boxplots suggests that red is clearly best and green clearly worst, and it is possible that all the colours are significantly different from each other.) If you did regular ANOVA, Tukey is what you need:\n\nTukeyHSD(toms.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = growthrate ~ colour, data = toms2)\n\n$colour\n                diff       lwr        upr     p adj\ngreen-blue   -3.8125 -5.281129 -2.3438706 0.0000006\nred-blue      6.0150  4.546371  7.4836294 0.0000000\nyellow-blue  -0.9825 -2.451129  0.4861294 0.2825002\nred-green     9.8275  8.358871 11.2961294 0.0000000\nyellow-green  2.8300  1.361371  4.2986294 0.0000766\nyellow-red   -6.9975 -8.466129 -5.5288706 0.0000000\n\n\nAll of the differences are (strongly) significant, except for yellow and blue, the two with middling growth rates on the boxplot. Thus we would have no hesitation in saying that growth rate is biggest in red light and smallest in green light.\nIf you did Welch ANOVA, you need Games-Howell, which you have to get from one of the packages that offers it:\n\nlibrary(PMCMRplus)\ngamesHowellTest(growthrate~factor(colour),data=toms2)\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: growthrate by factor(colour)\n\n\n       blue    green   red    \ngreen  1.6e-05 -       -      \nred    1.5e-06 4.8e-09 -      \nyellow 0.18707 0.00011 5.8e-07\n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nThe conclusions are the same as for the Tukey: all the means are significantly different except for yellow and blue. Finally, if you did Mood’s median test, you need this one:\n\npairwise_median_test(toms2, growthrate, colour)\n\n\n\n  \n\n\n\nSame conclusions again. This is what I would have guessed; the conclusions from Tukey were so clear-cut that it really didn’t matter which way you went; you’d come to the same conclusion.\nThat said, what I am looking for from you is a sensible choice of analysis of variance (ANOVA, Welch’s ANOVA or Mood’s median test) for a good reason, followed by the right follow-up for the test you did. Even though the conclusions are all the same no matter what you do here, I want you to get used to following the right method, so that you will be able to do the right thing when it does matter.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#pain-relief-in-migraine-headaches-again-1",
    "href": "tidying-data.html#pain-relief-in-migraine-headaches-again-1",
    "title": "15  Tidying data",
    "section": "15.17 Pain relief in migraine headaches (again)",
    "text": "15.17 Pain relief in migraine headaches (again)\nThe data in link are from a study of pain relief in migraine headaches. Specifically, 27 subjects were randomly assigned to receive one of three pain relieving drugs, labelled A, B and C. Each subject reported the number of hours of pain relief they obtained (that is, the number of hours between taking the drug and the migraine symptoms returning). A higher value is therefore better. Can we make some recommendation about which drug is best for the population of migraine sufferers?\n\nRead in and display the data. Take a look at the data file first, and see if you can say why read_table will work and read_delim will not.\n\nSolution\nThe key is two things: the data values are lined up in columns, and there is more than one space between values. The second thing is why read_delim will not work. If you look carefully at the data file, you’ll see that the column names are above and aligned with the columns. read_table doesn’t actually need things to be lined up in columns; all it actually needs is for there to be one or more spaces between columns.\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/migraine.txt\"\nmigraine &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  DrugA = col_double(),\n  DrugB = col_double(),\n  DrugC = col_double()\n)\n\nmigraine\n\n\n\n  \n\n\n\nSuccess.\n\\(\\blacksquare\\)\n\nWhat is it about the experimental design that makes a one-way analysis of variance plausible for data like this?\n\nSolution\nEach experimental subject only tested one drug, so that we have 27 independent observations, nine from each drug. This is exactly the setup that a one-way ANOVA requires. Compare that to, for example, a situation where you had only 9 subjects, but they each tested all the drugs (so that each subject produced three measurements). That is like a three-measurement version of matched pairs, a so-called repeated-measures design, which requires its own kind of analysis.9\n\\(\\blacksquare\\)\n\nWhat is wrong with the current format of the data as far as doing a one-way ANOVA analysis is concerned? (This is related to the idea of whether or not the data are “tidy”.)\n\nSolution\nFor our analysis, we need one column of pain relief time and one column labelling the drug that the subject in question took. Or, if you prefer to think about what would make these data “tidy”: there are 27 subjects, so there ought to be 27 rows, and all three columns are measurements of pain relief, so they ought to be in one column.\n\\(\\blacksquare\\)\n\n“Tidy” the data to produce a data frame suitable for your analysis.\n\nSolution\nThis is pivot_longer. The column names are going to be stored in a column drug, and the corresponding values in a column called painrelief (use whatever names you like):\n\nmigraine %&gt;% \n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") -&gt; migraine2\n\nSince I was making all the columns longer, I used the select-helper everything() to do that. Using instead DrugA:DrugC or starts_with(\"Drug\") would also be good. Try them. starts_with is not case-sensitive, as far as I remember, so starts_with(\"drug\") will also work here.\nWe do indeed have a new data frame with 27 rows, one per observation, and 2 columns, one for each variable: the pain relief hours, plus a column identifying which drug that pain relief time came from. Exactly what aov needs.\nYou can probably devise a better name for your new data frame.\n\\(\\blacksquare\\)\n\nGo ahead and run your one-way ANOVA (and Tukey if necessary). Assume for this that the pain relief hours in each group are sufficiently close to normally distributed with sufficiently equal spreads.\n\nSolution\nMy last sentence absolves us from doing the boxplots that we would normally insist on doing.\n\npainrelief.1 &lt;- aov(painrelief ~ drug, data = migraine2)\nsummary(painrelief.1)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndrug         2  41.19   20.59   7.831 0.00241 **\nResiduals   24  63.11    2.63                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere are (strongly) significant differences among the drugs, so it is definitely worth firing up Tukey to figure out where the differences are:\n\nTukeyHSD(painrelief.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = painrelief ~ drug, data = migraine2)\n\n$drug\n                  diff        lwr      upr     p adj\nDrugB-DrugA  2.8888889  0.9798731 4.797905 0.0025509\nDrugC-DrugA  2.2222222  0.3132065 4.131238 0.0203671\nDrugC-DrugB -0.6666667 -2.5756824 1.242349 0.6626647\n\n\nBoth the differences involving drug A are significant, and because a high value of painrelief is better, in both cases drug A is worse than the other drugs. Drugs B and C are not significantly different from each other.\nExtra: we can also use the “pipe” to do this all in one go:\n\nmigraine %&gt;%\n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") %&gt;%\n  aov(painrelief ~ drug, data = .) %&gt;%\n  summary()\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndrug         2  41.19   20.59   7.831 0.00241 **\nResiduals   24  63.11    2.63                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nwith the same results as before. Notice that I never actually created a second data frame by name; it was created by pivot_longer and then immediately used as input to aov.10 I also used the data=. trick to use “the data frame that came out of the previous step” as my input to aov.\nRead the above like this: “take migraine, and then make everything longer, creating new columns drug and painrelief, and then do an ANOVA of painrelief by drug, and then summarize the results.”\nWhat is even more alarming is that I can feed the output from aov straight into TukeyHSD:\n\nmigraine %&gt;%\n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") %&gt;%\n  aov(painrelief ~ drug, data = .) %&gt;%\n  TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = painrelief ~ drug, data = .)\n\n$drug\n                  diff        lwr      upr     p adj\nDrugB-DrugA  2.8888889  0.9798731 4.797905 0.0025509\nDrugC-DrugA  2.2222222  0.3132065 4.131238 0.0203671\nDrugC-DrugB -0.6666667 -2.5756824 1.242349 0.6626647\n\n\nI wasn’t sure whether this would work, since the output from aov is an R list rather than a data frame, but the output from aov is sent into TukeyHSD whatever kind of thing it is.\nWhat I am missing here is to display the result of aov and use it as input to TukeyHSD. Of course, I had to discover that this could be solved, and indeed it can:\n\nmigraine %&gt;%\n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") %&gt;%\n  aov(painrelief ~ drug, data = .) %&gt;%\n  {\n    print(summary(.))\n    .\n  } %&gt;%\n  TukeyHSD()\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndrug         2  41.19   20.59   7.831 0.00241 **\nResiduals   24  63.11    2.63                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = painrelief ~ drug, data = .)\n\n$drug\n                  diff        lwr      upr     p adj\nDrugB-DrugA  2.8888889  0.9798731 4.797905 0.0025509\nDrugC-DrugA  2.2222222  0.3132065 4.131238 0.0203671\nDrugC-DrugB -0.6666667 -2.5756824 1.242349 0.6626647\n\n\nThe odd-looking second-last line of that again uses the . trick for “whatever came out of the previous step”. The thing inside the curly brackets is two commands one after the other; the first is to display the summary of that aov11 and the second is to just pass whatever came out of the previous line, the output from aov, on, unchanged, into TukeyHSD.\nIn the Unix/Linux world this is called tee, where you print something and pass it on to the next step. The name tee comes from a (real physical) pipe that plumbers would use to split water flow into two, which looks like a letter T.\n\\(\\blacksquare\\)\n\nWhat recommendation would you make about the best drug or drugs? Explain briefly.\n\nSolution\nDrug A is significantly the worst, so we eliminate that. But there is no significant difference between drugs B and C, so we have no reproducible reason for preferring one rather than the other. Thus, we recommend “either B or C”. If you weren’t sure which way around the drugs actually came out, then you should work out the mean pain relief score by drug:\n\nmigraine2 %&gt;%\n  group_by(drug) %&gt;%\n  summarize(m = mean(painrelief))\n\n\n\n  \n\n\n\nThese confirm that A is worst, and there is nothing much to choose between B and C. You should not recommend drug B over drug C on this evidence, just because its (sample) mean is higher. The point about significant differences is that they are supposed to stand up to replication: in another experiment, or in real-life experiences with these drugs, the mean pain relief score for drug A is expected to be worst, but between drugs B and C, sometimes the mean of B will come out higher and sometimes C’s mean will be higher, because there is no significant difference between them.12 Another way is to draw a boxplot of pain-relief scores:\n\nggplot(migraine2, aes(x = drug, y = painrelief)) + geom_boxplot()\n\n\n\n\nThe medians of drugs B and C are actually exactly the same. Because the pain relief values are all whole numbers (and there are only 9 in each group), you get that thing where enough of them are equal that the median and third quartiles are equal, actually for two of the three groups.\nDespite the weird distributions, I’m willing to call these groups sufficiently symmetric for the ANOVA to be OK, but I didn’t ask you to draw the boxplot, because I didn’t want to confuse the issue with this. The point of this question was to get the data tidy enough to do an analysis.\nAs I said, I didn’t want you to have to get into this, but if you are worried, you know what the remedy is — Mood’s median test. Don’t forget to use the right data frame:\n\nlibrary(smmr)\nmedian_test(migraine2, painrelief, drug)\n\n$table\n       above\ngroup   above below\n  DrugA     0     8\n  DrugB     5     2\n  DrugC     6     0\n\n$test\n       what        value\n1 statistic 1.527273e+01\n2        df 2.000000e+00\n3   P-value 4.825801e-04\n\n\nBecause the pain relief scores are integers, there are probably a lot of them equal to the overall median. There were 27 observations altogether, but Mood’s median test will discard any that are equal to this value. There must have been 9 observations in each group to start with, but if you look at each row of the table, there are only 8 observations listed for drug A, 7 for drug B and 6 for drug C, so there must have been 1, 2 and 3 (totalling 6) observations equal to the median that were discarded.\nThe P-value is a little bigger than came out of the \\(F\\)-test, but the conclusion is still that there are definitely differences among the drugs in terms of pain relief. The table at the top of the output again suggests that drug A is worse than the others, but to confirm that you’d have to do Mood’s median test on all three pairs of drugs, and then use Bonferroni to allow for your having done three tests:\n\npairwise_median_test(migraine2, painrelief, drug)\n\n\n\n  \n\n\n\nDrug A gives worse pain relief (fewer hours) than both drugs B and C, which are not significantly different from each hour. This is exactly what you would have guessed from the boxplot.\nI adjusted the P-values as per Bonferroni by multiplying them by 3 (so that I could still compare with 0.05), but it makes no sense to have a P-value, which is a probability, greater than 1, so an “adjusted P-value” that comes out greater than 1 is rounded back down to 1. You interpret this as being “no evidence at all of a difference in medians” between drugs B and C.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#location-species-and-disease-in-plants-1",
    "href": "tidying-data.html#location-species-and-disease-in-plants-1",
    "title": "15  Tidying data",
    "section": "15.18 Location, species and disease in plants",
    "text": "15.18 Location, species and disease in plants\nThe table below is a “contingency table”, showing frequencies of diseased and undiseased plants of two different species in two different locations:\n\nSpecies     Disease present         Disease absent\n          Location X Location Y  Location X Location Y\nA            44         12          38        10\nB            28         22          20        18\n\nThe data were saved as link. In that file, the columns are coded by two letters: a p or an a to denote presence or absence of disease, and an x or a y to denote location X or Y. The data are separated by multiple spaces and aligned with the variable names.\n\nRead in and display the data.\n\nSolution\nread_table again. You know this because, when you looked at the data file, which of course you did (didn’t you?), you saw that the data values were aligned by columns with multiple spaces between them:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/disease.txt\"\ntbl &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Species = col_character(),\n  px = col_double(),\n  py = col_double(),\n  ax = col_double(),\n  ay = col_double()\n)\n\ntbl\n\n\n\n  \n\n\n\nI was thinking ahead, since I’ll be wanting to have one of my columns called disease, so I’m not calling the data frame disease.\nYou’ll also have noticed that I simplified the data frame that I had you read in, because the original contingency table I showed you has two header rows, and we have to have one header row. So I mixed up the information in the two header rows into one.\n\\(\\blacksquare\\)\n\nExplain briefly how these data are not “tidy”.\n\nSolution\nThe simple answer is that there are 8 frequencies, that each ought to be in a row by themselves. Or, if you like, there are three variables, Species, Disease status and Location, and each of those should be in a column of its own. Either one of these ideas, or something like it, is good. I need you to demonstrate that you know something about “tidy data” in this context.\n\\(\\blacksquare\\)\n\nUse a suitable tidyr tool to get all the things that are the same into a single column. (You’ll need to make up a temporary name for the other new column that you create.) Show your result.\n\nSolution\npivot_longer is the tool. All the columns apart from Species contain frequencies. They are frequencies in disease-location combinations, so I’ll call the column of “names” disloc. Feel free to call it temp for now if you prefer:\n\ntbl %&gt;% pivot_longer(-Species, names_to=\"disloc\", values_to = \"frequency\") -&gt; tbl.2\ntbl.2\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nExplain briefly how the data frame you just created is still not “tidy” yet.\n\nSolution\nThe column I called disloc actually contains two variables, disease and location, which need to be split up. A check on this is that we have two columns (not including the frequencies), but back in (b) we found three variables, so there ought to be three non-frequency columns.\n\\(\\blacksquare\\)\n\nUse one more tidyr tool to make these data tidy, and show your result.\n\nSolution\nThis means splitting up disloc into two separate columns, splitting after the first character, thus:\n\n(tbl.2 %&gt;% separate(disloc, c(\"disease\", \"location\"), 1) -&gt; tbl.3)\n\n\n\n  \n\n\n\nThis is now tidy: eight frequencies in rows, and three non-frequency columns. (Go back and look at your answer to part (b) and note that the issues you found there have all been resolved now.)\nExtra: my reading of one of the vignettes (the one called pivot) for tidyr suggests that pivot_longer can do both the making longer and the separating in one shot:\n\ntbl %&gt;% pivot_longer(-Species, names_to=c(\"disease\", \"location\"), names_sep=1, values_to=\"frequency\")\n\n\n\n  \n\n\n\nAnd I (amazingly) got that right first time!\nThe idea is that you recognize that the column names are actually two things: a disease status and a location. To get pivot_longer to recognize that, you put two things in the names_to. Then you have to say how the two things in the columns are separated: this might be by an underscore or a dot, or, as here, “after the first character” (just as in separate). Using two names and some indication of what separates them then does a combined pivot-longer-and-separate, all in one shot.\nThe more I use pivot_longer, the more I marvel at the excellence of its design: it seems to be easy to guess how to make things work.\n\\(\\blacksquare\\)\n\nLet’s see if we can re-construct the original contingency table (or something equivalent to it). Use the function xtabs. This requires first a model formula with the frequency variable on the left of the squiggle, and the other variables separated by plus signs on the right. Second it requires a data frame, with data=. Feed your data frame from the previous part into xtabs. Save the result in a variable and display the result.\n\nSolution\n\ntbl.4 &lt;- xtabs(frequency ~ Species + disease + location, data = tbl.3)\ntbl.4\n\n, , location = x\n\n       disease\nSpecies  a  p\n      A 38 44\n      B 20 28\n\n, , location = y\n\n       disease\nSpecies  a  p\n      A 10 12\n      B 18 22\n\n\nThis shows a pair of contingency tables, one each for each of the two locations (in general, the variable you put last on the right side of the model formula). You can check that everything corresponds with the original data layout at the beginning of the question, possibly with some things rearranged (but with the same frequencies in the same places).\n\\(\\blacksquare\\)\n\nTake the output from the last part and feed it into the function ftable. How has the output been changed? Which do you like better? Explain briefly.\n\nSolution\nThis:\n\nftable(tbl.4)\n\n                location  x  y\nSpecies disease               \nA       a                38 10\n        p                44 12\nB       a                20 18\n        p                28 22\n\n\nThis is the same output, but shown more compactly. (Rather like a vertical version of the original data, in fact.) I like ftable better because it displays the data in the smallest amount of space, though I’m fine if you prefer the xtabs output because it spreads things out more. This is a matter of taste. Pick one and tell me why you prefer it, and I’m good.\nThat’s the end of what you had to do, but I thought I would do some modelling and try to find out what’s associated with disease. The appropriate modelling with frequencies is called “log-linear modelling”, and it assumes that the log of the frequencies has a linear relationship with the effects of the other variables. This is not quite as simple as the log transformations we had before, because bigger frequencies are going to be more variable, so we fit a generalized linear model with a Poisson-distributed response and log link. (It’s better if you know what that means, but you ought to be able to follow the logic if you don’t. Chapter 29 has more on this.)\nFirst, fit a model predicting frequency from everything, including all the interactions. (The reason for doing it this way will become clear later):\n\nmodel.1 &lt;- glm(frequency ~ Species * location * disease, data = tbl.3, family = \"poisson\")\ndrop1(model.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe residuals are all zero because this model fits perfectly. The problem is that it is very complicated, so it offers no insight. So what we do is to look at the highest-order interaction Species:location:disease and see whether it is significant. It is not, so we can remove it. This is reminiscent of variable selection in regression, where we pull the least significant thing out of the model in turn until we can go no further. But here, we have additional things to think about: we have to get rid of all the three-way interactions before we can tackle the two-way ones, and all the two-way ones before we can tackle the main effects. There is a so-called “nested” structure happening here that says you don’t look at, say, Species, until you have removed all the higher-order interactions involving Species. Not clear yet? Don’t fret. drop1 allows you to assess what is currently up for grabs (here, only the three-way interaction, which is not significant, so out it comes).\nLet’s get rid of that three-way interaction. This is another use for update that you might have seen in connection with multiple regression (to make small changes to a big model):\n\nmodel.2 &lt;- update(model.1, . ~ . - Species:location:disease)\ndrop1(model.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nNotice how update saved us having to write the whole model out again.\nNow the three two-way interactions are up for grabs: Species:location, Species:disease and location:disease. The last of these is the least significant, so out it comes. I did some copying and pasting, but I had to remember which model I was working with and what I was removing:\n\nmodel.3 &lt;- update(model.2, . ~ . - location:disease)\ndrop1(model.3, test = \"Chisq\")\n\n\n\n  \n\n\n\nSpecies:disease comes out, but it looks as if Species:location will have to stay:\n\nmodel.4 &lt;- update(model.3, . ~ . - Species:disease)\ndrop1(model.4, test = \"Chisq\")\n\n\n\n  \n\n\n\nSpecies:location indeed stays. That means that anything “contained in” it also has to stay, regardless of its main effect. So the only candidate for removal now is disease: not significant, out it comes:\n\nmodel.5 &lt;- update(model.4, . ~ . - disease)\ndrop1(model.5, test = \"Chisq\")\n\n\n\n  \n\n\n\nAnd now we have to stop.\nWhat does this final model mean? Well, frequency depends significantly on the Species:location combination, but not on anything else. To see how, we make a contingency table of species by location (totalling up over disease status, since that is not significant):\n\nxtabs(frequency ~ Species + location, data = tbl.3)\n\n       location\nSpecies  x  y\n      A 82 22\n      B 48 40\n\n\nMost of the species A’s are at location X, but the species B’s are about evenly divided between the two locations. Or, if you prefer (equally good): location X has mostly species A, while location Y has mostly species B. You can condition on either variable and compare the conditional distribution of the other one.\nNow, this is rather interesting, because this began as a study of disease, but disease has completely disappeared from our final model! That means that nothing in our final model has any relationship with disease. Indeed, if you check the original table, you’ll find that disease is present slightly more than it’s absent, for all combinations of species and location. That is, neither species nor location has any particular association with (effect on) disease, since disease prevalence doesn’t change appreciably if you change location, species or the combination of them.\nThe way an association with disease would show up is if a disease:something interaction had been significant and had stayed in the model, that something would have been associated with disease. For example, if the disease:Species table had looked like this:\n\ndisease &lt;- c(\"a\", \"a\", \"p\", \"p\")\nSpecies &lt;- c(\"A\", \"B\", \"A\", \"B\")\nfrequency &lt;- c(10, 50, 30, 30)\nxx &lt;- tibble(disease, Species, frequency)\nxtabs(frequency ~ disease + Species, data=xx)\n\n       Species\ndisease  A  B\n      a 10 50\n      p 30 30\n\n\nFor species A, disease is present 75% of the time, but for species B it’s present less than 40% of the time. So in this one there ought to be a significant association between disease and species:\n\nxx.1 &lt;- glm(frequency ~ disease * Species, data = xx, family = \"poisson\")\ndrop1(xx.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nAnd so there is. Nothing can come out of the model. (This is the same kind of test as a chi-squared test for association.\nThe log-linear model is a multi-variable generalization of that.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#mating-songs-in-crickets-1",
    "href": "tidying-data.html#mating-songs-in-crickets-1",
    "title": "15  Tidying data",
    "section": "15.19 Mating songs in crickets",
    "text": "15.19 Mating songs in crickets\nMale tree crickets produce “mating songs” by rubbing their wings together to produce a chirping sound. It is hypothesized that female tree crickets identify males of the correct species by how fast (in chirps per second) the male’s mating song is. This is called the “pulse rate”. Some data for two species of crickets are in link. The columns, which are unlabelled, are temperature and pulse rate (respectively) for Oecanthus exclamationis (first two columns) and Oecanthus niveus (third and fourth columns). The columns are separated by tabs. There are some missing values in the first two columns because fewer exclamationis crickets than niveus crickets were measured. The research question is whether males of the different species have different average pulse rates. It is also of interest to see whether temperature has an effect, and if so, what. Before we get to that, however, we have some data organization to do.\n\nRead in the data, allowing for the fact that you have no column names. You’ll see that the columns have names X1 through X4. This is OK.\n\nSolution\nTab-separated, so read_tsv; no column names, so col_names=FALSE:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/crickets.txt\"\ncrickets &lt;- read_tsv(my_url, col_names = FALSE)\n\nRows: 17 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (4): X1, X2, X3, X4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncrickets\n\n\n\n  \n\n\n\nAs promised.\nIf you didn’t catch the tab-separated part, this probably happened to you:\n\nd &lt;- read_delim(my_url, \" \", col_names = FALSE)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 17 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): X1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis doesn’t look good:\n\nproblems(d)\n\n\n\n  \n\n\n\nThe “expected columns” being 1 should bother you, since we know there are supposed to be 4 columns. At this point, we take a look at what got read in:\n\nd\n\n\n\n  \n\n\n\nand there you see the \\t or “tab” characters separating the values, instead of spaces. (This is what I tried first, and once I looked at this, I realized that read_tsv was what I needed.)\n\\(\\blacksquare\\)\n\nTidy these untidy data, going as directly as you can to something tidy. (Some later parts show you how it used to be done.) Begin by: (i) adding a column of row numbers, (ii) rename-ing the columns to species name, an underscore, and the variable contents (keeping pulserate as one word), and then use pivot_longer. Note that the column names encode two things.\n\nSolution\nTake this one piece of the pipeline at a time: that is, first check that you got the renaming right and looking at what you have, before proceeding to the pivot_longer. The syntax of rename is new name equals old name, and I like to split this over several lines to make it easier to read:\n\ncrickets %&gt;% \n  mutate(row=row_number()) %&gt;% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) \n\n\n\n  \n\n\n\nThe first part of each column name is the species and the second part is what was measured each time, separated by an underscore. To handle that in pivot_longer, you give two names of new columns to create (in names_to), and say what they’re separated by:\n\ncrickets %&gt;% \n  mutate(row=row_number()) %&gt;% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) %&gt;% \n  pivot_longer(-row, names_to=c(\"species\", \"measurement\"), names_sep=\"_\", values_to = \"obs\")\n\n\n\n  \n\n\n\nThis is tidy now, but we went a step too far: that column measurement should be two columns, called temperature and pulserate, which means it should be made wider. The obvious way is this:\n\ncrickets %&gt;% \n  mutate(row=row_number()) %&gt;% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) %&gt;% \n  pivot_longer(-row, names_to=c(\"species\", \"measurement\"), names_sep=\"_\", values_to = \"obs\") %&gt;% \n  pivot_wider(names_from=measurement, values_from=obs)\n\n\n\n  \n\n\n\nThe row numbers are cricket-within-species, which isn’t very meaningful, but we needed something for the pivot_wider to key on, to recognize what needed to go in which row. The way it works is it uses anything not mentioned in names_from or values_from as a “key”: each unique combination belongs in a row. Here that would be the combination of row and species, which is a good key because each species appears once with each row number.\nThis works, but a better way is to recognize that one of the variants of pivot_longer will do this all at once (something to think about when you have a longer followed by a wider). The key is that temperature and pulse rate need to be column names, so the second thing in names_to has to be that special thing .value, and you remove the values_to since it is now clear where the values are coming from:\n\ncrickets %&gt;% \n  mutate(row=row_number()) %&gt;% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) %&gt;% \n  pivot_longer(-row, names_to=c(\"species\", \".value\"), names_sep=\"_\") \n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nIf you found (b) a bit much to take in, the rest of the way we take a rather more leisurely approach towards the tidying.\n\nThese data are rather far from being tidy. There need to be three variables, temperature, pulse rate and species, and there are \\(14+17=31\\) observations altogether. This one is tricky in that there are temperature and pulse rate for each of two levels of a factor, so I’ll suggest combining the temperature and chirp rate together into one thing for each species, then pivoting them longer (“combining”), then pivoting them wider again (“splitting”). Create new columns, named for each species, that contain the temperature and pulse rate for that species in that order, united together. For the rest of this question, start from the data frame you read in, and build a pipe, one or two steps at a time, to save creating a lot of temporary data frames.\nSolution\nBreathe, and then begin. unite creates new columns by joining together old ones:13\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4)\n\n\n\n  \n\n\n\nNote that the original columns X1:X4 are gone, which is fine, because the information we needed from them is contained in the two new columns. unite by default uses an underscore to separate the joined-together values, which is generally safe since you won’t often find those in data.\nDigression: unite-ing with a space could cause problems if the data values have spaces in them already. Consider this list of names:\n\nnames &lt;- c(\"Cameron McDonald\", \"Durwin Yang\", \"Ole Gunnar Solskjaer\", \"Mahmudullah\")\n\nTwo very former students of mine, a Norwegian soccer player, and a Bangladeshi cricketer. Only one of these has played for Manchester United:\n\nmanu &lt;- c(F, F, T, F)\n\nand let’s make a data frame:\n\nd &lt;- tibble(name = names, manu = manu)\nd\n\n\n\n  \n\n\n\nNow, what happens if we unite those columns, separating them by a space?\n\nd %&gt;% unite(joined, name:manu, sep = \" \")\n\n\n\n  \n\n\n\nIf we then try to separate them again, what happens?\n\nd %&gt;%\n  unite(joined, name:manu, sep = \" \") %&gt;%\n  separate(joined, c(\"one\", \"two\"), \" \")\n\nWarning: Expected 2 pieces. Additional pieces discarded in 3 rows [1, 2, 3].\n\n\n\n\n  \n\n\n\nThings have gotten lost: most of the original values of manu and some of the names. If we use a different separator character, either choosing one deliberately or going with the default underscore, everything works swimmingly:\n\nd %&gt;%\n  unite(joined, name:manu, sep = \":\") %&gt;%\n  separate(joined, c(\"one\", \"two\"), \":\")\n\n\n\n  \n\n\n\nand we are back to where we started.\nIf you run just the unite line (move the pipe symbol to the next line so that the unite line is complete as it stands), you’ll see what happened.\n\\(\\blacksquare\\)\n\nThe two columns exclamationis and niveus that you just created are both temperature-pulse rate combos, but for different species. Collect them together into one column, labelled by species. (This is a straight tidyr pivot_longer, even though the columns contain something odd-looking.)\n\nSolution\nThus, this, naming the new column temp_pulse since it contains both of those things. Add to the end of the pipe you started building in the previous part:\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4) %&gt;%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", values_to = \"temp_pulse\")\n\n\n\n  \n\n\n\nYep. You’ll see both species of crickets, and you’ll see some missing values at the bottom, labelled, at the moment, NA_NA.\nThis is going to get rather long, but don’t fret: we debugged the two unite lines before, so if you get any errors, they must have come from the pivot_longer. So that would be the place to check.\n\\(\\blacksquare\\)\n\nNow split up the temperature-pulse combos at the underscore, into two separate columns. This is separate. When specifying what to separate by, you can use a number (“split after this many characters”) or a piece of text, in quotes (“when you see this text, split at it”).\n\nSolution\nThe text to split by is an underscore (in quotes), since unite by default puts an underscore in between the values it pastes together. Glue the separate onto the end. We are creating two new variables temperature and pulse_rate:\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4) %&gt;%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", values_to = \"temp_pulse\") %&gt;% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\")\n\n\n\n  \n\n\n\nYou’ll note that unite and separate are opposites (“inverses”) of each other, but we haven’t just done something and then undone it, because we have a pivot_longer in between; in fact, arranging it this way has done precisely the tidying we wanted.\n\\(\\blacksquare\\)\n\nAlmost there. Temperature and pulse rate are still text (because unite turned them into text), but they should be numbers. Create new variables that are numerical versions of temperature and pulse rate (using as.numeric). Check that you have no extraneous variables (and, if necessary, get rid of the ones you don’t want). (Species is also text and really ought to be a factor, but having it as text doesn’t seem to cause any problems.) You can, if you like, use parse_number instead of as.numeric. They should both work. The distinction I prefer to make is that parse_number is good for text with a number in it (that we want to pull the number out of), while as.numeric is for turning something that looks like a number but isn’t one into a genuine number.14\n\nSolution\nmutate-ing into a column that already exists overwrites the variable that’s already there (which saves us some effort here).\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4) %&gt;%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", values_to = \"temp_pulse\") %&gt;% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\") %&gt;%\n  mutate(temperature = as.numeric(temperature)) %&gt;%\n  mutate(pulse_rate = as.numeric(pulse_rate)) -&gt; crickets.1\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `temperature = as.numeric(temperature)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `pulse_rate = as.numeric(pulse_rate)`.\nCaused by warning:\n! NAs introduced by coercion\n\ncrickets.1\n\n\n\n  \n\n\n\nI saved the data frame this time, since this is the one we will use for our analysis.\nThe warning message tells us that we got genuine missing-value NAs back, which is probably what we want. Specifically, they got turned from missing text to missing numbers!15 The R word “coercion” means values being changed from one type of thing to another type of thing. (We’ll ignore the missings and see if they cause us any trouble. The same warning messages will show up on graphs later.) So I have 34 rows (including three rows of missings) instead of the 31 rows I would have liked. Otherwise, success.\nThere is (inevitably) another way to do this. We are doing the as.numeric twice, exactly the same on two different columns, and when you are doing the same thing on a number of columns, here a mutate with the same function, you have the option of using across. This is the same idea that we used way back to compute numerical summaries of a bunch of columns:\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4) %&gt;%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", \n               values_to = \"temp_pulse\") %&gt;% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\") %&gt;%\n  mutate(across(c(temperature, pulse_rate), \\(x) as.numeric(x)))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(c(temperature, pulse_rate), function(x) as.numeric(x))`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\n\n  \n\n\n\nCan’t I just say that these are columns 2 and 3?\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4) %&gt;%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", \n               values_to = \"temp_pulse\") %&gt;% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\") %&gt;%\n  mutate(across(2:3, \\(x) as.numeric(x)))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(2:3, function(x) as.numeric(x))`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\n\n  \n\n\n\nYes. Equally good. What goes into the across is the same as can go into a select: column numbers, names, or any of those “select helpers” like starts_with.\nYou might think of using across here on the quantitative columns, but remember the reason for doing this: all the columns are text, before you convert temperature and pulse rate to numbers, and so there’s no way to pick out just the two columns you want that way.\nCheck that the temperature and pulse rate columns are now labelled dbl, which means they actually are decimal numbers (and don’t just look like decimal numbers).\nEither way, using unite and then separate means that all the columns we created we want to keep (or, all the ones we would have wanted to get rid of have already been gotten rid of).\nNow we could actually do some statistics. That we do elsewhere.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#number-1-songs-1",
    "href": "tidying-data.html#number-1-songs-1",
    "title": "15  Tidying data",
    "section": "15.20 Number 1 songs",
    "text": "15.20 Number 1 songs\nThe data file link contains a lot of information about songs popular in 2000. This dataset is untidy. Our ultimate aim is to answer “which song occupied the #1 position for the largest number of weeks?”. To do that, we will build a pipe that starts from the data frame read in from the URL above, and finishes with an answer to the question. I will take you through this step by step. Each part will involve adding something to the pipe you built previously (possibly after removing a line or two that you used to display the previous result).\n\nRead the data and display what you have.\n\nSolution\n\nbillboard &lt;- read_csv(\"http://stat405.had.co.nz/data/billboard.csv\")\n\nRows: 317 Columns: 83\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (3): artist.inverted, track, genre\ndbl  (66): year, x1st.week, x2nd.week, x3rd.week, x4th.week, x5th.week, x6th...\nlgl  (11): x66th.week, x67th.week, x68th.week, x69th.week, x70th.week, x71st...\ndate  (2): date.entered, date.peaked\ntime  (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThere are a lot of columns. What does this look like?\n\nbillboard\n\n\n\n  \n\n\n\nOn yours, you will definitely see a little arrow top right saying “there are more columns”, and you will have to click on it several times to see them all. A lot of the ones on the right will be missing.\n\\(\\blacksquare\\)\n\nThe columns x1st.week through x76th.week contain the rank of each song in the Billboard chart in that week, with week 1 being the first week that the song appeared in the chart. Convert all these columns into two: an indication of week, called week, and of rank, called rank. Most songs appeared in the Billboard chart for a lot less than 76 weeks, so there are missing values, which you want to remove. (I say “indication of week” since this will probably be text at the moment). Display your new data frame. Do you have fewer columns? Why do you have a lot more rows? Explain briefly.\n\nSolution\nAs is often the case, the first step is pivot_longer, to reduce all those columns to something easier to deal with. The columns we want to make longer are the ones ending in “week”:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T)\n\n\n\n  \n\n\n\nThe “values” (ranks) have missings in them, which we wanted to get rid of.\nThere are now only 9 columns, a lot fewer than we started with. This is (I didn’t need you to say) because we have collected together all those week columns into one (a column called rank with an indication of which week it came from). The logic of the pivot_longer is that all those columns contain ranks (which is what make them the same), but they are ranks from different weeks (which is what makes them different).\nWhat has actually happened is that we have turned “wide” format into “long” format. This is not very insightful, so I would like you to go a bit further in your explanation. The original data frame encodes the rank of each song in each week, and what the pivot_longer has done is to make that explicit: in the new data frame, each song’s rank in each week appears in one row, so that there are as many rows as there are song-week combinations. The original data frame had 317 songs over 76 weeks, so this many:\n\n317 * 76\n\n[1] 24092\n\n\nsong-week combinations.\nNot every song appeared in the Billboard chart for 76 weeks, so our tidy data frame has a lot fewer rows than this.\nYou need to say that the original data frame had each song appearing once (on one line), but now each song appears on multiple rows, one for each week that the song was in the chart. Or something equivalent to that.\n\\(\\blacksquare\\)\n\nBoth your week and rank columns are (probably) text. Create new columns that contain just the numeric values, and display just your new columns, again adding onto the end of your pipe. If it so happens that rank is already a number, leave it as it is.\n\nSolution\nMy rank is already a number, so I could leave it; for later, I make a copy of it called rakn_numbe. The week has a number in it, which I can extract using parse_number:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nThe meaning of your week-number column is that it refers to the number of weeks after the song first appeared in the Billboard chart. That is, if a song’s first appearance (in date.entered) is July 24, then week 1 is July 24, week 2 is July 31, week 3 is August 7, and so on. Create a column current by adding the appropriate number of days, based on your week number, to date.entered. Display date.entered, your week number, and current to show that you have calculated the right thing. Note that you can add a number of days onto a date and you will get another date.\n\nSolution\nThere is a (small) gotcha here: if you read carefully, you’ll see that “week 1” is actually “week 0” in terms of the number of days to add on to date.entered. So you have to subtract one from the number of weeks before you multiply it by seven to get a number of days. After that thinking, this:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  select(date.entered, week_number, current)\n\n\n\n  \n\n\n\nDon’t forget to use your week-turned-into-number, or else it won’t work! (This bit me too, so you don’t need to feel bad.)\nYou can also combine the three column-definition statements into one mutate. It doesn’t matter; as soon as you have defined a column, you can use it in defining another column, even within the same mutate.\nAnyway, the rows displayed are all week_number 1, so the current date should be the same as date.entered, and is. (These are all the first week that a song is in the Billboard chart).\nYou might be thinking that this is not much of a check, and you would be right. A handy trick is to display a random sample of 10 (say) out of the 5,000-odd rows of the data frame. To do that, add the line sample_n(10) on the end, like this:\n\nbillboard %&gt;%\n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(\n    week_number = parse_number(week),\n    rank_number = rank\n  ) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  select(date.entered, week_number, current) %&gt;%\n  sample_n(10)\n\n\n\n  \n\n\n\nThis gives a variety of rows to check. The first current should be \\(7-1=6\\) weeks, or about a month and a half, after the date the song entered the chart, and so it is; the second and third ones should be \\(18-1=17\\) weeks after entry, which is very close to a third of a year (\\(17 \\times 3 = 51\\)), or four months. November to March is indeed four months. The fourth one is the first week on the charts, so the current date and the date entered should be (and are) the same. And so on.\nYour random selection of rows is likely to be different from mine, but the same kind of thinking will enable you to check whether it makes sense.\n\\(\\blacksquare\\)\n\nReaching the #1 rank on the Billboard chart is one of the highest accolades in the popular music world. List all the songs that reached rank 1. For these songs, list the artist (as given in the data set), the song title, and the date(s) for which the song was ranked number 1. Arrange the songs in date order of being ranked #1. Display all the songs (I found 55 of them).\n\nSolution\nTo the previous pipe, add the last lines below. You can use either rank (text) or what I called rank_number (a number). It doesn’t matter here, since we are only checking for equal-to, not something like “less than”:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  filter(rank == 1) %&gt;%\n  arrange(current) %&gt;%\n  select(artist.inverted, track, current)\n\n\n\n  \n\n\n\nYou’ll see the first ten rows, as here, but with clickable buttons to see the next 10 (and the previous 10 if you have moved beyond 1–10). The “artist” column is called artist.inverted because, if the artist is a single person rather than a group, their last name is listed first. The song title appears in the column track.\nThe song by Destiny’s Child spills into 2001 because it entered the chart in 2000, and the data set keeps a record of all such songs until they drop out of the chart. I’m not sure what happened to the song that was #1 on January 8, 2000; maybe it entered the chart in 199916 and so is not listed here.\n\\(\\blacksquare\\)\n\nUse R to find out which song held the #1 rank for the largest number of weeks. For this, you can assume that the song titles are all unique (if it’s the same song title, it’s the same song), but the artists might not be (for example, Madonna might have had two different songs reach the #1 rank). The information you need is in the output you obtained for the previous part, so it’s a matter of adding some code to the end of that. The last mark was for displaying only the song that was ranked #1 for the largest number of weeks, or for otherwise making it easy to see which song it was.\n\nSolution\nThis is a question of using count, but on the track title:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  filter(rank == 1) %&gt;%\n  arrange(current) %&gt;%\n  select(artist.inverted, track, current) %&gt;%\n  count(track)\n\n\n\n  \n\n\n\nThen you can scan down the n column, find that the biggest number is 11, and say: it’s the song “Independent Women Part I” by Destiny’s Child. This is 3 points (out of 4, when the question was to be handed in).\nBut, this is a data frame, so anything we can do to a data frame we can do to this, like listing out only the row(s) where n is equal to its maximum value:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  filter(rank == 1) %&gt;%\n  arrange(current) %&gt;%\n  select(artist.inverted, track, current) %&gt;%\n  count(track) %&gt;% \n  filter(n == max(n))\n\n\n\n  \n\n\n\nor arranging them in (most logically, descending) order by n to make it easier to pick out the top one:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  filter(rank == 1) %&gt;%\n  arrange(current) %&gt;%\n  select(artist.inverted, track, current) %&gt;%\n  count(track) %&gt;% \n  arrange(desc(n))\n\n\n\n  \n\n\n\nEither of those would have netted you the 4th point.\nIf you want to be a little bit more careful, you can make an artist-track combination as below. This would catch occasions where the same song by two different artists made it to #1, or two different songs that happened to have the same title did. It’s not very likely that the same artist would record two different songs with the same title, though it is possible that the same song by the same artist could appear in the Billboard chart on two different occasions.17\nI think I want to create an artist-song combo fairly early in my pipe, and then display that later, something like this. This means replacing track by my combo later in the pipe, wherever it appears:\n\nbillboard %&gt;%\n  pivot_longer(x1st.week:x76th.week, names_to = \"week\", values_to = \"rank\", values_drop_na = T) %&gt;%\n  mutate(\n    week_number = parse_number(week),\n    rank_number = rank\n  ) %&gt;%\n  mutate(combo = paste(track, artist.inverted, sep = \" by \")) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  filter(rank == 1) %&gt;%\n  arrange(current) %&gt;%\n  select(combo, current) %&gt;%\n  count(combo) %&gt;%\n  arrange(desc(n))\n\n\n\n  \n\n\n\nI don’t think it makes any difference here, but it might in other years, or if you look over several years where you might get cover versions of the same song performed by different artists.\nZero-point bonus: how many of these artists have you heard of? How many have your parents heard of? (I followed popular music quite closely much earlier than this, in the early 1980s in the UK. I remember both Madonna and U2 when they first became famous. U2’s first single was called “Fire” and it just scraped into the UK top 40. Things changed after that.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#bikes-on-college-1",
    "href": "tidying-data.html#bikes-on-college-1",
    "title": "15  Tidying data",
    "section": "15.21 Bikes on College",
    "text": "15.21 Bikes on College\nThe City of Toronto collects all kinds of data on aspects of life in the city. See link. One collection of data is records of the number of cyclists on certain downtown streets. The data in link are a record of the cyclists on College Street on the block west from Huron to Spadina on September 24, 2010. In the spreadsheet, each row relates to one cyclist. The first column is the time the cyclist was observed (to the nearest 15 minutes). After that, there are four pairs of columns. The observer filled in (exactly) one X in each pair of columns, according to whether (i) the cyclist was male or female, (ii) was or was not wearing a helmet, (iii) was or was not carrying a passenger on the bike, (iv) was or was not riding on the sidewalk. We want to create a tidy data frame that has the time in each row, and has columns containing appropriate values, often TRUE or FALSE, for each of the four variables measured.\nI will lead you through the process, which will involve developing a (long) pipe, one step at a time.\n\nTake a look at the spreadsheet (using Excel or similar: this may open when you click the link). Are there any obvious header rows? Is there any extra material before the data start? Explain briefly.\n\nSolution\nThis is what I see (you should see something that looks like this):\n\nThere are really two rows of headers (the rows highlighted in yellow). The actual information that says what the column pair is about is in the first of those two rows, and the second row indicates which category of the information above this column refers to. This is not the usual way that the column headers encode what the columns are about: we are used to having one column gender that would take the values female or male, or a column helmet containing the values yes or no. (You might be sensing pivot_longer here, which may be one way of tackling this, but I lead you into another idea below.) There are also six lines above the highlighted ones that contain background information about this study. (This is where I got the information about the date of the study and which block of which street it is about.) I am looking for two things: the apparent header line is actually two lines (the ones in yellow), and there are extra lines above that which are not data.\n\\(\\blacksquare\\)\n\nRead the data into an R data frame. Read without headers, and instruct R how many lines to skip over using skip= and a suitable number. When this is working, display the first few lines of your data frame. Note that your columns have names X1 through X9.\n\nSolution\nThe actual data start on line 9, so we need to skip 8 lines. col_names=F is the way to say that we have no column names (not ones that we want to use, anyway). Just typing the name of the data frame will display “a few” (that is, 10) lines of it, so that you can check it for plausibleness:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/bikes.csv\"\nbikes &lt;- read_csv(my_url, skip = 8, col_names = FALSE)\n\nRows: 1958 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): X2, X3, X4, X5, X6, X7, X8, X9\ntime (1): X1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbikes\n\n\n\n  \n\n\n\nThis seems to have worked: a column with times in it, and four pairs of columns, with exactly one of each pair having an X in it. The variable names X1 through X9 were generated by read_csv, as it does when you read in data with col_names=FALSE. The times are correctly times, and the other columns are all text. The blank cells in the spreadsheet have appeared in our data frame as “missing” (NA). The notation &lt;NA&gt; means “missing text” (as opposed to a missing number, say).\nThe first line in our data frame contains the first 7:00 (am) cyclist, so it looks as if we skipped the right number of lines.\n\\(\\blacksquare\\)\n\nWhat do you notice about the times in your first column? What do you think those “missing” times should be?\n\nSolution\nThere are some times and some missing values. It seems a reasonable guess that the person recording the data only recorded a time when a new period of 15 minutes had begun, so that the missing times should be the same as the previous non-missing one: For example, the first five rows are cyclists observed at 7:00 am (or, at least, between 7:00 and 7:15). So they should be recorded as 7:00, and the ones in rows 7–10 should be recorded as 7:15, and so on.\n\\(\\blacksquare\\)\n\nFind something from the tidyverse that will fill18 in those missing values with the right thing. Start a pipe from the data frame you read in, that updates the appropriate column with the filled-in times.\n\nSolution\nfill from tidyr fills in the missing times with the previous non-missing value. (This will mean finding the help for fill in R Studio or online.) I told you it was a giveaway. If you look in the help for fill via ?fill (or if you Google tidyr::fill, which is the full name for “the fill that lives in tidyr”), you’ll see that it requires up to two things (not including the data frame): a column to fill, and a direction to fill it (the default of “down” is exactly what we want). Thus:\n\nbikes %&gt;% fill(X1)\n\n\n\n  \n\n\n\nSuccess!\nWe will probably want to rename X1 to something like time, so let’s do that now before we forget. There is a rename that does about what you’d expect:\n\nbikes %&gt;% fill(X1) %&gt;% rename(Time = X1)\n\n\n\n  \n\n\n\nThe only thing I keep forgetting is that the syntax of rename is “new name equals old name”. Sometimes I think it’s the other way around, and then I wonder why it doesn’t work.\nI gave it a capital T so as not to confuse it with other things in R called time.\n\\(\\blacksquare\\)\n\nR’s ifelse function works like =IF in Excel. You use it to create values for a new variable, for example in a mutate. The first input to it is a logical condition (something that is either true or false); the second is the value your new variable should take if the condition is true, and the third is the value of your new variable if the condition is false. Create a new column gender in your data frame that is “male” or “female” depending on the value of your X2 column, using mutate. (You can assume that exactly one of the second and third columns has an X in it.) Add your code to the end of your pipe and display (the first 10 rows of) the result.\n\nSolution\nUnder the assumption we are making, we only have to look at column X2 and we ignore X3 totally:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(X2 == \"X\", \"male\", \"female\"))\n\n\n\n  \n\n\n\nOh, that didn’t work. The gender column is either male or missing; the two missing ones here should say female. What happened? Let’s just look at our logical condition this time:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(isX = (X2 == \"X\"))\n\n\n\n  \n\n\n\nThis is not true and false, it is true and missing. The idea is that if X2 is missing, we don’t (in general) know what its value is: it might even be X! So if X2 is missing, any comparison of it with another value ought to be missing as well.\nThat’s in general. Here, we know where those missing values came from: they were blank cells in the spreadsheet, so we actually have more information.\nPerhaps a better way to go is to test whether X2 is missing (in which case, it’s a female cyclist). R has a function is.na which is TRUE if the thing inside it is missing and FALSE if the thing inside it has some non-missing value. In our case, it goes like this:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\"))\n\n\n\n  \n\n\n\nOr you can test X3 for missingness: if missing, it’s male, otherwise it’s female. That also works.\nThis made an assumption that the person recording the X’s actually did mark an X in exactly one of the columns. For example, the columns could both be missing, or both have an X in them. This gives us more things to check, at least three. ifelse is good for something with only two alternatives, but when you have more, case_when is much better.19 Here’s how that goes. Our strategy is to check for three things: (i) X2 has an X and X3 is missing; (ii) X2 is missing and X3 has an X; (iii) anything else, which is an error:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = case_when(\n    X2 == \"X\" & is.na(X3) ~ \"Male\",\n    is.na(X2) & X3 == \"X\" ~ \"Female\",\n    TRUE                  ~ \"Error!\"\n  ))\n\n\n\n  \n\n\n\nIt seems nicest to format it with the squiggles lining up, so you can see what possible values gender might take.\nThe structure of the case_when is that the thing you’re checking for goes on the left of the squiggle, and the value you want your new variable to take goes on the right. What it does is to go down the list of conditions that you are checking for, and as soon as it finds one that is true, it grabs the value on the right of the squiggle and moves on to the next row. The usual way to write these is to have a catch-all condition at the end that is always true, serving to make sure that your new variable always gets some value. TRUE is, um, always true. If you want an English word for the last condition of your case_when, “otherwise” is a nice one.\nI wanted to check that the observer did check exactly one of V2 and V3 as I asserted, which can be done by gluing this onto the end:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = case_when(\n    X2 == \"X\" & is.na(X3) ~ \"Male\",\n    is.na(X2) & X3 == \"X\" ~ \"Female\",\n    TRUE ~ \"Error!\"\n  )) %&gt;%\n  count(gender)\n\n\n\n  \n\n\n\nThere are only Males and Females, so the observer really did mark exactly one X. (As a bonus, you see that there were slightly more male cyclists than female ones.)\nExtra: I was wondering how pivot_longer would play out here. The way to do it seems to be to rename the columns we want first, and get rid of the others:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  rename(male = X2, female = X3) %&gt;% \n  select(-starts_with(\"X\"))\n\n\n\n  \n\n\n\nEach row should have one X and one missing in it, so we may as well drop the missings as we pivot-longer:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  rename(male = X2, female = X3) %&gt;% \n  select(-starts_with(\"X\")) %&gt;% \n  pivot_longer(-Time, names_to=\"gender\", values_to=\"observed\", \n               values_drop_na = TRUE)\n\n\n\n  \n\n\n\nThe observed column is kind of pointless, since its value is always X. But we do have a check: the previous data frame had 1958 rows, with an X in either the male or the female column. This data frame has the gender of each observed cyclist in the gender column, and it also has 1958 rows. So, either way, that’s how many cyclists were observed in total.\n\\(\\blacksquare\\)\n\nCreate variables helmet, passenger and sidewalk in your data frame that are TRUE if the “Yes” column contains X and FALSE otherwise. This will use mutate again, but you don’t need ifelse: just set the variable equal to the appropriate logical condition. As before, the best way to create these variables is to test the appropriate things for missingness. Note that you can create as many new variables as you like in one mutate. Show the first few lines of your new data frame. (Add your code onto the end of the pipe you made above.)\n\nSolution\nOn the face of it, the way to do this is to go looking for X’s:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %&gt;%\n  mutate(\n    helmet = (X4 == \"X\"),\n    passenger = (X6 == \"X\"),\n    sidewalk = (X8 == \"X\")\n  )\n\n\n\n  \n\n\n\nBut, we run into the same problem that we did with gender: the new variables are either TRUE or missing, never FALSE.\nThe solution is the same: look for the things that are missing if the cyclist is wearing a helmet, carrying a passenger or riding on the sidewalk. These are X5, X7, X9 respectively:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %&gt;%\n  mutate(\n    helmet = is.na(X5),\n    passenger = is.na(X7),\n    sidewalk = is.na(X9)\n  )\n\n\n\n  \n\n\n\nAgain, you can do the mutate all on one line if you want to, or all four variable assignments in one mutate, but I used newlines and indentation to make the structure clear.\nIt is less elegant, though equally good for the purposes of the assignment, to use ifelse for these as well, which would go like this, for example:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(X2 == \"X\", \"male\", \"female\")) %&gt;%\n  mutate(helmet = ifelse(is.na(X5), TRUE, FALSE))\n\n\n\n  \n\n\n\nand the same for passenger and sidewalk. The warning is, whenever you see a TRUE and a FALSE in an ifelse, that you could probably get rid of the ifelse and use the logical condition directly.20\nFor gender, though, you need the ifelse (or a case_when) because the values you want it to take are male and female, something other than TRUE and FALSE.\nI like to put brackets around logical conditions when I am assigning them to a variable or defining new columns containing them. If I don’t, I get something like\n\nhelmet &lt;- V4 == \"X\"\n\nwhich actually works, but is hard to read. Well, I think it works. Let’s check:\n\nexes &lt;- c(\"X\", \"\", \"X\", \"\", \"X\")\ny &lt;- exes == \"X\"\ny\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE\n\n\nYes it does. But I would not recommend writing it this way, because unless you are paying attention, you won’t notice that == is testing for “logically equal” rather than putting something in a column.\nIt works because of a thing called “operator precedence”: the logical-equals is evaluated first, and the result of that is saved in the variable. But unless you or your readers remember that, it’s better to write\n\ny &lt;- (exes == \"X\")\n\nto draw attention to the order of calculation. This is the same reason that\n\n4 + 5 * 6\n\n[1] 34\n\n\nevaluates this way rather than doing the addition first and getting 54. BODMAS and all that.\nThe pivot_longer approach works for these too. Rename the columns as yes and no, and then give the names_to column a name like helmet. Give the values_to column a name like what2, to make it easier to remove later. And then do the same with the others, one pivot_longer at a time. (Keep all the columns, and then discard them at the end if you want. That way you don’t risk deleting something you might need later.)\n\\(\\blacksquare\\)\n\nFinally (for the data manipulation), get rid of all the original columns, keeping only the new ones that you created. Save the results in a data frame and display its first few rows.\n\nSolution\nThis is a breath of fresh air after all the thinking needed above: this is just select, added to the end:\n\nmybikes &lt;- bikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %&gt;%\n  mutate(\n    helmet = is.na(X5),\n    passenger = is.na(X7),\n    sidewalk = is.na(X9)\n  ) %&gt;%\n  select(-(X2:X9))\nmybikes\n\n\n\n  \n\n\n\nYou might not have renamed your X1, in which case, you still have it, but need to keep it (because it holds the times).\nAnother way to do this is to use a “select-helper”, thus:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %&gt;%\n  mutate(\n    helmet = is.na(X5),\n    passenger = is.na(X7),\n    sidewalk = is.na(X9)\n  ) %&gt;%\n  select(-num_range(\"X\", 2:9))\n\n\n\n  \n\n\n\nThis means “get rid of all the columns whose names are X followed by a number 2 through 9”.\nThe pipe looks long and forbidding, but you built it (and tested it) a little at a time. Which is how you do it.\n\\(\\blacksquare\\)\n\nThe next few parts are a quick-fire analysis of the data set. They can all be solved using count. How many male and how many female cyclists were observed in total?\n\nSolution\nI already got this one when I was checking for observer-notation errors earlier:\n\nmybikes %&gt;% count(gender)\n\n\n\n  \n\n\n\n861 females and 1097 males.\n\\(\\blacksquare\\)\n\nHow many male and female cyclists were not wearing helmets?\n\nSolution\nYou can count two variables at once, in which case you get counts of all combinations of them:\n\nmybikes %&gt;% count(gender, helmet)\n\n\n\n  \n\n\n\n403 females and 604 males were not wearing helmets, picking out what we need.\nThe real question of interest here is “what proportion of male and female cyclists were not wearing helmets?”21 This has a rather elegant solution that I will have to explain. First, let’s go back to the group_by and summarize version of the count here:\n\nmybikes %&gt;%\n  group_by(gender, helmet) %&gt;%\n  summarize(the_count = n())\n\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nThat’s the same table we got just now. Now, let’s calculate a proportion and see what happens:\n\nmybikes %&gt;%\n  group_by(gender, helmet) %&gt;%\n  summarize(the_count = n()) %&gt;%\n  mutate(prop = the_count / sum(the_count))\n\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nWe seem to have the proportions of males and females who were and were not wearing a helmet, and you can check that this is indeed the case, for example:\n\n403 / (403 + 458)\n\n[1] 0.4680604\n\n\n47% of females were not wearing helmets, while 55% of males were helmetless. (You can tell from the original frequencies that a small majority of females wore helmets and a small majority of males did not.)\nNow, we have to ask ourselves: how on earth did that work?\nWhen you calculate a summary (like our sum(count) above), it figures that you can’t want the sum by gender-helmet combination, since you already have those in count. You must want the sum over something. What? What happens is that it goes back to the group_by and “peels off” the last thing there, which in this case is helmet, leaving only gender. It then sums the counts for each gender, giving us what we wanted.\nIt just blows my mind that someone (ie., Hadley Wickham) could (i) think that this would be a nice syntax to have (instead of just being an error), (ii) find a way to implement it and (iii) find a nice logical explanation (“peeling off”) to explain how it worked.\nWhat happens if we switch the order of the things in the group_by?\n\nmybikes %&gt;%\n  group_by(helmet, gender) %&gt;%\n  summarize(the_count = n()) %&gt;%\n  mutate(prop = the_count / sum(the_count))\n\n`summarise()` has grouped output by 'helmet'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nNow we get the proportion of helmeted riders of each gender, which is not the same as what we had before. Before, we had “out of males” and “out of females”; now we have “out of helmeted riders” and “out of helmetless riders”. (The riders with helmets are almost 50–50 males and females, but the riders without helmets are about 60% male.)\nThis is row and column proportions in a contingency table, B22 style.\nNow, I have to see whether the count variant of this works:\n\nmybikes %&gt;%\n  count(gender, helmet) %&gt;%\n  mutate(prop = n / sum(n))\n\n\n\n  \n\n\n\nIt doesn’t. Well, it kind of does, but it divided by the sum of all of them rather than “peeling off”, so these are overall proportions rather than row or column proportions.\nSo I think you have to do this the group_by and summarize way.\n\\(\\blacksquare\\)\n\nHow many cyclists were riding on the sidewalk and carrying a passenger?\n\nSolution\nNot too many, I’d hope. Again:\n\nmybikes %&gt;% count(passenger, sidewalk)\n\n\n\n  \n\n\n\nWe’re looking for the “true”, “true” entry of that table, which seems to have vanished. That means the count is zero: none at all. (There were only 5 passenger-carrying riders, and they were all on the road.)\n\\(\\blacksquare\\)\n\nWhat was the busiest 15-minute period of the day, and how many cyclists were there then?\n\nSolution\nThe obvious way is to list every 15-minute period and eyeball the largest frequency. There are quite a few 15-minute periods, so be prepared to hit Next a few times (or use View):\n\nmybikes %&gt;% count(Time) \n\n\n\n  \n\n\n\n17:15, or 5:15 pm, with 128 cyclists.\nBut, computers are meant to save us that kind of effort. How? Note that the output from count is itself a data frame, so anything you can do to a data frame, you can do to it: for example, display only the rows where the frequency equals the maximum frequency:\n\nmybikes %&gt;%\n  count(Time) %&gt;%\n  filter(n == max(n))\n\n\n\n  \n\n\n\nThat will actually display all the times where the cyclist count equals the maximum, of which there might be more than one.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#feeling-the-heat-1",
    "href": "tidying-data.html#feeling-the-heat-1",
    "title": "15  Tidying data",
    "section": "15.22 Feeling the heat",
    "text": "15.22 Feeling the heat\nIn summer, the city of Toronto issues Heat Alerts for “high heat or humidity that is expected to last two or more days”. The precise definitions are shown at link. During a heat alert, the city opens Cooling Centres and may extend the hours of operation of city swimming pools, among other things. All the heat alert days from 2001 to 2016 are listed at link.\nThe word “warning” is sometimes used in place of “alert” in these data. They mean the same thing.22\n\nRead the data into R, and display the data frame. Note that there are four columns:\n\n\na numerical id (numbered upwards from the first Heat Alert in 2001; some of the numbers are missing)\nthe date of the heat alert, in year-month-day format with 4-digit years.\na text code for the type of heat alert\ntext describing the kind of heat alert. This can be quite long.\n\nSolution\nA .csv, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/heat.csv\"\nheat &lt;- read_csv(my_url)\n\nRows: 200 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): code, text\ndbl  (1): id\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nheat\n\n\n\n  \n\n\n\nYou might get a truncated text as I did, or you might have to click to see more of it. In any case, we won’t be using the text, so you can just forget about it from here on.\n\\(\\blacksquare\\)\n\nIn your data frame, are the dates stored as genuine dates or as text? How can you tell?\n\nSolution\nLook at the top of the column on your display of the data frame. Under the date column it says date rather than chr (which means “text”), so these are genuine dates. This happened because the data file contained the dates in year-month-day order, so read_csv read them in as dates. (If they had been in some other order, they would have been read in as text and we would need to use lubridate to make them into dates.)\n\\(\\blacksquare\\)\n\nWhich different heat alert codes do you have, and how many of each?\n\nSolution\ncount, most easily:\n\nheat %&gt;% count(code)\n\n\n\n  \n\n\n\nAlternatively, group_by and summarize:\n\nheat %&gt;% group_by(code) %&gt;% summarize(count = n())\n\n\n\n  \n\n\n\n(note that n() gives the number of rows, in each group if you have groups.)\nThere are six different codes, but EHAD only appears once.\n\\(\\blacksquare\\)\n\nUse the text in your dataset (or look back at the original data file) to describe briefly in your own words what the various codes represent.\n\nSolution\nYou can check that each time a certain code appears, the text next to it is identical. The six codes and my brief descriptions are:\n\nEHA: (Start of) Extended Heat Alert\nEHAD: Extreme Heat Alert downgraded to Heat Alert\nEHAE: Extended Heat Alert continues\nHA: (Start of) Heat Alert\nHAE: Heat Alert continues\nHAU: Heat Alert upgraded to Extended Heat Alert\n\nI thought there was such a thing as an Extreme Heat Alert, but here the word is (usually) Extended, meaning a heat alert that extends over several days, long in duration rather than extremely hot. The only place Extreme occurs is in EHAD, which only occurs once. I want your answer to say or suggest something about whether a code applies only to continuing heat alerts (ie., that EHAD, EHAE, HAE and HAU are different from the others).\n\\(\\blacksquare\\)\n\nHow many (regular and extended) heat alert events are there altogether? A heat alert event is a stretch of consecutive days, on all of which there is a heat alert or extended heat alert. Hints: (i) you can answer this from output you already have; (ii) how can you tell when a heat alert event starts?\n\nSolution\nThis turned out to be more messed-up than I thought. There is a detailed discussion below. The codes EHAD, EHAE, HAE, HAU all indicate that there was a heat alert on the day before. Only the codes HA and EHA can indicate the start of a heat alert (event). The problem is that HA and EHA sometimes indicate the start of a heat alert event and sometimes one that is continuing. You can check by looking at the data that HA and EHA days can (though they don’t always: see below) have a non-heat-alert day before (below) them in the data file: for example, August 4, 2012 is an HA day, but August 3 of that year was not part of any kind of heat alert. I had intended the answer to be this:\n\nSo we get the total number of heat alert events by totalling up the number of HA and EHA days: \\(59+93=152\\).\n\nThis is not right because there are some consecutive EHA days, eg. 5–8 July 2010, so that EHA sometimes indicates the continuation of an extended heat alert and sometimes the start of one. I was expecting EHA to be used only for the start, and one of the other codes to indicate a continuation. The same is (sometimes) true of HA. So reasonable answers to the question as set include:\n\n93, the number of HAs\n59, the number of EHAs\n152, the number of HAs and EHAs combined\n“152 or less”, “between 93 and 152”, ``between 59 and 152’’ to reflect that not all of these mark the start of a heat alert event.\n\nAny of these, or something similar with an explanation of how you got your answer, are acceptable. In your career as a data scientist, you will often run into this kind of thing, and it will be your job to do something with the data and explain what you did so that somebody else can decide whether they believe you or not. A good explanation, even if it is not correct, will help you get at the truth because it will inspire someone to say “in fact, it goes this way”, and then the two of you can jointly figure out what’s actually going on. Detailed discussion follows. If you have any ambitions of working with data, you should try to follow the paragraphs below, because they indicate how you would get an actual answer to the question.\nI think the key is the number of days between one heat alert day and the next one. dplyr has a function diff that works out exactly this. Building a pipeline, just because:\n\nheat %&gt;%\n  select(-text) %&gt;%\n  mutate(daycount = as.numeric(date)) %&gt;%\n  mutate(daydiff = abs(c(diff(daycount), 0)))\n\n\n\n  \n\n\n\nOof. I have some things to keep track of here:\n\nGet rid of the text, since it serves no purpose here.\nThe date column is a proper Date (we checked).\nThen I want the date as number of days; since it is a number of days internally, I just make it a number with as.numeric.\nThen I use diff to get the difference between each date and the previous one, remembering to glue a 0 onto the end so that I have the right number of differences.\nSince the dates are most recent first, I take the absolute value so that the daydiff values are positive (except for the one that is 0 on the end).\n\nStill with me? All right. You can check that the daydiff values are the number of days between the date on that line and the line below it. For example, there were 24 days between August 13 and September 6.\nNow, when daydiff is 1, there was also a heat alert on the previous day (the line below in the file), but when daydiff is not 1, that day must have been the start of a heat alert event. So if I count the non-1’s, that will count the number of heat alert events there were. (That includes the difference of 0 on the first day, the one at the end of the file.)\nThus my pipeline continues like this:\n\nheat %&gt;%\n  select(-text) %&gt;%\n  mutate(daycount = as.numeric(date)) %&gt;%\n  mutate(daydiff = abs(c(diff(daycount), 0))) %&gt;%\n  count(daydiff != 1)\n\n\n\n  \n\n\n\nAnd that’s how many actual heat alert events there were: 79, less even than the number of HAs. So that tells me that a lot of my HAs and EHAs were actually continuations of heat alert events rather than the start of them. I think I need to have a word with the City of Toronto about their data collection processes.\ncount will count anything that is, or can be made into, a categorical variable. It doesn’t have to be one of the columns of your data frame; here it is something that is either TRUE or FALSE about every row of the data frame.\nOne step further: what is the connection between the codes and the start of heat alert events? We can figure that out now:\n\nheat %&gt;%\n  select(-text) %&gt;%\n  mutate(daycount = as.numeric(date)) %&gt;%\n  mutate(daydiff = abs(c(diff(daycount), 0))) %&gt;%\n  mutate(start = (daydiff != 1)) %&gt;%\n  count(code, start)\n\n\n\n  \n\n\n\nI made a column start that is TRUE at the start of a heat alert event and FALSE otherwise, by comparing the days from the previous heat alert day with 1. Then I can make a table, or, as here, the dplyr equivalent with count.23 Or group_by and summarize. What this shows is that EHAD, EHAE, HAE and HAU never go with the start of a heat alert event (as they shouldn’t). But look at the HAs and EHAs. For the HAs, 73 of them go with the start of an event, but 20 do not. For the EHAs, just 6 of them go with the start, and 53 do not. (Thus, counting just the HAs was very much a reasonable thing to do.)\nThe 79 heat alert events that we found above had 73 of them starting with an HA, and just 6 starting with an EHA. I wasn’t quite sure how this would come out, but I knew it had something to do with the number of days between one heat alert day and the next, so I calculated those first and then figured out what to do with them.\n\\(\\blacksquare\\)\n\nWe are going to investigate how many heat alert days there were in each year. To do that, we have to extract the year from each of our dates.\n\nSolution\nThis will need the lubridate package, but you don’t need to load it specifically because it is now loaded with the tidyverse:\n\nheat %&gt;% select(-text) %&gt;% mutate(year = year(date)) %&gt;% sample_n(10)\n\n\n\n  \n\n\n\nThat seems to have worked. I listed a random sample of rows to get back to previous years. Having convinced myself that it worked, let me save it:\n\nheat %&gt;% select(-text) %&gt;% mutate(year = year(date)) -&gt; heat\n\n\\(\\blacksquare\\)\n\nCount the number of heat alert days for each year, by tabulating the year variable. Looking at this table, would you say that there have been more heat alert days in recent years? Explain (very) briefly.\n\nSolution\nCount them again:\n\nheat %&gt;% count(year)\n\n\n\n  \n\n\n\nThere are various things you could say, most of which are likely to be good. My immediate reaction is that most of the years with a lot of heat-alert days are in the last few years, and most of the years with not many are near the start, so there is something of an upward trend. Having said that, 2014 is unusually low (that was a cool summer), and 2005 was unusually high. (Was that the summer of the big power outage? I forget.24\nYou could also reasonably say that there isn’t much pattern: the number of heat-alert days goes up and down. In fact, anything that’s not obviously nonsense will do.\nI was thinking about making a graph of these frequencies against year, and sticking some kind of smooth trend on it. This uses the output we just got, which is itself a data frame:\n\nheat %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n)) + geom_point() + geom_smooth(se = F)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe pattern is very scattered, as is commonly the case with environmental-science-type data, but there is a very small upward trend. So it seems that either answer is justified, either “there is no trend” or “there is something of an upward trend”.\nThe other thing I notice on this plot is that if there are a lot of heat-alert days one year, there will probably also be a lot in the next year (and correspondingly if the number of heat-alert days is below average: it tends to be below average again in the next year). This pattern is known to time-series people as “autocorrelation” and indicates that the number of heat-alert days in one year and the next is not independent: if you know one year, you can predict the next year. (Assessment of trend and autocorrelation are hard to untangle properly.)\nExtra 1: I learn from Environmental Science grad students (of whom we have a number at UTSC) that the approved measure of association is called the Mann-Kendall correlation, which is the Kendall correlation of the data values with time. In the same way that we use the sign test when we doubt normality, and it uses the data more crudely but safely, the regular (so-called Pearson) correlation assumes normality (of the errors in the regression of one variable on the other), and when you doubt that (as you typically do with this kind of data) you compute a different kind of correlation with time. What the Kendall correlation does is to take each pair of observations and ask whether the trend with time is uphill or downhill. For example, there were 3 heat-alert days in 2009, 16 in 2010 and 12 in 2011. Between 2009 and 2010, the trend is uphill (increasing with time), and also between 2009 and 2011 (there were more heat-alert days in the later year), but between 2010 and 2011 the trend is downhill. The idea of the Kendall correlation is you take all the pairs of points, of which there are typically rather a lot, count up how many pairs are uphill and how many downhill, and apply a formula to get a correlation between \\(-1\\) and 1. (If there are about an equal number of uphills and downhills, the correlation comes out near 0; if they are mostly uphill, the correlation is near 1, and if they are mostly downhill, the correlation is near \\(-1\\).) It doesn’t matter how uphill or downhill the trends are, only the number of each, in the same way that the sign test only counts the number of values above or below the hypothesized median, not how far above or below they are.\nThis can be calculated, and even tested:\n\nheat %&gt;%\n  count(year) %&gt;%\n  with(., cor.test(year, n, method = \"kendall\"))\n\nWarning in cor.test.default(year, n, method = \"kendall\"): Cannot compute exact\np-value with ties\n\n\n\n    Kendall's rank correlation tau\n\ndata:  year and n\nz = 0.31612, p-value = 0.7519\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n0.05907646 \n\n\nThe Mann-Kendall correlation is a thoroughly unremarkable 0.06, and with only 16 data points, a null hypothesis that the correlation is zero is far from being rejected, P-value 0.7519 as shown. So this is no evidence of a time trend at all.\nExtra 2: I’d like to say a word about how I got these data. They came from link. If you take a look there, there are no obvious rows and columns. This format is called JSON. Look a bit more carefully and you’ll see stuff like this, repeated:\n\n{\"id\":\"232\",\"date\":\"2016-09-08\",\"code\":\"HAU\",\n\"text\":\"Toronto's Medical Officer of Health has upgraded the Heat Warning to an Extended Heat Warning\"}\n\none for each heat alert day. These are “keys” (on the left side of the :) and “values” (on the right side).25 The keys are column headers (if the data were in a data frame) and the values are the data values that would be in that column. In JSON generally, there’s no need for the keys to be the same in every row, but if they are, as they are here, the data can be arranged in a data frame. How? Read on.\nI did this in R, using a package called jsonlite, with this code:\n\nlibrary(jsonlite)\nurl &lt;- \"http://app.toronto.ca/opendata/heat_alerts/heat_alerts_list.json\"\nheat &lt;- fromJSON(url, simplifyDataFrame = T)\nhead(heat)\nwrite_csv(heat, \"heat.csv\")\n\nAfter loading the package, I create a variable url that contains the URL for the JSON file. The fromJSON line takes something that is JSON (which could be in text, a file or a URL) and converts it to and saves it in a data frame. Finally, I save the data frame in a .csv file. That’s the .csv file you used. If you run that code, you’ll get a .csv file of heat alerts right up to the present, and you can update my analysis.\nWhy .csv? If I had used write_delim, the values would have been separated by spaces. But, the text is a sentence of several words, which are themselves separated by spaces. I could have had you read in everything else and not the text, and then separated-by-spaces would have been fine, but I wanted you to see the text so that you could understand the code values. So .csv is what it was.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#isoflavones-1",
    "href": "tidying-data.html#isoflavones-1",
    "title": "15  Tidying data",
    "section": "15.23 Isoflavones",
    "text": "15.23 Isoflavones\nThe plant called kudzu was imported to the US South from Japan. It is rich in isoflavones, which are believed to be beneficial for bones. In a study, rats were randomly assigned to one of three diets: one with a low dose of isoflavones from kudzu, one with a high dose, and a control diet with no extra isoflavones. At the end of the study, each rat’s bone density was measured, in milligrams per square centimetre. The data as recorded are shown in http://ritsokiguess.site/isoflavones.txt.26 There are 15 observations for each treatment, and hence 45 altogether.\nHere are some code ideas you might need to use later, all part of the tidyverse. You may need to find out how they work.\n\ncol_names (in the read_ functions)\nconvert (in various tidyverse functions)\nfill\nna_if\nrename\nseparate_rows\nskip (in the read_ functions)\nvalues_drop_na (in the pivot_ functions)\n\nIf you use any of these, cite the webpage(s) or other source(s) where you learned about them.\n\nTake a look at the data file. Describe briefly what you see.\n\nSolution\nThe data values are (at least kind of) aligned in columns, suggesting read_table. There are up to six bone density values in each row, with a header that spans all of them (by the looks of it). The treatment column looks all right except that some of the rows are blank. The blank treatments are the same as the ones in the row(s) above them, you can infer, because there are 15 observations for each treatment, six, six, and then three. (This is how a spreadsheet is often laid out: blank means the same as the previous line.)27\nThis, you might observe, will need some tidying.\n\\(\\blacksquare\\)\n\nRead in the data, using read_table, and get it into a tidy form, suitable for making a graph. This means finishing with (at least) a column of treatments with a suitable name (the treatments will be text) and a column of bone density values (numbers), one for each rat. You can have other columns as well; there is no obligation to get rid of them. Describe your process clearly enough that someone new to this data set would be able to understand what you have done and reproduce it on another similar dataset. Before you begin, think about whether or not you want to keep the column headers that are in the data file or not. (It can be done either way, but one way is easier than the other.)\n\nSolution\nThe tidying part is a fair bit easier to see if you do not read the column headers. A clue to this is that bone_mineral_density is not aligned with the values (of bone mineral density) below it. The next question is how to do that. You might remember col_names=FALSE from when the data file has no column headers at all, but here it does have headers; we just want to skip over them. Keep reading in the documentation for read_table, and you’ll find an option skip that does exactly that, leading to:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/isoflavones.txt\"\nbmd0a &lt;- read_table(my_url, col_names = FALSE, skip = 1)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  X1 = col_character(),\n  X2 = col_double(),\n  X3 = col_double(),\n  X4 = col_double(),\n  X5 = col_double(),\n  X6 = col_double(),\n  X7 = col_double()\n)\n\n\nWarning: 6 parsing failures.\nrow col  expected    actual                                                 file\n  2  -- 7 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  3  -- 7 columns 3 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  5  -- 7 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  6  -- 7 columns 3 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  8  -- 7 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n... ... ......... ......... ....................................................\nSee problems(...) for more details.\n\nbmd0a\n\n\n\n  \n\n\n\nIf you miss the skip, the first row of “data” will be those column headers that were in the data file, and you really don’t want that. This link talks about both col_names and skip.\nThis, however, is looking very promising. A pivot_longer will get those columns of numbers into one column, which we can call something like bmd, and but, not so fast. What about those blank treatments in X1? The first two blank ones are control, the next two are low_dose and the last two are high_dose. How do we fill them in? The word “fill” might inspire you to read up on fill. Except that this doesn’t quite work, because it replaces missings with the non-missing value above them, and we have blanks, not missings.\nAll right, can we replace the blanks with missings, and then fill those? This might inspire you to go back to the list of ideas in the question, and find out what na_if does: namely, exactly this! Hence:\n\nbmd0a %&gt;% mutate(X1=na_if(X1, \"\")) %&gt;% \nfill(X1) \n\n\n\n  \n\n\n\nRun this one line at a time to see how it works. fill takes a column with missing values to replace, namely X1, and na_if takes two things: a column containing some values to make NA, and the values that should be made NA, namely the blank ones.\nSo that straightens out the treatment column. It needs renaming; you can do that now, or wait until later. I’m going to wait on that.\nYou need to organize the treatment column first, before you do the pivot_longer, or else that won’t work.28\nNow, we need to get one column of bone mass densities, instead of six. This you’ll recognize as a standard pivot_longer, with one tweak: those missing values in X5 through X7, which we want to get rid of. You might remember that this is what values_drop_na does:\n\nbmd0a %&gt;% mutate(X1=na_if(X1, \"\")) %&gt;% \nfill(X1) %&gt;% \npivot_longer(X2:X7, names_to=\"old\", values_to=\"bmd\", values_drop_na=TRUE)\n\n\n\n  \n\n\n\nIf you didn’t think of values_drop_na, do the pivot without, and then check that you have too many rows because the missings are still there (there are 45 rats but you have 54 rows), so add a drop_na() to the end of your pipe. The only missing values are in the column I called bmd.\nThis is almost there. We have a numeric column of bone mass densities, a column called old that we can ignore, and a treatment column with a stupid name that we can fix. I find rename backwards: the syntax is new name equals old name, so you start with the name that doesn’t exist yet and finish with the one you want to get rid of:\n\nbmd0a %&gt;% mutate(X1=na_if(X1, \"\")) %&gt;% \nfill(X1) %&gt;% \npivot_longer(X2:X7, names_to=\"old\", values_to=\"bmd\", values_drop_na=TRUE) %&gt;% \nrename(treatment=X1) -&gt; bmd1b\nbmd1b\n\n\n\n  \n\n\n\nDone!\nThe best way to describe this kind of work is to run your pipeline up to a point that needs explanation, describe what comes next, and then run the whole pipeline again up to the next point needing explanation, rinse and repeat. (This avoids creating unnecessary temporary dataframes, since the purpose of the pipe is to avoid those.)\nThe guideline for description is that if you don’t know what’s going to happen next, your reader won’t know either. For me, that was these steps:\n\nread the data file without row names and see how it looks\nfix up the treatment column (convincing myself and the reader that we were now ready to pivot-longer)\ndo the pivot_longer and make sure it worked\nrename the treatment column\n\nSo, I said there was another way. This happens to have a simple but clever solution. It starts from wondering “what happens if I read the data file with column headers, the normal way? Do it and find out:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/isoflavones.txt\"\nbmd0b &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  treatment = col_character(),\n  bone_mineral_density = col_double()\n)\n\n\nWarning: 9 parsing failures.\nrow col  expected    actual                                                 file\n  1  -- 2 columns 7 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  2  -- 2 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  3  -- 2 columns 3 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  4  -- 2 columns 7 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  5  -- 2 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n... ... ......... ......... ....................................................\nSee problems(...) for more details.\n\nbmd0b\n\n\n\n  \n\n\n\nThis looks … strange. There are two column headers, and so there are two columns. It so happened that this worked because the text bone_mineral_density is long enough to span all the columns of numbers. That second column is actually text: six or three numbers as text with spaces between them.\nThe first thing is, as before, to fill in the missing treatments, which is as above, but changing some names:\n\nbmd0b %&gt;% mutate(treatment=na_if(treatment, \"\")) %&gt;% \nfill(treatment) \n\n\n\n  \n\n\n\nThe way we learned in class for dealing with this kind of thing is separate. It is rather unwieldy here since we have to split bone_mineral_density into six (temporary) things:\n\nbmd0b %&gt;% mutate(treatment=na_if(treatment, \"\")) %&gt;% \nfill(treatment) %&gt;% \nseparate(bone_mineral_density, into = c(\"z1\", \"z2\", \"z3\", \"z4\", \"z5\", \"z6\"))\n\nWarning: Expected 6 pieces. Missing pieces filled with `NA` in 9 rows [1, 2, 3, 4, 5, 6,\n7, 8, 9].\n\n\n\n\n  \n\n\n\nThis works, though if you check, there’s a warning that some of the rows don’t have six values. However, these have been replaced by missings, which is just fine. From here, we do exactly what we did before: pivot-longer all the columns I called z-something, and get rid of the missings.\nHaving thought of separate, maybe you’re now wondering what separate_rows does. It turns out that it bypasses the business of creating extra columns and then pivoting them longer, thus:\n\nbmd0b %&gt;% mutate(treatment=na_if(treatment, \"\")) %&gt;% \nfill(treatment)  %&gt;% \nseparate_rows(bone_mineral_density, convert = TRUE) -&gt; bmd1a\nbmd1a\n\n\n\n  \n\n\n\nBoom! This takes all the things in that mess in bone_mineral_density, splits them up into individual data values, and puts them one per row back into the same column. The convert is needed because otherwise the values in the second column would be text and you wouldn’t be able to plot them. (If you don’t see that, use a mutate to convert the column into the numerical version of itself.)\n\\(\\blacksquare\\)\n\nThe statistician on this study is thinking about running an ordinary analysis of variance to compare the bone mineral density for the different treatments. Obtain a plot from your tidy dataframe that will help her decide whether that is a good idea.\n\nSolution\nThe key issues here are whether the values within each treatment group are close enough to normally distributed, and, if they are, whether the spreads are close enough to equal. The best plot is therefore a normal quantile plot of each of the three groups, in facets. You can do this without scales=\"free\":\n\nggplot(bmd1b, aes(sample=bmd)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~treatment)\n\n\n\n\nThe value of doing it this way is that you also get a sense of variability, from the slopes of the lines, or from how much of each box is filled vertically. (Here, the high-dose values are more spread-out than the other two groups, which are similar in spread.)\nYou could also do it with scales = \"free\":\n\nggplot(bmd1b, aes(sample=bmd)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~treatment, scales = \"free\")\n\n\n\n\nThe value of doing it this way is that you fill the facets (what I called “not wasting real estate” on an earlier assignment), and so you get a better assessment of normality, but the downside is that you will need another plot, for example a boxplot (see below) to assess equality of spreads if you are happy with the normality.\nI’m happy with either way of making the normal quantile plots, as long as you have a reason for your choice, coming from what you will be using the normal quantile plot for. You might not think of saying that here as you do it, but when you do the next part, you may realize that you need to assess equality of spreads, and in that case you should come back here and add a reason for using or not using scales = \"free\".\nThe next-best graph here is boxplots:\n\nggplot(bmd1b, aes(x=treatment, y=bmd)) + geom_boxplot()\n\n\n\n\nThis is not so good because it doesn’t address normality as directly (just giving you a general sense of shape). On the other hand, you can assess spread directly with a boxplot; see discussion above.\nThe grader is now probably thoroughly confused, so let me summarize possible answers in order of quality:\n\nA normal quantile plot of all three groups, using scales = \"free\" or not, with a good reason. (If with scales = \"free\", and there needs to be a comparison of spread, there needs to be a boxplot or similar below as well. That’s what I meant by “any additional graphs” in the next part.)\nA normal quantile plot of all three groups, using scales = \"free\" or not, without a good reason.\nA side-by-side boxplot. Saying in addition that normality doesn’t matter so much because we have moderate-sized samples of 15 and therefore that boxplots are good enough moves this answer up a place.\n\nNote that getting the graph is (relatively) easy once you have the tidy data, but is impossible if you don’t! This is the way the world of applied statistics works; without being able to get your data into the right form, you won’t be able to do anything else. This question is consistent with that fact; I’m not going to give you a tidy version of the data so that you can make some graphs. The point of this question is to see whether you can get the data tidy enough, and if you can, you get the bonus of being able to do something straightforward with it.\n\\(\\blacksquare\\)\n\nBased on your graph, and any additional graphs you wish to draw, what analysis would you recommend for this dataset? Explain briefly. (Don’t do the analysis.)\n\nSolution\nMake a decision about normality first. You need all three groups to be sufficiently normal. I don’t think there’s any doubt about the high-dose and low-dose groups; these are if anything short-tailed, which is not a problem for the ANOVA. You might find that the control group is OK too; make a call. Or you might find it skewed to the right, something suggested rather more by the boxplot. My take, from looking at the normal quantile plot, is that the highest value in the control group is a little too high, but with a sample size of 15, the Central Limit Theorem will take care of that. For yourself, you can find a bootstrapped sampling distribution of the sample mean for the control group and see how normal it looks.\nIf you are not happy with the normality, recommend Mood’s median test.\nIf you are OK with the normality, you need to assess equal spreads. You can do this from a boxplot, where the high-dose group clearly has bigger spread. Or, if you drew normal quantile plots without scales = \"free\", compare the slopes of the lines. This means that you need to recommend a Welch ANOVA.\nIf your normal quantile plots looked like this:\n\nggplot(bmd1b, aes(sample=bmd)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~treatment, scales = \"free\")\n\n\n\n\nthe only way to assess spread is to make another plot, and for this job, the boxplot is best.\nExtra 1: the bootstrapped sampling distribution of the sample mean for the control group goes this way:\n\nbmd1b %&gt;% \nfilter(treatment == \"control\") -&gt; d\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(d$bmd, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nNo problems there. The Welch ANOVA is fine.\nExtra 2: You might be curious how the analysis comes out. Here is Welch:\n\noneway.test(bmd~treatment, data=bmd1b)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  bmd and treatment\nF = 2.5385, num df = 8.000, denom df = 7.718, p-value = 0.1082\n\n\nNot all the same means, so use Games-Howell to explore:\n\ngamesHowellTest(bmd~factor(treatment), data = bmd1b)\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: bmd by factor(treatment)\n\n\n          198  209  218 226  232 245  control high_dose\n209       0.75 -    -   -    -   -    -       -        \n218       -    -    -   -    -   -    -       -        \n226       0.73 1.00 -   -    -   -    -       -        \n232       -    -    -   -    -   -    -       -        \n245       0.20 0.41 -   0.51 -   -    -       -        \ncontrol   0.13 0.80 -   0.97 -   0.77 -       -        \nhigh_dose 0.15 0.51 -   0.70 -   1.00 0.97    -        \nlow_dose  0.18 0.89 -   0.99 -   0.72 1.00    0.94     \n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nHigh dose is significantly different from both the other two, which are not significantly different from each other.\nMood’s median test, for comparison:\n\nmedian_test(bmd1b, bmd, treatment)\n\n$table\n           above\ngroup       above below\n  198           0     2\n  209           1     3\n  218           1     1\n  226           2     3\n  232           1     1\n  245           4     1\n  control       3     2\n  high_dose     4     2\n  low_dose      2     3\n\n$test\n       what     value\n1 statistic 6.0666667\n2        df 8.0000000\n3   P-value 0.6397643\n\n\nNot any significant differences, although it is a close thing.\nThe table of aboves and belows suggests the same thing as the Welch test: the high-dose values are mainly high, and the others are mostly low. But with these sample sizes it is not strong enough evidence. My guess is that the median test is lacking power compared to the Welch test; having seen that the Welch test is actually fine, it is better to use that here.29\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#jockos-garage-1",
    "href": "tidying-data.html#jockos-garage-1",
    "title": "15  Tidying data",
    "section": "15.24 Jocko’s Garage",
    "text": "15.24 Jocko’s Garage\nInsurance adjusters are concerned that Jocko’s Garage is giving estimates for repairing car damage that are too high. To see whether this is indeed the case, ten cars that had been in collisions were taken to both Jocko’s Garage and another garage, and the two estimates for repair were recorded. The data as recorded are here.\n\nTake a look at the data file (eg. by using your web browser). How are the data laid out? Do there appear to be column headers?\n\nSolution\nThe data are laid out in aligned columns, so that we will need to use read_table to read it in. There are no column headers, since there is no line at the top of the file saying what each column represents. (The fact that I was asking about column headers is kind of a clue that something non-standard is happening there.)\n\\(\\blacksquare\\)\n\nRead in and display the data file, bearing in mind what you just concluded about it. What names did the columns acquire?\n\nSolution\nAs mentioned above, you’ll need read_table, plus col_names=FALSE to not read the first row as column names:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/jocko.txt\"\ncars0 &lt;- read_table(my_url, col_names = FALSE)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  X1 = col_character(),\n  X2 = col_character(),\n  X3 = col_double(),\n  X4 = col_double(),\n  X5 = col_double(),\n  X6 = col_double(),\n  X7 = col_double()\n)\n\ncars0\n\n\n\n  \n\n\n\nThe column names have become X1 through X7. You’ll need to work with these in a minute, so it is good to be aware of that now.\nI used a “temporary” name for my dataframe since we are going to be tidying it before we do anything with it, and I’m saving the “good” name cars for the tidy one.\n\\(\\blacksquare\\)\n\nMake this data set tidy. That is, you need to end up with columns containing the repair cost estimates at each of the two garages and also identifying the cars, with each observation on one row. Describe your thought process. (It needs to be possible for the reader to follow your description and understand why it works.) Save your tidy dataframe.\n\nSolution\nThis looks very far from tidy right now. The things in X2 look like they will need to be variable names eventually, but there are two copies of them, and there are also five columns of data values that need eventually to become three. Having all the data values in one column might be a useful place to start:\n\ncars0 %&gt;% pivot_longer(X3:X7, names_to=\"old_cols\", values_to=\"values\") \n\n\n\n  \n\n\n\nThis is tidier, but it’s now too long: this has 30 rows but there are only 10 cars, or, depending on your point of view, there are 20 observations on 10 individual cars, so you could justify (in some way) having 20 rows, but not 30.\nNow, therefore, we need to pivot wider. But to get to this point, we had to try pivoting longer to see what it did, and then go from there. I don’t think it’s at all obvious that this is what will happen, so I think you need to do a pivot-longer first, talk about it, and then move on.\nFrom here, we want to make columns whose names are the things in X2, and whose values are the things in values. This is exactly what pivot_wider does, so add that to our pipe:\n\ncars0 %&gt;% pivot_longer(X3:X7, names_to=\"names\", values_to=\"values\") %&gt;% \npivot_wider(names_from = X2, values_from = values) -&gt; cars\ncars\n\n\n\n  \n\n\n\nThis is now tidy: one row for each of the 10 cars, one column containing the repair estimates for each car at each of the two garages, and a column identifying the cars. I think this is best because this is a matched-pairs study, and so you want the two measurements for each individual car in columns next to each other (for t.test with paired=TRUE).\nI think it is best to show the whole pipeline here, even though you are making R work a little harder, rather than having to make up a temporary variable name for the output from pivot_longer (that you are never going to look at again after this).\nIf you thought there were 20 observations, you have a bit more work to do (that you will have to undo later to get the right graph), namely:\n\ncars %&gt;% pivot_longer(c(Jocko, Other), names_to=\"garage\", values_to=\"estimate\") -&gt; cars1\ncars1\n\n\n\n  \n\n\n\nThis would be the right thing to do if you had independent observations (that is, 20 different cars, and you randomly choose a garage to send each one to). But you can have a car assessed for repair without actually repairing it, so it makes more sense to send each car to both garages, and compare like with like. Compare the kids learning to read; once a child has learned to read, you can’t teach them to read again, so that study had to be done with two independent samples.\nExtra: I thought about starting by making the dataframe even wider:\n\ncars0 %&gt;% pivot_wider(names_from = X2, values_from = X3:X7)\n\n\n\n  \n\n\n\nThis is sort of the right thing, but there are repeat columns now, depending on where the data values came from in cars0. What we want to do now is some kind of pivot_longer, creating three columns called Car, Jocko, and Other. If we only had one kind of thing to make longer, this would be a standard pivot_longer. But we have three. There are two “extras” to pivot_longer that will get you to the right place. The first one is to give multiple inputs to names_to, because the column names encode two things: where in the original data frame the value came from (which is now junk to us), and what the value actually represents, which we definitely do want to keep. I don’t have a good name for it, though, so I’ll call it z for now. Note that we need a names_sep that says what the two things in the column names are separated by, the underscore that the pivot_wider put in there:\n\ncars0 %&gt;% pivot_wider(names_from = X2, values_from = X3:X7) %&gt;% \npivot_longer(-X1, names_to = c(\"junk\", \"z\"), names_sep=\"_\")\n\n\n\n  \n\n\n\nThis is now exactly what I got by starting with pivot_longer, and so the same pivot_wider that I finished with before will tidy this up:\n\ncars0 %&gt;% pivot_wider(names_from = X2, values_from = X3:X7) %&gt;% \npivot_longer(-X1, names_to = c(\"junk\", \"z\"), names_sep=\"_\") %&gt;% \npivot_wider(names_from = z, values_from = value)\n\n\n\n  \n\n\n\nThis is now tidy, so you have achieved what you set out to do, but you have not done it the best way, so you should expect to lose a little something.\nThis kind of longer-then-wider happens often enough that there is an option in pivot_longer to do it in one step. Let’s remind ourselves of where we were:\n\ncars0 %&gt;% pivot_wider(names_from = X2, values_from = X3:X7) \n\n\n\n  \n\n\n\nThe second part of those funky column names needs to become the names of our new columns. To make that happen in one step, you put the special indicator .value in where we had z before:\n\ncars0 %&gt;% pivot_wider(names_from = X2, values_from = X3:X7) %&gt;% \npivot_longer(-X1, names_to = c(\"junk\", \".value\"), names_sep=\"_\")\n\n\n\n  \n\n\n\nand as if by magic, we have tidiness. It’s best to discover this and do it in two steps, though by starting with pivot_wider you have made it more difficult for yourself. By starting with pivot_longer, it is a very standard longer-then-wider, and there is nothing extra you have to learn. (The column X1 I added to the data so that pivot_wider would work smoothly. See what happens if you remove it with select(-X1) before you start pivoting.)\nThere is usually a relatively simple way to do these, and if your way is complicated, that is an invitation to try it again a different way. I don’t think there’s a way to do it in one step, though, because those things in X2 have to get to column names somehow, and they can only do so by being attached to which original column the values came from.\nAll of these ideas are here, which is a dense read, but worth working through to see what is possible. This problem is of the type in “Longer, then wider”.\n\\(\\blacksquare\\)\n\nMake a suitable graph to assess the comparison of interest, and say briefly what your graph tells you.\n\nSolution\nYou might be tempted to look at cars, see two quantitative variables, and think “scatterplot”:\n\nggplot(cars, aes(x=Jocko, y=Other)) + geom_point()\n\n\n\n\nThis says that a repair that is more expensive at one garage is more expensive at the other as well, which is true, but it’s an answer to the wrong question. We care about whether Jocko’s Garage is more expensive than the other one on the same car. To rescue the scatterplot, you can add the line \\(y=x\\) to the graph and see which side of the line the points are, which you might have to find out how to do:\n\nggplot(cars, aes(x=Jocko, y=Other)) + geom_point() + geom_abline(slope = 1, intercept = 0)\n\n\n\n\nMore of the points are below and to the right of the line, indicating that Jocko’s Garage is typically more expensive (in the cases where the other garage is more expensive, there is not much in it).\nThere is a more direct approach here, based on the idea that a matched pairs test looks at the differences between the two estimates for each car: work out the differences, and make a one-sample plot of them:\n\ncars %&gt;% mutate(diff=Jocko-Other) %&gt;% \nggplot(aes(x=diff)) + geom_histogram(bins = 4)\n\n\n\n\nMost of the differences, this way around, are positive, so the indication is that Jocko’s Garage is indeed more expensive. Don’t have too many bins.\nA one-sample boxplot of the differences would also work:\n\ncars %&gt;% mutate(diff=Jocko-Other) %&gt;% \nggplot(aes(x=1, y=diff)) + geom_boxplot()\n\n\n\n\nThis tells you that at least 75% of the differences are positive.\nIf you ended up with my cars1:\n\ncars1\n\n\n\n  \n\n\n\nthis is “obviously” a boxplot:\n\nggplot(cars1, aes(x=garage, y=estimate)) + geom_boxplot()\n\n\n\n\nexcept that you have not used the fact that each group is measurements on the same 10 cars. Here is a way to rescue that:\n\nggplot(cars1, aes(x=garage, y=estimate, group=Car)) + geom_point() + geom_line()\n\n\n\n\nThe majority of the lines go downhill, so Jocko’s Garage is more expensive most of the time. (The lines are really another way to look at the differences.) This last graph I would be good with, since it shows which pairs of measurements are related because of being on the same cars.\n\\(\\blacksquare\\)\n\nCarry out a test to make an appropriate comparison of the mean estimates. What do you conclude, in the context of the data?\n\nSolution\nComparing means requires the right flavour of \\(t\\)-test, in this case a matched-pairs one, with a one-sided alternative, since we were concerned that the Jocko estimates were bigger. In a matched pairs test, alternative says how the first column you name compares with the other one. If your columns are the opposite way to mine, your alternative needs to be “less”:\n\nwith(cars, t.test(Jocko, Other, paired = TRUE, alternative = \"greater\"))\n\n\n    Paired t-test\n\ndata:  Jocko and Other\nt = 2.8749, df = 9, p-value = 0.009164\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 40.76811      Inf\nsample estimates:\nmean difference \n          112.5 \n\n\nRemember that this flavour of \\(t\\)-test doesn’t take a data=, so you need to use with or dollar signs.\nThe P-value is actually just less than 0.01, so we can definitely conclude that the Jocko estimates are bigger on average.\nIf you calculated the differences earlier, feel free to use them here:\n\ncars %&gt;% mutate(diff=Jocko-Other) %&gt;% \nwith(., t.test(diff, mu=0, alternative = \"greater\"))\n\n\n    One Sample t-test\n\ndata:  diff\nt = 2.8749, df = 9, p-value = 0.009164\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 40.76811      Inf\nsample estimates:\nmean of x \n    112.5 \n\n\nSaving the data frame with the differences in it is probably smart.\nAgain, if you got to cars1, you might think to do this:\n\nt.test(estimate~garage, data=cars1, alternative=\"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  estimate by garage\nt = 0.32056, df = 17.798, p-value = 0.3761\nalternative hypothesis: true difference in means between group Jocko and group Other is greater than 0\n95 percent confidence interval:\n -496.4343       Inf\nsample estimates:\nmean in group Jocko mean in group Other \n             1827.5              1715.0 \n\n\nbut you would be wrong, because the two groups are not independent (they’re the same cars at each garage). You have also lost the significant result, because some of the repairs are more expensive than others (at both garages), and this introduces extra variability that this test does not account for.\nI said to compare the means, so I don’t want a sign test here. If you think we should be doing one, you’ll need to make the case for it properly: first, calculate and plot the differences and make the case that they’re not normal enough. I see left-skewness in the histogram of differences, but personally I don’t find this bad enough to worry about. If you do, make that case (but, a sample of size 10 even from a normal distribution might look this skewed) and then run the right test:\n\ncars %&gt;% mutate(diff=Jocko-Other) %&gt;% \nsign_test(diff, 0)\n\n$above_below\nbelow above \n    2     7 \n\n$p_values\n  alternative    p_value\n1       lower 0.98046875\n2       upper 0.08984375\n3   two-sided 0.17968750\n\n\nThe upper-tail P-value is the one you want (explain why), and this is not quite significant. This is different from the correct \\(t\\)-test for a couple of reasons: there is probably not much power with this small sample, and the two estimates that are higher at the Other garage are not much higher, which the \\(t\\)-test accounts for but the sign test does not.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#tidying-electricity-consumption-1",
    "href": "tidying-data.html#tidying-electricity-consumption-1",
    "title": "15  Tidying data",
    "section": "15.25 Tidying electricity consumption",
    "text": "15.25 Tidying electricity consumption\nHow does the consumption of electricity depend on temperature? To find out, a short-term study was carried out by a utility company based in a certain area. For a period of two years, the average monthly temperature was recorded (in degrees Fahrenheit), the mean daily demand for electricity per household (in kilowatt hours), and the cost per kilowatt hour of electricity for that year (8 cents for the first year and 10 cents for the second, which it will be easiest to treat as categorical).\nThe data were laid out in an odd way, as shown in http://ritsokiguess.site/datafiles/utils.txt, in aligned columns: the twelve months of temperature were laid out on two lines for the first year, then the twelve months of consumption for the first year on the next two lines, and then four more lines for the second year laid out the same way. Thus the temperature of 31 in the first line goes with the consumption of 55 in the third line, and the last measurements for that year are the 78 at the end of the second line (temperature) and 73 at the end of the fourth line (consumption). Lines 5 through 8 of the data file are the same thing for the second year (when electricity was more expensive).\nThe data seem to have been laid out in order of temperature, rather than in order of months, which I would have thought would make more sense. But this is what we have.\n\nRead in and display the data file, bearing in mind that it has no column names.\n\nSolution\nThat means col_names = FALSE when reading in. I gave this a “disposable” name, saving the good name utils for the tidy version:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/utils.txt\"\nutils0 &lt;- read_table(my_url, col_names = FALSE)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  X1 = col_character(),\n  X2 = col_character(),\n  X3 = col_double(),\n  X4 = col_double(),\n  X5 = col_double(),\n  X6 = col_double(),\n  X7 = col_double(),\n  X8 = col_double()\n)\n\nutils0\n\n\n\n  \n\n\n\nThe columns have acquired names X1 through X8. It doesn’t really matter what these names are, but as we will see shortly, it matters that they have names.\n\\(\\blacksquare\\)\n\nArrange these data tidily, so that there is a column of price (per kilowatt hour), a column of temperatures, and a column of consumptions. Describe your process, including why you got list-columns (if you did) and what you did about them (if necessary).\n\nSolution\nThis question is asking about your process as well as your answer, so I think it’s best to build a pipeline one step at a time (which corresponds in any case to how you would figure out what to do). The first step seems to be to make longer, for example getting all those numbers in one column. I’m not quite sure what to call the new columns, so I’ll make up some names and figure things out later:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") \n\n\n\n  \n\n\n\nIf you scroll down, X2 has consumptions as well as temperatures, so we need to get that straightened out.\nThis, so far, is actually a lot like the weather one in lecture (where we had a max and a min temperature), and the solution is the same: follow up with a pivot_wider to get the temperatures and consumptions in their own columns:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \npivot_wider(names_from = X2, values_from = value) \n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} %&gt;%\n  dplyr::group_by(X1, col, X2) %&gt;%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n  dplyr::filter(n &gt; 1L)\n\n\n\n\n  \n\n\n\nExcept that it didn’t appear to work. Although it actually did. These are list-columns. I actually recorded lecture 14a to help you with this. (See also the discussion in Extra 3 of the last part of the writers question, and the word “list” at the top of temperature and consumption). Each cell holds two numbers instead of the one you were probably expecting.\nWhy did that happen? The warning above the output is a clue. Something is going to be “not uniquely identified”. Think about how pivot_wider works. It has to decide which row and column of the wider dataframe to put each value in. The column comes from the names_from: temperature or consumption. So that’s not a problem. The row comes from the combination of the other columns not named in the pivot_wider: that means the ones called X1 and col. (Another way to see that is the columns in the result from the pivot_wider that do not have values in them: not temperature or consumption, the other two.)\nIf you look back at the things in col, they go from X3 to X8, so there are six of them. There are two values in X1, so there are \\(2 \\times 6 = 12\\) combinations of the two, and so 12 rows in the wider dataframe. This has two columns, and thus \\(12 \\times 2 = 24\\) cells altogether. But there were 48 values in the longer dataframe (go back and look: it has 48 rows), so there isn’t enough room for all of them here.\nIf you go back and look at the longer dataframe, you’ll see, for example, that there are two temperature values that go with an X1 of 8 cents and a col of X3, so that they will both have to be jammed into one cell of the wider dataframe.\nThe resolution of the list-columns here is the same as in the one about the writers: unnest them, and then you can ignore the warning:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \npivot_wider(names_from = X2, values_from = value) %&gt;% \nunnest(c(temperature, consumption)) -&gt; utils\n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} %&gt;%\n  dplyr::group_by(X1, col, X2) %&gt;%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n  dplyr::filter(n &gt; 1L)\n\nutils\n\n\n\n  \n\n\n\nThere were 24 months of data, and a temperature and consumption for each, so this is now tidy and I can give it a proper name.\nExtra: if you got to here and got scared:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \npivot_wider(names_from = X2, values_from = value) \n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} %&gt;%\n  dplyr::group_by(X1, col, X2) %&gt;%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n  dplyr::filter(n &gt; 1L)\n\n\n\n\n  \n\n\n\nwhich is an entirely reasonable reaction, you might have asked yourself how you could have prevented this from happening. The problem, as discussed earlier, is with the rows, and that the X1-col combinations repeat. Let’s go back to “longer”:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") \n\n\n\n  \n\n\n\nRows 1 and 7, 2 and 8, etc, are “replicates” in that they have the same X1 and col values but different temperatures. This is because they come from the same column in the original layout of the data (the 31 and the 62 are underneath each other). This means that the first six rows are “replicate 1” and the next six are “replicate 2”. Scrolling down, we then get to 8 cents and consumption, and we need to do the same again. So if we make a column that has 1s and 2s in the right places (six 1s, six 2s, repeat), we should then have unique rows for the pivot_wider.\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \nmutate(replicate = rep(1:2, each = 6, length.out = 48))\n\n\n\n  \n\n\n\nrep does repeats like this: something to repeat (the numbers 1 through 2), how many times to repeat each one (six times), and how long the final thing has to be (48, since there were 48 rows in the longer dataframe).\nThen, this time, if we do the pivot_wider, it should give us something tidy:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \nmutate(replicate = rep(1:2, each = 6, length.out = 48)) %&gt;% \npivot_wider(names_from = X2, values_from = value) \n\n\n\n  \n\n\n\nand so it does, with 24 rows for the 24 months.\nAnother, perhaps easier, way to think about this (you might find it easier, anyway) is to go back to the original dataframe and make the replicate there:\n\nutils0\n\n\n\n  \n\n\n\nThe first two rows are replicates (both 8 cents and temperature), then the third and fourth, and so on. So setting a replicate column as 1, 2, 1, 2 etc should do it, and this is short enough to type directly. Do this first, then the pivot_longer, then the pivot_wider as we did before, and we should end up with something tidy:\n\nutils0 %&gt;% mutate(replicate = c(1,2,1,2,1,2,1,2)) %&gt;% \npivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \npivot_wider(names_from = X2, values_from = value) %&gt;% \nunnest(c(temperature, consumption)) \n\n\n\n  \n\n\n\nIf you check this, you’ll see that replicate gets turned into the same thing in the longer dataframe that we had earlier, so you can do it either way.\nThe moral of the story is that when you are planning to do a pivot-wider, you ought to devote some attention to which rows things are going into. Sometimes you can get away with just doing it and it works, but thinking about rows is how to diagnose it when it doesn’t. (The ideas below also appear in Lecture 14a.) Here’s another mini-example where the observations are matched pairs but they come to us long, like two-sample data:\n\nd &lt;- tribble(\n~obs, ~y, ~time,\n1, 10, \"pre\",\n2, 13, \"post\",\n3, 12, \"pre\",\n4, 14, \"post\",\n5, 13, \"pre\",\n6, 15, \"post\"\n)\nd %&gt;% pivot_wider(names_from = time, values_from = y)\n\n\n\n  \n\n\n\nOh. The columns are all right, but the rows certainly are not.\nThe problem is that the only thing left after y and time have been used in the pivot_wider is the column obs, and there are six values there, so there are six rows. This is, in a way, the opposite of the problem we had before; now, there is not enough data to fill the twelve cells of the wider dataframe. For example, there is no pre measurement in the row where obs is 2, so this cell of the wider dataframe is empty: it has a missing value in it.\nThe problem is that the obs column numbered the six observations 1 through 6, but really they are three groups of two observations on three people, so instead of obs we need a column called person that shows which observations are the matched pairs, like this:\n\nd &lt;- tribble(\n~person, ~y, ~time,\n1, 10, \"pre\",\n1, 13, \"post\",\n2, 12, \"pre\",\n2, 14, \"post\",\n3, 13, \"pre\",\n3, 15, \"post\"\n)\n\nNow there are going to be three rows with a pre and a post in each:\n\nd %&gt;% pivot_wider(names_from = time, values_from = y)\n\n\n\n  \n\n\n\npivot_wider requires more thinking than pivot_longer, and when it does something mysterious, that’s when you need to have some understanding of how it works, so that you can fix things up.\n\\(\\blacksquare\\)\n\nMake a suitable graph of temperature, consumption and price in your tidy dataframe. Add smooth trends if appropriate. If you were unable to get the data tidy, use my tidy version here. (If you need the other file, right-click on “here” and Copy Link Address.)\n\nSolution\nI said earlier to treat price (rather badly labelled as X1) as categorical, so we have two quantitative variables and one categorical. This suggests a scatterplot with the two prices distinguished by colours. (We don’t have a mechanism for making three-dimensional plots, and in any case if you have a quantitative variable with not that many distinct different values, you can often treat that as categorical, such as price here.)\nBefore we make a graph, though, we should rename X1. The way you might think of is to create a new column with the same values as X1, but a new name.30 Like this. Consumption is the outcome, so it goes on the \\(y\\)-axis:\n\nutils %&gt;% \nmutate(price = X1) %&gt;% \nggplot(aes(x = temperature, y = consumption, colour = price)) + \ngeom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI said smooth trends rather than lines, because you don’t know until you draw the graph whether the trends are lines. If they’re not, there’s not much point in drawing lines through them. These ones are rather clearly curves, which we take up in the next part.\nIf you fail to rename X1, that’s what will appear on the legend, and the first thing your reader would ask is “what is X1?” When writing, you need to think of your reader, since they are (in the real world) paying you for your work.\nExtra: there is an official rename also. I haven’t used that in class, so if you discover this, make sure to say where you found out about it from:\n\nutils %&gt;% \nrename(price = X1)\n\n\n\n  \n\n\n\nThe syntax is “new name equals old one”. I used to think it was something like “take the column called X1 and rename it to price”, but as you see, that’s exactly backwards. The English-language version is “create a new column called price from the column previously called X1”.\n\\(\\blacksquare\\)\n\nWhat patterns or trends do you see on your graph? Do they make practical sense? There are two things I would like you to comment on.\n\nSolution\nThe two things are:\n\nthe relationships are both curves, going down and then up again.\nthe blue curve is above the red one.\n\nIf the temperature is low (30 degrees F is just below freezing), people will need to heat their houses, and the electricity consumption to do this is reflected in the curves being higher at the left. (Not all people have electric heating, but at least some people do, and electric heating uses a lot of electricity.) When the temperature is high, people will turn on the air-conditioning (which is usually electric), and that explains the sharp increase in consumption at high temperatures. In between is the zone where the house stays a good temperature without heating or cooling.\nSo why is the blue curve above the red one? This is saying that when electricity is cheaper, people will use more of it. (This seems to be particularly true when the temperature is high; people might “crank” the air-conditioning if it doesn’t cost too much to run.) Conversely, if electricity is more expensive, people might be more thoughtful about what temperature to turn on the heating or AC. (For example, in summer you might keep the drapes closed so that it stays cooler inside without needing to use the AC so much.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#tidy-blood-pressure-1",
    "href": "tidying-data.html#tidy-blood-pressure-1",
    "title": "15  Tidying data",
    "section": "15.26 Tidy blood pressure",
    "text": "15.26 Tidy blood pressure\nGoing to the dentist is scary for a lot of people. One way in which this might show up is that people might have higher blood pressure on average before their dentist’s appointment than an hour after the appointment is done. Ten randomly-chosen individuals have their (systolic31) blood pressure measured while they are in a dentist’s waiting room, and then again one hour after their appointment is finished.\nYou might have seen a tidy version of this data set before.\nThe data as I originally received it is in http://ritsokiguess.site/datafiles/blood_pressure2.csv.\n\nRead in and display the data as originally received.\n\nSolution\nYou ought to be suspicious that something is going to be up with the layout. With that in mind, I’m using a “disposable” name for this dataframe:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/blood_pressure2.csv\"\nbp0 &lt;- read_csv(my_url)\n\nRows: 2 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): time\ndbl (10): p1, p2, p3, p4, p5, p6, p7, p8, p9, p10\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbp0\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nDescribe briefly how the data you read in is not tidy, bearing in mind how the data were collected and how they would be analysed.\n\nSolution\nIn short, the things that are rows should be columns, and the things that are columns should be rows. Or, the individuals (people, here) should be in rows but they are in columns. Or, the variables (time points) should be in columns but they are in rows. One of those.\nAnother way to go at it is to note that the numbers are all blood pressure measurements, and so should be all in one column, labelled by which time and individual they belong to. This is, however, not quite right, for reasons of how the data were collected. They are pairs of measurements on the same individual, and so there should be (for something like a matched pairs \\(t\\)-test) a column of before measurements and a column of after measurements. This will mean some extra work in the next part to get it tidy.\n\\(\\blacksquare\\)\n\nProduce a tidy dataframe from the one you read in from the file. (How many rows should you have?)\n\nSolution\nThe usual starting point for these is to get all the measurements into one column and see what to do after that. This is pivot_longer:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") \n\n\n\n  \n\n\n\nThis would be tidy if we had 20 independent observations from 20 different people. But we don’t. We only have 10 people, with two measurements on each, so we should only have 10 rows. Having made things longer, they are now too long, and we have to make it wider again.\nWe want to have a column of before and a column of after, so the names of the new columns are coming from what I called time. The values are coming from what I called bp, so, gluing the pivot_wider on the end of the pipe:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %&gt;% \n  pivot_wider(names_from = time, values_from = bp) -&gt; blood_pressure\nblood_pressure\n\n\n\n  \n\n\n\nThis is now tidy, so I gave it a “permanent” name.\nI laid out the steps of my thinking, so you could follow my logic. I’m expecting your thinking to be about the same as mine, but the work you hand in can certainly be the finished pipe I had just above, as if you thought of it right away.\nExtra: pivot_wider is smarter than you think, but it can be helpful to know what it does, in order to help diagnose when things go wrong. Let’s go back and look at the too-long dataframe again:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") \n\n\n\n  \n\n\n\nEach one of those values in bp has to go somewhere in the wider dataframe. In particular, it needs to go in a particular row and column. The column is pretty obvious: the column whose name is in the time column. But the row is much less obvious. How does pivot_wider figure it out? Well, it looks for all combinations of values in the other columns, the ones not mentioned in the pivot_wider, and makes a row for each of those. In this case, the only other column is person, so it makes one row for each person. Since there is one before and one after measurement for each person, everything works smoothly.\nThis enables us to try a couple of what-ifs to see what can go wrong.\nFirst, what if there’s no person column at all, so there is nothing to say what row an observation should go in?\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %&gt;% \nselect(-person) %&gt;% \npivot_wider(names_from = time, values_from = bp)\n\nWarning: Values from `bp` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} %&gt;%\n  dplyr::group_by(time) %&gt;%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n  dplyr::filter(n &gt; 1L)\n\n\n\n\n  \n\n\n\nIt kinda works, but with a warning. The warning says “values are not uniquely identified”, which is a posh way to say that it doesn’t know where to put them (because there is no longer a way to say which row each observation should go in).\nHere’s another one, similar:\n\nd &lt;- tribble(\n~g, ~id, ~y,\n\"a\", \"p1\", 10,\n\"a\", \"p2\", 11,\n\"b\", \"p1\", 12,\n\"b\", \"p2\", 13,\n\"a\", \"p3\", 14,\n\"a\", \"p1\", 15\n)\nd\n\n\n\n  \n\n\n\nWhen we do this:\n\nd %&gt;% pivot_wider(names_from = g, values_from = y)\n\nWarning: Values from `y` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} %&gt;%\n  dplyr::group_by(id, g) %&gt;%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n  dplyr::filter(n &gt; 1L)\n\n\n\n\n  \n\n\n\nwe get list-columns again (and the same warning). What this output is telling you is that mostly there is one number per id-group combination (the dbl[1]) but there are two observations labelled id p1 and group a, and no observations at all labelled id p3 and group b. It turns out^[I know because I made these data up. that the last row of the tribble contains errors. Fix them, and all is good:\n\nd &lt;- tribble(\n~g, ~id, ~y,\n\"a\", \"p1\", 10,\n\"a\", \"p2\", 11,\n\"b\", \"p1\", 12,\n\"b\", \"p2\", 13,\n\"a\", \"p3\", 14,\n\"b\", \"p3\", 15\n)\nd %&gt;% pivot_wider(names_from = g, values_from = y)\n\n\n\n  \n\n\n\nOne last one:\n\nd &lt;- tribble(\n~id, ~g, ~y,\n1, \"a\", 10,\n2, \"a\", 11,\n3, \"a\", 12,\n4, \"b\", 13,\n5, \"b\", 14,\n6, \"b\", 15\n)\nd\n\n\n\n  \n\n\n\nand then\n\nd %&gt;% pivot_wider(names_from = g, values_from = y)\n\n\n\n  \n\n\n\nWhere did those missing values come from? If you go back and look at this d, you’ll see that each person has only one measurement, either an a or a b, not both. There is, for example, nothing to go in the a column for person number 4, because their only measurement was in group b. This kind of thing happens with two independent samples, and is a warning that you don’t need to pivot wider; it’s already tidy:\n\nd\n\n\n\n  \n\n\n\nThink about the kind of layout you need for a two-sample \\(t\\)-test.\n\\(\\blacksquare\\)\n\nWhat kind of test might you run on these data? Explain briefly.\n\nSolution\nThis is a matched-pairs experiment, so it needs a matched-pairs analysis. This could be a matched-pairs \\(t\\)-test, or a sign test on the differences (testing that the population median difference is zero). You can suggest either, since we haven’t drawn any graphs yet, but “sign test” is not enough; you need to say something about what kind of sign test. (It’s actually quicker to answer “matched-pairs \\(t\\)-test” since you don’t need any more detail than that.)\n\\(\\blacksquare\\)\n\nDraw a suitable graph of these data.\n\nSolution\nGiven that we are going to do a matched-pairs analysis of some kind, the best graph looks at the differences between the two measurements. So calculate them first, and then make a one-sample plot of them, such as a histogram:\n\nblood_pressure %&gt;% mutate(diff = before - after) %&gt;% \nggplot(aes(x=diff)) + geom_histogram(bins=5)\n\n\n\n\nYou will need a suitably small number of bins, since we only have ten observations. You can take the differences the other way around if you prefer; they will then be mostly negative.\nYou might have looked at the two quantitative columns and thought “scatterplot”:\n\nggplot(blood_pressure, aes(x=before, y=after)) + geom_point()\n\n\n\n\nThis says that if the blood pressure before was large, the blood pressure afterwards is as well. This is fair enough, but it is the answer to a question we don’t care about. What we do care about is whether the after measurement is bigger than the before one for the same person, which this graph does not show. So this is not the best.\nTo rescue this graph, you can add the line \\(y=x\\) to it. The value of this is that a point above this line has the after measurement bigger than the corresponding before one, and a point below the line has the after measurement smaller. You will need to find out how to add a line with a given intercept and slope to the plot, since I haven’t shown you how to do it. It’s called geom_abline, thus:\n\nggplot(blood_pressure, aes(x=before, y=after)) + geom_point() +\ngeom_abline(intercept = 0, slope = 1)\n\n\n\n\nThis is insightful, because most of the points are below the line, so that most of the before measurements were bigger than the corresponding after ones.\nNote that if you put a regression line on your plot, you will need to offer a convincing explanation of why that offers insight, which I think you will find difficult.\nFinally, if you thought the long data frame was tidy, this one:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") \n\n\n\n  \n\n\n\nthen you can rescue some points here by making a suitable plot of that. A boxplot is not enough:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %&gt;% \nggplot(aes(x=time, y=bp)) + geom_boxplot()\n\n\n\n\nbecause you have lost the connection between the two measurements for each person. To keep that connection, start with the same plot but as points rather than boxes:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %&gt;% \nggplot(aes(x=time, y=bp)) + geom_point()\n\n\n\n\nand then join the two points that belong to the same person. This is done with geom_line as usual, only you have to say which points are going to be joined, namely the two for each person, and to do that, you specify person in group:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %&gt;% \nggplot(aes(x=time, y=bp, group=person)) + geom_point() + geom_line()\n\n\n\n\nMost of the lines are going uphill, so most of the after measurements are less than the corresponding before ones.32\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#footnotes",
    "href": "tidying-data.html#footnotes",
    "title": "15  Tidying data",
    "section": "",
    "text": "You can try it without. See below.↩︎\nYou could just as well make the point that the text 20.8 contains the number 20.8 and nothing else, so that parsing it as text in search of a number will pull out 20.8 as a number. If that logic works for you, go with it.↩︎\nOh, what a giveaway.↩︎\nUnlike thunderstorm watch and thunderstorm warning, which mean different things.↩︎\nEvidently the units were chosen for ease of recording; had the values been in grams instead, the person recording the data would have had to put a 0 and a decimal point on the front of each value. This is the old meaning of the word “coding”; making the data values be whole numbers and/or small deviations from something makes them easier to record, and in pre-computer days easier to calculate with. You will also see the same word used for classifying survey responses into categories, which is similar but not quite the same thing.↩︎\nA blood pressure is usually given as two numbers, like “120 over 80”. The first number, which is the one shown in our data, is called the systolic blood pressure. It is the pressure in the arteries when the heart is pumping. The second is called the diastolic blood pressure, and it is the pressure in the arteries when the heart is resting.↩︎\nYou can try it without. See below.↩︎\nSometimes the column playing the role of rep is interesting to us, but not here.↩︎\nTo allow for the fact that measurements on the same subject are not independent but correlated.↩︎\nAnd then thrown away.↩︎\nIt needs print around it to display it, as you need print to display something within a loop or a function.↩︎\nThis talks about means rather than individual observations; in individual cases, sometimes even drug A will come out best. But we’re interested in population means, since we want to do the greatest good for the greatest number. “Greatest good for the greatest number” is from Jeremy Bentham, 1748–1832, British philosopher and advocate of utilitarianism.↩︎\nAs str_c or paste do, actually, but the advantage of unite is that it gets rid of the other columns, which you probably no longer need.↩︎\nYou could just as well make the point that the text 20.8 contains the number 20.8 and nothing else, so that parsing it as text in search of a number will pull out 20.8 as a number. If that logic works for you, go with it.↩︎\nYou might think that missing is just missing, but R distinguishes between types of missing.↩︎\nWhich was the title of a song by Prince.↩︎\nAs, for example, when Prince died.↩︎\nOh, what a giveaway.↩︎\nIn some languages it is called switch. Python appears not to have it. What you do there instead is to use a Python dictionary to pick out the value you want.↩︎\nIf I was helping you, and you were struggling with ifelse but finally mastered it, it seemed easier to suggest that you used it again for the others.↩︎\nBut I didn’t want to complicate this question any farther.↩︎\nUnlike thunderstorm watch and thunderstorm warning, which mean different things.↩︎\nI did not know until just now that you could put two variables in a count and you get counts of all the combinations of them. Just goes to show the value of “try it and see”.↩︎\nI looked it up. It was 2003, my first summer in Ontario. I realize as I write this that you may not be old enough to remember these years. Sometimes I forget how old I am.↩︎\nThis is the same kind of thing as a “dictionary” in Python.↩︎\nEvidently the units were chosen for ease of recording; had the values been in grams instead, the person recording the data would have had to put a 0 and a decimal point on the front of each value. This is the old meaning of the word coding; making the data values be whole numbers and/or small deviations from something makes them easier to record, and in pre-computer days easier to calculate with. You will also see the same word used for classifying survey responses into categories, which is similar but not quite the same thing.↩︎\nIt shouldn’t be, but it often is.↩︎\nData tidying has a lot of this kind of thing: try something, see that it doesn’t work, figure out what went wrong, fix that, repeat. The work you hand in, or show to your boss, won’t necessarily look very much like your actual process.↩︎\nThis is the opposite way to the usual: when two tests disagree, it is usually the one with fewer assumptions that is preferred, but in this case, the Welch ANOVA is fine, and the median test fails to give significance because it is not using the data as efficiently.↩︎\nThis actually creates a copy of the original column, so if you look you now have two columns with the same thing in them, one with a bad name and one with a good one.↩︎\nA blood pressure is usually given as two numbers, like “120 over 80”. The first number, which is the one shown in our data, is called the systolic blood pressure. It is the pressure in the arteries when the heart is pumping. The second is called the diastolic blood pressure, and it is the pressure in the arteries when the heart is resting.↩︎\nThis is known in the trade as a spaghetti plot because the lines resemble strands of spaghetti.↩︎"
  },
  {
    "objectID": "simple-regression.html#rainfall-in-california",
    "href": "simple-regression.html#rainfall-in-california",
    "title": "16  Simple regression",
    "section": "16.1 Rainfall in California",
    "text": "16.1 Rainfall in California\nThe data in link are rainfall and other measurements for 30 weather stations in California. Our aim is to understand how well the annual rainfall at these stations (measured in inches) can be predicted from the other measurements, which are the altitude (in feet above sea level), the latitude (degrees north of the equator) and the distance from the coast (in miles).\n\nRead the data into R. You’ll have to be careful here, since the values are space-delimited, but sometimes by more than one space, to make the columns line up. read_table2, with filename or url, will read it in. One of the variables is called rainfall, so as long as you do not call the data frame that, you should be safe.\nMake a boxplot of the rainfall figures, and explain why the values are reasonable. (A rainfall cannot be negative, and it is unusual for a annual rainfall to exceed 60 inches.) A ggplot boxplot needs something on the \\(x\\)-axis: the number 1 will do.\nPlot rainfall against each of the other quantitative variables (that is, not station).\nLook at the relationship of each other variable with rainfall. Justify the assertion that latitude seems most strongly related with rainfall. Is that relationship positive or negative? linear? Explain briefly.\nFit a regression with rainfall as the response variable, and latitude as your explanatory variable. What are the intercept, slope and R-squared values? Is there a significant relationship between rainfall and your explanatory variable? What does that mean?\nFit a multiple regression predicting rainfall from all three of the other (quantitative) variables. Display the results. Comment is coming up later.\nWhat is the R-squared for the regression of the last part? How does that compare with the R-squared of your regression in part (e)?\nWhat do you conclude about the importance of the variables that you did not include in your model in (e)? Explain briefly.\nMake a suitable hypothesis test that the variables altitude and fromcoast significantly improve the prediction of rainfall over the use of latitude alone. What do you conclude?"
  },
  {
    "objectID": "simple-regression.html#carbon-monoxide-in-cigarettes",
    "href": "simple-regression.html#carbon-monoxide-in-cigarettes",
    "title": "16  Simple regression",
    "section": "16.2 Carbon monoxide in cigarettes",
    "text": "16.2 Carbon monoxide in cigarettes\nThe (US) Federal Trade Commission assesses cigarettes according to their tar, nicotine and carbon monoxide contents. In a particular year, 25 brands were assessed. For each brand, the tar, nicotine and carbon monoxide (all in milligrams) were measured, along with the weight in grams. Our aim is to predict carbon monoxide from any or all of the other variables. The data are in link. These are aligned by column (except for the variable names), with more than one space between each column of data.\n\nRead the data into R, and check that you have 25 observations and 4 variables.\nRun a regression to predict carbon monoxide from the other variables, and obtain a summary of the output.\nWhich one of your explanatory variables would you remove from this regression? Explain (very) briefly. Go ahead and fit the regression without it, and describe how the change in R-squared from the regression in (b) was entirely predictable.\nFit a regression predicting carbon monoxide from nicotine only, and display the summary.\nnicotine was far from being significant in the model of (c), and yet in the model of (d), it was strongly significant, and the R-squared value of (d) was almost as high as that of (c). What does this say about the importance of nicotine as an explanatory variable? Explain, as briefly as you can manage.\nMake a “pairs plot”: that is, scatter plots between all pairs of variables. This can be done by feeding the whole data frame into plot.1 Do you see any strong relationships that do not include co? Does that shed any light on the last part? Explain briefly (or “at length” if that’s how it comes out)."
  },
  {
    "objectID": "simple-regression.html#maximal-oxygen-uptake-in-young-boys",
    "href": "simple-regression.html#maximal-oxygen-uptake-in-young-boys",
    "title": "16  Simple regression",
    "section": "16.3 Maximal oxygen uptake in young boys",
    "text": "16.3 Maximal oxygen uptake in young boys\nA physiologist wanted to understand the relationship between physical characteristics of pre-adolescent boys and their maximal oxygen uptake (millilitres of oxygen per kilogram of body weight). The data are in link for a random sample of 10 pre-adolescent boys. The variables are (with units):\n\nuptake: Oxygen uptake (millitres of oxygen per kilogram of body weight)\nage: boy’s age (years)\nheight: boy’s height (cm)\nweight: boy’s weight (kg)\nchest: chest depth (cm).\n\n\nRead the data into R and confirm that you do indeed have 10 observations.\nFit a regression predicting oxygen uptake from all the other variables, and display the results.\n(A one-mark question.) Would you say, on the evidence so far, that the regression fits well or badly? Explain (very) briefly.\nIt seems reasonable that an older boy should have a greater oxygen uptake, all else being equal. Is this supported by your output? Explain briefly.\nIt seems reasonable that a boy with larger weight should have larger lungs and thus a statistically significantly larger oxygen uptake. Is that what happens here? Explain briefly.\nFit a model that contains only the significant explanatory variables from your first regression. How do the R-squared values from the two regressions compare? (The last sentence asks for more or less the same thing as the next part. Answer it either here or there. Either place is good.)\nHow has R-squared changed between your two regressions? Describe what you see in a few words.\nCarry out a test comparing the fit of your two regression models. What do you conclude, and therefore what recommendation would you make about the regression that would be preferred?\nObtain a table of correlations between all the variables in the data frame. Do this by feeding the whole data frame into cor. We found that a regression predicting oxygen uptake from just height was acceptably good. What does your table of correlations say about why that is? (Hint: look for all the correlations that are large.)"
  },
  {
    "objectID": "simple-regression.html#facebook-friends-and-grey-matter",
    "href": "simple-regression.html#facebook-friends-and-grey-matter",
    "title": "16  Simple regression",
    "section": "16.4 Facebook friends and grey matter",
    "text": "16.4 Facebook friends and grey matter\nIs there a relationship between the number of Facebook friends a person has, and the density of grey matter in the areas of the brain associated with social perception and associative memory? To find out, a 2012 study measured both of these variables for a sample of 40 students at City University in London (England). The data are at link. The grey matter density is on a \\(z\\)-score standardized scale. The values are separated by tabs.\nThe aim of this question is to produce an R Markdown report that contains your answers to the questions below.\nYou should aim to make your report flow smoothly, so that it would be pleasant for a grader to read, and can stand on its own as an analysis (rather than just being the answer to a question that I set you). Some suggestions: give your report a title and arrange it into sections with an Introduction; add a small amount of additional text here and there explaining what you are doing and why. I don’t expect you to spend a large amount of time on this, but I do hope you will make some effort. (My report came out to 4 Word pages.)\n\nRead in the data and make a scatterplot for predicting the number of Facebook friends from the grey matter density. On your scatterplot, add a smooth trend.\nDescribe what you see on your scatterplot: is there a trend, and if so, what kind of trend is it? (Don’t get too taken in by the exact shape of your smooth trend.) Think “form, direction, strength”.\nFit a regression predicting the number of Facebook friends from the grey matter density, and display the output.\nIs the slope of your regression line significantly different from zero? What does that mean, in the context of the data?\nAre you surprised by the results of parts (b) and (d)? Explain briefly.\nObtain a scatterplot with the regression line on it.\nObtain a plot of the residuals from the regression against the fitted values, and comment briefly on it."
  },
  {
    "objectID": "simple-regression.html#endogenous-nitrogen-excretion-in-carp",
    "href": "simple-regression.html#endogenous-nitrogen-excretion-in-carp",
    "title": "16  Simple regression",
    "section": "16.5 Endogenous nitrogen excretion in carp",
    "text": "16.5 Endogenous nitrogen excretion in carp\nA paper in Fisheries Science reported on variables that affect “endogenous nitrogen excretion” or ENE in carp raised in Japan. A number of carp were divided into groups based on body weight, and each group was placed in a different tank. The mean body weight of the carp placed in each tank was recorded. The carp were then fed a protein-free diet three times daily for a period of 20 days. At the end of the experiment, the amount of ENE in each tank was measured, in milligrams of total fish body weight per day. (Thus it should not matter that some of the tanks had more fish than others, because the scaling is done properly.)\nFor this question, write a report in R Markdown that answers the questions below and contains some narrative that describes your analysis. Create an HTML document from your R Markdown.\n\nRead the data in from link. There are 10 tanks.\nCreate a scatterplot of ENE (response) against bodyweight (explanatory). Add a smooth trend to your plot.\nIs there an upward or downward trend (or neither)? Is the relationship a line or a curve? Explain briefly.\nFit a straight line to the data, and obtain the R-squared for the regression.\nObtain a residual plot (residuals against fitted values) for this regression. Do you see any problems? If so, what does that tell you about the relationship in the data?\nFit a parabola to the data (that is, including an \\(x\\)-squared term). Compare the R-squared values for the models in this part and part (d). Does that suggest that the parabola model is an improvement here over the linear model?\nIs the test for the slope coefficient for the squared term significant? What does this mean?\nMake the scatterplot of part (b), but add the fitted curve. Describe any way in which the curve fails to fit well.\nObtain a residual plot for the parabola model. Do you see any problems with it? (If you do, I’m not asking you to do anything about them in this question, but I will.)"
  },
  {
    "objectID": "simple-regression.html#salaries-of-social-workers",
    "href": "simple-regression.html#salaries-of-social-workers",
    "title": "16  Simple regression",
    "section": "16.6 Salaries of social workers",
    "text": "16.6 Salaries of social workers\nAnother salary-prediction question: does the number of years of work experience that a social worker has help to predict their salary? Data for 50 social workers are in link.\n\nRead the data into R. Check that you have 50 observations on two variables. Also do something to check that the years of experience and annual salary figures look reasonable overall.\nMake a scatterplot showing how salary depends on experience. Does the nature of the trend make sense?\nFit a regression predicting salary from experience, and display the results. Is the slope positive or negative? Does that make sense?\nObtain and plot the residuals against the fitted values. What problem do you see?\nThe problem you unearthed in the previous part is often helped by a transformation. Run Box-Cox on your data to find a suitable transformation. What transformation is suggested?\nCalculate a new variable as suggested by your transformation. Use your transformed response in a regression, showing the summary.\nObtain and plot the residuals against the fitted values for this regression. Do you seem to have solved the problem with the previous residual plot?"
  },
  {
    "objectID": "simple-regression.html#predicting-volume-of-wood-in-pine-trees",
    "href": "simple-regression.html#predicting-volume-of-wood-in-pine-trees",
    "title": "16  Simple regression",
    "section": "16.7 Predicting volume of wood in pine trees",
    "text": "16.7 Predicting volume of wood in pine trees\nIn forestry, the financial value of a tree is the volume of wood that it contains. This is difficult to estimate while the tree is still standing, but the diameter is easy to measure with a tape measure (to measure the circumference) and a calculation involving \\(\\pi\\), assuming that the cross-section of the tree is at least approximately circular. The standard measurement is “diameter at breast height” (that is, at the height of a human breast or chest), defined as being 4.5 feet above the ground.\nSeveral pine trees had their diameter measured shortly before being cut down, and for each tree, the volume of wood was recorded. The data are in link. The diameter is in inches and the volume is in cubic inches. Is it possible to predict the volume of wood from the diameter?\n\nRead the data into R and display the values (there are not very many).\nMake a suitable plot.\nDescribe what you learn from your plot about the relationship between diameter and volume, if anything.\nFit a (linear) regression, predicting volume from diameter, and obtain the summary. How would you describe the R-squared?\nDraw a graph that will help you decide whether you trust the linearity of this regression. What do you conclude? Explain briefly.\nWhat would you guess would be the volume of a tree of diameter zero? Is that what the regression predicts? Explain briefly.\nA simple way of modelling a tree’s shape is to pretend it is a cone, like this, but probably taller and skinnier:\n\n\nwith its base on the ground. What is the relationship between the diameter (at the base) and volume of a cone? (If you don’t remember, look it up. You’ll probably get a formula in terms of the radius, which you’ll have to convert. Cite the website you used.)\n\nFit a regression model that predicts volume from diameter according to the formula you obtained in the previous part. You can assume that the trees in this data set are of similar heights, so that the height can be treated as a constant.\nDisplay the results."
  },
  {
    "objectID": "simple-regression.html#tortoise-shells-and-eggs",
    "href": "simple-regression.html#tortoise-shells-and-eggs",
    "title": "16  Simple regression",
    "section": "16.8 Tortoise shells and eggs",
    "text": "16.8 Tortoise shells and eggs\nA biologist measured the length of the carapace (shell) of female tortoises, and then x-rayed the tortoises to count how many eggs they were carrying. The length is measured in millimetres. The data are in link. The biologist is wondering what kind of relationship, if any, there is between the carapace length (as an explanatory variable) and the number of eggs (as a response variable).\n\nRead in the data, and check that your values look reasonable.\nObtain a scatterplot, with a smooth trend, of the data.\nThe biologist expected that a larger tortoise would be able to carry more eggs. Is that what the scatterplot is suggesting? Explain briefly why or why not.\nFit a straight-line relationship and display the summary.\nAdd a squared term to your regression, fit that and display the summary.\nIs a curve better than a line for these data? Justify your answer in two ways: by comparing a measure of fit, and by doing a suitable test of significance.\nMake a residual plot for the straight line model: that is, plot the residuals against the fitted values. Does this echo your conclusions of the previous part? In what way? Explain briefly."
  },
  {
    "objectID": "simple-regression.html#roller-coasters",
    "href": "simple-regression.html#roller-coasters",
    "title": "16  Simple regression",
    "section": "16.9 Roller coasters",
    "text": "16.9 Roller coasters\nA poll on the Discovery Channel asked people to nominate the best roller-coasters in the United States. We will examine the 10 roller-coasters that received the most votes. Two features of a roller-coaster that are of interest are the distance it drops from start to finish, measured here in feet2 and the duration of the ride, measured in seconds. Is it true that roller-coasters with a bigger drop also tend to have a longer ride? The data are at link.3\n\nRead the data into R and verify that you have a sensible number of rows and columns.\nMake a scatterplot of duration (response) against drop (explanatory), labelling each roller-coaster with its name in such a way that the labels do not overlap. Add a regression line to your plot.\nWould you say that roller-coasters with a larger drop tend to have a longer ride? Explain briefly.\nFind a roller-coaster that is unusual compared to the others. What about its combination of drop and duration is unusual?"
  },
  {
    "objectID": "simple-regression.html#running-and-blood-sugar",
    "href": "simple-regression.html#running-and-blood-sugar",
    "title": "16  Simple regression",
    "section": "16.10 Running and blood sugar",
    "text": "16.10 Running and blood sugar\nA diabetic wants to know how aerobic exercise affects his blood sugar. When his blood sugar reaches 170 (mg/dl), he goes out for a run at a pace of 10 minutes per mile. He runs different distances on different days. Each time he runs, he measures his blood sugar after the run. (The preferred blood sugar level is between 80 and 120 on this scale.) The data are in the file link. Our aim is to predict blood sugar from distance.\n\nRead in the data and display the data frame that you read in.\nMake a scatterplot and add a smooth trend to it.\nWould you say that the relationship between blood sugar and running distance is approximately linear, or not? It is therefore reasonable to use a regression of blood sugar on distance? Explain briefly.\nFit a suitable regression, and obtain the regression output.\nHow would you interpret the slope? That is, what is the slope, and what does that mean about blood sugar and running distance?\nIs there a (statistically) significant relationship between running distance and blood sugar? How do you know? Do you find this surprising, given what you have seen so far? Explain briefly.\nThis diabetic is planning to go for a 3-mile run tomorrow and a 5-mile run the day after. Obtain suitable 95% intervals that say what his blood sugar might be after each of these runs.\nWhich of your two intervals is longer? Does this make sense? Explain briefly."
  },
  {
    "objectID": "simple-regression.html#calories-and-fat-in-pizza",
    "href": "simple-regression.html#calories-and-fat-in-pizza",
    "title": "16  Simple regression",
    "section": "16.11 Calories and fat in pizza",
    "text": "16.11 Calories and fat in pizza\nThe file at link came from a spreadsheet of information about 24 brands of pizza: specifically, per 5-ounce serving, the number of calories, the grams of fat, and the cost (in US dollars). The names of the pizza brands are quite long. This file may open in a spreadsheet when you browse to the link, depending on your computer’s setup.\n\nRead in the data and display at least some of the data frame. Are the variables of the right types? (In particular, why is the number of calories labelled one way and the cost labelled a different way?)\nMake a scatterplot for predicting calories from the number of grams of fat. Add a smooth trend. What kind of relationship do you see, if any?\nFit a straight-line relationship, and display the intercept, slope, R-squared, etc. Is there a real relationship between the two variables, or is any apparent trend just chance?\nObtain a plot of the residuals against the fitted values for this regression. Does this indicate that there are any problems with this regression, or not? Explain briefly.\nThe research assistant in this study returns with two new brands of pizza (ones that were not in the original data). The fat content of a 5-ounce serving was 12 grams for the first brand and 20 grams for the second brand. For each of these brands of pizza, obtain a suitable 95% interval for the number of calories contained in a 5-ounce serving."
  },
  {
    "objectID": "simple-regression.html#where-should-the-fire-stations-be",
    "href": "simple-regression.html#where-should-the-fire-stations-be",
    "title": "16  Simple regression",
    "section": "16.12 Where should the fire stations be?",
    "text": "16.12 Where should the fire stations be?\nIn city planning, one major issue is where to locate fire stations. If a city has too many fire stations, it will spend too much on running them, but if it has too few, there may be unnecessary fire damage because the fire trucks take too long to get to the fire.\nThe first part of a study of this kind of issue is to understand the relationship between the distance from the fire station (measured in miles in our data set) and the amount of fire damage caused (measured in thousands of dollars). A city recorded the fire damage and distance from fire station for 15 residential fires (which you can take as a sample of “all possible residential fires in that city”). The data are in link.\n\nRead in and display the data, verifying that you have the right number of rows and the right columns.\n* Obtain a 95% confidence interval for the mean fire damage. (There is nothing here from STAD29, and your answer should have nothing to do with distance.)\nDraw a scatterplot for predicting the amount of fire damage from the distance from the fire station. Add a smooth trend to your plot.\n* Is there a relationship between distance from fire station and fire damage? Is it linear or definitely curved? How strong is it? Explain briefly.\nFit a regression predicting fire damage from distance. How is the R-squared consistent (or inconsistent) with your answer from part~(here)?\n Obtain a 95% confidence interval for the mean fire damage for a residence that is 4 miles from the nearest fire station*. (Note the contrast with part~(here).)\nCompare the confidence intervals of parts (here) and (here). Specifically, compare their centres and their lengths, and explain briefly why the results make sense."
  },
  {
    "objectID": "simple-regression.html#making-it-stop",
    "href": "simple-regression.html#making-it-stop",
    "title": "16  Simple regression",
    "section": "16.13 Making it stop",
    "text": "16.13 Making it stop\nIf you are driving, and you hit the brakes, how far do you travel before coming to a complete stop? Presumably this depends on how fast you are going. Knowing this relationship is important in setting speed limits on roads. For example, on a very bendy road, the speed limit needs to be low, because you cannot see very far ahead, and there could be something just out of sight that you need to stop for.\nData were collected for a typical car and driver, as shown in http://ritsokiguess.site/datafiles/stopping.csv. These are American data, so the speeds are miles per hour and the stopping distances are in feet.\n\nRead in and display (probably all of) the data.\nMake a suitable plot of the data.\nDescribe any trend you see in your graph.\nFit a linear regression predicting stopping distance from speed. (You might have some misgivings about doing this, but do it anyway.)\nPlot the residuals against the fitted values for this regression.\nWhat do you learn from the residual plot? Does that surprise you? Explain briefly.\nWhat is the actual relationship between stopping distance and speed, according to the physics? See if you can find out. Cite any books or websites that you use: that is, include a link to a website, or give enough information about a book that the grader could find it.\nFit the relationship that your research indicated (in the previous part) and display the results. Comment briefly on the R-squared value.\nSomebody says to you “if you have a regression with a high R-squared, like 95%, there is no need to look for a better model.” How do you respond to this? Explain briefly."
  },
  {
    "objectID": "simple-regression.html#predicting-height-from-foot-length",
    "href": "simple-regression.html#predicting-height-from-foot-length",
    "title": "16  Simple regression",
    "section": "16.14 Predicting height from foot length",
    "text": "16.14 Predicting height from foot length\nIs it possible to estimate the height of a person from the length of their foot? To find out, 33 (male) students had their height and foot length measured. The data are in http://ritsokiguess.site/datafiles/heightfoot.csv.\n\nRead in and display (some of) the data. (If you are having trouble, make sure you have exactly the right URL. The correct URL has no spaces or other strange characters in it.)\nMake a suitable plot of the two variables in the data frame.\nAre there any observations not on the trend of the other points? What is unusual about those observations?\nFit a regression predicting height from foot length, including any observations that you identified in the previous part. For that regression, plot the residuals against the fitted values and make a normal quantile plot of the residuals.\nEarlier, you identified one or more observations that were off the trend. How does this point or points show up on each of the plots you drew in the previous part?\nAny data points that concerned you earlier were actually errors. Create and save a new data frame that does not contain any of those data points.\nRun a regression predicting height from foot length for your data set without errors. Obtain a plot of the residuals against fitted values and a normal quantile plot of the residuals for this regression.\nDo you see any problems on the plots you drew in the previous part? Explain briefly.\nFind a way to plot the data and both regression lines on the same plot, in such a way that you can see which regression line is which. If you get help from anything outside the course materials, cite your source(s).\nDiscuss briefly how removing the observation(s) that were errors has changed where the regression line goes, and whether that is what you expected.\n\nMy solutions follow:"
  },
  {
    "objectID": "simple-regression.html#rainfall-in-california-1",
    "href": "simple-regression.html#rainfall-in-california-1",
    "title": "16  Simple regression",
    "section": "16.15 Rainfall in California",
    "text": "16.15 Rainfall in California\nThe data in link are rainfall and other measurements for 30 weather stations in California. Our aim is to understand how well the annual rainfall at these stations (measured in inches) can be predicted from the other measurements, which are the altitude (in feet above sea level), the latitude (degrees north of the equator) and the distance from the coast (in miles).\n\nRead the data into R. You’ll have to be careful here, since the values are space-delimited, but sometimes by more than one space, to make the columns line up. read_table, with filename or url, will read it in. One of the variables is called rainfall, so as long as you do not call the data frame that, you should be safe.\n\nSolution\nI used rains as the name of my data frame:\n\nmy_url=\"http://ritsokiguess.site/datafiles/calirain.txt\"\nrains=read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  station = col_character(),\n  rainfall = col_double(),\n  altitude = col_double(),\n  latitude = col_double(),\n  fromcoast = col_double()\n)\n\n\nI have the right number of rows and columns.\nI don’t need you to investigate the data yet (that happens in the next part), but this is interesting (to me):\n\nrains\n\n\n\n  \n\n\n\nSome of the station names are two words, but they have been smooshed into one word, so that read_table will recognize them as a single thing. Someone had already done that for us, so I didn’t even have to do it myself.\nIf the station names had been two genuine words, a .csv would probably have been the best choice (the actual data values being separated by commas then, and not spaces).\n\\(\\blacksquare\\)\n\nMake a boxplot of the rainfall figures, and explain why the values are reasonable. (A rainfall cannot be negative, and it is unusual for a annual rainfall to exceed 60 inches.) A ggplot boxplot needs something on the \\(x\\)-axis: the number 1 will do.\n\nSolution\n\nggplot(rains,aes(y=rainfall,x=1))+geom_boxplot()\n\n\n\n\nThere is only one rainfall over 60 inches, and the smallest one is close to zero but positive, so that is good.\nAnother possible plot here is a histogram, since there is only one quantitative variable:\n\nggplot(rains, aes(x=rainfall))+geom_histogram(bins=7)\n\n\n\n\nThis clearly shows the rainfall value above 60 inches, but some other things are less clear: are those two rainfall values around 50 inches above or below 50, and are those six rainfall values near zero actually above zero? Extra: What stations have those extreme values? Should you wish to find out:\n\nrains %&gt;% filter(rainfall&gt;60)\n\n\n\n  \n\n\n\nThis is a place right on the Pacific coast, almost up into Oregon (it’s almost the northernmost of all the stations). So it makes sense that it would have a high rainfall, if anywhere does. (If you know anything about rainy places, you’ll probably think of Vancouver and Seattle, in the Pacific Northwest.) Here it is: link. Which station has less than 2 inches of annual rainfall?\n\nrains %&gt;% filter(rainfall&lt;2)  \n\n\n\n  \n\n\n\nThe name of the station is a clue: this one is in the desert. So you’d expect very little rain. Its altitude is negative, so it’s actually below sea level. This is correct. Here is where it is: link.\n\\(\\blacksquare\\)\n\nPlot rainfall against each of the other quantitative variables (that is, not station).\n\nSolution\nThat is, altitude, latitude and fromcoast. The obvious way to do this (perfectly acceptable) is one plot at a time:\n\nggplot(rains,aes(y=rainfall,x=altitude))+geom_point()\n\n\n\n\n\nggplot(rains,aes(y=rainfall,x=latitude))+geom_point()\n\n\n\n\nand finally\n\nggplot(rains,aes(y=rainfall,x=fromcoast))+geom_point()\n\n\n\n\nYou can add a smooth trend to these if you want. Up to you. Just the points is fine with me.\nHere is a funky way to get all three plots in one shot:\n\nrains %&gt;% \n  pivot_longer(altitude:fromcoast, names_to=\"xname\",values_to=\"x\") %&gt;%\n  ggplot(aes(x=x,y=rainfall))+geom_point()+\n  facet_wrap(~xname,scales=\"free\")\n\n\n\n\nThis always seems extraordinarily strange if you haven’t run into it before. The strategy is to put all the \\(x\\)-variables you want to plot into one column and then plot your \\(y\\) against the \\(x\\)-column. Thus: make a column of all the \\(x\\)’s glued together, labelled by which \\(x\\) they are, then plot \\(y\\) against \\(x\\) but make a different sub-plot or “facet” for each different \\(x\\)-name. The last thing is that each \\(x\\) is measured on a different scale, and unless we take steps, all the sub-plots will have the same scale on each axis, which we don’t want.\nI’m not sure I like how it came out, with three very tall plots. facet_wrap can also take an nrow or an ncol, which tells it how many rows or columns to use for the display. Here, for example, two columns because I thought three was too many:\n\nrains %&gt;% \n  pivot_longer(altitude:fromcoast, names_to=\"xname\",values_to=\"x\") %&gt;%\n  ggplot(aes(x=x,y=rainfall))+geom_point()+\n  facet_wrap(~xname,scales=\"free\",ncol=2)\n\n\n\n\nNow, the three plots have come out about square, or at least “landscape”, which I like a lot better.\n\\(\\blacksquare\\)\n\nLook at the relationship of each other variable with rainfall. Justify the assertion that latitude seems most strongly related with rainfall. Is that relationship positive or negative? linear? Explain briefly.\n\nSolution\nLet’s look at the three variables in turn:\n\naltitude: not much of anything. The stations near sea level have rainfall all over the place, though the three highest-altitude stations have the three highest rainfalls apart from Crescent City.\nlatitude: there is a definite upward trend here, in that stations further north (higher latitude) are likely to have a higher rainfall. I’d call this trend linear (or, not obviously curved), though the two most northerly stations have one higher and one much lower rainfall than you’d expect.\nfromcoast: this is a weak downward trend, though the trend is spoiled by those three stations about 150 miles from the coast that have more than 40 inches of rainfall.\n\nOut of those, only latitude seems to have any meaningful relationship with rainfall.\n\\(\\blacksquare\\)\n\nFit a regression with rainfall as the response variable, and latitude as your explanatory variable. What are the intercept, slope and R-squared values? Is there a significant relationship between rainfall and your explanatory variable? What does that mean?\n\nSolution\nSave your lm into a variable, since it will get used again later:\n\nrainfall.1=lm(rainfall~latitude,data=rains)\nsummary(rainfall.1)\n\n\nCall:\nlm(formula = rainfall ~ latitude, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.297  -7.956  -2.103   6.082  38.262 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -113.3028    35.7210  -3.172  0.00366 ** \nlatitude       3.5950     0.9623   3.736  0.00085 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.82 on 28 degrees of freedom\nMultiple R-squared:  0.3326,    Adjusted R-squared:  0.3088 \nF-statistic: 13.96 on 1 and 28 DF,  p-value: 0.0008495\n\n\nMy intercept is \\(-113.3\\), slope is \\(3.6\\) and R-squared is \\(0.33\\) or 33%. (I want you to pull these numbers out of the output and round them off to something sensible.) The slope is significantly nonzero, its P-value being 0.00085: rainfall really does depend on latitude, although not strongly so.\nExtra: Of course, I can easily do the others as well, though you don’t have to:\n\nrainfall.2=lm(rainfall~fromcoast,data=rains)\nsummary(rainfall.2)\n\n\nCall:\nlm(formula = rainfall ~ fromcoast, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.240  -9.431  -6.603   2.871  51.147 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.77306    4.61296   5.154 1.82e-05 ***\nfromcoast   -0.05039    0.04431  -1.137    0.265    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.54 on 28 degrees of freedom\nMultiple R-squared:  0.04414,   Adjusted R-squared:   0.01 \nF-statistic: 1.293 on 1 and 28 DF,  p-value: 0.2651\n\n\nHere, the intercept is 23.8, the slope is \\(-0.05\\) and R-squared is a dismal 0.04 (4%). This is a way of seeing that this relationship is really weak, and it doesn’t even have a curve to the trend or anything that would compensate for this. I looked at the scatterplot again and saw that if it were not for the point bottom right which is furthest from the coast and has almost no rainfall, there would be almost no trend at all. The slope here is not significantly different from zero, with a P-value of 0.265.\nFinally:\n\nrainfall.3=lm(rainfall~altitude,data=rains)\nsummary(rainfall.3)\n\n\nCall:\nlm(formula = rainfall ~ altitude, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.620  -8.479  -2.729   4.555  58.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16.514799   3.539141   4.666  6.9e-05 ***\naltitude     0.002394   0.001428   1.676    0.105    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.13 on 28 degrees of freedom\nMultiple R-squared:  0.09121,   Adjusted R-squared:  0.05875 \nF-statistic:  2.81 on 1 and 28 DF,  p-value: 0.1048\n\n\nThe intercept is 16.5, the slope is 0.002 and the R-squared is 0.09 or 9%, also terrible. The P-value is 0.105, which is not small enough to be significant.\nSo it looks as if it’s only latitude that has any impact at all. This is the only explanatory variable with a significantly nonzero slope. On its own, at least.\n\\(\\blacksquare\\)\n\nFit a multiple regression predicting rainfall from all three of the other (quantitative) variables. Display the results. Comment is coming up later.\n\nSolution\nThis, then:\n\nrainfall.4=lm(rainfall~latitude+altitude+fromcoast,data=rains)\nsummary(rainfall.4)\n\n\nCall:\nlm(formula = rainfall ~ latitude + altitude + fromcoast, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.722  -5.603  -0.531   3.510  33.317 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.024e+02  2.921e+01  -3.505 0.001676 ** \nlatitude     3.451e+00  7.949e-01   4.342 0.000191 ***\naltitude     4.091e-03  1.218e-03   3.358 0.002431 ** \nfromcoast   -1.429e-01  3.634e-02  -3.931 0.000559 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.1 on 26 degrees of freedom\nMultiple R-squared:  0.6003,    Adjusted R-squared:  0.5542 \nF-statistic: 13.02 on 3 and 26 DF,  p-value: 2.205e-05\n\n\n\\(\\blacksquare\\)\n\nWhat is the R-squared for the regression of the last part? How does that compare with the R-squared of your regression in part (e)?\n\nSolution\nThe R-squared is 0.60 (60%), which is quite a bit bigger than the R-squared of 0.33 (33%) we got back in (e).\n\\(\\blacksquare\\)\n\nWhat do you conclude about the importance of the variables that you did not include in your model in (e)? Explain briefly.\n\nSolution\nBoth variables altitude and fromcoast are significant in this regression, so they have something to add over and above latitude when it comes to predicting rainfall, even though (and this seems odd) they have no apparent relationship with rainfall on their own. Another way to say this is that the three variables work together as a team to predict rainfall, and together they do much better than any one of them can do by themselves.\nThis also goes to show that the scatterplots we began with don’t get to the heart of multi-variable relationships, because they are only looking at the variables two at a time.\n\\(\\blacksquare\\)\n\nMake a suitable hypothesis test that the variables altitude and fromcoast significantly improve the prediction of rainfall over the use of latitude alone. What do you conclude?\n\nSolution\nThis calls for anova. Feed this two fitted models, smaller (fewer explanatory variables) first. The null hypothesis is that the two models are equally good (so we should go with the smaller); the alternative is that the larger model is better, so that the extra complication is worth it:\n\nanova(rainfall.1,rainfall.4)  \n\n\n\n  \n\n\n\nThe P-value is small, so we reject the null in favour of the alternative: the regression with all three explanatory variables fits better than the one with just latitude, so the bigger model is the one we should go with.\nIf you have studied these things: this one is a “multiple-partial \\(F\\)-test”, for testing the combined significance of more than one \\(x\\) but less than all the \\(x\\)’s.4\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#carbon-monoxide-in-cigarettes-1",
    "href": "simple-regression.html#carbon-monoxide-in-cigarettes-1",
    "title": "16  Simple regression",
    "section": "16.16 Carbon monoxide in cigarettes",
    "text": "16.16 Carbon monoxide in cigarettes\nThe (US) Federal Trade Commission assesses cigarettes according to their tar, nicotine and carbon monoxide contents. In a particular year, 25 brands were assessed. For each brand, the tar, nicotine and carbon monoxide (all in milligrams) were measured, along with the weight in grams. Our aim is to predict carbon monoxide from any or all of the other variables. The data are in link. These are aligned by column (except for the variable names), with more than one space between each column of data.\n\nRead the data into R, and check that you have 25 observations and 4 variables.\n\nSolution\nThis specification calls for read_table:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ftccigar.txt\"\ncigs &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  tar = col_double(),\n  nicotine = col_double(),\n  weight = col_double(),\n  co = col_double()\n)\n\ncigs\n\n\n\n  \n\n\n\nYes, I have 25 observations on 4 variables indeed.\nread_delim won’t work (try it and see what happens), because that would require the values to be separated by exactly one space.\n\\(\\blacksquare\\)\n\nRun a regression to predict carbon monoxide from the other variables, and obtain a summary of the output.\n\nSolution\nThe word “summary” is meant to be a big clue that summary is what you need:\n\ncigs.1 &lt;- lm(co ~ tar + nicotine + weight, data = cigs)\nsummary(cigs.1)\n\n\nCall:\nlm(formula = co ~ tar + nicotine + weight, data = cigs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89261 -0.78269  0.00428  0.92891  2.45082 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.2022     3.4618   0.925 0.365464    \ntar           0.9626     0.2422   3.974 0.000692 ***\nnicotine     -2.6317     3.9006  -0.675 0.507234    \nweight       -0.1305     3.8853  -0.034 0.973527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.446 on 21 degrees of freedom\nMultiple R-squared:  0.9186,    Adjusted R-squared:  0.907 \nF-statistic: 78.98 on 3 and 21 DF,  p-value: 1.329e-11\n\n\n\\(\\blacksquare\\)\n\nWhich one of your explanatory variables would you remove from this regression? Explain (very) briefly. Go ahead and fit the regression without it, and describe how the change in R-squared from the regression in (b) was entirely predictable.\n\nSolution\nFirst, the \\(x\\)-variable to remove. The obvious candidate is weight, since it has easily the highest, and clearly non-significant, P-value. So, out it comes:\n\ncigs.2 &lt;- lm(co ~ tar + nicotine, data = cigs)\nsummary(cigs.2)\n\n\nCall:\nlm(formula = co ~ tar + nicotine, data = cigs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89941 -0.78470 -0.00144  0.91585  2.43064 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.0896     0.8438   3.662 0.001371 ** \ntar           0.9625     0.2367   4.067 0.000512 ***\nnicotine     -2.6463     3.7872  -0.699 0.492035    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.413 on 22 degrees of freedom\nMultiple R-squared:  0.9186,    Adjusted R-squared:  0.9112 \nF-statistic: 124.1 on 2 and 22 DF,  p-value: 1.042e-12\n\n\nR-squared has dropped from 0.9186 to 0.9186! That is, taking out weight has not just had a minimal effect on R-squared; it’s not changed R-squared at all. This is because weight was so far from being significant: it literally had nothing to add.\nAnother way of achieving the same thing is via the function update, which takes a fitted model object and describes the change that you want to make:\n\ncigs.2a &lt;- update(cigs.1, . ~ . - weight)\nsummary(cigs.2a)\n\n\nCall:\nlm(formula = co ~ tar + nicotine, data = cigs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89941 -0.78470 -0.00144  0.91585  2.43064 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.0896     0.8438   3.662 0.001371 ** \ntar           0.9625     0.2367   4.067 0.000512 ***\nnicotine     -2.6463     3.7872  -0.699 0.492035    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.413 on 22 degrees of freedom\nMultiple R-squared:  0.9186,    Adjusted R-squared:  0.9112 \nF-statistic: 124.1 on 2 and 22 DF,  p-value: 1.042e-12\n\n\nThis can be shorter than describing the whole model again, as you do with the cigs.2 version of lm. The syntax is that you first specify a “base” fitted model object that you’re going to update. Because the model cigs.1 contains all the information about the kind of model it is, and which data frame the data come from, R already knows that this is a linear multiple regression and which \\(x\\)’s it contains. The second thing to describe is the change from the “base”. In this case, we want to use the same response variable and all the same explanatory variables that we had before, except for weight. This is specified by a special kind of model formula where . means “whatever was there before”: in English, “same response and same explanatories except take out weight”.\n\\(\\blacksquare\\)\n\nFit a regression predicting carbon monoxide from nicotine only, and display the summary.\n\nSolution\nAs you would guess:\n\ncigs.3 &lt;- lm(co ~ nicotine, data = cigs)\nsummary(cigs.3)\n\n\nCall:\nlm(formula = co ~ nicotine, data = cigs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3273 -1.2228  0.2304  1.2700  3.9357 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.6647     0.9936   1.675    0.107    \nnicotine     12.3954     1.0542  11.759 3.31e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.828 on 23 degrees of freedom\nMultiple R-squared:  0.8574,    Adjusted R-squared:  0.8512 \nF-statistic: 138.3 on 1 and 23 DF,  p-value: 3.312e-11\n\n\n\\(\\blacksquare\\)\n\nnicotine was far from being significant in the model of (c), and yet in the model of (d), it was strongly significant, and the R-squared value of (d) was almost as high as that of (c). What does this say about the importance of nicotine as an explanatory variable? Explain, as briefly as you can manage.\n\nSolution\nWhat this says is that you cannot say anything about the “importance” of nicotine without also describing the context that you’re talking about. By itself, nicotine is important, but when you have tar in the model, nicotine is not important: precisely, it now has nothing to add over and above the predictive value that tar has. You might guess that this is because tar and nicotine are “saying the same thing” in some fashion. We’ll explore that in a moment.\n\\(\\blacksquare\\)\n\nMake a “pairs plot”: that is, scatter plots between all pairs of variables. This can be done by feeding the whole data frame into plot.5 Do you see any strong relationships that do not include co? Does that shed any light on the last part? Explain briefly (or “at length” if that’s how it comes out).\n\nSolution\nPlot the entire data frame:\n\nplot(cigs)\n\n\n\n\nWe’re supposed to ignore co, but I comment that strong relationships between co and both of tar and nicotine show up here, along with weight being at most weakly related to anything else.\nThat leaves the relationship of tar and nicotine with each other. That also looks like a strong linear trend. When you have correlations between explanatory variables, it is called “multicollinearity”.\nHaving correlated \\(x\\)’s is trouble. Here is where we find out why. The problem is that when co is large, nicotine is large, and a large value of tar will come along with it. So we don’t know whether a large value of co is caused by a large value of tar or a large value of nicotine: there is no way to separate out their effects because in effect they are “glued together”.\nYou might know of this effect (in an experimental design context) as “confounding”: the effect of tar on co is confounded with the effect of nicotine on co, and you can’t tell which one deserves the credit for predicting co.\nIf you were able to design an experiment here, you could (in principle) manufacture a bunch of cigarettes with high tar; some of them would have high nicotine and some would have low. Likewise for low tar. Then the correlation between nicotine and tar would go away, their effects on co would no longer be confounded, and you could see unambiguously which one of the variables deserves credit for predicting co. Or maybe it depends on both, genuinely, but at least then you’d know.\nWe, however, have an observational study, so we have to make do with the data we have. Confounding is one of the risks we take when we work with observational data.\nThis was a “base graphics” plot. There is a way of doing a ggplot-style “pairs plot”, as this is called, thus:\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\ncigs %&gt;% ggpairs(progress = FALSE)\n\n\n\n\nAs ever, install.packages first, in the likely event that you don’t have this package installed yet. Once you do, though, I think this is a nicer way to get a pairs plot.\nThis plot is a bit more sophisticated: instead of just having the scatterplots of the pairs of variables in the row and column, it uses the diagonal to show a “kernel density” (a smoothed-out histogram), and upper-right it shows the correlation between each pair of variables. The three correlations between co, tar and nicotine are clearly the highest.\nIf you want only some of the columns to appear in your pairs plot, select them first, and then pass that data frame into ggpairs. Here, we found that weight was not correlated with anything much, so we can take it out and then make a pairs plot of the other variables:\n\ncigs %&gt;% select(-weight) %&gt;% ggpairs(progress = FALSE)\n\n\n\n\nThe three correlations that remain are all very high, which is entirely consistent with the strong linear relationships that you see bottom left.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#maximal-oxygen-uptake-in-young-boys-1",
    "href": "simple-regression.html#maximal-oxygen-uptake-in-young-boys-1",
    "title": "16  Simple regression",
    "section": "16.17 Maximal oxygen uptake in young boys",
    "text": "16.17 Maximal oxygen uptake in young boys\nA physiologist wanted to understand the relationship between physical characteristics of pre-adolescent boys and their maximal oxygen uptake (millilitres of oxygen per kilogram of body weight). The data are in link for a random sample of 10 pre-adolescent boys. The variables are (with units):\n\nuptake: Oxygen uptake (millitres of oxygen per kilogram of body weight)\nage: boy’s age (years)\nheight: boy’s height (cm)\nweight: boy’s weight (kg)\nchest: chest depth (cm).\n\n\nRead the data into R and confirm that you do indeed have 10 observations.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/youngboys.txt\"\nboys &lt;- read_delim(my_url, \" \")\n\nRows: 10 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (5): uptake, age, height, weight, chest\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nboys\n\n\n\n  \n\n\n\n10 boys (rows) indeed.\n\\(\\blacksquare\\)\n\nFit a regression predicting oxygen uptake from all the other variables, and display the results.\n\nSolution\nFitting four explanatory variables with only ten observations is likely to be pretty shaky, but we press ahead regardless:\n\nboys.1 &lt;- lm(uptake ~ age + height + weight + chest, data = boys)\nsummary(boys.1)\n\n\nCall:\nlm(formula = uptake ~ age + height + weight + chest, data = boys)\n\nResiduals:\n        1         2         3         4         5         6         7         8 \n-0.020697  0.019741 -0.003649  0.038470 -0.023639 -0.026026  0.050459 -0.014380 \n        9        10 \n 0.004294 -0.024573 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.774739   0.862818  -5.534 0.002643 ** \nage         -0.035214   0.015386  -2.289 0.070769 .  \nheight       0.051637   0.006215   8.308 0.000413 ***\nweight      -0.023417   0.013428  -1.744 0.141640    \nchest        0.034489   0.085239   0.405 0.702490    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03721 on 5 degrees of freedom\nMultiple R-squared:  0.9675,    Adjusted R-squared:  0.9415 \nF-statistic:  37.2 on 4 and 5 DF,  p-value: 0.0006513\n\n\n\\(\\blacksquare\\)\n\n(A one-mark question.) Would you say, on the evidence so far, that the regression fits well or badly? Explain (very) briefly.\n\nSolution\nR-squared of 0.97 (97%) is very high, so I’d say this regression fits very well. That’s all. I said “on the evidence so far” to dissuade you from overthinking this, or thinking that you needed to produce some more evidence. That, plus the fact that this was only one mark.\n\\(\\blacksquare\\)\n\nIt seems reasonable that an older boy should have a greater oxygen uptake, all else being equal. Is this supported by your output? Explain briefly.\n\nSolution\nIf an older boy has greater oxygen uptake (the “all else equal” was a hint), the slope of age should be positive. It is not: it is \\(-0.035\\), so it is suggesting (all else equal) that a greater age goes with a smaller oxygen uptake. The reason why this happens (which you didn’t need, but you can include it if you like) is that age has a non-small P-value of 0.07, so that the age slope is not significantly different from zero. With all the other variables, age has nothing to add over and above them, and we could therefore remove it.\n\\(\\blacksquare\\)\n\nIt seems reasonable that a boy with larger weight should have larger lungs and thus a statistically significantly larger oxygen uptake. Is that what happens here? Explain briefly.\n\nSolution\nLook at the P-value for weight. This is 0.14, not small, and so a boy with larger weight does not have a significantly larger oxygen uptake, all else equal. (The slope for weight is not significantly different from zero either.) I emphasized “statistically significant” to remind you that this means to do a test and get a P-value.\n\\(\\blacksquare\\)\n\nFit a model that contains only the significant explanatory variables from your first regression. How do the R-squared values from the two regressions compare? (The last sentence asks for more or less the same thing as the next part. Answer it either here or there. Either place is good.)\n\nSolution\nOnly height is significant, so that’s the only explanatory variable we need to keep. I would just do the regression straight rather than using update here:\n\nboys.2 &lt;- lm(uptake ~ height, data = boys)\nsummary(boys.2)\n\n\nCall:\nlm(formula = uptake ~ height, data = boys)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.069879 -0.033144  0.001407  0.009581  0.084012 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.843326   0.609198  -6.309 0.000231 ***\nheight       0.040718   0.004648   8.761 2.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05013 on 8 degrees of freedom\nMultiple R-squared:  0.9056,    Adjusted R-squared:  0.8938 \nF-statistic: 76.75 on 1 and 8 DF,  p-value: 2.258e-05\n\n\nIf you want, you can use update here, which looks like this:\n\nboys.2a &lt;- update(boys.1, . ~ . - age - weight - chest)\nsummary(boys.2a)\n\n\nCall:\nlm(formula = uptake ~ height, data = boys)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.069879 -0.033144  0.001407  0.009581  0.084012 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.843326   0.609198  -6.309 0.000231 ***\nheight       0.040718   0.004648   8.761 2.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05013 on 8 degrees of freedom\nMultiple R-squared:  0.9056,    Adjusted R-squared:  0.8938 \nF-statistic: 76.75 on 1 and 8 DF,  p-value: 2.258e-05\n\n\nThis doesn’t go quite so smoothly here because there are three variables being removed, and it’s a bit of work to type them all.\n\\(\\blacksquare\\)\n\nHow has R-squared changed between your two regressions? Describe what you see in a few words.\n\nSolution\nR-squared has dropped by a bit, from 97% to 91%. (Make your own call: pull out the two R-squared numbers, and say a word or two about how they compare. I don’t much mind what you say: “R-squared has decreased (noticeably)”, “R-squared has hardly changed”. But say something.)\n\\(\\blacksquare\\)\n\nCarry out a test comparing the fit of your two regression models. What do you conclude, and therefore what recommendation would you make about the regression that would be preferred?\n\nSolution\nThe word “test” again implies something that produces a P-value with a null hypothesis that you might reject. In this case, the test that compares two models differing by more than one \\(x\\) uses anova, testing the null hypothesis that the two regressions are equally good, against the alternative that the bigger (first) one is better. Feed anova two fitted model objects, smaller first:\n\nanova(boys.2, boys.1)\n\n\n\n  \n\n\n\nThis P-value of 0.123 is not small, so we do not reject the null hypothesis. There is not a significant difference in fit between the two models. Therefore, we should go with the smaller model boys.2 because it is simpler.\nThat drop in R-squared from 97% to 91% was, it turns out, not significant: the three extra variables could have produced a change in R-squared like that, even if they were worthless.6\nIf you have learned about “adjusted R-squared”, you might recall that this is supposed to go down only if the variables you took out should not have been taken out. But adjusted R-squared goes down here as well, from 94% to 89% (not quite as much, therefore). What happens is that adjusted R-squared is rather more relaxed about keeping variables than the anova \\(F\\)-test is; if we had used an \\(\\alpha\\) of something like 0.10, the decision between the two models would have been a lot closer, and this is reflected in the adjusted R-squared values.\n\\(\\blacksquare\\)\n\nObtain a table of correlations between all the variables in the data frame. Do this by feeding the whole data frame into cor. We found that a regression predicting oxygen uptake from just height was acceptably good. What does your table of correlations say about why that is? (Hint: look for all the correlations that are large.)\n\nSolution\nCorrelations first:\n\ncor(boys)\n\n          uptake       age    height    weight     chest\nuptake 1.0000000 0.1361907 0.9516347 0.6576883 0.7182659\nage    0.1361907 1.0000000 0.3274830 0.2307403 0.1657523\nheight 0.9516347 0.3274830 1.0000000 0.7898252 0.7909452\nweight 0.6576883 0.2307403 0.7898252 1.0000000 0.8809605\nchest  0.7182659 0.1657523 0.7909452 0.8809605 1.0000000\n\n\nThe correlations with age are all on the low side, but all the other correlations are high, not just between uptake and the other variables, but between the explanatory variables as well.\nWhy is this helpful in understanding what’s going on? Well, imagine a boy with large height (a tall one). The regression boys.2 says that this alone is enough to predict that such a boy’s oxygen uptake is likely to be large, since the slope is positive. But the correlations tell you more: a boy with large height is also (somewhat) likely to be older (have large age), heavier (large weight) and to have larger chest cavity. So oxygen uptake does depend on those other variables as well, but once you know height you can make a good guess at their values; you don’t need to know them.\nFurther remarks: age has a low correlation with uptake, so its non-significance earlier appears to be “real”: it really does have nothing extra to say, because the other variables have a stronger link with uptake than age. Height, however, seems to be the best way of relating oxygen uptake to any of the other variables. I think the suppositions from earlier about relating oxygen uptake to “bigness”7 in some sense are actually sound, but age and weight and chest capture “bigness” worse than height does. Later, when you learn about Principal Components, you will see that the first principal component, the one that best captures how the variables vary together, is often “bigness” in some sense.\nAnother way to think about these things is via pairwise scatterplots. The nicest way to produce these is via ggpairs from package GGally:\n\nboys %&gt;% ggpairs(progress = FALSE)\n\n\n\n\nA final remark: with five variables, we really ought to have more than ten observations (something like 50 would be better). But with more observations and the same correlation structure, the same issues would come up again, so the question would not be materially changed.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#facebook-friends-and-grey-matter-1",
    "href": "simple-regression.html#facebook-friends-and-grey-matter-1",
    "title": "16  Simple regression",
    "section": "16.18 Facebook friends and grey matter",
    "text": "16.18 Facebook friends and grey matter\nIs there a relationship between the number of Facebook friends a person has, and the density of grey matter in the areas of the brain associated with social perception and associative memory? To find out, a 2012 study measured both of these variables for a sample of 40 students at City University in London (England). The data are at link. The grey matter density is on a \\(z\\)-score standardized scale. The values are separated by tabs.\nThe aim of this question is to produce an R Markdown report that contains your answers to the questions below.\nYou should aim to make your report flow smoothly, so that it would be pleasant for a grader to read, and can stand on its own as an analysis (rather than just being the answer to a question that I set you). Some suggestions: give your report a title and arrange it into sections with an Introduction; add a small amount of additional text here and there explaining what you are doing and why. I don’t expect you to spend a large amount of time on this, but I do hope you will make some effort. (My report came out to 4 Word pages.)\n\nRead in the data and make a scatterplot for predicting the number of Facebook friends from the grey matter density. On your scatterplot, add a smooth trend.\n\nSolution\nBegin your document with a code chunk containing library(tidyverse). The data values are separated by tabs, which you will need to take into account:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/facebook.txt\"\nfb &lt;- read_tsv(my_url)\n\nRows: 40 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (2): GMdensity, FBfriends\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfb\n\n\n\n  \n\n\nggplot(fb, aes(x = GMdensity, y = FBfriends)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\nDescribe what you see on your scatterplot: is there a trend, and if so, what kind of trend is it? (Don’t get too taken in by the exact shape of your smooth trend.) Think “form, direction, strength”.\n\nSolution\nI’d say there seems to be a weak, upward, apparently linear trend. The points are not especially close to the trend, so I don’t think there’s any justification for calling this other than “weak”. (If you think the trend is, let’s say, “moderate”, you ought to say what makes you think that: for example, that the people with a lot of Facebook friends also tend to have a higher grey matter density. I can live with a reasonably-justified “moderate”.) The reason I said not to get taken in by the shape of the smooth trend is that this has a “wiggle” in it: it goes down again briefly in the middle. But this is likely a quirk of the data, and the trend, if there is any, seems to be an upward one.\n\\(\\blacksquare\\)\n\nFit a regression predicting the number of Facebook friends from the grey matter density, and display the output.\n\nSolution\nThat looks like this. You can call the “fitted model object” whatever you like, but you’ll need to get the capitalization of the variable names correct:\n\nfb.1 &lt;- lm(FBfriends ~ GMdensity, data = fb)\nsummary(fb.1)\n\n\nCall:\nlm(formula = FBfriends ~ GMdensity, data = fb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-339.89 -110.01   -5.12   99.80  303.64 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   366.64      26.35  13.916  &lt; 2e-16 ***\nGMdensity      82.45      27.58   2.989  0.00488 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 165.7 on 38 degrees of freedom\nMultiple R-squared:  0.1904,    Adjusted R-squared:  0.1691 \nF-statistic: 8.936 on 1 and 38 DF,  p-value: 0.004882\n\n\nI observe, though I didn’t ask you to, that the R-squared is pretty awful, going with a correlation of\n\nsqrt(0.1904)\n\n[1] 0.4363485\n\n\nwhich would look like as weak of a trend as we saw.8\n\\(\\blacksquare\\)\n\nIs the slope of your regression line significantly different from zero? What does that mean, in the context of the data?\n\nSolution\nThe P-value of the slope is 0.005, which is less than 0.05. Therefore the slope is significantly different from zero. That means that the number of Facebook friends really does depend on the grey matter density, for the whole population of interest and not just the 40 students observed here (that were a sample from that population). I don’t mind so much what you think the population is, but it needs to be clear that the relationship applies to a population. Another way to approach this is to say that you would expect this relationship to show up again in another similar experiment. That also works, because it gets at the idea of reproducibility.\n\\(\\blacksquare\\)\n\nAre you surprised by the results of parts (b) and (d)? Explain briefly.\n\nSolution\nI am surprised, because I thought the trend on the scatterplot was so weak that there would not be a significant slope. I guess there was enough of an upward trend to be significant, and with \\(n=40\\) observations we were able to get a significant slope out of that scatterplot. With this many observations, even a weak correlation can be significantly nonzero. You can be surprised or not, but you need to have some kind of consideration of the strength of the trend on the scatterplot as against the significance of the slope. For example, if you decided that the trend was “moderate” in strength, you would be justified in being less surprised than I was. Here, there is the usual issue that we have proved that the slope is not zero (that the relationship is not flat), but we may not have a very clear idea of what the slope actually is. There are a couple of ways to get a confidence interval. The obvious one is to use R as a calculator and go up and down twice its standard error (to get a rough idea):\n\n82.45 + 2 * 27.58 * c(-1, 1)\n\n[1]  27.29 137.61\n\n\nThe c() thing is to get both confidence limits at once. The smoother way is this:\n\nconfint(fb.1)\n\n                2.5 %   97.5 %\n(Intercept) 313.30872 419.9810\nGMdensity    26.61391 138.2836\n\n\nFeed confint a “fitted model object” and it’ll give you confidence intervals (by default 95%) for all the parameters in it.\nThe confidence interval for the slope goes from about 27 to about 138. That is to say, a one-unit increase in grey matter density goes with an increase in Facebook friends of this much. This is not especially insightful: it’s bigger than zero (the test was significant), but other than that, it could be almost anything. This is where the weakness of the trend comes back to bite us. With this much scatter in our data, we need a much larger sample size to estimate accurately how big an effect grey matter density has.\n\\(\\blacksquare\\)\n\nObtain a scatterplot with the regression line on it.\n\nSolution\nJust a modification of (a):\n\nggplot(fb, aes(x = GMdensity, y = FBfriends)) + geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\nObtain a plot of the residuals from the regression against the fitted values, and comment briefly on it.\n\nSolution\nThis is, to my mind, the easiest way:\n\nggplot(fb.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nThere is some “magic” here, since the fitted model object is not actually a data frame, but it works this way. That looks to me like a completely random scatter of points. Thus, I am completely happy with the straight-line regression that we fitted, and I see no need to improve it.\n(You should make two points here: one, describe what you see, and two, what it implies about whether or not your regression is satisfactory.)\nCompare that residual plot with this one:\n\nggplot(fb.1, aes(x = .fitted, y = .resid)) +\n  geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nNow, why did I try adding a smooth trend, and why is it not necessarily a good idea? The idea of a residual plot is that there should be no trend, and so the smooth trend curve ought to go straight across. The problem is that it will tend to wiggle, just by chance, as here: it looks as if it goes up and down before flattening out. But if you look at the points, they are all over the place, not close to the smooth trend at all. So the smooth trend is rather deceiving. Or, to put it another way, to indicate a real problem, the smooth trend would have to be a lot farther from flat than this one is. I’d call this one basically flat.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#endogenous-nitrogen-excretion-in-carp-1",
    "href": "simple-regression.html#endogenous-nitrogen-excretion-in-carp-1",
    "title": "16  Simple regression",
    "section": "16.19 Endogenous nitrogen excretion in carp",
    "text": "16.19 Endogenous nitrogen excretion in carp\nA paper in Fisheries Science reported on variables that affect “endogenous nitrogen excretion” or ENE in carp raised in Japan. A number of carp were divided into groups based on body weight, and each group was placed in a different tank. The mean body weight of the carp placed in each tank was recorded. The carp were then fed a protein-free diet three times daily for a period of 20 days. At the end of the experiment, the amount of ENE in each tank was measured, in milligrams of total fish body weight per day. (Thus it should not matter that some of the tanks had more fish than others, because the scaling is done properly.)\nFor this question, write a report in R Markdown that answers the questions below and contains some narrative that describes your analysis. Create an HTML document from your R Markdown.\n\nRead the data in from link. There are 10 tanks.\n\nSolution\nJust this. Listing the data is up to you, but doing so and commenting that the values appear to be correct will improve your report.\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/carp.txt\"\ncarp &lt;- read_delim(my_url, \" \")\n\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): tank, bodyweight, ENE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncarp\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nCreate a scatterplot of ENE (response) against bodyweight (explanatory). Add a smooth trend to your plot.\n\nSolution\n\nggplot(carp, aes(x = bodyweight, y = ENE)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis part is just about getting the plot. Comments are coming in a minute. Note that ENE is capital letters, so that ene will not work.\n\\(\\blacksquare\\)\n\nIs there an upward or downward trend (or neither)? Is the relationship a line or a curve? Explain briefly.\n\nSolution\nThe trend is downward: as bodyweight increases, ENE decreases. However, the decrease is rapid at first and then levels off, so the relationship is nonlinear. I want some kind of support for an assertion of non-linearity: anything that says that the slope or rate of decrease is not constant is good.\n\\(\\blacksquare\\)\n\nFit a straight line to the data, and obtain the R-squared for the regression.\n\nSolution\nlm. The first stage is to fit the straight line, saving the result in a variable, and the second stage is to look at the “fitted model object”, here via summary:\n\ncarp.1 &lt;- lm(ENE ~ bodyweight, data = carp)\nsummary(carp.1)\n\n\nCall:\nlm(formula = ENE ~ bodyweight, data = carp)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.800 -1.957 -1.173  1.847  4.572 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.40393    1.31464   8.675 2.43e-05 ***\nbodyweight  -0.02710    0.01027  -2.640   0.0297 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.928 on 8 degrees of freedom\nMultiple R-squared:  0.4656,    Adjusted R-squared:  0.3988 \nF-statistic: 6.971 on 1 and 8 DF,  p-value: 0.0297\n\n\nFinally, you need to give me a (suitably rounded) value for R-squared: 46.6% or 47% or the equivalents as a decimal. I just need the value at this point. This kind of R-squared is actually pretty good for natural data, but the issue is whether we can improve it by fitting a non-linear model.9\n\\(\\blacksquare\\)\n\nObtain a residual plot (residuals against fitted values) for this regression. Do you see any problems? If so, what does that tell you about the relationship in the data?\n\nSolution\nThis is the easiest way: feed the output of the regression straight into ggplot:\n\nggplot(carp.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\n\\(\\blacksquare\\)\n\nFit a parabola to the data (that is, including an \\(x\\)-squared term). Compare the R-squared values for the models in this part and part (d). Does that suggest that the parabola model is an improvement here over the linear model?\n\nSolution\nAdd bodyweight-squared to the regression. Don’t forget the I():\n\ncarp.2 &lt;- lm(ENE ~ bodyweight + I(bodyweight^2), data = carp)\nsummary(carp.2)\n\n\nCall:\nlm(formula = ENE ~ bodyweight + I(bodyweight^2), data = carp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0834 -1.7388 -0.5464  1.3841  2.9976 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     13.7127373  1.3062494  10.498 1.55e-05 ***\nbodyweight      -0.1018390  0.0288109  -3.535  0.00954 ** \nI(bodyweight^2)  0.0002735  0.0001016   2.692  0.03101 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.194 on 7 degrees of freedom\nMultiple R-squared:  0.7374,    Adjusted R-squared:  0.6624 \nF-statistic: 9.829 on 2 and 7 DF,  p-value: 0.009277\n\n\nR-squared has gone up from 47% to 74%, a substantial improvement. This suggests to me that the parabola model is a substantial improvement.10\nI try to avoid using the word “significant” in this context, since we haven’t actually done a test of significance.\nThe reason for the I() is that the up-arrow has a special meaning in lm, relating to interactions between factors (as in ANOVA), that we don’t want here. Putting I() around it means “use as is”, that is, raise bodyweight to power 2, rather than using the special meaning of the up-arrow in lm.\nBecause it’s the up-arrow that is the problem, this applies whenever you’re raising an explanatory variable to a power (or taking a reciprocal or a square root, say).\n\\(\\blacksquare\\)\n\nIs the test for the slope coefficient for the squared term significant? What does this mean?\n\nSolution\nLook along the bodyweight-squared line to get a P-value of 0.031. This is less than the default 0.05, so it is significant. This means, in short, that the quadratic model is a significant improvement over the linear one.11 Said longer: the null hypothesis being tested is that the slope coefficient of the squared term is zero (that is, that the squared term has nothing to add over the linear model). This is rejected, so the squared term has something to add in terms of quality of prediction.\n\\(\\blacksquare\\)\n\nMake the scatterplot of part (b), but add the fitted curve. Describe any way in which the curve fails to fit well.\n\nSolution\nThis is a bit slippery, because the points to plot and the fitted curve are from different data frames. What you do in this case is to put a data= in one of the geoms, which says “don’t use the data frame that was in the ggplot, but use this one instead”. I would think about starting with the regression object carp.2 as my base data frame, since we want (or I want) to do two things with that: plot the fitted values and join them with lines. Then I want to add the original data, just the points:\n\nggplot(carp.2, aes(x = carp$bodyweight, y = .fitted), colour = \"blue\") +\n  geom_line(colour = \"blue\") +\n  geom_point(data = carp, aes(x = bodyweight, y = ENE))\n\n\n\n\nThis works, but is not very aesthetic, because the bodyweight that is plotted against the fitted values is in the wrong data frame, and so we have to use the dollar-sign thing to get it from the right one.\nA better way around this is “augment” the data with output from the regression object. This is done using augment from package broom:\n\nlibrary(broom)\ncarp.2a &lt;- augment(carp.2, carp)\ncarp.2a\n\n\n\n  \n\n\n\nso now you see what carp.2a has in it, and then:\n\ng &lt;- ggplot(carp.2a, aes(x = bodyweight, y = .fitted)) +\n  geom_line(colour = \"blue\") +\n  geom_point(aes(y = ENE))\n\nThis is easier coding: there are only two non-standard things. The first is that the fitted-value lines should be a distinct colour like blue so that you can tell them from the data points. The second thing is that for the second geom_point, the one that plots the data, the \\(x\\) coordinate bodyweight is correct so that we don’t have to change that; we only have to change the \\(y\\)-coordinate, which is ENE. The plot is this:\n\ng\n\n\n\n\nConcerning interpretation, you have a number of possibilities here. The simplest is that the points in the middle are above the curve, and the points at the ends are below. (That is, negative residuals at the ends, and positive ones in the middle, which gives you a hint for the next part.) Another is that the parabola curve fails to capture the shape of the relationship; for example, I see nothing much in the data suggesting that the relationship should go back up, and even given that, the fitted curve doesn’t go especially near any of the points.\nI was thinking that the data should be fit better by something like the left half of an upward-opening parabola, but I guess the curvature on the left half of the plot suggests that it needs most of the left half of the parabola just to cover the left half of the plot.\nThe moral of the story, as we see in the next part, is that the parabola is the wrong curve for the job.\n\\(\\blacksquare\\)\n\nObtain a residual plot for the parabola model. Do you see any problems with it? (If you do, I’m not asking you to do anything about them in this question, but I will.)\n\n\\(\\blacksquare\\)\nThe same idea as before for the other residual plot. Use the fitted model object carp.2 as your data frame for the ggplot:\n\nggplot(carp.2, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nI think this is still a curve (or, it goes down and then sharply up at the end). Either way, there is still a pattern.\nThat was all I needed, but as to what this means: our parabola was a curve all right, but it appears not to be the right kind of curve. I think the original data looks more like a hyperbola (a curve like \\(y=1/x\\)) than a parabola, in that it seems to decrease fast and then gradually to a limit, and that suggests, as in the class example, that we should try an asymptote model. Note how I specify it, with the I() thing again, since / has a special meaning to lm in the same way that ^ does:\n\ncarp.3 &lt;- lm(ENE ~ I(1 / bodyweight), data = carp)\nsummary(carp.3)\n\n\nCall:\nlm(formula = ENE ~ I(1/bodyweight), data = carp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.29801 -0.12830  0.04029  0.26702  0.91707 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       5.1804     0.2823   18.35 8.01e-08 ***\nI(1/bodyweight) 107.6690     5.8860   18.29 8.21e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6121 on 8 degrees of freedom\nMultiple R-squared:  0.9766,    Adjusted R-squared:  0.9737 \nF-statistic: 334.6 on 1 and 8 DF,  p-value: 8.205e-08\n\n\nThat fits extraordinarily well, with an R-squared up near 98%. The intercept is the asymptote, which suggests a (lower) limit of about 5.2 for ENE (in the limit for large bodyweight). We would have to ask the fisheries scientist whether this kind of thing is a reasonable biological mechanism. It says that a carp always has some ENE, no matter how big it gets, but a smaller carp will have a lot more.\nDoes the fitted value plot look reasonable now? This is augment again since the fitted values and observed data come from different data frames:\n\nlibrary(broom)\naugment(carp.3, carp) %&gt;%\n  ggplot(aes(x = bodyweight, y = .fitted)) +\n  geom_line(colour = \"blue\") +\n  geom_point(aes(y = ENE))\n\n\n\n\nI’d say that does a really nice job of fitting the data. But it would be nice to have a few more tanks with large-bodyweight fish, to convince us that we have the shape of the trend right.\nAnd, as ever, the residual plot. That’s a lot easier than the plot we just did:\n\nggplot(carp.3, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nAll in all, that looks pretty good (and certainly a vast improvement over the ones you got before).\nWhen you write up your report, you can make it flow better by writing it in a way that suggests that each thing was the obvious thing to do next: that is, that you would have thought to do it next, rather than me telling you what to do.\nMy report (as an R Markdown file) is at link. Download it, knit it, play with it.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#salaries-of-social-workers-1",
    "href": "simple-regression.html#salaries-of-social-workers-1",
    "title": "16  Simple regression",
    "section": "16.20 Salaries of social workers",
    "text": "16.20 Salaries of social workers\nAnother salary-prediction question: does the number of years of work experience that a social worker has help to predict their salary? Data for 50 social workers are in link.\n\nRead the data into R. Check that you have 50 observations on two variables. Also do something to check that the years of experience and annual salary figures look reasonable overall.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/socwork.txt\"\nsoc &lt;- read_delim(my_url, \" \")\n\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): experience, salary\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsoc\n\n\n\n  \n\n\n\nThat checks that we have the right number of observations; to check that we have sensible values, something like summary is called for:\n\nsummary(soc)\n\n   experience        salary     \n Min.   : 1.00   Min.   :16105  \n 1st Qu.:13.50   1st Qu.:36990  \n Median :20.00   Median :50948  \n Mean   :18.12   Mean   :50171  \n 3rd Qu.:24.75   3rd Qu.:65204  \n Max.   :28.00   Max.   :99139  \n\n\nA person working in any field cannot have a negative number of years of experience, and cannot have more than about 40 years of experience (or else they would have retired). Our experience numbers fit that. Salaries had better be five or six figures, and salaries for social workers are not generally all that high, so these figures look reasonable.\nA rather more tidyverse way is this:\n\nsoc %&gt;% \n  summarize(across(everything(), \n                   list(min = \\(x) min(x),  max = \\(x) max(x))))\n\n\n\n  \n\n\n\nThis gets the minimum and maximum of all the variables. I would have liked them arranged in a nice rectangle (min and max as rows, the variables as columns), but that’s not how this came out. We fix that shortly.\nThe code so far uses across. This means to do something across multiple columns. In this case, we want to do the calculation on all the columns, so we use the select-helper everything. You can use any of the other select-helpers like starts_with, or you could do something like where(is.numeric) to do your summaries only on the quantitative columns (which would also work here). The thing after the everything() means “for each column selected, work out the min and max of it”; x is our name for “the variable we are looking at at the moment”.\nWhat, you want a nice rectangle? This is a pivot-longer, but a fancy version because the column names encode two kinds of things, a variable and a statistic. I took the view that I wanted variables in columns (as usual), and the different summary statistics in rows. This means that the first part of the column names we created above (eg. the salary part of salary_min) should stay in columns, and the rest of it should be pivoted longer. That means using the special name .value for the things that should stay as columns:\n\nsoc %&gt;% \n  summarize(across(everything(), \n                   list(min = \\(x) min(x),  max = \\(x) max(x)))) %&gt;% \n  pivot_longer(everything(), \n               names_to = c(\".value\", \"statistic\"), \n               names_sep = \"_\"\n               )\n\n\n\n  \n\n\n\nNote that we’re using two simpler tools here, rather than one complicated one: first we get the summary statistics, and once we have that, we can do some tidying to get it arranged the way we want.\nYour first guess is likely to be to make it too long:\n\nsoc %&gt;% \n  summarize(across(everything(), \n                   list(min = \\(x) min(x),  max = \\(x) max(x)))) %&gt;% \n  pivot_longer(everything(), \n               names_to = c(\"variable\", \"statistic\"), \n               names_sep = \"_\", \n               values_to = \"value\"\n               )\n\n\n\n  \n\n\n\nand then you’ll have to make it wider, or recall that you can do the thing with .value. We are working “columnwise”, doing something for each column, no matter how many there are. My go-to for this stuff is here.\nAnother way to work is with the five-number summary. This gives a more nuanced picture of the data values we have.12\nThe base-R five-number summary looks like this:\n\nqq &lt;- quantile(soc$experience)\nqq\n\n   0%   25%   50%   75%  100% \n 1.00 13.50 20.00 24.75 28.00 \n\n\nThis is what’s known as a “named vector”. The numbers on the bottom are the summaries themselves, and the names above say which percentile you are looking at. Unfortunately, the tidyverse doesn’t like names, so modelling after the above doesn’t quite work:\n\nsoc %&gt;% \n  summarize(across(everything(), list(q = \\(x) quantile(x))))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n\n\n  \n\n\n\nYou can guess which percentile is which (they have to be in order), but this is not completely satisfactory. It also gives a warning because the summary is five numbers long, rather than only one (like the mean, for example), and this is not the preferred way to handle this.\nThe warning mentions reframe, which is new (as in, less than a year old as I write this). Let’s see how it goes here:\n\nsoc %&gt;% \n  reframe(q_exp = quantile(experience), q_sal = quantile(salary))\n\n\n\n  \n\n\n\nThe idea is that reframe is like summarize, but it is designed for when your summary function returns more than one number, not just one number per group like mean or median do.\nThis is not quite the best (I don’t see the percentiles and I have to repeat myself), but at least I no longer get a warning. Here’s how you do it with across:\n\nsoc %&gt;% \n  reframe(across(everything(), \\(x) enframe(quantile(x)), .unpack = TRUE))\n\n\n\n  \n\n\n\nThe enframe turns a “named vector” (that is, a thing like my qq above) into a dataframe with two columns, one called name with the names (percentiles), and one called value with the values. By using across, you get those two columns for each variable, and you can see which of the five numbers is which percentile in each case.\n\\(\\blacksquare\\)\n\nMake a scatterplot showing how salary depends on experience. Does the nature of the trend make sense?\n\nSolution\nThe usual:\n\nggplot(soc, aes(x = experience, y = salary)) + geom_point()\n\n\n\n\nAs experience goes up, salary also goes up, as you would expect. Also, the trend seems more or less straight.\n\\(\\blacksquare\\)\n\nFit a regression predicting salary from experience, and display the results. Is the slope positive or negative? Does that make sense?\n\nSolution\n\nsoc.1 &lt;- lm(salary ~ experience, data = soc)\nsummary(soc.1)\n\n\nCall:\nlm(formula = salary ~ experience, data = soc)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17666.3  -5498.2   -726.7   4667.7  27811.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11368.7     3160.3   3.597 0.000758 ***\nexperience    2141.4      160.8  13.314  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8642 on 48 degrees of freedom\nMultiple R-squared:  0.7869,    Adjusted R-squared:  0.7825 \nF-statistic: 177.3 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\nThe slope is (significantly) positive, which squares with our guess (more experience goes with greater salary), and also the upward trend on the scatterplot. The value of the slope is about 2,000; this means that one more year of experience goes with about a $2,000 increase in salary.\n\\(\\blacksquare\\)\n\nObtain and plot the residuals against the fitted values. What problem do you see?\n\nSolution\nThe easiest way to do this with ggplot is to plot the regression object (even though it is not actually a data frame), and plot the .fitted and .resid columns in it, not forgetting the initial dots:\n\nggplot(soc.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nI see a “fanning-out”: the residuals are getting bigger in size (further away from zero) as the fitted values get bigger. That is, when the (estimated) salary gets larger, it also gets more variable.\nFanning-out is sometimes hard to see. What you can do if you suspect that it might have happened is to plot the absolute value of the residuals against the fitted values. The absolute value is the residual without its plus or minus sign, so if the residuals are getting bigger in size, their absolute values are getting bigger. That would look like this:\n\nggplot(soc.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI added a smooth trend to this to help us judge whether the absolute-value-residuals are getting bigger as the fitted values get bigger. It looks to me as if the overall trend is an increasing one, apart from those few small fitted values that have larger-sized residuals. Don’t get thrown off by the kinks in the smooth trend. Here is a smoother version:\n\nggplot(soc.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth(span = 2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe larger fitted values, according to this, have residuals larger in size.\nThe thing that controls the smoothness of the smooth trend is the value of span in geom_smooth. The default is 0.75. The larger the value you use, the smoother the trend; the smaller, the more wiggly. I’m inclined to think that the default value is a bit too small. Possibly this value is too big, but it shows you the idea.\n\\(\\blacksquare\\)\n\nThe problem you unearthed in the previous part is often helped by a transformation. Run Box-Cox on your data to find a suitable transformation. What transformation is suggested?\n\nSolution\nYou’ll need to load (and install if necessary) the package MASS that contains boxcox:\n\nlibrary(MASS)\n\nWhen you run this, you may see a warning containing the word “masked”. I talk about that below.\n\nboxcox(salary ~ experience, data = soc)\n\n\n\n\nThat one looks like \\(\\lambda=0\\) or log. You could probably also justify fourth root (power 0.25), but log is a very common transformation, which people won’t need much persuasion to accept.\nExtra: There’s one annoyance with MASS: it has a select (which I have never used), and if you load tidyverse first and MASS second, as I have done here, when you mean to run the column-selection select, it will actually run the select that comes from MASS, and give you an error that you will have a terrible time debugging. That’s what that “masked” message was when you loaded MASS. This is a great place to learn about the conflicted package. See here for how it works. (Scroll down to under the list of files.)\nIf you want to insist on something like “the select that lives in dplyr”, you can do that by saying dplyr::select. But this is kind of cumbersome if you don’t need to do it.\n\\(\\blacksquare\\)\n\nUse your transformed response in a regression, showing the summary.\n\nSolution\nYou can do the transformation right in the lm, as I do below, or if you prefer, you can create a new column that is the log-salary and then use that in the lm. Either way is good:\n\nsoc.3 &lt;- lm(log(salary) ~ experience, data = soc)\nsummary(soc.3)\n\n\nCall:\nlm(formula = log(salary) ~ experience, data = soc)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.35435 -0.09046 -0.01725  0.09739  0.26355 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9.841315   0.056356  174.63   &lt;2e-16 ***\nexperience  0.049979   0.002868   17.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1541 on 48 degrees of freedom\nMultiple R-squared:  0.8635,    Adjusted R-squared:  0.8607 \nF-statistic: 303.7 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\n\\(\\blacksquare\\)\n\nObtain and plot the residuals against the fitted values for this regression. Do you seem to have solved the problem with the previous residual plot?\n\nSolution\nAs we did before, treating the regression object as if it were a data frame:\n\nggplot(soc.3, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nThat, to my mind, is a horizontal band of points, so I would say yes, I have solved the fanning out.\nOne concern I have about the residuals is that there seem to be a couple of very negative values: that is, are the residuals normally distributed as they should be? Well, that’s easy enough to check:\n\nggplot(soc.3, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nThe issues here are that those bottom two values are a bit too low, and the top few values are a bit bunched up (that curve at the top). It is really not bad, though, so I am making the call that I don’t think I needed to worry. Note that the transformation we found here is the same as the log-salary used by the management consultants in the backward-elimination question, and with the same effect: an extra year of experience goes with a percent increase in salary.\nWhat increase? Well, the slope is about 0.05, so adding a year of experience is predicted to increase log-salary by 0.05, or to multiply actual salary by\n\nexp(0.05)\n\n[1] 1.051271\n\n\nor to increase salary by about 5%.13\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#predicting-volume-of-wood-in-pine-trees-1",
    "href": "simple-regression.html#predicting-volume-of-wood-in-pine-trees-1",
    "title": "16  Simple regression",
    "section": "16.21 Predicting volume of wood in pine trees",
    "text": "16.21 Predicting volume of wood in pine trees\nIn forestry, the financial value of a tree is the volume of wood that it contains. This is difficult to estimate while the tree is still standing, but the diameter is easy to measure with a tape measure (to measure the circumference) and a calculation involving \\(\\pi\\), assuming that the cross-section of the tree is at least approximately circular. The standard measurement is “diameter at breast height” (that is, at the height of a human breast or chest), defined as being 4.5 feet above the ground.\nSeveral pine trees had their diameter measured shortly before being cut down, and for each tree, the volume of wood was recorded. The data are in link. The diameter is in inches and the volume is in cubic inches. Is it possible to predict the volume of wood from the diameter?\n\nRead the data into R and display the values (there are not very many).\n\nSolution\nObserve that the data values are separated by spaces, and therefore that read_delim will do it:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/pinetrees.txt\"\ntrees &lt;- read_delim(my_url, \" \")\n\nRows: 10 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): diameter, volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntrees\n\n\n\n  \n\n\n\nThat looks like the data file.\n\\(\\blacksquare\\)\n\nMake a suitable plot.\n\nSolution\nNo clues this time. You need to recognize that you have two quantitative variables, so that a scatterplot is called for. Also, the volume is the response, so that should go on the \\(y\\)-axis:\n\nggplot(trees, aes(x = diameter, y = volume)) + geom_point()\n\n\n\n\nYou can put a smooth trend on it if you like, which would look like this:\n\nggplot(trees, aes(x = diameter, y = volume)) +\n  geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI’ll take either of those for this part, though I think the smooth trend actually obscures the issue here (because there is not so much data).\n\\(\\blacksquare\\)\n\nDescribe what you learn from your plot about the relationship between diameter and volume, if anything.\n\nSolution\nThe word “relationship” offers a clue that a scatterplot would have been a good idea, if you hadn’t realized by now. I am guided by “form, direction, strength” in looking at a scatterplot:\n\nForm: it is an apparently linear relationship.\nDirection: it is an upward trend: that is, a tree with a larger diameter also has a larger volume of wood. (This is not very surprising.)\nStrength: I’d call this a strong (or moderate-to-strong) relationship. (We’ll see in a minute what the R-squared is.)\n\nYou don’t need to be as formal as this, but you do need to get at the idea that it is an upward trend, apparently linear, and at least fairly strong.14\n\\(\\blacksquare\\)\n\nFit a (linear) regression, predicting volume from diameter, and obtain the summary. How would you describe the R-squared?\n\nSolution\nMy naming convention is (usually) to call the fitted model object by the name of the response variable and a number.15\n\nvolume.1 &lt;- lm(volume ~ diameter, data = trees)\nsummary(volume.1)\n\n\nCall:\nlm(formula = volume ~ diameter, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.497  -9.982   1.751   8.959  28.139 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -191.749     23.954  -8.005 4.35e-05 ***\ndiameter      10.894      0.801  13.600 8.22e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.38 on 8 degrees of freedom\nMultiple R-squared:  0.9585,    Adjusted R-squared:  0.9534 \nF-statistic:   185 on 1 and 8 DF,  p-value: 8.217e-07\n\n\nR-squared is nearly 96%, so the relationship is definitely a strong one.\nI also wanted to mention the broom package, which was installed with the tidyverse but which you need to load separately. It provides two handy ways to summarize a fitted model (regression, analysis of variance or whatever):\n\nlibrary(broom)\nglance(volume.1)\n\n\n\n  \n\n\n\nThis gives a one-line summary of a model, including things like R-squared. This is handy if you’re fitting more than one model, because you can collect the one-line summaries together into a data frame and eyeball them.\nThe other summary is this one:\n\ntidy(volume.1)\n\n\n\n  \n\n\n\nThis gives a table of intercepts, slopes and their P-values, but the value to this one is that it is a data frame, so if you want to pull anything out of it, you know how to do that:16\n\ntidy(volume.1) %&gt;% filter(term == \"diameter\")\n\n\n\n  \n\n\n\nThis gets the estimated slope and its P-value, without worrying about the corresponding things for the intercept, which are usually of less interest anyway.\n\\(\\blacksquare\\)\n\nDraw a graph that will help you decide whether you trust the linearity of this regression. What do you conclude? Explain briefly.\n\nSolution\nThe thing I’m fishing for is a residual plot (of the residuals against the fitted values), and on it you are looking for a random mess of nothingness:\n\nggplot(volume.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nMake a call. You could say that there’s no discernible pattern, especially with such a small data set, and therefore that the regression is fine. Or you could say that there is fanning-in: the two points on the right have residuals close to 0 while the points on the left have residuals larger in size. Say something.\nI don’t think you can justify a curve or a trend, because the residuals on the left are both positive and negative.\nMy feeling is that the residuals on the right are close to 0 because these points have noticeably larger diameter than the others, and they are influential points in the regression that will pull the line closer to themselves. This is why their residuals are close to zero. But I am happy with either of the points made in the paragraph under the plot.\n\\(\\blacksquare\\)\n\nWhat would you guess would be the volume of a tree of diameter zero? Is that what the regression predicts? Explain briefly.\n\nSolution\nLogically, a tree that has diameter zero is a non-existent tree, so its volume should be zero as well. In the regression, the quantity that says what volume is when diameter is zero is the intercept. Here the intercept is \\(-192\\), which is definitely not zero. In fact, if you look at the P-value, the intercept is significantly less than zero. Thus, the model makes no logical sense for trees of small diameter. The smallest tree in the data set has diameter 18, which is not really small, I suppose, but it is a little disconcerting to have a model that makes no logical sense.\n\\(\\blacksquare\\)\n\nA simple way of modelling a tree’s shape is to pretend it is a cone, like this, but probably taller and skinnier:\n\n\nwith its base on the ground. What is the relationship between the diameter (at the base) and volume of a cone? (If you don’t remember, look it up. You’ll probably get a formula in terms of the radius, which you’ll have to convert. Cite the website you used.)\nSolution\nAccording to link, the volume of a cone is \\(V=\\pi r^2h/3\\), where \\(V\\) is the volume, \\(r\\) is the radius (at the bottom of the cone) and \\(h\\) is the height. The diameter is twice the radius, so replace \\(r\\) by \\(d/2\\), \\(d\\) being the diameter. A little algebra gives \\[ V = \\pi d^2 h / 12.\\]\n\\(\\blacksquare\\)\n\nFit a regression model that predicts volume from diameter according to the formula you obtained in the previous part. You can assume that the trees in this data set are of similar heights, so that the height can be treated as a constant.\nDisplay the results.\n\nSolution\nAccording to my formula, the volume depends on the diameter squared, which I include in the model thus:\n\nvolume.2 &lt;- lm(volume ~ I(diameter^2), data = trees)\nsummary(volume.2)\n\n\nCall:\nlm(formula = volume ~ I(diameter^2), data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.708  -9.065  -5.722   3.032  40.816 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -30.82634   13.82243   -2.23   0.0563 .  \nI(diameter^2)   0.17091    0.01342   12.74 1.36e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.7 on 8 degrees of freedom\nMultiple R-squared:  0.953, Adjusted R-squared:  0.9471 \nF-statistic: 162.2 on 1 and 8 DF,  p-value: 1.359e-06\n\n\nThis adds an intercept as well, which is fine (there are technical difficulties around removing the intercept).\nThat’s as far as I wanted you to go, but (of course) I have a few comments.\nThe intercept here is still negative, but not significantly different from zero, which is a step forward. The R-squared for this regression is very similar to that from our linear model (the one for which the intercept made no sense). So, from that point of view, either model predicts the data well. I should look at the residuals from this one:\n\nggplot(volume.2, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nI really don’t think there are any problems there.\nNow, I said to assume that the trees are all of similar height. This seems entirely questionable, since the trees vary quite a bit in diameter, and you would guess that trees with bigger diameter would also be taller. It seems more plausible that the same kind of trees (pine trees in this case) would have the same “shape”, so that if you knew the diameter you could predict the height, with larger-diameter trees being taller. Except that we don’t have the heights here, so we can’t build a model for that.\nSo I went looking in the literature. I found this paper: link. This gives several models for relationships between volume, diameter and height. In the formulas below, there is an implied “plus error” on the right, and the \\(\\alpha_i\\) are parameters to be estimated.\nFor predicting height from diameter (equation 1 in paper):\n\\[  h = \\exp(\\alpha_1+\\alpha_2 d^{\\alpha_3}) \\]\nFor predicting volume from height and diameter (equation 6):\n\\[  V = \\alpha_1 d^{\\alpha_2} h^{\\alpha_3} \\]\nThis is a take-off on our assumption that the trees were cone-shaped, with cone-shaped trees having \\(\\alpha_1=\\pi/12\\), \\(\\alpha_2=2\\) and \\(\\alpha_3=1\\). The paper uses different units, so \\(\\alpha_1\\) is not comparable, but \\(\\alpha_2\\) and \\(\\alpha_3\\) are (as estimated from the data in the paper, which were for longleaf pine) quite close to 2 and 1.\nLast, the actual relationship that helps us: predicting volume from just diameter (equation 5):\n\\[  V = \\alpha_1 d^{\\alpha_2}\\]\nThis is a power law type of relationship. For example, if you were willing to pretend that a tree was a cone with height proportional to diameter (one way of getting at the idea of a bigger tree typically being taller, instead of assuming constant height as we did), that would imply \\(\\alpha_2=3\\) here.\nThis is non-linear as it stands, but we can bash it into shape by taking logs:\n\\[\n\\ln V = \\ln \\alpha_1 + \\alpha_2 \\ln d\n\\]\nso that log-volume has a linear relationship with log-diameter and we can go ahead and estimate it:\n\nvolume.3 &lt;- lm(log(volume) ~ log(diameter), data = trees)\nsummary(volume.3)\n\n\nCall:\nlm(formula = log(volume) ~ log(diameter), data = trees)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40989 -0.22341  0.01504  0.10459  0.53596 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -5.9243     1.1759  -5.038    0.001 ** \nlog(diameter)   3.1284     0.3527   8.870 2.06e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3027 on 8 degrees of freedom\nMultiple R-squared:  0.9077,    Adjusted R-squared:  0.8962 \nF-statistic: 78.68 on 1 and 8 DF,  p-value: 2.061e-05\n\n\nThe parameter that I called \\(\\alpha_2\\) above is the slope of this model, 3.13. This is a bit different from the figure in the paper, which was 2.19. I think these are comparable even though the other parameter is not (again, measurements in different units, plus, this time we need to take the log of it). I think the “slopes” are comparable because we haven’t estimated our slope all that accurately:\n\nconfint(volume.3)\n\n                  2.5 %    97.5 %\n(Intercept)   -8.635791 -3.212752\nlog(diameter)  2.315115  3.941665\n\n\nFrom 2.3 to 3.9. It is definitely not zero, but we are rather less sure about what it is, and 2.19 is not completely implausible.\nThe R-squared here, though it is less than the other ones we got, is still high. The residuals are these:\n\nggplot(volume.3, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nwhich again seem to show no problems. The residuals are smaller in size now because of the log transformation: the actual and predicted log-volumes are smaller numbers than the actual and predicted volumes, so the residuals are now closer to zero.\nDoes this model behave itself at zero? Well, roughly at least: if the diameter is very small, its log is very negative, and the predicted log-volume is also very negative (the slope is positive). So the predicted actual volume will be close to zero. If you want to make that mathematically rigorous, you can take limits, but that’s the intuition. We can also do some predictions: set up a data frame that has a column called diameter with some diameters to predict for:\n\nd &lt;- tibble(diameter = c(1, 2, seq(5, 50, 5)))\nd\n\n\n\n  \n\n\n\nand then feed that into predictionsfrom package marginaleffects:\n\np &lt;- cbind(predictions(volume.3, newdata = d)) \np %&gt;% select(diameter, estimate, conf.low, conf.high) -&gt; pp\npp\n\n\n\n  \n\n\n\nThese are predicted log-volumes, so we’d better anti-log them. log in R is natural logs, so this is inverted using exp. The ends of the confidence intervals can be exp-ed as well, which I do all at once:\n\npp %&gt;% mutate(across(-diameter, \\(x) exp(x)))\n\n\n\n  \n\n\n\nFor a diameter near zero, the predicted volume appears to be near zero as well. If you don’t like the scientific notation:\n\npp %&gt;% mutate(across(-diameter, \\(x) exp(x))) %&gt;% \n  mutate(across(-diameter, \\(x) format(x, scientific = FALSE)))\n\n\n\n  \n\n\n\nNote now that these, though they look like numbers, are actually text, so if you want to display numbers in non-scientific notation like this, do it at the very end, after you have finished any calculations with the numbers.\n\nI mentioned broom earlier. We can make a data frame out of the one-line summaries of our three models:\n\nbind_rows(glance(volume.1), glance(volume.2), glance(volume.3))\n\n\n\n  \n\n\n\n(I mistakenly put glimpse instead of glance there the first time. The former is for a quick look at a data frame, while the latter is for a quick look at a model.)\nThe three R-squareds are all high, with the one from the third model being a bit lower as we saw before.\nMy code is rather repetitious. There has to be a way to streamline it. I was determined to find out how. My solution involves putting the three models in a list-column, and then using rowwise to get the glance output for each one.\n\ntibble(i = 1:3, model = list(volume.1, volume.2, volume.3)) %&gt;% \n  rowwise() %&gt;% \n  mutate(glances = list(glance(model))) %&gt;% \n  unnest(glances)\n\n\n\n  \n\n\n\nI almost got caught by forgetting the list on the definition of glances. I certainly need it, because the output from glance is a (one-row) dataframe, not a single number.\nIt works. You see the three R-squared values in the first column of numbers. The third model is otherwise a lot different from the others because it has a different response variable.\nOther thoughts:\nHow might you measure or estimate the height of a tree (other than by climbing it and dropping a tape measure down)? One way, that works if the tree is fairly isolated, is to walk away from its base. Periodically, you point at the top of the tree, and when the angle between your arm and the ground reaches 45 degrees, you stop walking. (If it’s greater than 45 degrees, you walk further away, and if it’s less, you walk back towards the tree.) The distance between you and the base of the tree is then equal to the height of the tree, and if you have a long enough tape measure you can measure it.\nThe above works because the tangent of 45 degrees is 1. If you have a device that will measure the actual angle,17 you can be any distance away from the tree, point the device at the top, record the angle, and do some trigonometry to estimate the height of the tree (to which you add the height of your eyes).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#tortoise-shells-and-eggs-1",
    "href": "simple-regression.html#tortoise-shells-and-eggs-1",
    "title": "16  Simple regression",
    "section": "16.22 Tortoise shells and eggs",
    "text": "16.22 Tortoise shells and eggs\nA biologist measured the length of the carapace (shell) of female tortoises, and then x-rayed the tortoises to count how many eggs they were carrying. The length is measured in millimetres. The data are in link. The biologist is wondering what kind of relationship, if any, there is between the carapace length (as an explanatory variable) and the number of eggs (as a response variable).\n\nRead in the data, and check that your values look reasonable.\n\nSolution\nLook at the data first. The columns are aligned and separated by more than one space, so it’s read_table:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/tortoise-eggs.txt\"\ntortoises &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  length = col_double(),\n  eggs = col_double()\n)\n\ntortoises\n\n\n\n  \n\n\n\nThose look the same as the values in the data file. (Some comment is needed here. I don’t much mind what, but something that suggests that you have eyeballed the data and there are no obvious problems: that is what I am looking for.)\n\\(\\blacksquare\\)\n\nObtain a scatterplot, with a smooth trend, of the data.\n\nSolution\nSomething like this:\n\nggplot(tortoises, aes(x = length, y = eggs)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\nThe biologist expected that a larger tortoise would be able to carry more eggs. Is that what the scatterplot is suggesting? Explain briefly why or why not.\n\nSolution\nThe biologist’s expectation is of an upward trend. But it looks as if the trend on the scatterplot is up, then down, ie. a curve rather than a straight line. So this is not what the biologist was expecting.\n\\(\\blacksquare\\)\n\nFit a straight-line relationship and display the summary.\n\nSolution\n\ntortoises.1 &lt;- lm(eggs ~ length, data = tortoises)\nsummary(tortoises.1)\n\n\nCall:\nlm(formula = eggs ~ length, data = tortoises)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7790 -1.1772 -0.0065  2.0487  4.8556 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.43532   17.34992  -0.025    0.980\nlength       0.02759    0.05631   0.490    0.631\n\nResidual standard error: 3.411 on 16 degrees of freedom\nMultiple R-squared:  0.01478,   Adjusted R-squared:  -0.0468 \nF-statistic:  0.24 on 1 and 16 DF,  p-value: 0.6308\n\n\nI didn’t ask for a comment, but feel free to observe that this regression is truly awful, with an R-squared of less than 2% and a non-significant effect of length.\n\\(\\blacksquare\\)\n\nAdd a squared term to your regression, fit that and display the summary.\n\nSolution\nThe I() is needed because the raise-to-a-power symbol has a special meaning in a model formula, and we want to not use that special meaning:\n\ntortoises.2 &lt;- lm(eggs ~ length + I(length^2), data = tortoises)\nsummary(tortoises.2)\n\n\nCall:\nlm(formula = eggs ~ length + I(length^2), data = tortoises)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0091 -1.8480 -0.1896  2.0989  4.3605 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -8.999e+02  2.703e+02  -3.329  0.00457 **\nlength       5.857e+00  1.750e+00   3.347  0.00441 **\nI(length^2) -9.425e-03  2.829e-03  -3.332  0.00455 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.671 on 15 degrees of freedom\nMultiple R-squared:  0.4338,    Adjusted R-squared:  0.3583 \nF-statistic: 5.747 on 2 and 15 DF,  p-value: 0.01403\n\n\nAnother way is to use update:\n\ntortoises.2a &lt;- update(tortoises.1, . ~ . + I(length^2))\nsummary(tortoises.2a)\n\n\nCall:\nlm(formula = eggs ~ length + I(length^2), data = tortoises)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0091 -1.8480 -0.1896  2.0989  4.3605 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -8.999e+02  2.703e+02  -3.329  0.00457 **\nlength       5.857e+00  1.750e+00   3.347  0.00441 **\nI(length^2) -9.425e-03  2.829e-03  -3.332  0.00455 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.671 on 15 degrees of freedom\nMultiple R-squared:  0.4338,    Adjusted R-squared:  0.3583 \nF-statistic: 5.747 on 2 and 15 DF,  p-value: 0.01403\n\n\n\\(\\blacksquare\\)\n\nIs a curve better than a line for these data? Justify your answer in two ways: by comparing a measure of fit, and by doing a suitable test of significance.\n\nSolution\nAn appropriate measure of fit is R-squared. For the straight line, this is about 0.01, and for the regression with the squared term it is about 0.43. This tells us that a straight line fits appallingly badly, and that a curve fits a lot better. This doesn’t do a test, though. For that, look at the slope of the length-squared term in the second regression; in particular, look at its P-value. This is 0.0045, which is small: the squared term is necessary, and taking it out would be a mistake. The relationship really is curved, and trying to describe it with a straight line would be a big mistake.\n\\(\\blacksquare\\)\n\nMake a residual plot for the straight line model: that is, plot the residuals against the fitted values. Does this echo your conclusions of the previous part? In what way? Explain briefly.\n\nSolution\nPlot the things called .fitted and .resid from the regression object, which is not a data frame but you can treat it as if it is for this:\n\nggplot(tortoises.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nUp to you whether you put a smooth trend on it or not:\n\nggplot(tortoises.1, aes(x = .fitted, y = .resid)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLooking at the plot, you see a curve, up and down. The most negative residuals go with small or large fitted values; when the fitted value is in the middle, the residual is usually positive. A curve on the residual plot indicates a curve in the actual relationship. We just found above that a curve does fit a lot better, so this is all consistent.\nAside: the grey “envelope” is wide, so there is a lot of scatter on the residual plot. The grey envelope almost contains zero all the way across, so the evidence for a curve (or any other kind of trend) is not all that strong, based on this plot. This is in great contrast to the regression with length-squared, where the length-squared term is definitely necessary.\nThat was all I wanted, but you can certainly look at other plots. Normal quantile plot of the residuals:\n\nggplot(tortoises.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis is not the best: the low values are a bit too low, so that the whole picture is (a little) skewed to the left.18\nAnother plot you can make is to assess fan-out: you plot the absolute value19 of the residuals against the fitted values. The idea is that if there is fan-out, the absolute value of the residuals will get bigger:\n\nggplot(tortoises.1, aes(x = .fitted, y = abs(.resid))) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI put the smooth curve on as a kind of warning: it looks as if the size of the residuals goes down and then up again as the fitted values increase. But the width of the grey “envelope” and the general scatter of the points suggests that there is really not much happening here at all. On a plot of residuals, the grey envelope is really more informative than the blue smooth trend. On this one, there is no evidence of any fan-out (or fan-in).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#roller-coasters-1",
    "href": "simple-regression.html#roller-coasters-1",
    "title": "16  Simple regression",
    "section": "16.23 Roller coasters",
    "text": "16.23 Roller coasters\nA poll on the Discovery Channel asked people to nominate the best roller-coasters in the United States. We will examine the 10 roller-coasters that received the most votes. Two features of a roller-coaster that are of interest are the distance it drops from start to finish, measured here in feet20 and the duration of the ride, measured in seconds. Is it true that roller-coasters with a bigger drop also tend to have a longer ride? The data are at link.21\n\nRead the data into R and verify that you have a sensible number of rows and columns.\n\nSolution\nA .csv, so the usual for that:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/coasters.csv\"\ncoasters &lt;- read_csv(my_url)\n\nRows: 10 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): coaster_name, state\ndbl (2): drop, duration\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncoasters\n\n\n\n  \n\n\n\nThe number of marks for this kind of thing has been decreasing through the course, since by now you ought to have figured out how to do it without looking it up.\nThere are 10 rows for the promised 10 roller-coasters, and there are several columns: the drop for each roller-coaster and the duration of its ride, as promised, as well as the name of each roller-coaster and the state that it is in. (A lot of them seem to be in Ohio, for some reason that I don’t know.) So this all looks good.\n\\(\\blacksquare\\)\n\nMake a scatterplot of duration (response) against drop (explanatory), labelling each roller-coaster with its name in such a way that the labels do not overlap. Add a regression line to your plot.\n\nSolution\nThe last part, about the labels not overlapping, is an invitation to use ggrepel, which is the way I’d recommend doing this. (If not, you have to do potentially lots of work organizing where the labels sit relative to the points, which is time you probably don’t want to spend.) Thus:\n\nlibrary(ggrepel)\nggplot(coasters, aes(x = drop, y = duration, label = coaster_name)) +\n  geom_point() + geom_text_repel() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nThe se=FALSE at the end is optional; if you omit it, you get that “envelope” around the line, which is fine here.\nNote that with the labelling done this way, you can easily identify which roller-coaster is which.\nThe warning seems to be ggplot being over-zealous; the geom_point and the geom_smooth don’t need a label, but geom_text_repel certainly does. If it bothers you, move the label into the geom_text_repel:\n\nggplot(coasters, aes(x = drop, y = duration)) +\n  geom_point() + geom_text_repel(aes(label = coaster_name)) + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\nWould you say that roller-coasters with a larger drop tend to have a longer ride? Explain briefly.\n\nSolution\nI think there are two good answers here: “yes” and “kind of”. Supporting “yes” is the fact that the regression line does go uphill, so that overall, or on average, roller-coasters with a larger drop do tend to have a longer duration of ride as well. Supporting “kind of” is the fact that, though the regression line goes uphill, there are a lot of roller-coasters that are some way off the trend, far from the regression line. I am happy to go with either of those. I could also go with “not really” and the same discussion that I attached to “kind of”.\n\\(\\blacksquare\\)\n\nFind a roller-coaster that is unusual compared to the others. What about its combination of drop and duration is unusual?\n\nSolution\nThis is an invitation to find a point that is a long way off the line. I think the obvious choice is my first one below, but I would take either of the others as well:\n\n“Nitro” is a long way above the line. That means it has a long duration, relative to its drop. There are two other roller-coasters that have a larger drop but not as long a duration. In other words, this roller-coaster drops slowly, presumably by doing a lot of twisting, loop-the-loop and so on.\n“The Beast” is a long way below the line, so it has a short duration relative to its drop. It is actually the shortest ride of all, but is only a bit below average in terms of drop. This suggests that The Beast is one of those rides that drops a long way quickly.\n“Millennium Force” has the biggest drop of all, but a shorter-than-average duration. This looks like another ride with a big drop in it.\n\nA roller-coaster that is “unusual” will have a residual that is large in size (either positive, like Nitro, or negative, like the other two). I didn’t ask you to find the residuals, but if you want to, augment from broom is the smoothest way to go:\n\nlibrary(broom)\nduration.1 &lt;- lm(duration ~ drop, data = coasters)\naugment(duration.1, coasters) %&gt;%\n  select(coaster_name, duration, drop, .resid) %&gt;%\n  arrange(desc(abs(.resid)))\n\n\n\n  \n\n\n\naugment produces a data frame (of the original data frame with some new columns that come from the regression), so I can feed it into a pipe to do things with it, like only displaying the columns I want, and arranging them in order by absolute value of residual, so that the roller-coasters further from the line come out first. This identifies the three that we found above. The fourth one, “Ghost Rider”, is like Nitro in that it takes a (relatively) long time to fall not very far. You can also put augment in the middle of a pipe. What you may have to do then is supply the original data frame name to augment so that you have everything:\n\ncoasters %&gt;%\n  lm(duration ~ drop, data = .) %&gt;%\n  augment(coasters) %&gt;%\n  arrange(desc(abs(.resid)))\n\n\n\n  \n\n\n\nI wanted to hang on to the roller-coaster names, so I added the data frame name to augment. If you don’t (that is, you just put augment() in the middle of a pipe), then augment “attempts to reconstruct the data from the model”.22 That means you wouldn’t get everything from the original data frame; you would just get the things that were in the regression. In this case, that means you would lose the coaster names.\nA technicality (but one that you should probably care about): augment takes up to two inputs: a fitted model object like my duration.1, and an optional data frame to include other things from, like the coaster names. I had only one input to it in the pipe because the implied first input was the output from the lm, which doesn’t have a name; the input coasters in the pipe was what would normally be the second input to augment.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#running-and-blood-sugar-1",
    "href": "simple-regression.html#running-and-blood-sugar-1",
    "title": "16  Simple regression",
    "section": "16.24 Running and blood sugar",
    "text": "16.24 Running and blood sugar\nA diabetic wants to know how aerobic exercise affects his blood sugar. When his blood sugar reaches 170 (mg/dl), he goes out for a run at a pace of 10 minutes per mile. He runs different distances on different days. Each time he runs, he measures his blood sugar after the run. (The preferred blood sugar level is between 80 and 120 on this scale.) The data are in the file link. Our aim is to predict blood sugar from distance.\n\nRead in the data and display the data frame that you read in.\n\nSolution\nFrom the URL is easiest. These are delimited by one space, as you can tell by looking at the file:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/runner.txt\"\nruns &lt;- read_delim(my_url, \" \")\n\nRows: 12 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): distance, blood_sugar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nruns\n\n\n\n  \n\n\n\nThat looks like my data file.\n\\(\\blacksquare\\)\n\nMake a scatterplot and add a smooth trend to it.\n\nSolution\n\nggplot(runs, aes(x = distance, y = blood_sugar)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nblood_sugar should be on the vertical axis, since this is what we are trying to predict. Getting the x and the y right is easy on these, because they are the \\(x\\) and \\(y\\) for your plot.\n\\(\\blacksquare\\)\n\nWould you say that the relationship between blood sugar and running distance is approximately linear, or not? It is therefore reasonable to use a regression of blood sugar on distance? Explain briefly.\n\nSolution\nI’d say that this is about as linear as you could ever wish for. Neither the pattern of points nor the smooth trend have any kind of noticeable bend in them. (Observing a lack of curvature in either the points or the smooth trend is enough.) The trend is a linear one, so using a regression will be just fine. (If it weren’t, the rest of the question would be kind of dumb.)\n\\(\\blacksquare\\)\n\nFit a suitable regression, and obtain the regression output.\n\nSolution\nTwo steps: lm and then summary:\n\nruns.1 &lt;- lm(blood_sugar ~ distance, data = runs)\nsummary(runs.1)\n\n\nCall:\nlm(formula = blood_sugar ~ distance, data = runs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8238 -3.6167  0.8333  4.0190  5.5476 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  191.624      5.439   35.23 8.05e-12 ***\ndistance     -25.371      1.618  -15.68 2.29e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.788 on 10 degrees of freedom\nMultiple R-squared:  0.9609,    Adjusted R-squared:  0.957 \nF-statistic: 245.7 on 1 and 10 DF,  p-value: 2.287e-08\n\n\n\\(\\blacksquare\\)\n\nHow would you interpret the slope? That is, what is the slope, and what does that mean about blood sugar and running distance?\n\nSolution\nThe slope is \\(-25.37\\). This means that for each additional mile run, the runner’s blood sugar will decrease on average by about 25 units.\nYou can check this from the scatterplot. For example, from 2 to 3 miles, average blood sugar decreases from about 140 to about 115, a drop of 25.\n\\(\\blacksquare\\)\n\nIs there a (statistically) significant relationship between running distance and blood sugar? How do you know? Do you find this surprising, given what you have seen so far? Explain briefly.\n\nSolution\nLook at the P-value either on the distance line (for its \\(t\\)-test) or for the \\(F\\)-statistic on the bottom line. These are the same: 0.000000023. (They will be the same any time there is one \\(x\\)-variable.) This P-value is way smaller than 0.05, so there is a significant relationship between running distance and blood sugar. This does not surprise me in the slightest, because the trend on the scatterplot is so clear, there’s no way it could have happened by chance if in fact there were no relationship between running distance and blood sugar.\n\\(\\blacksquare\\)\n\nThis diabetic is planning to go for a 3-mile run tomorrow and a 5-mile run the day after. Obtain suitable 95% intervals that say what his blood sugar might be after each of these runs.\n\nSolution\nThis is a prediction interval, in each case, since we are talking about individual runs of 3 miles and 5 miles (not the mean blood sugar after all runs of 3 miles, which is what a confidence interval for the mean response would be). The procedure is to set up a data frame with the two distance values in it, and then feed that and the regression object into predict, coming up in a moment.\n\ndists &lt;- c(3, 5)\nnew &lt;- tibble(distance = dists)\nnew\n\n\n\n  \n\n\n\nThe important thing is that the name of the column of the new data frame must be exactly the same as the name of the explanatory variable in the regression. If they don’t match, predict won’t work. At least, it won’t work properly.23\nIf your first thought is datagrid, well, that will also work:\n\nnew2 &lt;- datagrid(model = runs.1, distance = c(5, 10))\nnew2\n\n\n\n  \n\n\n\nUse whichever of these methods comes to your mind.\nThen, predict, because you want prediction intervals rather than confidence intervals for the mean response (which is what marginaleffects gives you):\n\npp &lt;- predict(runs.1, new, interval = \"p\")\npp\n\n        fit       lwr       upr\n1 115.50952 104.37000 126.64905\n2  64.76667  51.99545  77.53788\n\n\nand display this with the distances by the side:\n\ncbind(new, pp)\n\n\n\n  \n\n\n\nor\n\ndata.frame(new, pp)\n\n\n\n  \n\n\n\nBlood sugar after a 3-mile run is predicted to be between 104 and 127; after a 5-mile run it is predicted to be between 52 and 77.5.\nExtra: both cbind and data.frame are “base R” ways of combining a data frame with something else to make a new data frame. They are not from the tidyverse. The tidyverse way is via tibble or bind_cols, but they are a bit more particular about what they will take: tibble takes vectors (single variables) and bind_cols takes vectors or data frames. The problem here is that pp is not either of those:\n\nclass(pp)\n\n[1] \"matrix\" \"array\" \n\n\nso that we have to use as_tibble first to turn it into a data frame, and thus:\n\npp %&gt;% as_tibble() %&gt;% bind_cols(new)\n\n\n\n  \n\n\n\nwhich puts things backwards, unless you do it like this:\n\nnew %&gt;% bind_cols(as_tibble(pp))\n\n\n\n  \n\n\n\nwhich is a pretty result from very ugly code.\nI also remembered that if you finish with a select, you get the columns in the order they were in the select:\n\npp %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(new) %&gt;%\n  select(c(distance, everything()))\n\n\n\n  \n\n\n\neverything is a so-called “select helper”. It means “everything except any columns you already named”, so this whole thing has the effect of listing the columns with distance first and all the other columns afterwards, in the order that they were in before.\n\\(\\blacksquare\\)\n\nWhich of your two intervals is longer? Does this make sense? Explain briefly.\n\nSolution\nThe intervals are about 22.25 and 25.5 units long. The one for a 5-mile run is a bit longer. I think this makes sense because 3 miles is close to the average run distance, so there is a lot of “nearby” data. 5 miles is actually longer than any of the runs that were actually done (and therefore we are actually extrapolating), but the important point for the prediction interval is that there is less nearby data: those 2-mile runs don’t help so much in predicting blood sugar after a 5-mile run. (They help some, because the trend is so linear. This is why the 5-mile interval is not so much longer. If the trend were less clear, the 5-mile interval would be more noticeably worse.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#calories-and-fat-in-pizza-1",
    "href": "simple-regression.html#calories-and-fat-in-pizza-1",
    "title": "16  Simple regression",
    "section": "16.25 Calories and fat in pizza",
    "text": "16.25 Calories and fat in pizza\nThe file at link came from a spreadsheet of information about 24 brands of pizza: specifically, per 5-ounce serving, the number of calories, the grams of fat, and the cost (in US dollars). The names of the pizza brands are quite long. This file may open in a spreadsheet when you browse to the link, depending on your computer’s setup.\n\nRead in the data and display at least some of the data frame. Are the variables of the right types? (In particular, why is the number of calories labelled one way and the cost labelled a different way?)\n\nSolution\nread_csv is the thing this time:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/Pizza.csv\"\npizza &lt;- read_csv(my_url)\n\nRows: 24 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Type\ndbl (3): Calories, Fat, Cost\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npizza\n\n\n\n  \n\n\n\nThe four variables are: the brand of pizza, which got read in as text, the number of calories (an integer), and the fat and cost, which are both decimal numbers so they get labelled dbl, which is short for “double-precision floating point number”.\nAnyway, these are apparently the right thing.\nExtra: I wanted to mention something else that I discovered yesterday.24 There is a package called rio that will read (and write) data in a whole bunch of different formats in a unified way.25 Anyway, the usual installation thing, done once:\n\ninstall.packages(\"rio\")\n\nwhich takes a moment since it probably has to install some other packages too, and then you read in a file like this:\n\nlibrary(rio)\npizza3 &lt;- import(my_url)\nhead(pizza3)\n\n\n\n  \n\n\n\nimport figures that you have a .csv file, so it calls up read_csv or similar.\nTechnical note: rio does not use the read_ functions, so what it gives you is actually a data.frame rather than a tibble, so that when you display it, you get the whole thing even if it is long. Hence the head here and below to display the first six lines.\nI originally had the data as an Excel spreadsheet, but import will gobble up that pizza too:\n\nmy_other_url &lt;- \"http://ritsokiguess.site/datafiles/Pizza_E29.xls\"\npizza4 &lt;- import(my_other_url)\nhead(pizza4)\n\n\n\n  \n\n\n\nThe corresponding function for writing a data frame to a file in the right format is, predictably enough, called export.\n\\(\\blacksquare\\)\n\nMake a scatterplot for predicting calories from the number of grams of fat. Add a smooth trend. What kind of relationship do you see, if any?\n\nSolution\nAll the variable names start with Capital Letters:\n\nggplot(pizza, aes(x = Fat, y = Calories)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThere is definitely an upward trend: the more fat, the more calories. The trend is more or less linear (or, a little bit curved: say what you like, as long as it’s not obviously crazy). I think, with this much scatter, there’s no real justification for fitting a curve.\n\\(\\blacksquare\\)\n\nFit a straight-line relationship, and display the intercept, slope, R-squared, etc. Is there a real relationship between the two variables, or is any apparent trend just chance?\n\nSolution\nlm, with summary:\n\npizza.1 &lt;- lm(Calories ~ Fat, data = pizza)\nsummary(pizza.1)\n\n\nCall:\nlm(formula = Calories ~ Fat, data = pizza)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-55.44 -11.67   6.18  17.87  41.61 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  194.747     21.605   9.014 7.71e-09 ***\nFat           10.050      1.558   6.449 1.73e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.79 on 22 degrees of freedom\nMultiple R-squared:  0.654, Adjusted R-squared:  0.6383 \nF-statistic: 41.59 on 1 and 22 DF,  p-value: 1.731e-06\n\n\nTo assess whether this trend is real or just chance, look at the P-value on the end of the Fat line, or on the bottom line where the \\(F\\)-statistic is (they are the same value of \\(1.73\\times 10^{-6}\\) or 0.0000017, so you can pick either). This P-value is really small, so the slope is definitely not zero, and therefore there really is a relationship between the two variables.\n\\(\\blacksquare\\)\n\nObtain a plot of the residuals against the fitted values for this regression. Does this indicate that there are any problems with this regression, or not? Explain briefly.\n\nSolution\nUse the regression object pizza.1:\n\nggplot(pizza.1, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nOn my residual plot, I see a slight curve in the smooth trend, but I am not worried about that because the residuals on the plot are all over the place in a seemingly random pattern (the grey envelope is wide and that is pretty close to going straight across). So I think a straight line model is satisfactory.\nThat’s all you needed, but it is also worth looking at a normal quantile plot of the residuals:\n\nggplot(pizza.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nA bit skewed to the left (the low ones are too low).\nAlso a plot of the absolute residuals, for assessing fan-out:\n\nggplot(pizza.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nA tiny bit of fan-in (residuals getting smaller in size as the fitted value gets bigger), but nothing much, I think.\nAnother way of assessing curvedness is to fit a squared term anyway, and see whether it is significant:\n\npizza.2 &lt;- update(pizza.1, . ~ . + I(Fat^2))\nsummary(pizza.2)\n\n\nCall:\nlm(formula = Calories ~ Fat + I(Fat^2), data = pizza)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.103 -14.280   5.513  15.423  35.474 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  90.2544    77.8156   1.160   0.2591  \nFat          25.9717    11.5121   2.256   0.0349 *\nI(Fat^2)     -0.5702     0.4086  -1.395   0.1775  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.25 on 21 degrees of freedom\nMultiple R-squared:  0.6834,    Adjusted R-squared:  0.6532 \nF-statistic: 22.66 on 2 and 21 DF,  p-value: 5.698e-06\n\n\nThe fat-squared term is not significant, so that curve on the smooth trend in the (first) residual plot was indeed nothing to get excited about.\n\\(\\blacksquare\\)\n\nThe research assistant in this study returns with two new brands of pizza (ones that were not in the original data). The fat content of a 5-ounce serving was 12 grams for the first brand and 20 grams for the second brand. For each of these brands of pizza, obtain a suitable 95% interval for the number of calories contained in a 5-ounce serving.\n\nSolution\nThe suitable interval here is a prediction interval, because we are interested in each case in the calorie content of the particular pizza brands that the research assistant returned with (and not, for example, in the mean calorie content for all brands of pizza that have 12 grams of fat per serving). Thus:\n\nnewfat &lt;- c(12, 20)\nnew &lt;- tibble(Fat = newfat)\nnew\n\n\n\n  \n\n\npreds &lt;- predict(pizza.1, new, interval = \"p\")\ncbind(new, preds)\n\n\n\n  \n\n\n\nUse datagrid to make new if you like, but it is a very simple dataframe, so there is no obligation to do it that way.\nOr, if you like:\n\nas_tibble(preds) %&gt;% bind_cols(new) %&gt;% select(Fat, everything())\n\n\n\n  \n\n\n\nFor the pizza with 12 grams of fat, the predicted calories are between 261 and 370 with 95% confidence, and for the pizza with 20 grams of fat, the calories are predicted to be between 337 and 454. (You should write down what these intervals are, and not leave the reader to find them in the output.)\n(Remember the steps: create a new data frame containing the values to predict for, and then feed that into predict along with the model that you want to use to predict with. The variable in the data frame has to be called precisely Fat with a capital F, otherwise it won’t work.)\nThese intervals are both pretty awful: you get a very weak picture of how many calories per serving the pizza brands in question might contain. This is for two reasons: (i) there was a fair bit of scatter in the original relationship, R-squared being around 65%, and (ii) even if we knew perfectly where the line went (which we don’t), there’s no guarantee that individual brands of pizza would be on it anyway. (Prediction intervals are always hit by this double whammy, in that individual observations suffer from variability in where the line goes and variability around whatever the line is.)\nI was expecting, when I put together this question, that the 20-grams-of-fat interval would be noticeably worse, because 20 is farther away from the mean fat content of all the brands. But there isn’t much to choose. For the confidence intervals for the mean calories of all brands with these fat contents, the picture is clearer:\n\nplot_cap(pizza.1, condition = \"Fat\")\n\n\n\n\nA fat value of 12 is close to the middle of the data, so the interval is shorter, but a value of 20 is out near the extreme and the interval is noticeably longer.\nThis part was a fair bit of work for 3 points, so I’m not insisting that you explain your choice of a prediction interval over a confidence interval, but I think it is still a smart thing to do, even purely from a marks point of view, because if you get it wrong for a semi-plausible reason, you might pick up some partial credit. Not pulling out your prediction intervals from your output is a sure way to lose a point, however.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#where-should-the-fire-stations-be-1",
    "href": "simple-regression.html#where-should-the-fire-stations-be-1",
    "title": "16  Simple regression",
    "section": "16.26 Where should the fire stations be?",
    "text": "16.26 Where should the fire stations be?\nIn city planning, one major issue is where to locate fire stations. If a city has too many fire stations, it will spend too much on running them, but if it has too few, there may be unnecessary fire damage because the fire trucks take too long to get to the fire.\nThe first part of a study of this kind of issue is to understand the relationship between the distance from the fire station (measured in miles in our data set) and the amount of fire damage caused (measured in thousands of dollars). A city recorded the fire damage and distance from fire station for 15 residential fires (which you can take as a sample of “all possible residential fires in that city”). The data are in link.\n\nRead in and display the data, verifying that you have the right number of rows and the right columns.\n\nSolution\nA quick check of the data reveals that the data values are separated by exactly one space, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/fire_damage.txt\"\nfire &lt;- read_delim(my_url, \" \")\n\nRows: 15 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): distance, damage\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfire\n\n\n\n  \n\n\n\n15 observations (rows), and promised, and a column each of distances and amounts of fire damage, also as promised.\n\\(\\blacksquare\\)\n\n* Obtain a 95% confidence interval for the mean fire damage. (There is nothing here from STAD29, and your answer should have nothing to do with distance.)\n\nSolution\nI wanted to dissuade you from thinking too hard here. It’s just an ordinary one-sample \\(t\\)-test, extracting the interval from it:\n\nt.test(fire$damage)\n\n\n    One Sample t-test\n\ndata:  fire$damage\nt = 12.678, df = 14, p-value = 4.605e-09\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.94488 30.88178\nsample estimates:\nmean of x \n 26.41333 \n\n\nOr\n\nwith(fire, t.test(damage))\n\n\n    One Sample t-test\n\ndata:  damage\nt = 12.678, df = 14, p-value = 4.605e-09\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.94488 30.88178\nsample estimates:\nmean of x \n 26.41333 \n\n\nIgnore the P-value (it’s testing that the mean is the default zero, which makes no sense). The confidence interval either way goes from 21.9 to 30.9 (thousand dollars).\n\\(\\blacksquare\\)\n\nDraw a scatterplot for predicting the amount of fire damage from the distance from the fire station. Add a smooth trend to your plot.\n\nSolution\nWe are predicting fire damage, so that goes on the \\(y\\)-axis:\n\nggplot(fire, aes(x = distance, y = damage)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\n* Is there a relationship between distance from fire station and fire damage? Is it linear or definitely curved? How strong is it? Explain briefly.\n\nSolution\nWhen the distance is larger, the fire damage is definitely larger, so there is clearly a relationship. I would call this one approximately linear: it wiggles a bit, but it is not to my mind obviously curved. I would also call it a strong relationship, since the points are close to the smooth trend.\n\\(\\blacksquare\\)\n\nFit a regression predicting fire damage from distance. How is the R-squared consistent (or inconsistent) with your answer from part~(here)?\n\nSolution\nThe regression is an ordinary lm:\n\ndamage.1 &lt;- lm(damage ~ distance, data = fire)\nsummary(damage.1)\n\n\nCall:\nlm(formula = damage ~ distance, data = fire)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4682 -1.4705 -0.1311  1.7915  3.3915 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  10.2779     1.4203   7.237 6.59e-06 ***\ndistance      4.9193     0.3927  12.525 1.25e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.316 on 13 degrees of freedom\nMultiple R-squared:  0.9235,    Adjusted R-squared:  0.9176 \nF-statistic: 156.9 on 1 and 13 DF,  p-value: 1.248e-08\n\n\nWe need to display the results, since we need to see the R-squared in order to say something about it.\nR-squared is about 92%, high, indicating a strong and linear relationship. Back in part~(here), I said that the relationship is linear and strong, which is entirely consistent with such an R-squared. (If you said something different previously, say how it does or doesn’t square with this kind of R-squared value.)\nPoints: one for fitting the regression, one for displaying it, and two (at the grader’s discretion) for saying what the R-squared is and how it’s consistent (or not) with part~(here).\nExtra: if you thought the trend was “definitely curved”, you would find that a parabola (or some other kind of curve) was definitely better than a straight line. Here’s the parabola:\n\ndamage.2 &lt;- lm(damage ~ distance + I(distance^2), data = fire)\nsummary(damage.2)\n\n\nCall:\nlm(formula = damage ~ distance + I(distance^2), data = fire)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8856 -1.6915 -0.0179  1.5490  3.6278 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    13.3395     2.5303   5.272 0.000197 ***\ndistance        2.6400     1.6302   1.619 0.131327    \nI(distance^2)   0.3376     0.2349   1.437 0.176215    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.227 on 12 degrees of freedom\nMultiple R-squared:  0.9347,    Adjusted R-squared:  0.9238 \nF-statistic: 85.91 on 2 and 12 DF,  p-value: 7.742e-08\n\n\nThere’s no evidence here that a quadratic is better.\nOr you might even have thought from the wiggles that it was more like cubic:\n\ndamage.3 &lt;- update(damage.2, . ~ . + I(distance^3))\nsummary(damage.3)\n\n\nCall:\nlm(formula = damage ~ distance + I(distance^2) + I(distance^3), \n    data = fire)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2325 -1.8377  0.0322  1.1512  3.1806 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    10.8466     4.3618   2.487   0.0302 *\ndistance        5.9555     4.9610   1.200   0.2552  \nI(distance^2)  -0.8141     1.6409  -0.496   0.6296  \nI(distance^3)   0.1141     0.1608   0.709   0.4928  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.274 on 11 degrees of freedom\nMultiple R-squared:  0.9376,    Adjusted R-squared:  0.9205 \nF-statistic: 55.07 on 3 and 11 DF,  p-value: 6.507e-07\n\n\nNo evidence that a cubic is better; that increase in R-squared up to about 94% is just chance (bearing in mind that adding any \\(x\\), even a useless one, will increase R-squared).\nHow bendy is the cubic?\n\nggplot(fire, aes(x = distance, y = damage)) + geom_point() +\n  geom_smooth(method = \"lm\") +\n  geom_line(data = damage.3, aes(y = .fitted), colour = \"red\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe cubic, in red, does bend a little, but it doesn’t do an obvious job of going through the points better than the straight line does. It seems to be mostly swayed by that one observation with damage over 40, and choosing a relationship by how well it fits one point is flimsy at the best of times. So, by Occam’s Razor, we go with the line rather than the cubic because it (i) fits equally well, (ii) is simpler.\n\\(\\blacksquare\\)\n\n Obtain a 95% confidence interval for the mean fire damage for a residence that is 4 miles from the nearest fire station*. (Note the contrast with part~(here).)\n\nSolution\nThis is a confidence interval for a mean response at a given value of the explanatory variable. This is as opposed to part~(here), which is averaged over all distances. So, follow the steps. Make a tiny data frame with this one value of distance:\n\nnew &lt;- datagrid(model = damage.1, distance = 4)\nnew\n\n\n\n  \n\n\n\nand then\n\npp &lt;- cbind(predictions(damage.1, newdata = new))\npp\n\n\n\n  \n\n\n\n28.5 to 31.4 (thousand dollars).\n(I saved this one because I want to refer to it again later.)\n\\(\\blacksquare\\)\n\nCompare the confidence intervals of parts (here) and (here). Specifically, compare their centres and their lengths, and explain briefly why the results make sense.\n\nSolution\nLet me just put them side by side for ease of comparison: part~(here) is:\n\nt.test(fire$damage)\n\n\n    One Sample t-test\n\ndata:  fire$damage\nt = 12.678, df = 14, p-value = 4.605e-09\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.94488 30.88178\nsample estimates:\nmean of x \n 26.41333 \n\n\nand part~(here)’s is\n\npp\n\n\n\n  \n\n\n\nThe centre of the interval is higher for the mean damage when the distance is 4. This is because the mean distance is a bit less than 4:\n\nfire %&gt;% summarize(m = mean(distance))\n\n\n\n  \n\n\n\nWe know it’s an upward trend, so our best guess at the mean damage is higher if the mean distance is higher (in (here), the distance is always 4: we’re looking at the mean fire damage for all residences that are 4 miles from a fire station.)\nWhat about the lengths of the intervals? The one in (here) is about \\(30.9-21.9=9\\) (thousand dollars) long, but the one in (here) is only \\(31.4-28.5=2.9\\) long, much shorter. This makes sense because the relationship is a strong one: knowing the distance from the fire station is very useful, because the bigger it is, the bigger the damage going to be, with near certainty. Said differently, if you know the distance, you can estimate the damage accurately. If you don’t know the distance (as is the case in (here)), you’re averaging over a lot of different distances and thus there is a lot of uncertainty in the amount of fire damage also.\nIf you have some reasonable discussion of the reason why the centres and lengths of the intervals differ, I’m happy. It doesn’t have to be the same as mine.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#making-it-stop-1",
    "href": "simple-regression.html#making-it-stop-1",
    "title": "16  Simple regression",
    "section": "16.27 Making it stop",
    "text": "16.27 Making it stop\nIf you are driving, and you hit the brakes, how far do you travel before coming to a complete stop? Presumably this depends on how fast you are going. Knowing this relationship is important in setting speed limits on roads. For example, on a very bendy road, the speed limit needs to be low, because you cannot see very far ahead, and there could be something just out of sight that you need to stop for.\nData were collected for a typical car and driver, as shown in http://ritsokiguess.site/datafiles/stopping.csv. These are American data, so the speeds are miles per hour and the stopping distances are in feet.\n\nRead in and display (probably all of) the data.\n\nSolution\nThe usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/stopping.csv\"\nstopping &lt;- read_csv(my_url)\n\nRows: 8 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): speed, distance\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstopping\n\n\n\n  \n\n\n\nThere are only eight observations.\n\\(\\blacksquare\\)\n\nMake a suitable plot of the data.\n\nSolution\nTwo quantitative variables means a scatterplot. Stopping distance is the outcome, so that goes on the \\(y\\)-axis:\n\nggplot(stopping, aes(x=speed, y=distance)) + geom_point()\n\n\n\n\n\\(\\blacksquare\\)\n\nDescribe any trend you see in your graph.\n\nSolution\nIt’s an upward trend, but not linear: the stopping distance seems to increase faster at higher speeds.\n\\(\\blacksquare\\)\n\nFit a linear regression predicting stopping distance from speed. (You might have some misgivings about doing this, but do it anyway.)\n\nSolution\nHaving observed a curved relationship, it seems odd to fit a straight line. But we are going to do it anyway and then critique what we have:\n\nstopping.1 &lt;- lm(distance~speed, data=stopping)\nsummary(stopping.1)\n\n\nCall:\nlm(formula = distance ~ speed, data = stopping)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.738 -22.351  -7.738  16.622  47.083 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -44.1667    22.0821   -2.00   0.0924 .  \nspeed         5.6726     0.5279   10.75 3.84e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 34.21 on 6 degrees of freedom\nMultiple R-squared:  0.9506,    Adjusted R-squared:  0.9424 \nF-statistic: 115.5 on 1 and 6 DF,  p-value: 3.837e-05\n\n\nExtra: note that R-squared is actually really high. We come back to that later.\n\\(\\blacksquare\\)\n\nPlot the residuals against the fitted values for this regression.\n\nSolution\n\nggplot(stopping.1, aes(x=.fitted, y=.resid)) + geom_point()\n\n\n\n\n\\(\\blacksquare\\)\n\nWhat do you learn from the residual plot? Does that surprise you? Explain briefly.\n\nSolution\nThe points on the residual plot form a (perfect) curve, so the original relationship was a curve. This is exactly what we saw on the scatterplot, so to me at least, this is no surprise.\n(Make sure you say how you know that the original relationship was a curve from looking at the residual plot. Joined-up thinking. There are two ways we know that the relationship is a curve. Get them both.)\n\\(\\blacksquare\\)\n\nWhat is the actual relationship between stopping distance and speed, according to the physics? See if you can find out. Cite any books or websites that you use: that is, include a link to a website, or give enough information about a book that the grader could find it.\n\nSolution\nI searched for “braking distance and speed” and found the first two things below, that seemed to be useful. Later, I was thinking about the fourth point (which came out of my head) and while searching for other things about that, I found the third thing:\n\na British road safety website, that says “The braking distance depends on how fast the vehicle was travelling before the brakes were applied, and is proportional to the square of the initial speed.”\nthe Wikipedia article on braking distance, which gives the actual formula. This is the velocity squared, divided by a constant that depends on the coefficient of friction. (That is why your driving instructor tells you to leave a bigger gap behind the vehicle in front if it is raining, and an even bigger gap if it is icy.)\nan Australian math booklet that talks specifically about braking distance and derives the formula (and the other equations of motion).\nalso, if you have done physics, you might remember the equation of motion \\(v^2 = u^2 + 2as\\), where \\(u\\) is the initial velocity, \\(v\\) is the final velocity, \\(a\\) is the acceleration and \\(s\\) is the distance covered. In this case, \\(v=0\\) (the car is stationary at the end), and so \\(-u^2/2a = s\\). The acceleration is negative (the car is slowing down), so the left side is, despite appearances, positive. There seems to be a standard assumption that deceleration due to braking is constant (the same for all speeds), at least if you are trying to stop a car in a hurry.\n\nThese are all saying that we should add a speed-squared term to our regression, and then we will have the relationship exactly right, according to the physics.\nExtra: Another way to measure how far you are behind the vehicle in front is time. Many of the British “motorways” (think 400-series highways) were built when I was young, and I remember a TV commercial that said “Only a Fool Breaks the Two Second Rule”.26 In those days (the linked one is from the 1970s),27 a lot of British drivers were not used to going that fast, or on roads that straight, so this was a way to know how big a gap to leave, so that you had time to take evasive action if needed. The value of the two-second rule is that it works for any speed, and you don’t have to remember a bunch of stopping distances. (When I did my (Canadian) driving theory test, I think I worked out and learned a formula for the stopping distances that I could calculate in my head. I didn’t have to get very close since the test was multiple-choice.)\n\\(\\blacksquare\\)\n\nFit the relationship that your research indicated (in the previous part) and display the results. Comment briefly on the R-squared value.\n\nSolution\nAdd a squared term in speed:\n\nstopping.2 &lt;- lm(distance~speed+I(speed^2), data=stopping)\nsummary(stopping.2)\n\n\nCall:\nlm(formula = distance ~ speed + I(speed^2), data = stopping)\n\nResiduals:\n       1        2        3        4        5        6        7        8 \n-1.04167  0.98214  0.08929  1.27976 -0.44643 -0.08929 -2.64881  1.87500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.041667   1.429997   0.728    0.499    \nspeed       1.151786   0.095433  12.069 6.89e-05 ***\nI(speed^2)  0.064583   0.001311  49.267 6.51e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.699 on 5 degrees of freedom\nMultiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 \nF-statistic: 2.462e+04 on 2 and 5 DF,  p-value: 1.039e-10\n\n\nThe R-squared now is basically 1, so that the model fits very close to perfectly.\nExtra: you probably found in your research that the distance should be just something times speed squared, with no constant or linear term. Here, though, we have a significant linear term as well. That is probably just chance, since the distances in the data look as if they have been rounded off. With more accurate values, I think the linear term would have been closer to zero.\nIf you want to go literally for the something-times-speed-squared, you can do that. This doesn’t quite work:\n\nstopping.3x &lt;- lm(distance~I(speed^2), data=stopping)\nsummary(stopping.3x)\n\n\nCall:\nlm(formula = distance ~ I(speed^2), data = stopping)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7327  -3.4670   0.6761   6.2323   8.4513 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.732704   4.362859   3.377   0.0149 *  \nI(speed^2)   0.079796   0.001805  44.218 8.96e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.514 on 6 degrees of freedom\nMultiple R-squared:  0.9969,    Adjusted R-squared:  0.9964 \nF-statistic:  1955 on 1 and 6 DF,  p-value: 8.958e-09\n\n\nbecause it still has an intercept in it. In R, the intercept is denoted by 1. It is always included, unless you explicitly remove it. Some odd things start to happen if you remove the intercept, so it is not a good thing to do unless you know what you are doing. The answers here have some good discussion. Having decided that you are going to remove the intercept, you can remove it the same way as anything else (see update in the multiple regression lecture) with “minus”. I haven’t shown you this, so if you do it, you will need to cite your source: that is, say where you learned what to do:\n\nstopping.3 &lt;- lm(distance~I(speed^2)-1, data=stopping)\nsummary(stopping.3)\n\n\nCall:\nlm(formula = distance ~ I(speed^2) - 1, data = stopping)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.6123  -0.7859  10.5314  15.5314  19.2141 \n\nCoefficients:\n           Estimate Std. Error t value Pr(&gt;|t|)    \nI(speed^2) 0.084207   0.001963   42.89 9.77e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.42 on 7 degrees of freedom\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.9957 \nF-statistic:  1840 on 1 and 7 DF,  p-value: 9.772e-10\n\n\nThe R-squared is still extremely high, much higher than for the straight line. The coefficient value, as I said earlier (citing Wikipedia), depends on the coefficient of friction; the stopping distances you see typically are based on a dry road, so you have to allow extra distance (or time: see above) if the road is not dry.\n\\(\\blacksquare\\)\n\nSomebody says to you “if you have a regression with a high R-squared, like 95%, there is no need to look for a better model.” How do you respond to this? Explain briefly.\n\nSolution\nAn example of a regression with an R-squared of 95% is the straight-line fit from earlier in this question. This is an example of a regression that fits well but is not appropriate because it doesn’t capture the form of the relationship.\nIn general, we are saying that no matter how high R-squared is, we might still be able to improve on the model we have. The flip side is that we might not be able to do any better (with another data set) than an R-squared of, say, 30%, because there is a lot of variability that is, as best as we can assess it, random and not explainable by anything.\nUsing R-squared as a measure of absolute model quality is, thus, a mistake. Or, to say it perhaps more clearly, asking “how high does R-squared have to be to indicate a good fit?” is asking the wrong question. The right thing to do is to concentrate on getting the form of the model right, and thereby get the R-squared as high as we can for that data set (which might be very high, as here, or not high at all).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#predicting-height-from-foot-length-1",
    "href": "simple-regression.html#predicting-height-from-foot-length-1",
    "title": "16  Simple regression",
    "section": "16.28 Predicting height from foot length",
    "text": "16.28 Predicting height from foot length\nIs it possible to estimate the height of a person from the length of their foot? To find out, 33 (male) students had their height and foot length measured. The data are in http://ritsokiguess.site/datafiles/heightfoot.csv.\n\nRead in and display (some of) the data. (If you are having trouble, make sure you have exactly the right URL. The correct URL has no spaces or other strange characters in it.)\n\nSolution\nThe usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/heightfoot.csv\"\nhf &lt;- read_csv(my_url)\n\nRows: 33 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): height, foot\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhf\n\n\n\n  \n\n\n\nCall the data frame whatever you like, but keeping away from the names height and foot is probably wise, since those are the names of the columns.\nThere are indeed 33 rows as promised.\nExtra: my comment in the question was to help you if you copy-pasted the file URL into R Studio. Depending on your setup, this might have gotten pasted with a space in it, at the point where it is split over two lines. The best way to proceed, one that won’t run into this problem, is to right-click on the URL and select Copy Link Address (or the equivalent on your system), and then it will put the whole URL on the clipboard in one piece, even if it is split over two lines in the original document, so that pasting it will work without problems.\n\\(\\blacksquare\\)\n\nMake a suitable plot of the two variables in the data frame.\n\nSolution\nThey are both quantitative, so a scatter plot is called for:\n\nggplot(hf, aes(y=height, x=foot)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI added a smooth trend, or you could just plot the points. (This is better than plotting a regression line at this stage, because we haven’t yet thought about whether the trend is straight.)\nNow that we’ve seen the scatterplot, the trend looks more or less straight (but you should take a look at the scatterplot first, with or without smooth trend, before you put a regression line on it). That point top left is a concern, though, which brings us to…\n\\(\\blacksquare\\)\n\nAre there any observations not on the trend of the other points? What is unusual about those observations?\n\nSolution\nThe observation with height greater than 80 at the top of the graph looks like an outlier and does not follow the trend of the rest of the points. Or, this individual is much taller than you would expect for someone with a foot length of 27 inches. Or, this person is over 7 feet tall, which makes little sense as a height. Say something about what makes this person be off the trend.\n\\(\\blacksquare\\)\n\nFit a regression predicting height from foot length, including any observations that you identified in the previous part. For that regression, plot the residuals against the fitted values and make a normal quantile plot of the residuals.\n\nSolution\nThese things. Displaying the summary of the regression is optional, but gives the grader an opportunity to check that your work is all right so far.\n\nhf.1 &lt;- lm(height~foot, data=hf)\nsummary(hf.1)\n\n\nCall:\nlm(formula = height ~ foot, data = hf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7491 -1.3901 -0.0310  0.8918 12.9690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  34.3363     9.9541   3.449 0.001640 ** \nfoot          1.3591     0.3581   3.795 0.000643 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.102 on 31 degrees of freedom\nMultiple R-squared:  0.3173,    Adjusted R-squared:  0.2952 \nF-statistic: 14.41 on 1 and 31 DF,  p-value: 0.0006428\n\nggplot(hf.1, aes(x=.fitted, y=.resid)) + geom_point()\n\n\n\nggplot(hf.1, aes(sample=.resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nNote that we did not exclude the off-trend point. Removing points because they are outliers is a bad idea. This is a good discussion of the issues.\n\\(\\blacksquare\\)\n\nEarlier, you identified one or more observations that were off the trend. How does this point or points show up on each of the plots you drew in the previous part?\n\nSolution\nOn its own at the top in both cases; the large positive residual on the first plot, and the unusually large value at the top right of the normal quantile plot. (You need to say one thing about each graph, or say as I did that the same kind of thing happens on both graphs.)\nExtra: in the residuals vs. fitted values, the other residuals show a slight upward trend. This is because the regression line for these data, with the outlier, is pulled (slightly) closer to the outlier and thus slightly further away from the other points, particularly the ones on the left, compared to the same data but with the outlier removed (which you will be seeing shortly). If the unusual point had happened to have an extreme \\(x\\) (foot length) as well, the effect would have been more pronounced.\nThis is the kind of thing I mean (made-up data):\n\ntibble(x = 1:10) %&gt;% \nmutate(y = rnorm(10, 10+2*x, 1)) %&gt;% \nmutate(y = ifelse(x == 9, 40, y)) -&gt; madeup\nmadeup\n\n\n\n  \n\n\n\n\nggplot(madeup, aes(x=x, y=y)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe second-last point is off a clearly linear trend otherwise (the smooth gets “distracted” by the outlying off-trend point). Fitting a regression anyway and looking at the residual plot gives this:\n\nmadeup.1 &lt;- lm(y~x, data = madeup)\nggplot(madeup.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nThis time you see a rather more obvious downward trend in the other residuals. The problem is not with them, but with the one very positive residual, corresponding to the outlier that is way off the trend on the scatterplot.\nThe residuals in a regression have to add up to zero. If one of them is very positive (as in the one you did and the example I just showed you), at least some of the other residuals have to become more negative to compensate – the ones on the right just above and the ones on the left in the one you did. If you have done STAC67, you will have some kind of sense of why that is: think about the two equations you have to solve to get the estimates of intercept and slope, and how they are related to the residuals. Slide 6 of this shows them; at the least squares estimates, these two partial derivatives both have to be zero, and the things inside the brackets are the residuals.\n\\(\\blacksquare\\)\n\nAny data points that concerned you earlier were actually errors. Create and save a new data frame that does not contain any of those data points.\n\nSolution\nFind a way to not pick that outlying point. For example, you can choose the observations with height less than 80:\n\nhf %&gt;% filter(height&lt;80) -&gt; hfx\nhfx\n\n\n\n  \n\n\n\nOnly 32 rows left.\nThere are many other possibilities. Find one.\n\\(\\blacksquare\\)\n\nRun a regression predicting height from foot length for your data set without errors. Obtain a plot of the residuals against fitted values and a normal quantile plot of the residuals for this regression.\n\nSolution\nCode-wise, the same as before, but with the new data set:\n\nhf.2 &lt;- lm(height~foot, data=hfx)\nsummary(hf.2)\n\n\nCall:\nlm(formula = height ~ foot, data = hfx)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5097 -1.0158  0.4757  1.1141  3.9951 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  30.1502     6.5411   4.609 7.00e-05 ***\nfoot          1.4952     0.2351   6.360 5.12e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.029 on 30 degrees of freedom\nMultiple R-squared:  0.5741,    Adjusted R-squared:  0.5599 \nF-statistic: 40.45 on 1 and 30 DF,  p-value: 5.124e-07\n\nggplot(hf.2, aes(x=.fitted, y=.resid)) + geom_point()\n\n\n\nggplot(hf.2, aes(sample=.resid)) + stat_qq() + stat_qq_line()\n\n\n\n\n\\(\\blacksquare\\)\n\nDo you see any problems on the plots you drew in the previous part? Explain briefly.\n\nSolution\nFor myself, I see a random scatter of points on the first plot, and points close to the line on the second one. Thus I don’t see any problems at all. I would declare myself happy with the second regression, after removing the outlier. (Remember that we removed the outlier because it was an error, not just because it was an outlier. Outliers can be perfectly correct data points, and if they are, they have to be included in the modelling.)\nYou might have a different point of view, in which case you need to make the case for it. You might see a (very mild) case of fanning out in the first plot, or the two most negative residuals might be a bit too low. These are not really outliers, though, not at least in comparison to what we had before.\nExtra: a standard diagnostic for fanning-out is to plot the absolute values of the residuals against the fitted values, with a smooth trend. If this looks like an increasing trend, there is fanning-out; a decreasing trend shows fanning-in. The idea is that we want to see whether the residuals are changing in size (for example, getting more positive and more negative both):\n\nggplot(hf.2, aes(x=.fitted, y=abs(.resid))) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nNo evidence of fanning-out at all. In fact, the residuals seem to be smallest in size in the middle.\nAnother thing you might think of is to try Box-Cox:\n\nboxcox(height~foot, data=hfx)\n\n\n\n\nIt looks as if the best \\(\\lambda\\) is \\(-1\\), and we should predict one over height from foot length. But this plot is deceiving, since it doesn’t even show the whole confidence interval for \\(\\lambda\\)! We should zoom out (a lot) more:\n\nboxcox(height~foot, data=hfx, lambda = seq(-8, 8, 0.1))\n\n\n\n\nThis shows that the confidence interval for \\(\\lambda\\) goes from about \\(-7\\) to almost 5: that is, any value of \\(\\lambda\\) in that interval is supported by the data! This very definitely includes the do-nothing \\(\\lambda=1\\), so there is really no support for any kind of transformation.\n\\(\\blacksquare\\)\n\nFind a way to plot the data and both regression lines on the same plot, in such a way that you can see which regression line is which. If you get help from anything outside the course materials, cite your source(s).\n\nSolution\nThis is the same idea as with the windmill data, page 22, though this one is a bit easier since everything is linear (no curves).\nThe easiest way is to use geom_smooth twice, once with the original data set, and then on the one with the outlier removed:\n\nggplot(hf, aes(y=height, x=foot)) + geom_point() + geom_smooth(method = \"lm\", se=F) +\ngeom_smooth(data=hfx, method=\"lm\", colour=\"red\", se=F)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nI would use the original data set as the “base”, since we want to plot its points (including the outlier) as well as its line. Then we want to plot just the line for the second data set. This entails using a data= in the second geom_smooth, to say that we want to get this regression line from a different data set, and also entails drawing this line in a different colour (or in some way distinguishing it from the first one). Putting the colour outside an aes is a way to make the whole line red. (Compare how you make points different colours according to their value on a third variable.)\nThis is, I think, the best way to do it. You can mimic the idea that I used for the windmill data:\n\nggplot(hf, aes(y=height, x=foot)) + geom_point() + geom_smooth(method = \"lm\", se=F) +\ngeom_line(data=hf.2, aes(y = .fitted))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nbut this is not as good, because you don’t need to use the trickery with geom_line: the second trend is another regression line not a curve, and we know how to draw regression lines with geom_smooth without having to actually fit them. (Doing it this way reveals that you are copying without thinking, instead of wondering whether there is a better way to do it.)\nThe idea of using a different data set in different “layers” of a plot is quite well-known. For example, the idea is the one in here, though used for a different purpose there (plotting different sets of points instead of different lines).\n\\(\\blacksquare\\)\n\nDiscuss briefly how removing the observation(s) that were errors has changed where the regression line goes, and whether that is what you expected.\n\nSolution\nThe regression line for the original data (my blue line) is pulled up compared to the one with outliers removed (red).\nThis is not very surprising, because we know that regression lines tend to get pulled towards outliers. What was surprising to me was that the difference wasn’t very big. Even at the low end, where the lines differ the most, the difference in predicted height is only about one inch. Since regression lines are based on means, I would have expected a big outlier to have moved the line a lot more.\nSay something about what you expected, and say something insightful about whether that was what you saw.\nExtra: the regression lines are very similar, but their R-squared values are not: 32% and 57% respectively. Having a point far from the line has a big (downward) impact on R-squared.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#footnotes",
    "href": "simple-regression.html#footnotes",
    "title": "16  Simple regression",
    "section": "",
    "text": "This is a base graphics graph rather than a ggplot one, but it will do for our purposes.↩︎\nRoller-coasters work by gravity, so there must be some drop.↩︎\nThese are not to be confused with what your mom insists that you place between your coffee mug and the table.↩︎\nIf you had just one \\(x\\), you’d use a \\(t\\)-test for its slope, and if you were testing all the \\(x\\)’s, you’d use the global \\(F\\)-test that appears in the regression output.↩︎\nThis is a base graphics graph rather than a ggplot one, but it will do for our purposes.↩︎\nRecall that adding \\(x\\)-variables to a regression will always make R-squared go up, even if they are just random noise.↩︎\nThis is not, I don’t think, a real word, but I mean size emphasizing how big a boy is generally, rather than how small.↩︎\nCorrelations have to go up beyond 0.50 before they start looking at all interesting.↩︎\nThe suspicion being that we can, since the scatterplot suggested serious non-linearity.↩︎\nAgain, not a surprise, given our initial scatterplot.↩︎\nNow we can use that word significant.↩︎\nThis might be overkill at this point, since we really only care about whether our data values are reasonable, and often just looking at the highest and lowest values will tell us that.↩︎\nMathematically, \\(e^x\\) is approximately \\(1+x\\) for small \\(x\\), which winds up meaning that the slope in a model like this, if it is small, indicates about the percent increase in the response associated with a 1-unit change in the explanatory variable. Note that this only works with \\(e^x\\) and natural logs, not base 10 logs or anything like that.↩︎\nWhen this was graded, it was 3 marks, to clue you in that there are three things to say.↩︎\nI have always used dots, but in the spirit of the tidyverse I suppose I should use underscores.↩︎\nThe summary output is more designed for looking at than for extracting things from.↩︎\nThese days, there are apps that will let you do this with your phone. I found one called Clinometer. See also link.↩︎\nThe very negative residuals are at the left and right of the residual plot; they are there because the relationship is a curve. If you were to look at the residuals from the model with length-squared, you probably wouldn’t see this.↩︎\nThe value, but throw away the minus sign if it has one.↩︎\nRoller-coasters work by gravity, so there must be some drop.↩︎\nThese are not to be confused with what your mom insists that you place between your coffee mug and the table.↩︎\nA quote from the package vignette.↩︎\nIt won’t give you an error, but it will go back to the original data frame to get distances to predict from, and you will get very confused. This is another example of (base) R trying to make life easier for you, but when it fails, it fails.↩︎\nR is like that: sometimes it seems as if it has infinite depth.↩︎\nIt does this by figuring what kind of thing you have, from the extension to its filename, and then calling an appropriate function to read in or write out the data. This is an excellent example of “standing on the shoulders of giants” to make our lives easier. The software does the hard work of figuring out what kind of thing you have and how to read it in; all we do is say import.↩︎\nThis is perhaps not a commercial so much as a public safety message.↩︎\nThere are some typical British cars of the era in the commercial.↩︎"
  },
  {
    "objectID": "multiple-regression.html#being-satisfied-with-hospital",
    "href": "multiple-regression.html#being-satisfied-with-hospital",
    "title": "17  Multiple regression",
    "section": "17.1 Being satisfied with hospital",
    "text": "17.1 Being satisfied with hospital\nA hospital administrator collects data to study the effect, if any, of a patient’s age, the severity of their illness, and their anxiety level, on the patient’s satisfaction with their hospital experience. The data, in the file link, are for 46 patients in a survey. The columns are: patient’s satisfaction score satis, on a scale of 0 to 100; the patient’s age (in years), the severity of the patient’s illness (also on a 0–100 scale), and the patient’s anxiety score on a standard anxiety test (scale of 0–5). Higher scores mean greater satisfaction, increased severity of illness and more anxiety.\n\nRead in the data and check that you have four columns in your data frame, one for each of your variables.\n* Obtain scatterplots of the response variable satis against each of the other variables.\nIn your scatterplots of (here), which relationship appears to be the strongest one?\n* Create a correlation matrix for all four variables. Does your strongest trend of the previous part have the strongest correlation?\nRun a regression predicting satisfaction from the other three variables, and display the output.\nDoes the regression fit well overall? How can you tell?\nTest the null hypothesis that none of your explanatory variables help, against the alternative that one or more of them do. (You’ll need an appropriate P-value. Which one is it?) What do you conclude?\nThe correlation between severity and satis is not small, but in my regression I found that severity was nowhere near significant. Why is this? Explain briefly. \nCarry out a backward elimination to determine which of age, severity and anxiety are needed to predict satisfaction. What do you get? Use \\(\\alpha = 0.10\\)."
  },
  {
    "objectID": "multiple-regression.html#salaries-of-mathematicians",
    "href": "multiple-regression.html#salaries-of-mathematicians",
    "title": "17  Multiple regression",
    "section": "17.2 Salaries of mathematicians",
    "text": "17.2 Salaries of mathematicians\nA researcher in a scientific foundation wanted to evaluate the relationship between annual salaries of mathematicians and three explanatory variables:\n\nan index of work quality\nnumber of years of experience\nan index of publication success.\n\nThe data can be found at link. Data from only a relatively small number of mathematicians were available.\n\nRead in the data and check that you have a sensible number of rows and the right number of columns. (What does “a sensible number of rows” mean here?)\nMake scatterplots of salary against each of the three explanatory variables. If you can, do this with one ggplot.\nComment briefly on the direction and strength of each relationship with salary.\n* Fit a regression predicting salary from the other three variables, and obtain a summary of the results.\nHow can we justify the statement “one or more of the explanatory variables helps to predict salary”? How is this consistent with the value of R-squared?\nWould you consider removing any of the variables from this regression? Why, or why not?\nDo you think it would be a mistake to take both of workqual and pubsucc out of the regression? Do a suitable test. Was your guess right?\nBack in part (here), you fitted a regression with all three explanatory variables. By making suitable plots, assess whether there is any evidence that (i) that the linear model should be a curve, (ii) that the residuals are not normally distributed, (iii) that there is “fan-out”, where the residuals are getting bigger in size as the fitted values get bigger? Explain briefly how you came to your conclusions in each case."
  },
  {
    "objectID": "multiple-regression.html#predicting-gpa-of-computer-science-students",
    "href": "multiple-regression.html#predicting-gpa-of-computer-science-students",
    "title": "17  Multiple regression",
    "section": "17.3 Predicting GPA of computer science students",
    "text": "17.3 Predicting GPA of computer science students\nThe file link contains some measurements of academic achievement for a number of university students studying computer science:\n\nHigh school grade point average\nMath SAT score\nVerbal SAT score\nComputer Science grade point average\nOverall university grade point average.\n\n\nRead in the data and display it (or at least the first ten lines).\n* Make a scatterplot of high school GPA against university GPA. Which variable should be the response and which explanatory? Explain briefly. Add a smooth trend to your plot.\nDescribe any relationship on your scatterplot: its direction, its strength and its shape. Justify your description briefly.\n* Fit a linear regression for predicting university GPA from high-school GPA and display the results.\nTwo students have been admitted to university. One has a high school GPA of 3.0 and the other a high school GPA of\n3.5. Obtain suitable intervals that summarize the GPAs that each of these two students might obtain in university.\n* Now obtain a regression predicting university GPA from high-school GPA as well as the two SAT scores. Display your results.\nTest whether adding the two SAT scores has improved the prediction of university GPA. What do you conclude?\nCarry out a backward elimination starting out from your model in part (here). Which model do you end up with? Is it the same model as you fit in (here)?\nThese students were studying computer science at university. Do you find your backward-elimination result sensible or surprising, given this? Explain briefly."
  },
  {
    "objectID": "multiple-regression.html#fish-and-mercury",
    "href": "multiple-regression.html#fish-and-mercury",
    "title": "17  Multiple regression",
    "section": "17.4 Fish and mercury",
    "text": "17.4 Fish and mercury\nEating fish is generally healthy, but sometimes the fish is contaminated with mercury. What might affect the amount of mercury in a fish? Large-mouth bass were studied in Florida lakes to examine factors that might affect the amount of mercury contamination. 38 lakes were used for the study. Water samples were taken from each lake and analyzed. Also, samples of fish were caught in each lake and the mercury concentration in their muscle tissue was measured. The resulting data are in http://ritsokiguess.site/datafiles/mercury.txt, separated by single spaces. The variables measured were as follows:\n\nstandardized mercury level (parts per million in 3-year-old fish)\nalkalinity of water (milligrams per litre)\ncalcium level of water (milligrams per litre)\npH of water (standard scale; see eg. this)\n\n\nRead in and display (some of) the data.\nPlot the mercury levels against each of the explanatory variables.\nDescribe any trends (or lack thereof) that you see on your graphs.\nConcerned by some of what you see on your plots, you consult with a fisheries scientist, who says that using the logs of alkalinity and calcium are often used in modelling this kind of data. Add columns containing the logs of these two variables to your dataframe, and run a regression predicting mercury from ph and the two new variables. Display the output from your regression.\nWhat might you learn from running Box-Cox here? Explain briefly, then run it (on the same variables as your regression above) and describe what the results tell you.\nUsing the results of the Box-Cox procedure and your previous work in the question, what seems to be the most appropriate regression now? Fit it, and display the results.\nObtain all the standard residual plots (the ones we have seen in this course) for this model. Describe any problems you see.\n\nMy solutions follow:"
  },
  {
    "objectID": "multiple-regression.html#being-satisfied-with-hospital-1",
    "href": "multiple-regression.html#being-satisfied-with-hospital-1",
    "title": "17  Multiple regression",
    "section": "17.5 Being satisfied with hospital",
    "text": "17.5 Being satisfied with hospital\nA hospital administrator collects data to study the effect, if any, of a patient’s age, the severity of their illness, and their anxiety level, on the patient’s satisfaction with their hospital experience. The data, in the file link, are for 46 patients in a survey. The columns are: patient’s satisfaction score satis, on a scale of 0 to 100; the patient’s age (in years), the severity of the patient’s illness (also on a 0–100 scale), and the patient’s anxiety score on a standard anxiety test (scale of 0–5). Higher scores mean greater satisfaction, increased severity of illness and more anxiety.\n\nRead in the data and check that you have four columns in your data frame, one for each of your variables.\n\nSolution\nThis one requires a little thought first. The data values are aligned in columns, and so are the column headers. Thus, read_table is what we need: ::: {.cell}\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/satisfaction.txt\"\nsatisf &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  satis = col_double(),\n  age = col_double(),\n  severity = col_double(),\n  anxiety = col_double()\n)\n\nsatisf\n\n\n\n  \n\n\n:::\n46 rows and 4 columns: satisfaction score (response), age, severity and anxiety (explanatory).\nThere is a small question about what to call the data frame. Basically, anything other than satis will do, since there will be confusion if your data frame has the same name as one of its columns.\n\\(\\blacksquare\\)\n\n* Obtain scatterplots of the response variable satis against each of the other variables.\n\nSolution\nThe obvious way is to do these one after the other: ::: {.cell}\nggplot(satisf, aes(x = age, y = satis)) + geom_point()\n\n\n\nggplot(satisf, aes(x = severity, y = satis)) + geom_point()\n\n\n\nggplot(satisf, aes(x = anxiety, y = satis)) + geom_point()\n\n\n\n:::\nThis is fine, but there is also a way of getting all three plots with one ggplot. This uses the facet_wrap trick, but to set that up, we have to have all the \\(x\\)-variables in one column, with an extra column labelling which \\(x\\)-variable that value was. This uses pivot_longer. The right way to do this is in a pipeline:\n\nsatisf %&gt;%\n  pivot_longer(-satis, names_to=\"xname\", values_to=\"x\") %&gt;%\n  ggplot(aes(x = x, y = satis)) + geom_point() +\n  facet_wrap(~xname, scales = \"free\", ncol = 2)\n\n\n\n\nSteps: collect together the columns age through anxiety into one column whose values go in x, with names in xname, then plot this new x against satisfaction score, with a separate facet for each different \\(x\\) (in xname).\nWhat’s the difference between facet_grid and facet_wrap? The difference is that with facet_wrap, we are letting ggplot arrange the facets how it wants to. In this case, we didn’t care which explanatory variable went on which facet, just as long as we saw all of them somewhere. Inside facet_wrap there are no dots: a squiggle, followed by the name(s) of the variable(s) that distinguish(es) the facets.1 The only “design” decision I made here was that the facets should be arranged somehow in two columns, but I didn’t care which ones should be where.\nIn facet_grid, you have a variable that you want to be displayed in rows or in columns (not just in “different facets”). I’ll show you how that works here. Since I am going to draw two plots, I should save the long data frame first and re-use it, rather than calculating it twice (so that I ought now to go back and do the other one using the saved data frame, really):\n\nsatisf %&gt;% \n  pivot_longer(age:anxiety, names_to=\"xname\", \n               values_to=\"x\") -&gt; satisf.long\nsatisf.long\n\n\n\n  \n\n\n\nIf, at this or any stage, you get confused, the way to un-confuse yourself is to fire up R Studio and do this yourself. You have all the data and code you need. If you do it yourself, you can run pipes one line at a time, inspect things, and so on.\nFirst, making a row of plots, so that xname is the \\(x\\) of the facets:\n\nggplot(satisf.long, aes(x = x, y = satis)) + geom_point() +\n  facet_grid(. ~ xname, scales = \"free\")\n\n\n\n\nI find these too tall and skinny to see the trends, as on the first facet_wrap plot.\nAnd now, making a column of plots, with xname as \\(y\\):\n\nggplot(satisf.long, aes(x = x, y = satis)) + geom_point() +\n  facet_grid(xname ~ ., scales = \"free\")\n\n\n\n\nThis one looks weird because the three \\(x\\)-variables are on different scales. The effect of the scales=\"free\" is to allow the satis scale to vary, but the x scale cannot because the facets are all in a line. Compare this:\n\nggplot(satisf.long, aes(x = x, y = satis)) + geom_point() +\n  facet_wrap(~xname, ncol = 1, scales = \"free\")\n\n\n\n\nThis time, the \\(x\\) scales came out different (and suitable), but I still like squarer plots better for judging relationships.\n\\(\\blacksquare\\)\n\nIn your scatterplots of (here), which relationship appears to be the strongest one?\n\nSolution\nAll the trends appear to be downward ones, but I think satis and age is the strongest trend. The other ones look more scattered to me.\n\\(\\blacksquare\\)\n\n* Create a correlation matrix for all four variables. Does your strongest trend of the previous part have the strongest correlation?\n\nSolution\nThis is a matter of running the whole data frame through cor: ::: {.cell}\ncor(satisf)\n\n              satis        age   severity    anxiety\nsatis     1.0000000 -0.7736828 -0.5874444 -0.6023105\nage      -0.7736828  1.0000000  0.4666091  0.4976945\nseverity -0.5874444  0.4666091  1.0000000  0.7945275\nanxiety  -0.6023105  0.4976945  0.7945275  1.0000000\n\n:::\nIgnoring the correlations of variables with themselves, the correlation of satisf with age, the one I picked out, is the strongest (the most negative trend). If you picked one of the other trends as the strongest, you need to note how close it is to the maximum correlation: for example, if you picked satis and severity, that’s the second highest correlation (in size).\n\\(\\blacksquare\\)\n\nRun a regression predicting satisfaction from the other three variables, and display the output.\n\nSolution\n\nsatisf.1 &lt;- lm(satis ~ age + severity + anxiety, data = satisf)\nsummary(satisf.1)\n\n\nCall:\nlm(formula = satis ~ age + severity + anxiety, data = satisf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.954  -7.154   1.550   6.599  14.888 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 162.8759    25.7757   6.319 4.59e-06 ***\nage          -1.2103     0.3015  -4.015  0.00074 ***\nseverity     -0.6659     0.8210  -0.811  0.42736    \nanxiety      -8.6130    12.2413  -0.704  0.49021    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.29 on 19 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.621 \nF-statistic: 13.01 on 3 and 19 DF,  p-value: 7.482e-05\n\n\n\\(\\blacksquare\\)\n\nDoes the regression fit well overall? How can you tell?\n\nSolution\nFor this, look at R-squared, which is 0.682 (68.2%). This is one of those things to have an opinion about. I’d say this is good but not great. I would not call it “poor”, since there definitely is a relationship, even if it’s not a stupendously good one.\n\\(\\blacksquare\\)\n\nTest the null hypothesis that none of your explanatory variables help, against the alternative that one or more of them do. (You’ll need an appropriate P-value. Which one is it?) What do you conclude?\n\nSolution\nThis one is the (global) \\(F\\)-test, whose P-value is at the bottom. It translates to 0.000000000154, so this is definitely small, and we reject the null. Thus, one or more of age, severity and anxiety helps to predict satisfaction. (I would like to see this last sentence, rather than just “reject the null”.)\n\\(\\blacksquare\\)\n\nThe correlation between severity and satis is not small, but in my regression I found that severity was nowhere near significant. Why is this? Explain briefly. \n\nSolution\nThe key thing to observe is that the \\(t\\)-test in the regression says how important a variable is given the others that are already in the regression, or, if you prefer, how much that variable adds to the regression, on top of the ones that are already there. So here, we are saying that severity has nothing to add, given that the regression already includes the others. (That is, high correlation and strong significance don’t always go together.) For a little more insight, look at the correlation matrix of (here) again. The strongest trend with satis is with age, and indeed age is the one obviously significant variable in the regression. The trend of severity with satis is somewhat downward, and you might otherwise have guessed that this is strong enough to be significant. But see that severity also has a clear relationship with age. A patient with low severity of disease is probably also younger, and we know that younger patients are likely to be more satisfied. Thus severity has nothing (much) to add.\nExtra 1: The multiple regression is actually doing something clever here. Just looking at the correlations, it appears that all three variables are helpful, but the regression is saying that once you have looked at age (“controlled for age”), severity of illness does not have an impact: the correlation of severity with satis is as big as it is almost entirely because of age. This gets into the domain of “partial correlation”. If you like videos, you can see link for this. I prefer regression, myself, since I find it clearer. anxiety tells a different story: this is close to significant (or is significant at the \\(\\alpha=0.10\\) level), so the regression is saying that anxiety does appear to have something to say about satis over and above age. This is rather odd, to my mind, since anxiety has only a slightly stronger correlation with satis and about the same with age as severity does. But the regression is telling the story to believe, because it handles all the inter-correlations, not just the ones between pairs of variables.\nI thought it would be rather interesting to do some predictions here. Let’s predict satisfaction for all combinations of high and low age, severity and anxiety. I’ll use the quartiles for high and low.\nThe easiest way to get those is via summary:\n\nsummary(satisf)\n\n     satis            age           severity        anxiety     \n Min.   :26.00   Min.   :28.00   Min.   :43.00   Min.   :1.800  \n 1st Qu.:50.00   1st Qu.:33.00   1st Qu.:48.00   1st Qu.:2.150  \n Median :60.00   Median :40.00   Median :50.00   Median :2.300  \n Mean   :61.35   Mean   :39.61   Mean   :50.78   Mean   :2.296  \n 3rd Qu.:73.50   3rd Qu.:44.50   3rd Qu.:53.50   3rd Qu.:2.400  \n Max.   :89.00   Max.   :55.00   Max.   :62.00   Max.   :2.900  \n\n\nand then use datagrid to make combinations for prediction:\n\nnew &lt;- datagrid(model = satisf.1, age = c(33, 44.5), \n                severity = c(48, 53.5), anxiety = c(2.15, 2.4))\nnew\n\n\n\n  \n\n\n\nEight rows for the \\(2^3 = 8\\) combinations. Then get the predictions for these:\n\ncbind(predictions(satisf.1, newdata = new))\n\n\n\n  \n\n\n\nExtra 2: the standard errors vary quite a bit. The smallest ones are where age, severity, and anxiety are all high or all low (the last row and the first one). This is where most of the data is, because the three explanatory variables are positively correlated with each other (if you know that one of them is high, the others will probably be high too). The other standard errors are higher because there is not much data “nearby”, and so we don’t know as much about the quality of the predictions there.\nExtra 3: we had to copy the quartile values into the new dataframe we were making (to predict from), which ought to have caused you some concern, since there was no guarantee that we copied them correctly. It would be better to make a dataframe with just the quartiles, and feed that into datagrid. Here’s how we can do that.\n\nsatisf %&gt;% \n  summarize(across(-satis,\n                   \\(x) quantile(x, c(0.25, 0.75)))) -&gt; d\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\nd\n\n\n\n  \n\n\n\nTo summarize several columns at once, use across. This one reads “for each column that is not satis, work out the first and third quartiles (25th and 75th percentiles) of it. Recall that the first input to quantile is what to compute percentiles of, and the optional2 second input is which percentiles to compute. Also, when summarize is fed a summary that is more than one number long (two quartiles, here) it will automatically be unnested longer, which happens to be exactly what we want here.\nSo now, we need to take the columns from here and feed them into datagrid. The way to do that is to use with:\n\nnew &lt;- with(d, datagrid(model = satisf.1, age = age, severity = severity, anxiety = anxiety))\nnew\n\n\n\n  \n\n\n\nThe clunky repetition is needed because the first (eg.) age in age = age is the name that the column in new is going to get, and the second age is the thing that supplies the values to be combined (the column of d called age). This is exactly the same new that we had before, and so the predictions will be exactly the same as they were before.\n\\(\\blacksquare\\)\n\nCarry out a backward elimination to determine which of age, severity and anxiety are needed to predict satisfaction. What do you get? Use \\(\\alpha = 0.10\\).\n\nSolution\nThis means starting with the regression containing all the explanatory variables, which is the one I called satisf.1:\n\nsummary(satisf.1)\n\n\nCall:\nlm(formula = satis ~ age + severity + anxiety, data = satisf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.954  -7.154   1.550   6.599  14.888 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 162.8759    25.7757   6.319 4.59e-06 ***\nage          -1.2103     0.3015  -4.015  0.00074 ***\nseverity     -0.6659     0.8210  -0.811  0.42736    \nanxiety      -8.6130    12.2413  -0.704  0.49021    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.29 on 19 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.621 \nF-statistic: 13.01 on 3 and 19 DF,  p-value: 7.482e-05\n\n\nPull out the least-significant (highest P-value) variable, which here is severity. We already decided that this had nothing to add:\n\nsatisf.2 &lt;- update(satisf.1, . ~ . - severity)\nsummary(satisf.2)\n\n\nCall:\nlm(formula = satis ~ age + anxiety, data = satisf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.868  -6.649   2.506   6.445  16.120 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 147.0751    16.7334   8.789 2.64e-08 ***\nage          -1.2434     0.2961  -4.199 0.000442 ***\nanxiety     -15.8906     8.2556  -1.925 0.068593 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.2 on 20 degrees of freedom\nMultiple R-squared:  0.6613,    Adjusted R-squared:  0.6275 \nF-statistic: 19.53 on 2 and 20 DF,  p-value: 1.985e-05\n\n\nIf you like, copy and paste the first lm, edit it to get rid of severity, and run it again. But when I have a “small change” to make to a model, I like to use update.\nHaving taken severity out, anxiety has become significant (at \\(\\alpha = 0.10\\)). Since all of the explanatory variables are now significant, this is where we stop. If we’re predicting satisfaction, we need to know both a patient’s age and their anxiety score: being older or more anxious is associated with a decrease in satisfaction.\nThere is also a function step that will do this for you:\n\nstep(satisf.1, direction = \"backward\", test = \"F\")\n\nStart:  AIC=110.84\nsatis ~ age + severity + anxiety\n\n           Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    \n- anxiety   1     52.41 2064.0 109.43  0.4951 0.4902110    \n- severity  1     69.65 2081.2 109.62  0.6579 0.4273559    \n&lt;none&gt;                  2011.6 110.84                      \n- age       1   1706.67 3718.3 122.97 16.1200 0.0007404 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=109.43\nsatis ~ age + severity\n\n           Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;                  2064.0 109.43                      \n- severity  1    402.78 2466.8 111.53  3.9029 0.0621629 .  \n- age       1   1960.56 4024.6 122.79 18.9977 0.0003042 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCall:\nlm(formula = satis ~ age + severity, data = satisf)\n\nCoefficients:\n(Intercept)          age     severity  \n    166.591       -1.260       -1.089  \n\n\nwith the same result.3 This function doesn’t actually use P-values; instead it uses a thing called AIC. At each step, the variable with the lowest AIC comes out, and when &lt;none&gt; bubbles up to the top, that’s when you stop. The test=\"F\" means “include an \\(F\\)-test”, but the procedure still uses AIC (it just shows you an \\(F\\)-test each time as well). In this case, the other variables were in the same order throughout, but they don’t have to be (in the same way that removing one variable from a multiple regression can dramatically change the P-values of the ones that remain). Here, at the first step, &lt;none&gt; and anxiety were pretty close, but when severity came out, taking out nothing was a lot better than taking out anxiety.\nThe test=\"F\" on the end gets you the P-values. Using the \\(F\\)-test is right for regressions; for things like logistic regression that we see later, test=\"Chisq\" is the right one to use.4\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "multiple-regression.html#salaries-of-mathematicians-1",
    "href": "multiple-regression.html#salaries-of-mathematicians-1",
    "title": "17  Multiple regression",
    "section": "17.6 Salaries of mathematicians",
    "text": "17.6 Salaries of mathematicians\nA researcher in a scientific foundation wanted to evaluate the relationship between annual salaries of mathematicians and three explanatory variables:\n\nan index of work quality\nnumber of years of experience\nan index of publication success.\n\nThe data can be found at link. Data from only a relatively small number of mathematicians were available.\n\nRead in the data and check that you have a sensible number of rows and the right number of columns. (What does “a sensible number of rows” mean here?)\n\nSolution\nThis is a tricky one. There are aligned columns, but the column headers are not aligned with them. Thus read_table2 is what you need. ::: {.cell}\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/mathsal.txt\"\nsalaries &lt;- read_table2(my_url)\n\nWarning: `read_table2()` was deprecated in readr 2.0.0.\nℹ Please use `read_table()` instead.\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  salary = col_double(),\n  workqual = col_double(),\n  experience = col_double(),\n  pubsucc = col_double()\n)\n\nsalaries\n\n\n\n  \n\n\n:::\n24 observations (“only a relatively small number”) and 4 columns, one for the response and one each for the explanatory variables.\nOr, if you like,\n\ndim(salaries)\n\n[1] 24  4\n\n\nfor the second part: 24 rows and 4 columns again. I note, with only 24 observations, that we don’t really have enough data to investigate the effects of three explanatory variables, but we’ll do the best we can. If the pattern, whatever it is, is clear enough, we should be OK.\n\\(\\blacksquare\\)\n\nMake scatterplots of salary against each of the three explanatory variables. If you can, do this with one ggplot.\n\nSolution\nThe obvious way to do this is as three separate scatterplots, and that will definitely work. But you can do it in one go if you think about facets, and about having all the \\(x\\)-values in one column (and the names of the \\(x\\)-variables in another column): ::: {.cell}\nsalaries %&gt;%\n  pivot_longer(-salary, names_to=\"xname\", values_to=\"x\") %&gt;%\n  ggplot(aes(x = x, y = salary)) + geom_point() +\n  facet_wrap(~xname, ncol = 2, scales = \"free\")\n\n\n\n:::\nIf you don’t see how that works, run it yourself, one line at a time.\nI was thinking ahead a bit while I was coding that: I wanted the three plots to come out about square, and I wanted the plots to have their own scales. The last thing in the facet_wrap does the latter, and arranging the plots in two columns (thinking of the plots as a set of four with one missing) gets them more or less square.\nIf you don’t think of those, try it without, and then fix up what you don’t like.\n\\(\\blacksquare\\)\n\nComment briefly on the direction and strength of each relationship with salary.\n\nSolution\nTo my mind, all of the three relationships are going uphill (that’s the “direction” part). experience is the strongest, and pubsucc looks the weakest (that’s the “strength” part). If you want to say there is no relationship with pubsucc, that’s fine too. This is a judgement call. Note that all the relationships are more or less linear (no obvious curves here). We could also investigate the relationships among the explanatory variables: ::: {.cell}\ncor(salaries)\n\n              salary  workqual experience   pubsucc\nsalary     1.0000000 0.6670958  0.8585582 0.5581960\nworkqual   0.6670958 1.0000000  0.4669511 0.3227612\nexperience 0.8585582 0.4669511  1.0000000 0.2537530\npubsucc    0.5581960 0.3227612  0.2537530 1.0000000\n\n:::\nMentally cut off the first row and column (salary is the response). None of the remaining correlations are all that high, so we ought not to have any multicollinearity problems.\n\\(\\blacksquare\\)\n\n* Fit a regression predicting salary from the other three variables, and obtain a summary of the results.\n\nSolution\n\nsalaries.1 &lt;- lm(salary ~ workqual + experience + pubsucc, data = salaries)\nsummary(salaries.1)\n\n\nCall:\nlm(formula = salary ~ workqual + experience + pubsucc, data = salaries)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2463 -0.9593  0.0377  1.1995  3.3089 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.84693    2.00188   8.915 2.10e-08 ***\nworkqual     1.10313    0.32957   3.347 0.003209 ** \nexperience   0.32152    0.03711   8.664 3.33e-08 ***\npubsucc      1.28894    0.29848   4.318 0.000334 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.753 on 20 degrees of freedom\nMultiple R-squared:  0.9109,    Adjusted R-squared:  0.8975 \nF-statistic: 68.12 on 3 and 20 DF,  p-value: 1.124e-10\n\n\n\\(\\blacksquare\\)\n\nHow can we justify the statement “one or more of the explanatory variables helps to predict salary”? How is this consistent with the value of R-squared?\n\nSolution\n“One or more of the explanatory variables helps” is an invitation to consider the (global) \\(F\\)-test for the whole regression. This has the very small P-value of \\(1.124\\times 10^{-10}\\) (from the bottom line of the output): very small, so one or more of the explanatory variables does help, and the statement is correct. The idea that something helps to predict salary suggests (especially with such a small number of observations) that we should have a high R-squared. In this case, R-squared is 0.9109, which is indeed high.\n\\(\\blacksquare\\)\n\nWould you consider removing any of the variables from this regression? Why, or why not?\n\nSolution\nLook at the P-values attached to each variable. These are all very small: 0.003, 0.00000003 and 0.0003, way smaller than 0.05. So it would be a mistake to take any, even one, of the variables out: doing so would make the regression much worse. If you need convincing of that, see what happens when we take the variable with the highest P-value out — this is workqual: ::: {.cell}\nsalaries.2 &lt;- lm(salary ~ experience + pubsucc, data = salaries)\nsummary(salaries.2)\n\n\nCall:\nlm(formula = salary ~ experience + pubsucc, data = salaries)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2723 -0.7865 -0.3983  1.7277  3.2060 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 21.02546    2.14819   9.788 2.82e-09 ***\nexperience   0.37376    0.04104   9.107 9.70e-09 ***\npubsucc      1.52753    0.35331   4.324    3e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.137 on 21 degrees of freedom\nMultiple R-squared:  0.8609,    Adjusted R-squared:  0.8477 \nF-statistic:    65 on 2 and 21 DF,  p-value: 1.01e-09\n\n:::\nR-squared has gone down from 91% to 86%: maybe not so much in the grand scheme of things, but it is noticeably less. Perhaps better, since we are comparing models with different numbers of explanatory variables, is to compare the adjusted R-squared: this has gone down from 90% to 85%. The fact that this has gone down at all is enough to say that taking out workqual was a mistake.5\nAnother way of seeing whether a variable has anything to add in a regression containing the others is a partial regression plot. We take the residuals from salaries.2 above and plot them against the variable we removed, namely workqual.6 If workqual has nothing to add, there will be no pattern; if it does have something to add, there will be a trend. Like this. I use augment from broom:\n\nlibrary(broom)\nsalaries.2 %&gt;%\n  augment(salaries) %&gt;%\n  ggplot(aes(x = workqual, y = .resid)) + geom_point()\n\n\n\n\nThis is a mostly straight upward trend. So we need to add a linear term in workqual to the regression.7\n\\(\\blacksquare\\)\n\nDo you think it would be a mistake to take both of workqual and pubsucc out of the regression? Do a suitable test. Was your guess right?\n\nSolution\nI think it would be a big mistake. Taking even one of these variables out of the regression is a bad idea (from the \\(t\\)-tests), so taking out more than one would be a really bad idea. To perform a test, fit the model without these two explanatory variables: ::: {.cell}\nsalaries.3 &lt;- lm(salary ~ experience, data = salaries)\n:::\nand then use anova to compare the two regressions, smaller model first:\n\nanova(salaries.3, salaries.1)\n\n\n\n  \n\n\n\nThe P-value is extremely small, so the bigger model is definitely better than the smaller one: we really do need all three variables. Which is what we guessed.\n\\(\\blacksquare\\)\n\nBack in part (here), you fitted a regression with all three explanatory variables. By making suitable plots, assess whether there is any evidence that (i) that the linear model should be a curve, (ii) that the residuals are not normally distributed, (iii) that there is “fan-out”, where the residuals are getting bigger in size as the fitted values get bigger? Explain briefly how you came to your conclusions in each case.\n\nSolution\nI intended that (i) should just be a matter of looking at residuals vs. fitted values: ::: {.cell}\nggplot(salaries.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n:::\nThere is no appreciable pattern on here, so no evidence of a curve (or apparently of any other problems).\nExtra: you might read this that we should check residuals against the \\(x\\)-variables as well, which is a similar trick to the above one for plotting response against each of the explanatories. There is one step first, though: use augment from broom first to get a dataframe with the original \\(x\\)-variables and the residuals in it. The following thus looks rather complicated, and if it confuses you, run the code a piece at a time to see what it’s doing:\n\nsalaries.1 %&gt;%\n  augment(salaries) %&gt;%\n  pivot_longer(workqual:pubsucc, names_to=\"xname\", values_to=\"x\") %&gt;%\n  ggplot(aes(x = x, y = .resid)) + geom_point() +\n  facet_wrap(~xname, scales = \"free\", ncol = 2)\n\n\n\n\nThese three residual plots are also pretty much textbook random, so no problems here either.\nFor (ii), look at a normal quantile plot of the residuals, which is not as difficult as the plot I just did:\n\nggplot(salaries.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nThat is really pretty good. Maybe the second smallest point is a bit far off the line, but otherwise there’s nothing to worry about. A quick place to look for problems is the extreme observations, and the largest and smallest residuals are almost exactly the size we’d expect them to be.\nOur graph for assessing fan-in or fan-out is to plot the absolute values of the residuals against the fitted values. The plot from (i) suggests that we won’t have any problems here, but to investigate:\n\nggplot(salaries.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis is pretty nearly straight across. You might think it increases a bit at the beginning, but most of the evidence for that comes from the one observation with fitted value near 30 that happens to have a residual near zero. Conclusions based on one observation are not to be trusted! In summary, I’m happy with this linear multiple regression, and I don’t see any need to do anything more with it. I am, however, willing to have some sympathy with opinions that differ from mine, if they are supported by those graphs above.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "multiple-regression.html#predicting-gpa-of-computer-science-students-1",
    "href": "multiple-regression.html#predicting-gpa-of-computer-science-students-1",
    "title": "17  Multiple regression",
    "section": "17.7 Predicting GPA of computer science students",
    "text": "17.7 Predicting GPA of computer science students\nThe file link contains some measurements of academic achievement for a number of university students studying computer science:\n\nHigh school grade point average\nMath SAT score\nVerbal SAT score\nComputer Science grade point average\nOverall university grade point average.\n\n\nRead in the data and display it (or at least the first ten lines).\n\nSolution\nThe usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/gpa.txt\"\ngpa &lt;- read_delim(my_url, \" \")\n\nRows: 105 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (5): high_GPA, math_SAT, verb_SAT, comp_GPA, univ_GPA\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngpa\n\n\n\n  \n\n\n\nTwo SAT scores and three GPAs, as promised.\n\\(\\blacksquare\\)\n\n* Make a scatterplot of high school GPA against university GPA. Which variable should be the response and which explanatory? Explain briefly. Add a smooth trend to your plot.\n\nSolution\nHigh school comes before university, so high school should be explanatory and university should be the response. (High school grades are used as an admission criterion to university, so we would hope they would have some predictive value.) ::: {.cell}\nggplot(gpa, aes(x = high_GPA, y = univ_GPA)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n:::\n\\(\\blacksquare\\)\n\nDescribe any relationship on your scatterplot: its direction, its strength and its shape. Justify your description briefly.\n\nSolution\nTaking these points one at a time:\n\ndirection: upward (a higher high-school GPA generally goes with a higher university GPA as well. Or you can say that the lowest high-school GPAs go with the lowest university GPAs, and high with high, at least most of the time).\nstrength: something like moderately strong, since while the trend is upward, there is quite a lot of scatter. (This is a judgement call: something that indicates that you are basing your description on something reasonable is fine.)\nshape: I’d call this “approximately linear” since there is no clear curve here. The smooth trend wiggles a bit, but not enough to make me doubt a straight line.\n\nLooking ahead, I also notice that when high-school GPA is high, university GPA is also consistently high, but when high-school GPA is low, the university GPA is sometimes low and sometimes high, a lot more variable. (This suggests problems with fan-in later.) In a practical sense, what this seems to show is that people who do well at university usually did well in high-school as well, but sometimes their high-school grades were not that good. This is especially true for people with university GPAs around 3.25.\n\\(\\blacksquare\\)\n\n* Fit a linear regression for predicting university GPA from high-school GPA and display the results.\n\nSolution\nJust this, therefore: ::: {.cell}\ngpa.1 &lt;- lm(univ_GPA ~ high_GPA, data = gpa)\nsummary(gpa.1)\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.69040 -0.11922  0.03274  0.17397  0.91278 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.09682    0.16663   6.583 1.98e-09 ***\nhigh_GPA     0.67483    0.05342  12.632  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2814 on 103 degrees of freedom\nMultiple R-squared:  0.6077,    Adjusted R-squared:  0.6039 \nF-statistic: 159.6 on 1 and 103 DF,  p-value: &lt; 2.2e-16\n\n:::\nExtra: this question goes on too long, so I didn’t ask you to look at the residuals from this model, but my comments earlier suggested that we might have had some problems with fanning-in (the variability of predictions getting less as the high-school GPA increases). In case you are interested, I’ll look at this here. First, residuals against fitted values:\n\nggplot(gpa.1, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nIs that evidence of a trend in the residuals? Dunno. I’m inclined to call this an “inconsequential wiggle” and say it’s OK. Note that the grey envelope includes zero all the way across.\nNormal quantile plot of residuals:\n\nggplot(gpa.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nA somewhat long-tailed distribution: compared to a normal distribution, the residuals are a bit too big in size, both on the positive and negative end.\nThe problem I was really worried about was the potential of fanning-in, which we can assess by looking at the absolute residuals:\n\nggplot(gpa.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThat is definitely a downward trend in the size of the residuals, and I think I was right to be worried before. The residuals should be of similar size all the way across.\nThe usual problem of this kind is fanning-out, where the residuals get bigger in size as the fitted values increase. The bigger values equals more spread is the kind of thing that a transformation like taking logs will handle: the bigger values are all brought downwards, so they will be both smaller and less variable than they were. This one, though, goes the other way, so the only kind of transformation that might help is one at the other end of the scale (think of the Box-Cox lambda scale), something like maybe reciprocal, \\(\\lambda=-1\\) maybe.\nThe other thought I had was that there is this kind of break around a high-school GPA of 3 (go back to the scatterplot of (here)): when the high-school GPA is higher than 3, the university GPA is very consistent (and shows a clear upward trend), but when the high-school GPA is less than 3, the university GPA is very variable and there doesn’t seem to be any trend at all. So maybe two separate analyses would be the way to go.\nAll right, how does Box-Cox work out here, if at all?\n\nlibrary(MASS)\nboxcox(univ_GPA ~ high_GPA, data = gpa)\n\n\n\n\nIt doesn’t. All right, that answers that question.\n\\(\\blacksquare\\)\n\nTwo students have been admitted to university. One has a high school GPA of 3.0 and the other a high school GPA of\n3.5. Obtain suitable intervals that summarize the GPAs that each of these two students might obtain in university.\n\nSolution\nSince we are talking about individual students, rather than the mean of all students with these GPAs, prediction intervals are called for. The first step is to make a data frame to predict from. This has to contain columns for all the explanatory variables, just high_GPA in this case:\n\nnew &lt;- datagrid(model = gpa.1, high_GPA = c(3,3.5))\nnew\n\n\n\n  \n\n\n\nIn general, the advantage to doing it this way is that you also get representative values for any other explanatory variables (means for quantitative ones, the most common category for categorical ones). But in this case, the dataframe has just one column with two values in it, so any other way to create this dataframe is equally good, and, you might say, easier, like this:\n\ngpa.new &lt;- tibble(high_GPA = c(3, 3.5))\ngpa.new\n\n\n\n  \n\n\n\nThe next thing to consider is whether you want a confidence interval for the mean response (the kind of thing predictions gives you), or a prediction interval for an individual response. In this case, it’s the prediction interval, because we want to infer how these individual students will fare. To get this, you need the old-fashioned predict rather than predictions:\n\npreds &lt;- predict(gpa.1, gpa.new, interval = \"p\")\npreds\n\n       fit      lwr      upr\n1 3.121313 2.560424 3.682202\n2 3.458728 2.896105 4.021351\n\n\nand display that side by side with the values it was predicted from:\n\ncbind(gpa.new, preds)\n\n\n\n  \n\n\n\nor this way, if you like it better:\n\nas_tibble(preds) %&gt;% bind_cols(gpa.new) %&gt;% select(high_GPA, everything())\n\n\n\n  \n\n\n\nThus the predicted university GPA for the student with high school GPA 3.0 is between 2.6 and 3.7, and for the student with high school GPA 3.5 is between 2.9 and 4.0. (I think this is a good number of decimals to give, but in any case, you should actually say what the intervals are.)\nExtra: I observe that these intervals are almost exactly the same length. This surprises me a bit, since I would have said that 3.0 is close to the average high-school GPA and 3.5 is noticeably higher. If that’s the case, the prediction interval for 3.5 should be longer (since there is less “nearby data”). Was I right about that?\n\ngpa %&gt;% summarize(\n  mean = mean(high_GPA),\n  med = median(high_GPA),\n  q1 = quantile(high_GPA, 0.25),\n  q3 = quantile(high_GPA, 0.75)\n)\n\n\n\n  \n\n\n\nMore or less: the mean is close to 3, and 3.5 is close to the third quartile. So the thing about the length of the prediction interval is a bit of a mystery. Maybe it works better for the confidence interval for the mean:\n\ncbind(predictions(gpa.1, newdata = new))\n\n\n\n  \n\n\n\nThese intervals are a lot shorter, since we are talking about all students with the high-school GPAs in question, and we therefore no longer have to worry about variation from student to student (which is considerable). But my supposition about length is now correct: the interval for 3.5, which is further from the mean, is a little longer than the interval for 3.0. Thinking about it, it seems that the individual-to-individual variation, which is large, is dominating things for our prediction interval above.\n\\(\\blacksquare\\)\n\n* Now obtain a regression predicting university GPA from high-school GPA as well as the two SAT scores. Display your results.\n\nSolution\nCreate a new regression with all the explanatory variables you want in it:\n\ngpa.2 &lt;- lm(univ_GPA ~ high_GPA + math_SAT + verb_SAT, data = gpa)\nsummary(gpa.2)\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + math_SAT + verb_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68186 -0.13189  0.01289  0.16186  0.93994 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.5793478  0.3422627   1.693   0.0936 .  \nhigh_GPA    0.5454213  0.0850265   6.415  4.6e-09 ***\nmath_SAT    0.0004893  0.0010215   0.479   0.6330    \nverb_SAT    0.0010202  0.0008123   1.256   0.2120    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2784 on 101 degrees of freedom\nMultiple R-squared:  0.6236,    Adjusted R-squared:  0.6124 \nF-statistic: 55.77 on 3 and 101 DF,  p-value: &lt; 2.2e-16\n\n\n\\(\\blacksquare\\)\n\nTest whether adding the two SAT scores has improved the prediction of university GPA. What do you conclude?\n\nSolution\nSince we added two explanatory variables, the \\(t\\)-tests in gpa.2 don’t apply (they tell us whether we can take out one \\(x\\)-variable). We might have some suspicions, but that’s all they are. So we have to do anova: ::: {.cell}\nanova(gpa.1, gpa.2)\n\n\n\n  \n\n\n:::\nIf you put the models the other way around, you’ll get a negative \\(F\\)-statistic and degrees of freedom, which doesn’t make much sense (although the test will still work).\nThe null hypothesis here is that the two models fit equally well. Since the P-value is not small, we do not reject that null hypothesis, and therefore we conclude that the two models do fit equally well, and therefore we prefer the smaller one, the one that predicts university GPA from just high-school GPA. (Or, equivalently, we conclude that those two SAT scores don’t add anything to the prediction of how well a student will do at university, once you know their high-school GPA.)\nThis might surprise you, given what the SATs are supposed to be for. But that’s what the data say.\n\\(\\blacksquare\\)\n\nCarry out a backward elimination starting out from your model in part (here). Which model do you end up with? Is it the same model as you fit in (here)?\n\nSolution\nIn the model of (here), math_SAT was the least significant, so that comes out first. (I use update but I’m not insisting that you do:) ::: {.cell}\ngpa.3 &lt;- update(gpa.2, . ~ . - math_SAT)\nsummary(gpa.3)\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + verb_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68430 -0.11268  0.01802  0.14901  0.95239 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.6838723  0.2626724   2.604   0.0106 *  \nhigh_GPA    0.5628331  0.0765729   7.350 5.07e-11 ***\nverb_SAT    0.0012654  0.0006283   2.014   0.0466 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2774 on 102 degrees of freedom\nMultiple R-squared:  0.6227,    Adjusted R-squared:  0.6153 \nF-statistic: 84.18 on 2 and 102 DF,  p-value: &lt; 2.2e-16\n\n:::\nHere is where we have to stop, since both high-school GPA and verbal SAT score are significant, and so taking either of them out would be a bad idea. This is a different model than the one of (here). This is the case, even though the model with high-school GPA only was not significantly worse than the model containing everything. (This goes to show that model-building doesn’t always have nice clear answers.) In the model I called gpa.2, neither of the SAT scores were anywhere near significant (considered singly), but as soon as we took out one of the SAT scores (my model gpa.3), the other one became significant. This goes to show that you shouldn’t take out more than one explanatory variable based on the results of the \\(t\\)-tests, and even if you test to see whether you should have taken out both of the SAT, you won’t necessarily get consistent results. Admittedly, it’s a close decision whether to keep or remove verb_SAT, since its P-value is close to 0.05. The other way of tackling this one is via step, which does the backward elimination for you (not that it was much work here): ::: {.cell}\nstep(gpa.2, direction = \"backward\", test = \"F\")\n\nStart:  AIC=-264.6\nuniv_GPA ~ high_GPA + math_SAT + verb_SAT\n\n           Df Sum of Sq     RSS     AIC F value    Pr(&gt;F)    \n- math_SAT  1    0.0178  7.8466 -266.36  0.2294     0.633    \n- verb_SAT  1    0.1223  7.9511 -264.97  1.5777     0.212    \n&lt;none&gt;                   7.8288 -264.60                      \n- high_GPA  1    3.1896 11.0184 -230.71 41.1486 4.601e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=-266.36\nuniv_GPA ~ high_GPA + verb_SAT\n\n           Df Sum of Sq     RSS     AIC F value    Pr(&gt;F)    \n&lt;none&gt;                   7.8466 -266.36                      \n- verb_SAT  1    0.3121  8.1587 -264.26  4.0571   0.04662 *  \n- high_GPA  1    4.1562 12.0028 -223.73 54.0268 5.067e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + verb_SAT, data = gpa)\n\nCoefficients:\n(Intercept)     high_GPA     verb_SAT  \n   0.683872     0.562833     0.001265  \n\n:::\nThis gives the same result as we did from our backward elimination. The tables with AIC in them are each step of the elimination, and the variable at the top is the one that comes out next. (When &lt;none&gt; gets to the top, that’s when you stop.) What happened is that the two SAT scores started off highest, but once we removed math_SAT, verb_SAT jumped below &lt;none&gt; and so the verbal SAT score had to stay.\nBoth the P-value and the AIC say that the decision about keeping or removing verb_SAT is very close.\n\\(\\blacksquare\\)\n\nThese students were studying computer science at university. Do you find your backward-elimination result sensible or surprising, given this? Explain briefly.\n\nSolution\nI would expect computer science students to be strong students generally, good at math and possibly not so good with words. So it is not surprising that high-school GPA figures into the prediction, but I would expect math SAT score to have an impact also, and it does not. It is also rather surprising that verbal SAT score predicts success at university for these computer science students; you wouldn’t think that having better skills with words would be helpful. So I’m surprised. Here, I’m looking for some kind of discussion about what’s in the final backward-elimination model, and what you would expect to be true of computer science students. Let’s look at the final model from the backward elimination again: ::: {.cell}\nsummary(gpa.3)\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + verb_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68430 -0.11268  0.01802  0.14901  0.95239 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.6838723  0.2626724   2.604   0.0106 *  \nhigh_GPA    0.5628331  0.0765729   7.350 5.07e-11 ***\nverb_SAT    0.0012654  0.0006283   2.014   0.0466 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2774 on 102 degrees of freedom\nMultiple R-squared:  0.6227,    Adjusted R-squared:  0.6153 \nF-statistic: 84.18 on 2 and 102 DF,  p-value: &lt; 2.2e-16\n\n:::\nThe two slope estimates are both positive, meaning that, all else equal, a higher value for each explanatory variable goes with a higher university GPA. This indicates that a higher verbal SAT score goes with a higher university GPA: this is across all the university courses that a student takes, which you would expect to be math and computer science courses for a Comp Sci student, but might include a few electives or writing courses. Maybe what is happening is that the math SAT score is telling the same story as the high-school GPA for these students, and the verbal SAT score is saying something different. (For example, prospective computer science students are mostly likely to have high math SAT scores, so there’s not much information there.)\nI think I need to look at the correlations:\n\ncor(gpa)\n\n          high_GPA  math_SAT  verb_SAT  comp_GPA  univ_GPA\nhigh_GPA 1.0000000 0.7681423 0.7261478 0.7914721 0.7795631\nmath_SAT 0.7681423 1.0000000 0.8352272 0.6877209 0.6627837\nverb_SAT 0.7261478 0.8352272 1.0000000 0.6387512 0.6503012\ncomp_GPA 0.7914721 0.6877209 0.6387512 1.0000000 0.9390459\nuniv_GPA 0.7795631 0.6627837 0.6503012 0.9390459 1.0000000\n\n\nWe’ll ignore comp_GPA (i) because we haven’t been thinking about it and (ii) because it’s highly correlated with the university GPA anyway. (There isn’t a sense that one of the two university GPAs is explanatory and the other is a response, since students are taking the courses that contribute to them at the same time.)\nThe highest correlation with university GPA of what remains is high-school GPA, so it’s not at all surprising that this features in all our regressions. The correlations between university GPA and the two SAT scores are about equal, so there appears to be no reason to favour one of the SAT scores over the other. But, the two SAT scores are highly correlated with each other (0.835), which suggests that if you have one, you don’t need the other, because they are telling more or less the same story.\nThat makes me wonder how a regression with the SAT math score and not the SAT verbal score would look:\n\ngpa.4 &lt;- lm(univ_GPA ~ high_GPA + math_SAT, data = gpa)\nsummary(gpa.4)\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + math_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68079 -0.12264  0.00741  0.16579  0.90010 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.6072916  0.3425047   1.773   0.0792 .  \nhigh_GPA    0.5710745  0.0827705   6.899  4.5e-10 ***\nmath_SAT    0.0012980  0.0007954   1.632   0.1058    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2792 on 102 degrees of freedom\nMultiple R-squared:  0.6177,    Adjusted R-squared:  0.6102 \nF-statistic:  82.4 on 2 and 102 DF,  p-value: &lt; 2.2e-16\n\n\nMath SAT does not quite significantly add anything to the prediction, which confirms that we do better to use the verbal SAT score (surprising though it seems). Though, the two regressions with the single SAT scores, gpa.3 and gpa.4, have almost the same R-squared values. It’s not clear-cut at all. In the end, you have to make a call and stand by it.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "multiple-regression.html#fish-and-mercury-1",
    "href": "multiple-regression.html#fish-and-mercury-1",
    "title": "17  Multiple regression",
    "section": "17.8 Fish and mercury",
    "text": "17.8 Fish and mercury\nEating fish is generally healthy, but sometimes the fish is contaminated with mercury. What might affect the amount of mercury in a fish? Large-mouth bass were studied in Florida lakes to examine factors that might affect the amount of mercury contamination. 38 lakes were used for the study. Water samples were taken from each lake and analyzed. Also, samples of fish were caught in each lake and the mercury concentration in their muscle tissue was measured. The resulting data are in http://ritsokiguess.site/datafiles/mercury.txt, separated by single spaces. The variables measured were as follows:\n\nstandardized mercury level (parts per million in 3-year-old fish)\nalkalinity of water (milligrams per litre)\ncalcium level of water (milligrams per litre)\npH of water (standard scale; see eg. this)\n\n\nRead in and display (some of) the data.\n\nSolution\nThe data values were separated by single spaces, so this one is read_delim:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/mercury.txt\"\nmercury &lt;- read_delim(my_url, \" \")\n\nRows: 38 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (4): mercury, alkalinity, calcium, ph\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmercury\n\n\n\n  \n\n\n\nExtra: I found these data in a textbook, and I couldn’t find them online anywhere, so I typed them in myself. This is how I did it.\nFirst, I entered the values as a piece of text:\n\nmercury_txt &lt;- \"\nmercury alkalinity calcium ph\n1330 2.5 2.9 4.6\n250 19.6 4.5 7.3\n450 5.2 2.8 5.4\n160 71.4 55.2 8.1\n720 26.4 9.2 5.8\n810 4.8 4.6 6.4\n710 6.6 2.7 5.4\n510 16.5 13.8 7.2\n1000 7.1 5.2 5.8\n150 83.7 66.5 8.2\n190 108.5 35.6 8.7\n1020 6.4 4.0 5.8\n450 7.5 2.0 4.4\n590 17.3 10.7 6.7\n810 7.0 6.3 6.9\n420 10.5 6.3 5.5\nO530 30.0 13.9 6.9\n310 55.4 15.9 7.3\n470 6.3 3.3 5.8\n250 67.0 58.6 7.8\n410 28.8 10.2 7.4\n160 119.1 38.4 7.9\n160 25.4 8.8 7.1\n230 106.5 90.7 6.8\n560 8.5 2.5 7.0\n890 87.6 85.5 7.5\n180 114.0 72.6 7.0\n190 97.5 45.5 6.8\n440 11.8 24.2 5.9\n160 66.5 26.0 8.3\n670 16.0 41.2 6.7\n550 5.0 23.6 6.2\n580 25.6 12.6 6.2\n980 1.2 2.1 4.3\n310 34.0 13.1 7.0\n430 15.5 5.2 6.9\n280 17.3 3.0 5.2\n250 71.8 20.5 7.9\n\"\n\nThen, I wanted to get these into a file laid out just like that, which is what writeLines does:\n\nwriteLines(mercury_txt, \"mercury.txt\")\n\nand then I uploaded the file to the course website.\n\\(\\blacksquare\\)\n\nPlot the mercury levels against each of the explanatory variables.\n\nSolution\nThe best way to do this is in one shot, using facets. This means organizing the dataframe so that there is one column of \\(y\\)-values, and also one column of \\(x\\)-values, with an additional column labelling which \\(x\\) it is. This, as you’ll recall, is exactly what pivot_longer does. To show you:\n\nmercury %&gt;% \npivot_longer(-mercury, names_to = \"xname\", values_to = \"xval\")\n\n\n\n  \n\n\n\nand now we plot mercury against xval in facets according to xname. The other thing to remember is that the explanatory variables are on different scales, so we should use scales=\"free\" to allow each facet to have its own scales:\n\nmercury %&gt;% \npivot_longer(-mercury, names_to = \"xname\", values_to = \"xval\") %&gt;% \nggplot(aes(x = xval, y = mercury)) + geom_point() + geom_smooth() + \nfacet_wrap(~xname, scales = \"free\", ncol = 2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI did one more thing, which is to note that I am going to be assessing these relationships in a moment, so I would rather have squarer plots than the tall skinny ones that come out by default (that is to say these):\n\nmercury %&gt;% \npivot_longer(-mercury, names_to = \"xname\", values_to = \"xval\") %&gt;% \nggplot(aes(x = xval, y = mercury)) + geom_point() + geom_smooth() + \nfacet_wrap(~xname, scales = \"free\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nTo make this happen, I added ncol=2, which says to arrange the facets in two columns (that is, as three cells of a \\(2\\times 2\\) grid), and that makes them come out more landscape than portrait. nrow=2 would have had the same effect.\nIf you are stuck, get three separate graphs, but note that you are making more work for yourself (that you will have to do again later).\n\\(\\blacksquare\\)\n\nDescribe any trends (or lack thereof) that you see on your graphs.\n\nSolution\nThink about “form, direction, strength” to guide you in interpreting what you see: is it a line or a curve, does it go up or down, and are the points mostly close to the trend or not? I think it makes most sense to talk about those things for the three trends one after the other:\n\nalkalinity: this is a curved trend, but downward (the rate of decrease is fast at the beginning and then levels off). There is one clear outlier, but otherwise most of the points are close to the trend.\ncalcium: this is a down-and-up curved trend, though I think most of the evidence for the “up” part is the outlier on the middle right of the graph; without that, it would probably be a levelling-off decreasing trend as for alkalinity. There seem to be more outliers on this plot, and, on top of that, the points are not that close to the trend.\nph: this is a downward trend, more or less linear, but of only moderate strength. The points can be some way from the trend, but (in contrast to the other plots) there don’t seem to be any points a long way away.\n\nIf you are going to talk about outliers, you need to be specific about where they are: describe where they are on the plot, or give approximate coordinates (you only need to be accurate enough to make it clear which point you are talking about). For example, I described one of the outliers on the calcium plot as “middle right”, or you could describe the same point as having calcium above 80 and mercury near 1000, which narrows it down enough. There is a grey area between outliers and not being close to the trend overall; if it is a lot of points, I’d call it a weaker trend (as for ph), but if it’s a few points that are a long way off, as for calcium, I’d call that outliers.\nExtra: most of the outliers are above the trends, which suggests that something to bring the high values down a bit would be helpful: that is, a transformation like log or square root. That’s coming up later.\n\\(\\blacksquare\\)\n\nConcerned by some of what you see on your plots, you consult with a fisheries scientist, who says that using the logs of alkalinity and calcium are often used in modelling this kind of data. Add columns containing the logs of these two variables to your dataframe, and run a regression predicting mercury from ph and the two new variables. Display the output from your regression.\n\nSolution\nI would create the two new variables and save them back into the original dataframe, myself:\n\nmercury %&gt;% \nmutate(log_alkalinity = log(alkalinity), log_calcium = log(calcium)) -&gt; mercury\n\nthough you could equally well create a new dataframe to hold them, as long as you remember to use the new dataframe from here on.\nThe regression has no great surprises:\n\nmercury.1 &lt;- lm(mercury ~ log_alkalinity + log_calcium + ph, data = mercury)\nsummary(mercury.1)\n\n\nCall:\nlm(formula = mercury ~ log_alkalinity + log_calcium + ph, data = mercury)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-326.02 -129.92  -27.67   71.17  581.76 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     1181.20     244.21   4.837 2.79e-05 ***\nlog_alkalinity  -221.18      58.87  -3.757 0.000646 ***\nlog_calcium       87.90      51.43   1.709 0.096520 .  \nph               -36.63      51.09  -0.717 0.478362    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 204.4 on 34 degrees of freedom\nMultiple R-squared:  0.5578,    Adjusted R-squared:  0.5188 \nF-statistic:  14.3 on 3 and 34 DF,  p-value: 3.424e-06\n\n\n\\(\\blacksquare\\)\n\nWhat might you learn from running Box-Cox here? Explain briefly, then run it (on the same variables as your regression above) and describe what the results tell you.\n\nSolution\nBox-Cox is to find out whether you need to transform the response variable, here mercury. The hint is that we have already transformed two of the explanatory variables, the ones that had some unusually large values, and so those are presumably now all right now.\nboxcox comes from the MASS package, so install and load that first.\n\nboxcox(mercury ~ log_alkalinity + log_calcium + ph, data = mercury)\n\n\n\n\nThe confidence interval for \\(\\lambda\\) goes from about \\(-0.4\\) to \\(0.5\\). The only round-number powers in there are 0 (log) and 0.5 (square root, right on the end). The \\(\\lambda\\) for the log transformation is right in the middle of the interval, so that’s what I would choose. That means that we should use log of mercury instead of mercury itself in a regression.\nExtra: I looked at the residual plots of the regression mercury.1, in the hope that they would point you towards some kind of transformation of mercury, but they really didn’t – the biggest feature was an upper-end outlier, more extreme than the one you see below, that appeared on all of them. So I didn’t have you produce those graphs.\n\\(\\blacksquare\\)\n\nUsing the results of the Box-Cox procedure and your previous work in the question, what seems to be the most appropriate regression now? Fit it, and display the results.\n\nSolution\nThis says to predict log-mercury (Box-Cox) from log-alkalinity, log-calcium (the fisheries scientist) and pH:\n\nmercury.2 &lt;- lm(log(mercury) ~ log_alkalinity + log_calcium + ph, data = mercury)\nsummary(mercury.2)\n\n\nCall:\nlm(formula = log(mercury) ~ log_alkalinity + log_calcium + ph, \n    data = mercury)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75244 -0.30191 -0.00783  0.23852  1.22932 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     7.55983    0.48981  15.434  &lt; 2e-16 ***\nlog_alkalinity -0.45880    0.11807  -3.886 0.000449 ***\nlog_calcium     0.14702    0.10315   1.425 0.163185    \nph             -0.07998    0.10248  -0.780 0.440527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4099 on 34 degrees of freedom\nMultiple R-squared:  0.6069,    Adjusted R-squared:  0.5723 \nF-statistic:  17.5 on 3 and 34 DF,  p-value: 4.808e-07\n\n\nThere’s no need to define a new column containing log-mercury in the dataframe, since you can define it in the lm. (Note for me: do I need to define new columns anywhere?)\n\\(\\blacksquare\\)\n\nObtain all the standard residual plots (the ones we have seen in this course) for this model. Describe any problems you see.\n\nSolution\nresiduals against fitted\n\nggplot(mercury.2, aes(x=.fitted, y=.resid)) + geom_point()\n\n\n\n\nnormal quantile plot of residuals\n\nggplot(mercury.2, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nagainst the explanatory variables. This uses the ideas of augment (from broom) and pivoting longer:\n\nmercury.2 %&gt;% augment(mercury) %&gt;% \npivot_longer(ph:log_calcium, names_to = \"xname\", values_to = \"xval\") %&gt;% \nggplot(aes(x = xval, y = .resid)) + geom_point() +\nfacet_wrap(~xname, scales = \"free\", ncol = 2)\n\n\n\n\nI think the only troublesome feature on there is the large positive residual that appears at the top of all the plots. Other than that, I see nothing troubling.\nYou might, if you look a bit longer (but remember, apophenia!) see a tiny up and down on the plot with log-alkalinity, and maybe a small downward trend on the plots with the other two explanatory variables, but I would need a lot of convincing to say that these are more than chance. You are looking for obvious trouble, and I really don’t think there’s any sign of that here.\nExtra: you sometimes see a downward trend on residual plots that have an outlier on them. This is because if you have an outlier, it can change the slope disproportionately from what it ought to be, and then most of the observations at one end will be underestimated and most of the observations at the other end will be overestimated.\nExtra 2: which one was that outlier anyway?\n\nmercury.2 %&gt;% augment(mercury) %&gt;% \nfilter(.resid &gt; 1)\n\n\n\n  \n\n\n\nHow do these compare to the other data values?\n\nsummary(mercury)\n\n    mercury         alkalinity        calcium             ph       \n Min.   : 150.0   Min.   :  1.20   Min.   : 2.000   Min.   :4.300  \n 1st Qu.: 250.0   1st Qu.:  7.20   1st Qu.: 4.525   1st Qu.:5.800  \n Median : 445.0   Median : 18.45   Median :11.650   Median :6.850  \n Mean   : 488.4   Mean   : 37.15   Mean   :22.361   Mean   :6.634  \n 3rd Qu.: 650.0   3rd Qu.: 66.88   3rd Qu.:33.200   3rd Qu.:7.300  \n Max.   :1330.0   Max.   :119.10   Max.   :90.700   Max.   :8.700  \n log_alkalinity    log_calcium    \n Min.   :0.1823   Min.   :0.6931  \n 1st Qu.:1.9738   1st Qu.:1.5096  \n Median :2.9131   Median :2.4520  \n Mean   :3.0193   Mean   :2.4801  \n 3rd Qu.:4.2028   3rd Qu.:3.4938  \n Max.   :4.7800   Max.   :4.5076  \n\n\nThe variable values are all high (even the pH, a modest-looking 7.5, is above Q3).\nRemember that the fitted value is of log-mercury, so we have to anti-log it to understand it (anti-log is “exp” since these are “natural” or base-\\(e\\) logs):\n\nexp(5.561902)\n\n[1] 260.3175\n\n\nThis was a much higher mercury value than expected, given the other variables.\n\\(\\blacksquare\\)\nnote to author: the datafile for _chemical.Rmd seems no longer to be readable."
  },
  {
    "objectID": "multiple-regression.html#footnotes",
    "href": "multiple-regression.html#footnotes",
    "title": "17  Multiple regression",
    "section": "",
    "text": "If there are more than one, they should be separated by plus signs as in lm. Each facet then has as many labels as variables. I haven’t actually done this myself, but from looking at examples, I think this is the way it works.↩︎\nIf you don’t give it, you get the five-number summary.↩︎\nThis is because we used \\(\\alpha = 0.10\\). step tends to keep explanatory variables that you might consider marginal because it uses AIC (see below) rather than P-values directly.↩︎\nThis is “F” in quotes, meaning \\(F\\)-test, not “F” without quotes, which means FALSE.↩︎\nAdjusted R-squareds are easier to compare in this context, since you don’t have to make a judgement about whether it has changed substantially, whatever you think substantially means.↩︎\nThe residuals have to be the ones from a regression not including the \\(x\\)-variable you’re testing.↩︎\nOr not take it out in the first place.↩︎"
  },
  {
    "objectID": "regression-categorical.html#crickets-revisited",
    "href": "regression-categorical.html#crickets-revisited",
    "title": "18  Regression with categorical variables",
    "section": "18.1 Crickets revisited",
    "text": "18.1 Crickets revisited\nThis is a continuation of the crickets problem that you may have seen before (minus the data tidying).\nMale tree crickets produce “mating songs” by rubbing their wings together to produce a chirping sound. It is hypothesized that female tree crickets identify males of the correct species by how fast (in chirps per second) the male’s mating song is. This is called the “pulse rate”. Some data for two species of crickets are in link as a CSV file. The columns are species (text), temperature, and pulse rate (numbers). This is the tidied version of the data set that the previous version of this question had you create. The research question is whether males of the different species have different average pulse rates. It is also of interest to see whether temperature has an effect, and if so, what.\n\nRead the data into R and display what you have.\nDo a two-sample \\(t\\)-test to see whether the mean pulse rates differ between species. What do you conclude?\nCan you do that two-sample \\(t\\)-test as a regression?\nThe analysis in the last part did not use temperature, however. Is it possible that temperature also has an effect? To assess this, draw a scatterplot of pulse rate against temperature, with the points distinguished, somehow, by the species they are from.1\nWhat does the plot tell you that the \\(t\\)-test doesn’t? How would you describe differences in pulse rates between species now?\nFit a regression predicting pulse rate from species and temperature. Compare the P-value for species in this regression to the one from the \\(t\\)-test. What does that tell you?\nMake suitable residual plots for the regression pulse.1."
  },
  {
    "objectID": "regression-categorical.html#pulse-rates-and-marching",
    "href": "regression-categorical.html#pulse-rates-and-marching",
    "title": "18  Regression with categorical variables",
    "section": "18.2 Pulse rates and marching",
    "text": "18.2 Pulse rates and marching\nForty students, some male and some female, measured their resting pulse rates. Then they marched in place for one minute and measured their pulse rate again. Our aim is to use regression to predict the pulse rate after the marching from the pulse rate before, and to see whether that is different for males and females. The data set is in http://ritsokiguess.site/datafiles/pulsemarch.csv.\n\nRead in and display (some of) the data.\nMake a suitable graph using all three variables, adding appropriate regression line(s) to the plot.\nExplain briefly but carefully how any effects of pulse rate before on pulse rate after, and also of sex on pulse rate after, show up on your plot. (If either explanatory variable has no effect, explain how you know.)\nRun a regression predicting pulse rate after from the other two variables. Display the output.\nLooking at your graph, does the significance (or lack of) of each of your two explanatory variables surprise you? Explain briefly.\nWhat does the numerical value of the Estimate for Sex in your regression output mean, in the context of this data set? Explain briefly.\n\nMy solutions follow:"
  },
  {
    "objectID": "regression-categorical.html#crickets-revisited-1",
    "href": "regression-categorical.html#crickets-revisited-1",
    "title": "18  Regression with categorical variables",
    "section": "18.3 Crickets revisited",
    "text": "18.3 Crickets revisited\nThis is a continuation of the crickets problem that you may have seen before (minus the data tidying).\nMale tree crickets produce “mating songs” by rubbing their wings together to produce a chirping sound. It is hypothesized that female tree crickets identify males of the correct species by how fast (in chirps per second) the male’s mating song is. This is called the “pulse rate”. Some data for two species of crickets are in link as a CSV file. The columns are species (text), temperature, and pulse rate (numbers). This is the tidied version of the data set that the previous version of this question had you create. The research question is whether males of the different species have different average pulse rates. It is also of interest to see whether temperature has an effect, and if so, what.\n\nRead the data into R and display what you have.\n\nSolution\nNothing terribly surprising here:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/crickets2.csv\"\ncrickets &lt;- read_csv(my_url)\n\nRows: 31 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): species\ndbl (2): temperature, pulse_rate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncrickets\n\n\n\n  \n\n\n\n31 crickets, which is what I remember. What species are there?\n\ncrickets %&gt;% count(species)\n\n\n\n  \n\n\n\nThat looks good. We proceed.\n\\(\\blacksquare\\)\n\nDo a two-sample \\(t\\)-test to see whether the mean pulse rates differ between species. What do you conclude?\n\nSolution\nDrag your mind way back to this: ::: {.cell}\nt.test(pulse_rate ~ species, data = crickets)\n\n\n    Welch Two Sample t-test\n\ndata:  pulse_rate by species\nt = 5.2236, df = 28.719, p-value = 1.401e-05\nalternative hypothesis: true difference in means between group exclamationis and group niveus is not equal to 0\n95 percent confidence interval:\n 14.08583 32.22677\nsample estimates:\nmean in group exclamationis        mean in group niveus \n                   85.58571                    62.42941 \n\n:::\nThere is strong evidence of a difference in means (a P-value around 0.00001), and the confidence interval says that the mean chirp rate is higher for exclamationis. That is, not just for the crickets that were observed here, but for all crickets of these two species.\n\\(\\blacksquare\\)\n\nCan you do that two-sample \\(t\\)-test as a regression?\n\nSolution\nHang onto the “pulse rate depends on species” idea and try that in lm: ::: {.cell}\npulse.0 &lt;- lm(pulse_rate ~ species, data = crickets)\nsummary(pulse.0)\n\n\nCall:\nlm(formula = pulse_rate ~ species, data = crickets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.486  -9.458  -1.729  13.342  22.271 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     85.586      3.316  25.807  &lt; 2e-16 ***\nspeciesniveus  -23.156      4.478  -5.171 1.58e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.41 on 29 degrees of freedom\nMultiple R-squared:  0.4797,    Adjusted R-squared:  0.4617 \nF-statistic: 26.74 on 1 and 29 DF,  p-value: 1.579e-05\n\n:::\nI had to use “model 0” for this since I already have a pulse.1 below and I didn’t want to go down and renumber everything.\nLook along the speciesniveus line. Ignoring the fact that it is negative, the \\(t\\)-statistic is almost the same as before (5.17 vs.\n5.22) and so is the P-value (\\(1.4 \\times 10^{-5}\\) vs. \\(1.6 \\times 10^{-5}\\)).\nWhy aren’t they exactly the same? Regression is assuming equal variances everywhere (that is, within the two species), and before, we did the Welch-Satterthwaite test that does not assume equal variances. What if we do the pooled \\(t\\)-test instead?\n\nt.test(pulse_rate ~ species, data = crickets, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  pulse_rate by species\nt = 5.1706, df = 29, p-value = 1.579e-05\nalternative hypothesis: true difference in means between group exclamationis and group niveus is not equal to 0\n95 percent confidence interval:\n 13.99690 32.31571\nsample estimates:\nmean in group exclamationis        mean in group niveus \n                   85.58571                    62.42941 \n\n\nNow the regression and the \\(t\\)-test do give exactly the same answers. We’ll think about that equal-spreads assumption again later.\n\\(\\blacksquare\\)\n\nThe analysis in the last part did not use temperature, however. Is it possible that temperature also has an effect? To assess this, draw a scatterplot of pulse rate against temperature, with the points distinguished, somehow, by the species they are from.2\n\nSolution\nOne of the wonderful things about ggplot is that doing the obvious thing works: ::: {.cell}\nggplot(crickets, aes(x = temperature, y = pulse_rate, colour = species)) +\n  geom_point()\n\n\n\n:::\n\\(\\blacksquare\\)\n\nWhat does the plot tell you that the \\(t\\)-test doesn’t? How would you describe differences in pulse rates between species now?\n\nSolution\nThe plot tells you that (for both species) as temperature goes up, pulse rate goes up as well. Allowing for that, the difference in pulse rates between the two species is even clearer than it was before. To see an example, pick a temperature, and note that the mean pulse rate at that temperature seems to be at least 10 higher for exclamationis, with a high degree of consistency. The \\(t\\)-test mixed up all the pulse rates at all the different temperatures. Even though the conclusion was clear enough, it could be clearer if we incorporated temperature into the analysis. There was also a potential source of unfairness in that the exclamationis crickets tended to be observed at higher temperatures than niveus crickets; since pulse rates increase with temperature, the apparent difference in pulse rates between the species might have been explainable by one species being observed mainly in higher temperatures. This was utterly invisible to us when we did the \\(t\\)-test, but it shows the importance of accounting for all the relevant variables when you do your analysis.3 If the species had been observed at opposite temperatures, we might have concluded4 that niveus have the higher pulse rates on average. I come back to this later when I discuss the confidence interval for species difference that comes out of the regression model with temperature.\n\\(\\blacksquare\\)\n\nFit a regression predicting pulse rate from species and temperature. Compare the P-value for species in this regression to the one from the \\(t\\)-test. What does that tell you?\n\nSolution\nThis is actually a so-called “analysis of covariance model”, which properly belongs in D29, but it’s really just a regression: ::: {.cell}\npulse.1 &lt;- lm(pulse_rate ~ species + temperature, data = crickets)\nsummary(pulse.1)\n\n\nCall:\nlm(formula = pulse_rate ~ species + temperature, data = crickets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0128 -1.1296 -0.3912  0.9650  3.7800 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -7.21091    2.55094  -2.827  0.00858 ** \nspeciesniveus -10.06529    0.73526 -13.689 6.27e-14 ***\ntemperature     3.60275    0.09729  37.032  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.786 on 28 degrees of freedom\nMultiple R-squared:  0.9896,    Adjusted R-squared:  0.9888 \nF-statistic:  1331 on 2 and 28 DF,  p-value: &lt; 2.2e-16\n\n:::\nThe P-value for species is now \\(6.27\\times 10^{-14}\\) or 0.00000000000006, which is even less than the P-value of 0.00001 that came out of the \\(t\\)-test. That is to say, when you know temperature, you can be even more sure of your conclusion that there is a difference between the species.\nThe R-squared for this regression is almost 99%, which says that if you know both temperature and species, you can predict the pulse rate almost exactly.\nIn the regression output, the slope for species is about \\(-10\\). It is labelled speciesniveus. Since species is categorical, lm uses the first category, exclamationis, as the baseline and expresses each other species relative to that. Since the slope is about \\(-10\\), it says that at any given temperature, the mean pulse rate for niveus is about 10 less than for exclamationis. This is pretty much what the scatterplot told us.\nWe can go a little further here:\n\nconfint(pulse.1)\n\n                   2.5 %    97.5 %\n(Intercept)   -12.436265 -1.985547\nspeciesniveus -11.571408 -8.559175\ntemperature     3.403467  3.802038\n\n\nThe second line says that the pulse rate for niveus is between about 8.5 and 11.5 less than for exclamationis, at any given temperature (comparing the two species at the same temperature as each other, but that temperature could be anything). This is a lot shorter than the CI that came out of the \\(t\\)-test, that went from 14 to 32. This is because we are now accounting for temperature, which also makes a difference. (In the \\(t\\)-test, the temperatures were all mixed up). What we also see is that the \\(t\\)-interval is shifted up compared to the one from the regression. This is because the \\(t\\)-interval conflates5 two things: the exclamationis crickets do have a higher pulse rate, but they were also observed at higher temperatures, which makes it look as if their pulse rates are more higher6 than they really are, when you account for temperature.\nThis particular model constrains the slope with temperature to be the same for both species (just the intercepts differ). If you want to allow the slopes to differ between species, you add an interaction between temperature and species:\n\npulse.2 &lt;- lm(pulse_rate ~ species * temperature, data = crickets)\nsummary(pulse.2)\n\n\nCall:\nlm(formula = pulse_rate ~ species * temperature, data = crickets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7031 -1.3417 -0.1235  0.8100  3.6330 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -11.0408     4.1515  -2.659    0.013 *  \nspeciesniveus              -4.3484     4.9617  -0.876    0.389    \ntemperature                 3.7514     0.1601  23.429   &lt;2e-16 ***\nspeciesniveus:temperature  -0.2340     0.2009  -1.165    0.254    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.775 on 27 degrees of freedom\nMultiple R-squared:  0.9901,    Adjusted R-squared:  0.989 \nF-statistic: 898.9 on 3 and 27 DF,  p-value: &lt; 2.2e-16\n\n\nTo see whether adding the interaction term added anything to the prediction,7 compare the model with and without using anova:\n\nanova(pulse.1, pulse.2)\n\n\n\n  \n\n\n\nThere’s no significant improvement by adding the interaction, so there’s no evidence that having different slopes for each species is necessary. This is the same interpretation as any anova for comparing two regressions: the two models are not significantly different in fit, so go with the simpler one, that is, the one without the interaction.\nNote that anova gave the same P-value as did the \\(t\\)-test for the slope coefficient for the interaction in summary, 0.254 in both cases. This is because there were only two species and therefore only one slope coefficient was required to distinguish them. If there had been three species, we would have had to look at the anova output to hunt for a difference among species, since there would have been two slope coefficients, each with its own P-value.8\nIf you haven’t seen interactions before, don’t worry about this. The idea behind it is that we are testing whether we needed lines with different slopes and we concluded that we don’t. Don’t worry so much about the mechanism behind pulse.2; just worry about how it somehow provides a way of modelling two different slopes, one for each species, which we can then test to see whether it helps.\nThe upshot is that we do not need different slopes; the model pulse.1 with the same slope for each species describes what is going on.\nggplot makes it almost laughably easy to add regression lines for each species to our plot, thus:\n\nggplot(crickets, aes(x = temperature, y = pulse_rate, colour = species)) +\n  geom_point() + geom_smooth(method = \"lm\", se = F)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe lines are almost exactly parallel, so having the same slope for each species makes perfect sense.\n\\(\\blacksquare\\)\n\nMake suitable residual plots for the regression pulse.1.\n\nSolution\nFirst, the plot of residuals against fitted values (after all, it is a regression): ::: {.cell}\nggplot(pulse.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n:::\nThis looks nice and random.\nNow, we plot the residuals against the explanatory variables. There are two, temperature and species, but the latter is categorical. We’ll have some extra issues around species, but before we get to that, we have to remember that the data and the output from the regression are in different places when we plot them. There are different ways to get around that. Perhaps the simplest is to use pulse.1 as our “default” data frame and then get temperature from the right place:\n\nggplot(pulse.1, aes(x = crickets$temperature, y = .resid)) + geom_point()\n\n\n\n\nI don’t see anything untoward there.\nSpecies. We want to compare the residuals for the two species, which is categorical. Since the residuals are quantitative, this suggests a boxplot. Remembering to get species from the right place again, that goes like this:\n\nggplot(pulse.1, aes(x = crickets$species, y = .resid)) + geom_boxplot()\n\n\n\n\nFor the residuals, the median should be zero within each group, and the two groups should be approximately normal with mean 0 and about the same spread. Same spread looks OK, since the boxes are almost exactly the same height, but the normality is not quite there, since both distributions are a little bit skewed to the right. That would also explain why the median residual in each group is a little bit less than zero, because the mathematics requires the overall mean residual to be zero, and the right-skewness would make the mean higher than the median.\nIs that non-normality really problematic? Well, I could look at the normal quantile plot of all the residuals together:\n\nggplot(pulse.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nThere’s a little weirdness at the top, and a tiny indication of a curve (that would suggest a little right-skewedness), but not really much to worry about. If that third-highest residual were a bit lower (say, 3 rather than 3.5) and maybe if the lowest residual was a bit lower, I don’t think we’d have anything to complain about at all.\nSo, I’m not worried.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "regression-categorical.html#pulse-rates-and-marching-1",
    "href": "regression-categorical.html#pulse-rates-and-marching-1",
    "title": "18  Regression with categorical variables",
    "section": "18.4 Pulse rates and marching",
    "text": "18.4 Pulse rates and marching\nForty students, some male and some female, measured their resting pulse rates. Then they marched in place for one minute and measured their pulse rate again. Our aim is to use regression to predict the pulse rate after the marching from the pulse rate before, and to see whether that is different for males and females. The data set is in http://ritsokiguess.site/datafiles/pulsemarch.csv.\n\nRead in and display (some of) the data.\n\nSolution\nAs usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/pulsemarch.csv\"\nmarch &lt;- read_csv(my_url)\n\nRows: 40 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Sex\ndbl (2): Before, After\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmarch\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake a suitable graph using all three variables, adding appropriate regression line(s) to the plot.\n\nSolution\nTwo quantitative and one categorical says scatterplot, with colour distinguishing the categories (two here). geom_smooth adds a regression line to the plot for each Sex, which is what we want. I used se=F to remove the grey envelopes from the plot (because I thought they confused the issue):\n\nggplot(march, aes(x=Before, y=After, colour=Sex)) + geom_point() + \ngeom_smooth(method = \"lm\", se=F)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHaving only one regression line is not so good because that only shows that pulse rate after goes up with pulse rate before, but not if and how the sexes differ.\nExtra: I took a shortcut of the process here, to make the question shorter. In practice, what you’d do is to put smooth trends on the plot first:\n\nggplot(march, aes(x=Before, y=After, colour=Sex)) + geom_point() + \ngeom_smooth(se=F)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe red trend looks curved, but if you look carefully, pretty much all of9 the evidence for the curve comes from that point on the right with pulse rate before over 90 and pulse rate after around 100. If it weren’t for that, the red trend would be pretty close to linear. As you’ll recall, a decision about the kind of trend based on one observation is a pretty flimsy decision.\nThen, having seen that the trends are not obviously curved, you would draw the plot with the straight lines. (Fitting separate curves is a whole different story that I didn’t want to get into.)\n\\(\\blacksquare\\)\n\nExplain briefly but carefully how any effects of pulse rate before on pulse rate after, and also of sex on pulse rate after, show up on your plot. (If either explanatory variable has no effect, explain how you know.)\n\nSolution\nThere is an upward trend, so if the pulse rate before is higher, so is the pulse rate after. This is true for both males and females. (Or, holding Sex fixed, that is, comparing two people of the same sex.)\nThe red line is always above the blue line, so at any given Before pulse rate, the After pulse rate for a female is predicted to be higher than that for a male.\nNote that you have to be careful: when talking about the effect of each explanatory variable, you have to hold the other one constant (in general, hold all the other ones constant). If you can word that in a way that makes sense in the context of the data you are looking at, so much the better.\n\\(\\blacksquare\\)\n\nRun a regression predicting pulse rate after from the other two variables. Display the output.\n\nSolution\nThus:\n\nmarch.1 &lt;- lm(After~Before+Sex, data=march)\nsummary(march.1)\n\n\nCall:\nlm(formula = After ~ Before + Sex, data = march)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.8653  -4.6319  -0.4271   3.3856  16.0047 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.8003     7.9217   2.499   0.0170 *  \nBefore        0.9064     0.1127   8.046  1.2e-09 ***\nSexMale      -4.8191     2.2358  -2.155   0.0377 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.918 on 37 degrees of freedom\nMultiple R-squared:  0.6468,    Adjusted R-squared:  0.6277 \nF-statistic: 33.87 on 2 and 37 DF,  p-value: 4.355e-09\n\n\nExtra: if you want “all the other variables except the response” as explanatory, there is also this shortcut:\n\nmarch.1a &lt;- lm(After~., data=march)\nsummary(march.1a)\n\n\nCall:\nlm(formula = After ~ ., data = march)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.8653  -4.6319  -0.4271   3.3856  16.0047 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.8003     7.9217   2.499   0.0170 *  \nSexMale      -4.8191     2.2358  -2.155   0.0377 *  \nBefore        0.9064     0.1127   8.046  1.2e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.918 on 37 degrees of freedom\nMultiple R-squared:  0.6468,    Adjusted R-squared:  0.6277 \nF-statistic: 33.87 on 2 and 37 DF,  p-value: 4.355e-09\n\n\n\\(\\blacksquare\\)\n\nLooking at your graph, does the significance (or lack of) of each of your two explanatory variables surprise you? Explain briefly.\n\nSolution\nWe noted a clear upward trend before, for both sexes, so there is no surprise that the Before pulse rate is significant.\nThe red dots (females) on the graph seemed to be on average above the blue ones (males), at least for similar before pulse rates. (This is not completely convincing, so you are entitled to be surprised also; note that the P-value, while significant, is not that small).\nExtra: comparing the lines is less convincing, because how do we get a feel for whether these lines are more different than chance? One deceiving way to (fail to) get a feel for this is to re-draw our plot but with the grey envelopes:\n\nggplot(march, aes(x=Before, y=After, colour=Sex)) + geom_point() + \ngeom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe grey envelopes overlap substantially, which would make you think the lines are not significantly different. But, this is not the right way to compare the lines. It is a similar problem to that of comparing two means (that we would normally do with a two-sample test of some kind) by working out the two one-sample confidence intervals, and seeing whether they overlap. If they do not, then you can be sure that the means differ, but if they do overlap, then you cannot say anything about whether the means differ: maybe they do, maybe they don’t. This one is analogous; the grey envelopes overlap, so maybe the lines differ, maybe they don’t. Looking at the grey envelopes in this case gives you no insight about whether males and females differ.\nHere is a short discussion of this issue (in the context of comparing two means).\n\\(\\blacksquare\\)\n\nWhat does the numerical value of the Estimate for Sex in your regression output mean, in the context of this data set? Explain briefly.\n\nSolution\nThe estimate is labelled SexMale, and its value is \\(-4.8\\).\nSex is a categorical variable, so it has a baseline category, which is the first one, Female. The Estimate SexMale shows how males compare to the baseline (females), at a fixed Before pulse rate. This value is \\(-4.8\\), so, at any Before pulse rate, the male After pulse rate is predicted to be 4.8 less than the female one.\nI think you have to mention the value \\(-4.8\\), so that you can talk intelligently about what it means for these data.\nExtra: the implication of our model is that the predicted difference is the same all the way along. You might have your doubts about that; you might think the lines are closer together on the left and further apart on the right. Another way to think about this is whether the lines are parallel: that is, whether they have the same slope. I’m inclined to think they do; the data points are fairly scattered, and I think the slopes would have to be a lot more different to be significantly different. But you don’t have to take my word for it: we can test this by adding an interaction term to the model. You might have seen this in ANOVA, where you are assessing the effect of one factor at different levels of the other. This is more or less the same idea. Note the * rather than the + in the model formula:10\n\nmarch.2 &lt;- lm(After~Before*Sex, data=march)\nsummary(march.2)\n\n\nCall:\nlm(formula = After ~ Before * Sex, data = march)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2831  -4.3638  -0.3965   3.4077  16.6188 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     13.4390    11.1416   1.206    0.236    \nBefore           0.9991     0.1604   6.230 3.43e-07 ***\nSexMale          7.9470    15.8095   0.503    0.618    \nBefore:SexMale  -0.1846     0.2263  -0.816    0.420    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.949 on 36 degrees of freedom\nMultiple R-squared:  0.6532,    Adjusted R-squared:  0.6243 \nF-statistic:  22.6 on 3 and 36 DF,  p-value: 2.11e-08\n\n\nThe Before:SexMale term tests the interaction, and you see it is nowhere near significant. There is no justification for having lines with different slopes for males and females.\nWe were lucky here in that Sex has only two levels, so looking at the summary gave us what we wanted. If we had had an Other category for Sex, for people who don’t identify with either male or female, there would be two Estimates in the summary table, one comparing Male with Female, and one comparing Other with Female.11 But maybe the significant difference is Male vs. Other, and we would never see it.\nTo look for any effect of a categorical variable, the right way is to use drop1, to see which variables, including categorical ones, can be removed as a whole, thus:12\n\ndrop1(march.2, test=\"F\")\n\n\n\n  \n\n\n\nThis only lists things that can be removed, in this case the interaction. It is not significant, so out it comes (resulting in our model march.1):\n\ndrop1(march.1, test=\"F\")\n\n\n\n  \n\n\n\nBoth remaining explanatory variables are significant, so we need to keep them both.\nOur categorical explanatory variable has only two levels, so drop1 and summary give the exact same P-values.\nExtra 2:\nLet’s go back and look at our data set again:\n\nmarch\n\n\n\n  \n\n\n\nYou might have been thinking, when we started, that these are before and after measurements on the same people, and so what we have here is matched pairs. So we do, but it’s not the kind of matched pairs we are accustomed to. Let’s begin by taking differences, getting rid of the Before and After columns, and see what we have left:\n\nmarch %&gt;% \nmutate(difference=After-Before) %&gt;% \nselect(-After, -Before) -&gt; march_paired\nmarch_paired\n\n\n\n  \n\n\n\nIn matched pairs, we are used to having one column of differences, and we test that for a mean or median of zero, to express no difference between before and after (or whatever it was). But now, we have an extra column Sex. We are not interested here in whether the differences average out to zero;13 we care more about whether the differences differ (!) between males and females. That is to say, we have a two-sample matched pairs test!\nAt this point, your head ought to be hurting!\nHowever, at this point what we are saying is that if you believe that the difference is a good way to summarize the effect of the exercise, then we have one measurement for each person, independent because different people’s measurements will be independent. It doesn’t matter where they came from. We have measurements on two groups, so some kind of two-sample test will be good. Which kind? Let’s look at a graph, a good one now being a boxplot:\n\nggplot(march_paired, aes(x=Sex, y=difference)) + geom_boxplot()\n\n\n\n\nOr, if you like, a facetted normal quantile plot:\n\nggplot(march_paired, aes(sample=difference)) +\nstat_qq() + stat_qq_line() +\nfacet_wrap(~Sex)\n\n\n\n\nIt seems to me that these are normal enough for a \\(t\\)-test, given the sample sizes (feel free to disagree):\n\nmarch_paired %&gt;% count(Sex)\n\n\n\n  \n\n\n\nThe spreads look a bit different, so I think I would prefer the Welch test here:\n\nt.test(difference~Sex, data=march_paired)\n\n\n    Welch Two Sample t-test\n\ndata:  difference by Sex\nt = 2.0307, df = 23.296, p-value = 0.05386\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -0.08848652  9.92181986\nsample estimates:\nmean in group Female   mean in group Male \n           13.375000             8.458333 \n\n\nThis time, there is not quite a significant difference between males and females. (The P-value is just the other side of 0.05.) Though the conclusion is different, the P-values are fairly similar.\nWhich test is better? I think treating it as matched pairs is assuming that the differences after minus before are the things to be looking at. This assumes that the after measurements are the before measurement plus a something that depends on treatment, but not on the before measurement. This would fail, for example, if all the after measurements are two times the before ones (so that the difference would be bigger if the before score was bigger). The regression approach is more flexible, because any linear relationship is taken care of. A matched-pairs model of this kind is a special case of the regression model but with the slope set to be 1. In our regression, the slope is less than 1, but not by much.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "regression-categorical.html#footnotes",
    "href": "regression-categorical.html#footnotes",
    "title": "18  Regression with categorical variables",
    "section": "",
    "text": "This was the actual reason I thought of this question originally: I wanted you to do this.↩︎\nThis was the actual reason I thought of this question originally: I wanted you to do this.↩︎\nAnd it shows the value of looking at relevant plots.↩︎\nMistakenly.↩︎\nMixes up.↩︎\nThis is actually grammatically correct.↩︎\nThough it’s hard to imagine being able to improve on an R-squared of 99%.↩︎\nThis wouldn’t have told us about the overall effect of species.↩︎\nMy mind just jumped to a former German soccer player by the name of Klaus Allofs.↩︎\nTo be precise, the * means “the main effects and the interaction together”; if you want to talk about just the interaction term, you denote it by :; note the Before:SexMale term in the summary table.↩︎\nFemale is the baseline, so everything gets compared with that, whether you like it or not.↩︎\nThe test piece says to do an \\(F\\)-test, which is different from without the quotes, which would mean not to do any tests, F without quotes meaning FALSE.↩︎\nI think it’s a given that pulse rates will be higher after exercise than before.↩︎"
  },
  {
    "objectID": "dates-and-times.html#growth-of-mizuna-lettuce-seeds",
    "href": "dates-and-times.html#growth-of-mizuna-lettuce-seeds",
    "title": "19  Dates and times",
    "section": "19.1 Growth of Mizuna lettuce seeds",
    "text": "19.1 Growth of Mizuna lettuce seeds\nIn 2010, a group of students planted some Mizuna lettuce seeds, and recorded how they grew. The data were saved in an Excel spreadsheet, which is at link. The columns are: the date, the height (in cm) of (I presume) the tallest plant, the amount of water added since the previous date (ml), the temperature in the container where the seedlings were growing, and any additional notes that the students made (edited for length by me). The top line of the data file is variable names.\n\nRead the spreadsheet data.\nMake a plot of height against your dates, with the points joined by lines.\nLabel each point on the plot with the amount of water added up to that point."
  },
  {
    "objectID": "dates-and-times.html#types-of-childbirth",
    "href": "dates-and-times.html#types-of-childbirth",
    "title": "19  Dates and times",
    "section": "19.2 Types of childbirth",
    "text": "19.2 Types of childbirth\nChildbirths can be of two types: a “vaginal” birth in which the child is born through the mother’s vagina in the normal fashion, and a “cesarean section” where a surgeon cuts through the wall of the mother’s abdomen, and the baby is delivered through the incision. Cesarean births are used when there are difficulties in pregnancy or during childbirth that would make a vaginal birth too risky. A hospital kept track of the number of vaginal and Cesarean births for the twelve months of 2012. Of interest is whether the Cesarean rate (the ratio of Cesarean births to all births) was increasing, decreasing or remaining stable over that time. The data may be found at link. The columns are the names of the months (in 2012), the number of cesarean births and the number of vaginal births. (The data are not real, but are typical of the kind of thing you would observe.)\n\nRead the data into R and display your data frame.\nCreate a column of actual dates and also a column of cesarean rates, as defined above. Store your new data frame in a variable and display it. For the dates, assume that each date is of the 1st of the month that it belongs to.\nPlot the cesarean rate against time, with a smooth trend. Do you see an upward trend, a downward trend, no trend, or something else?\nTry to summarize the trend you just found with a correlation. What goes wrong? How can you fix it?"
  },
  {
    "objectID": "dates-and-times.html#wolves-and-caribou",
    "href": "dates-and-times.html#wolves-and-caribou",
    "title": "19  Dates and times",
    "section": "19.3 Wolves and caribou",
    "text": "19.3 Wolves and caribou\nIn Denali National Park, Alaska, the size of the wolf population depends on the size of the caribou population (since wolves hunt and kill caribou). This is a large national park, so caribou are found in very large herds, so big, in fact, that the well-being of the entire herd is not threatened by wolf attacks.1 Can the size of the caribou population be used to predict the size of the wolf population? The data can be found at link. The columns are: the date of the survey,2 the name of the park employee in charge of the survey, the caribou population (in hundreds) and the wolf population (actual count).3\n\nTake a look at the data file. How would you describe its format? Read it into R, and check that you got something sensible.\nCreate a new data frame where the column labelled date is now a genuine R Date, using something from lubridate.\nCreate new columns containing the days of the week and the month names for the dates.\nEnough playing around with dates. Make a scatterplot of caribou population (explanatory) against wolf population (response). Do you see any relationship?\nOn your plot from the previous part, label each point with the year it belongs to. You can do this in two steps: first make a new column containing just the years, and then use it as labels for the points on the plot.\nMake a plot of caribou population against time (this is done the obvious way). What seems to be happening to the caribou population over time?\nThe caribou and wolf populations over time are really “time series”. See if you can make a plot of both the caribou and wolf populations against time. You can make two \\(y\\)-axes, one for caribou and one for wolf; this will probably require some research on your part to figure out."
  },
  {
    "objectID": "dates-and-times.html#dealing-with-dates-in-the-worcester-heart-attack-study",
    "href": "dates-and-times.html#dealing-with-dates-in-the-worcester-heart-attack-study",
    "title": "19  Dates and times",
    "section": "19.4 Dealing with dates in the Worcester Heart Attack study",
    "text": "19.4 Dealing with dates in the Worcester Heart Attack study\nThe Worcester Heart Attack Study is an ongoing study of heart attacks in the Worcester, MA area. The main purpose of the study is to investigate changes over time in incidence and death rates, and also the use of different treatment approaches. We will be mainly using this data set to investigate data handling and dealing with dates. The data can be found at link.\n\nRead the data into R. The reading-in part is straightforward, but check what type of thing each column is. Is that what it should be?\nThe date columns should be R dates. They are not year-month-day, so converting them via as.Date (which is what read_delim tries to do) will not work. Load the lubridate package, and create new columns in your data frame that are properly dates. Save your data frame, and list it to demonstrate that it worked.\nCreate three new variables diff1, diff2, diff3 that are the numbers of days between each of your dates, and save the data frame in which they have been created. Verify that at least some of them are the same as los and lenfol.\nConstruct side-by-side boxplots of the length of followup by each followup status. You’ll need to make sure that the followup status, as it gets fed into ggplot, is a factor, or, at least, not the number that it is now."
  },
  {
    "objectID": "dates-and-times.html#going-to-sleep",
    "href": "dates-and-times.html#going-to-sleep",
    "title": "19  Dates and times",
    "section": "19.5 Going to sleep",
    "text": "19.5 Going to sleep\nA student keeps track of what time they go to bed and what time they get up in the morning. They also have an app on their phone that measures the number of hours they were asleep during that time. The data for one week are in http://ritsokiguess.site/datafiles/sleeping.csv, in the 24-hour clock.\n\nRead in and display the data. What type of things are each of your columns?\nWork out the fractional number of hours that the student was in bed each of these nights. (They may not have been asleep this whole time.) Your result needs to be a number since we will be doing some calculations with it shortly.\nThe student is concerned with something they call “sleep efficiency”. This is the percentage of time in bed spent sleeping. Work out the student’s sleep efficiency for the seven nights in this dataframe. Which night was the student’s sleep efficiency greatest?\nDisplay the time spent in bed each night as a number of hours, minutes and seconds.\nMake a graph of what time the student went to bed each night. Bear in mind that you only need the times, not the dates, and that you want a graph that is informative, showing appropriately the distribution of times the student went to bed.\n\nMy solutions follow:"
  },
  {
    "objectID": "dates-and-times.html#growth-of-mizuna-lettuce-seeds-1",
    "href": "dates-and-times.html#growth-of-mizuna-lettuce-seeds-1",
    "title": "19  Dates and times",
    "section": "19.6 Growth of Mizuna lettuce seeds",
    "text": "19.6 Growth of Mizuna lettuce seeds\nIn 2010, a group of students planted some Mizuna lettuce seeds, and recorded how they grew. The data were saved in an Excel spreadsheet, which is at link. The columns are: the date, the height (in cm) of (I presume) the tallest plant, the amount of water added since the previous date (ml), the temperature in the container where the seedlings were growing, and any additional notes that the students made (edited for length by me). The top line of the data file is variable names.\n\nRead the spreadsheet data.\n\nSolution\nThis is read_excel from package readxl. I’m not sure what will happen to the dates yet. Note that this needs a “local” copy of the spreadsheet (that is, you have to download it and save it on your computer, then upload it to rstudio.cloud), possibly using file.choose to help R find it. I put my copy in the same project folder as I was working in, so I just need the file name: ::: {.cell}\nlibrary(readxl)\nmizuna &lt;- read_excel(\"mizuna.xlsx\")\nmizuna\n\n\n\n  \n\n\n:::\nThe dates did get read properly. dttm is “date-time”, so I guess it’s allowing for the possibility that my dates had times attached as well. Unlike with SAS (as we see later), the years came out right.\n\\(\\blacksquare\\)\n\nMake a plot of height against your dates, with the points joined by lines.\n\nSolution\n\nggplot(mizuna, aes(x = date, y = height)) + geom_point() + geom_line()\n\n\n\n\n\\(\\blacksquare\\)\n\nLabel each point on the plot with the amount of water added up to that point.\n\nSolution\nThis is water again. The way to do this is to load ggrepel, then add geom_text_repel to the plot, by adding label=water to the original aes: ::: {.cell}\nlibrary(ggrepel)\nggplot(mizuna, aes(x = date, y = height, label = water)) +\n  geom_point() + geom_line() + geom_text_repel(colour = \"red\")\n\n\n\n:::\nI made the text red, so that you can see it more easily. It “repels” away from the points, but not from the lines joining them. Which makes me wonder whether this would work better (I explain alpha afterwards):\n\nlibrary(ggrepel)\nggplot(mizuna, aes(x = date, y = height, label = water)) +\n  geom_point() + geom_line() + geom_label_repel(colour = \"red\", alpha = 0.7)\n\n\n\n\nThe difference between text and label is that text just uses the text of the variable to mark the point, while label puts that text in a box.\nI think it works better. You can see where the line goes (under the boxes with the labels in them), but you can see the labels clearly.\nWhat that alpha does is to make the thing it’s attached to (the labels) partly transparent. If you leave it out (try it), the black line disappears completely under the label boxes and you can’t see where it goes at all. The value you give for alpha says how transparent the thing is, from 1 (not transparent at all) down to 0 (invisible). I first tried 0.3, and you could hardly see the boxes; then I tried 0.7 so that the boxes were a bit more prominent but the lines underneath were still slightly visible, and I decided that this is what I liked. I think making the labels a different colour was a good idea, since that helps to distinguish the number on the label from the line underneath.\nYou can apply alpha to pretty much any ggplot thing that might be on top of something else, to make it possible to see what’s underneath it. The commonest use for it is if you have a scatterplot with a lot of points; normally, you only see some of the points, because the plot is then a sea of black. But if you make the points partly transparent, you can see more of what’s nearby that would otherwise have been hidden.\nAt some point, I also have to show you folks jitter, which plots in slightly different places points that would otherwise overprint each other exactly, and you wouldn’t know how many of them there were, like the outliers on the boxplots of German children near the new airport.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dates-and-times.html#types-of-childbirth-1",
    "href": "dates-and-times.html#types-of-childbirth-1",
    "title": "19  Dates and times",
    "section": "19.7 Types of childbirth",
    "text": "19.7 Types of childbirth\nChildbirths can be of two types: a “vaginal” birth in which the child is born through the mother’s vagina in the normal fashion, and a “cesarean section” where a surgeon cuts through the wall of the mother’s abdomen, and the baby is delivered through the incision. Cesarean births are used when there are difficulties in pregnancy or during childbirth that would make a vaginal birth too risky. A hospital kept track of the number of vaginal and Cesarean births for the twelve months of 2012. Of interest is whether the Cesarean rate (the ratio of Cesarean births to all births) was increasing, decreasing or remaining stable over that time. The data may be found at link. The columns are the names of the months (in 2012), the number of cesarean births and the number of vaginal births. (The data are not real, but are typical of the kind of thing you would observe.)\n\nRead the data into R and display your data frame.\n\nSolution\nThis is a space-delimited text file, which means:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/birthtypes.txt\"\nbirths &lt;- read_delim(my_url, \" \")\n\nRows: 12 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): month\ndbl (2): cesarean, vaginal\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbirths\n\n\n\n  \n\n\n\nSome text and two numbers for each month. Check.\n\\(\\blacksquare\\)\n\nCreate a column of actual dates and also a column of cesarean rates, as defined above. Store your new data frame in a variable and display it. For the dates, assume that each date is of the 1st of the month that it belongs to.\n\nSolution\nThe easiest way is to use str_c or paste to create a text date with year, month and day in some order, and then to use the appropriate function from lubridate to turn that into an actual date. If you use str_c, you (probably) need the sep thing to make sure the values get a space between them; paste does this automatically. (The next question is whether ymd or whatever can cope without spaces, but I’m not exploring that.) The cesarean rate is cesarean divided by cesarean plus vaginal: ::: {.cell}\nlibrary(lubridate)\nb2 &lt;- births %&gt;%\n  mutate(datestr = str_c(\"2012\", month, \"1\", sep = \" \")) %&gt;%\n  mutate(thedate = ymd(datestr)) %&gt;%\n  mutate(cesarean_rate = cesarean / (cesarean + vaginal))\nb2\n\n\n\n  \n\n\n:::\nIf you don’t like that, create columns that contain 2012 and 1 all the way down. If you set a column name equal to a single value, that single value gets repeated the right number of times:4\n\nbirths %&gt;% mutate(year = 2012, day = 1)\n\n\n\n  \n\n\n\nand then use unite as in class. The distinction is that unite only works on columns. It also “swallows up” the columns that it is made out of; in this case, the original year, month and day disappear:\n\nbirths %&gt;%\n  mutate(year = 2012, day = 1) %&gt;%\n  unite(datestr, year, month, day) %&gt;%\n  mutate(thedate = ymd(datestr)) %&gt;%\n  mutate(cesarean_rate = cesarean / (cesarean + vaginal)) -&gt; b3\nb3 %&gt;% mutate(the_month = month(thedate))\n\n\n\n  \n\n\n\nI don’t mind which order you glue your year, month and day together, as long as you construct the dates with the consistent lubridate function.\n\\(\\blacksquare\\)\n\nPlot the cesarean rate against time, with a smooth trend. Do you see an upward trend, a downward trend, no trend, or something else?\n\nSolution\nThis is a scatterplot with time on the \\(x\\) axis: ::: {.cell}\nggplot(b3, aes(x = thedate, y = cesarean_rate)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n:::\nI like this better than joining the points by lines, since we already have a trend on the plot, but you can do that in some contrasting way:\n\nggplot(b3, aes(x = thedate, y = cesarean_rate)) + geom_point() +\n  geom_line(linetype = \"dashed\") + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI see a downward trend. (“A downward trend with a wiggle” if you like.) There is a certain unevenness in the trend of the actual data, but the overall picture appears to be downhill.\n\\(\\blacksquare\\)\n\nTry to summarize the trend you just found with a correlation. What goes wrong? How can you fix it?\n\nSolution\nSomething like this is the obvious guess:\n\nwith(b3, cor(thedate, cesarean_rate))\n\nError in cor(thedate, cesarean_rate): 'x' must be numeric\n\n\nThis fails because thedate is not of itself a number. But lurking in the background is how the date is actually represented: as a number of days since Jan 1, 1970. Thus, passing it through as.numeric might turn it into that:\n\nb3 %&gt;% mutate(numeric_date = as.numeric(thedate)) -&gt; b5\nb5\n\n\n\n  \n\n\n\nA little mental calculation suggests that these dates in 2012 are a bit over 40 years, that is \\(40 \\times 365 \\simeq 14000\\) days, since the “zero” date of Jan 1, 1970, and so it turns out. This suggests that we can calculate a correlation with the numeric dates:\n\nwith(b5, cor(numeric_date, cesarean_rate))\n\n[1] -0.7091219\n\n\nand we can make a test of the null hypothesis that the correlation is zero (against a two-sided alternative) thus:\n\nwith(b5, cor.test(numeric_date, cesarean_rate))\n\n\n    Pearson's product-moment correlation\n\ndata:  numeric_date and cesarean_rate\nt = -3.1804, df = 10, p-value = 0.009813\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9119078 -0.2280145\nsample estimates:\n       cor \n-0.7091219 \n\n\nThat downward trend is more than just chance, with a P-value just under 0.01. Having said that, though, if you look at the confidence interval for the correlation, it includes almost all the negative values it could be, so that with only 12 observations we really know very little about the correlation other than that it appears to be negative.\nExtra: In practice, you would typically have a much longer time series of measurements than this, such as monthly measurements for several years. In looking at only one year, like we did here, we could get trapped by seasonal effects: for example, cesarean rates might always go down through the year and then jump up again in January. Looking at several years would enable us to disentangle seasonal effects that happen every year from long-term trends. (As an example of this, think of Toronto snowfall: there is almost always snow in the winter and there is never snow in the summer, a seasonal effect, but in assessing climate change, you want to think about long-term trends in snowfall, after allowing for which month you’re looking at.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dates-and-times.html#wolves-and-caribou-1",
    "href": "dates-and-times.html#wolves-and-caribou-1",
    "title": "19  Dates and times",
    "section": "19.8 Wolves and caribou",
    "text": "19.8 Wolves and caribou\nIn Denali National Park, Alaska, the size of the wolf population depends on the size of the caribou population (since wolves hunt and kill caribou). This is a large national park, so caribou are found in very large herds, so big, in fact, that the well-being of the entire herd is not threatened by wolf attacks.5 Can the size of the caribou population be used to predict the size of the wolf population? The data can be found at link. The columns are: the date of the survey,6 the name of the park employee in charge of the survey, the caribou population (in hundreds) and the wolf population (actual count).7\n\nTake a look at the data file. How would you describe its format? Read it into R, and check that you got something sensible.\n\nSolution\nThis looks at first sight as if it’s separated by spaces, but most of the data values are separated by more than one space. If you look further, you’ll see that the values are lined up in columns, with the variable names aligned at the top.\nThis used to be exactly the kind of thing that read_table would read, but no longer. We start with the usual library(tidyverse): ::: {.cell}\nlibrary(tidyverse)\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/caribou.txt\"\ndenali &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  date = col_character(),\n  name = col_character(),\n  caribou = col_character(),\n  wolf = col_character()\n)\n\n\nWarning: 7 parsing failures.\nrow col  expected    actual                                             file\n  1  -- 4 columns 5 columns 'http://ritsokiguess.site/datafiles/caribou.txt'\n  2  -- 4 columns 5 columns 'http://ritsokiguess.site/datafiles/caribou.txt'\n  3  -- 4 columns 5 columns 'http://ritsokiguess.site/datafiles/caribou.txt'\n  4  -- 4 columns 6 columns 'http://ritsokiguess.site/datafiles/caribou.txt'\n  5  -- 4 columns 5 columns 'http://ritsokiguess.site/datafiles/caribou.txt'\n... ... ......... ......... ................................................\nSee problems(...) for more details.\n\ndenali\n\n\n\n  \n\n\n:::\nThe spaces within the names have spilled into the next column, so that the dates and first names are (mostly) correct, but the initials should stay with the names.\nThe right thing now is read_fwf, where fwf stands for “fixed-width format”\n\ndl_file &lt;- \"caribou.txt\"\ndownload.file(my_url, dl_file)\nread_fwf(dl_file)\n\nRows: 8 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\n\nchr (4): X1, X2, X3, X4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n  \n\n\n\nThat’s much closer, but it thought the column names were part of the data. We seem to need to enter them specifically:\n\nmy_cols &lt;- c(\"date\", \"name\", \"caribou\", \"wolf\")\ndenali &lt;- read_fwf(dl_file, fwf_empty(dl_file, col_names = my_cols), skip = 1)\n\nRows: 7 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\n\nchr (2): date, name\ndbl (2): caribou, wolf\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndenali\n\n\n\n  \n\n\n\nand that’s worked. The fwf_empty says to guess where the columns are based on where there are spaces all the way down (as read_table used to do), and to use the specified column names. The top line of the datafile is those column names, though, so we need to skip that row. A bit fiddly.\nAnyway, that (finally) worked: four columns with the right names, and the counts of caribou and wolf are numbers. There are only seven years of surveys; in real-life data you would have more. But the point here is working with dates.\nThe only (small) weirdness is that the dates are text rather than having been converted into dates. This is because they are not year-month-day, which is the only format that gets automatically converted into dates when read in. (You could use mdy from lubridate to make them dates.)\nExtra: you might have wondered how the names survived, even though they have spaces in them, sometimes more than one. Here’s how the file looks:\n\ndate       name             caribou wolf\n09/01/1995 David S.         30       66\n09/24/1996 Youngjin K.      34       79\n10/03/1997 Srinivasan M.    27       70\n09/15/1998 Lee Anne J.      25       60\n09/08/1999 Stephanie T.     17       48\n09/03/2000 Angus Mc D.      23       55\n10/06/2001 David S.         20       60\n\nWhat read_table looks for is columns that contain spaces all the way down, and separates the values there. For example, between the year ofdate and the first name in name there is a space all the way down. After the names and before the caribou counts there are several spaces, and there is one space between the words caribou and wolf in the header line that goes all the way down. Thus four columns, date, name, caribou and wolf. This means that the spaces within the names don’t cause any problems at all, since the spaces aren’t in the same place in every line.8\n\\(\\blacksquare\\)\n\nCreate a new data frame where the column labelled date is now a genuine R Date, using something from lubridate.\n\nSolution\nWhat you do is to look at the format of the dates as they are now. They appear to be month-day-year, American style.9\nThus the function needed is mdy. It doesn’t matter whether the months are names or numbers: ::: {.cell}\ndenali %&gt;% mutate(date = mdy(date)) -&gt; denali\ndenali\n\n\n\n  \n\n\n:::\nI lived on the edge and overwrote both my column and the whole data frame.10\nThe dates are displayed in ISO format, year-month-day. You see at the top of the column that they now really are dates, not just pieces of text that look like dates.\n\\(\\blacksquare\\)\n\nCreate new columns containing the days of the week and the month names for the dates.\n\nSolution\nThis involves digging in the lubridate help to find out how to extract things from a date. It turns out that wday extracts the day of the week from a date, by default as a number, and month gets the month, also by default as a number: ::: {.cell}\ndenali %&gt;% mutate(mon = month(date), wd = wday(date))\n\n\n\n  \n\n\n:::\nThis is not what we wanted, though; we wanted the names of the months and of the days. To fix that, add label=T to both functions:\n\ndenali %&gt;% mutate(mon = month(date, label = T), wd = wday(date, label = T))\n\n\n\n  \n\n\n\nand that cracks it.\nNo need to save this data frame anywhere, since we’re not using any of this later.\nExtra: the ord means “ordered factor”, which makes sense since these are categorical variables with a natural order. This means that you could do something like counting the number of surveys in each month like this:\n\ndenali %&gt;%\n  mutate(mon = month(date, label = T), wd = wday(date, label = T)) %&gt;%\n  count(mon)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nEnough playing around with dates. Make a scatterplot of caribou population (explanatory) against wolf population (response). Do you see any relationship?\n\nSolution\nNothing terribly surprising here: ::: {.cell}\nggplot(denali, aes(x = caribou, y = wolf)) + geom_point()\n\n\n\n:::\nIf you like, add a smooth trend to it:11\n\nggplot(denali, aes(x = caribou, y = wolf)) + geom_point() + geom_smooth(se = F)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis is an upward trend: when one population is large, the other one is large too. This is typical for predator-prey relationships: when there is more to eat (more caribou) the wolf population goes up, and when less, it goes down.\n\\(\\blacksquare\\)\n\nOn your plot from the previous part, label each point with the year it belongs to. You can do this in two steps: first make a new column containing just the years, and then use it as labels for the points on the plot.\n\nSolution\nI’m going to use geom_text_repel for the labels from package ggrepel. The year values are gotten using the lubridate function year:\n\ndenali %&gt;%\n  mutate(year = year(date)) %&gt;%\n  ggplot(aes(x = caribou, y = wolf, label = year)) + geom_point() + geom_text_repel()\n\n\n\n\nI thought about joining up the points in year order. This is actually not geom_line as you would have guessed, since what that does is to join points in the order of the variable on the \\(x\\)-axis.12 To join points in the order that they are in the data (what we want here, because the points are in time order in the data), use instead geom_path:\n\ndenali %&gt;%\n  mutate(year = year(date)) %&gt;%\n  ggplot(aes(x = caribou, y = wolf, label = year)) + geom_point() +\n  geom_text_repel() + geom_path()\n\n\n\n\nIn 1996, both populations were large, and both showed a steady decline until 1999. In 2000 and 2001, both populations seemed to be on the way up again, and you can imagine that in a couple of years, things would go back to about where they were in 1995.\n\\(\\blacksquare\\)\n\nMake a plot of caribou population against time (this is done the obvious way). What seems to be happening to the caribou population over time?\n\nSolution\nMake a scatterplot, with the survey date as explanatory variable, and caribou population as response (since time always goes on the \\(x\\)-axis): ::: {.cell}\nggplot(denali, aes(x = date, y = caribou)) + geom_point() + geom_line()\n\n\n\n:::\nI used an ordinary geom_line this time, to connect neighbouring years, as is often done with a time series. The overall trend is downward, though the 1999 value might be a low from which the population is recovering.\n\\(\\blacksquare\\)\n\nThe caribou and wolf populations over time are really “time series”. See if you can make a plot of both the caribou and wolf populations against time. You can make two \\(y\\)-axes, one for caribou and one for wolf; this will probably require some research on your part to figure out.\n\nSolution\nThe obvious starting point is to note that both the caribou and wolf columns are animal populations, just of different animals. One way of plotting both populations is to pivot_longer them up into one longer column, and then plot them against time, with the two animals distinguished by colour: ::: {.cell}\ndenali %&gt;%\n  pivot_longer(caribou:wolf, names_to=\"animal\", values_to=\"population\") %&gt;%\n  ggplot(aes(x = date, y = population, colour = animal)) +\n  geom_point() + geom_line()\n\n\n\n:::\nThis is not quite the story, though, because the caribou and wolf populations are on different scales. The caribou population is numbered in hundreds, while the wolf population is an actual count.\nThe surveys are late in the year, so the one that is nearly in 1996 is actually the 1995 survey.\nWhat would be nice would be to have a secondary \\(y\\)-axis, so that there were two \\(y\\)-scales, one for each animal. This is very easy to manipulate, though (you can change either scale and get a very different-looking graph), so we ought to be careful.\nAll right, so let’s put the caribou on the left:\n\nggplot(denali, aes(x = date, y = caribou)) + geom_line()\n\n\n\n\nOr we can add a colour aesthetic to distinguish the caribou from the wolf populations, that we’re going to add in a moment. This looks rather odd at first:\n\nggplot(denali, aes(x = date, y = caribou, colour = \"caribou\")) + geom_line()\n\n\n\n\nNow we think about adding the wolf numbers. This is done by adding a second geom_line, overriding the y and the colour to specify that this is wolf now:\n\nggplot(denali, aes(x = date, y = caribou, colour = \"caribou\")) +\n  geom_line() +\n  geom_line(aes(y = wolf, colour = \"wolf\"))\n\n\n\n\nWhat has happened is that we get lines of different colour for each animal, with a legend. So far so good. The problem is that the wolf numbers are about 2.5 times bigger than the caribou numbers,13 so that we don’t get a good sense of how they go up and down together. If we divided the wolf numbers by 2.5, we would see this better:\n\nggplot(denali, aes(x = date, y = caribou, colour = \"caribou\")) +\n  geom_line() +\n  geom_line(aes(y = wolf / 2.5, colour = \"wolf\"))\n\n\n\n\nNow we get to the secondary \\(y\\)-axis. We want to label this wolf and have it reflect that we actually made the graph by dividing the wolf values by 2.5:\n\nggplot(denali, aes(x = date, y = caribou, colour = \"caribou\")) +\n  geom_line() +\n  geom_line(aes(y = wolf / 2.5, colour = \"wolf\")) +\n  scale_y_continuous(sec.axis = sec_axis(~ . * 2.5, name = \"wolf\"))\n\n\n\n\nWoo, and, very possibly, hoo. I got most of these ideas from link.\nNow we see how the populations vary over time, and also that they vary together.\nThis is about the only double-\\(y\\)-axis setup that I like, with scales chosen so that both the series vary about the same amount. By “discreetly” changing the wolf scale, you could make it look as if one population was much bigger than the other, or varied much more than the other. Lies and statistics.\nIn my opinion, too many people just plot series against time, possibly with a second \\(y\\)-axis.14 Variables that vary together, like the wolf and caribou populations here, ought to be plotted against each other on a scatterplot, possibly with the time points labelled.\nThe ambitious among you may like to compare the graphs here with other predator-prey relationships. If you are of a mathematical bent, you might look into the Lotka-Volterra equations, which is a system of two differential equations describing how changes in one population cause changes in the other population.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dates-and-times.html#dealing-with-dates-in-the-worcester-heart-attack-study-1",
    "href": "dates-and-times.html#dealing-with-dates-in-the-worcester-heart-attack-study-1",
    "title": "19  Dates and times",
    "section": "19.9 Dealing with dates in the Worcester Heart Attack study",
    "text": "19.9 Dealing with dates in the Worcester Heart Attack study\nThe Worcester Heart Attack Study is an ongoing study of heart attacks in the Worcester, MA area. The main purpose of the study is to investigate changes over time in incidence and death rates, and also the use of different treatment approaches. We will be mainly using this data set to investigate data handling and dealing with dates. The data can be found at link.\n\nRead the data into R. The reading-in part is straightforward, but check what type of thing each column is. Is that what it should be?\n\nSolution\nThis is read_delim: ::: {.cell}\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/whas500.txt\"\nwhas &lt;- read_delim(my_url, \" \")\n\nRows: 500 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr  (3): admitdate, disdate, fdate\ndbl (19): id, age, gender, hr, sysbp, diasbp, bmi, cvd, afb, sho, chf, av3, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwhas\n\n\n\n  \n\n\n:::\nTo see what type everything is, note that when you display a tibble, the type of all the columns on the screen is displayed at the top. Click the little right-arrow to see more columns and to check their type.\nAll the numbers are properly integer (int) or decimal (dbl) numbers, but the date columns are chr or text. This means that they haven’t been read as Dates (because they were not in year-month-day order). This is (as we will see) unlike SAS, which determined that they were dates, and even used the first 20 rows of the file to determine what format of dates they were.\n\\(\\blacksquare\\)\n\nThe date columns should be R dates. They are not year-month-day, so converting them via as.Date (which is what read_delim tries to do) will not work. Load the lubridate package, and create new columns in your data frame that are properly dates. Save your data frame, and list it to demonstrate that it worked.\n\nSolution\nLoad lubridate first: ::: {.cell}\nlibrary(lubridate)\n:::\nThese dates are day-month-year, so we need dmy from lubridate:\n\nwhas %&gt;% mutate(\n  admit = dmy(admitdate),\n  dis = dmy(disdate),\n  f = dmy(fdate)\n) -&gt; whas2\nglimpse(whas2)\n\nRows: 500\nColumns: 25\n$ id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ age       &lt;dbl&gt; 83, 49, 70, 70, 70, 70, 57, 55, 88, 54, 48, 75, 48, 54, 67, …\n$ gender    &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, …\n$ hr        &lt;dbl&gt; 89, 84, 83, 65, 63, 76, 73, 91, 63, 104, 95, 154, 85, 95, 93…\n$ sysbp     &lt;dbl&gt; 152, 120, 147, 123, 135, 83, 191, 147, 209, 166, 160, 193, 1…\n$ diasbp    &lt;dbl&gt; 78, 60, 88, 76, 85, 54, 116, 95, 100, 106, 110, 123, 80, 65,…\n$ bmi       &lt;dbl&gt; 25.54051, 24.02398, 22.14290, 26.63187, 24.41255, 23.24236, …\n$ cvd       &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, …\n$ afb       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ sho       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ chf       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ av3       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ miord     &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ mitype    &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ year      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ admitdate &lt;chr&gt; \"13-01-1997\", \"19-01-1997\", \"01-01-1997\", \"17-02-1997\", \"01-…\n$ disdate   &lt;chr&gt; \"18-01-1997\", \"24-01-1997\", \"06-01-1997\", \"27-02-1997\", \"07-…\n$ fdate     &lt;chr&gt; \"31-12-2002\", \"31-12-2002\", \"31-12-2002\", \"11-12-1997\", \"31-…\n$ los       &lt;dbl&gt; 5, 5, 5, 10, 6, 1, 5, 4, 4, 5, 5, 10, 7, 21, 4, 1, 13, 14, 6…\n$ dstat     &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lenfol    &lt;dbl&gt; 2178, 2172, 2190, 297, 2131, 1, 2122, 1496, 920, 2175, 2173,…\n$ fstat     &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ admit     &lt;date&gt; 1997-01-13, 1997-01-19, 1997-01-01, 1997-02-17, 1997-03-01,…\n$ dis       &lt;date&gt; 1997-01-18, 1997-01-24, 1997-01-06, 1997-02-27, 1997-03-07,…\n$ f         &lt;date&gt; 2002-12-31, 2002-12-31, 2002-12-31, 1997-12-11, 2002-12-31,…\n\n\nThere are a lot of columns, so I used glimpse. The three new variables we created are at the end of the list. They are correctly Dates, and they have the right values, the ones we can see at least.\nThe indentation is up to you. I think it’s nice to make the creations of the three new variables line up. You can also make the opening and closing brackets on the long mutate aligned, or you can do as I have done here and put two closing brackets on the end. The rationale for this is that each of the variable definition lines in the mutate ends either with a comma or an extra closing bracket, the latter being on the last line. Your choice here is a matter of taste or (in your working life) the coding norms of the team you’re working with.\nExtra: you may have been offended by the repetition above. It so happens that these columns’ names all end in date and they are the only ones that do, so we can use a “select helper” to select only them, and then submit all of them to a mutate via across, which goes like this:\n\nwhas %&gt;% mutate(across(ends_with(\"date\"), \\(date) dmy(date))) %&gt;% glimpse()\n\nRows: 500\nColumns: 22\n$ id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ age       &lt;dbl&gt; 83, 49, 70, 70, 70, 70, 57, 55, 88, 54, 48, 75, 48, 54, 67, …\n$ gender    &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, …\n$ hr        &lt;dbl&gt; 89, 84, 83, 65, 63, 76, 73, 91, 63, 104, 95, 154, 85, 95, 93…\n$ sysbp     &lt;dbl&gt; 152, 120, 147, 123, 135, 83, 191, 147, 209, 166, 160, 193, 1…\n$ diasbp    &lt;dbl&gt; 78, 60, 88, 76, 85, 54, 116, 95, 100, 106, 110, 123, 80, 65,…\n$ bmi       &lt;dbl&gt; 25.54051, 24.02398, 22.14290, 26.63187, 24.41255, 23.24236, …\n$ cvd       &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, …\n$ afb       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ sho       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ chf       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ av3       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ miord     &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ mitype    &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ year      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ admitdate &lt;date&gt; 1997-01-13, 1997-01-19, 1997-01-01, 1997-02-17, 1997-03-01,…\n$ disdate   &lt;date&gt; 1997-01-18, 1997-01-24, 1997-01-06, 1997-02-27, 1997-03-07,…\n$ fdate     &lt;date&gt; 2002-12-31, 2002-12-31, 2002-12-31, 1997-12-11, 2002-12-31,…\n$ los       &lt;dbl&gt; 5, 5, 5, 10, 6, 1, 5, 4, 4, 5, 5, 10, 7, 21, 4, 1, 13, 14, 6…\n$ dstat     &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lenfol    &lt;dbl&gt; 2178, 2172, 2190, 297, 2131, 1, 2122, 1496, 920, 2175, 2173,…\n$ fstat     &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n\n\nOne line, as you see, not three. The English-language version of this reads “for each of the columns whose name ends with date, work out dmy of it”, that is to say, convert it into a date. We can use any of the select-helpers in this, including listing the column numbers or names; in this case our date variables all ended with date.\nThis overwrites the original date columns (you can see that they are now dates), but you can give them new names thus. This is inside the across inside the mutate, so it needs two close-brackets after (the probable reason for the error if you get one):\n\nwhas %&gt;% mutate(across(ends_with(\"date\"), \\(date) dmy(date), \n                       .names = \"{.col}_d\")) %&gt;% glimpse()\n\nRows: 500\nColumns: 25\n$ id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ age         &lt;dbl&gt; 83, 49, 70, 70, 70, 70, 57, 55, 88, 54, 48, 75, 48, 54, 67…\n$ gender      &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0…\n$ hr          &lt;dbl&gt; 89, 84, 83, 65, 63, 76, 73, 91, 63, 104, 95, 154, 85, 95, …\n$ sysbp       &lt;dbl&gt; 152, 120, 147, 123, 135, 83, 191, 147, 209, 166, 160, 193,…\n$ diasbp      &lt;dbl&gt; 78, 60, 88, 76, 85, 54, 116, 95, 100, 106, 110, 123, 80, 6…\n$ bmi         &lt;dbl&gt; 25.54051, 24.02398, 22.14290, 26.63187, 24.41255, 23.24236…\n$ cvd         &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1…\n$ afb         &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0…\n$ sho         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ chf         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1…\n$ av3         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ miord       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0…\n$ mitype      &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1…\n$ year        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ admitdate   &lt;chr&gt; \"13-01-1997\", \"19-01-1997\", \"01-01-1997\", \"17-02-1997\", \"0…\n$ disdate     &lt;chr&gt; \"18-01-1997\", \"24-01-1997\", \"06-01-1997\", \"27-02-1997\", \"0…\n$ fdate       &lt;chr&gt; \"31-12-2002\", \"31-12-2002\", \"31-12-2002\", \"11-12-1997\", \"3…\n$ los         &lt;dbl&gt; 5, 5, 5, 10, 6, 1, 5, 4, 4, 5, 5, 10, 7, 21, 4, 1, 13, 14,…\n$ dstat       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ lenfol      &lt;dbl&gt; 2178, 2172, 2190, 297, 2131, 1, 2122, 1496, 920, 2175, 217…\n$ fstat       &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1…\n$ admitdate_d &lt;date&gt; 1997-01-13, 1997-01-19, 1997-01-01, 1997-02-17, 1997-03-0…\n$ disdate_d   &lt;date&gt; 1997-01-18, 1997-01-24, 1997-01-06, 1997-02-27, 1997-03-0…\n$ fdate_d     &lt;date&gt; 2002-12-31, 2002-12-31, 2002-12-31, 1997-12-11, 2002-12-3…\n\n\nThe three columns on the end are the new actual-dates we created. To give them new names, use .names inside across, and in that is a recipe that says how to make the new names. {.col} means the name the column had before, and the _d after that means to add that to the old name to make the new one.\n\\(\\blacksquare\\)\n\nCreate three new variables diff1, diff2, diff3 that are the numbers of days between each of your dates, and save the data frame in which they have been created. Verify that at least some of them are the same as los and lenfol.\n\nSolution\nI don’t know what R’s internal storage is for dates (it might be seconds or milliseconds or anything, not necessarily days), so subtracting them requires care; you have to divide by the length of a day (in whatever units), thus: ::: {.cell}\nwhas3 &lt;- whas2 %&gt;% mutate(\n  diff1 = (dis - admit) / ddays(1),\n  diff2 = (f - admit) / ddays(1),\n  diff3 = (f - dis) / ddays(1)\n)\nglimpse(whas3)\n\nRows: 500\nColumns: 28\n$ id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ age       &lt;dbl&gt; 83, 49, 70, 70, 70, 70, 57, 55, 88, 54, 48, 75, 48, 54, 67, …\n$ gender    &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, …\n$ hr        &lt;dbl&gt; 89, 84, 83, 65, 63, 76, 73, 91, 63, 104, 95, 154, 85, 95, 93…\n$ sysbp     &lt;dbl&gt; 152, 120, 147, 123, 135, 83, 191, 147, 209, 166, 160, 193, 1…\n$ diasbp    &lt;dbl&gt; 78, 60, 88, 76, 85, 54, 116, 95, 100, 106, 110, 123, 80, 65,…\n$ bmi       &lt;dbl&gt; 25.54051, 24.02398, 22.14290, 26.63187, 24.41255, 23.24236, …\n$ cvd       &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, …\n$ afb       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ sho       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ chf       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ av3       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ miord     &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ mitype    &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ year      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ admitdate &lt;chr&gt; \"13-01-1997\", \"19-01-1997\", \"01-01-1997\", \"17-02-1997\", \"01-…\n$ disdate   &lt;chr&gt; \"18-01-1997\", \"24-01-1997\", \"06-01-1997\", \"27-02-1997\", \"07-…\n$ fdate     &lt;chr&gt; \"31-12-2002\", \"31-12-2002\", \"31-12-2002\", \"11-12-1997\", \"31-…\n$ los       &lt;dbl&gt; 5, 5, 5, 10, 6, 1, 5, 4, 4, 5, 5, 10, 7, 21, 4, 1, 13, 14, 6…\n$ dstat     &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lenfol    &lt;dbl&gt; 2178, 2172, 2190, 297, 2131, 1, 2122, 1496, 920, 2175, 2173,…\n$ fstat     &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ admit     &lt;date&gt; 1997-01-13, 1997-01-19, 1997-01-01, 1997-02-17, 1997-03-01,…\n$ dis       &lt;date&gt; 1997-01-18, 1997-01-24, 1997-01-06, 1997-02-27, 1997-03-07,…\n$ f         &lt;date&gt; 2002-12-31, 2002-12-31, 2002-12-31, 1997-12-11, 2002-12-31,…\n$ diff1     &lt;dbl&gt; 5, 5, 5, 10, 6, 1, 5, 4, 4, 5, 5, 10, 7, 21, 4, 1, 13, 14, 6…\n$ diff2     &lt;dbl&gt; 2178, 2172, 2190, 297, 2131, 1, 2122, 1496, 920, 2175, 2173,…\n$ diff3     &lt;dbl&gt; 2173, 2167, 2185, 287, 2125, 0, 2117, 1492, 916, 2170, 2168,…\n\n:::\nThe extra d on the front of ddays indicates that these are what is known to lubridate as “durations”: a period of time 1 day long that could be any day (as opposed to “June 1, 1970” which is 1 day long, but tied to a particular day).\nlos should be the number of days in hospital, what I calculated as diff1, and lenfol should be the time from being admitted to last followup, which is my diff2. My output from glimpse confirms that.\nExtra: of course, checking that the first few values match is a nice confirmation, but is not actually a proof. For that, we should compare all 500 values, and it would be best to do it in such a way that R is comparing all 500 values for us, since it would be a lot more reliable than the human eye. R has a function all.equal which does exactly that. By way of warmup:\n\nx &lt;- 1:4\ny &lt;- 1:4\nz &lt;- c(1, 2, 3, 5)\nall.equal(x, y)\n\n[1] TRUE\n\nall.equal(x, z)\n\n[1] \"Mean relative difference: 0.25\"\n\n\nI thought the second one was just going to say FALSE, but it gave us a message instead, saying how close x and z were on average, so that we could decide whether they were close enough to call equal, or, as in this case, not.\nAnyway:\n\nwith(whas3, all.equal(lenfol, diff2))\n\n[1] TRUE\n\nwith(whas3, all.equal(los, diff1))\n\n[1] TRUE\n\n\nso they really are all equal, all 500 of them.15\n\\(\\blacksquare\\)\n\nConstruct side-by-side boxplots of the length of followup by each followup status. You’ll need to make sure that the followup status, as it gets fed into ggplot, is a factor, or, at least, not the number that it is now.\n\nSolution\nThe easiest way to make a factor is to wrap fstat, which is a numeric 0 or 1, in factor(): ::: {.cell}\nggplot(whas3, aes(x = factor(fstat), y = lenfol)) + geom_boxplot()\n\n\n\n:::\nOr create a factor version of fstat first:\n\nwhas3 %&gt;%\n  mutate(ffstat = factor(fstat)) %&gt;%\n  ggplot(aes(x = ffstat, y = lenfol)) + geom_boxplot()\n\n\n\n\nI think the second way looks better, because you get a cleaner \\(x\\)-axis on your plot. But if you’re doing this for exploration, rather than as something that’s going to appear in a report for your boss, the first way is fine.\nggplot also treats text stuff as categorical where needed, so this also works:\n\nwhas3 %&gt;%\n  mutate(cfstat = as.character(fstat)) %&gt;%\n  ggplot(aes(x = cfstat, y = lenfol)) + geom_boxplot()\n\n\n\n\nExtra: this is an example of what’s called “survival data”: the purpose of the study was to see what affected how long a person survived after a heart attack. Each patient was followed up for the number of days in lenfol, but followup could have stopped for two (or more) reasons: the patient died (indicated by fstat being 1), or something else happened to them (fstat is 0), such as moving away from where the study was conducted, getting another disease, the funding for this study running out, or simply losing touch with the people doing the study. Such a patient is called “lost to followup” or “censored”, and all we know about their survival is that they were still alive when last seen, but we don’t know how long they lived after that.\nFor example:\n\nwhas %&gt;% select(id, lenfol, fstat)\n\n\n\n  \n\n\n\nThe patient with id 4 died after 297 days, but patients 1 through 3 lived for over 2000 days and were still alive when last seen. My guess for patients 1 through 3 is that the study ended and they were still alive:\n\nwhas %&gt;% summarize(maxfol = max(lenfol)/365.25)\n\n\n\n  \n\n\n\nThe longest time anyone was followed up was six and a half years. Studies like this are funded for some number of years (say 10), and people can join after the beginning. (If they happen to join near the end, they won’t get followed up for very long.)\nWe’re not going to analyze these data, but if we were, we would want to take advantage of the information in the patient who lived for “at least 2178 days”. Looking only at the patients who we knew to have died would be wasteful and might introduce a bias; for example, if we were comparing several treatments, and one of the treatments was so good that almost everybody on it was still alive at the end, we would want to have a strong inference that this treatment was the best.\nWith that in mind, let’s redraw our boxplot with better labels for the followup status:\n\nwhas3 %&gt;% \n  mutate(followup_status = ifelse(fstat == 1, \"died\", \"censored\")) %&gt;% \n  ggplot(aes(x = followup_status, y = lenfol)) + geom_boxplot()\n\n\n\n\nNow we have a clearer sense of what is going on. Out of the patients who died, some of them survived a long time, but most of them died fairly quickly. Out of the patients who were censored, the times they were observed were all over the place, which suggests that (at least for the ones still in the study at the end) they joined the study at all kinds of different times.\nAnother graph that is possible here is a facetted histogram:\n\nwhas3 %&gt;% \n  mutate(followup_status = ifelse(fstat == 1, \"died\", \"censored\")) %&gt;% \n  ggplot(aes(x = lenfol)) + geom_histogram(bins = 10) +\n  facet_wrap(~followup_status)\n\n\n\n\nThe right-skewed distribution of times to death is what we saw from the boxplot, but what is that periodic thing on the left? Let’s convert the days to years and draw again:\n\nwhas3 %&gt;% \n  mutate(followup_status = ifelse(fstat == 1, \"died\", \"censored\")) %&gt;% \n  mutate(followup_years = lenfol/365.25) %&gt;% \n  ggplot(aes(x = followup_years)) + geom_histogram(bins = 20) +\n  facet_wrap(~followup_status)\n\n\n\n\nThat’s odd. On the left, it looks as if there were bursts of patients admitted to the study at around 1.5, 3.5, and 5.5 years from the end. (These are, remember, all people who survived and mostly people who survived to the end.) Not what I would have expected – I would have expected a steady stream of patients, the heart attack victims as they happened to come in.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dates-and-times.html#going-to-sleep-1",
    "href": "dates-and-times.html#going-to-sleep-1",
    "title": "19  Dates and times",
    "section": "19.10 Going to sleep",
    "text": "19.10 Going to sleep\nA student keeps track of what time they go to bed and what time they get up in the morning. They also have an app on their phone that measures the number of hours they were asleep during that time. The data for one week are in http://ritsokiguess.site/datafiles/sleeping.csv, in the 24-hour clock.\n\nRead in and display the data. What type of things are each of your columns?\n\nSolution\nThe usual, to start:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/sleeping.csv\"\nsleep &lt;- read_csv(my_url)\n\nRows: 7 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (1): sleep.time\ndttm (2): bed.time, rise.time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsleep\n\n\n\n  \n\n\n\nOn mine, the sleep time is an ordinary decimal number, but the two times are something called dttm, which I can guess means date-time. In your notebook, you might see S3:POSIXct, and you probably don’t know what that is (although you can guess).16\nIf you search for this, you’ll find some links to the help files, but a bit further down is this, which says it in a few words: “These objects store the number of seconds (for POSIXct) … since January 1st 1970 at midnight.”17\nMake the claim that the first two columns are genuine date-times, and if they are labelled S3:POSIXct for you, say how you know. That is to say, they may look like pieces of text laid out as date-times, but they are actual date-times stored internally as seconds since Jan 1 1970 and displayed nicely. Thus we do not need to use ymd_hms or anything similar to deal with them.\n\\(\\blacksquare\\)\n\nWork out the fractional number of hours that the student was in bed each of these nights. (They may not have been asleep this whole time.) Your result needs to be a number since we will be doing some calculations with it shortly.\n\nSolution\nSince these are genuine date-times, you can take the difference, but the unit is not predictable. Internally, these are stored as a number of seconds, but it displays a “nice” unit:\n\nsleep %&gt;% mutate(in_bed = rise.time - bed.time)\n\n\n\n  \n\n\n\nIn this case, we did get a number of hours, but in the next part, we are going to do a calculation like this:\n\nsleep %&gt;% mutate(in_bed = rise.time - bed.time) %&gt;% \nmutate(ratio = sleep.time / in_bed)\n\nError in `mutate()`:\nℹ In argument: `ratio = sleep.time/in_bed`.\nCaused by error in `/.difftime`:\n! second argument of / cannot be a \"difftime\" object\n\n\nand this doesn’t work because you can’t divide a number by a time. (What would its units be?) So we have to turn in_bed into a number, and to do that we can divide by the number of seconds in an hour:\n\nsleep %&gt;% mutate(in_bed = (rise.time - bed.time) / dhours(1))\n\n\n\n  \n\n\n\nThis is now correctly a (decimal) number.\n\\(\\blacksquare\\)\n\nThe student is concerned with something they call “sleep efficiency”. This is the percentage of time in bed spent sleeping. Work out the student’s sleep efficiency for the seven nights in this dataframe. Which night was the student’s sleep efficiency greatest?\n\nSolution\nDivide the sleep time by the in-bed time and multiply by 100. To answer the last part of the question, you might think of sorting these in descending order as well:\n\nsleep %&gt;% mutate(in_bed = (rise.time - bed.time) / dhours(1)) %&gt;% \nmutate(efficiency = sleep.time / in_bed * 100) %&gt;% \narrange(desc(efficiency))\n\n\n\n  \n\n\n\nThe night of September 8. This was the night the student went to bed the latest, but they were asleep almost all the time they were in bed.\n\\(\\blacksquare\\)\n\nDisplay the time spent in bed each night as a number of hours, minutes and seconds.\n\nSolution\nThe idea here is to display the time between going to bed and getting up as an interval, using %--%, and then turn that into a period:\n\nsleep %&gt;% mutate(in_bed_hms = as.period(bed.time %--% rise.time))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake a graph of what time the student went to bed each night. Bear in mind that you only need the times, not the dates, and that you want a graph that is informative, showing appropriately the distribution of times the student went to bed.\n\nSolution\nIf you just pull out the times, some of them will be at the end of the day and some will be at the beginning. Extracting the hours, minutes and seconds is one way:18\n\nsleep %&gt;% mutate(h = hour(bed.time), m = minute(bed.time), s = second(bed.time))\n\n\n\n  \n\n\n\nYou could convert these into fractional hours to make a histogram of:\n\nsleep %&gt;% mutate(h = hour(bed.time), m = minute(bed.time), s = second(bed.time)) %&gt;% \nmutate(bed_time_hours = h + m / 60 + s / (60*60))\n\n\n\n  \n\n\n\nbut if you make a histogram of these, this is what you get:\n\nsleep %&gt;% mutate(h = hour(bed.time), m = minute(bed.time), s = second(bed.time)) %&gt;% \nmutate(bed_time_hours = h + m / 60 + s / (60*60)) %&gt;% \nggplot(aes(x = bed_time_hours)) + geom_histogram(bins = 5)\n\n\n\n\nbut this makes no sense because the bedtimes after midnight are on the end of the previous day, not the beginning of the next one!\nWith that in mind, let’s move the bedtimes that are, say, before 3:00am to the end of the previous day by adding 24 to them before we make the graph:\n\nsleep %&gt;% mutate(h = hour(bed.time), m = minute(bed.time), s = second(bed.time)) %&gt;% \nmutate(bed_time_hours = h + m / 60 + s / (60*60)) %&gt;% \nmutate(bed_time_hours = ifelse(bed_time_hours &lt; 3, bed_time_hours + 24, bed_time_hours)) %&gt;% \nggplot(aes(x = bed_time_hours)) + geom_histogram(bins = 5)\n\n\n\n\nThis gives a sense of where the bedtimes are. If you’re used to reading the 24-hour clock, you’ll know that 23 is 11:00pm, and you’ll have a sense that some of the bedtimes were 11 or a bit earlier and some were around midnight. (I like the 24-hour clock.) There are only 7 observations, so the graph you get won’t look very nice as a histogram, but at least this one says something about when the student went to bed, in a way that puts times just after midnight next to times just before. You should give some thought about the number of bins; with only 7 observations, even 5 bins is pushing it, but this looked nicer to me than 4 bins.\nIf you’re more used to the 12-hour clock, you’ll want to convert the times to something between 10 and 12. You can do this with an ifelse as above, subtracting 12 from the ones before midnight and adding 12 to the ones after. Or you can recognize this as modulo arithmetic (the clock is a classic case: what is 10:00pm plus 3 hours?) A little thought will reveal that subtracting (or adding) 12 hours and taking the result modulo 24 would do it: the pre-midnight bedtimes will get turned into a number like 10 or 11, and the post-midnight ones to 12 and a bit. R has a modulo operator, which is %% (cite your source: mine was this):\n\nsleep %&gt;% mutate(h = hour(bed.time), m = minute(bed.time), s = second(bed.time)) %&gt;% \nmutate(bed_time_hours = h + m / 60 + s / (60*60)) %&gt;% \nmutate(bed_time_hours = (bed_time_hours - 12) %% 24) %&gt;% \nggplot(aes(x = bed_time_hours)) + geom_histogram(bins = 5)\n\n\n\n\nand you might find the \\(x\\)-scale of that easier to cope with. (The bins have come out differently, for some reason.)\nI think the best graph uses the fact that date-times plot nicely, so if we keep them as date-times, the \\(x\\)-scale will look nice. The problem is that they are times on different days. What if we faked it up so that they were all on the same day (or, at least, consecutive days, to account for the ones after midnight)?\nLet’s look at our dataframe again:\n\nsleep\n\n\n\n  \n\n\n\nThe rise.time values are all a.m., and on consecutive days, so if we subtract consecutive numbers of days from the bed.times, we’ll put them all on appropriate days too:\n\nsleep %&gt;% mutate(time_of_bed = bed.time - ddays(0:6))\n\n\n\n  \n\n\n\nThese are all around the midnight at the end of September 1, so some of them are in the early hours of September 2. Now, if we make a histogram of those:\n\nsleep %&gt;% mutate(time_of_bed = bed.time - ddays(0:6)) %&gt;% \nggplot(aes(x = time_of_bed)) + geom_histogram(bins = 5)\n\n\n\n\nNow the \\(x\\)-axis formatting looks like a time, and spills seamlessly into the next day. (There was no real range of dates, so the formatting is of the times only.)\nOne more embellishment, idea from here:\n\nsleep %&gt;% mutate(time_of_bed = bed.time - ddays(0:6)) %&gt;% \nggplot(aes(x = time_of_bed)) + geom_histogram(bins = 5) +\nscale_x_datetime(date_labels = \"%l:%M %p\")\n\n\n\n\nThe scale_x and scale_y functions customize the \\(x\\) and \\(y\\) axes respectively. Inside date_labels go some codes that say what time units you want to display: in this case, the 12-hour hours, the minutes, and whether the time is AM or PM. The codes come from a function called strftime, and a full list is here. Alternatively, you can look up the help for R’s function of the same name with ?strftime.19\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dates-and-times.html#footnotes",
    "href": "dates-and-times.html#footnotes",
    "title": "19  Dates and times",
    "section": "",
    "text": "In fact, it is believed that wolves help keep caribou herds strong by preventing over-population: that is, the weakest caribou are the ones taken by wolves.↩︎\nThe survey is always taken in the fall, but the date varies.↩︎\nCounting animals in a region, especially rare, hard-to-find animals, is a whole science in itself. Our data are probably themselves estimates (with some uncertainty).↩︎\nThis is an example of R’s so-called “recycling rules”.↩︎\nIn fact, it is believed that wolves help keep caribou herds strong by preventing over-population: that is, the weakest caribou are the ones taken by wolves.↩︎\nThe survey is always taken in the fall, but the date varies.↩︎\nCounting animals in a region, especially rare, hard-to-find animals, is a whole science in itself. Our data are probably themselves estimates (with some uncertainty).↩︎\nThe only way this would fail is if every first name had the same number of letters in it; then the space between first name and initial of last name would be in the same place in every line.↩︎\nNot a surprise since Denali National Park is in Alaska.↩︎\nIt’s actually not really living on the edge, because if it doesn’t work, you go back and read the data in from the file again.↩︎\nThis wiggles more than I would like, with such a small number of observations. Try putting something like span=2 in the smooth to make it less wiggly.↩︎\nI have to say that I didn’t know that until just now.↩︎\nWhich means, if you stop to think about it, that there are actually about 40 times more caribou than wolves.↩︎\nAnd all too often with Excel (spit).↩︎\nThe computer scientists among you will note that I shouldn’t have done this, because diff1 through diff3 are double-precision decimal numbers, so I should have tested their equality with lenfol and los by working out the absolute differences and testing whether they were all small. On consulting the help for all.equal, though, I find that it does work properly, because it actually tests whether the things being compared differ by less than a quantity tolerance which defaults to 0.000000015, and if they do it calls them equal. This is all tied in with the difference between integers and decimal numbers as they are represented on a computer: exactly and approximately, respectively. A double-precision number has about 16 significant digits of accuracy; equal things won’t have all 16 digits equal, most likely, but they would be expected to have at least 8 of those digits the same. CSCA08 stuff, I imagine. This is where you can casually toss around terms like “machine epsilon”. Oh! I just realized something. You know how very very small P-values are shown in R as &lt;2.2e-16? That’s the machine epsilon. Anything smaller than that is indistinguishable from zero, and you can’t have a P-value be exactly zero. The default tolerance I mentioned above is the square root of this, which is normally used for such things.↩︎\nWhich one you see will depend on which type of output you are looking at. In your notebook you will probably see the POSIXCT thing, but in your HTML or PDF output it will probably say ``dttm’’.↩︎\nThe piece in quotes comes word-for-word from the source: it is exactly what the author said. Except that the author also talks about dates, which don’t concern us here, so I removed that bit and replaced it with the three dots, called an “ellipsis”, to show that the author said some extra stuff that I didn’t quote. I checked that what remained does actually still capture what the author said. Extra-in-note: an ellipsis is not to be confused with the conic section called an ellipse, and these three dots are not to be confused with the three dots in an R function, where they mean “anything else that was passed in to the function”. Both uses of the three dots capture the idea of “something was missed out”.↩︎\nMake sure you use “hour” and not “hours” as I did the first time. That computes the total number of hours between the zero date of Jan 1, 1970 and the time given, and so is way too large to be an answer here!↩︎\nConfusingly, uppercase I and lowercase l not only look the same, but they also both display the 12-hour hour. The former adds a zero to the front if the hour is a single digit, and the latter does not. All the hours here have two digits, though, so it comes out the same whichever you use.↩︎"
  },
  {
    "objectID": "functions.html#making-some-r-functions",
    "href": "functions.html#making-some-r-functions",
    "title": "20  Functions",
    "section": "20.1 Making some R functions",
    "text": "20.1 Making some R functions\nLet’s write some simple R functions to convert temperatures, and later to play with text.\n\nA temperature in Celsius is converted to one in Kelvin by adding 273.15. (A temperature of \\(-273.15\\) Celsius, 0 Kelvin, is the “absolute zero” temperature that nothing can be colder than.) Write a function called c_to_k that converts an input Celsius temperature to one in Kelvin, and test that it works.\nWrite a function to convert a Fahrenheit temperature to Celsius. The way you do that is to subtract 32 and then multiply by \\(5/9\\).\nUsing the functions you already wrote, write a function to convert an input Fahrenheit temperature to Kelvin.\nRewrite your Fahrenheit-to-Celsius convertor to take a suitable default value and check that it works as a default.\nWhat happens if you feed your Fahrenheit-to-Celsius convertor a vector of Fahrenheit temperatures? What if you use it in a mutate?\nWrite another function called wrap that takes two arguments: a piece of text called text, which defaults to hello, and another piece of text called outside, which defaults to *. The function returns text with the text outside placed before and after, so that calling the function with the defaults should return *hello*. To do this, you can use str_c from stringr (loaded with the tidyverse) which places its text arguments side by side and glues them together into one piece of text. Test your function briefly.\nWhat happens if you want to change the default outside but use the default for text? How do you make sure that happens? Explore.\nWhat happens if you feed your function wrap a vector for either of its arguments? What about if you use it in a mutate?"
  },
  {
    "objectID": "functions.html#the-collatz-sequence",
    "href": "functions.html#the-collatz-sequence",
    "title": "20  Functions",
    "section": "20.2 The Collatz sequence",
    "text": "20.2 The Collatz sequence\nThe Collatz sequence is a sequence of integers \\(x_1, x_2, \\ldots\\) defined in a deceptively simple way: if \\(x_n\\) is the current term of the sequence, then \\(x_{n+1}\\) is defined as \\(x_n/2\\) if \\(x_n\\) is even, and \\(3x_n+1\\) if \\(x_n\\) is odd. We are interested in understanding how this sequence behaves; for example, what happens to it as \\(n\\) gets large, for different choices of the first term \\(x_1\\)? We will explore this numerically with R; the ambitious among you might like to look into the mathematics of it.\n\nWhat happens to the sequence when it reaches 4? What would be a sensible way of defining where it ends? Explain briefly.\nWrite an R function called is_odd that returns TRUE if its input is an odd number and FALSE if it is even (you can assume that the input is an integer and not a decimal number). To do that, you can use the function %% where a %% b is the remainder when a is divided by b. To think about oddness or evenness, consider the remainder when you divide by 2.\nWrite an R function called hotpo11 that takes an integer as input and returns the next number in the Collatz sequence. To do this, use the function you just wrote that determines whether a number is even or odd.\nNow write a function hotpo that will return the whole Collatz sequence for an input \\(x_1\\). For this, assume that you will eventually get to 1.\nWrite two (very small) functions that take an entire sequence as input and return (i) the length of the sequence and (ii) the maximum value it attains.\nMake a data frame consisting of the values 11 through 20, and, using tidyverse ideas, obtain a data frame containing the Collatz sequences starting at each of those values, along with their lengths and their maximum values. Which sequence is longest? Which one goes up highest?"
  },
  {
    "objectID": "functions.html#coefficient-of-variation",
    "href": "functions.html#coefficient-of-variation",
    "title": "20  Functions",
    "section": "20.3 Coefficient of Variation",
    "text": "20.3 Coefficient of Variation\nThe coefficient of variation of a vector x is defined as the standard deviation of x divided by the mean of x.\n\nWrite a function called cv that calculates the coefficient of variation of its input and returns the result. You should use base R’s functions that reliably compute the pieces that you need.\nUse your function to find the coefficient of variation of the set of integers 1 through 5.\nDefine a vector as follows:\n\n\nv &lt;- c(-2.8, -1.8, -0.8, 1.2, 4.2)\n\nWhat is its coefficient of variation, according to your function? Does this make sense? Why did this happen? Explain briefly.\n\nMost people only calculate a coefficient of variation if there are no negative numbers. Rewrite your function so that it gives an error if there are any negative numbers in the input, and test it with the vector v above. Hint: you might need to add error=TRUE to your chunk header to allow your document to preview/knit (inside the curly brackets at the top of the chunk, after a comma)."
  },
  {
    "objectID": "functions.html#rescaling",
    "href": "functions.html#rescaling",
    "title": "20  Functions",
    "section": "20.4 Rescaling",
    "text": "20.4 Rescaling\nSuppose we have this vector of values:\n\nz &lt;- c(10, 14, 11)\nz\n\n[1] 10 14 11\n\n\nWe want to scale these so that the smallest value is 0 and the largest is 1. We are going to be doing this a lot, so we are going to write a function that will work for any input.\n\nUsing a copy of my z, work out min(z) and max(z). What do they do? Explain (very) briefly.\nWhat do these lines of code do, using the same z that I had? Run them and see, and describe briefly what s contains.\n\n\nlo &lt;- min(z)\nhi &lt;- max(z)\ns &lt;- (z - lo) / (hi - lo)\ns\n\n\nWrite a function called rescale that implements the calculation above, for any input vector called x. (Note that I changed the name.)\nTest your function on my z, and on another vector of your choosing. Explain briefly why the answer you get from your vector makes sense.\nWhat happens if your input to rescale is a vector of numbers all the same? Give an example. Rewrite your function to intercept this case and give a helpful error message.\nMake a dataframe (containing any numeric values), and in it create a new column containing the rescaled version of one of its columns, using your function. Show your result.\nWe might want to rescale the input not to be between 0 and 1, but between two values a and b that we specify as input. If a and/or b are not given, we want to use the values 0 for a and 1 for b. Rewrite your function to rescale the input to be between a and b instead of 0 and 1. Hint: allow your function to produce values between 0 and 1 as before, and then note that if all the values in a vector s are between 0 and 1, then all the values in a+(b-a)*s are between \\(a\\) and \\(b\\).\nTest your new function two or more times, on input where you know or can guess what the output is going to be. In each case, explain briefly why your output makes sense.\n\nMy solutions follow:"
  },
  {
    "objectID": "functions.html#making-some-r-functions-1",
    "href": "functions.html#making-some-r-functions-1",
    "title": "20  Functions",
    "section": "20.5 Making some R functions",
    "text": "20.5 Making some R functions\nLet’s write some simple R functions to convert temperatures, and later to play with text.\n\nA temperature in Celsius is converted to one in Kelvin by adding 273.15. (A temperature of \\(-273.15\\) Celsius, 0 Kelvin, is the “absolute zero” temperature that nothing can be colder than.) Write a function called c_to_k that converts an input Celsius temperature to one in Kelvin, and test that it works.\n\nSolution\nThis is mostly an exercise in structuring your function correctly. Let’s call the input C (uppercase C, since lowercase c has a special meaning to R): ::: {.cell}\nc_to_k &lt;- function(C) {\n  C + 273.15\n}\nc_to_k(0)\n\n[1] 273.15\n\nc_to_k(20)\n\n[1] 293.15\n\n:::\nThis is the simplest way to do it: the last line of the function, if calculated but not saved, is the value that gets returned to the outside world. The checks suggest that it worked.\nIf you’re used to Python or similar, you might prefer to calculate the value to be returned and then return it. You can do that in R too:\n\nc_to_k &lt;- function(C) {\n  K &lt;- C + 273.15\n  return(K)\n}\nc_to_k(0)\n\n[1] 273.15\n\nc_to_k(20)\n\n[1] 293.15\n\n\nThat works just as well, and for the rest of this question, you can go either way.2\n\\(\\blacksquare\\)\n\nWrite a function to convert a Fahrenheit temperature to Celsius. The way you do that is to subtract 32 and then multiply by \\(5/9\\).\n\nSolution\nOn the model of the previous one, we should call this f_to_c. I’m going to return the last line, but you can save the calculated value and return that instead: ::: {.cell}\nf_to_c &lt;- function(F) {\n  (F - 32) * 5 / 9\n}\nf_to_c(32)\n\n[1] 0\n\nf_to_c(50)\n\n[1] 10\n\nf_to_c(68)\n\n[1] 20\n\n:::\nAmericans are very good at saying things like “temperatures in the 50s”, which don’t mean much to me, so I like to have benchmarks to work with: these are the Fahrenheit versions of 0, 10, and 20 Celsius.\nThus “in the 50s” means “between about 10 and 15 Celsius”.\n\\(\\blacksquare\\)\n\nUsing the functions you already wrote, write a function to convert an input Fahrenheit temperature to Kelvin.\n\nSolution\nThis implies that you can piggy-back on the functions you just wrote, which goes as below. First you convert the Fahrenheit to Celsius, and then you convert that to Kelvin. (This is less error-prone than trying to use algebra to get a formula for this conversion and then implementing that): ::: {.cell}\nf_to_k &lt;- function(F) {\n  C &lt;- f_to_c(F)\n  K &lt;- c_to_k(C)\n  return(K)\n}\nf_to_k(32)\n\n[1] 273.15\n\nf_to_k(68)\n\n[1] 293.15\n\n:::\nThese check because in Celsius they are 0 and 20 and we found the Kelvin equivalents of those to be these values earlier.\nI wrote this one with a return because I thought it made the structure clearer: run one function, save the result, run another function, save the result, then return what you’ve got.\n\\(\\blacksquare\\)\n\nRewrite your Fahrenheit-to-Celsius convertor to take a suitable default value and check that it works as a default.\n\nSolution\nYou can choose any default you like. I’ll take a default of 68 (what I would call “a nice day”): ::: {.cell}\nf_to_c &lt;- function(F = 68) {\n  (F - 32) * 5 / 9\n}\nf_to_c(68)\n\n[1] 20\n\nf_to_c()\n\n[1] 20\n\n:::\nThe change is in the top line of the function. You see the result: if we run it without an input, we get the same answer as if the input had been 68.\n\\(\\blacksquare\\)\n\nWhat happens if you feed your Fahrenheit-to-Celsius convertor a vector of Fahrenheit temperatures? What if you use it in a mutate?\n\nSolution\nTry it and see: ::: {.cell}\ntemps &lt;- seq(30, 80, 10)\ntemps\n\n[1] 30 40 50 60 70 80\n\nf_to_c(temps)\n\n[1] -1.111111  4.444444 10.000000 15.555556 21.111111 26.666667\n\n:::\nEach of the Fahrenheit temperatures gets converted into a Celsius one. This is perhaps more useful in a data frame, thus:\n\ntibble(temps = seq(30, 80, 10)) %&gt;%\n  mutate(celsius = f_to_c(temps))\n\n\n\n  \n\n\n\nAll the temperatures are side-by-side with their equivalents.\nHere’s another way to do the above:\n\ntemps &lt;- seq(30, 80, 10)\ntemps %&gt;%\n  enframe(value = \"fahrenheit\") %&gt;%\n  mutate(celsius = f_to_c(temps))\n\n\n\n  \n\n\n\nenframe creates a two-column data frame out of a vector (like temps). A vector can have “names”, in which case they’ll be used as the name column; the values will go in a column called value unless you rename it, as I did.\n\\(\\blacksquare\\)\n\nWrite another function called wrap that takes two arguments: a piece of text called text, which defaults to hello, and another piece of text called outside, which defaults to *. The function returns text with the text outside placed before and after, so that calling the function with the defaults should return *hello*. To do this, you can use str_c from stringr (loaded with the tidyverse) which places its text arguments side by side and glues them together into one piece of text. Test your function briefly.\n\nSolution\nThis: ::: {.cell}\nwrap &lt;- function(text = \"hello\", outside = \"*\") {\n  str_c(outside, text, outside)\n}\n:::\nI can run this with the defaults:\n\nwrap()\n\n[1] \"*hello*\"\n\n\nor with text of my choosing:\n\nwrap(\"goodbye\", \"_\")\n\n[1] \"_goodbye_\"\n\n\nI think that’s what I meant by “test briefly”.\n\\(\\blacksquare\\)\n\nWhat happens if you want to change the default outside but use the default for text? How do you make sure that happens? Explore.\n\nSolution\nThe obvious thing is this, which doesn’t work: ::: {.cell}\nwrap(\"!\")\n\n[1] \"*!*\"\n\n:::\nThis takes text to be !, and outside to be the default. How do we get outside to be ! instead? The key is to specify the input by name:\n\nwrap(outside = \"!\")\n\n[1] \"!hello!\"\n\n\nThis correctly uses the default for text.\nIf you specify inputs without names, they are taken to be in the order that they appear in the function definition. As soon as they get out of order, which typically happens by using the default for something early in the list, as we did here for text, you have to specify names for anything that comes after that. These are the names you put on the function’s top line.\nYou can always use names:\n\nwrap(text = \"thing\", outside = \"**\")\n\n[1] \"**thing**\"\n\n\nand if you use names, they don’t even have to be in order:\n\nwrap(outside = \"!?\", text = \"fred\")\n\n[1] \"!?fred!?\"\n\n\n\\(\\blacksquare\\)\n\nWhat happens if you feed your function wrap a vector for either of its arguments? What about if you use it in a mutate?\n\nSolution\nLet’s try: ::: {.cell}\nmytext &lt;- c(\"a\", \"b\", \"c\")\nwrap(text = mytext)\n\n[1] \"*a*\" \"*b*\" \"*c*\"\n\n:::\n\nmyout &lt;- c(\"*\", \"!\")\nwrap(outside = myout)\n\n[1] \"*hello*\" \"!hello!\"\n\n\nIf one of the inputs is a vector, the other one gets “recycled” as many times as the vector is long. What if they’re both vectors?\n\nmytext2 &lt;- c(\"a\", \"b\", \"c\", \"d\")\nwrap(mytext2, myout)\n\nError in `str_c()`:\n! Can't recycle `..1` (size 2) to match `..2` (size 4).\n\n\nThis gives an error because str_c won’t let you recycle things that are both vectors.\nThe mutate thing is easier, because all the columns in a data frame have to be the same length. LETTERS is a vector with the uppercase letters in it:\n\ntibble(mytext = LETTERS[1:6], myout = c(\"*\", \"**\", \"!\", \"!!\", \"_\", \"__\")) %&gt;%\n  mutate(newthing = wrap(mytext, myout))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "functions.html#the-collatz-sequence-1",
    "href": "functions.html#the-collatz-sequence-1",
    "title": "20  Functions",
    "section": "20.6 The Collatz sequence",
    "text": "20.6 The Collatz sequence\nThe Collatz sequence is a sequence of integers \\(x_1, x_2, \\ldots\\) defined in a deceptively simple way: if \\(x_n\\) is the current term of the sequence, then \\(x_{n+1}\\) is defined as \\(x_n/2\\) if \\(x_n\\) is even, and \\(3x_n+1\\) if \\(x_n\\) is odd. We are interested in understanding how this sequence behaves; for example, what happens to it as \\(n\\) gets large, for different choices of the first term \\(x_1\\)? We will explore this numerically with R; the ambitious among you might like to look into the mathematics of it.\n\nWhat happens to the sequence when it reaches 4? What would be a sensible way of defining where it ends? Explain briefly.\n\nSolution\nWhen the sequence reaches 4 (that is, when its current term is 4), the next term is 2 and the one after that is 1. Then the following term is 4 again (\\((3 \\times 1)+1\\)) and then it repeats indefinitely, \\(4, 2, 1, 4, 2, 1, \\ldots\\). I think a sensible way to define where the sequence ends is to say “when it reaches 1”, since if you start at 2 you’ll never reach 4 (so “when it reaches 4” won’t work), and it seems at least plausible that it will hit the cycle 4, 2, 1 sometime.\n\\(\\blacksquare\\)\n\nWrite an R function called is_odd that returns TRUE if its input is an odd number and FALSE if it is even (you can assume that the input is an integer and not a decimal number). To do that, you can use the function %% where a %% b is the remainder when a is divided by b. To think about oddness or evenness, consider the remainder when you divide by 2.\n\nSolution\nLet’s try this out. For example, 5 is odd and 6 is even, so ::: {.cell}\n5 %% 2\n\n[1] 1\n\n6 %% 2\n\n[1] 0\n\n:::\nWhen a number is odd, its remainder on dividing by 2 is 1, and when even, the remainder is 0. There is an additional shortcut here in that 1 is the numeric value of TRUE and 0 of FALSE, so all we have to do is calculate the remainder on dividing by 2, turn it into a logical, and return it:\n\nis_odd &lt;- function(x) {\n  r &lt;- x %% 2\n  as.logical(r)\n}\n\nYou probably haven’t seen as.logical before, but it’s the same idea as as.numeric: turn something that looks like a TRUE or FALSE into something that actually is.\nWe should test it:\n\nis_odd(19)\n\n[1] TRUE\n\nis_odd(12)\n\n[1] FALSE\n\nis_odd(0)\n\n[1] FALSE\n\n\n0 is usually considered an even number, so this is good.\n\\(\\blacksquare\\)\n\nWrite an R function called hotpo13 that takes an integer as input and returns the next number in the Collatz sequence. To do this, use the function you just wrote that determines whether a number is even or odd.\n\nSolution\nThe logic is “if the input is odd, return 3 times it plus 1, otherwise return half of it”. The R structure is an if-then-else: ::: {.cell}\nhotpo1 &lt;- function(x) {\n  if (is_odd(x)) 3 * x + 1 else x / 2\n}\n:::\nIn R, the condition that is tested goes in brackets, and then if the value-if-true and the value-if-false are single statements, you just type them. (If they are more complicated than that, you put them in curly brackets.) Now you see the value of writing is_odd earlier; this code almost looks like the English-language description of the sequence. If we had not written is_odd before, the condition would have looked something like\n\nif (x %% 2 == 1) 3 * x + 1 else x / 2\n\nwhich would have been a lot harder to read.\nAll right, let’s try that out:\n\nhotpo1(4)\n\n[1] 2\n\nhotpo1(7)\n\n[1] 22\n\nhotpo1(24)\n\n[1] 12\n\n\nThat looks all right so far.\n\\(\\blacksquare\\)\n\nNow write a function hotpo that will return the whole Collatz sequence for an input \\(x_1\\). For this, assume that you will eventually get to 1.\n\nSolution\nThis is a loop, but not a for loop (or something that we could do rowwise), because we don’t know how many times we have to go around. This is the kind of thing that we should use a while loop for: “keep going while a condition is true”. In this case, we should keep going if we haven’t reached 1 yet. If we haven’t reached 1, we should generate the next value of the sequence and glue it onto what we have so far. To initialize the sequence, we start with the input value. There is an R trick to glue a value onto the sequence, which is to use c with a vector and a value, and save it back into the vector: ::: {.cell}\nhotpo &lt;- function(x) {\n  sequence &lt;- x\n  term &lt;- x\n  while (term &gt; 1) {\n    term &lt;- hotpo1(term)\n    sequence &lt;- c(sequence, term)\n  }\n  sequence\n}\n:::\nI use term to hold the current term of the sequence, and overwrite it with the next one (since I don’t need the old one any more).\nDoes it work?\n\nhotpo(4)\n\n[1] 4 2 1\n\nhotpo(12)\n\n [1] 12  6  3 10  5 16  8  4  2  1\n\nhotpo(97)\n\n  [1]   97  292  146   73  220  110   55  166   83  250  125  376  188   94   47\n [16]  142   71  214  107  322  161  484  242  121  364  182   91  274  137  412\n [31]  206  103  310  155  466  233  700  350  175  526  263  790  395 1186  593\n [46] 1780  890  445 1336  668  334  167  502  251  754  377 1132  566  283  850\n [61]  425 1276  638  319  958  479 1438  719 2158 1079 3238 1619 4858 2429 7288\n [76] 3644 1822  911 2734 1367 4102 2051 6154 3077 9232 4616 2308 1154  577 1732\n [91]  866  433 1300  650  325  976  488  244  122   61  184   92   46   23   70\n[106]   35  106   53  160   80   40   20   10    5   16    8    4    2    1\n\n\n97 is a wild ride, but it does eventually get to 1.\nExtra: where I originally saw this, which was “Metamagical Themas” by Douglas Hofstadter, he was illustrating the programming language Lisp and the process of recursion, whereby you define a function in terms of itself. This one is a natural for that, because the Collatz sequence starting at \\(x\\) is \\(x\\) along with the Collatz sequence starting at the next term. For example, if you start at 12, the next term is 6, so that the Collatz sequence starting at 12 is 12 followed by the Collatz sequence starting at 6. There is no dependence any further back. You can do recursion in R also; there is no problem with a function calling itself:\n\nhotpo_rec &lt;- function(x) {\n  if (x == 1) 1 else c(x, hotpo_rec(hotpo1(x)))\n}\n\nRecursive functions have two parts: a “base case” that says how you know you are done (the 1 here), and a “recursion” that says how you move to a simpler case, here working out the next term, getting the whole sequence for that, and gluing the input onto the front. It seems paradoxical that you define a function in terms of itself, but what you are doing is calling a simpler sequence, in this case one that is length one shorter than the sequence for the original input. Thus, we hope,4 we will eventually reach 1.\nDoes it work?\n\nhotpo_rec(12)\n\n [1] 12  6  3 10  5 16  8  4  2  1\n\nhotpo_rec(97)\n\n  [1]   97  292  146   73  220  110   55  166   83  250  125  376  188   94   47\n [16]  142   71  214  107  322  161  484  242  121  364  182   91  274  137  412\n [31]  206  103  310  155  466  233  700  350  175  526  263  790  395 1186  593\n [46] 1780  890  445 1336  668  334  167  502  251  754  377 1132  566  283  850\n [61]  425 1276  638  319  958  479 1438  719 2158 1079 3238 1619 4858 2429 7288\n [76] 3644 1822  911 2734 1367 4102 2051 6154 3077 9232 4616 2308 1154  577 1732\n [91]  866  433 1300  650  325  976  488  244  122   61  184   92   46   23   70\n[106]   35  106   53  160   80   40   20   10    5   16    8    4    2    1\n\n\nIt does.\nRecursive functions are often simple to understand, but they are not always very efficient. They can take a lot of memory, because they have to handle the intermediate calls to the function, which they have to save to use later (in the case of hotpo_rec(97) there are a lot of those). Recursive functions are often paired with a technique called “memoization”, where each time you calculate the function’s value, you save it in another array. The first thing you do in the recursive function is to check whether you already have the answer, in which case you just look it up and return it. It was a lot of work here to calculate the sequence from 97, but if we had saved the results, we would already have the answers for 292, 146, 73, 220 and so on, and getting those later would be a table lookup rather than another recursive calculation.\n\\(\\blacksquare\\)\n\nWrite two (very small) functions that take an entire sequence as input and return (i) the length of the sequence and (ii) the maximum value it attains.\n\nSolution\nThese are both one-liners. Call the input whatever you like:\n\nhotpo_len &lt;- function(sequence) length(sequence)\nhotpo_max &lt;- function(sequence) max(sequence)\n\nBecause they are one-liners, you don’t even need the curly brackets, although there’s no problem if they are there.\nTesting:\n\nhotpo_len(hotpo(12))\n\n[1] 10\n\nhotpo_max(hotpo(97))\n\n[1] 9232\n\n\nThis checks with what we had before.\n\\(\\blacksquare\\)\n\nMake a data frame consisting of the values 11 through 20, and, using tidyverse ideas, obtain a data frame containing the Collatz sequences starting at each of those values, along with their lengths and their maximum values. Which sequence is longest? Which one goes up highest?\n\nSolution\nThis one uses rowwise ideas:5\n\ntibble(x = 11:20) %&gt;%\n  rowwise %&gt;% \n  mutate(sequence = list(hotpo(x))) %&gt;%\n  mutate(length = hotpo_len(sequence)) %&gt;%\n  mutate(high = hotpo_max(sequence))\n\n\n\n  \n\n\n\nFirst, we obtain a list-column containing the sequences (which is why its calculation needs a list around it), then two ordinary columns of their lengths and their maximum values.\nThe sequences for 18 and 19 are the longest, but the sequence for 15 goes up the highest.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "functions.html#coefficient-of-variation-1",
    "href": "functions.html#coefficient-of-variation-1",
    "title": "20  Functions",
    "section": "20.7 Coefficient of Variation",
    "text": "20.7 Coefficient of Variation\nThe coefficient of variation of a vector x is defined as the standard deviation of x divided by the mean of x.\n\nWrite a function called cv that calculates the coefficient of variation of its input and returns the result. You should use base R’s functions that reliably compute the pieces that you need.\n\nSolution\nI like to make a function skeleton first to be sure I remember all the pieces. You can do this by making a code chunk, typing fun, waiting a moment, and selecting the “snippet” that is offered to you. That gives you this, with your cursor on name:\n\nname &lt;- function(variables) {\n\n}\n\nGive your function a name (I asked you to use cv), hit tab, enter a name like x for the input, hit tab again, and then fill in the body of the function, to end up with something like this:\n\ncv &lt;- function(x) {\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\n\nI think this is best, for reasons discussed below.\nSome observations:\n\nin the last line of the function, you calculate something, and that something is the thing that gets returned. You do not need to save it in a variable, because then you have to return that variable by having its name alone on the last line.\nR has a return() function, but it is bad style to use it to return a value at the end of a function. The time to use it is when you test something early in the function and want to return early with a value like zero or missing without doing any actual calculation. Looking ahead a bit, you might want to return “missing” if the mean is zero before dividing by it, which you could do like this, this being good style:\n\n\ncv2 &lt;- function(x) {\nmean_x &lt;- mean(x)\nif (mean_x == 0) return(NA)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\n\n\nuse the built-in mean and sd rather than trying to roll your own, because they have been tested by many users over a long time and they work. That’s what I meant by “pieces” in the question.\nyou might be tempted to do something like this:\n\n\ncv3 &lt;- function(x) {\nsd(x)/mean(x)\n}\n\nThis will work, but it is not a good habit to get into, and thus not best as an answer to this question. This one line of code says three things: “work out the SD of x”, “work out the mean of x”, and “divide them by each other”. It is much clearer, for anyone else reading the code (including yourself in six months when you have forgotten what you were thinking) to have the three things one per line, so that anyone reading the code sees that each line does one thing and what that one thing is. There is a reason why production code and code golf are two very different things. Code that is easy to read is also easy to maintain, either by you or others.\n\nusing something other than mean and sd as the names of your intermediate results is a good idea because these names are already used by R (as the names of the functions that compute the mean and SD). Redefining names that R uses can make other code behave unpredictably and cause hard-to-find bugs.6\n\n\\(\\blacksquare\\)\n\nUse your function to find the coefficient of variation of the set of integers 1 through 5.\n\nSolution\nThis can be as simple as\n\ncv(1:5)\n\n[1] 0.5270463\n\n\nor, equally good, define this into a vector first:\n\ny &lt;- 1:5\ncv(y)\n\n[1] 0.5270463\n\n\nor, displaying a little less understanding, type the five numbers into the vector first (or directly as input to the function):\n\ny &lt;- c(1, 2, 3, 4, 5)\ncv(y)\n\n[1] 0.5270463\n\ncv(c(1, 2, 3, 4, 5))\n\n[1] 0.5270463\n\n\n\\(\\blacksquare\\)\n\nDefine a vector as follows:\n\n\nv &lt;- c(-2.8, -1.8, -0.8, 1.2, 4.2)\n\nWhat is its coefficient of variation, according to your function? Does this make sense? Why did this happen? Explain briefly.\nSolution\nTry it and see:\n\ncv(v)\n\n[1] 6.248491e+16\n\n\nA very large number, much bigger than any of the data values; with these human-sized numbers, we’d expect a human-sized coefficient of variation as well.\nWhat actually happened was that the mean of v is this:\n\nmean(v)\n\n[1] 4.440892e-17\n\n\nZero, or close enough, so that in calculating the coefficient of variation, we divided by something that was (almost) zero and got a result that was (almost) infinite.7\nThe possibility of getting a zero mean is why most people only calculate a coefficient of variation if all of the numbers are positive, which brings us to the next part:\n\\(\\blacksquare\\)\n\nMost people only calculate a coefficient of variation if there are no negative numbers. Rewrite your function so that it gives an error if there are any negative numbers in the input, and test it with the vector v above. Hint: you might need to add error=TRUE to your chunk header to allow your document to preview/knit (inside the curly brackets at the top of the chunk, after a comma).\n\nSolution\nThis is a case for stopifnot, or of if coupled with stop. Check this up front, as the first thing you do before you calculate anything else. As to what to check, there are several possibilities:\n\nstop if any of the numbers are negative\ncontinue if all of the numbers are positive\nstop if the smallest number is negative (the smallest number is negative if and only if not all the numbers are positive)\n\nR has functions any and all that do what you’d expect:\n\nw &lt;- 1:5\nw\n\n[1] 1 2 3 4 5\n\nany(w&gt;3.5)\n\n[1] TRUE\n\nall(w&lt;4.5)\n\n[1] FALSE\n\n\nAre there any numbers greater than 3.5 (yes, 4 and 5); are all the numbers less than 4.5 (no, 5 isn’t).\nCite your sources for these if you use either of them, since this is the first place in the course that I’m mentioning either of them.\nRemember that if you use stopifnot, the condition that goes in there is what has to be true if the function is to run; if you use if and stop, the condition is what will stop the function running. With that in mind, I would code my three possibilities above this way. First off, here’s the original:\n\ncv &lt;- function(x) {\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\n\nthen, stop if any of the numbers are negative:\n\ncv &lt;- function(x) {\nif (any(x&lt;0)) stop(\"A value is negative\")\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): A value is negative\n\n\ncontinue if all the numbers are positive\n\ncv &lt;- function(x) {\nstopifnot(all(x&gt;0))\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): all(x &gt; 0) is not TRUE\n\n\nstop if the smallest value is negative\n\ncv &lt;- function(x) {\nif (min(x)&lt;0) stop(\"Smallest value is negative\")\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): Smallest value is negative\n\n\nThere are (at least) three other possibilities: you can negate the logical condition and interchange if/stop and stopifnot, thus (at the expense of some clarity of reading):\ncontinue if it is not true that any of the numbers are negative\n\ncv &lt;- function(x) {\nstopifnot(!any(x&lt;0))\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): !any(x &lt; 0) is not TRUE\n\n\n(you might be thinking of De Morgan’s laws here)\nstop if it is not true that all the numbers are positive\n\ncv &lt;- function(x) {\nif (!all(x&gt;0)) stop(\"Not all values are positive\")\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): Not all values are positive\n\n\ncontinue if the smallest value is not negative\n\ncv &lt;- function(x) {\nstopifnot(min(x)&gt;=0)\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): min(x) &gt;= 0 is not TRUE\n\n\nor another way to do the last one, a more direct negation of the condition, which at my guess needs some extra brackets:\n\ncv &lt;- function(x) {\nstopifnot(!(min(x)&lt;0))\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): !(min(x) &lt; 0) is not TRUE\n\n\nThis one is hard to parse: what does that last message mean? I would take a negative off each side and read it as “min of x is negative is TRUE”, but that takes extra effort.\nI said that last one needed some extra brackets. This is, I thought, to get the order of operations right (operator precedence); it turns out not to matter because “not” has lower precedence than most other things, so that these do actually work (the “not” is evaluated after the less-than and the other things, so last of all here, even though it appears to be “glued” to the min):\n\n!min(v)&lt;0\n\n[1] FALSE\n\n!min(1:5)&lt;0\n\n[1] TRUE\n\n\nSee this for details. See especially the second set of examples, the ones beginning with “Special operators”, and see especially-especially the comment at the bottom of these examples! That is to say, you should put in the extra brackets unless you also make the case that they are not needed, because anyone reading your code is guaranteed to be confused by it when they read it (including you in six months, because you will not remember the operator priority of “not”).\nMy take is that one of the first three of the seven possibilities for coding stopifnot or if with stop is the best, since these more obviously encode the condition for continuing or stopping as appropriate. There are two things here: one is that you have to get the code right, but the second is that you have to get the code clear, so that it is obvious to anyone reading it that it does the right thing (once again, this includes you in six months). On that score, the first three alternatives are a direct expression of what you want to achieve, and the last four make it look as if you found a way of coding it that worked and stopped there, without thinking about whether there were any other, clearer or more expressive, possibilities.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "functions.html#rescaling-1",
    "href": "functions.html#rescaling-1",
    "title": "20  Functions",
    "section": "20.8 Rescaling",
    "text": "20.8 Rescaling\nSuppose we have this vector of values:\n\nz &lt;- c(10, 14, 11)\nz\n\n[1] 10 14 11\n\n\nWe want to scale these so that the smallest value is 0 and the largest is 1. We are going to be doing this a lot, so we are going to write a function that will work for any input.\n\nUsing a copy of my z, work out min(z) and max(z). What do they do? Explain (very) briefly.\n\nSolution\nSimply this – define z first:\n\nz &lt;- c(10, 14, 11)\nmin(z)\n\n[1] 10\n\nmax(z)\n\n[1] 14\n\n\nThey are respectively (and as you would guess) the smallest and largest of the values in z. (A nice gentle warmup, but I wanted to get you on the right track for what is coming up.)\n\\(\\blacksquare\\)\n\nWhat do these lines of code do, using the same z that I had? Run them and see, and describe briefly what s contains.\n\n\nlo &lt;- min(z)\nhi &lt;- max(z)\ns &lt;- (z - lo) / (hi - lo)\ns\n\nSolution\nHere we go:\n\nlo &lt;- min(z)\nhi &lt;- max(z)\ns &lt;- (z - lo) / (hi - lo)\ns\n\n[1] 0.00 1.00 0.25\n\n\nThe lowest value became 0, the highest 1, and the other one to something in between. Saying this shows the greatest insight.\nExtra: the reason for doing it in three steps rather than one (see below) is (i) it makes it clearer what is going on (and thus makes it less likely that you make a mistake), and (ii) it is more efficient, since my way only finds the minimum once instead of twice. Compare this approach with mine above:\n\n(z - min(z)) / (max(z) - min(z))\n\n[1] 0.00 1.00 0.25\n\n\nMore complicated with all the brackets, and two min(z). Admittedly the difference here will be thousandths of a second, but why call a function twice when you don’t have to?\n\\(\\blacksquare\\)\n\nWrite a function called rescale that implements the calculation above, for any input vector called x. (Note that I changed the name.)\n\nSolution\nWrite a function skeleton:\n\nrescale &lt;- function(x) {\n\n}\n\nand inside the curly brackets put the code from above, replacing z with x everywhere:\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\n(x - lo) / (hi - lo)\n}\n\nYou’ll need to make sure your function returns something to the outside world. Either don’t save the last line in s (as I did here), or save it in s and then return s:\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\ns &lt;- (x - lo) / (hi - lo)\ns\n}\n\nor use return() if you must, but be aware that this is bad style in R (unlike Python, where you need it). The approved way of using return in R is when you are returning something earlier than the last line of a function, for example, you are testing a simple case first and returning the value that goes with that, before getting down to the serious computation.\nExtra: in the spirit of what’s coming up below, you might check first whether the maximum and minimum are the same and return something else if that’s the case:\n\nrescale0 &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nif (lo == hi) return(0)\ns &lt;- (x - lo) / (hi - lo)\ns\n}\n\nThis is good style; in this case, if lo and hi are the same, we want to return something else (zero) to the outside world, and then we do the calculation, knowing that lo and hi are different, so that we are sure we are not dividing by zero (but I get ahead of myself).\nDoing it this way, there is something to be careful of: a function ought to return a predictable type of thing: numbers, in this case. If you have your function return text on error, like this:\n\nrescale0 &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nif (lo == hi) return(\"High and low need to be different\")\ns &lt;- (x - lo) / (hi - lo)\ns\n}\n\nthen you can get into trouble:\n\nrescale0(z)\n\n[1] 0.00 1.00 0.25\n\nrescale0(c(3,3,3))\n\n[1] \"High and low need to be different\"\n\n\nThe first one is numbers and the second one is text.\n\\(\\blacksquare\\)\n\nTest your function on my z, and on another vector of your choosing. Explain briefly why the answer you get from your vector makes sense.\n\nSolution\nOn my z:\n\nrescale(z)\n\n[1] 0.00 1.00 0.25\n\n\nThe same values as I got before, so that works.\nFor your vector, use whatever you like. I think it makes sense to have the values already in order, to make it easier to check. Here’s one possibility:\n\nw &lt;- 2:6\nw\n\n[1] 2 3 4 5 6\n\n\nand then\n\nrescale(w)\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\n\nThe smallest value is 2, which goes to zero; the largest is 6, which goes to 1, and the others are equally spaced between in both the input and the output.\nAnother possibility is to use a vector with values whose largest and smallest you can clearly see:\n\nw &lt;- c(10, 11, 100, 0, 20)\nrescale(w)\n\n[1] 0.10 0.11 1.00 0.00 0.20\n\n\nClearly the smallest value is 0 and the largest is 100. These become 0 and 1, and these particular values make it easy to see what happened: each of the other values got divided by 100.\nSome discussion is needed here, in that you need to say something convincing about why your answer is right.\nExtra: This is why I had you use a name other than z for the input to your function. The function can be used on any input, not just the z that we tested it on. There’s another R-specific reason, which is that you need to be careful about using the named inputs only. Consider this function:\n\nff &lt;- function(x) {\nx + z\n}\n\nff(10)\n\n[1] 20 24 21\n\n\nWhere did z come from? R used the z we had before, which is rather dangerous: what if we had a z lying around from some completely different work? Much better to have a function work with only inputs in the top line:\n\nff &lt;- function(x, z) {\nx + z\n}\nff(10, 3)\n\n[1] 13\n\nff(10)\n\nError in ff(10): argument \"z\" is missing, with no default\n\n\nThe first time, the two inputs are added together, but the second time it tells you it was expecting a value to use for z and didn’t see one. Much safer.\n\\(\\blacksquare\\)\n\nWhat happens if your input to rescale is a vector of numbers all the same? Give an example. Rewrite your function to intercept this case and give a helpful error message.\n\nSolution\nFirst, try it and see. Any collection of values all the same will do:\n\nrescale(c(3,3,3))\n\n[1] NaN NaN NaN\n\n\nNaN stands for “not a number”. The way we got it is that the minimum and maximum were the same, so our function ended up dividing by zero (in fact, working out zero divided by zero). This is, in R terms, not even an error, but the answer is certainly not helpful.\nThe easiest way to check inputs is to use stopifnot to express what should be true if the function is to proceed. Here, we want the maximum and minimum to be different, so:\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(hi != lo)\n(x - lo) / (hi - lo)\n}\nrescale(c(3,3,3))\n\nError in rescale(c(3, 3, 3)): hi != lo is not TRUE\n\n\nThis is much clearer: I only have to recall what my hi and lo are to see what the problem is.\nExtra 1: by calculating and saving the min and max up front, I still only need to calculate them once. If you do it this way:\n\nrescale &lt;- function(x) {\nstopifnot(max(x) != min(x))\n(x - min(x)) / (max(x) - min(x))\n}\nrescale(c(3,3,3))\n\nError in rescale(c(3, 3, 3)): max(x) != min(x) is not TRUE\n\n\nyou get a slightly more informative error message, but you have calculated the max twice and the min three times for no reason.\nExtra 2: stopifnot is shorthand for this:\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nif (hi == lo) stop(\"min and max are the same!\")\n(x - lo) / (hi - lo)\n}\nrescale(c(3,3,3))\n\nError in rescale(c(3, 3, 3)): min and max are the same!\n\n\nI didn’t show you this, so if you use stop, you must tell me where you found out about it. This is better than returning some text (see rescale0 above) or printing a message: it’s an error, so you want to make it look like an error. I am very sympathetic to being persuaded that this is better than stopifnot, because you can customize the message (and, also, you don’t have to go through the double-negative contortions of stopifnot). Another way to use stopifnot and get a customized message is this one (that I only learned about right when you were writing this Assignment):\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(\"high and low must be different\" = (hi != lo))\n(x - lo) / (hi - lo)\n}\nrescale(c(3,3,3))\n\nError in rescale(c(3, 3, 3)): high and low must be different\n\n\nThis is called a “named argument”, and the name, if given, is used as an error message.\nExtra 3: returning to my rescale0 from above:\n\nrescale0\n\nfunction(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nif (lo == hi) return(\"High and low need to be different\")\ns &lt;- (x - lo) / (hi - lo)\ns\n}\n&lt;bytecode: 0x559b8818a710&gt;\n\n\nthis can get you into trouble if you use it in a dataframe. This is a bit complicated, since it has to use list-columns. Here we go:\n\ntibble(x = list(z, c(3,3,3)))\n\n\n\n  \n\n\n\nJust to check that this does contain what you think it does:\n\ntibble(x = list(z, c(3,3,3))) %&gt;% unnest(x)\n\n\n\n  \n\n\n\nSo now, for each of those two input vectors, what happens when we run rescale0 on them? This is rowwise:\n\ntibble(x = list(z, c(3,3,3))) %&gt;% \n  rowwise() %&gt;% \n  mutate(ans = list(rescale0(x)))\n\n\n\n  \n\n\n\nThe first ans is a vector of 3 numbers, and the second one is one piece of text (the “error message”). I was actually surprised it got this far. So what happens when we unnest the second column?\n\ntibble(x = list(z, c(3,3,3))) %&gt;% \n  rowwise() %&gt;% \n  mutate(ans = list(rescale0(x))) %&gt;% \n  unnest(ans)\n\nError in `list_unchop()`:\n! Can't combine `x[[1]]` &lt;double&gt; and `x[[2]]` &lt;character&gt;.\n\n\nNow we get a confusing error: it’s here that combining some numbers and some text in one column of a dataframe doesn’t work. To forestall this, we need to go back and rewrite rescale0 to not mix things up. Having it return an error, as the latest version of rescale does, gives an error here too, but at least we know what it means:\n\ntibble(x = list(z, c(3,3,3))) %&gt;% \n  rowwise() %&gt;% \n  mutate(ans = list(rescale(x))) %&gt;% \n  unnest(ans)\n\nError in `mutate()`:\nℹ In argument: `ans = list(rescale(x))`.\nℹ In row 2.\nCaused by error in `rescale()`:\n! high and low must be different\n\n\nbecause this is the error we anticipated: it says “somewhere within the list-column x, specifically in its second row, is a vector where everything is the same”.\n\\(\\blacksquare\\)\n\nMake a dataframe (containing any numeric values), and in it create a new column containing the rescaled version of one of its columns, using your function. Show your result.\n\nSolution\nThis is less difficult than you might be expecting: make a dataframe with at least one numeric column, and use mutate:\n\nd &lt;- tibble(y=2:6)\nd\n\n\n\n  \n\n\n\nand then\n\nd %&gt;% mutate(s=rescale(y))\n\n\n\n  \n\n\n\nYou can supply the values for what I called y, or use random numbers. It’s easier for you to check that it has worked if your column playing the role of my y has not too many values in it.\nExtra: this is actually already in the tidyverse under the name percent_rank (“percentile ranks”):\n\nd %&gt;% mutate(s = percent_rank(y))\n\n\n\n  \n\n\n\nThe value 5, for example, is at the 75th percentile.\n\\(\\blacksquare\\)\n\nWe might want to rescale the input not to be between 0 and 1, but between two values a and b that we specify as input. If a and/or b are not given, we want to use the values 0 for a and 1 for b. Rewrite your function to rescale the input to be between a and b instead of 0 and 1. Hint: allow your function to produce values between 0 and 1 as before, and then note that if all the values in a vector s are between 0 and 1, then all the values in a+(b-a)*s are between \\(a\\) and \\(b\\).\n\nSolution\nI’m showing you my thought process in this one. The answer I want from you is the one at the end.\nSo, start by copying and pasting what you had before:\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(hi != lo)\n(x - lo) / (hi - lo)\n}\n\nOn the top line, add the extra inputs and their default values. I also changed the name of my function, for reasons you’ll see later:\n\nrescale2 &lt;- function(x, a=0, b=1) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(hi != lo)\n(x - lo) / (hi - lo)\n}\n\nSave the last line, since we have to do something else with it:\n\nrescale2 &lt;- function(x, a=0, b=1) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(hi != lo)\ns &lt;- (x - lo) / (hi - lo)\n}\n\nand finally add the calculation in the hint, which we don’t need to save because we are returning it:\n\nrescale2 &lt;- function(x, a=0, b=1) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(hi != lo)\ns &lt;- (x - lo) / (hi - lo)\na + (b-a) * s\n}\n\nThis complete function is what I want to see from you. (You should keep the stopifnot, because this function will have the exact same problem as the previous one if all the values in x are the same.)\nA better way is to observe that you can call functions inside functions. The function above is now a bit messy since it has several steps. Something that corresponds better to my hint is to call the original rescale first, and then modify its result:\n\nrescale3 &lt;- function(x, a=0, b=1) {\ns &lt;- rescale(x)\na + (b-a) * s\n}\n\nThe logic to this is rather clearly “rescale the input to be between 0 and 1, then rescale that to be between \\(a\\) and \\(b\\).” My rescale2 does exactly the same thing, but it’s much less clear that it does so, unless you happen to have in your head how rescale works. (I think you are more likely to remember, sometime in the future, what rescale does, compared to precisely how it works.)\nThat is why rescale3 is better than rescale2. Remember that you can, and generally should, use functions that have already been written (by you or someone else) as part of functions that do more complex things. See also my second point below.\nExtra: there are two important principles about why functions are important:\n\nthey allow you to re-do a calculation on many different inputs (the point I’ve been making up to now)\nby abstracting a calculation into a thing with a name, it makes it easier to understand that calculation’s role in something bigger. The only thing we had to remember in rescale3 is what the last line did, because the name of the function called on the first line tells us what happens there. This is much easier than remembering what the first four lines of rescale2 do.\n\nThe second principle here is what psychologists call “chunking”: you view a thing like my function rescale as a single item, rather than as four separate lines of code, and then that single item can be part of something larger (like my rescale3), and you have a smaller number of things to keep track of.\n\\(\\blacksquare\\)\n\nTest your new function two or more times, on input where you know or can guess what the output is going to be. In each case, explain briefly why your output makes sense.\n\nSolution\nI’ll start by using the default values for a and b (so I don’t have to specify them):\n\nrescale2(2:6)\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\nrescale3(2:6)\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\n\nI did both of the variants of my function; of course, you’ll only have one variant.\nWe got the same answer as before for the same input, so the default values \\(a=0, b=1\\) look as if they have been used.\nLet’s try a different one:\n\nv &lt;- c(7, 11, 12)\nrescale2(v, 10, 30)\n\n[1] 10 26 30\n\nrescale3(v, 10, 30)\n\n[1] 10 26 30\n\n\nThe lowest value in v has become 10, and the highest has become 30. (Also, the in-between value 11 was closer to 12 than to 7, and it has become something closer to 30 than to 10.)\nExtra: you can also name any of your inputs:\n\nrescale3(x=v, a=10, b=30)\n\n[1] 10 26 30\n\n\nand if you name them, you can shuffle the order too:\n\nrescale3(a=10, b=30, x=v)\n\n[1] 10 26 30\n\n\nThe point of doing more than one test is to check that different aspects of your function all work. Therefore, the best testing here checks that the defaults work, and that the answer is sensible for some different a and b (to check that this works as well).\nWhen you write your version of rescale with the optional inputs, it’s best if you do it so that the things you have to supply (the vector of numbers) is first. If you put a and b first, when you want to omit them, you’ll have to call the input vector by name, like this:\n\nrescale3(x=v)\n\n[1] 0.0 0.8 1.0\n\n\nbecause otherwise the input vector will be taken to be a, not what you want.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "functions.html#footnotes",
    "href": "functions.html#footnotes",
    "title": "20  Functions",
    "section": "",
    "text": "Hotpo is short for half or triple-plus-one.↩︎\nR style is to use the last line of the function for the return value, unless you are jumping out of the function before the end, in which case use return.↩︎\nHotpo is short for half or triple-plus-one.↩︎\nNobody knows whether you always get to 1, but also nobody has ever found a case where you don’t. Collatz’s conjecture, that you will get to 1 eventually, is known to be true for all starting \\(x_1\\) up to some absurdly large number, but not for all starting points.↩︎\nI should have been more careful in my functions to make sure everything was integers, and, in particular, to do integer division by 2 because I knew that this division was going to come out even.↩︎\nThis is something I’ve done in the past and been bitten by, so I am trying to get myself not to do it any more.↩︎\nIn computing with decimal numbers, things are almost never exactly zero or exactly infinite; they are very small or very big. The mean here, which you would calculate to be zero, is less than the so-called machine epsilon, which is about 10 to the minus 16 in R (R works in double precision). A mean that small is, to the computer, indistinguishable from zero. It came out that way because the last value is added to a total that is by that point negative, and so you have a loss of accuracy because of subtracting nearly equal quantities. I learned all about this stuff in my first real computer science course, which I think was a numerical math course, some absurd number of years ago. It looks as if this gets taught in CSCC37 these days.↩︎"
  },
  {
    "objectID": "vector-matrix.html#heights-and-foot-lengths-again",
    "href": "vector-matrix.html#heights-and-foot-lengths-again",
    "title": "21  Vector and matrix algebra",
    "section": "21.1 Heights and foot lengths again",
    "text": "21.1 Heights and foot lengths again\nEarlier, we investigated some data on predicting the height of a person from the length of their foot. The data were in http://ritsokiguess.site/datafiles/heightfoot.csv.\n\nRead in and display (some of) the data.\nIn your regression course, you learned (or will learn) the matrix formulation of the least squares estimates of intercept and slope. This produces a vector \\(\\hat\\beta\\) containing estimates of both the intercept and the slope, from the formula\n\n\\[ \\hat\\beta = (X^T X)^{-1} X^T y, \\]\nwhere:\n\n\\(X\\) is a matrix containing a column of 1s followed by all the columns of explanatory variables\n\\(X^T\\) denotes the (matrix) transpose of \\(X\\)\n\\(M^{-1}\\) denotes the inverse of the matrix \\(M\\)\n\\(y\\) denotes the column of response variable values.\n\nUse the formula above to obtain the least squares estimates of intercepts and slope for this regression, using R’s vector-matrix algebra. Hint: you are advised to do the calculation in steps, or else it will be very hard to read, and hard for the grader to check that it is correct.\n\nVerify that your calculation is correct by running the regression.\n\nMy solutions follow:"
  },
  {
    "objectID": "vector-matrix.html#heights-and-foot-lengths-again-1",
    "href": "vector-matrix.html#heights-and-foot-lengths-again-1",
    "title": "21  Vector and matrix algebra",
    "section": "21.2 Heights and foot lengths again",
    "text": "21.2 Heights and foot lengths again\nEarlier, we investigated some data on predicting the height of a person from the length of their foot. The data were in http://ritsokiguess.site/datafiles/heightfoot.csv.\n\nRead in and display (some of) the data.\n\nSolution\nCopy what you did before:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/heightfoot.csv\"\nhf &lt;- read_csv(my_url)\n\nRows: 33 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): height, foot\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhf\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nIn your regression course, you learned (or will learn) the matrix formulation of the least squares estimates of intercept and slope. This produces a vector \\(\\hat\\beta\\) containing estimates of both the intercept and the slope, from the formula\n\n\\[ \\hat\\beta = (X^T X)^{-1} X^T y, \\]\nwhere:\n\n\\(X\\) is a matrix containing a column of 1s followed by all the columns of explanatory variables\n\\(X^T\\) denotes the (matrix) transpose of \\(X\\)\n\\(M^{-1}\\) denotes the inverse of the matrix \\(M\\)\n\\(y\\) denotes the column of response variable values.\n\nUse the formula above to obtain the least squares estimates of intercepts and slope for this regression, using R’s vector-matrix algebra. Hint: you are advised to do the calculation in steps, or else it will be very hard to read, and hard for the grader to check that it is correct.\nSolution\nThere is some setup first: we have to get hold of \\(X\\) and \\(y\\) from the data as a matrix and a vector respectively. I would use tidyverse ideas to do this, and then turn them into a matrix at the end, which I think is best. Don’t forget to create a column of 1s to make the first column of \\(X\\)\n\nhf %&gt;% mutate(one=1) %&gt;% \nselect(one, foot) %&gt;% \nas.matrix() -&gt; X\nhead(X)\n\n     one foot\n[1,]   1 27.0\n[2,]   1 29.0\n[3,]   1 25.5\n[4,]   1 27.9\n[5,]   1 27.0\n[6,]   1 26.0\n\n\n(head displays the first six rows, or else you’ll be displaying all 33, which is too many.)\nAnother approach is this:\n\nX &lt;- cbind(1, hf$foot)\nhead(X)\n\n     [,1] [,2]\n[1,]    1 27.0\n[2,]    1 29.0\n[3,]    1 25.5\n[4,]    1 27.9\n[5,]    1 27.0\n[6,]    1 26.0\n\n\nNote that the recycling rules mean that a column with only one value in it will be repeated to the length of the other one, and so this is better than working out how many observations there are and repeating 1 that many times.\nThe choice here is whether to use tidyverse stuff and turn into a matrix at the end, or make a matrix at the start (which is what cbind from base R is doing). I don’t believe you’ve seen that in this course, so you ought to cite your source if you go that way.\nThe simplest choice for making \\(y\\) is this:\n\ny &lt;- hf$height\ny\n\n [1] 66.5 73.5 70.0 71.0 73.0 71.0 71.0 69.5 73.0 71.0 69.0 69.0 73.0 75.0 73.0\n[16] 72.0 69.0 68.0 72.5 78.0 79.0 71.0 74.0 66.0 71.0 71.0 71.0 84.0 77.0 72.0\n[31] 70.0 76.0 68.0\n\n\nThis also works:\n\nhf %&gt;% select(height) %&gt;% pull(height)\n\n [1] 66.5 73.5 70.0 71.0 73.0 71.0 71.0 69.5 73.0 71.0 69.0 69.0 73.0 75.0 73.0\n[16] 72.0 69.0 68.0 72.5 78.0 79.0 71.0 74.0 66.0 71.0 71.0 71.0 84.0 77.0 72.0\n[31] 70.0 76.0 68.0\n\n\n(remembering that you don’t want to have anything that’s a dataframe), or this:\n\nhf %&gt;% select(height) %&gt;% as.matrix() -&gt; yy\nhead(yy)\n\n     height\n[1,]   66.5\n[2,]   73.5\n[3,]   70.0\n[4,]   71.0\n[5,]   73.0\n[6,]   71.0\n\n\nremembering that a (column) vector and a 1-column matrix are the same thing as R is concerned.\nNow we want to construct some things. I would go at it this way, rather than trying to do everything at once (if you do, you will either get lost now, or in six months when you try to figure out what you did):\n\nXt &lt;- t(X) # X-transpose\nXtX &lt;- Xt %*% X \nXtXi &lt;- solve(XtX)\nXty &lt;- Xt %*% y\nXtXi %*% Xty\n\n          [,1]\n[1,] 34.336335\n[2,]  1.359062\n\n\nThe intercept is 34.33 and the slope is 1.36.\nThese compute, respectively, \\(X^T\\), \\(X^T X\\), the inverse of that, \\(X^T y\\) and \\(\\hat\\beta\\). Expect credit for laying out your calculation clearly.\nExtra: the value of this formula is that it applies no matter whether you have one \\(x\\)-variable, as here (or in the windmill data), or whether you have a lot (as in the asphalt data). In either case, \\(\\hat\\beta\\) contains the estimate of the intercept followed by all the slope estimates, however many there are. There are also matrix formulas that tell you how the slopes or the residuals will change if you remove one observation or one explanatory variable, so that something like step will work very efficiently, and calculations for leverages likewise.\n\\(\\blacksquare\\)\n\nVerify that your calculation is correct by running the regression.\n\nSolution\nThe usual lm:\n\nhf.1 &lt;- lm(height ~ foot, data = hf)\nsummary(hf.1)\n\n\nCall:\nlm(formula = height ~ foot, data = hf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7491 -1.3901 -0.0310  0.8918 12.9690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  34.3363     9.9541   3.449 0.001640 ** \nfoot          1.3591     0.3581   3.795 0.000643 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.102 on 31 degrees of freedom\nMultiple R-squared:  0.3173,    Adjusted R-squared:  0.2952 \nF-statistic: 14.41 on 1 and 31 DF,  p-value: 0.0006428\n\n\nThe same.\nExtra: in this “well-conditioned” case,1 it makes no difference, but if \\(X^T X\\) is almost singular, so that it almost doesn’t have an inverse (for example, some of your explanatory variables are highly correlated with each other), you can get into trouble. Regression calculations in practice use something more sophisticated like the singular value decomposition of \\(X^TX\\) to diagnose whether \\(X^TX\\) is actually singular or almost so, which from a numerical point of view is almost as bad, and to produce a sensible answer in that case.\nI guess I should try to make up one where it struggles. Let me do one with two \\(x\\)’s that are strongly correlated:\n\nd &lt;- tribble(\n~x1, ~x2, ~y,\n10, 20, 55,\n11, 19.0001, 60,\n12, 17.9999, 61,\n13, 17.0001, 64,\n14, 15.9998, 66,\n15, 15.0001, 67\n)\nd\n\n\n\n  \n\n\n\nx2 is almost exactly equal to 30 minus x1. What’s the right answer?\n\nd.1 &lt;- lm(y ~ x1 + x2, data = d)\nsummary(d.1)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = d)\n\nResiduals:\n       1        2        3        4        5        6 \n-1.37530  1.26859  0.03118  0.63549  0.43765 -0.99760 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -11837.3   138685.6  -0.085    0.937\nx1             398.0     4622.9   0.086    0.937\nx2             395.7     4622.8   0.086    0.937\n\nResidual standard error: 1.303 on 3 degrees of freedom\nMultiple R-squared:  0.9485,    Adjusted R-squared:  0.9141 \nF-statistic: 27.61 on 2 and 3 DF,  p-value: 0.0117\n\ncoef(d.1)\n\n(Intercept)          x1          x2 \n-11837.2938    398.0000    395.6835 \n\n\nYou should be right away suspicious here: the R-squared is high, but neither of the explanatory variables are significant! (This actually means that you can remove one of them, either one.) The standard errors are also suspiciously large, never a good sign. If you’ve done C67, you might be thinking about variance inflation factors here:\n\nlibrary(car)\nvif(d.1)\n\n       x1        x2 \n220326271 220326271 \n\n\nThese are both huge (greater than 5 or 10 or whatever guideline you use), indicating that they are highly correlated with each other (as we know they are).\nAll right, how does the matrix algebra work? This is just the same as before:\n\nd %&gt;% mutate(one=1) %&gt;% \nselect(one, starts_with(\"x\")) %&gt;% \nas.matrix() -&gt; X\nhead(X)\n\n     one x1      x2\n[1,]   1 10 20.0000\n[2,]   1 11 19.0001\n[3,]   1 12 17.9999\n[4,]   1 13 17.0001\n[5,]   1 14 15.9998\n[6,]   1 15 15.0001\n\n\nand then\n\ny &lt;- d$y\nXt &lt;- t(X) # X-transpose\nXtX &lt;- Xt %*% X \nXtXi &lt;- solve(XtX)\nXty &lt;- Xt %*% y\nXtXi %*% Xty\n\n           [,1]\none -11837.1777\nx1     397.9962\nx2     395.6796\n\n\nThese answers are actually noticeably different from the right answers (with a few more decimals here):\n\ncoef(d.1)\n\n(Intercept)          x1          x2 \n-11837.2938    398.0000    395.6835 \n\n\nOne way of finding out how nearly singular \\(X^TX\\) is is to look at its eigenvalues. You’ll recall that a singular matrix has one or more zero eigenvalues:\n\neigen(XtX)\n\neigen() decomposition\n$values\n[1] 2.781956e+03 3.404456e+01 8.805965e-11\n\n$vectors\n            [,1]        [,2]        [,3]\n[1,] -0.04643281 -0.00782885  0.99889074\n[2,] -0.57893109 -0.81469635 -0.03329647\n[3,] -0.81405331  0.57983495 -0.03329628\n\n\nThe third eigenvalue is \\(8.8 \\times 10^{-11}\\), which is very close to zero, especially compared to the other two, which are 34 and over two thousand. This is a very nearly singular matrix, and hence \\((X^TX)^{-1}\\) is very close to not existing at all, and that would mean that you couldn’t even compute the intercept and slope estimates, never mind hope to get close to the right answer.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "vector-matrix.html#footnotes",
    "href": "vector-matrix.html#footnotes",
    "title": "21  Vector and matrix algebra",
    "section": "",
    "text": "Which in this case means that the column of 1s and the actual x values are not strongly correlated, which means that the x-values vary enough.↩︎"
  },
  {
    "objectID": "bootstrap.html#air-conditioning-failures",
    "href": "bootstrap.html#air-conditioning-failures",
    "title": "22  The Bootstrap",
    "section": "22.1 Air conditioning failures",
    "text": "22.1 Air conditioning failures\nBack in 1963, there was a report on failures in air-conditioning equipment in aircraft. For one aircraft, the air-conditioning equipment failed 12 times, and the number of hours it ran before failing each time was recorded. The data are in link. Boeing was interested in the mean failure time, because the company wanted to plan for engineers to fix the failures (and thus needed to estimate a failure rate).\nThere is randomization here. Your answers will differ slightly from mine, unless you throw in this before you start (or at least before you generate your first random numbers).\n\nset.seed(457299)\n\n\nRead in the data, and observe that you have the correct number of rows. (Note that the failure times are in ascending order).\nWhat do you notice about the shape of the distribution of failure times? Explain briefly.\nObtain the means of 1000 bootstrap samples (that is, samples from the data with replacement). Save them.\nMake a normal quantile plot of your bootstrap distribution. What do you see? Explain briefly.\nObtain the 95% bootstrap percentile confidence interval for the mean.\nObtain the 95% bootstrap-\\(t\\) confidence interval for the mean, and compare your two intervals.\nObtain the BCa 95% confidence interval for the mean.\nCompare the BCa confidence interval with the other ones. Which one would you recommend? Explain briefly."
  },
  {
    "objectID": "bootstrap.html#air-conditioning-failures-bootstrapping-the-median",
    "href": "bootstrap.html#air-conditioning-failures-bootstrapping-the-median",
    "title": "22  The Bootstrap",
    "section": "22.2 Air conditioning failures: bootstrapping the median",
    "text": "22.2 Air conditioning failures: bootstrapping the median\nWith a skewed data distribution such as the air-conditioning failure times, we might be interested in inference for the median. One way to get a confidence interval for the median is to invert the sign test, as in smmr, but another way is to obtain a bootstrap sampling distribution for the median. How do these approaches compare for the air-conditioning data? We explore this here.\n\nRead in the air-conditioning data again (if you don’t already have it lying around). The link is in the previous question.\nUse smmr to get a confidence interval for the median (based on the sign test).\nObtain the bootstrap distribution of the sample median. Make a normal quantile plot of it. What do you notice? Explain briefly.\nObtain a 95% bootstrap percentile confidence interval for the median. How does it compare with the one you obtained earlier?\nObtain a 95% BCa interval. Compare it with the two other intervals you found."
  },
  {
    "objectID": "bootstrap.html#comparing-eyesight",
    "href": "bootstrap.html#comparing-eyesight",
    "title": "22  The Bootstrap",
    "section": "22.3 Comparing eyesight",
    "text": "22.3 Comparing eyesight\nDo people see on average better with their left eye or their right eye, or is there no difference? To find out, 15 subjects were shown a sequence of images, some to their left eye and some to their right (with a blindfold on the other eye). The subjects were asked to identify some objects in each image, and were given an overall score for each eye, based on their ability to identify objects with each eye. (A higher score is better.) Data in http://ritsokiguess.site/datafiles/eyesight.csv.\n\nRead in and display (some of) the data.\nExplain briefly why looking at differences (say right minus left) makes sense for these data, and calculate and save a dataframe with the differences added to it.\nMake a suitable normal quantile plot, and describe what it tells you.\nObtain a bootstrap distribution of the sample median.\nMake a histogram of your bootstrap distribution of the median. Use a lot of bins, such as the default 30, for this. What do you notice about the distribution? Why did it come out this way?\nFind a 95% percentile interval for the population median.1\nFind the BCA 95% confidence interval for the population median difference.\nWhat do your intervals tell us about any possible difference between left eye and right eye in terms of ability to identify objects in images? Do the intervals agree or disagree about this?"
  },
  {
    "objectID": "bootstrap.html#bootstrapping-the-irs-data",
    "href": "bootstrap.html#bootstrapping-the-irs-data",
    "title": "22  The Bootstrap",
    "section": "22.4 Bootstrapping the IRS data",
    "text": "22.4 Bootstrapping the IRS data\nYou might recall the IRS data from when we were learning about the sign test. The idea was that we wanted to see how long “on average” it took people to fill out a tax form. The data are in http://ritsokiguess.site/datafiles/irs.txt.\n\nRead in and display (some of) the data. There is only one column of data, so you can pretend the values are separated by anything.\nObtain a bootstrap distribution of the sample median.\nMake a suitable graph of the bootstrap distribution of the median. What seems odd about it? Why did that happen? (Hint: use more bins on your plot than usual, like 50.)\nFind 95% percentile and bootstrap-\\(t\\) intervals for the population median. (Hint: your dataframe of bootstrapped medians may still be rowwise, so you might need to run ungroup first.)\n\nMy solutions follow:"
  },
  {
    "objectID": "bootstrap.html#air-conditioning-failures-1",
    "href": "bootstrap.html#air-conditioning-failures-1",
    "title": "22  The Bootstrap",
    "section": "22.5 Air conditioning failures",
    "text": "22.5 Air conditioning failures\nBack in 1963, there was a report on failures in air-conditioning equipment in aircraft. For one aircraft, the air-conditioning equipment failed 12 times, and the number of hours it ran before failing each time was recorded. The data are in link. Boeing was interested in the mean failure time, because the company wanted to plan for engineers to fix the failures (and thus needed to estimate a failure rate).\nThere is randomization here. Your answers will differ slightly from mine, unless you throw in this before you start (or at least before you generate your first random numbers).\n\nset.seed(457299)\n\n\nRead in the data, and observe that you have the correct number of rows. (Note that the failure times are in ascending order).\n\nSolution\nThis is a .csv so read_csv is the thing: ::: {.cell}\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/pasias/master/air_conditioning.csv\"\naircon &lt;- read_csv(my_url)\n\nRows: 12 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): failure, hours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\naircon\n\n\n\n  \n\n\n:::\nTwelve rows (12 failure times).\n\\(\\blacksquare\\)\n\nWhat do you notice about the shape of the distribution of failure times? Explain briefly.\n\nSolution\nMake a suitable graph. The obvious one is a histogram:\n\nggplot(aircon, aes(x = hours)) + geom_histogram(bins = 7)\n\n\n\n\nYou’ll have to play with the number of bins (there are only 12 observations). I got 7 from the Freedman-Diaconis rule:\n\nnclass.FD(aircon$hours)\n\n[1] 7\n\n\nI was a little suspicious that the data would not be much like normal (I have run into failure times before), so I kept away from the Sturges rule.\nAnother possibility is a one-group boxplot:\n\nggplot(aircon, aes(y = hours, x = 1)) + geom_boxplot()\n\n\n\n\nIf you like, you can do a normal quantile plot. I rank that third here, because there is nothing immediately implying a comparison with the normal distribution, but I would accept it:\n\nggplot(aircon, aes(sample = hours)) + stat_qq() + stat_qq_line()\n\n\n\n\nPick a visual and defend it.\nAll three of these graphs are showing a strong skewness to the right.\nExtra: this is probably not a surprise, because a time until failure cannot be less than zero, and distributions with a limit tend to be skewed away from that limit. (If you look back at the data, there are some very small failure times, but there are also some very big ones. The very small ones are saying that the lower limit matters.) If you were modelling these times until failure, you might use a distribution like the exponential or gamma or Weibull.\n\\(\\blacksquare\\)\n\nObtain the means of 1000 bootstrap samples (that is, samples from the data with replacement). Save them.\n\nSolution\nSomething like this, therefore:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(sample(aircon$hours, replace = TRUE))) %&gt;% \n  mutate(sample_mean = mean(sample)) -&gt; means\nmeans\n\n\n\n  \n\n\n\nForgetting the rowwise will cause all sorts of trouble.\n\\(\\blacksquare\\)\n\nMake a normal quantile plot of your bootstrap distribution. What do you see? Explain briefly.\n\nSolution\nThis:\n\nggplot(means, aes(sample = sample_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis is still skewed to the right (it has a curved shape, or, the low values and the high values are both too high compared to the normal).\nExtra: this is less skewed than the original data was, because, with a sample size of 12, we have a little help from the Central Limit Theorem, but not much. This picture is the one that has to be normal enough for \\(t\\) procedures to work, and it is not. This comes back into the picture when we compare our confidence intervals later.\nAlso, it makes sense to see how normal a sampling distribution of a mean is, so a normal quantile plot would be my first choice for this.\n\\(\\blacksquare\\)\n\nObtain the 95% bootstrap percentile confidence interval for the mean.\n\nSolution\nThis is the 2.5 and 97.5 percentiles of the bootstrapped sampling distribution of the mean:\n\nquantile(means$sample_mean, c(0.025, 0.975))\n\n     2.5%     97.5% \n 47.05625 187.93333 \n\n\n\\(\\blacksquare\\)\n\nObtain the 95% bootstrap-\\(t\\) confidence interval for the mean, and compare your two intervals.\n\nSolution\nThe key is to remember that the original sample (and thus each bootstrap sample) had \\(n=12\\), so there are \\(12-1=11\\) df. (The fact that there were 1000 bootstrap samples is neither here nor there). This is how I like to do it:\n\nt_star &lt;- qt(0.975, 11)\nt_star\n\n[1] 2.200985\n\nmean(means$sample_mean) + c(-1, 1) * t_star * sd(means$sample_mean)\n\n[1]  25.33401 186.81249\n\n\nThe c(-1, 1) thing is the calculation version of the \\(\\pm\\), and gets both limits at once. Pull the above apart to see how it works. If you don’t like that, you might prefer something like this:\n\nthe_mean &lt;- mean(means$sample_mean)\nthe_sd &lt;- sd(means$sample_mean)\nmargin &lt;- t_star * the_sd\nthe_mean - margin\n\n[1] 25.33401\n\nthe_mean + margin\n\n[1] 186.8125\n\n\nI apologize for the crazy first line of that! As for comparison: the bootstrap-\\(t\\) interval goes down a lot further, though the upper limits are quite similar (on this scale). Both intervals are very long and don’t tell us much about the population mean time to failure, which is not very surprising given the small sample size (\\(n=12\\)) and the large variability in the data.\nExtra: the non-normality of the bootstrap (sampling) distribution says that we should definitely not trust the bootstrap-\\(t\\), and probably not the bootstrap percentile interval either. Which brings us to the next part.\n\\(\\blacksquare\\)\n\nObtain the BCa 95% confidence interval for the mean.\n\nSolution\nThis means (possibly) installing and (certainly) loading the bootstrap package, and then:\n\ntheta &lt;- function(x) {\n  mean(x)\n}\nbca_all &lt;- with(aircon, bcanon(hours, 1000, theta))\nbca &lt;- bca_all$confpoints\nbca\n\n     alpha bca point\n[1,] 0.025  55.58333\n[2,] 0.050  61.25000\n[3,] 0.100  70.66667\n[4,] 0.160  78.50000\n[5,] 0.840 160.66667\n[6,] 0.900 178.50000\n[7,] 0.950 204.25000\n[8,] 0.975 228.75000\n\n\nPull out the ones from this that you need: the top one and the bottom one, to get an interval of 55.6 to 228.8.\nI seem to need to define the function theta first and pass it into bcanon as the third input. You may have more luck with bcanon(hours, 1000, mean) than I did. Try it.\nOr, if you feel like some extra coding: turn this matrix into a data frame, grab the rows you want, and then the column you want:\n\nbca %&gt;%\n  as_tibble() %&gt;%\n  filter(alpha %in% c(0.025, 0.975)) %&gt;%\n  pull(`bca point`)\n\n[1]  55.58333 228.75000\n\n\n\\(\\blacksquare\\)\n\nCompare the BCa confidence interval with the other ones. Which one would you recommend? Explain briefly.\n\nSolution\nIn this example, the bootstrap-\\(t\\) and percentile intervals are very different, so we should use neither of them, and prefer the BCa interval.\nExtra: as usual in this kind of case, the BCa contains values for the mean pulled out into the long tail, but that’s a proper adjustment for the sampling distribution being skewed.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "bootstrap.html#air-conditioning-failures-bootstrapping-the-median-1",
    "href": "bootstrap.html#air-conditioning-failures-bootstrapping-the-median-1",
    "title": "22  The Bootstrap",
    "section": "22.6 Air conditioning failures: bootstrapping the median",
    "text": "22.6 Air conditioning failures: bootstrapping the median\nWith a skewed data distribution such as the air-conditioning failure times, we might be interested in inference for the median. One way to get a confidence interval for the median is to invert the sign test, as in smmr, but another way is to obtain a bootstrap sampling distribution for the median. How do these approaches compare for the air-conditioning data? We explore this here.\n\nRead in the air-conditioning data again (if you don’t already have it lying around). The link is in the previous question.\n\nSolution\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/pasias/master/air_conditioning.csv\"\naircon &lt;- read_csv(my_url)\n\nRows: 12 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): failure, hours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\naircon\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nUse smmr to get a confidence interval for the median (based on the sign test).\n\nSolution\nInput to ci_median is data frame and column: ::: {.cell}\nci_median(aircon, hours)\n\n[1]   7.002319 129.998291\n\n:::\n\\(\\blacksquare\\)\n\nObtain the bootstrap distribution of the sample median. Make a normal quantile plot of it. What do you notice? Explain briefly.\n\nSolution\nThe usual do-it-yourself bootstrap:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(samples = list(sample(aircon$hours, replace = TRUE))) %&gt;% \n  mutate(medians = median(samples)) -&gt; meds\n\nI actually copied and pasted my code from the previous problem, changing mean to median.\nAs for a plot, well, this:\n\nggplot(meds, aes(sample = medians)) + stat_qq() + stat_qq_line()\n\n\n\n\nNot only does this not look very normal, but also there are those curious horizontal patches of points (that, you might recall, are characteristic of a discrete distribution). This has happened because there are only a few possible medians: the median has to be either a data value or halfway between two data values, so there are only something like \\(2(12)-1=23\\) different possible medians, with the ones in the middle being more likely.\nThis also shows up on a histogram, but only if you have enough bins. (If you don’t have enough bins, some of the neighbouring possible values end up in the same bin; here, the aim is to have enough bins to show the discreteness, rather than the usual thing of having few enough bins to show the shape.)\n\nggplot(meds, aes(x = medians)) + geom_histogram(bins = 30)\n\n\n\n\n\\(\\blacksquare\\)\n\nObtain a 95% bootstrap percentile confidence interval for the median. How does it compare with the one you obtained earlier?\n\nSolution\nAlso, the usual:\n\nquantile(meds$medians, c(0.025, 0.975))\n\n 2.5% 97.5% \n 12.5 115.0 \n\n\nThis goes down and up not quite so far as the interval from smmr. That might be because the smmr interval is too wide (based on a not-very-powerful test), or because the bootstrap quantile interval is too narrow (as it usually is). It’s hard to tell which it is.\n\\(\\blacksquare\\)\n\nObtain a 95% BCa interval. Compare it with the two other intervals you found.\n\nSolution\nYet more copying and pasting (from the previous question): ::: {.cell}\ntheta &lt;- function(x) {\n  median(x)\n}\nbca_all &lt;- with(aircon, bcanon(hours, 1000, theta))\nbca &lt;- bca_all$confpoints\nbca\n\n     alpha bca point\n[1,] 0.025      12.5\n[2,] 0.050      12.5\n[3,] 0.100      18.0\n[4,] 0.160      30.5\n[5,] 0.840      94.5\n[6,] 0.900      98.0\n[7,] 0.950     100.0\n[8,] 0.975     115.0\n\n:::\nAgain, I seem to need to define the tiny function, while you can probably call bcanon(hours, 1000, median). Try it and see.\nMy BCa interval is a little longer than the bootstrap percentile interval and a little shorter than the one that came from the sign test. I would guess that the BCa interval is the most trustworthy of the three, though there is here not that much difference between them. All the intervals are again very long, a reflection of the small sample size and large variability.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "bootstrap.html#comparing-eyesight-1",
    "href": "bootstrap.html#comparing-eyesight-1",
    "title": "22  The Bootstrap",
    "section": "22.7 Comparing eyesight",
    "text": "22.7 Comparing eyesight\nDo people see on average better with their left eye or their right eye, or is there no difference? To find out, 15 subjects were shown a sequence of images, some to their left eye and some to their right (with a blindfold on the other eye). The subjects were asked to identify some objects in each image, and were given an overall score for each eye, based on their ability to identify objects with each eye. (A higher score is better.) Data in http://ritsokiguess.site/datafiles/eyesight.csv.\n\nRead in and display (some of) the data.\n\nSolution\nThis is a csv, so no surprises:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/eyesight.csv\"\nsight &lt;- read_csv(my_url)\n\nRows: 15 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): person, right, left\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsight\n\n\n\n  \n\n\n\n15 observations, with the subjects labelled by number, and a score for each subject and each eye.\n\\(\\blacksquare\\)\n\nExplain briefly why looking at differences (say right minus left) makes sense for these data, and calculate and save a dataframe with the differences added to it.\n\nSolution\nThis is matched pairs data, with two observations for each subject. A matched pairs analysis, whether by a sign test or a matched-pairs \\(t\\)-test, would be based on one difference for each subject, and so those would make sense to calculate. (You’ll recall that a matched pairs analysis uses the differences and not the original data.)\nThus, saving back into our original dataframe:\n\nsight %&gt;% \nmutate(difference = right - left) -&gt; sight\nsight\n\n\n\n  \n\n\n\nExtra: this is one of those cases where having long data would make it very much more difficult to work out the differences for each person. Try it and see. How will you match up the two measurements for each person?\n\\(\\blacksquare\\)\n\nMake a suitable normal quantile plot, and describe what it tells you.\n\nSolution\nA normal quantile plot of the differences, therefore, since normality of the two individual scores is immaterial:\n\nggplot(sight, aes(sample = difference)) + stat_qq() +\nstat_qq_line()\n\n\n\n\nWe have what I think is best described as “long tails”, with the high values being too high and the low ones being a bit too low for a normal distribution. I think this is a better description than “outliers” because outliers are isolated unusual values, not five observations out of fifteen!\nThe plot is telling us that a matched-pairs \\(t\\)-test is questionable, and that we might do a sign test instead. Or, as we explore in this question, find a bootstrap distribution (in this case, for the median).\nExtra: the one kind of sensible plot that uses the original data in this situation would be a scatterplot, since the right and left scores are matched up:\n\nggplot(sight, aes(x = right, y = left)) + \ngeom_point() + geom_abline(slope = 1, intercept = 0)\n\n\n\n\nI added the line \\(y = x\\) to the plot. The value of doing that is that a point to the right and below the line has the right-eye score bigger than the left-eye one, and vice versa for a point to the left and above. This plot tells us that a small majority of the subjects had a higher score with the right eye, and for the ones that had a higher score with the left eye, the difference wasn’t usually very big.\nThis plot tells us nothing about normality of differences, though (not without some careful looking), which is one of the things we usually care about.\n\\(\\blacksquare\\)\n\nObtain a bootstrap distribution of the sample median.\n\nSolution\nBorrow the idea from lecture, replacing mean with median:\n\ntibble(sim = 1:1000) %&gt;% \nrowwise() %&gt;% \nmutate(sample = list(sample(sight$difference, replace = TRUE))) %&gt;% \nmutate(my_median = median(sample)) -&gt; meds\nmeds\n\n\n\n  \n\n\n\nThe steps are:\n\ncreate a dataframe with a column called sim to label the simulations\nfrom here on out, work “rowwise”, that is, with one row at a time\ngenerate a bootstrap sample for each row. A bootstrap sample is fifteen observations rather than just one, so we are making a list-column and thus the list has to go on the front\nwork out the median of each bootstrap sample. Remember, the rowwise applies until you cancel it,2 and so this will be the median of the bootstrap sample on each row, one at a time.\n\nAs ever, if you want to see what’s going on, run this one line at a time.\n\\(\\blacksquare\\)\n\nMake a histogram of your bootstrap distribution of the median. Use a lot of bins, such as the default 30, for this. What do you notice about the distribution? Why did it come out this way?\n\nSolution\nFor this histogram, there is no need to specify a number of bins (unless you want to):\n\nggplot(meds, aes(x = my_median)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe distribution is very discrete (this shows up more clearly with more bins).\nThe data values are all integers (and therefore so are the differences). The median of an odd number of data values must be one of the data values, and the bootstrap samples only contain (varying numbers of copies of) the differences in the original dataset, so each bootstrap sample must have a median that is an integer too.\nExtra: in case you are thinking that this happened because the data values were integers, no, it would happen even if the data were decimal numbers. Let’s make some fake data of 15 random normals and then do the same thing again:\n\nfake_data &lt;- tibble(x = rnorm(15))\nfake_data\n\n\n\n  \n\n\n\nand once again bootstrap the median:\n\ntibble(sim = 1:1000) %&gt;% \nrowwise() %&gt;% \nmutate(sample = list(sample(fake_data$x, replace = TRUE))) %&gt;% \nmutate(my_median = median(sample)) -&gt; meds2\nmeds2\n\n\n\n  \n\n\n\nYou can see even from these few that the bootstrap distribution of the median has repeats, so there should also be some discreteness here:\n\nggplot(meds2, aes(x = my_median)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe discreteness is a property of the fact that we were bootstrapping the median, and the median has to be one of the data values.\nTo confirm that, recall that our original data were integers:\n\nsight\n\n\n\n  \n\n\n\nbut even for these, if you bootstrap the mean, you don’t get the same discreteness:\n\ntibble(sim = 1:1000) %&gt;% \nrowwise() %&gt;% \nmutate(sample = list(sample(sight$difference, replace = TRUE))) %&gt;% \nmutate(my_mean = mean(sample)) -&gt; means\nmeans %&gt;% \nggplot(aes(x = my_mean)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis is too many bins for 1000 bootstrap samples, so the shape is kind of irregular, but there are not the big gaps that the bootstrap distribution of the sample median has. Indeed, this ought to be somewhere near normal and is:\n\nggplot(means, aes(sample = my_mean)) + stat_qq() +\nstat_qq_line()\n\n\n\n\n(This is saying that the Central Limit Theorem is really helping, even for a sample size of only 15 from clearly non-normal data, so the paired \\(t\\) may not be as bad as we would have thought.)\n\\(\\blacksquare\\)\n\nFind a 95% percentile interval for the population median.3\n\nSolution\nThe percentile interval comes from the middle 95% of the bootstrap distribution of medians:\n\nquantile(meds$my_median, c(0.025, 0.975))\n\n 2.5% 97.5% \n   -2     4 \n\n\nThe bootstrap percentile interval goes from \\(-2\\) to 4. Like the CI for the median based on the sign test, the ends of this interval must be data values.\nExtra: for comparison, the interval from the sign test is this:\n\nci_median(sight, difference)\n\n[1] -1.997070  4.994629\n\n\nwhich is, when rounded off, from \\(-2\\) to 5, very like the percentile interval.\n\\(\\blacksquare\\)\n\nFind the BCA 95% confidence interval for the population median difference.\n\nSolution\nLoad (and if necessary install) the bootstrap package, and then:\n\nbca &lt;- bcanon(sight$difference, 1000, median)\nbca$confpoints\n\n     alpha bca point\n[1,] 0.025        -2\n[2,] 0.050        -2\n[3,] 0.100         0\n[4,] 0.160         0\n[5,] 0.840         3\n[6,] 0.900         3\n[7,] 0.950         4\n[8,] 0.975         4\n\n\n\\(-2\\) to 4, in this case like the percentile interval.4 Note how this one is data values also.\n\\(\\blacksquare\\)\n\nWhat do your intervals tell us about any possible difference between left eye and right eye in terms of ability to identify objects in images? Do the intervals agree or disagree about this?\n\nSolution\nThe intervals are not quite all the same, but one thing they have in common is that they all have a negative lower limit and a positive upper one (more positive than the negative one is negative). This says that 0 is a plausible difference in each case, and thus it is reasonable to conclude that there is no evidence of any difference between the two eyes, based on this sample of 15 subjects.\nThe intervals do all go more positive than negative, which says that if anything the scores are better with the right eye than the left on average (from the way around that we took the differences). However, there is no evidence here that this is any more than chance.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "bootstrap.html#bootstrapping-the-irs-data-1",
    "href": "bootstrap.html#bootstrapping-the-irs-data-1",
    "title": "22  The Bootstrap",
    "section": "22.8 Bootstrapping the IRS data",
    "text": "22.8 Bootstrapping the IRS data\nYou might recall the IRS data from when we were learning about the sign test. The idea was that we wanted to see how long “on average” it took people to fill out a tax form. The data are in http://ritsokiguess.site/datafiles/irs.txt.\n\nRead in and display (some of) the data. There is only one column of data, so you can pretend the values are separated by anything.\n\nSolution\nPretty much any of the read_ functions will work, even this one:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/irs.txt\"\nirs &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Time = col_double()\n)\n\nirs\n\n\n\n  \n\n\n\nOne column called Time.\n\\(\\blacksquare\\)\n\nObtain a bootstrap distribution of the sample median.\n\nSolution\nThe lecture notes use the exact same dataset, so you can borrow ideas from there:\nSet up a dataframe with one row for each bootstrap sample you’re going to draw, 1000 in this case:\n\ntibble(sim = 1:1000)\n\n\n\n  \n\n\n\nCreate a column with a new bootstrap sample for each sim. This means doing rowwise first and then wrapping the sampling in list because you are creating a list-column of samples:\n\ntibble(sim = 1:1000) %&gt;% \nrowwise() %&gt;% \nmutate(sample = list(sample(irs$Time, replace = TRUE)))\n\n\n\n  \n\n\n\nEach thing in sample has 30 observations in it (one bootstrap sample). If you want, you can unnest to take a look at the values; they should be the ones in the dataset, possibly with extra repeats.\nNext, work out the median of each bootstrapped sample, which is simple because we are still working rowwise:\n\ntibble(sim = 1:1000) %&gt;% \nrowwise() %&gt;% \nmutate(sample = list(sample(irs$Time, replace = TRUE))) %&gt;% \nmutate(my_median = median(sample)) -&gt; bs\nbs\n\n\n\n  \n\n\n\nAs you realize, bs stands for “bootstrap”. Of course.\n\\(\\blacksquare\\)\n\nMake a suitable graph of the bootstrap distribution of the median. What seems odd about it? Why did that happen? (Hint: use more bins on your plot than usual, like 50.)\n\nSolution\nThe medians are already in a dataframe, so go straight ahead:\n\nggplot(bs, aes(x = my_median)) + geom_histogram(bins = 50)\n\n\n\n\nWhat we are seeing at this resolution is that the distribution is very irregular, with funny holes in it, more than you would expect even with this many bins. By way of comparison, the bootstrap distribution of the mean looks a lot smoother:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(irs$Time, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 50)\n\n\n\n\nThis is somewhat irregular, because we really have too many bins, but there are not nearly so many holes and irregular heights as on the plot for the median. I had you use a lot of bins in this special case because I wanted you to see just how irregular the bootstrapped distribution for the median really was.\nSo why did that happen? Think about what the sample median is for 30 observations: it is the mean of the 15th and 16th smallest values when you arrange them in order. A bootstrap sample must contain the same values as the original dataset (just probably not the same frequencies of them). So the median of a bootstrap sample must be the average of two of the values in the original dataset, and probably two that were close together. What that means is that there are not very many possible medians of the bootstrap samples, and they form a clearly discrete rather than a continuous distribution. (The sample mean, on the other hand, uses all the values in the bootstrap sample, and so there are a lot more possible bootstrap means than bootstrap medians; the distribution of those is as good as continuous.)\nWhat this means is that bootstrapping for medians is odd (it always looks like this), but that’s what the bootstrap distribution looks like.\n\\(\\blacksquare\\)\n\nFind 95% percentile and bootstrap-\\(t\\) intervals for the population median. (Hint: your dataframe of bootstrapped medians may still be rowwise, so you might need to run ungroup first.)\n\nSolution\nThe percentile interval comes from the middle 95% of the bootstrap distribution of medians. The dataframe bs is still rowwise, so we have to undo that first to do it the obvious way:\n\nbs %&gt;% \n  ungroup() %&gt;% \n  summarize(ci1 = quantile(my_median, 0.025),\n            ci2 = quantile(my_median, 0.975))\n\n\n\n  \n\n\n\nOr, pull out just that column and find the two quantiles of that, for which there are two ways, the base R way:\n\nquantile(bs$my_median, c(0.025, 0.975))\n\n 2.5% 97.5% \n  121   215 \n\n\nand the slightly odd-looking:\n\nbs %&gt;% pull(my_median) %&gt;% \n  quantile(c(0.025, 0.975))\n\n 2.5% 97.5% \n  121   215 \n\n\nAll of these get you to the same place. There is even one more:\n\nbs %&gt;% \n  ungroup() %&gt;% \n  reframe(ci = quantile(my_median, c(0.025, 0.975)))\n\n\n\n  \n\n\n\nThis is reframe rather than summarize because quantile in this case returns two numbers, the two percentiles we want, and summarize expects only one. (This is newish behaviour.) Another way is to use summarize, but wrap the call to quantile in list so that it returns only one thing (the list, containing two numbers, but bundled up in one list). Then you need to unnest it to see the values:\n\nbs %&gt;% \n  ungroup() %&gt;% \n  summarize(ci = list(quantile(my_median, c(0.025, 0.975)))) %&gt;% \n  unnest(ci)  \n\n\n\n  \n\n\n\nIf you had rowwise in the back of your mind, you might have tried this. Try it up to but not including the unnest to see how it works.\nFor the bootstrap \\(t\\), estimate the population median as the sample median:\n\nmed &lt;- median(irs$Time)\nmed\n\n[1] 172.5\n\n\nget its standard error from the SD of the bootstrap distribution of medians:\n\nse &lt;- sd(bs$my_median)\nse\n\n[1] 23.31839\n\n\nthen go up and down twice this (or 1.96 if you believe in \\(z\\)):\n\nmed + c(-2, 2)*se\n\n[1] 125.8632 219.1368\n\n\nExtra: in this case, we also have the CI for the median that came out of the sign test:\n\nlibrary(smmr)\nci_median(irs, Time)\n\n[1] 119.0065 214.9955\n\n\nThis one is actually very close to the bootstrap percentile interval, while the bootstrap \\(t\\) interval is higher at both ends.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "bootstrap.html#footnotes",
    "href": "bootstrap.html#footnotes",
    "title": "22  The Bootstrap",
    "section": "",
    "text": "I was also going to have you do a bootstrap-t interval, but I’m not completely convinced I got that right when I was explaining it to you before.↩︎\nThat is done using ungroup, should you ever need to stop working rowwise. This seems like an odd choice of function, since the usual use of ungroup is to undo a group-by, but what ungroup actually does is to remove any special properties a dataframe has, including both groups and any rowwise behaviour.↩︎\nI was also going to have you do a bootstrap-t interval, but I’m not completely convinced I got that right when I was explaining it to you before.↩︎\nThey don’t often agree this well, but all of these intervals in this situation but have data values at their endpoints, and all of our data values are integers.↩︎"
  },
  {
    "objectID": "stan.html#estimating-proportion-in-favour-from-a-survey",
    "href": "stan.html#estimating-proportion-in-favour-from-a-survey",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.1 Estimating proportion in favour from a survey",
    "text": "23.1 Estimating proportion in favour from a survey\nYou are probably familiar with the kind of surveys where you are given a statement, like “I am the kind of person that finishes a task they start”, and you have to express your agreement or disagreement with it. Usually, you are given a five-point or seven-point scale on which you express your level of agreement (from “strongly agree” through “neither agree nor disagree” to “strongly disagree”, for example). Here, we will simplify things a little and only allow respondents to agree or disagree. So the kind of data you would have is a number of people that took part, and the number of these that said “agree”.\nCommon assumptions that are made in this kind of analysis are: (i) the responses are independent of each other, and (ii) each respondent has the same unknown probability of agreeing. You might quibble about (ii), but the assumption we are making here is that we know nothing about the respondents apart from whether they agreed or disagreed. (In practice, we’d collect all kinds of demographic information about each respondent, and this might give us a clue about how they’ll respond, but here we’re keeping it simple.) Under our assumptions, the number of respondents that agree has a binomial distribution with \\(n\\) being our sample size, and \\(p\\) being the probability we are trying to estimate. Let’s estimate \\(p\\) using Stan: that is to say, let’s obtain the posterior distribution of \\(p\\).\n\nIn R Studio, open a new Stan file (with File, New File, Stan File). You’ll see a template file of Stan code. Edit the model section to reflect that you have observed a number of successes x that we are modelling to have a binomial distribution with number of trials n and success probability p.\nIn the line of Stan code you wrote, there should be three variables. Which of these are parameters and which are data? Explain briefly.\nI hope you found that there is only one parameter, p, in this problem. We know that \\(0 \\le p \\le 1\\), and we need a prior distribution for it. A common choice is a beta distribution. Look at the Stan manual, link. The density function is given in 19.1.1. It has two parameters \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\). \\(B(\\alpha, \\beta)\\) given there is a constant. Add to your model section to express that p has a prior distribution with parameters alpha and beta. (alpha and beta will be input data when we run this code.)\nAbove your model section, complete a parameters section that says what kind of variable p is. If p has upper or lower limits, put these in as well. You can edit the parameters section that is in the template.\nEverything else is data. Complete a data section (edit the one in the template) to say what type of thing everything else is, including limits if it has any. Don’t forget the parameters in the prior distribution!\nSave your code, if you haven’t already. I used the filename binomial.stan. In your Stan code window, at the top right, you’ll see a button marked Check. This checks whether your code is syntactically correct. Click it.\nCompile your model. (This may take a minute or so, depending on how fast your R Studio is.) When the spinny thing stops spinning, it’s done.\nIn most surveys, the probability to be estimated is fairly close to 0.5. A beta prior with \\(\\alpha=\\beta=2\\) expresses the idea that any value of p is possible, but values near 0.5 are more likely.\n\nA survey of 277 randomly selected adult female shoppers was taken. 69 of them agreed that when an advertised item is not available at the local supermarket, they request a raincheck.\nUsing the above information, set up a data list suitable for input to a run of stan.\n\nSample from the posterior distribution of p with these data, and display your results.\n\n\nObtain a 90% posterior interval for the probability that a randomly chosen adult female shopper will request a raincheck.\nObtain a 95% (frequentist) confidence interval for p, and compare the results. (Hint: prop.test.) Comment briefly.\n(optional) This is one of those problems where you can obtain the answer analytically. What is the posterior distribution of \\(p\\), using a prior \\(beta(\\alpha, \\beta)\\) distribution for \\(p\\) and observing \\(x\\) successes out of \\(n\\) trials?"
  },
  {
    "objectID": "stan.html#bayesian-regression",
    "href": "stan.html#bayesian-regression",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.2 Bayesian regression",
    "text": "23.2 Bayesian regression\nIn this question, we will develop Stan code to run a simple linear regression, and later apply it to some data (and do a bit of elicitation of prior distributions along the way).\n\nCreate a .stan file that will run a simple linear regression predicting a variable y from a variable x, estimating an intercept a and a slope b. Use normal prior distributions for a and b, and allow the means and SDs of the prior distributions for a and b to be specified (as data, later). The regression model says that the response y has a normal distribution with mean a+bx and SD sigma which is also estimated. Give this a prior chi-squared distribution with a prior mean that is also input.\nCheck your Stan code for syntactic correctness, and when it is correct, compile it.\nWe are going to be analyzing some data on vocabulary size (the number of words known) by children of different ages. It is suspected that the relationship between age and vocabulary size is approximately linear. You go consult with an early childhood expert, and they tell you this:\n\n\nIn children of age up to about six, vocabulary almost always increases by between 300 and 700 words per year.\nI can’t talk about vocabulary of children of age 0, because children don’t start learning to talk until age about 18 months (1.5 years).\nChildren of age 1.5 years almost always have a vocabulary between 0 and 500 words (depending on exactly what age they started talking.)\nEven if we know a child’s age, our prediction of their vocabulary size might be off by as much as 200 words.\n\nUse this information to obtain parameters for your prior distributions.\n\nSome data were collected on age and vocabulary size of 10 randomly selected children, shown here: link. Read in and display the data; the values are separated by single spaces.\nUse this dataset, along with your prior distribution from above, to obtain posterior distributions for intercept, slope and error SD. What is the 95% posterior interval for the slope?\nPlot a histogram of the posterior distribution of the slope. Does its shape surprise you? Explain briefly.\nWhat can we say about the vocabulary size of a randomly selected child of age 5 (a new one, not the one in the original data set)? Use an appropriate predictive distribution."
  },
  {
    "objectID": "stan.html#estimating-p-the-bayesian-way",
    "href": "stan.html#estimating-p-the-bayesian-way",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.3 Estimating \\(p\\) the Bayesian way",
    "text": "23.3 Estimating \\(p\\) the Bayesian way\nA binomial experiment with 8 trials produces the following results: success, failure, success, success, failure, success, success, success. (Each result is therefore a Bernoulli trial.) The person who gave you the data says that the success probability is most likely somewhere near 0.5, but might be near 0 or 1. The aim of this question is to estimate the success probability using Bayesian methods.\nIn this question, use cmdstanr (see this site for instructions). Documentation for Stan is here. You will probably want to be running R on your own computer.\n\nWrite a Stan program that will estimate the success probability \\(p\\). To do this, start with the likelihood (Stan has a function bernoulli that takes one parameter, the success probability). The data, as 1s and 0s, will be in a vector x. Use a beta distribution with unknown parameters as a prior for p. (We will worry later what those parameters should be.)\nCompile your code, correcting any errors until it compiles properly.\nThe person who brought you the data told you that the success probability p should be somewhere near 0.5 (and is less likely to be close to 0 or 1). Use this information to pick a prior distribution for p. (The exact answer you get doesn’t really matter, but try to interpret the statement in some kind of sensible way.)\nCreate an R list that contains all your data for your Stan model. Remember that Stan expects the data in x to be 0s and 1s.\nRun your Stan model to obtain a simulated posterior distribution, using all the other defaults.\nMake a plot of the posterior distribution of the probability of success. (Use the posterior and bayesplot packages if convenient.)\nThe posterior predictive distribution is rather odd here: the only possible values that can be observed are 0 and 1. Nonetheless, obtain the posterior predictive distribution for these data, and explain briefly why it is not surprising that it came out as it did.\n\nMy solutions follow:"
  },
  {
    "objectID": "stan.html#estimating-proportion-in-favour-from-a-survey-1",
    "href": "stan.html#estimating-proportion-in-favour-from-a-survey-1",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.4 Estimating proportion in favour from a survey",
    "text": "23.4 Estimating proportion in favour from a survey\nYou are probably familiar with the kind of surveys where you are given a statement, like “I am the kind of person that finishes a task they start”, and you have to express your agreement or disagreement with it. Usually, you are given a five-point or seven-point scale on which you express your level of agreement (from “strongly agree” through “neither agree nor disagree” to “strongly disagree”, for example). Here, we will simplify things a little and only allow respondents to agree or disagree. So the kind of data you would have is a number of people that took part, and the number of these that said “agree”.\nCommon assumptions that are made in this kind of analysis are: (i) the responses are independent of each other, and (ii) each respondent has the same unknown probability of agreeing. You might quibble about (ii), but the assumption we are making here is that we know nothing about the respondents apart from whether they agreed or disagreed. (In practice, we’d collect all kinds of demographic information about each respondent, and this might give us a clue about how they’ll respond, but here we’re keeping it simple.) Under our assumptions, the number of respondents that agree has a binomial distribution with \\(n\\) being our sample size, and \\(p\\) being the probability we are trying to estimate. Let’s estimate \\(p\\) using Stan: that is to say, let’s obtain the posterior distribution of \\(p\\).\n\nIn R Studio, open a new Stan file (with File, New File, Stan File). You’ll see a template file of Stan code. Edit the model section to reflect that you have observed a number of successes x that we are modelling to have a binomial distribution with number of trials n and success probability p.\n\nSolution\nThis is quicker to do than to ask for. Make a guess at this:\n\nmodel {\n  // likelihood\n  x ~ binomial(n, p);\n}\nand then check the manual link, looking for Sampling Statement, to make sure that this is what is expected. It is. (I got to this page by googling “Stan binomial distribution”.)\nThe “likelihood” line with the two slashes is a comment, C++ style. It is optional, but I like to have it to keep things straight.\n\\(\\blacksquare\\)\n\nIn the line of Stan code you wrote, there should be three variables. Which of these are parameters and which are data? Explain briefly.\n\nSolution\nThe way to think about this is to ask yourself which of x, n, and p are being given to the Stan code as data, and which you are trying to estimate. The only thing we are estimating here is p, so that is a parameter. The number of trials n and the number of successes x are data that you will observe (treated as “given” or “fixed” in the Bayesian framework).\n\\(\\blacksquare\\)\n\nI hope you found that there is only one parameter, p, in this problem. We know that \\(0 \\le p \\le 1\\), and we need a prior distribution for it. A common choice is a beta distribution. Look at the Stan manual, link. The density function is given in 19.1.1. It has two parameters \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\). \\(B(\\alpha, \\beta)\\) given there is a constant. Add to your model section to express that p has a prior distribution with parameters alpha and beta. (alpha and beta will be input data when we run this code.)\n\nSolution\nYour model section should now look like this:\n\nmodel {\n  // prior\n  p ~ beta(alpha, beta);\n  // likelihood\n  x ~ binomial(n, p);\n}\n\n\\(\\blacksquare\\)\n\nAbove your model section, complete a parameters section that says what kind of variable p is. If p has upper or lower limits, put these in as well. You can edit the parameters section that is in the template.\n\nSolution\np is a real variable taking values between 0 and 1, so this:\n\nparameters {\n  real&lt;lower=0, upper=1&gt; p;\n}\n\n\\(\\blacksquare\\)\n\nEverything else is data. Complete a data section (edit the one in the template) to say what type of thing everything else is, including limits if it has any. Don’t forget the parameters in the prior distribution!\n\nSolution\nWe said before that n and x were (genuine) data. These are positive integers; also x cannot be bigger than n (why not?). In the data section also go the parameters alpha and beta of the prior distribution. These are real numbers bigger than zero. These two together give us this:\n\ndata {\n  int&lt;lower=0&gt; n;\n  int&lt;lower=0, upper=n&gt; x;\n  real&lt;lower=0&gt; alpha;\n  real&lt;lower=0&gt; beta;\n}\n\nPutting in lower and upper limits, if you have them, will help because if you happen to enter data that does not respect the limits, you’ll get an error right there, and you won’t waste time sampling.\nIt is more important to put in limits in the parameters section, because that is telling the sampler not to go there (eg. a value of \\(p\\) outside \\([0,1]\\)).\n\\(\\blacksquare\\)\n\nSave your code, if you haven’t already. I used the filename binomial.stan. In your Stan code window, at the top right, you’ll see a button marked Check. This checks whether your code is syntactically correct. Click it.\n\nSolution\nThis appeared in my console:\n\n&gt; rstan:::rstudio_stanc(\"binomial.stan\")\nbinomial.stan is syntactically correct.\n\nIf you don’t see this, there is some kind of code error. You’ll then see some output that points you to a line of your code. The error is either there or at the end of the previous line (eg. you forgot a semicolon). Here is a typical one:\n\n&gt; rstan:::rstudio_stanc(\"binomial.stan\")\nSYNTAX ERROR, MESSAGE(S) FROM PARSER:\nerror in 'model377242ac03ef_binomial' at line 24, column 0\n-------------------------------------------------\n22: parameters {\n23:   real&lt;lower=0, upper=1&gt; p\n24: }\n^\n25: \n-------------------------------------------------\n\nPARSER EXPECTED: \";\"\nError in stanc(filename, allow_undefined = TRUE) : \nfailed to parse Stan model 'binomial' due to the above error.\n\nThe compiler (or at least the code checker) was expecting a semicolon, and when it got to the close-curly-bracket on line 24, that was where it knew that the semicolon was missing (and thus it objected there and not earlier). The above was on my own computer. When I tried it on rstudio.cloud, I thought I had everything correct but I got an error message like this:\n\nError in sink(type = \"output\") : invalid connection\n\nthat I couldn’t get rid of. This might happen to you also. If you get an error, fix it and check again. Repeat until your code is “syntactically correct”. (That means that it will compile, but not that it will necessarily do what you want.) This process is an integral part of coding, so get used to it. It doesn’t matter how many errors you make; what matters is that you find and correct them all.\n\\(\\blacksquare\\)\n\nCompile your model. (This may take a minute or so, depending on how fast your R Studio is.) When the spinny thing stops spinning, it’s done.\n\nSolution\nGo down to the console and type something like ::: {.cell}\nbinomial &lt;- cmdstan_model(\"binomial.stan\")\n:::\nIf it doesn’t work, make sure you installed and loaded cmdstanr first, with install.packages and library respectively.\nIf it sits there and does nothing for a while, this is actually a good sign. If it finds an error, it will tell you. If you get your command prompt &gt; back without it saying anything, that means it worked. (This is a Unix thing: no comment means no error.)\nIf you happen to compile it a second time, without changing anything in the Stan code, it won’t make you wait while it compiles again: it will say “Model executable is up to date!”.\n\\(\\blacksquare\\)\n\nIn most surveys, the probability to be estimated is fairly close to 0.5. A beta prior with \\(\\alpha=\\beta=2\\) expresses the idea that any value of p is possible, but values near 0.5 are more likely.\n\nA survey of 277 randomly selected adult female shoppers was taken. 69 of them agreed that when an advertised item is not available at the local supermarket, they request a raincheck.\nUsing the above information, set up a data list suitable for input to a run of stan.\nSolution\nLook in your data section, and see what you need to provide values for. The order doesn’t matter; make a list with the named pieces and their values, in some order. You need values for these four things: ::: {.cell}\nbinomial_data &lt;- list(n = 277, x = 69, alpha = 2, beta = 2)\n:::\nExtra: in case you are wondering where the parameters for the prior came from: in this case, I looked on the Wikipedia page for the beta distribution and saw that \\(\\alpha=\\beta=2\\) is a good shape, so I used that. In practice, getting a reasonable prior is a more difficult problem, called “elicitation”. What you have to do is ask a subject matter expert what they think p might be, giving you a range of values such as a guessed-at 95% confidence interval, like “I think p is almost certainly between 0.1 and 0.6”. Then you as a statistician have to choose values for alpha and beta that match this, probably by trial and error. The beta distribution is part of R, so this is doable, for example like this:\n\ncrossing(alpha = 1:10, beta = 1:10) %&gt;%\n  mutate(\n    lower = qbeta(0.025, alpha, beta),\n    upper = qbeta(0.975, alpha, beta)\n  ) %&gt;%\n  mutate(sse = (lower - 0.1)^2 + (upper - 0.6)^2) %&gt;%\n  arrange(sse)\n\n\n\n  \n\n\n\nThis says that \\(\\alpha=4, \\beta=8\\) is a pretty good choice.1\nMy process:\n\nPick some values of alpha and beta to try, and make all possible combinations of them.\nFind the 2.5 and 97.5 percentiles of the beta distribution for each of those values. The “inverse CDF” (the value \\(x\\) that has this much of the probability below it) is what we want here; this is obtained in R by putting q in front of the name of the distribution. For example, does this make sense to you? ::: {.cell}\n\nqnorm(0.025)\n\n[1] -1.959964\n\n:::\n\nWe want the lower limit to be close to 0.1 and the upper limit to be close to 0.6. Working out the sum of squared errors for each alpha-beta combo is a way to do this; if sse is small, that combination of alpha and beta gave lower and upper limits close to 0.1 and 0.6.\nArrange the sse values smallest to largest. The top rows are the best choices of alpha and beta.\n\n\\(\\blacksquare\\)\n\nSample from the posterior distribution of p with these data, and display your results.\n\nSolution\nThis is what I got: ::: {.cell}\nbinomial_fit &lt;- binomial$sample(binomial_data)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.\n\nbinomial_fit\n\n variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n     lp__ -159.34 -159.06 0.72 0.30 -160.84 -158.84 1.00     2020     2370\n     p       0.25    0.25 0.03 0.03    0.21    0.29 1.00     1388     1978\n\n:::\nYour results should be similar, though probably not identical, to mine. (There is a lot of randomness involved here.)\n\\(\\blacksquare\\)\n\nObtain a 90% posterior interval for the probability that a randomly chosen adult female shopper will request a raincheck.\n\nSolution\nRead off the q5 and q95 values for p. Mine are 0.21 and 0.29.\n\\(\\blacksquare\\)\n\nObtain a 95% (frequentist) confidence interval for p, and compare the results. (Hint: prop.test.) Comment briefly.\n\nSolution\nIf you remember this well enough, you can do it by hand, but there’s no need: ::: {.cell}\nprop.test(69, 277)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  69 out of 277, null probability 0.5\nX-squared = 68.751, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.2001721 0.3051278\nsample estimates:\n        p \n0.2490975 \n\n:::\nMy 95% intervals are very close.\nNumerically, this is because the only (material) difference between them is the presence of the prior in the Bayesian approach. We have quite a lot of data, though, so the choice of prior is actually not that important (“the data overwhelm the prior”). I could have used alpha=8, beta=4 that I obtained in the Extra above, and it wouldn’t have made any noticeable difference.\nConceptually, though, the interpretations of these intervals are very different: the Bayesian posterior interval really does say “the probability of \\(p\\) being between 0.20 and 0.31 is 0.95”, while for the confidence interval you have to talk about repeated sampling: “the procedure producing the 95% confidence interval will contain the true value of \\(p\\) in 95% of all possible samples”. This might seem clunky in comparison; a Bayesian would tell you that the interpretation of the posterior interval is what you want the interpretation of the confidence interval to be!\n\\(\\blacksquare\\)\n\n(optional) This is one of those problems where you can obtain the answer analytically. What is the posterior distribution of \\(p\\), using a prior \\(beta(\\alpha, \\beta)\\) distribution for \\(p\\) and observing \\(x\\) successes out of \\(n\\) trials?\n\nSolution\nWith this stuff, you can throw away any constants.\nThe likelihood is (proportional to) \\[ p^x (1-p)^{n-x}.\\] There is a binomial coefficient that I threw away.\nLook up the form of the beta density if you don’t know it (or look above): the prior for \\(p\\) is proportional to\n\\[ p^{\\alpha-1} (1-p)^{\\beta-1}.\\]\nPosterior is proportional to likelihood times prior:\n\\[ p^{x + \\alpha - 1} (1-p)^{n-x +\\beta - 1}\\]\nwhich is recognized as a beta distribution with parameters \\(x+\\alpha\\), \\(n-x+\\beta\\). Typically (unless you are very sure about \\(p\\) a priori (that is, before collecting any data)), \\(x\\) and \\(n-x\\) will be much larger than \\(\\alpha\\) and \\(\\beta\\), so this will look a lot like a binomial likelihood, which is why the confidence interval and posterior interval in our example came out very similar. I leave it to you to decide which you prefer: algebra and intelligence (and luck, often), or writing code to sample from the posterior. I know what I prefer!\nExtra: one of the people behind Stan is on Twitter with handle @betanalpha.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "stan.html#bayesian-regression-1",
    "href": "stan.html#bayesian-regression-1",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.5 Bayesian regression",
    "text": "23.5 Bayesian regression\nIn this question, we will develop Stan code to run a simple linear regression, and later apply it to some data (and do a bit of elicitation of prior distributions along the way).\n\nCreate a .stan file that will run a simple linear regression predicting a variable y from a variable x, estimating an intercept a and a slope b. Use normal prior distributions for a and b, and allow the means and SDs of the prior distributions for a and b to be specified (as data, later). The regression model says that the response y has a normal distribution with mean a+bx and SD sigma which is also estimated. Give this a prior chi-squared distribution with a prior mean that is also input.\n\nSolution\nThis is a lot. Breathe. Pause. Then, in R Studio, File, New File and Stan File. Leave the template there, and change what you need as you go. I would start with the model part. The likelihood part says that y has a normal distribution with mean a+bx and SD sigma, thus:\n\n// likelihood\ny ~ normal(a+b*x, sigma);\n\nThere is a subtlety here that I’ll get to later, but this is the easiest way to begin. Next, take a look at what’s here. x and y are data, and the other things, a, b, sigma are parameters. These last three need prior distributions. I said to use normal distributions for the first two, and a chi-squared distribution for the last one. (In practice, of course, you get to choose these, in consultation with the subject matter expert, but these are likely to be pretty reasonable.) I’ve given the parameters of these prior distributions longish names, so I hope I’m trading more typing for less confusion:\n\nmodel {\n  // prior\n  a ~ normal(prior_int_mean, prior_int_sd);\n  b ~ normal(prior_slope_mean, prior_slope_sd);\n  sigma ~ chi_square(prior_sigma_mean);\n  // likelihood\n  y ~ normal(a+b*x, sigma);\n}\n\nThe chi-squared distribution is written that way in Stan, and has only one parameter, a degrees of freedom that is also its mean.\nOur three parameters then need to be declared, in the parameters section. a and b can be any real number, while sigma has to be positive:\n\nparameters {\n  real a;\n  real b;\n  real&lt;lower=0&gt; sigma;\n}\n\nEverything else is data, and we have a lot of data this time:\n\ndata {\n  int&lt;lower=0&gt; n;\n  vector[n] x;\n  vector[n] y;\n  real prior_int_mean;\n  real&lt;lower=0&gt; prior_int_sd;\n  real prior_slope_mean;\n  real&lt;lower=0&gt; prior_slope_sd;\n  real&lt;lower=0&gt; prior_sigma_mean;\n}\n\nThe five things at the bottom are the prior distribution parameters, which we are going to be eliciting later. The means for intercept and slope can be anything; the prior SDs have to be positive, and so does the prior mean for sigma, since it’s actually a degrees of freedom that has to be positive.\nNow we come to two pieces of subtlety. The first is that the x and y are going to have some (unknown) number of values in them, but we need to declare them with some length. The solution to that is to have the number of observations n also be part of the data. Once we have that, we can declare x and y to be of length n with no problems.\nThe second piece of subtlety is that you were probably expecting this:\n\nreal x[n];\nreal y[n];\n\nThis is usually what you need, but the problem is that when you work out a+b*x later on, it doesn’t work because you are trying to multiply an array of values x by a single value b. (Try it.) There are two ways around this: (i), if you instead declare x and y to be (real) vectors of length n, Stan borrows from R’s multiplication of a vector by a scalar and it works, by multiplying each element of the vector by the scalar. Or, (ii), you can go back to declaring x and y as real things of length n, and use a loop to get each y from its corresponding x, like this:\n\nfor (i in 1:n) {\n  y[i] ~ normal(a + b * x[i], sigma)\n}\n\n\nand this works because a, b, and x[i] are all scalar. I have to say that I don’t really understand the distinction between real x[n] and vector[n] x, except that sometimes one works and the other doesn’t.\nThe manual tells you that the vector way is “much faster”, though in a simple problem like this one I doubt that it makes any noticeable difference.\nMy code looks like this, in total:\n\ndata {\n  int&lt;lower=0&gt; n;\n  vector[n] x;\n  vector[n] y;\n  real prior_int_mean;\n  real&lt;lower=0&gt; prior_int_sd;\n  real prior_slope_mean;\n  real&lt;lower=0&gt; prior_slope_sd;\n  real&lt;lower=0&gt; prior_sigma_mean;\n}\n\nparameters {\n  real a;\n  real b;\n  real&lt;lower=0&gt; sigma;\n}\n\nmodel {\n  // prior\n  a ~ normal(prior_int_mean, prior_int_sd);\n  b ~ normal(prior_slope_mean, prior_slope_sd);\n  sigma ~ chi_square(prior_sigma_mean);\n  // likelihood\n  y ~ normal(a+b*x, sigma);\n}\n\n\n\\(\\blacksquare\\)\n\nCheck your Stan code for syntactic correctness, and when it is correct, compile it.\n\nSolution\nClick the Check button top right of the window where your Stan code is. If it finds any errors, correct them and try again.\nTo compile, the usual thing:\n\nreg &lt;- cmdstan_model(\"reg.stan\")\n\nand wait for it to do its thing. With luck, Check will have found all the errors and this will quietly (eventually) do its job.\n\\(\\blacksquare\\)\n\nWe are going to be analyzing some data on vocabulary size (the number of words known) by children of different ages. It is suspected that the relationship between age and vocabulary size is approximately linear. You go consult with an early childhood expert, and they tell you this:\n\n\nIn children of age up to about six, vocabulary almost always increases by between 300 and 700 words per year.\nI can’t talk about vocabulary of children of age 0, because children don’t start learning to talk until age about 18 months (1.5 years).\nChildren of age 1.5 years almost always have a vocabulary between 0 and 500 words (depending on exactly what age they started talking.)\nEven if we know a child’s age, our prediction of their vocabulary size might be off by as much as 200 words.\n\nUse this information to obtain parameters for your prior distributions.\nSolution\nThis is the typical kind of way in which you would elicit a prior distribution; you try to turn what the expert tells you into something you can use.\nLet’s assume that the “almost always” above corresponds to a 95% confidence interval, and since our intercept and slope have prior normal distributions, this is, to the accuracy that we are working, mean plus/minus 2 SD. (You can make different assumptions and you’ll get a somewhat different collection of prior distributions.)\nThe first statement talks about the change in vocabulary size per year. This is talking about the slope. The supposed 95% confidence interval given translates to \\(500 \\pm 2(100)\\), so the prior mean for the slope is 500 and the prior SD is 100.\nNot so hard. The problems start with the second one.\nWe want a prior mean and SD for the intercept, that is, for the mean and SD of vocabulary size at age 0, but the expert (in their second statement) is telling us this makes no sense. The third statement says that at age 1.5, a 95% CI for vocabulary size is \\(250 \\pm 2(125)\\). You can go a number of different ways from here, but a simple one is use our best guess for the slope, 500, to project back 1.5 years from here by decreasing the mean by \\((500)(1.5)=750\\), that is, to \\(-500 \\pm 2(125)\\).\nThe last one we need is the prior mean for sigma. This is what the last statement is getting at. Up to you whether you think this is an estimate of sigma or twice sigma. Let’s take 200 as a prior estimate of sigma, to be safe.\nYou see that getting a useful prior depends on asking the right questions and making good use of the answers you get.\nSome people like to use “ignorance” priors, where you assign equal probability to all possible values of the parameter. I don’t, because these are saying that a slope of 10 million is just as likely as a slope of 1, regardless of the actual circumstances; you will almost always have some idea of what you are expecting. It might be vague, but it won’t be infinitely vague.\n\\(\\blacksquare\\)\n\nSome data were collected on age and vocabulary size of 10 randomly selected children, shown here: link. Read in and display the data; the values are separated by single spaces.\n\nSolution\nThus: ::: {.cell}\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/pasias/master/vocab.txt\"\nvocabulary &lt;- read_delim(my_url, \" \")\n\nRows: 10 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): age, vocab\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvocabulary\n\n\n\n  \n\n\n:::\n\\(\\blacksquare\\)\n\nUse this dataset, along with your prior distribution from above, to obtain posterior distributions for intercept, slope and error SD. What is the 95% posterior interval for the slope?\n\nSolution\nTwo parts: set up the data, and then sample it:\n\nreg_data &lt;- list(\n  n = 10, x = vocabulary$age, y = vocabulary$vocab,\n  prior_int_mean = -500,\n  prior_int_sd = 125,\n  prior_slope_mean = 500,\n  prior_slope_sd = 100,\n  prior_sigma_mean = 200\n)\nreg.1 &lt;- reg$sample(reg_data)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.6 seconds.\n\nreg.1\n\n variable    mean  median    sd   mad      q5     q95 rhat ess_bulk ess_tail\n    lp__   373.71  374.04  1.25  0.99  371.17  375.04 1.00     1549     2251\n    a     -610.05 -610.48 97.57 97.05 -770.30 -447.95 1.00     1573     1846\n    b      520.20  520.28 26.49 26.18  475.25  563.36 1.00     1513     1951\n    sigma  189.07  188.65 19.01 18.65  159.01  221.19 1.00     2330     2294\n\n\nOne line per parameter (plus the log-posterior distribution, not very useful to us). To get a 95% posterior interval for the slope, use the 2.5 and 97.5 percentiles of the posterior for b, which are 467 and 572. (This is about \\(520 \\pm 52\\), rounding crudely, while the prior distribution said \\(500 \\pm 200\\), so the data have allowed us to estimate the slope a fair bit more accurately.)\n\\(\\blacksquare\\)\n\nPlot a histogram of the posterior distribution of the slope. Does its shape surprise you? Explain briefly.\n\nSolution\nThis is most easily mcmc_hist from bayesplot:\n\nmcmc_hist(reg.1$draws(\"b\"), binwidth = 20)\n\n\n\n\nI’m guessing you have a better intuition for bins as opposed to binwidth (the latter being what you need here), so you can try it without giving a binwidth at all (and getting way too many bins), and then see if you can figure out what binwidth should be to get you a sensible number of bins. This one looks pretty good to me.\nThe shape is very normal. This is because everything is normal: the prior and the data-generating process both, so it is not surprising at all that the posterior came out normal. (You may remember from your regression course that if you have a normal regression model, the slope also has a normal distribution.)\n\\(\\blacksquare\\)\n\nWhat can we say about the vocabulary size of a randomly selected child of age 5 (a new one, not the one in the original data set)? Use an appropriate predictive distribution.\n\nSolution\nIf you have done a regression course, you might recognize this as being the Bayesian version of a prediction interval. How might we make a predictive distribution for this? Well, first we need to extract the sampled values from the posteriors:\n\nas_draws_df(reg.1$draws()) %&gt;%\n  as_tibble() -&gt; sims\nsims\n\n\n\n  \n\n\n\nand now we need to simulate some response values for our notional child of age 5. That means simulating for an x of 5, using each of those values of a, b and sigma:\n\nsims %&gt;%\n  rowwise() %&gt;% \n  mutate(sim_vocab = rnorm(1, a + b * 5, sigma)) -&gt; sims2\nsims2\n\n\n\n  \n\n\nggplot(sims2, aes(x = sim_vocab)) + geom_histogram(bins = 20)\n\n\n\n\nThat’s the distribution of the vocabulary size of children aged 5. We can get a 95% interval from this the usual way: find the 2.5 and 97.5 percentiles:\n\nwith(sims2, quantile(sim_vocab, c(0.025, 0.975)))\n\n    2.5%    97.5% \n1580.558 2387.346 \n\n\nThe actual child of age 5 that we observed had a vocabulary of 2060 words, squarely in the middle of this interval.\nIs the posterior predictive interval like the prediction interval?\n\nvocabulary.1 &lt;- lm(vocab ~ age, data = vocabulary)\nnew &lt;- tibble(age = 5)\npredict(vocabulary.1, new, interval = \"p\")\n\n       fit      lwr      upr\n1 2027.939 1818.223 2237.656\n\n\nIt seems a bit wider.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "stan.html#estimating-p-the-bayesian-way-1",
    "href": "stan.html#estimating-p-the-bayesian-way-1",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.6 Estimating \\(p\\) the Bayesian way",
    "text": "23.6 Estimating \\(p\\) the Bayesian way\nA binomial experiment with 8 trials produces the following results: success, failure, success, success, failure, success, success, success. (Each result is therefore a Bernoulli trial.) The person who gave you the data says that the success probability is most likely somewhere near 0.5, but might be near 0 or 1. The aim of this question is to estimate the success probability using Bayesian methods.\nIn this question, use cmdstanr (see this site for instructions). Documentation for Stan is here. You will probably want to be running R on your own computer.\n\nWrite a Stan program that will estimate the success probability \\(p\\). To do this, start with the likelihood (Stan has a function bernoulli that takes one parameter, the success probability). The data, as 1s and 0s, will be in a vector x. Use a beta distribution with unknown parameters as a prior for p. (We will worry later what those parameters should be.)\n\nSolution\nFile, New and Stan. Leave the template program there if you like, as a reminder of what to do. In the model section is where the likelihood goes, like this:2\nmodel {\n// likelihood\nx ~ bernoulli(p);\n}\nThe right one here is bernoulli since your data are Bernoulli trials (successes and failures, coded as 1s and 0s). If you had a summarized total number of successes and a number of trials, then that would be binomial. It actually doesn’t make any difference which way you do it, but it’s probably easier to think about it this way because it’s more like the Poisson one in lecture.\nThinking ahead, x is going to be data, and p is a parameter, so p will need a prior distribution. The standard one for a Bernoulli success probability is a beta distribution. This is actually the conjugate prior, if you have learned about those: if p has a beta prior and the likelihood is Bernoulli, then the posterior is also beta. Back in the days when algebra was your only hope for this kind of thing, conjugate priors were very helpful, but now that we can sample from any posterior, the fact that a prior is conjugate is neither here nor there. Having said that, the beta distribution is a nice choice for a prior for this, because it is restricted to \\([0, 1]\\) the same way that a Bernoulli p is.\nI’m going leave the prior parameters for p unknown for now; we’ll just call them a and b.3 Here’s our completed model section:\nmodel {\n// prior\np ~ beta(a, b);\n// likelihood\nx ~ bernoulli(p);\n}\na and b are not parameters; they are some numbers that we will supply, so they will be part of the data section. Leaving them unspecified like this, rather than hard-coding them, is good coding practice, since the code we finish with can be used for any Bernoulli estimation problem, not just the one we happen to have.\nThere is only one parameter, p, so the parameters section is short:\nparameters {\nreal&lt;lower=0,upper=1&gt; p;\n}\nWe know that p must be between 0 and 1, so we specify that here so that the sampler doesn’t stray into impossible values for p.\nThat goes before the model section. Everything else is data. We also want to avoid hard-coding the number of observations, so we will also have an n as data, which we declare first, so we can declare the array of values x to be of length n:\ndata {\nint&lt;lower=0&gt; n;\nreal a;\nreal b;\nint&lt;lower=0, upper=1&gt; x[n];\n}\nx is an integer array of length n. This is how you declare one of those: the type is first, along with any limits, and then the length of the array is appended in square brackets to the name of the array.\nArrange your code in a file with extension .stan, with data first, parameters second, and model third. I called mine bernoulli.stan.\nExtra: there are two ways to declare a real-valued array y: as real y[n], or as vector[n] y. Sometimes it matters which way you do it (and I don’t have a clear sense of when it matters). The two ways differ in what you can do with them.\n\\(\\blacksquare\\)\n\nCompile your code, correcting any errors until it compiles properly.\n\nSolution\ncmdstanr goes like this:\n\nm2 &lt;- cmdstan_model(\"bernoulli.stan\")\n\n\nm2\n\ndata {\n  int&lt;lower=0&gt; n;\n  real a;\n  real b;\n  array[n] int&lt;lower=0, upper=1&gt; x;\n}\n\nparameters {\n  real&lt;lower=0,upper=1&gt; p;\n}\n\nmodel {\n  // prior\n  p ~ beta(a, b);\n  // likelihood\n  x ~ bernoulli(p);\n}\n\n\nIf it doesn’t compile, you have some fixing up to do. The likely first problem is that you have missed a semicolon somewhere. The error message will at least give you a hint about where the problem is. Fix any errors you see and try again. If you end up with a different error message, that at least is progress.\n\\(\\blacksquare\\)\n\nThe person who brought you the data told you that the success probability p should be somewhere near 0.5 (and is less likely to be close to 0 or 1). Use this information to pick a prior distribution for p. (The exact answer you get doesn’t really matter, but try to interpret the statement in some kind of sensible way.)\n\nSolution\nI don’t know how much intuition you have for what beta distributions look like, so let’s play around a bit. Let’s imagine we have a random variable \\(Y\\) that has a beta distribution. This distribution has two parameters, usually called \\(a\\) and \\(b\\). Let’s draw some pictures and see if we can find something that would serve as a prior. R has dbeta that is the beta distribution density function.\nStart by choosing some values for \\(Y\\):\n\ny &lt;- seq(0, 1, 0.01)\ny\n\n  [1] 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14\n [16] 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29\n [31] 0.30 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44\n [46] 0.45 0.46 0.47 0.48 0.49 0.50 0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58 0.59\n [61] 0.60 0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.70 0.71 0.72 0.73 0.74\n [76] 0.75 0.76 0.77 0.78 0.79 0.80 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89\n [91] 0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1.00\n\n\nthen work out dbeta of these for your choice of parameters, then plot it. I’m going straight to a function for this, since I anticipate doing it several times. This y and the two parameters should be input to the function:\n\nplot_beta &lt;- function(y, a, b) {\n  tibble(y=y) %&gt;% \n  mutate(density = dbeta(y, a, b)) %&gt;% \n  ggplot(aes(x = y, y = density)) + geom_line()\n}\nplot_beta(y, 1, 1)\n\n\n\n\nThe beta with parameters 1 and 1 is a uniform distribution. (If you look up the beta density function, you’ll see why that is.)\nLet’s try another:\n\nplot_beta(y, 3, 2)\n\n\n\n\nThis one is skewed to the left. You might guess that having the second parameter bigger would make it skewed to the right:\n\nplot_beta(y, 3, 7)\n\n\n\n\nwhich indeed is the case. If you try some other values, you’ll see that this pattern with the skewness continues to hold. Furthermore, the right-skewed distributions have their peak to the left of 0.5, and the left-skewed ones have their peak to the right of 0.5.\nTherefore, you would think, having the two parameters the same would give a symmetric distribution:\n\nplot_beta(y, 2, 2)\n\n\n\n\nNote that the peak is now at 0.5, which is what we wanted.\nThe question called for a prior distribution of values “somewhere near 0.5”, and you could reasonably say that this does the job. What does 3 and 3 look like?\n\nplot_beta(y, 3, 3)\n\n\n\n\nThis is more concentrated around 0.5, and as you increase the two parameter values while keeping them equal, it gets more concentrated still:\n\nplot_beta(y, 20, 20)\n\n\n\n\nFor our purposes, this is undoubtedly too concentrated around 0.5; there is no chance of \\(y\\) being outside \\([0.25, 0.75]\\). I would go with parameters 2 and 2 or maybe 3 and 3. As I said, pretty much any choice of parameter values that are both the same is at least somewhat justifiable.\nIf you don’t want to go through all of this, find some pictures of beta distributions with different parameters, and pick one you like. The Wikipedia page is one place (from which you would probably pick 2 and 2). Here is another, from which you might pick 5 and 5.\nIn practice, you would have some back-and-forth with the person who brought you the data, and try to match what they are willing to say about p, without looking at the data, to what you know or can find out about the beta distribution. This process is called “prior elicitation”.\nExtra: if you have ever obtained the posterior distribution in this case by algebra, you might recall that the effect of the prior distribution is to add some “fake” Bernoulli trials to the data. With \\(a=b=2\\), for example, you imagine \\(2+2-2 = 2\\) fake trials, with \\(2-1=1\\) success and \\(2-1=1\\) failure, to add to the data. This brings the estimate of p a little closer to 0.5 than it would be with just plain maximum likelihood.\n\\(\\blacksquare\\)\n\nCreate an R list that contains all your data for your Stan model. Remember that Stan expects the data in x to be 0s and 1s.\n\nSolution\nTurn those successes and failures in the question into a vector of 0 and 1 values, with 1 being success (you wanted to estimate the probability of success): they were success, failure, success, success, failure, success, success, success.\n\nx &lt;- c(1, 0, 1, 1, 0, 1, 1, 1)\nx\n\n[1] 1 0 1 1 0 1 1 1\n\n\nThen make a “named list” of inputs to your Stan program, including the parameter values for the prior distribution (I went with 2 and 2):\n\nstan_data &lt;- list(\nn = 8,\na = 2,\nb = 2,\nx = x\n)\nstan_data\n\n$n\n[1] 8\n\n$a\n[1] 2\n\n$b\n[1] 2\n\n$x\n[1] 1 0 1 1 0 1 1 1\n\n\n\\(\\blacksquare\\)\n\nRun your Stan model to obtain a simulated posterior distribution, using all the other defaults.\n\nSolution\nThe cmdstanr way:\n\nfit2 &lt;- m2$sample(data = stan_data)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.7 seconds.\n\nfit2\n\n variable  mean median   sd  mad    q5   q95 rhat ess_bulk ess_tail\n     lp__ -8.18  -7.88 0.75 0.33 -9.66 -7.64 1.00     1926     2095\n     p     0.67   0.68 0.13 0.14  0.44  0.87 1.00     1371     1602\n\n\nThis one gives you a 90% posterior interval instead of a 95% one, but the posterior mean is 0.66, as before, and the interval says that p is likely bigger than about 0.4; the data did not narrow it down much apart from that.\n\\(\\blacksquare\\)\n\nMake a plot of the posterior distribution of the probability of success. (Use the posterior and bayesplot packages if convenient.)\n\nSolution\nThis means extracting the sampled values of \\(p\\) first. The cmdstanr way is not very convenient, at least at first:\n\nbern.2a &lt;- fit2$draws()\nstr(bern.2a)\n\n 'draws_array' num [1:1000, 1:4, 1:2] -8.38 -9.05 -7.74 -8.44 -9.31 ...\n - attr(*, \"dimnames\")=List of 3\n  ..$ iteration: chr [1:1000] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ chain    : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ variable : chr [1:2] \"lp__\" \"p\"\n\n\nThis is a 3-dimensional array (sample by chain by variable). For plotting and so on, we really want this as a dataframe. At this point, I would use the posterior and bayesplot packages, which you should install following the instructions for cmdstanr at the top of this page. Put the names of the extra two packages in place of the cmdstanr that you see there.\n\nlibrary(posterior)\nlibrary(bayesplot)\n\nTo get the samples as a dataframe:\n\nbern.2 &lt;- as_draws_df(fit2$draws())\nbern.2\n\n\n\n  \n\n\n\nYou don’t even need to go this far to make a plot of the posterior distribution, because bayesplot does it automatically:\n\nmcmc_hist(fit2$draws(\"p\"), binwidth =  0.05)\n\n\n\n\nRather annoyingly, this plot function passes binwidth on to geom_histogram, but not bins!\nThis is skewed to the left. The reason for the skewness here is that the upper limit for \\(p\\) is 1, and there is a reasonable chance of \\(p\\) being close to 1, so the distribution is skewed in the opposite direction. There is basically no chance of \\(p\\) being close to zero. If we had had more data, it is more likely that the values of \\(p\\) near 0 and 1 would be ruled out, and then we might have ended up with something more symmetric.\nExtra: If you remember the algebra for this, the posterior distribution for p actually has a beta distribution, with parameters \\(2+6=8\\) and \\(2+2=4\\).4 Our simulated posterior looks to have the right kind of shape to be this, being skewed to the left.\n\\(\\blacksquare\\)\n\nThe posterior predictive distribution is rather odd here: the only possible values that can be observed are 0 and 1. Nonetheless, obtain the posterior predictive distribution for these data, and explain briefly why it is not surprising that it came out as it did.\n\nSolution\nWith cmdstanr, start from what I called bern.2.5\nThe way to obtain the (sampled) posterior predictive distribution is to get the posterior distribution of values of \\(p\\) in a dataframe, and make a new column as random values from the data-generating mechanism (here Bernoulli). This is easier to do and then talk about:\n\nbern.2 %&gt;% \n  as_tibble() %&gt;% \n  mutate(x = rbernoulli(4000, p)) -&gt; ppd\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `x = rbernoulli(4000, p)`.\nCaused by warning:\n! `rbernoulli()` was deprecated in purrr 1.0.0.\n\nppd\n\n\n\n  \n\n\n\nThe values of x in the last column are TRUE for success and FALSE for failure (they could have been 1 and 0). Thus, the first x is a Bernoulli trial with success probability the first value of p, the second one uses the second value of p, and so on. Most of the success probabilities are bigger than 0.5, so most of the posterior predictive distribution is successes.\nIt seems to go better if you turn bern.2 into a tibble before generating x.\nA bar chart would be an appropriate plot (you can either think of x as categorical, or as a discrete 0 or 1):\n\nggplot(ppd, aes(x=x)) + geom_bar()\n\n\n\n\nwhich shows the majority of successes in the posterior predictive distribution. The idea is that the data and the posterior predictive distribution ought to be similar, and we did indeed have a majority of successes in our data as well.\nYou might have been perplexed by the 4000 in the code above. bernoulli is vectorized, meaning that if you give it a vector of values for p, it will generate Bernoulli trials for each one in turn, and the whole result should be 4000 values long altogether. We’ll see a way around that in a moment, but you could also do this using rbinom (random binomials) if you do it right:\n\nbern.2 %&gt;% \n  as_tibble() %&gt;% \n  mutate(x = rbinom(4000, 1, p)) \n\n\n\n  \n\n\n\nThere are 4000 random binomials altogether, and each one has one trial. This is confusing, and a less confusing way around this is to work one row at time with rowwise:\n\nbern.2 %&gt;% \n  as_tibble() %&gt;% \n  rowwise() %&gt;% \n  mutate(x = rbernoulli(1, p))\n\n\n\n  \n\n\n\nor\n\nbern.2 %&gt;% \n  as_tibble() %&gt;% \n  rowwise() %&gt;%\n  mutate(x = rbinom(1, 1, p)) \n\n\n\n  \n\n\n\nExtra: I’d also like to put in a plug for the tidybayes package. This works best with rstan, though it will work with cmdstanr also. The first thing it will help you with is setting up the data:\n\nlibrary(tidybayes)\ntibble(x) %&gt;% compose_data()\n\n$x\n[1] 1 0 1 1 0 1 1 1\n\n$n\n[1] 8\n\n\nStarting from a dataframe of data (our x), this returns you a list that you can submit as data = to sampling. Note that it counts how many observations you have, on the basis that you’ll be sending this to Stan as well (we did).\nAnother thing that this will do is to handle categorical variables. Say you had something like this, with g being a group label:\n\nd &lt;- tribble(\n~g, ~y,\n\"a\", 10,\n\"a\", 11,\n\"a\", 12,\n\"b\", 13,\n\"b\", 14,\n\"b\", 15\n)\ncompose_data(d)\n\n$g\n[1] 1 1 1 2 2 2\n\n$n_g\n[1] 2\n\n$y\n[1] 10 11 12 13 14 15\n\n$n\n[1] 6\n\n\nKnowing that Stan only has real and int, it labels the groups with numbers, and keeps track of how many groups there are as well as how many observations. These are all things that Stan needs to know. See slides 32 and 34 of my lecture notes, where I prepare to fit an ANOVA model. The tidybayes way is, I have to say, much cleaner than the way I did it in the lecture notes. After you have fitted the model, tidybayes lets you go back and re-associate the real group names with the ones Stan used, so that you could get a posterior mean and interval for each of the two groups.\nAfter obtaining the posterior distribution, tidybayes also helps in understanding it. This is how you get hold of the sampled values. Install laRs using\n\ndevtools::install_github(\"Agasax/laRs\")\n\n\nlibrary(laRs) \nbern.2 %&gt;% \n  spread_draws(p)\n\n\n\n  \n\n\n\nwhich you can then summarize:\n\nbern.2 %&gt;% spread_draws(p) %&gt;% \n  median_hdi()\n\n\n\n  \n\n\n\nThe median of the posterior distribution, along with a 95% Bayesian posterior interval based on the highest posterior density. There are other possibilities.\nOr you can plot it:\n\nbern.2 %&gt;%\n  spread_draws(p) %&gt;% \n  ggplot(aes(x = p)) + stat_slab()\n\n\n\n\n(a density plot)\nor, posterior predictive distribution:\n\nbern.2 %&gt;% \n  spread_draws(p) %&gt;% \n  rowwise() %&gt;% \n  mutate(x = rbernoulli(1, p)) %&gt;% \n  ggplot(aes(x = x)) +\n  geom_bar()\n\n\n\n\nThis is a nice introduction to tidybayes, with a running example.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "stan.html#footnotes",
    "href": "stan.html#footnotes",
    "title": "23  Bayesian Statistics with Stan",
    "section": "",
    "text": "Alpha and beta don’t have to be integers; you could use seq to create sequences of values for alpha and beta that include decimal numbers.↩︎\nThe comment line, with two slashes on the front, is optional but will help you keep track of what’s what.↩︎\nWe’ll come back later to the question of what a and b should be for our situation.↩︎\nThe first 2 in each case is the parameter of the prior distribution and the second number is the number of successes or failures observed in the data.↩︎\nI am writing this a couple of days after the Ever Given was freed from blocking the Suez Canal. One of the memes I saw about this was actually a meme-upon-a-meme: on the picture of the tiny tractor and the huge ship, someone had superimposed that picture of Bernie Sanders sitting on his chair. Feel the bern.2.↩︎"
  },
  {
    "objectID": "logistic-regression.html#finding-wolf-spiders-on-the-beach",
    "href": "logistic-regression.html#finding-wolf-spiders-on-the-beach",
    "title": "24  Logistic regression",
    "section": "24.1 Finding wolf spiders on the beach",
    "text": "24.1 Finding wolf spiders on the beach\n A team of Japanese researchers were investigating what would cause the burrowing wolf spider Lycosa ishikariana* to be found on a beach. They hypothesized that it had to do with the size of the grains of sand on the beach. They went to 28 beaches in Japan, measured the average sand grain size (in millimetres), and observed the presence or absence of this particular spider on each beach. The data are in link.\n\nWhy would logistic regression be better than regular regression in this case?\nRead in the data and check that you have something sensible. (Look at the data file first: the columns are aligned but the headers are not aligned with them.)\nMake a boxplot of sand grain size according to whether the spider is present or absent. Does this suggest that sand grain size has something to do with presence or absence of the spider?\nFit a logistic regression predicting the presence or absence of spiders from the grain size, and obtain its summary. Note that you will need to do something with the response variable first.\nIs there a significant relationship between grain size and presence or absence of spiders at the \\(\\alpha=0.10\\) level? Explain briefly.\nObtain predicted probabilities of spider presence for a representative collection of grain sizes. I only want predicted probabilities, not any kind of intervals.\nGiven your previous work in this question, does the trend you see in your predicted probabilities surprise you? Explain briefly."
  },
  {
    "objectID": "logistic-regression.html#killing-aphids",
    "href": "logistic-regression.html#killing-aphids",
    "title": "24  Logistic regression",
    "section": "24.2 Killing aphids",
    "text": "24.2 Killing aphids\nAn experiment was designed to examine how well the insecticide rotenone kills aphids that feed on the chrysanthemum plant called Macrosiphoniella sanborni. The explanatory variable is the log concentration (in milligrams per litre) of the insecticide. At each of the five different concentrations, approximately 50 insects were exposed. The number of insects exposed at each concentration, and the number killed, are shown below.\n\nLog-Concentration   Insects exposed    Number killed   \n0.96                       50              6               \n1.33                       48              16              \n1.63                       46              24              \n2.04                       49              42              \n2.32                       50              44              \n\n\nGet these data into R. You can do this by copying the data into a file and reading that into R (creating a data frame), or you can enter the data manually into R using c, since there are not many values. In the latter case, you can create a data frame or not, as you wish. Demonstrate that you have the right data in R.\n* Looking at the data, would you expect there to be a significant effect of log-concentration? Explain briefly.\nWe are going to do a logistic regression to predict how likely an insect is to be killed, as it depends on the log-concentration. Create a suitable response variable, bearing in mind (i) that we have lots of insects exposed to each different concentration, and (ii) what needs to go into each column of the response.\nRun a suitable logistic regression, and obtain a summary of the results.\nDoes your analysis support your answer to (here)? Explain briefly.\nObtain predicted probabilities of an insect’s being killed at each of the log-concentrations in the data set.\nPeople in this kind of work are often interested in the “median lethal dose”. In this case, that would be the log-concentration of the insecticide that kills half the insects. Based on your predictions, roughly what do you think the median lethal dose is?"
  },
  {
    "objectID": "logistic-regression.html#the-effects-of-substance-a",
    "href": "logistic-regression.html#the-effects-of-substance-a",
    "title": "24  Logistic regression",
    "section": "24.3 The effects of Substance A",
    "text": "24.3 The effects of Substance A\nIn a dose-response experiment, animals (or cell cultures or human subjects) are exposed to some toxic substance, and we observe how many of them show some sort of response. In this experiment, a mysterious Substance A is exposed at various doses to 100 cells at each dose, and the number of cells at each dose that suffer damage is recorded. The doses were 10, 20, … 70 (mg), and the number of damaged cells out of 100 were respectively 10, 28, 53, 77, 91, 98, 99.\n\nFind a way to get these data into R, and show that you have managed to do so successfully.\nWould you expect to see a significant effect of dose for these data? Explain briefly.\nFit a logistic regression modelling the probability of a cell being damaged as it depends on dose. Display the results. (Comment on them is coming later.)\nDoes your output indicate that the probability of damage really does increase with dose? (There are two things here: is there really an effect, and which way does it go?)\nObtain predicted damage probabilities for some representative doses.\nDraw a graph of the predicted probabilities, and to that add the observed proportions of damage at each dose. Hints: you will have to calculate the observed proportions first. See here, near the bottom, to find out how to add data to one of these graphs. The geom_point line is the one you need.\n\nLooking at the predicted probabilities, would you say that the model fits well? Explain briefly."
  },
  {
    "objectID": "logistic-regression.html#what-makes-an-animal-get-infected",
    "href": "logistic-regression.html#what-makes-an-animal-get-infected",
    "title": "24  Logistic regression",
    "section": "24.4 What makes an animal get infected?",
    "text": "24.4 What makes an animal get infected?\nSome animals got infected with a parasite. We are interested in whether the likelihood of infection depends on any of the age, weight and sex of the animals. The data are at link. The values are separated by tabs.\n\nRead in the data and take a look at the first few lines. Is this one animal per line, or are several animals with the same age, weight and sex (and infection status) combined onto one line? How can you tell?\n* Make suitable plots or summaries of infected against each of the other variables. (You’ll have to think about sex, um, you’ll have to think about the sex variable, because it too is categorical.) Anything sensible is OK here. You might like to think back to what we did in Question here for inspiration. (You can also investigate table, which does cross-tabulations.)\nWhich, if any, of your explanatory variables appear to be related to infected? Explain briefly.\nFit a logistic regression predicting infected from the other three variables. Display the summary.\n* Which variables, if any, would you consider removing from the model? Explain briefly.\nAre the conclusions you drew in (here) and (here) consistent, or not? Explain briefly.\n* The first and third quartiles of age are 26 and 130; the first and third quartiles of weight are 9 and 16. Obtain predicted probabilities for all combinations of these and sex. (You’ll need to start by making a new data frame, using datagrid to get all the combinations.)"
  },
  {
    "objectID": "logistic-regression.html#the-brain-of-a-cat",
    "href": "logistic-regression.html#the-brain-of-a-cat",
    "title": "24  Logistic regression",
    "section": "24.5 The brain of a cat",
    "text": "24.5 The brain of a cat\nA large number (315) of psychology students were asked to imagine that they were serving on a university ethics committee hearing a complaint against animal research being done by a member of the faculty. The students were told that the surgery consisted of implanting a device called a cannula in each cat’s brain, through which chemicals were introduced into the brain and the cats were then given psychological tests. At the end of the study, the cats’ brains were subjected to histological analysis. The complaint asked that the researcher’s authorization to carry out the study should be withdrawn, and the cats should be handed over to the animal rights group that filed the complaint. It was suggested that the research could just as well be done with computer simulations.\nAll of the psychology students in the survey were told all of this. In addition, they read a statement by the researcher that no animal felt much pain at any time, and that computer simulation was not an adequate substitute for animal research. Each student was also given one of the following scenarios that explained the benefit of the research:\n\n“cosmetic”: testing the toxicity of chemicals to be used in new lines of hair care products.\n“theory”: evaluating two competing theories about the function of a particular nucleus in the brain.\n“meat”: testing a synthetic growth hormone said to potentially increase meat production.\n“veterinary”: attempting to find a cure for a brain disease that is killing domesticated cats and endangered species of wild cats.\n“medical”: evaluating a potential cure for a debilitating disease that afflicts many young adult humans.\n\nFinally, each student completed two questionnaires: one that would assess their “relativism”: whether or not they believe in universal moral principles (low score) or whether they believed that the appropriate action depends on the person and situation (high score). The second questionnaire assessed “idealism”: a high score reflects a belief that ethical behaviour will always lead to good consequences (and thus that if a behaviour leads to any bad consequences at all, it is unethical).1\nAfter being exposed to all of that, each student stated their decision about whether the research should continue or stop.\nI should perhaps stress at this point that no actual cats were harmed in the collection of these data (which can be found as a .csv file at link). The variables in the data set are these:\n\ndecision: whether the research should continue or stop (response)\nidealism: score on idealism questionnaire\nrelativism: score on relativism questionnaire\ngender of student\nscenario of research benefits that the student read.\n\nA more detailed discussion^(“[If you can believe it.] of this study is at link.\n\nRead in the data and check by looking at the structure of your data frame that you have something sensible. Do not call your data frame decision, since that’s the name of one of the variables in it.\nFit a logistic regression predicting decision from gender. Is there an effect of gender?\nTo investigate the effect (or non-effect) of gender, create a contingency table by feeding decision and gender into table. What does this tell you?\n* Is your slope for gender in the previous logistic regression positive or negative? Is it applying to males or to females? Looking at the conclusions from your contingency table, what probability does that mean your logistic regression is actually modelling?\nAdd the two variables idealism and relativism to your logistic regression. Do either or both of them add significantly to your model? Explain briefly.\nAdd the variable scenario to your model. That is, fit a new model with that variable plus all the others.\nUse anova to compare the models with and without scenario. You’ll have to add a test=\"Chisq\" to your anova, to make sure that the test gets done. Does scenario make a difference or not, at \\(\\alpha=0.10\\)? Explain briefly. (The reason we have to do it this way is that scenario is a factor with five levels, so it has four slope coefficients. To test them all at once, which is what we need to make an overall test for scenario, this is the way it has to be done.)\nLook at the summary of your model that contained scenario. Bearing in mind that the slope coefficient for scenariocosmetic is zero (since this is the first scenario alphabetically), which scenarios have the most positive and most negative slope coefficients? What does that tell you about those scenarios’ effects?\nDescribe the effects that having (i) a higher idealism score and (ii) a higher relativity score have on a person’s probability of saying that the research should stop. Do each of these increase or decrease that probability? Explain briefly."
  },
  {
    "objectID": "logistic-regression.html#how-not-to-get-heart-disease",
    "href": "logistic-regression.html#how-not-to-get-heart-disease",
    "title": "24  Logistic regression",
    "section": "24.6 How not to get heart disease",
    "text": "24.6 How not to get heart disease\nWhat is associated with heart disease? In a study, a large number of variables were measured, as follows:\n\nage (years)\nsex male or female\npain.type Chest pain type (4 values: typical angina, atypical angina, non-anginal pain, asymptomatic)\nresting.bp Resting blood pressure, on admission to hospital\nserum.chol Serum cholesterol\nhigh.blood.sugar: greater than 120, yes or no\nelectro resting electrocardiographic results (normal, having ST-T, hypertrophy)\nmax.hr Maximum heart rate\nangina Exercise induced angina (yes or no)\noldpeak ST depression induced by exercise relative to rest. See link.\nslope Slope of peak exercise ST segment. Sloping up, flat or sloping down\ncolored number of major vessels (0–3) coloured by fluoroscopy\nthal normal, fixed defect, reversible defect\nheart.disease yes, no (response)\n\nI don’t know what most of those are, but we will not let that stand in our way. Our aim is to find out what variables are associated with heart disease, and what values of those variables give high probabilities of heart disease being present. The data are in link.\n\nRead in the data. Display the first few lines and convince yourself that those values are reasonable.\nIn a logistic regression, what probability will be predicted here? Explain briefly but convincingly. (Is each line of the data file one observation or a summary of several?)\n* Fit a logistic regression predicting heart disease from everything else (if you have a column called X or X1, ignore that), and display the results.\nQuite a lot of our explanatory variables are factors. To assess whether the factor as a whole should stay or can be removed, looking at the slopes won’t help us very much (since they tell us whether the other levels of the factor differ from the baseline, which may not be a sensible comparison to make). To assess which variables are candidates to be removed, factors included (properly), we can use drop1. Feed drop1 a fitted model and the words test=\"Chisq\" (take care of the capitalization!) and you’ll get a list of P-values. Which variable is the one that you would remove first? Explain briefly.\nI’m not going to make you do the whole backward elimination (I’m going to have you use step for that later), but do one step: that is, fit a model removing the variable you think should be removed, using update, and then run drop1 again to see which variable will be removed next.\nUse step to do a backward elimination to find which variables have an effect on heart disease. Display your final model (which you can do by saving the output from step in a variable, and asking for the summary of that. In step, you’ll need to specify a starting model (the one from part (here)), the direction of elimination, and the test to display the P-value for (the same one as you used in drop1). (Note: the actual decision to keep or drop explanatory variables is based on AIC rather than the P-value, with the result that step will sometimes keep variables you would have dropped, with P-values around 0.10.)\nDisplay the summary of the model that came out of step.\nWe are going to make a large number of predictions. Create and save a data frame that contains predictions for all combinations of representative values for all the variables in the model that came out of step. By “representative” I mean all the values for a categorical variable, and the five-number summary for a numeric variable. (Note that you will get a lot of predictions.)\nFind the largest predicted probability (which is the predicted probability of heart disease) and display all the variables that it was a prediction for.\nCompare the summary of the final model from step with your highest predicted heart disease probability and the values of the other variables that make it up. Are they consistent?"
  },
  {
    "objectID": "logistic-regression.html#successful-breastfeeding",
    "href": "logistic-regression.html#successful-breastfeeding",
    "title": "24  Logistic regression",
    "section": "24.7 Successful breastfeeding",
    "text": "24.7 Successful breastfeeding\nA regular pregnancy lasts 40 weeks, and a baby that is born at or before 33 weeks is called “premature”. The number of weeks at which a baby is born is called its “gestational age”. Premature babies are usually smaller than normal and may require special care. It is a good sign if, when the mother and baby leave the hospital to go home, the baby is successfully breastfeeding.\nThe data in link are from a study of 64 premature infants. There are three columns: the gestational age (a whole number of weeks), the number of babies of that gestational age that were successfully breastfeeding when they left the hospital, and the number that were not. (There were multiple babies of the same gestational age, so the 64 babies are summarized in 6 rows of data.)\n\nRead the data into R and display the data frame.\nVerify that there were indeed 64 infants, by having R do a suitable calculation on your data frame that gives the right answer for the right reason.\nDo you think, looking at the data, that there is a relationship between gestational age and whether or not the baby was successfully breastfeeding when it left the hospital? Explain briefly.\nWhy is logistic regression a sensible technique to use here? Explain briefly.\nFit a logistic regression to predict the probability that an infant will be breastfeeding from its gestational age. Show the output from your logistic regression.\nDoes the significance or non-significance of the slope of gest.age surprise you? Explain briefly.\nIs your slope (in the Estimate column) positive or negative? What does that mean, in terms of gestational ages and breastfeeding? Explain briefly.\nObtain the predicted probabilities that an infant will successfully breastfeed for a representative collection of gestational ages."
  },
  {
    "objectID": "logistic-regression.html#making-it-over-the-mountains",
    "href": "logistic-regression.html#making-it-over-the-mountains",
    "title": "24  Logistic regression",
    "section": "24.8 Making it over the mountains",
    "text": "24.8 Making it over the mountains\nIn 1846, the Donner party (Donner and Reed families) left Springfield, Illinois for California in covered wagons. After reaching Fort Bridger, Wyoming, the leaders decided to find a new route to Sacramento. They became stranded in the eastern Sierra Nevada mountains at a place now called Donner Pass, when the region was hit by heavy snows in late October. By the time the survivors were rescued on April 21, 1847, 40 out of 87 had died.\nAfter the rescue, the age and gender of each person in the party was recorded, along with whether they survived or not. The data are in link.\n\nRead in the data and display its structure. Does that agree with the description in the previous paragraph?\nMake graphical or numerical summaries for each pair of variables. That is, you should make a graph or numerical summary for each of age vs. gender, age vs.\nsurvived and gender vs. survived. In choosing the kind of graph or summary that you will use, bear in mind that survived and gender are factors with two levels.\nFor each of the three graphs or summaries in the previous question, what do they tell you about the relationship between the pair of variables concerned? Explain briefly.\nFit a logistic regression predicting survival from age and gender. Display the summary.\nDo both explanatory variables have an impact on survival? Does that seem to be consistent with your numerical or graphical summaries? Explain briefly.\nAre the men typically older, younger or about the same age as the women? Considering this, explain carefully what the negative gendermale slope in your logistic regression means.\nObtain predicted probabilities of survival for each combination of some representative ages and of the two genders in this dataset.\nDo your predictions support your conclusions from earlier about the effects of age and gender? Explain briefly."
  },
  {
    "objectID": "logistic-regression.html#who-needs-the-most-intensive-care",
    "href": "logistic-regression.html#who-needs-the-most-intensive-care",
    "title": "24  Logistic regression",
    "section": "24.9 Who needs the most intensive care?",
    "text": "24.9 Who needs the most intensive care?\nThe “APACHE II” is a scale for assessing patients who arrive in the intensive care unit (ICU) of a hospital. These are seriously ill patients who may die despite the ICU’s best attempts. APACHE stands for “Acute Physiology And Chronic Health Evaluation”.2 The scale score is calculated from several physiological measurements such as body temperature, heart rate and the Glasgow coma scale, as well as the patient’s age. The final result is a score between 0 and 71, with a higher score indicating more severe health issues. Is it true that a patient with a higher APACHE II score has a higher probability of dying?\nData from one hospital are in link. The columns are: the APACHE II score, the total number of patients who had that score, and the number of patients with that score who died.\n\nRead in and display the data (however much of it displays). Why are you convinced that have the right thing?\nDoes each row of the data frame relate to one patient or sometimes to more than one? Explain briefly.\nExplain why this is the kind of situation where you need a two-column response, and create this response variable, bearing in mind that I will (later) want you to estimate the probability of dying, given the apache score.\nFit a logistic regression to estimate the probability of death from the apache score, and display the results.\nIs there a significant effect of apache score on the probability of survival? Explain briefly.\nIs the effect of a larger apache score to increase or to decrease the probability of death? Explain briefly.\nObtain the predicted probability of death for a representative collection of the apache scores that were in the data set. Do your predictions behave as you would expect (from your earlier work)?\nMake a plot of predicted death probability against apache score (joined by lines) with, also on the plot, the observed proportion of deaths within each apache score, plotted against apache score. Does there seem to be good agreement between observation and prediction? Hint: calculate what you need to from the original dataframe first, save it, then make a plot of the predictions, and then to the plot add the appropriate thing from the dataframe you just saved."
  },
  {
    "objectID": "logistic-regression.html#go-away-and-dont-come-back",
    "href": "logistic-regression.html#go-away-and-dont-come-back",
    "title": "24  Logistic regression",
    "section": "24.10 Go away and don’t come back!",
    "text": "24.10 Go away and don’t come back!\nWhen a person has a heart attack and survives it, the major concern of health professionals is to prevent the person having a second heart attack. Two factors that are believed to be important are anger and anxiety; if a heart attack survivor tends to be angry or anxious, they are believed to put themselves at increased risk of a second heart attack.\nTwenty heart attack survivors took part in a study. Each one was given a test designed to assess their anxiety (a higher score on the test indicates higher anxiety), and some of the survivors took an anger management course. The data are in link; y and n denote “yes” and “no” respectively. The columns denote (i) whether or not the person had a second heart attack, (ii) whether or not the person took the anger management class, (iii) the anxiety score.\n\nRead in and display the data.\n* Fit a logistic regression predicting whether or not a heart attack survivor has a second heart attack, as it depends on anxiety score and whether or not the person took the anger management class. Display the results.\nIn the previous part, how can you tell that you were predicting the probability of having a second heart attack (as opposed to the probability of not having one)?\n* For the two possible values y and n of anger and the anxiety scores 40, 50 and 60, make a data frame containing all six combinations, and use it to obtain predicted probabilities of a second heart attack. Display your predicted probabilities side by side with what they are predictions for.\nUse your predictions from the previous part to describe the effect of changes in anxiety and anger on the probability of a second heart attack.\nAre the effects you described in the previous part consistent with the summary output from glm that you obtained in (here)? Explain briefly how they are, or are not. (You need an explanation for each of anxiety and anger, and you will probably get confused if you look at the P-values, so don’t.)\n\nMy solutions follow:"
  },
  {
    "objectID": "logistic-regression.html#finding-wolf-spiders-on-the-beach-1",
    "href": "logistic-regression.html#finding-wolf-spiders-on-the-beach-1",
    "title": "24  Logistic regression",
    "section": "24.11 Finding wolf spiders on the beach",
    "text": "24.11 Finding wolf spiders on the beach\n A team of Japanese researchers were investigating what would cause the burrowing wolf spider Lycosa ishikariana* to be found on a beach. They hypothesized that it had to do with the size of the grains of sand on the beach. They went to 28 beaches in Japan, measured the average sand grain size (in millimetres), and observed the presence or absence of this particular spider on each beach. The data are in link.\n\nWhy would logistic regression be better than regular regression in this case?\n\nSolution\nBecause the response variable, whether or not the spider is present, is a categorical yes/no success/failure kind of variable rather than a quantitative numerical one, and when you have this kind of response variable, this is when you want to use logistic regression.\n\\(\\blacksquare\\)\n\nRead in the data and check that you have something sensible. (Look at the data file first: the columns are aligned but the headers are not aligned with them.)\n\nSolution\nThe nature of the file means that you need read_table2: ::: {.cell}\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/spiders.txt\"\nspider &lt;- read_table2(my_url)\n\nWarning: `read_table2()` was deprecated in readr 2.0.0.\nℹ Please use `read_table()` instead.\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Grain.size = col_double(),\n  Spiders = col_character()\n)\n\nspider\n\n\n\n  \n\n\n:::\nWe have a numerical sand grain size and a categorical variable Spiders that indicates whether the spider was present or absent. As we were expecting. (The categorical variable is actually text or chr, which will matter in a minute.)\n\\(\\blacksquare\\)\n\nMake a boxplot of sand grain size according to whether the spider is present or absent. Does this suggest that sand grain size has something to do with presence or absence of the spider?\n\nSolution\n\nggplot(spider, aes(x = Spiders, y = Grain.size)) + geom_boxplot()\n\n\n\n\nThe story seems to be that when spiders are present, the sand grain size tends to be larger. So we would expect to find some kind of useful relationship in the logistic regression.\nNote that we have reversed the cause and effect here: in the boxplot we are asking “given that the spider is present or absent, how big are the grains of sand?”, whereas the logistic regression is asking “given the size of the grains of sand, how likely is the spider to be present?”. But if one variable has to do with the other, we would hope to see the link either way around.\n\\(\\blacksquare\\)\n\nFit a logistic regression predicting the presence or absence of spiders from the grain size, and obtain its summary. Note that you will need to do something with the response variable first.\n\nSolution\nThe presence or absence of spiders is our response. This is actually text at the moment: ::: {.cell}\nspider\n\n\n\n  \n\n\n:::\nso we need to make a factor version of it first. I’m going to live on the edge and overwrite everything:\n\nspider &lt;- spider %&gt;% mutate(Spiders = factor(Spiders))\nspider\n\n\n\n  \n\n\n\nSpiders is now a factor, correctly. (Sometimes a text variable gets treated as a factor, sometimes it needs to be an explicit factor. This is one of those times.) Now we go ahead and fit the model. I’m naming this as response-with-a-number, so I still have the Capital Letter. Any choice of name is good, though.\n\nSpiders.1 &lt;- glm(Spiders ~ Grain.size, family = \"binomial\", data = spider)\nsummary(Spiders.1)\n\n\nCall:\nglm(formula = Spiders ~ Grain.size, family = \"binomial\", data = spider)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)   -1.648      1.354  -1.217   0.2237  \nGrain.size     5.122      3.006   1.704   0.0884 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.165  on 27  degrees of freedom\nResidual deviance: 30.632  on 26  degrees of freedom\nAIC: 34.632\n\nNumber of Fisher Scoring iterations: 5\n\n\n\\(\\blacksquare\\)\n\nIs there a significant relationship between grain size and presence or absence of spiders at the \\(\\alpha=0.10\\) level? Explain briefly.\n\nSolution\nThe P-value on the Grain.size line is just less than 0.10 (it is 0.088), so there is just a significant relationship. It isn’t a very strong significance, though. This might be because we don’t have that much data: even though we have 28 observations, which, on the face of it, is not a very small sample size, each observation doesn’t tell us much: only whether the spider was found on that beach or not. Typical sample sizes in logistic regression are in the hundreds — the same as for opinion polls, because you’re dealing with the same kind of data. The weak significance here lends some kind of weak support to the researchers’ hypothesis, but I’m sure they were hoping for something better.\n\\(\\blacksquare\\)\n\nObtain predicted probabilities of spider presence for a representative collection of grain sizes. I only want predicted probabilities, not any kind of intervals.\n\nSolution\nMake a data frame of values to predict from directly, using tribble or for that matter datagrid. For some reason, I chose these four values:\n\nnew &lt;- tribble(\n  ~Grain.size,\n  0.2,\n  0.5,\n  0.8,\n  1.1\n)\nnew\n\n\n\n  \n\n\n\nand then\n\ncbind(predictions(Spiders.1, newdata = new))\n\n\n\n  \n\n\n\nOne of the above is all I need. I prefer the first one, since that way we don’t even have to decide what th e representative values are.\nExtra:\nNote that the probabilities don’t go up linearly. (If they did, they would soon get bigger than 1!). It’s actually the log-odds that go up linearly.\nTo verify this, you can add a type to the predictions:\n\ncbind(predictions(Spiders.1, newdata = new, type = \"link\"))\n\n\n\n  \n\n\n\nThis one, as you see shortly, makes more sense with equally-spaced grain sizes.\nThe meaning of that slope coefficient in the summary, which is about 5, is that if you increase grain size by 1, you increase the log-odds by 5. In the table above, we increased the grain size by 0.3 each time, so we would expect to increase the log-odds by \\((0.3)(5)=1.5\\), which is (to this accuracy) what happened.\nLog-odds are hard to interpret. Odds are a bit easier, and to get them, we have to exp the log-odds:\n\ncbind(predictions(Spiders.1, newdata = new, type = \"link\")) %&gt;% \n  mutate(exp_pred = exp(estimate))\n\n\n\n  \n\n\n\nThus, with each step of 0.3 in grain size, we multiply the odds of finding a spider by about\n\nexp(1.5)\n\n[1] 4.481689\n\n\nor about 4.5 times. If you’re a gambler, this might give you a feel for how large the effect of grain size is. Or, of course, you can just look at the probabilities.\n\\(\\blacksquare\\)\n\nGiven your previous work in this question, does the trend you see in your predicted probabilities surprise you? Explain briefly.\n\nSolution\nMy predicted probabilities go up as grain size goes up. This fails to surprise me for a couple of reasons: first, in my boxplots, I saw that grain size tended to be larger when spiders were present, and second, in my logistic regression summary, the slope was positive, so likewise spiders should be more likely as grain size goes up. Observing just one of these things is enough, though of course it’s nice if you can spot both.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#killing-aphids-1",
    "href": "logistic-regression.html#killing-aphids-1",
    "title": "24  Logistic regression",
    "section": "24.12 Killing aphids",
    "text": "24.12 Killing aphids\nAn experiment was designed to examine how well the insecticide rotenone kills aphids that feed on the chrysanthemum plant called Macrosiphoniella sanborni. The explanatory variable is the log concentration (in milligrams per litre) of the insecticide. At each of the five different concentrations, approximately 50 insects were exposed. The number of insects exposed at each concentration, and the number killed, are shown below.\n\nLog-Concentration   Insects exposed    Number killed   \n0.96                       50              6               \n1.33                       48              16              \n1.63                       46              24              \n2.04                       49              42              \n2.32                       50              44              \n\n\nGet these data into R. You can do this by copying the data into a file and reading that into R (creating a data frame), or you can enter the data manually into R using c, since there are not many values. In the latter case, you can create a data frame or not, as you wish. Demonstrate that you have the right data in R.\n\nSolution\nThere are a couple of ways. My current favourite is the tidyverse-approved tribble method. A tribble is a “transposed tibble”, in which you copy and paste the data, inserting column headings and commas in the right places. The columns don’t have to line up, since it’s the commas that determine where one value ends and the next one begins: ::: {.cell}\ndead_bugs &lt;- tribble(\n  ~log_conc, ~exposed, ~killed,\n  0.96, 50, 6,\n  1.33, 48, 16,\n  1.63, 46, 24,\n  2.04, 49, 42,\n  2.32, 50, 44\n)\ndead_bugs\n\n\n\n  \n\n\n:::\nNote that the last data value has no comma after it, but instead has the closing bracket of tribble.\nYou can have extra spaces if you wish. They will just be ignored. If you are clever in R Studio, you can insert a column of commas all at once (using “multiple cursors”). I used to do it like this. I make vectors of each column using c and then glue the columns together into a data frame: ::: {.cell}\nlog_conc &lt;- c(0.96, 1.33, 1.63, 2.04, 2.32)\nexposed &lt;- c(50, 48, 46, 49, 50)\nkilled &lt;- c(6, 16, 24, 42, 44)\ndead_bugs2 &lt;- tibble(log_conc, exposed, killed)\ndead_bugs2\n\n\n\n  \n\n\n:::\nThe values are correct — I checked them.\nNow you see why tribble stands for “transposed tibble”: if you want to construct a data frame by hand, you have to work with columns and then glue them together, but tribble allows you to work “row-wise” with the data as you would lay it out on the page.\nThe other obvious way to read the data values without typing them is to copy them into a file and read that. The values as laid out are aligned in columns. They might be separated by tabs, but they are not. (It’s hard to tell without investigating, though a tab is by default eight spaces and these values look farther apart than that.) I copied them into a file exposed.txt in my current folder (or use file.choose):\n\nbugs2 &lt;- read_table(\"exposed.txt\")\n\nWarning: Missing column names filled in: 'X6' [6]\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  `Log-Concentration` = col_double(),\n  Insects = col_double(),\n  exposed = col_double(),\n  Number = col_logical(),\n  killed = col_character(),\n  X6 = col_character()\n)\n\n\nWarning: 5 parsing failures.\nrow col  expected    actual          file\n  1  -- 6 columns 4 columns 'exposed.txt'\n  2  -- 6 columns 4 columns 'exposed.txt'\n  3  -- 6 columns 4 columns 'exposed.txt'\n  4  -- 6 columns 4 columns 'exposed.txt'\n  5  -- 6 columns 4 columns 'exposed.txt'\n\nbugs2\n\n\n\n  \n\n\n\nThis didn’t quite work: the last column Number killed got split into two, with the actual number killed landing up in Number and the column killed being empty. If you look at the data file, the data values for Number killed are actually aligned with the word Number, which is why it came out this way. Also, you’ll note, the column names have those “backticks” around them, because they contain illegal characters like a minus sign and spaces. Perhaps a good way to pre-empt3 all these problems is to make a copy of the data file with the illegal characters replaced by underscores, which is my file exposed2.txt:\n\nbugs2 &lt;- read_table(\"exposed2.txt\")\n\nWarning: Missing column names filled in: 'X4' [4]\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Log_Concentration = col_double(),\n  Insects_exposed = col_double(),\n  Number_killed = col_double(),\n  X4 = col_logical()\n)\n\nbugs2\n\n\n\n  \n\n\n\nThis is definitely good. We’d have to be careful with Capital Letters this way, but it’s definitely good.\nYou may have thought that this was a lot of fuss to make about reading in data, but the point is that data can come your way in lots of different forms, and you need to be able to handle whatever you receive so that you can do some analysis with it.\n\\(\\blacksquare\\)\n\n* Looking at the data, would you expect there to be a significant effect of log-concentration? Explain briefly.\n\nSolution\nThe numbers of insects killed goes up sharply as the concentration increases, while the numbers of insects exposed don’t change much. So I would expect to see a strong, positive effect of concentration, and I would expect it to be strongly significant, especially since we have almost 250 insects altogether.\n\\(\\blacksquare\\)\n\nWe are going to do a logistic regression to predict how likely an insect is to be killed, as it depends on the log-concentration. Create a suitable response variable, bearing in mind (i) that we have lots of insects exposed to each different concentration, and (ii) what needs to go into each column of the response.\n\nSolution\nThere needs to be a two-column response variable. The first column needs to be the number of “successes” (insects killed, here) and the second needs to be the number of “failures” (insects that survived). We don’t actually have the latter, but we know how many insects were exposed in total to each dose, so we can work it out. Like this: ::: {.cell}\ndead_bugs %&gt;%\n  mutate(survived = exposed - killed) %&gt;%\n  select(killed, survived) %&gt;%\n  as.matrix() -&gt; response\nresponse\n\n     killed survived\n[1,]      6       44\n[2,]     16       32\n[3,]     24       22\n[4,]     42        7\n[5,]     44        6\n\n:::\nglm requires an R matrix rather than a data frame, so the last stage of our pipeline is to create one (using the same numbers that are in the data frame: all the as. functions do is to change what type of thing it is, without changing its contents).\nIt’s also equally good to create the response outside of the data frame and use cbind to glue its columns together:\n\nresp2 &lt;- with(\n  dead_bugs,\n  cbind(killed, survived = exposed - killed)\n)\nresp2\n\n     killed survived\n[1,]      6       44\n[2,]     16       32\n[3,]     24       22\n[4,]     42        7\n[5,]     44        6\n\n\n\\(\\blacksquare\\)\n\nRun a suitable logistic regression, and obtain a summary of the results.\n\nSolution\nI think you know how to do this by now: ::: {.cell}\nbugs.1 &lt;- glm(response ~ log_conc, family = \"binomial\", data = dead_bugs)\nsummary(bugs.1)\n\n\nCall:\nglm(formula = response ~ log_conc, family = \"binomial\", data = dead_bugs)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -4.8923     0.6426  -7.613 2.67e-14 ***\nlog_conc      3.1088     0.3879   8.015 1.11e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 96.6881  on 4  degrees of freedom\nResidual deviance:  1.4542  on 3  degrees of freedom\nAIC: 24.675\n\nNumber of Fisher Scoring iterations: 4\n\n:::\n\\(\\blacksquare\\)\n\nDoes your analysis support your answer to (here)? Explain briefly.\n\nSolution\nThat’s a very small P-value, \\(1.1\\times 10^{-15}\\), on log_conc, so there is no doubt that concentration has an effect on an insect’s chances of being killed. This is exactly what I guessed in (here), which I did before looking at the results, honest!\n\\(\\blacksquare\\)\n\nObtain predicted probabilities of an insect’s being killed at each of the log-concentrations in the data set.\n\nSolution\nUse a newdata that is the original dataframe:\n\ncbind(predictions(bugs.1, newdata = dead_bugs))\n\n\n\n  \n\n\n\nThe advantage of this is that you can compare the observed with the predicted. For example, 44 out of 50 insects were killed at log-dose 2.32, which is a proportion of 0.88, pretty close to the prediction of 0.91.\nExtra: you could also make a plot of these. The normal thing is to use plot_cap, but this time, the response variable is that matrix thing outside the dataframe, which confuses the issue. So let’s make a more detailed set of predictions and plot them ourselves (effectively, doing the same thing plot_cap does but building it ourselves):\n\nnew &lt;- tibble(log_conc = seq(0.5, 2.5, 0.01))\ncbind(predictions(bugs.1, newdata = new)) %&gt;% \n  select(log_conc, estimate, conf.low, conf.high) %&gt;% \n  ggplot(aes(x = log_conc, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3)\n\n\n\n\nAs the log-concentration goes up, you can see how clearly the probability of the aphid being killed goes up. The confidence band is narrow because there is lots of data (almost 250 aphids altogether).\nThe final alpha = 0.3 makes the ribbon see-through, so that you can see the actual predictions behind it. A smaller value of alpha makes it more transparent, but I think this strikes a decent compromise between being able to clearly see both the predictions and the confidence limits.\n\\(\\blacksquare\\)\n\nPeople in this kind of work are often interested in the “median lethal dose”. In this case, that would be the log-concentration of the insecticide that kills half the insects. Based on your predictions, roughly what do you think the median lethal dose is?\n\nSolution\nThe log-concentration of 1.63 is predicted to kill just over half the insects, so the median lethal dose should be a bit less than 1.63. It should not be as small as 1.33, though, since that log-concentration only kills less than a third of the insects. So I would guess somewhere a bit bigger than 1.5. Any guess somewhere in this ballpark is fine: you really cannot be very accurate.\nIf you read through the Extra to the previous part (or at least looked at the graph), the median lethal dose is where the curve of predictions passes through 0.5 on the \\(y\\)-axis. This is at a log-concentration of just less than 1.6; if you judge from the scale where the crossing-point is, it’s something like 1.57.\nExtra: this is kind of a strange prediction problem, because we know what the response variable should be, and we want to know what the explanatory variable’s value is. Normally we do predictions the other way around.4 So the only way to get a more accurate figure is to try some different log-concentrations, and see which one gets closest to a probability 0.5 of killing the insect.\nSomething like this would work:\n\nnew &lt;- datagrid(model = bugs.1, log_conc = seq(1.5, 1.63, 0.01))\ncbind(predictions(bugs.1, newdata = new))\n\n\n\n  \n\n\n\nThe closest one of these to a probability of 0.5 is 0.4971, which goes with a log-concentration of 1.57: indeed, a bit bigger than 1.5 and a bit less than 1.63, and close to what I read off from my graph. The seq in the construction of the new data frame is “fill sequence”: go from 1.5 to 1.63 in steps of 0.01. We are predicting for values of log_conc that we chose, so the way to go is to make a new dataframe with datagrid and then feed that into predictions with newdata.\nNow, of course this is only our “best guess”, like a single-number prediction in regression. There is uncertainty attached to it (because the actual logistic regression might be different from the one we estimated), so we ought to provide a confidence interval for it. But I’m riding the bus as I type this, so I can’t look it up right now.\nLater: there’s a function called dose.p in MASS that appears to do this:\n\nlethal &lt;- dose.p(bugs.1)\nlethal\n\n             Dose         SE\np = 0.5: 1.573717 0.05159576\n\n\nWe have a sensible point estimate (the same 1.57 that we got by hand), and we have a standard error, so we can make a confidence interval by going up and down twice it (or 1.96 times it) from the estimate. The structure of the result is a bit arcane, though:\n\nstr(lethal)\n\n 'glm.dose' Named num 1.57\n - attr(*, \"names\")= chr \"p = 0.5:\"\n - attr(*, \"SE\")= num [1, 1] 0.0516\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr \"p = 0.5:\"\n  .. ..$ : NULL\n - attr(*, \"p\")= num 0.5\n\n\nIt is what R calls a “vector with attributes”. To get at the pieces and calculate the interval, we have to do something like this:\n\n(lethal_est &lt;- as.numeric(lethal))\n\n[1] 1.573717\n\n(lethal_SE &lt;- as.vector(attr(lethal, \"SE\")))\n\n[1] 0.05159576\n\n\nand then make the interval:\n\nlethal_est + c(-2, 2) * lethal_SE\n\n[1] 1.470526 1.676909\n\n\n1.47 to 1.68.\nI got this idea from page 4.14 of link. I think I got a little further than he did. An idea that works more generally is to get several intervals all at once, say for the “quartile lethal doses” as well:\n\nlethal &lt;- dose.p(bugs.1, p = c(0.25, 0.5, 0.75))\nlethal\n\n              Dose         SE\np = 0.25: 1.220327 0.07032465\np = 0.50: 1.573717 0.05159576\np = 0.75: 1.927108 0.06532356\n\n\nThis looks like a data frame or matrix, but is actually a “named vector”, so enframe will get at least some of this and turn it into a genuine data frame:\n\nenframe(lethal)\n\n\n\n  \n\n\n\nThat doesn’t get the SEs, so we’ll make a new column by grabbing the “attribute” as above:\n\nenframe(lethal) %&gt;% mutate(SE = attr(lethal, \"SE\"))\n\n\n\n  \n\n\n\nand now we make the intervals by making new columns containing the lower and upper limits:\n\nenframe(lethal) %&gt;%\n  mutate(SE = attr(lethal, \"SE\")) %&gt;%\n  mutate(LCL = value - 2 * SE, UCL = value + 2 * SE)\n\n\n\n  \n\n\n\nNow we have intervals for the median lethal dose, as well as for the doses that kill a quarter and three quarters of the aphids.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#the-effects-of-substance-a-1",
    "href": "logistic-regression.html#the-effects-of-substance-a-1",
    "title": "24  Logistic regression",
    "section": "24.13 The effects of Substance A",
    "text": "24.13 The effects of Substance A\nIn a dose-response experiment, animals (or cell cultures or human subjects) are exposed to some toxic substance, and we observe how many of them show some sort of response. In this experiment, a mysterious Substance A is exposed at various doses to 100 cells at each dose, and the number of cells at each dose that suffer damage is recorded. The doses were 10, 20, … 70 (mg), and the number of damaged cells out of 100 were respectively 10, 28, 53, 77, 91, 98, 99.\n\nFind a way to get these data into R, and show that you have managed to do so successfully.\n\nSolution\nThere’s not much data here, so we don’t need to create a file, although you can do so if you like (in the obvious way: type the doses and damaged cell numbers into a .txt file or spreadsheet and read in with the appropriate read_ function). Or, use a tribble:\n\ndr &lt;- tribble(\n  ~dose, ~damaged,\n  10, 10,\n  20, 28,\n  30, 53,\n  40, 77,\n  50, 91,\n  60, 98,\n  70, 99\n)\ndr\n\n\n\n  \n\n\n\nOr, make a data frame with the values typed in:\n\ndr2 &lt;- tibble(\n  dose = seq(10, 70, 10),\n  damaged = c(10, 28, 53, 77, 91, 98, 99)\n)\ndr2\n\n\n\n  \n\n\n\nseq fills a sequence “10 to 70 in steps of 10”, or you can just list the doses.\nI like this better than making two columns not attached to a data frame, but that will work as well.\nFor these, find a way you like, and stick with it.\n\\(\\blacksquare\\)\n\nWould you expect to see a significant effect of dose for these data? Explain briefly.\n\nSolution\nThe number of damaged cells goes up sharply as the dose goes up (from a very small number to almost all of them). So I’d expect to see a strongly significant effect of dose. This is far from something that would happen by chance.\n\\(\\blacksquare\\)\n\nFit a logistic regression modelling the probability of a cell being damaged as it depends on dose. Display the results. (Comment on them is coming later.)\n\nSolution\nThis has a bunch of observations at each dose (100 cells, in fact), so we need to do the two-column response thing: the successes in the first column and the failures in the second. This means that we first need to calculate the number of cells at each dose that were not damaged, by subtracting the number that were damaged from 100. R makes this easier than you’d think, as you see. A mutate is the way to go: create a new column in dr and save back in dr (because I like living on the edge):\n\ndr &lt;- dr %&gt;% mutate(undamaged = 100 - damaged)\ndr\n\n\n\n  \n\n\n\nThe programmer in you is probably complaining “but, 100 is a number and damaged is a vector of 7 numbers. How does R know to subtract each one from 100?” Well, R has what’s known as “recycling rules”: if you try to add or subtract (or elementwise multiply or divide) two vectors of different lengths, it recycles the smaller one by repeating it until it’s as long as the longer one. So rather than 100-damaged giving an error, it does what you want.5\nI took the risk of saving the new data frame over the old one. If it had failed for some reason, I could have started again.\nNow we have to make our response “matrix” with two columns, using cbind:\n\nresponse &lt;- with(dr, cbind(damaged, undamaged))\nresponse\n\n     damaged undamaged\n[1,]      10        90\n[2,]      28        72\n[3,]      53        47\n[4,]      77        23\n[5,]      91         9\n[6,]      98         2\n[7,]      99         1\n\n\nNote that each row adds up to 100, since there were 100 cells at each dose. This is actually trickier than it looks: the two things in cbind are columns (vectors), and cbind glues them together to make an R matrix:\n\nclass(response)\n\n[1] \"matrix\" \"array\" \n\n\nwhich is what glm needs here, even though it looks a lot like a data frame (which wouldn’t work here). This would be a data frame:\n\ndr %&gt;% select(damaged, undamaged) %&gt;% class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nand would therefore be the wrong thing to give glm. So I had to do it with cbind, or use some other trickery, like this:\n\ndr %&gt;%\n  select(damaged, undamaged) %&gt;%\n  as.matrix() -&gt; resp\nclass(resp)\n\n[1] \"matrix\" \"array\" \n\n\nNow we fit our model:\n\ncells.1 &lt;- glm(response ~ dose, family = \"binomial\", data = dr)\nsummary(cells.1)\n\n\nCall:\nglm(formula = response ~ dose, family = \"binomial\", data = dr)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.275364   0.278479  -11.76   &lt;2e-16 ***\ndose         0.113323   0.008315   13.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 384.13349  on 6  degrees of freedom\nResidual deviance:   0.50428  on 5  degrees of freedom\nAIC: 31.725\n\nNumber of Fisher Scoring iterations: 4\n\n\n\\(\\blacksquare\\)\n\nDoes your output indicate that the probability of damage really does increase with dose? (There are two things here: is there really an effect, and which way does it go?)\n\nSolution\nThe slope of dose is significantly nonzero (P-value less than \\(2.2 \\times 10^{-16}\\), which is as small as it can be). The slope itself is positive, which means that as dose goes up, the probability of damage goes up. That’s all I needed, but if you want to press on: the slope is 0.113, so an increase of 1 in dose goes with an increase of 0.113 in the log-odds of damage. Or it multiplies the odds of damage by \\(\\exp(0.113)\\). Since 0.113 is small, this is about \\(1.113\\) (mathematically, \\(e^x\\simeq 1+x\\) if \\(x\\) is small), so that the increase is about 11%. If you like, you can get a rough CI for the slope by going up and down twice its standard error (this is the usual approximately-normal thing). Here that would be ::: {.cell}\n0.113323 + c(-2, 2) * 0.008315\n\n[1] 0.096693 0.129953\n\n:::\nI thought about doing that in my head, but thought better of it, since I have R just sitting here. The interval is short (we have lots of data) and definitely does not contain zero.\n\\(\\blacksquare\\)\n\nObtain predicted damage probabilities for some representative doses.\n\nSolution\nPick some representative doses first, and then use them as newdata. The ones in the original data are fine (there are only seven of them):\n\np &lt;- cbind(predictions(cells.1, newdata = dr))\np\n\n\n\n  \n\n\n\nI saved mine to use again later, but you don’t have to unless you want to use your predictions again later.\n\\(\\blacksquare\\)\n\nDraw a graph of the predicted probabilities, and to that add the observed proportions of damage at each dose. Hints: you will have to calculate the observed proportions first. See here, near the bottom, to find out how to add data to one of these graphs. The geom_point line is the one you need.\n\nLooking at the predicted probabilities, would you say that the model fits well? Explain briefly.\nSolution\nThis ought to be based on plot_cap, but that doesn’t work here because of the response that’s not part of the dataframe. So we will be making this ourselves. Let’s start with a plot of the predictions, using the prediction we did just now:\n\nggplot(p, aes(x = dose, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3)\n\n\n\n\nThis is not a smooth curve like the ones plot_cap makes, but that’s all right, because we want to compare the predictions with the data.\nLet’s take a look at our original dataframe:\n\ndr\n\n\n\n  \n\n\n\nTo that we need to add a column of proportion damaged, which is damaged divided by damaged plus undamaged. This last ought to be 100, but data can go missing for any number of reasons, so it pays not to assume that they add up to 100 every time:\n\ndr %&gt;% mutate(prop = damaged / (damaged + undamaged)) -&gt; dr2\ndr2\n\n\n\n  \n\n\n\nCheck. I saved this to add to the graph later.\nNow you can add a geom_point with a data = and an aes, making the points red, except that the obvious doesn’t quite work:\n\nggplot(p, aes(x = dose, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) +\n  geom_point(data = dr2, aes(x =  dose, y = prop), colour = \"red\")\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 3rd layer.\nCaused by error:\n! object 'conf.low' not found\n\n\nThe message is not helpful, but I can tell you where it comes from. When you add something to a plot like this, all the things in the original ggplot are “inherited” by anything else that you add to the plot, so that you either have to overwrite them with something new (as I did with x and y) or you get the previous values, one of which was evidently conf.low. To override this behaviour, which we don’t want because we have nothing called conf.low in our data, add inherit.aes = FALSE to the geom_point:6\n\nggplot(p, aes(x = dose, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) +\n  geom_point(data = dr2, aes(x =  dose, y = prop), colour = \"red\", inherit.aes = FALSE)\n\n\n\n\nI also made the data points red (you don’t need to, but if you want to, put the colour outside the aes, to make it clear that the colour is not determined by any of the variables in your dataframe).\nThe predicted probabilities ought to be close to the observed proportions. They are in fact very close to them, so the model fits very well indeed.\nYour actual words are a judgement call, so precisely what you say doesn’t matter so much, but I think this model fit is actually closer than you could even hope to expect, it’s that good. But, your call. I think your answer ought to contain “close” or “fits well” at the very least.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#what-makes-an-animal-get-infected-1",
    "href": "logistic-regression.html#what-makes-an-animal-get-infected-1",
    "title": "24  Logistic regression",
    "section": "24.14 What makes an animal get infected?",
    "text": "24.14 What makes an animal get infected?\nSome animals got infected with a parasite. We are interested in whether the likelihood of infection depends on any of the age, weight and sex of the animals. The data are at link. The values are separated by tabs.\n\nRead in the data and take a look at the first few lines. Is this one animal per line, or are several animals with the same age, weight and sex (and infection status) combined onto one line? How can you tell?\n\nSolution\nThe usual beginnings, bearing in mind the data layout: ::: {.cell}\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/infection.txt\"\ninfect &lt;- read_tsv(my_url)\n\nRows: 81 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): infected, sex\ndbl (2): age, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ninfect\n\n\n\n  \n\n\n:::\nSuccess. This appears to be one animal per line, since there is no indication of frequency (of “how many”). If you were working as a consultant with somebody’s data, this would be a good thing to confirm with them before you went any further.\nYou can check a few more lines to convince yourself and the story is much the same. The other hint is that you have actual categories of response, which usually indicates one individual per row, but not always. If it doesn’t, you have some extra work to do to bash it into the right format.\nExtra: let’s see whether we can come up with an example of that. I’ll make a smaller example, and perhaps the place to start is “all possible combinations” of a few things. crossing is the same idea as datagrid, except that it doesn’t need a model, and so is better for building things from scratch:\n\nd &lt;- crossing(age = c(10, 12), gender = c(\"f\", \"m\"), infected = c(\"y\", \"n\"))\nd\n\n\n\n  \n\n\n\nThese might be one individual per row, or they might be more than one, as they would be if we have a column of frequencies:\n\nd &lt;- d %&gt;% mutate(freq = c(12, 19, 17, 11, 18, 26, 16, 8))\nd\n\n\n\n  \n\n\n\nNow, these are multiple observations per row (the presence of frequencies means there’s no doubt about that), but the format is wrong: infected is my response variable, and we want the frequencies of infected being y or n in separate columns — that is, we have to untidy the data a bit to make it suitable for modelling. This is pivot_wider, the opposite of pivot_longer:\n\nd %&gt;% pivot_wider(names_from=infected, values_from=freq)\n\n\n\n  \n\n\n\nNow you can pull out the columns y and n and make them into your response, and predict that from age and gender.\nThe moral of this story is that if you are going to have multiple observations per row, you probably want the combinations of explanatory variables one per row, but you want the categories of the response variable in separate columns.\nBack to where we were the rest of the way.\n\\(\\blacksquare\\)\n\n* Make suitable plots or summaries of infected against each of the other variables. (You’ll have to think about sex, um, you’ll have to think about the sex variable, because it too is categorical.) Anything sensible is OK here. You might like to think back to what we did in Question here for inspiration. (You can also investigate table, which does cross-tabulations.)\n\nSolution\nWhat comes to my mind for the numerical variables age and weight is boxplots:\n\nggplot(infect, aes(x = infected, y = age)) + geom_boxplot()\n\n\n\nggplot(infect, aes(x = infected, y = weight)) + geom_boxplot()\n\n\n\n\nThe variables sex and infected are both categorical. I guess a good plot for those would be some kind of grouped bar plot, which I have to think about. So let’s first try a numerical summary, a cross-tabulation, which is gotten via table:\n\nwith(infect, table(sex, infected))\n\n        infected\nsex      absent present\n  female     17      11\n  male       47       6\n\n\nOr, if you like the tidyverse:\n\ninfect %&gt;% count(sex, infected)\n\n\n\n  \n\n\n\nNow, bar plots. Let’s start with one variable. The basic bar plot has categories of a categorical variable along the \\(x\\)-axis and each bar is a count of how many observations were in that category. What is nice about geom_bar is that it will do the counting for you, so that the plot is just this:\n\nggplot(infect, aes(x = sex)) + geom_bar()\n\n\n\n\nThere are about twice as many males as females.\nYou may think that this looks like a histogram, which it almost does, but there is an important difference: the kind of variable on the \\(x\\)-axis. Here, it is a categorical variable, and you count how many observations fall in each category (at least, ggplot does). On a histogram, the \\(x\\)-axis variable is a continuous numerical one, like height or weight, and you have to chop it up into intervals (and then you count how many observations are in each chopped-up interval).\nTechnically, on a bar plot, the bars have a little gap between them (as here), whereas the histogram bars are right next to each other, because the right side of one histogram bar is the left side of the next.\nAll right, two categorical variables. The idea is that you have each bar divided into sub-bars based on the frequencies of a second variable, which is specified by fill. Here’s the basic idea:\n\nggplot(infect, aes(x = sex, fill = infected)) + geom_bar()\n\n\n\n\nThis is known in the business as a “stacked bar chart”. The issue is how much of each bar is blue, which is unnecessarily hard to judge because the male bar is taller. (Here, it is not so bad, because the amount of blue in the male bar is smaller and the bar is also taller. But we got lucky here.)\nThere are two ways to improve this. One is known as a “grouped bar chart”, which goes like this:\n\nggplot(infect, aes(x = sex, fill = infected)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\nThe absent and present frequencies for females are next to each other, and the same for males, and you can read off how big they are from the \\(y\\)-scale. This is my preferred graph for two (or more than two) categorical variables.\nYou could switch the roles of sex and infected and get a different chart, but one that conveys the same information. Try it. (The reason for doing it the way around I did is that being infected or not is the response and sex is explanatory, so that on my plot you can ask “out of the males, how many were infected?”, which is the way around that makes sense.)\nThe second way is to go back to stacked bars, but make them the same height, so you can compare the fractions of the bars that are each colour. This is position=\"fill\":\n\nggplot(infect, aes(x = sex, fill = infected)) +\n  geom_bar(position = \"fill\")\n\n\n\n\nThis also shows that more of the females were infected than the males, but without getting sidetracked into the issue that there were more males to begin with.\nI wrote this question in early 2017. At that time, I wrote:\n\nI learned about this one approximately two hours ago. I just ordered Hadley Wickham’s new book “R for Data Science” from Amazon, and it arrived today. It’s in there. (A good read, by the way. I’m thinking of using it as a recommended text next year.) As is so often the way with ggplot, the final answer looks very simple, but there is a lot of thinking required to get there, and you end up having even more respect for Hadley Wickham for the clarity of thinking that enabled this to be specified in a simple way.\n\n(end quote)\n\\(\\blacksquare\\)\n\nWhich, if any, of your explanatory variables appear to be related to infected? Explain briefly.\n\nSolution\nLet’s go through our output from (here). In terms of age, when infection is present, animals are (slightly) older. So there might be a small age effect. Next, when infection is present, weight is typically a lot less. So there ought to be a big weight effect. Finally, from the table, females are somewhere around 50-50 infected or not, but very few males are infected. So there ought to be a big sex effect as well. This also appears in the grouped bar plot, where the red (“absent”) bar for males is much taller than the blue (“present”) bar, but for females the two bars are almost the same height. So the story is that we would expect a significant effect of sex and weight, and maybe of age as well.\n\\(\\blacksquare\\)\n\nFit a logistic regression predicting infected from the other three variables. Display the summary.\n\nSolution\nThus: ::: {.cell}\ninfect.1 &lt;- glm(infected ~ age + weight + sex, family = \"binomial\", data = infect)\n\nError in eval(family$initialize): y values must be 0 &lt;= y &lt;= 1\n\n:::\nOh, I forgot to turn infected into a factor. This is the shortcut way to do that:\n\ninfect.1 &lt;- glm(factor(infected) ~ age + weight + sex, family = \"binomial\", data = infect)\nsummary(infect.1)\n\n\nCall:\nglm(formula = factor(infected) ~ age + weight + sex, family = \"binomial\", \n    data = infect)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.609369   0.803288   0.759 0.448096    \nage          0.012653   0.006772   1.868 0.061701 .  \nweight      -0.227912   0.068599  -3.322 0.000893 ***\nsexmale     -1.543444   0.685681  -2.251 0.024388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 83.234  on 80  degrees of freedom\nResidual deviance: 59.859  on 77  degrees of freedom\nAIC: 67.859\n\nNumber of Fisher Scoring iterations: 5\n\n\nOr you could create a new or redefined column in the data frame containing the factor version of infected, for example in this way:\n\ninfect %&gt;%\n  mutate(infected = factor(infected)) %&gt;%\n  glm(infected ~ age + weight + sex, family = \"binomial\", data = .) -&gt; infect.1a\nsummary(infect.1a)\n\n\nCall:\nglm(formula = infected ~ age + weight + sex, family = \"binomial\", \n    data = .)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.609369   0.803288   0.759 0.448096    \nage          0.012653   0.006772   1.868 0.061701 .  \nweight      -0.227912   0.068599  -3.322 0.000893 ***\nsexmale     -1.543444   0.685681  -2.251 0.024388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 83.234  on 80  degrees of freedom\nResidual deviance: 59.859  on 77  degrees of freedom\nAIC: 67.859\n\nNumber of Fisher Scoring iterations: 5\n\n\nEither way is good, and gives the same answer. The second way uses the data=. trick to ensure that the input data frame to glm is the output from the previous step, the one with the factor version of infected in it. The data=. is needed because glm requires a model formula first rather than a data frame (if the data were first, you could just omit it).\n\\(\\blacksquare\\)\n\n* Which variables, if any, would you consider removing from the model? Explain briefly.\n\nSolution\nThis is the same idea as in multiple regression: look at the end of the line for each variable to get its individual P-value, and if that’s not small, you can take that variable out. age has a P-value of 0.062, which is (just) larger than 0.05, so we can consider removing this variable. The other two P-values, 0.00089 and 0.024, are definitely less than 0.05, so those variables should stay.\nAlternatively, you can say that the P-value for age is small enough to be interesting, and therefore that age should stay. That’s fine, but then you need to be consistent in the next part.\nYou probably noted that sex is categorical. However, it has only the obvious two levels, and such a categorical variable can be assessed for significance this way. If you were worried about this, the right way to go is drop1:\n\ndrop1(infect.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe P-values are similar, but not identical.7\nI have to stop and think about this. There is a lot of theory that says there are several ways to do stuff in regression, but they are all identical. The theory doesn’t quite apply the same in generalized linear models (of which logistic regression is one): if you had an infinite sample size, the ways would all be identical, but in practice you’ll have a very finite amount of data, so they won’t agree.\nI’m thinking about my aims here: I want to decide whether each \\(x\\)-variable should stay in the model, and for that I want a test that expresses whether the model fits significantly worse if I take it out. The result I get ought to be the same as physically removing it and comparing the models with anova, eg. for age:\n\ninfect.1b &lt;- update(infect.1, . ~ . - age)\nanova(infect.1b, infect.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThis is the same thing as drop1 gives.\nSo, I think: use drop1 to assess whether anything should come out of a model like this, and use summary to obtain the slopes to interpret (in this kind of model, whether they’re positive or negative, and thus what kind of effect each explanatory variable has on the probability of whatever-it-is.)\n\\(\\blacksquare\\)\n\nAre the conclusions you drew in (here) and (here) consistent, or not? Explain briefly.\n\nSolution\nI think they are extremely consistent. When we looked at the plots, we said that weight and sex had large effects, and they came out definitely significant. There was a small difference in age between the infected and non-infected groups, and age came out borderline significant (with a P-value definitely larger than for the other variables, so that the evidence of its usefulness was weaker).\n\\(\\blacksquare\\)\n\n* The first and third quartiles of age are 26 and 130; the first and third quartiles of weight are 9 and 16. Obtain predicted probabilities for all combinations of these and sex. (You’ll need to start by making a new data frame, using datagrid to get all the combinations.)\n\nSolution\nHere’s how datagrid goes. Note my use of plural names to denote the things I want all combinations of:\n\nages &lt;- c(26, 130)\nweights &lt;- c(9, 16)\nsexes &lt;- c(\"female\", \"male\")\nnew &lt;- datagrid(model = infect.1, age = ages, weight = weights, sex = sexes)\nnew\n\n\n\n  \n\n\n\nThis is about on the upper end of what you would want to do just using the one line with datagrid in it and putting the actual values in instead of ages, weights etc. To my mind, once it gets longer than about this long, doing on one line starts to get unwieldy. But your taste might be different than mine.\nAside:\nI could have asked you to include some more values of age and weight, for example the median as well, to get a clearer picture. But that would have made infect.new bigger, so I stopped here. (If we had been happy with five representative values of age and weight, we could have done predictions (below) with variables and not had to make new at all.)\ndatagrid makes a data frame from input vectors, so it doesn’t matter if those are different lengths. In fact, it’s also possible to make this data frame from things like quartiles stored in a data frame. To do that, you wrap the whole datagrid in a with:\n\nd &lt;- tibble(age = ages, weight = weights, sex = sexes)\nd\n\n\n\n  \n\n\n\n\nnew &lt;- with(d, datagrid(model = infect.1, age=age, weight=weight, sex=sex))\nnew\n\n\n\n  \n\n\n\nThis one is a little confusing because in eg. age = age, the first age refers to the column that will be in new, and the second one refers to the values that are going in there, namely the column called age in the dataframe d.8\nEnd of aside.\nNext, the predictions:\n\ncbind(predictions(infect.1, new))\n\n\n\n  \n\n\n\nExtra: I didn’t ask you to comment on these, since the question is long enough already. But that’s not going to stop me!\nThese, in predicted, are predicted probabilities of infection.9\nThe way I remember the one-column-response thing is that the first level is the baseline (as it is in a regression with a categorical explanatory variable), and the second level is the one whose probability is modelled (in the same way that the second, third etc. levels of a categorical explanatory variable are the ones that appear in the summary table).\nLet’s start with sex. The probabilities of a female being infected are all much higher than of a corresponding male (with the same age and weight) being infected. Compare, for example, lines 1 and 2. Or 3 and 4. Etc. So sex has a big effect.\nWhat about weight? As weight goes from 9 to 16, with everything else the same, the predicted probability of infection goes sharply down. This is what we saw before: precisely, the boxplot showed us that infected animals were likely to be less heavy.\nLast, age. As age goes up, the probabilities go (somewhat) up as well. Compare, for example, lines 1 and 5 or lines 4 and 8. I think this is a less dramatic change than for the other variables, but that’s a judgement call.\nI got this example from (horrible URL warning) here: link It starts on page 275 in my edition. He goes at the analysis a different way, but he finishes with another issue that I want to show you.\nLet’s work out the residuals and plot them against our quantitative explanatory variables. I think the best way to do this is augment from broom, to create a data frame containing the residuals alongside the original data:\n\nlibrary(broom)\ninfect.1a &lt;- infect.1 %&gt;% augment(infect)\ninfect.1a %&gt;% as_tibble()\n\n\n\n  \n\n\nggplot(infect.1a, aes(x = weight, y = .resid)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\ninfect.1a is, I think, a genuine data.frame rather than a tibble.\nI don’t quite know what to make of that plot. It doesn’t look quite random, and yet there are just some groups of points rather than any real kind of trend.\nThe corresponding plot with age goes the same way:\n\nggplot(infect.1a, aes(x = age, y = .resid)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nCrawley found the slightest suggestion of an up-and-down curve in there. I’m not sure I agree, but that’s what he saw. As with a regular regression, the residuals against anything should look random, with no trends. (Though the residuals from a logistic regression can be kind of odd, because the response variable can only be 1 or 0.) Crawley tries adding squared terms to the logistic regression, which goes like this. The glm statement is long, as they usually are, so it’s much easier to use update:\n\ninfect.2 &lt;- update(infect.1, . ~ . + I(age^2) + I(weight^2))\n\nAs we saw before, when thinking about what to keep, we want to look at drop1:\n\ndrop1(infect.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe squared terms are both significant. The linear terms, age and weight, have to stay, regardless of their significance.10 What do the squared terms do to the predictions? Before, there was a clear one-directional trend in the relationships with age and weight. Has that changed? Let’s see. We’ll need a few more ages and weights to investigate with. I think the set of five representative values that comes out of predictions with variables would be ideal (and saves us having to make another new).\nLet’s start by assessing the effect of age:\n\nsummary(infect)\n\n   infected              age             weight          sex           \n Length:81          Min.   :  1.00   Min.   : 1.00   Length:81         \n Class :character   1st Qu.: 26.00   1st Qu.: 9.00   Class :character  \n Mode  :character   Median : 87.00   Median :13.00   Mode  :character  \n                    Mean   : 83.65   Mean   :11.49                     \n                    3rd Qu.:130.00   3rd Qu.:16.00                     \n                    Max.   :206.00   Max.   :18.00                     \n\nnew &lt;- datagrid(model = infect.2, age = c(1, 26, 84, 130, 206))\nnew\n\n\n\n  \n\n\ncbind(predictions(infect.2, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nThe actual values of age we chose are as shown. The other columns are constant; the values are the mean weight and the more common sex. We really can see the effect of age with all else constant.\nHere, the predicted infection probabilities go up with age and then come down again, as a squared term in age will allow them to do, compared to what we had before:\n\ncbind(predictions(infect.1, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nwhere the probabilities keep on going up.\nAll right, what about the effect of weight? Here’s the first model:\n\nnew &lt;- datagrid(model = infect.2, weight = c(1, 9, 13, 16, 18))\ncbind(predictions(infect.1, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nand here’s the second one with squared terms:\n\ncbind(predictions(infect.2, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nThis one is not so dissimilar: in the linear model, the predicted probabilities of infection start high and go down, but in the model with squared terms they go slightly up before going down.\nWe couldn’t have a squared term in sex, since there are only two sexes (in this data set), so the predictions might be pretty similar for the two models:\n\nnew &lt;- datagrid(model = infect.2, sex = c(\"female\", \"male\"))\ncbind(predictions(infect.1, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nand\n\ncbind(predictions(infect.2, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nThey are actually quite different. For the model with squared terms in age and weight, the predicted probabilities of infection are a lot higher for both males and females, at least at these (mean) ages and weights.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#the-brain-of-a-cat-1",
    "href": "logistic-regression.html#the-brain-of-a-cat-1",
    "title": "24  Logistic regression",
    "section": "24.15 The brain of a cat",
    "text": "24.15 The brain of a cat\nA large number (315) of psychology students were asked to imagine that they were serving on a university ethics committee hearing a complaint against animal research being done by a member of the faculty. The students were told that the surgery consisted of implanting a device called a cannula in each cat’s brain, through which chemicals were introduced into the brain and the cats were then given psychological tests. At the end of the study, the cats’ brains were subjected to histological analysis. The complaint asked that the researcher’s authorization to carry out the study should be withdrawn, and the cats should be handed over to the animal rights group that filed the complaint. It was suggested that the research could just as well be done with computer simulations.\nAll of the psychology students in the survey were told all of this. In addition, they read a statement by the researcher that no animal felt much pain at any time, and that computer simulation was not an adequate substitute for animal research. Each student was also given one of the following scenarios that explained the benefit of the research:\n\n“cosmetic”: testing the toxicity of chemicals to be used in new lines of hair care products.\n“theory”: evaluating two competing theories about the function of a particular nucleus in the brain.\n“meat”: testing a synthetic growth hormone said to potentially increase meat production.\n“veterinary”: attempting to find a cure for a brain disease that is killing domesticated cats and endangered species of wild cats.\n“medical”: evaluating a potential cure for a debilitating disease that afflicts many young adult humans.\n\nFinally, each student completed two questionnaires: one that would assess their “relativism”: whether or not they believe in universal moral principles (low score) or whether they believed that the appropriate action depends on the person and situation (high score). The second questionnaire assessed “idealism”: a high score reflects a belief that ethical behaviour will always lead to good consequences (and thus that if a behaviour leads to any bad consequences at all, it is unethical).11\nAfter being exposed to all of that, each student stated their decision about whether the research should continue or stop.\nI should perhaps stress at this point that no actual cats were harmed in the collection of these data (which can be found as a .csv file at link). The variables in the data set are these:\n\ndecision: whether the research should continue or stop (response)\nidealism: score on idealism questionnaire\nrelativism: score on relativism questionnaire\ngender of student\nscenario of research benefits that the student read.\n\nA more detailed discussion12 of this study is at link.\n\nRead in the data and check by looking at the structure of your data frame that you have something sensible. Do not call your data frame decision, since that’s the name of one of the variables in it.\n\nSolution\nSo, like this, using the name decide in my case:\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/decision.csv\"\ndecide &lt;- read_csv(my_url)\n\nRows: 315 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): decision, gender, scenario\ndbl (2): idealism, relativism\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndecide\n\n\n\n  \n\n\n\nThe variables are all the right things and of the right types: the decision, gender and the scenario are all text (representing categorical variables), and idealism and relativism, which were scores on a test, are quantitative (numerical). There are, as promised, 315 observations.\n\\(\\blacksquare\\)\n\nFit a logistic regression predicting decision from gender. Is there an effect of gender?\n\nSolution\nTurn the response into a factor somehow, either by creating a new variable in the data frame or like this:\n\ndecide.1 &lt;- glm(factor(decision) ~ gender, data = decide, family = \"binomial\")\nsummary(decide.1)\n\n\nCall:\nglm(formula = factor(decision) ~ gender, family = \"binomial\", \n    data = decide)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.8473     0.1543   5.491 3.99e-08 ***\ngenderMale   -1.2167     0.2445  -4.976 6.50e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 425.57  on 314  degrees of freedom\nResidual deviance: 399.91  on 313  degrees of freedom\nAIC: 403.91\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe P-value for gender is \\(6.5 \\times 10^{-7}\\), which is very small, so there is definitely an effect of gender. It’s not immediately clear what kind of effect it is: that’s the reason for the next part, and we’ll revisit this slope coefficient in a moment. Categorical explanatory variables are perfectly all right as text. Should I have used drop1 to assess the significance? Maybe:\n\ndrop1(decide.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe thing is, this gives us a P-value but not a slope, which we might have wanted to try to interpret. Also, the P-value in summary is so small that it is likely to be still significant in drop1 as well.\n\\(\\blacksquare\\)\n\nTo investigate the effect (or non-effect) of gender, create a contingency table by feeding decision and gender into table. What does this tell you?\n\nSolution\n\nwith(decide, table(decision, gender))\n\n          gender\ndecision   Female Male\n  continue     60   68\n  stop        140   47\n\n\nFemales are more likely to say that the study should stop (a clear majority), while males are more evenly split, with a small majority in favour of the study continuing.\nIf you want the column percents as well, you can use prop.table. Two steps: save the table from above into a variable, then feed that into prop.table, calling for column percentages rather than row percentages:\n\ntab &lt;- with(decide, table(decision, gender))\nprop.table(tab, 2)\n\n          gender\ndecision      Female      Male\n  continue 0.3000000 0.5913043\n  stop     0.7000000 0.4086957\n\n\nWhy column percentages? Well, thinking back to STAB22 or some such place, when one of your variables is acting like a response or outcome (decision here), make the percentages out of the other one. Given that a student is a female, how likely are they to call for the research to stop? The other way around makes less sense: given that a person wanted the research to stop, how likely are they to be female?\nAbout 70% of females and 40% of males want the research to stop. That’s a giant-sized difference. No wonder it was significant.\nThe other way of making the table is to use xtabs, with the same result:\n\nxtabs(~ decision + gender, data = decide)\n\n          gender\ndecision   Female Male\n  continue     60   68\n  stop        140   47\n\n\nIn this one, the frequency variable goes on the left side of the squiggle. We don’t have one here (each row of the data frame represents one student), so we leave the left side blank. I tried putting a . there, but that doesn’t work since there is no “whatever was there before” as there is, for example, in update.\n\\(\\blacksquare\\)\n\n* Is your slope for gender in the previous logistic regression positive or negative? Is it applying to males or to females? Looking at the conclusions from your contingency table, what probability does that mean your logistic regression is actually modelling?\n\nSolution\nMy slope is \\(-1.2167\\), negative, and it is attached to males (note that the slope is called gendermale: because “female” is before “male” alphabetically, females are used as the baseline and this slope says how males compare to them). This negative male coefficient means that the probability of whatever is being modelled is less for males than it is for females. Looking at the contingency table for the last part, the probability of “stop” should be less for males, so the logistic regression is actually modelling the probability of “stop”. Another way to reason that this must be the right answer is that the two values of decision are continue and stop; continue is first alphabetically, so it’s the baseline, and the other one, stop, is the one whose probability is being modelled. That’s why I made you do that contingency table. Another way to think about this is to do a prediction, which would go like this:\n\nnew &lt;- datagrid(model = decide.1, gender = levels(factor(decide$gender)))\nnew\n\n\n\n  \n\n\ncbind(predictions(decide.1, newdata = new))\n\n\n\n  \n\n\n\nTechnical note: we could simply supply the two genders in the definition of new, remembering to type the Capital Letters. The other way is to find out which genders there are in the data, and one way is to temporarily turn gender into a factor and then find out which different values it has, which are called “levels”.\nThe probability of whatever-it-is is exactly 70% for females and about 40% for males. A quick look at the contingency table shows that exactly 70% (\\(140/200\\)) of the females think the research should stop, and a bit less than 50% of the males think the same thing. So the model is predicting the probability of “stop”.\nThere’s a logic to this: it’s not just this way “because it is”. It’s the same idea of the first category, now of the response factor, being a “baseline”, and what actually gets modelled is the second category, relative to the baseline.\n\\(\\blacksquare\\)\n\nAdd the two variables idealism and relativism to your logistic regression. Do either or both of them add significantly to your model? Explain briefly.\n\nSolution\nThe obvious way of doing this is to type out the entire model, with the two new variables on the end. You have to remember to turn decision into a factor again: ::: {.cell}\ndecide.2 &lt;- glm(factor(decision) ~ gender + idealism + relativism,\n  data = decide, family = \"binomial\"\n)\nsummary(decide.2)\n\n\nCall:\nglm(formula = factor(decision) ~ gender + idealism + relativism, \n    family = \"binomial\", data = decide)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.4876     0.9787  -1.520  0.12849    \ngenderMale   -1.1710     0.2679  -4.372 1.23e-05 ***\nidealism      0.6893     0.1115   6.180 6.41e-10 ***\nrelativism   -0.3432     0.1245  -2.757  0.00584 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 425.57  on 314  degrees of freedom\nResidual deviance: 346.50  on 311  degrees of freedom\nAIC: 354.5\n\nNumber of Fisher Scoring iterations: 4\n\n:::\nThis is not so bad, copying and pasting. But the way I like better, when you’re making a smallish change to a longish model, is to use update:\n\ndecide.2 &lt;- update(decide.1, . ~ . + idealism + relativism)\nsummary(decide.2)\n\n\nCall:\nglm(formula = factor(decision) ~ gender + idealism + relativism, \n    family = \"binomial\", data = decide)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.4876     0.9787  -1.520  0.12849    \ngenderMale   -1.1710     0.2679  -4.372 1.23e-05 ***\nidealism      0.6893     0.1115   6.180 6.41e-10 ***\nrelativism   -0.3432     0.1245  -2.757  0.00584 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 425.57  on 314  degrees of freedom\nResidual deviance: 346.50  on 311  degrees of freedom\nAIC: 354.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nEither way is good. The conclusion you need to draw is that they both have something to add, because their P-values are both less than 0.05.\nOr (and perhaps better) you can look at drop1 of either of these:\n\ndrop1(decide.2, test = \"Chisq\")\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nAdd the variable scenario to your model. That is, fit a new model with that variable plus all the others.\n\nSolution\nTo my mind, update wins hands down here: ::: {.cell}\ndecide.3 &lt;- update(decide.2, . ~ . + scenario)\n:::\nYou can display the summary here if you like, but we’re not going to look at it yet.\n\\(\\blacksquare\\)\n\nUse anova to compare the models with and without scenario. You’ll have to add a test=\"Chisq\" to your anova, to make sure that the test gets done. Does scenario make a difference or not, at \\(\\alpha=0.10\\)? Explain briefly. (The reason we have to do it this way is that scenario is a factor with five levels, so it has four slope coefficients. To test them all at once, which is what we need to make an overall test for scenario, this is the way it has to be done.)\n\nSolution\nThese are the models that you fit in the last two parts: ::: {.cell}\nanova(decide.2, decide.3, test = \"Chisq\")\n\n\n\n  \n\n\n:::\nThe P-value is not less than 0.05, but it is less than 0.10, which is what I implied to assess it with, so the scenario does make some kind of difference.\nExtra: another way to do this, which I like better (but the anova way was what I asked in the original question), is to look at decide.3 and ask “what can I get rid of”, in such a way that categorical variables stay or go as a whole. This is done using drop1. It’s a little different from the corresponding thing in regression because the right way to do the test is not an F test, but now a chi-squared test (this is true for all generalized linear models of which logistic regression is one):\n\ndrop1(decide.3, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe test for scenario has four degrees of freedom (since there are five scenarios), and is in fact exactly the same test as in anova, significant at \\(\\alpha=0.10\\).\n\\(\\blacksquare\\)\n\nLook at the summary of your model that contained scenario. Bearing in mind that the slope coefficient for scenariocosmetic is zero (since this is the first scenario alphabetically), which scenarios have the most positive and most negative slope coefficients? What does that tell you about those scenarios’ effects?\n\nSolution\nAll right. This is the model I called decide.3: ::: {.cell}\nsummary(decide.3)\n\n\nCall:\nglm(formula = factor(decision) ~ gender + idealism + relativism + \n    scenario, family = \"binomial\", data = decide)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -1.5694     1.0426  -1.505   0.1322    \ngenderMale          -1.2551     0.2766  -4.537 5.70e-06 ***\nidealism             0.7012     0.1139   6.156 7.48e-10 ***\nrelativism          -0.3264     0.1267  -2.576   0.0100 *  \nscenariomeat         0.1565     0.4283   0.365   0.7149    \nscenariomedical     -0.7095     0.4202  -1.688   0.0914 .  \nscenariotheory       0.4501     0.4271   1.054   0.2919    \nscenarioveterinary  -0.1672     0.4159  -0.402   0.6878    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 425.57  on 314  degrees of freedom\nResidual deviance: 338.06  on 307  degrees of freedom\nAIC: 354.06\n\nNumber of Fisher Scoring iterations: 4\n\n:::\nThe most positive coefficient is for theory and the most negative one is for medical. (The zero coefficient is in the middle.) Since we are modelling the probability of saying that the research should stop (part (here)), this means that:\n\nthe “theory” scenario (evaluating theories about brain function) is most likely to lead to someone saying that the research should stop (other things being equal)\nthe “medical” scenario (finding a cure for a human disease) is most likely to lead to someone saying that the research should continue (or least likely to say that it should stop), again, other things being equal.\n\nThese make some kind of sense because being exposed to a scenario where there are tangible benefits later ought to be most favourable to the research continuing, and people are not going to be impressed by something that is “only theoretical” without any clear benefits.\nWe can also tackle this by doing some predictions. We want all the categories for scenario, and we might as well use average values for everything else:\n\nnew &lt;- datagrid(model = decide.3, scenario = levels(factor(decide$scenario)))\nnew\n\n\n\n  \n\n\ncbind(predictions(decide.3, newdata = new)) %&gt;% select(estimate, gender, idealism, relativism, scenario)\n\n\n\n  \n\n\n\nThe scenarios are over on the right, and the values of the other variables are the same all the way down (means for the quantitative ones, the most common category for the categorical ones). Having checked that this is indeed the case, we really only need the predictions and the scenarios.\nThis echoes what we found before: the probability of saying that the research should stop is highest for “theory” and the lowest for “medical”.\nI assumed in my model that the effect of the scenarios was the same for males and females. If I wanted to test that, I’d have to add an interaction and test that. This works most nicely using update and then anova, to fit the model with interaction and compare it with the model without:\n\ndecide.4 &lt;- update(decide.3, . ~ . + gender * scenario)\nanova(decide.3, decide.4, test = \"Chisq\")\n\n\n\n  \n\n\n\nNo evidence at all that the scenarios have different effects for the different genders. The appropriate predictions should show that too:\n\nnew &lt;- datagrid(model = decide.4, \n                gender = levels(factor(decide$gender)),\n                scenario = levels(factor(decide$scenario)))\ncbind(predictions(decide.4, newdata = new)) %&gt;% select(estimate, gender, idealism, relativism, scenario)\n\n\n\n  \n\n\n\nThe predicted probabilities that the experiment should be stopped are a lot lower for males than females across the board (this is the strongly significant gender effect). But, given that, the probability for medical is the lowest for both males and females, and the probability for theory is the highest for females and almost the highest for males. (The pattern for males and females is similar enough to suggest that there should be no interaction.)\nSo fitting an interaction was a waste of time, but it was worth checking whether it was.\n\\(\\blacksquare\\)\n\nDescribe the effects that having (i) a higher idealism score and (ii) a higher relativity score have on a person’s probability of saying that the research should stop. Do each of these increase or decrease that probability? Explain briefly.\n\nSolution\nLook back at the summary for the model that I called decide.3. (Or decide.2: the slope coefficients are very similar.) The one for idealism is positive, so that a higher idealism score goes with a greater likelihood of saying that the research should stop. The slope coefficient for relativity is negative, so it’s the other way around: a higher relativity score goes with a lower chance of saying that the research should stop. That’s all I needed, but as an extra, we can look back at the description of these scales in the question. The relativism one was that a person believed that the most moral action depends on the situation (as opposed to a person having something like religious faith that asserts universal moral principles that are always true. That would be a low score on the relativism scale). Somebody with a low score on this scale might believe something like “it is always wrong to experiment on animals”, whereas somebody with a high relativism score might say that it was sometimes justified. Thus, other things being equal, a low relativism score would go with “stop” and a high relativism score would (or might) go with “continue”. This would mean a negative slope coefficient for relativism, which is what we observed. (Are you still with me? There was some careful thinking there.)\nWhat about idealism? This is a belief that ethical behaviour will always lead to good consequences, and thus, if the consequences are bad, the behaviour must not have been ethical. A person who scores high on idealism is likely to look at the consequences (experimentation on animals), see that as a bad thing, and thus conclude that the research should be stopped. The idealism slope coefficient, by that argument, should be positive, and is.\nThis can also be done by a prediction. In the light of what we have done before, the suggestion is this. Idealism and relativism are quantitative, so let’s grab their quartiles, giving us \\(4 = 2 \\times 2\\) combinations:\n\nnew &lt;- datagrid(model = decide.3, \n                idealism = quantile(decide$idealism, c(0.25, 0.75)),\n                relativism = quantile(decide$relativism, c(0.25, 0.75)))\nnew\n\n\n\n  \n\n\ncbind(predictions(decide.3, newdata = new)) %&gt;% select(estimate, gender, idealism, relativism, scenario)\n\n\n\n  \n\n\n\nFor both of the idealism scores, the higher relativism score went with a lower probability of “stop” (the “negative” effect), and for both of the relativism scores, the higher idealism score went with a higher probability of “stop” (the positive effect).\nYet another way to assess this would be to make a graph:\n\nplot_cap(decide.3, condition = c(\"relativism\", \"idealism\"))\n\n\n\n\nThe story from here is the same: as relativism increases (move from left to right), the probability of stop decreases, but as idealism increases (from the red line up to the purple one), the probability of stop increases.\nThat’s quite enough discussion of the question, except that the data didn’t come to me in the form that you see them, so I figured I would like to share the story of the data processing as well. I think this is important because in your future work you are likely to spend a lot of your time getting data from how you receive it to something suitable for analysis.\nThese data came from a psychology study (with, probably, the students in a class serving as experimental subjects). Social scientists like to use SPSS software, so the data came to me as an SPSS .sav file.13 The least-fuss way of handling this that I could think of was to use import from the rio package, which I think I mentioned before:\n\nlibrary(rio)\nx &lt;- import(\"/home/ken/Downloads/Logistic.sav\")\nstr(x)\n\n'data.frame':   315 obs. of  11 variables:\n $ decision   : num  0 1 1 0 1 1 0 0 0 0 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n  ..- attr(*, \"labels\")= Named num [1:2] 0 1\n  .. ..- attr(*, \"names\")= chr [1:2] \"stop\" \"continue\"\n $ idealism   : num  8.2 6.8 8.2 7.4 1.7 5.6 7.2 7.8 7.8 8 ...\n  ..- attr(*, \"format.spss\")= chr \"F12.4\"\n $ relatvsm   : num  5.1 5.3 6 6.2 3.1 7.7 6.7 4 4.7 7.6 ...\n  ..- attr(*, \"format.spss\")= chr \"F12.4\"\n $ gender     : num  0 1 0 0 0 1 0 1 0 0 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n  ..- attr(*, \"labels\")= Named num [1:2] 0 1\n  .. ..- attr(*, \"names\")= chr [1:2] \"Female\" \"Male\"\n $ cosmetic   : num  1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n $ theory     : num  0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n $ meat       : num  0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n $ veterin    : num  0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n $ idealism_LN: num  2.104 1.917 2.104 2.001 0.531 ...\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n  ..- attr(*, \"display_width\")= int 13\n $ relatvsm_LN: num  1.63 1.67 1.79 1.82 1.13 ...\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n  ..- attr(*, \"display_width\")= int 13\n $ scenario   : num  1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n  ..- attr(*, \"display_width\")= int 10\n\n\nThe last line str displays the “structure” of the data frame that was obtained. Normally a data frame read into R has a much simpler structure than this, but this is R trying to interpret how SPSS does things. Here, each column (listed on the lines beginning with a dollar sign) has some values, listed after num; they are all numeric, even the categorical ones. What happened to the categorical variables is that they got turned into numbers, and they have a names “attribute” further down that says what those numbers actually represent. Thus, on the gender line, the subjects are a female (0), then a male (1), then three females, then a male, and so on. Variables like gender are thus so far neither really factors nor text variables, and so we’ll have to do a bit of processing before we can use them: we want to replace the numerical values by the appropriate “level”.\nTo turn a numeric variable into text depending on the value, we can use ifelse, but this gets unwieldy if there are more than two values to translate. For that kind of job, I think case_when is a lot easier to read. It also lets us have a catch-all for catching errors — “impossible” values occur distressingly often in real data:\n\nxx &lt;- x %&gt;%\n  mutate(\n    decision = case_when(\n      decision == 0 ~ \"stop\",\n      decision == 1 ~ \"continue\",\n      TRUE ~ \"error\"\n    ),\n    gender = case_when(\n      gender == 0 ~ \"Female\",\n      gender == 1 ~ \"Male\",\n      TRUE ~ \"error\"\n    ),\n    scenario = case_when(\n      scenario == 1 ~ \"cosmetic\",\n      scenario == 2 ~ \"theory\",\n      scenario == 3 ~ \"meat\",\n      scenario == 4 ~ \"veterinary\",\n      scenario == 5 ~ \"medical\",\n      TRUE ~ \"error\"\n    )\n  )\nxx %&gt;% as_tibble() %&gt;% select(-(cosmetic:veterin))\n\n\n\n  \n\n\n\nxx is a “real” data.frame (that’s what rio reads in), and has some extra columns that we don’t want to see right now.\nI have three new variables being created in one mutate. Each is being created using a case_when. The thing on the left of each squiggle is a logical condition being tested; the first of these logical conditions to come out TRUE provides the value for the new variable on the right of the squiggle. Thus, if the (old) scenario is 2, the new scenario will be theory. The TRUE lines in each case provide something that is guaranteed to be true, even if all the other lines are false (eg. if scenario is actually recorded as 7, which would be an error).\nI overwrote the old variable values with the new ones, which is a bit risky, but then I’d have more things to get rid of later.\nMy next step is to check that I don’t actually have any errors:\n\nxx %&gt;% count(scenario, gender, decision)\n\n\n\n  \n\n\n\nDon’t see any errors there.\nSo now let’s write what we have to a file. I think a .csv would be smart:\n\nxx %&gt;%\n  select(decision, idealism, relatvsm, gender, scenario) %&gt;%\n  write_csv(\"decision.csv\")\n\nThere is one more tiny detail: in SPSS, variable names can have a maximum of eight letters. “Relativism” has 10. So the original data file had the name “relativism” minus the two “i”s. I changed that so you would be dealing with a proper English word. (That change is not shown here.)\nThere is actually a town called Catbrain. It’s in England, near Bristol, and seems to be home to a street of car dealerships. One of the questions in the chapter on making maps asks you to find out where it is exactly.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#how-not-to-get-heart-disease-1",
    "href": "logistic-regression.html#how-not-to-get-heart-disease-1",
    "title": "24  Logistic regression",
    "section": "24.16 How not to get heart disease",
    "text": "24.16 How not to get heart disease\nWhat is associated with heart disease? In a study, a large number of variables were measured, as follows:\n\nage (years)\nsex male or female\npain.type Chest pain type (4 values: typical angina, atypical angina, non-anginal pain, asymptomatic)\nresting.bp Resting blood pressure, on admission to hospital\nserum.chol Serum cholesterol\nhigh.blood.sugar: greater than 120, yes or no\nelectro resting electrocardiographic results (normal, having ST-T, hypertrophy)\nmax.hr Maximum heart rate\nangina Exercise induced angina (yes or no)\noldpeak ST depression induced by exercise relative to rest. See link.\nslope Slope of peak exercise ST segment. Sloping up, flat or sloping down\ncolored number of major vessels (0–3) coloured by fluoroscopy\nthal normal, fixed defect, reversible defect\nheart.disease yes, no (response)\n\nI don’t know what most of those are, but we will not let that stand in our way. Our aim is to find out what variables are associated with heart disease, and what values of those variables give high probabilities of heart disease being present. The data are in link.\n\nRead in the data. Display the first few lines and convince yourself that those values are reasonable.\n\nSolution\nA .csv file, so: ::: {.cell}\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/heartf.csv\"\nheart &lt;- read_csv(my_url)\n\nNew names:\nRows: 270 Columns: 15\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(8): sex, pain.type, high.blood.sugar, electro, angina, slope, thal, hea... dbl\n(7): ...1, age, resting.bp, serum.chol, max.hr, oldpeak, colored\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nheart\n\n\n\n  \n\n\n:::\nYou should check that the variables that should be numbers actually are, that the variables that should be categorical have (as far as is shown) the right values as per my description above, and you should make some comment in that direction.\nMy variables appear to be correct, apart possibly for that variable X1 which is actually just the row number.\n\\(\\blacksquare\\)\n\nIn a logistic regression, what probability will be predicted here? Explain briefly but convincingly. (Is each line of the data file one observation or a summary of several?)\n\nSolution\nEach line of the data file is a single observation, not frequencies of yes and no (like the premature babies question, later, is). The response variable is a factor, so the first level is the baseline and the second level is the one predicted. R puts factor levels alphabetically, so no is first and yes is second. That is, a logistic regression will predict the probability that a person does have heart disease. I want to see that logic (which is why I said “convincingly”): one observation per line, and therefore that the second level of the factor is predicted, which is yes.\n\\(\\blacksquare\\)\n\n* Fit a logistic regression predicting heart disease from everything else (if you have a column called X or X1, ignore that), and display the results.\n\nSolution\nA lot of typing, since there are so many variables. Don’t forget that the response variable must be a factor: ::: {.cell}\nheart.1 &lt;- glm(factor(heart.disease) ~ age + sex + pain.type + resting.bp + serum.chol +\n  high.blood.sugar + electro + max.hr + angina + oldpeak + slope + colored + thal,\nfamily = \"binomial\", data = heart\n)\n:::\nYou can split this over several lines (and probably should), but make sure to end each line in such a way that there is unambiguously more to come, for example with a plus or a comma (though probably the fact that you have an unclosed bracket will be enough).\nThe output is rather lengthy:\n\nsummary(heart.1)\n\n\nCall:\nglm(formula = factor(heart.disease) ~ age + sex + pain.type + \n    resting.bp + serum.chol + high.blood.sugar + electro + max.hr + \n    angina + oldpeak + slope + colored + thal, family = \"binomial\", \n    data = heart)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -3.973837   3.133311  -1.268 0.204707    \nage                 -0.016007   0.026394  -0.606 0.544208    \nsexmale              1.763012   0.580761   3.036 0.002400 ** \npain.typeatypical   -0.997298   0.626233  -1.593 0.111264    \npain.typenonanginal -1.833394   0.520808  -3.520 0.000431 ***\npain.typetypical    -2.386128   0.756538  -3.154 0.001610 ** \nresting.bp           0.026004   0.012080   2.153 0.031346 *  \nserum.chol           0.006621   0.004228   1.566 0.117322    \nhigh.blood.sugaryes -0.370040   0.626396  -0.591 0.554692    \nelectronormal       -0.633593   0.412073  -1.538 0.124153    \nelectroSTT           0.013986   3.184512   0.004 0.996496    \nmax.hr              -0.019337   0.011486  -1.683 0.092278 .  \nanginayes            0.596869   0.460540   1.296 0.194968    \noldpeak              0.449245   0.244631   1.836 0.066295 .  \nslopeflat            0.827054   0.966139   0.856 0.391975    \nslopeupsloping      -0.122787   1.041666  -0.118 0.906166    \ncolored              1.199839   0.280947   4.271 1.95e-05 ***\nthalnormal           0.146197   0.845517   0.173 0.862723    \nthalreversible       1.577988   0.838550   1.882 0.059863 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 370.96  on 269  degrees of freedom\nResidual deviance: 168.90  on 251  degrees of freedom\nAIC: 206.9\n\nNumber of Fisher Scoring iterations: 6\n\n\nI didn’t ask you for further comment, but note that quite a lot of these variables are factors, so you get slopes for things like pain.typeatypical. When you have a factor in a model, there is a slope for each level except for the first, which is a baseline (and its slope is taken to be zero). That would be asymptomatic for pain.type. The \\(t\\)-tests for the other levels of pain.type say whether that level of pain type differs significantly (in terms of probability of heart disease) from the baseline level. Here, pain type atypical is not significantly different from the baseline, but the other two pain types, nonanginal and typical, are significantly different. If you think about this from an ANOVA-like point of view, the question about pain.type’s significance is really “is there at least one of the pain types that is different from the others”, and if we’re thinking about whether we should keep pain.type in the logistic regression, this is the kind of question we should be thinking about.\n\\(\\blacksquare\\)\n\nQuite a lot of our explanatory variables are factors. To assess whether the factor as a whole should stay or can be removed, looking at the slopes won’t help us very much (since they tell us whether the other levels of the factor differ from the baseline, which may not be a sensible comparison to make). To assess which variables are candidates to be removed, factors included (properly), we can use drop1. Feed drop1 a fitted model and the words test=\"Chisq\" (take care of the capitalization!) and you’ll get a list of P-values. Which variable is the one that you would remove first? Explain briefly.\n\nSolution\nFollowing the instructions:\n\ndrop1(heart.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe highest P-value, 0.5525, goes with high.blood.sugar, so this one comes out first. (The P-value for age is almost as high, 0.5427, so you might guess that this will be next.)\nYou might be curious about how these compare with the P-values on summary. These two P-values are almost the same as the ones on summary, because they are a two-level factor and a numeric variable respectively, and so the tests are equivalent in the two cases. (The P-values are not identical because the tests on summary and drop1 are the kind of thing that would be identical on a regular regression but are only “asymptotically the same” in logistic regression, so you’d expect them to be close without being the same, as here. “Asymptotically the same” means that if you had an infinitely large sample size, they’d be identical, but our sample size of 200-odd individuals is not infinitely large! Anyway, the largest P-value on the summary is 0.9965, which goes with electroSTT. electro, though, is a factor with three levels; this P-value says that STT is almost identical (in its effects on heart disease) with the baseline hypertrophy. But there is a third level, normal, which is a bit different from hypertrophy. So the factor electro overall has some effect on heart disease, which is reflected in the drop1 P-value of 0.12: this might go later, but it has to stay for now because at least one of its levels is different from the others in its effect on heart disease. (In backward elimination, multi-level factors are removed in their entirety if none of their levels have a different effect from any of the others.)\nThe power just went out here, so I am using my laptop on battery on its own screen, rather than on the big screen I have in my office, which is much better.\n\\(\\blacksquare\\)\n\nI’m not going to make you do the whole backward elimination (I’m going to have you use step for that later), but do one step: that is, fit a model removing the variable you think should be removed, using update, and then run drop1 again to see which variable will be removed next.\n\nSolution\nupdate is the obvious choice here, since we’re making a small change to a very big model: ::: {.cell}\nheart.2 &lt;- update(heart.1, . ~ . - high.blood.sugar)\ndrop1(heart.2, test = \"Chisq\")\n\n\n\n  \n\n\n:::\nThe power is back.\nThe next variable to go is indeed age, with a P-value that has hardly changed: it is now 0.5218.\n\\(\\blacksquare\\)\n\nUse step to do a backward elimination to find which variables have an effect on heart disease. Display your final model (which you can do by saving the output from step in a variable, and asking for the summary of that. In step, you’ll need to specify a starting model (the one from part (here)), the direction of elimination, and the test to display the P-value for (the same one as you used in drop1). (Note: the actual decision to keep or drop explanatory variables is based on AIC rather than the P-value, with the result that step will sometimes keep variables you would have dropped, with P-values around 0.10.)\n\nSolution\nThe hints ought to lead you to this: ::: {.cell}\nheart.3 &lt;- step(heart.1, direction = \"backward\", test = \"Chisq\")\n\nStart:  AIC=206.9\nfactor(heart.disease) ~ age + sex + pain.type + resting.bp + \n    serum.chol + high.blood.sugar + electro + max.hr + angina + \n    oldpeak + slope + colored + thal\n\n                   Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n- high.blood.sugar  1   169.25 205.25  0.3528 0.5525052    \n- age               1   169.27 205.27  0.3705 0.5427474    \n- electro           2   171.31 205.31  2.4119 0.2994126    \n- angina            1   170.55 206.55  1.6562 0.1981121    \n&lt;none&gt;                  168.90 206.90                      \n- slope             2   172.98 206.98  4.0844 0.1297422    \n- serum.chol        1   171.34 207.34  2.4484 0.1176468    \n- max.hr            1   171.84 207.84  2.9391 0.0864608 .  \n- oldpeak           1   172.44 208.44  3.5449 0.0597303 .  \n- resting.bp        1   173.78 209.78  4.8793 0.0271810 *  \n- thal              2   180.78 214.78 11.8809 0.0026308 ** \n- sex               1   179.16 215.16 10.2684 0.0013533 ** \n- pain.type         3   187.85 219.85 18.9557 0.0002792 ***\n- colored           1   191.78 227.78 22.8878 1.717e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=205.25\nfactor(heart.disease) ~ age + sex + pain.type + resting.bp + \n    serum.chol + electro + max.hr + angina + oldpeak + slope + \n    colored + thal\n\n             Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n- electro     2   171.65 203.65  2.3963  0.301750    \n- age         1   169.66 203.66  0.4104  0.521756    \n- angina      1   170.78 204.78  1.5323  0.215764    \n- slope       2   173.18 205.18  3.9288  0.140240    \n&lt;none&gt;            169.25 205.25                      \n- serum.chol  1   171.69 205.69  2.4458  0.117841    \n- max.hr      1   172.35 206.35  3.0969  0.078440 .  \n- oldpeak     1   173.26 207.26  4.0094  0.045248 *  \n- resting.bp  1   173.84 207.84  4.5942  0.032080 *  \n- sex         1   179.27 213.27 10.0229  0.001546 ** \n- thal        2   181.61 213.61 12.3588  0.002072 ** \n- pain.type   3   190.54 220.54 21.2943 9.145e-05 ***\n- colored     1   191.87 225.87 22.6232 1.971e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=203.65\nfactor(heart.disease) ~ age + sex + pain.type + resting.bp + \n    serum.chol + max.hr + angina + oldpeak + slope + colored + \n    thal\n\n             Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n- age         1   172.03 202.03  0.3894 0.5326108    \n- angina      1   173.13 203.13  1.4843 0.2231042    \n&lt;none&gt;            171.65 203.65                      \n- slope       2   175.99 203.99  4.3442 0.1139366    \n- max.hr      1   175.00 205.00  3.3560 0.0669599 .  \n- serum.chol  1   175.11 205.11  3.4610 0.0628319 .  \n- oldpeak     1   175.42 205.42  3.7710 0.0521485 .  \n- resting.bp  1   176.61 206.61  4.9639 0.0258824 *  \n- thal        2   182.91 210.91 11.2633 0.0035826 ** \n- sex         1   182.77 212.77 11.1221 0.0008531 ***\n- pain.type   3   192.83 218.83 21.1859 9.632e-05 ***\n- colored     1   194.90 224.90 23.2530 1.420e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=202.03\nfactor(heart.disease) ~ sex + pain.type + resting.bp + serum.chol + \n    max.hr + angina + oldpeak + slope + colored + thal\n\n             Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n- angina      1   173.57 201.57  1.5385 0.2148451    \n&lt;none&gt;            172.03 202.03                      \n- slope       2   176.33 202.33  4.2934 0.1168678    \n- max.hr      1   175.00 203.00  2.9696 0.0848415 .  \n- serum.chol  1   175.22 203.22  3.1865 0.0742492 .  \n- oldpeak     1   175.92 203.92  3.8856 0.0487018 *  \n- resting.bp  1   176.63 204.63  4.5911 0.0321391 *  \n- thal        2   183.38 209.38 11.3500 0.0034306 ** \n- sex         1   183.97 211.97 11.9388 0.0005498 ***\n- pain.type   3   193.71 217.71 21.6786 7.609e-05 ***\n- colored     1   195.73 223.73 23.6997 1.126e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=201.57\nfactor(heart.disease) ~ sex + pain.type + resting.bp + serum.chol + \n    max.hr + oldpeak + slope + colored + thal\n\n             Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;            173.57 201.57                      \n- slope       2   178.44 202.44  4.8672 0.0877201 .  \n- serum.chol  1   176.83 202.83  3.2557 0.0711768 .  \n- max.hr      1   177.52 203.52  3.9442 0.0470322 *  \n- oldpeak     1   177.79 203.79  4.2135 0.0401045 *  \n- resting.bp  1   178.56 204.56  4.9828 0.0256006 *  \n- thal        2   186.22 210.22 12.6423 0.0017978 ** \n- sex         1   185.88 211.88 12.3088 0.0004508 ***\n- pain.type   3   200.68 222.68 27.1025 5.603e-06 ***\n- colored     1   196.98 222.98 23.4109 1.308e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n:::\nThe output is very long. In terms of AIC, which is what step uses, age hangs on for a bit, but eventually gets eliminated.\nThere are a lot of variables left.\n\\(\\blacksquare\\)\n\nDisplay the summary of the model that came out of step.\n\nSolution\nThis:\n\nsummary(heart.3)\n\n\nCall:\nglm(formula = factor(heart.disease) ~ sex + pain.type + resting.bp + \n    serum.chol + max.hr + oldpeak + slope + colored + thal, family = \"binomial\", \n    data = heart)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -4.818418   2.550437  -1.889 0.058858 .  \nsexmale              1.850559   0.561583   3.295 0.000983 ***\npain.typeatypical   -1.268233   0.604488  -2.098 0.035903 *  \npain.typenonanginal -2.086204   0.486591  -4.287 1.81e-05 ***\npain.typetypical    -2.532340   0.748941  -3.381 0.000722 ***\nresting.bp           0.024125   0.011077   2.178 0.029410 *  \nserum.chol           0.007142   0.003941   1.812 0.069966 .  \nmax.hr              -0.020373   0.010585  -1.925 0.054262 .  \noldpeak              0.467028   0.233280   2.002 0.045284 *  \nslopeflat            0.859564   0.922749   0.932 0.351582    \nslopeupsloping      -0.165832   0.991474  -0.167 0.867167    \ncolored              1.134561   0.261547   4.338 1.44e-05 ***\nthalnormal           0.323543   0.813442   0.398 0.690818    \nthalreversible       1.700314   0.805127   2.112 0.034699 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 370.96  on 269  degrees of freedom\nResidual deviance: 173.57  on 256  degrees of freedom\nAIC: 201.57\n\nNumber of Fisher Scoring iterations: 6\n\n\nNot all of the P-values in the step output wound up being less than 0.05, but they are all at least reasonably small. As discussed above, some of the P-values in the summary are definitely not small, but they go with factors where there are significant effects somewhere. For example, thalnormal is not significant (that is, normal is not significantly different from the baseline fixed), but the other level reversible is different from fixed. You might be wondering about slope: on the summary there is nothing close to significance, but on the step output, slope has at least a reasonably small P-value of 0.088. This is because the significant difference does not involve the baseline: it’s actually between flat with a positive slope and upsloping with a negative one.\n\\(\\blacksquare\\)\n\nWe are going to make a large number of predictions. Create and save a data frame that contains predictions for all combinations of representative values for all the variables in the model that came out of step. By “representative” I mean all the values for a categorical variable, and the five-number summary for a numeric variable. (Note that you will get a lot of predictions.)\n\nSolution\nThe hard work is in listing all the variables. The easiest way to make sure you have them all is to look at the summary of your best model (mine was called heart.3) first, and copy them from the Call at the top. This is easier than looking at the table of Coefficients (or tidy output) because for categorical variables like pain.type you will have to distinguish the name of the variable from its levels. For example, the table of Coefficients has pain.typeatypical and pain.typenonanginal. Is it obvious to you where the variable name ends and its level begins?14\nAll right, let’s set up our dataframe to predict from. This needs the five-number summary of quantitative variables (via quantile), and the levels of the categorical ones (via levels(factor())). Take a deep breath and begin:\n\nnew &lt;- datagrid(model = heart.3, \n                sex = levels(factor(heart$sex)),\n                pain.type = levels(factor(heart$pain.type)),\n                resting.bp = quantile(heart$resting.bp),\n                serum.chol = quantile(heart$serum.chol),\n                max.hr = quantile(heart$max.hr),\n                oldpeak = quantile(heart$oldpeak),\n                slope = levels(factor(heart$slope)),\n                colored = quantile(heart$colored),\n                thal = levels(factor(heart$thal)))\nnew\n\n\n\n  \n\n\n\n\np &lt;- cbind(predictions(heart.3, newdata = new))\np\n\n\n\n  \n\n\n\nThere are a mere 108,000 rows here (and a fair few columns also). That is fine — as long as you don’t display them all for a grader to have to page through!\n\\(\\blacksquare\\)\n\nFind the largest predicted probability (which is the predicted probability of heart disease) and display all the variables that it was a prediction for.\n\nSolution\nThe (at current writing) approved way to do this is to use slice_max. This finds the rows with maximum value(s) on a variable, which is exactly what we want. It goes like this:\n\np %&gt;% slice_max(estimate, n = 1)\n\n\n\n  \n\n\n\nThe inputs to slice_max are the column whose maximum value you want, and the number of rows you want (so n = 3 would display the three rows with the highest predicted probabilities).\nVariations:\n\nby using prop instead of n, you can display the proportion of rows with the highest values on your variable, such as the 10% of rows with the highest predicted probabilities with prop = 0.10\nthere is also slice_min that displays the rows with the lowest values on a variable, or the input proportion of rows with the lowest values\nthere are alse slice_head and slice_tail that display the first and last (respectively) rows in a dataframe. The default display of a dataframe in an R Notebook is thus\n\n\np %&gt;% slice_head(n = 10)\n\n\n\n  \n\n\n\nexcept that the default display also tells you how many rows there are altogether.\n\nyou may have run into slice_sample, which displays a randomly-chosen number or proportion of rows from a dataframe. This is useful after you read in a dataframe from a file, if you want to get a sense of what kind of values you have in your dataframe (for example, if they are ordered by something and looking at the first ten rows won’t tell you the whole story, such as having males listed first and you want to check that there are some females as well):\n\n\np %&gt;% slice_sample(n = 10)\n\n\n\n  \n\n\n\nIf you didn’t think of slice_max, there are lots of other ways. Find one. Here are some examples:\n\np %&gt;% filter(estimate == max(estimate))\n\n\n\n  \n\n\n\nor if you didn’t think of that, you can find the maximum first, and then display the rows with predictions close to it:\n\np %&gt;% summarize(m = max(estimate))\n\n\n\n  \n\n\np %&gt;% filter(estimate &gt; 0.999998)\n\n\n\n  \n\n\n\nor even find which row has the maximum, and then display that row:\n\np %&gt;% summarize(row = which.max(estimate))\n\n\n\n  \n\n\np %&gt;% slice(67059)\n\n\n\n  \n\n\n\nor sort the rows by estimate, descending, and display the top few:\n\np %&gt;% arrange(desc(estimate)) %&gt;% slice(1:8)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nCompare the summary of the final model from step with your highest predicted heart disease probability and the values of the other variables that make it up. Are they consistent?\n\nSolution\nSince we were predicting the probability of heart disease, a more positive slope in the model from step will be associated with a higher probability of heart disease. So, there, we are looking for a couple of things: if the variable is a factor, we’re looking for the level with the most positive slope (bearing in mind that this might be the baseline), and for a numeric variable, if the slope is positive, a high value is associated with heart disease, and if negative, a low value. Bearing that in mind, we go back to my summary(heart.3):\n\nsummary(heart.3)\n\n\nCall:\nglm(formula = factor(heart.disease) ~ sex + pain.type + resting.bp + \n    serum.chol + max.hr + oldpeak + slope + colored + thal, family = \"binomial\", \n    data = heart)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -4.818418   2.550437  -1.889 0.058858 .  \nsexmale              1.850559   0.561583   3.295 0.000983 ***\npain.typeatypical   -1.268233   0.604488  -2.098 0.035903 *  \npain.typenonanginal -2.086204   0.486591  -4.287 1.81e-05 ***\npain.typetypical    -2.532340   0.748941  -3.381 0.000722 ***\nresting.bp           0.024125   0.011077   2.178 0.029410 *  \nserum.chol           0.007142   0.003941   1.812 0.069966 .  \nmax.hr              -0.020373   0.010585  -1.925 0.054262 .  \noldpeak              0.467028   0.233280   2.002 0.045284 *  \nslopeflat            0.859564   0.922749   0.932 0.351582    \nslopeupsloping      -0.165832   0.991474  -0.167 0.867167    \ncolored              1.134561   0.261547   4.338 1.44e-05 ***\nthalnormal           0.323543   0.813442   0.398 0.690818    \nthalreversible       1.700314   0.805127   2.112 0.034699 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 370.96  on 269  degrees of freedom\nResidual deviance: 173.57  on 256  degrees of freedom\nAIC: 201.57\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nsex: being male has the higher risk, by a lot\npain: all the slopes shown are negative, so the highest risk goes with the baseline one asymptomatic.\nresting.bp: positive slope, so higher risk with higher value.\nserum.chol: same.\nmax.hr: negative slope, so greatest risk with smaller value.\noldpeak: positive slope, greater risk with higher value again.\nslope: flat has greatest risk.\ncolored: positive slope, so beware of higher value.\nthal: reversible has greatest risk.\n\nThen we can do the same thing for the prediction.\nand the highest prediction:\n\np %&gt;% slice_max(estimate, n = 1)\n\n\n\n  \n\n\n\nFor the numerical variables, we may need to check back to the previous part to see whether the value shown was high or low. Once you have done that, you can see that the variable values for the highest predicted probability do indeed match the ones we thought should be the highest risk.\nExtra: the interesting thing about this is that even after adjusting for all of the other variables, there is a greater risk of heart disease if you are male (and the model shows that the risk is much greater). That is to say, it’s being male that makes the difference, not the fact that any of the other variables are different for males.\nIt’s rather difficult to scan 108,000 predictions to see the effect of being male, but we can do this:\n\nnew &lt;- datagrid(model = heart.3, sex = levels(factor(heart$sex)))\ncbind(predictions(heart.3, newdata = new))\n\n\n\n  \n\n\n\nWhat this does is to choose a single representative value for all the other variables: the mean for a quantitative variable like resting blood pressure, and the most common category for a categorical variable like pain.type. If you scan all the way along the two rows, you find that the values for all the variables are the same except for sex at the end. The predicted probabilities of heart disease are very different for males and females (much higher for males), especially given that all else really is equal.\nTo see this graphically, we can use plot_cap, and we can include another variable such as resting blood pressure. It’s better to list the quantitative one first:\n\nplot_cap(heart.3, condition = c(\"resting.bp\", \"sex\"))\n\n\n\n\nAs the resting blood pressure increases, the probability of heart disease increases, but the blue line for males is well above the red one for females all the way across. For example, for a 50% chance of heart disease, this will happen for males with a resting blood pressure of about 120, but for females not until the resting blood pressure reaches 190!\nPerhaps, therefore, the easiest way to avoid a heart attack is to not be male!\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#successful-breastfeeding-1",
    "href": "logistic-regression.html#successful-breastfeeding-1",
    "title": "24  Logistic regression",
    "section": "24.17 Successful breastfeeding",
    "text": "24.17 Successful breastfeeding\nA regular pregnancy lasts 40 weeks, and a baby that is born at or before 33 weeks is called “premature”. The number of weeks at which a baby is born is called its “gestational age”. Premature babies are usually smaller than normal and may require special care. It is a good sign if, when the mother and baby leave the hospital to go home, the baby is successfully breastfeeding.\nThe data in link are from a study of 64 premature infants. There are three columns: the gestational age (a whole number of weeks), the number of babies of that gestational age that were successfully breastfeeding when they left the hospital, and the number that were not. (There were multiple babies of the same gestational age, so the 64 babies are summarized in 6 rows of data.)\n\nRead the data into R and display the data frame.\n\nSolution\nNo great challenge here, I hope: ::: {.cell}\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/breastfeed.csv\"\nbreastfeed &lt;- read_csv(my_url)\n\nRows: 6 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): gest.age, bf.yes, bf.no\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbreastfeed\n\n\n\n  \n\n\n:::\nThat looks reasonable.\n\\(\\blacksquare\\)\n\nVerify that there were indeed 64 infants, by having R do a suitable calculation on your data frame that gives the right answer for the right reason.\n\nSolution\nThe second and third columns are all frequencies, so it’s a question of adding them up. For example:\n\nbreastfeed %&gt;% summarize(total = sum(bf.yes) + sum(bf.no))\n\n\n\n  \n\n\n\nor if you want to go nuts (this one pivot-longers all the frequencies together into one column and then adds them up):\n\nbreastfeed %&gt;%\n  pivot_longer(bf.yes:bf.no, names_to=\"yesno\", values_to=\"freq\") %&gt;%\n  summarize(total = sum(freq))\n\n\n\n  \n\n\n\nFind a way to get it done. If it works and it does the right thing, it’s good.\nDo not copy the numbers out of your data frame, type them in again and use R to add them up. Do something with your data frame as you read it in.\n\\(\\blacksquare\\)\n\nDo you think, looking at the data, that there is a relationship between gestational age and whether or not the baby was successfully breastfeeding when it left the hospital? Explain briefly.\n\nSolution\nThe babies with the youngest gestational age (the most premature) were mostly not breastfeeding when they left the hospital. Most of the 30- and 31-week babies were breastfeeding, and almost all of the 32- and 33-week babies were breastfeeding. So I think there will be a relationship: as gestational age increases, the probability that the baby will be breastfeeding will also increase. (This, looking ahead, suggests a positive slope in a logistic regression.)\n\\(\\blacksquare\\)\n\nWhy is logistic regression a sensible technique to use here? Explain briefly.\n\nSolution\nThe response variable is a yes/no: whether or not an infant is breastfeeding. We want to predict the probability of the response being in one or the other category. This is what logistic regression does. (The explanatory variable(s) are usually numerical, as here, but they could be factors as well, or a mixture. The key is the kind of response. The number of babies that are successfully breastfeeding at a certain gestational age is modelled as binomial with \\(n\\) being the total number of babies of that gestational age, and \\(p\\) being something that might depend, and here does depend, on gestational age.)\n\\(\\blacksquare\\)\n\nFit a logistic regression to predict the probability that an infant will be breastfeeding from its gestational age. Show the output from your logistic regression.\n\nSolution\nThese are summarized data, rather than one infant per line, so what we have to do is to make a two-column response “matrix”, successes in the first column and failures in the second, and then predict that from gestational age. (That’s why this was three marks rather than two.) So, let’s make the response first: ::: {.cell}\nresponse &lt;- with(breastfeed, cbind(bf.yes, bf.no))\nresponse\n\n     bf.yes bf.no\n[1,]      2     4\n[2,]      2     3\n[3,]      7     2\n[4,]      7     2\n[5,]     16     4\n[6,]     14     1\n\n:::\nor, more Tidyverse-like, but we have to remember to turn it into a matrix:\n\nresponse &lt;- breastfeed %&gt;%\n  select(starts_with(\"bf\")) %&gt;%\n  as.matrix()\nresponse\n\n     bf.yes bf.no\n[1,]      2     4\n[2,]      2     3\n[3,]      7     2\n[4,]      7     2\n[5,]     16     4\n[6,]     14     1\n\n\nI used a select-helper, because what immediately came to me was that the names of the columns I wanted started with bf, but whatever way you have that works is good. Now we fit the logistic regression:\n\nbreastfeed.1 &lt;- glm(response ~ gest.age, data = breastfeed, family = \"binomial\")\nsummary(breastfeed.1)\n\n\nCall:\nglm(formula = response ~ gest.age, family = \"binomial\", data = breastfeed)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -16.7198     6.0630  -2.758  0.00582 **\ngest.age      0.5769     0.1977   2.918  0.00352 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11.1772  on 5  degrees of freedom\nResidual deviance:  1.4968  on 4  degrees of freedom\nAIC: 19.556\n\nNumber of Fisher Scoring iterations: 4\n\n\n\\(\\blacksquare\\)\n\nDoes the significance or non-significance of the slope of gest.age surprise you? Explain briefly.\n\nSolution\nThe slope is significant (P-value 0.0035 is much less than 0.05). We said above that we expected there to be a relationship between gestational age and whether or not the baby was breastfeeding, and this significant slope is confirming that there is a relationship. So this is exactly what we expected to see, and not a surprise at all. If you concluded above that you did not see a relationship, you should colour yourself surprised here. Consistency.\n\\(\\blacksquare\\)\n\nIs your slope (in the Estimate column) positive or negative? What does that mean, in terms of gestational ages and breastfeeding? Explain briefly.\n\nSolution\nMy slope is 0.5769, positive. That means that as the explanatory variable, gestational age, increases, the probability of the event (that the baby is breastfeeding) also increases. This is also what I observed above: almost all of the near-term (large gestational age) babies were breastfeeding, whereas a fair few of the small-gestational-age (very premature) ones were not.\nWe know that the event is “is breastfeeding” rather than “is not”, because the first column of our response matrix is “yes” rather than “no”:\n\nresponse\n\n     bf.yes bf.no\n[1,]      2     4\n[2,]      2     3\n[3,]      7     2\n[4,]      7     2\n[5,]     16     4\n[6,]     14     1\n\n\n(If you had happened to make your response matrix the other way around, the event would have been “is not breastfeeding”, and then your slope would have been the same size but negative.)\n\\(\\blacksquare\\)\n\nObtain the predicted probabilities that an infant will successfully breastfeed for a representative collection of gestational ages.\n\nSolution\nPick some gestational ages, like the median and quartiles, or just pick some values like 25, 30, 35:15\n\nnew &lt;- datagrid(model = breastfeed.1, gest.age = c(25, 30, 35))\ncbind(predictions(breastfeed.1, newdata = new))\n\n\n\n  \n\n\n\nOr, if you wanted to make a graph of the observed and predicted proportions/probabilities, you would have to build it yourself since the response variable is not in the dataframe, like this:\n\nnew &lt;- datagrid(model = breastfeed.1, gest.age = 28:33)\np &lt;- cbind(predictions(breastfeed.1, new))\nggplot(p, aes(x = gest.age, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3)\n\n\n\n\nor, if you wanted to add the data to this:\n\nbreastfeed %&gt;% \n  mutate(total = bf.yes + bf.no, \n         proportion = bf.yes / total) -&gt; d\nggplot(p, aes(x = gest.age, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) +\n  geom_point(data = d, aes(x = gest.age, y = proportion, size = total), inherit.aes = FALSE)\n\n\n\n\nWhat did I do there? I first created some new variables: total is the total number of babies of each gestational age, and proportion is the observed proportion of breastfeeding babies at each gestational age (number of yes divided by total). pred which are the predictions we did above. Then I repeat the plot_cap, and on that plot I add the observed proportions against gestational age (as points). To do that, I need to change which dataframe I am using (the temporary one called d with the proportions and totals in it), and the variables I am plotting (which are now the ones in d). The way I do that is to put an aes inside the geom_line to say “use this x and y instead”. I also wanted to draw attention to the gestational ages where more babies were observed; I did this by making the size of the plotted points proportional to how many babies there were at that gestational age (which was the quantity total I calculated above).\nThe legend for total tells you what size point corresponds to how many total babies. The final thing is that we used some other things (like the min and max of the confidence interval) that we would also have to supply values for were it not for the inherit.aes at the end; this means “don’t use anything from the original ggplot but supply everything yourself”.\nThe idea is that the observed and predicted should be reasonably close, or at least not grossly different, especially when there is a lot of data (the circles are large), and I think they are close, which indicates that our model is doing a good job. You can see that there is not much data on the left, with small gestational ages, so the confidence interval around the predictions is wider there. On the right, where there is lots of data, the interval is narrower. When the gestational age is large (the baby is closer to being full term rather than premature), there is a good chance that the baby will be able to breastfeed, and we are fairly sure about that.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#making-it-over-the-mountains-1",
    "href": "logistic-regression.html#making-it-over-the-mountains-1",
    "title": "24  Logistic regression",
    "section": "24.18 Making it over the mountains",
    "text": "24.18 Making it over the mountains\nIn 1846, the Donner party (Donner and Reed families) left Springfield, Illinois for California in covered wagons. After reaching Fort Bridger, Wyoming, the leaders decided to find a new route to Sacramento. They became stranded in the eastern Sierra Nevada mountains at a place now called Donner Pass, when the region was hit by heavy snows in late October. By the time the survivors were rescued on April 21, 1847, 40 out of 87 had died.\nAfter the rescue, the age and gender of each person in the party was recorded, along with whether they survived or not. The data are in link.\n\nRead in the data and display its structure. Does that agree with the description in the previous paragraph?\n\nSolution\nNothing very new here: ::: {.cell}\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/donner.txt\"\ndonner &lt;- read_delim(my_url, \" \")\n\nRows: 45 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): gender, survived\ndbl (1): age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndonner\n\n\n\n  \n\n\n:::\nThe ages look like ages, and the other variables are categorical as promised, with the right levels. So this looks sensible.\nI usually write (or rewrite) the data description myself, but the one I found for these data at link was so nice that I used it as is. You can see that the data format used on that website is not as nice as ours. (I did some work to make it look nicer for you, with proper named categories rather than 0 and 1.)\n\\(\\blacksquare\\)\n\nMake graphical or numerical summaries for each pair of variables. That is, you should make a graph or numerical summary for each of age vs. gender, age vs.\nsurvived and gender vs. survived. In choosing the kind of graph or summary that you will use, bear in mind that survived and gender are factors with two levels.\n\nSolution\nThinking about graphs first: we have two numeric-vs-factor graphs (the ones involving age), which I think should be boxplots, though they could be something like side-by-side histograms (if you are willing to grapple with facet_grid). The other two variables are both factors, so a good graph for them would be something like a grouped bar plot (as in the question about parasites earlier). If you prefer numerical summaries: you could do mean age (or some other summary of age) for each group defined by gender or survivorship, and you could do a cross-tabulation of frequencies for gender and survival. I’ll take any mixture of graphs and numerical summaries that addresses each pair of variables somehow and summarizes them in a sensible way. Starting with age vs. gender: ::: {.cell}\nggplot(donner, aes(x = gender, y = age)) + geom_boxplot()\n\n\n\n:::\nor:\n\nggplot(donner, aes(x = age)) + geom_histogram(bins = 10) + facet_grid(gender ~ .)\n\n\n\n\nor:\n\ndonner %&gt;% group_by(gender) %&gt;% summarize(m = mean(age))\n\n\n\n  \n\n\n\nAge vs. survived is the same idea:\n\nggplot(donner, aes(x = survived, y = age)) + geom_boxplot()\n\n\n\n\nor:\n\nggplot(donner, aes(x = age)) + geom_histogram(bins = 10) + facet_grid(survived ~ .)\n\n\n\n\nor:\n\ndonner %&gt;% group_by(survived) %&gt;% summarize(m = mean(age))\n\n\n\n  \n\n\n\nFor survived against gender, the obvious thing is a cross-tabulation, gotten like this:\n\nwith(donner, table(gender, survived))\n\n        survived\ngender   no yes\n  female  5  10\n  male   20  10\n\n\nor like this:\n\ndonner %&gt;% group_by(gender, survived) %&gt;% summarize(n = n())\n\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nFor a graph, borrow the grouped bar-plot idea from the parasites question:\n\nggplot(donner, aes(x = gender, fill = survived)) + geom_bar(position = \"dodge\")\n\n\n\n\nI think this way around for x and fill is better, since we want to ask something like “out of the females, how many survived” (that is, gender is explanatory and survival response).\n\\(\\blacksquare\\)\n\nFor each of the three graphs or summaries in the previous question, what do they tell you about the relationship between the pair of variables concerned? Explain briefly.\n\nSolution\nI think the graphs are the easiest thing to interpret, but use whatever you got:\n\nThe females on average were younger than the males. (This was still true with the medians, even though those very old males might have pulled the mean up.)\nThe people who survived were on average younger than those who didn’t (or, the older people tended not to survive).\nA greater proportion of females survived than males.\n\nA relevant point about each relationship is good.\n\\(\\blacksquare\\)\n\nFit a logistic regression predicting survival from age and gender. Display the summary.\n\nSolution\nEach row of the data frame is one person, so we can use the survived column without going through that two-column response business. However, the response variable survived is a categorical variable expressed as text, so we need to make it into a factor first. Either create a new variable that is the factor version of survived, or do it right in the glm: ::: {.cell}\ndonner.1 &lt;- glm(factor(survived) ~ age + gender, family = \"binomial\", data = donner)\nsummary(donner.1)\n\n\nCall:\nglm(formula = factor(survived) ~ age + gender, family = \"binomial\", \n    data = donner)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  3.23041    1.38686   2.329   0.0198 *\nage         -0.07820    0.03728  -2.097   0.0359 *\ngendermale  -1.59729    0.75547  -2.114   0.0345 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 51.256  on 42  degrees of freedom\nAIC: 57.256\n\nNumber of Fisher Scoring iterations: 4\n\n:::\nWe ought to take a moment to think about what is being predicted here:\n\nlevels(factor(donner$survived))\n\n[1] \"no\"  \"yes\"\n\n\nThe baseline is the first of these, no, and the thing that is predicted is the probability of the second one, yes (that is, the probability of surviving).\n\\(\\blacksquare\\)\n\nDo both explanatory variables have an impact on survival? Does that seem to be consistent with your numerical or graphical summaries? Explain briefly.\n\nSolution\nBoth explanatory variables have a P-value (just) less than 0.05, so they both have an impact on survival: taking either one of them out of the logistic regression would be a mistake. To see if this makes sense, go back to your plots or summaries, the ones involving survival. For age, the mean or median age of the survivors was less than for the people who died, by five year (median) or eight years (mean), so it makes sense that there would be an age effect. For gender, two-thirds of the women survived and two-thirds of the men died, so there ought to be a gender effect and is.\n\\(\\blacksquare\\)\n\nAre the men typically older, younger or about the same age as the women? Considering this, explain carefully what the negative gendermale slope in your logistic regression means.\n\nSolution\nThe men are typically older than the women. The negative (and significant) gendermale slope means that the probability of a male surviving is less than that of a woman of the same age. Or, though the males are typically older, even after you allow for that, the males have worse survival. (genderfemale is the baseline.) You need to get at the idea of “all else equal” when you’re assessing regression slopes of any kind: regular regression, logistic regression or survival analysis (coming up later). That’s why I said “carefully” in the question. If I say “carefully” or “precisely”, a complete answer is looking for a specific thing to show that you understand the issue completely.\n\\(\\blacksquare\\)\n\nObtain predicted probabilities of survival for each combination of some representative ages and of the two genders in this dataset.\n\nSolution\nMake a dataframe containing some ages (you pick them) and the two genders, in combination, using datagrid:\n\nsummary(donner)\n\n      age          gender            survived        \n Min.   :15.0   Length:45          Length:45         \n 1st Qu.:24.0   Class :character   Class :character  \n Median :28.0   Mode  :character   Mode  :character  \n Mean   :31.8                                        \n 3rd Qu.:40.0                                        \n Max.   :65.0                                        \n\n\n\nnew &lt;- datagrid(model = donner.1, age = c(15, 25, 30, 40, 65), gender = c(\"female\", \"male\"))\nnew\n\n\n\n  \n\n\n\n\ncbind(predictions(donner.1, newdata = new))\n\n\n\n  \n\n\n\n(There are five ages chosen, something like a five-number summary, and two genders here, so ten predictions.)\nThese, remember, are predicted probabilities of surviving.\n\\(\\blacksquare\\)\n\nDo your predictions support your conclusions from earlier about the effects of age and gender? Explain briefly.\n\nSolution\nWe said before that the probability of survival was lower if the age was higher. This is confirmed here: for example, look at the odd-numbered rows 1, 3, 5, 7, 9, which are all females of increasing ages; the probability of survival decreases. (Or look at males, in the even-numbered rows; the effect is the same.)\nTo see the effect of gender, look at two predictions of different genders but the same age (eg. rows 1 and 2). The female is always predicted to have the higher survival probability. This is also what we saw before. The effect of gender is substantial, but not strongly significant, because we only have 45 observations, not so many when all we know about each person is whether they survived or not. I wanted you to think about these different ways to understand the model, and to understand that they all say the same thing, in different ways (and thus you can look at whichever of them is most convenient or comprehensible). For the logistic and survival models, I find looking at predictions to be the easiest way to understand what the model is saying.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#who-needs-the-most-intensive-care-1",
    "href": "logistic-regression.html#who-needs-the-most-intensive-care-1",
    "title": "24  Logistic regression",
    "section": "24.19 Who needs the most intensive care?",
    "text": "24.19 Who needs the most intensive care?\nThe “APACHE II” is a scale for assessing patients who arrive in the intensive care unit (ICU) of a hospital. These are seriously ill patients who may die despite the ICU’s best attempts. APACHE stands for “Acute Physiology And Chronic Health Evaluation”.16 The scale score is calculated from several physiological measurements such as body temperature, heart rate and the Glasgow coma scale, as well as the patient’s age. The final result is a score between 0 and 71, with a higher score indicating more severe health issues. Is it true that a patient with a higher APACHE II score has a higher probability of dying?\nData from one hospital are in link. The columns are: the APACHE II score, the total number of patients who had that score, and the number of patients with that score who died.\n\nRead in and display the data (however much of it displays). Why are you convinced that have the right thing?\n\nSolution\nData values separated by one space, so: ::: {.cell}\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/apache.txt\"\nicu &lt;- read_delim(my_url, \" \")\n\nRows: 38 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): apache, patients, deaths\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nicu\n\n\n\n  \n\n\n:::\nI had to stop and think about what to call the data frame, since one of the columns is called apache.\nAnyway, I appear to have an apache score between 0 and something, a number of patients and a number of deaths (that is no bigger than the number of patients). If you check the original data, the apache scores go up to 41 and are all the values except for a few near the end, so it makes perfect sense that there would be 38 rows.\nBasically, any comment here is good, as long as you make one and it has something to do with the data.\napache scores could be as high as 71, but I imagine a patient would have to be very ill to get a score anywhere near that high.\n\\(\\blacksquare\\)\n\nDoes each row of the data frame relate to one patient or sometimes to more than one? Explain briefly.\n\nSolution\nSometimes to more than one. The number in the patients column says how many patients that line refers to: that is to say (for example) the row where apache equals 6 represents all the patients whose apache score was 6, however many of them there were (14 in this case). I had to be careful with the wording because the first two rows of the data frame actually do refer to only one patient each (who survived in both cases), but the later rows do refer to more than one patient.\n\\(\\blacksquare\\)\n\nExplain why this is the kind of situation where you need a two-column response, and create this response variable, bearing in mind that I will (later) want you to estimate the probability of dying, given the apache score.\n\nSolution\nThis needs a two-column response precisely because each row represents (or could represent) more than one observation. The two columns are the number of observations referring to the event of interest (dying), and the number of observations where that didn’t happen (survived). We don’t actually have the numbers of survivals, but we can calculate these by subtracting from the numbers of patients (since a patient must have either lived or died): ::: {.cell}\nresponse &lt;- icu %&gt;%\n  mutate(survivals = patients - deaths) %&gt;%\n  select(deaths, survivals) %&gt;%\n  as.matrix()\nresponse\n\n      deaths survivals\n [1,]      0         1\n [2,]      0         1\n [3,]      1         3\n [4,]      0        11\n [5,]      3         6\n [6,]      3        11\n [7,]      4         8\n [8,]      5        17\n [9,]      3        30\n[10,]      6        13\n[11,]      5        26\n[12,]      5        12\n[13,]     13        19\n[14,]      7        18\n[15,]      7        11\n[16,]      8        16\n[17,]      8        19\n[18,]     13         6\n[19,]      7         8\n[20,]      6         7\n[21,]      9         8\n[22,]     12         2\n[23,]      7         6\n[24,]      8         3\n[25,]      8         4\n[26,]      2         4\n[27,]      5         2\n[28,]      1         2\n[29,]      4         3\n[30,]      4         1\n[31,]      3         0\n[32,]      3         0\n[33,]      1         0\n[34,]      1         0\n[35,]      1         0\n[36,]      1         0\n[37,]      1         0\n[38,]      0         1\n\n:::\nnoting that the deaths column has to come first since that’s what we want the probability of. It has to be a matrix, so as.matrix is the final step. You can quickly check that the two numbers in each row add up to the number of patients for that row.\nOr do everything outside of the data frame:\n\nsurvivals &lt;- with(icu, patients - deaths)\nresp &lt;- with(icu, cbind(deaths, survivals))\nresp\n\n      deaths survivals\n [1,]      0         1\n [2,]      0         1\n [3,]      1         3\n [4,]      0        11\n [5,]      3         6\n [6,]      3        11\n [7,]      4         8\n [8,]      5        17\n [9,]      3        30\n[10,]      6        13\n[11,]      5        26\n[12,]      5        12\n[13,]     13        19\n[14,]      7        18\n[15,]      7        11\n[16,]      8        16\n[17,]      8        19\n[18,]     13         6\n[19,]      7         8\n[20,]      6         7\n[21,]      9         8\n[22,]     12         2\n[23,]      7         6\n[24,]      8         3\n[25,]      8         4\n[26,]      2         4\n[27,]      5         2\n[28,]      1         2\n[29,]      4         3\n[30,]      4         1\n[31,]      3         0\n[32,]      3         0\n[33,]      1         0\n[34,]      1         0\n[35,]      1         0\n[36,]      1         0\n[37,]      1         0\n[38,]      0         1\n\nclass(resp)\n\n[1] \"matrix\" \"array\" \n\n\nOr use the dollar sign instead of the withs. Any of those is good.\nI have no objection to your displaying the response matrix.\n\\(\\blacksquare\\)\n\nFit a logistic regression to estimate the probability of death from the apache score, and display the results.\n\nSolution\n\napache.1 &lt;- glm(response ~ apache, family = \"binomial\", data = icu)\nsummary(apache.1)\n\n\nCall:\nglm(formula = response ~ apache, family = \"binomial\", data = icu)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.2903     0.2765  -8.282  &lt; 2e-16 ***\napache        0.1156     0.0160   7.227 4.94e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 106.009  on 37  degrees of freedom\nResidual deviance:  43.999  on 36  degrees of freedom\nAIC: 125.87\n\nNumber of Fisher Scoring iterations: 4\n\n\nMy naming convention has gotten messed up again. This should really be called deaths.1 or something like that, but that would be a really depressing name.\n\\(\\blacksquare\\)\n\nIs there a significant effect of apache score on the probability of survival? Explain briefly.\n\nSolution\nA gimme two points. The P-value for apache is \\(4.94 \\times 10^{-13}\\), very small, so apache score definitely has an effect on the probability of survival.\n\\(\\blacksquare\\)\n\nIs the effect of a larger apache score to increase or to decrease the probability of death? Explain briefly.\n\nSolution\nThe slope coefficient for apache is 0.1156, positive, and since we are modelling the probability of death (the first column of the response matrix), this says that as apache goes up, the probability of death goes up as well. If you made your response matrix with the columns the other way around, the slope coefficient for apache should be \\(-0.1156\\), but the explanation should come to the same place, because this says that the probability of survival goes down as apache goes up.\n\\(\\blacksquare\\)\n\nObtain the predicted probability of death for a representative collection of the apache scores that were in the data set. Do your predictions behave as you would expect (from your earlier work)?\n\nSolution\n“Representative” is a clue that you can choose more or less what values you like. The median and quartiles, or the five-number summary, or something near those, are reasonable choices:\n\nsummary(icu)\n\n     apache         patients         deaths      \n Min.   : 0.00   Min.   : 1.00   Min.   : 0.000  \n 1st Qu.:10.25   1st Qu.: 3.00   1st Qu.: 1.000  \n Median :19.50   Median :11.50   Median : 4.000  \n Mean   :19.55   Mean   :11.95   Mean   : 4.605  \n 3rd Qu.:28.75   3rd Qu.:17.75   3rd Qu.: 7.000  \n Max.   :41.00   Max.   :33.00   Max.   :13.000  \n\nnew &lt;- datagrid(model = apache.1, apache = c(10, 20, 30))\nnew\n\n\n\n  \n\n\n\n\ncbind(predictions(apache.1, newdata = new))\n\n\n\n  \n\n\n\nAs the apache score goes up, the predicted probability of death also goes up. This is what we would expect to see given the positive slope on apache in the model apache.1. (Again, if your model had the columns of the response the other way around, you were predicting the probability of survival, and your predictions should then show it going down rather than up to go with the negative slope you would then have had.)\n\\(\\blacksquare\\)\n\nMake a plot of predicted death probability against apache score (joined by lines) with, also on the plot, the observed proportion of deaths within each apache score, plotted against apache score. Does there seem to be good agreement between observation and prediction? Hint: calculate what you need to from the original dataframe first, save it, then make a plot of the predictions, and then to the plot add the appropriate thing from the dataframe you just saved.\n\nSolution\nAll right, following the hint, let’s start with the original dataframe, called icu. The plot of the predictions is going to show predicted probabilities of death, so from the data we need to find the observed proportions of death at each apache score. Our dataframe has one row for each apache score, so this is not too hard. We have the total number of patients at each score, and the number of deaths out of those patients, so the proportion that died is the second of those divided by the first one:\n\nicu %&gt;% mutate(proportion = deaths / patients) -&gt; d\nd\n\n\n\n  \n\n\n\nThat looks all right.\nTo plot predicted values, your first port of call would normally be plot_cap, since the job of this function is to make a lot of predictions and construct a nice plot of them, but this time we cannot since the response variable is a matrix and thus not part of the dataframe. So we have to make this ourselves. Let’s do the predictions again with more Apache scores, so that we get a better plot:\n\nnew &lt;- datagrid(model = apache.1, apache = 0:40)\ncbind(predictions(apache.1, newdata = new))\n\n\n\n  \n\n\n\nThen, make a graph of these:\n\ncbind(predictions(apache.1, newdata = new)) %&gt;% \n  ggplot(aes(x = apache, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3)\n\n\n\n\nThis again shows that the predicted probability of death goes up sharply with apache score. Let’s add the data to this graph. The trick is to remember how to add points to a graph you already have, when the points come from a different data set:\n\ncbind(predictions(apache.1, newdata = new)) %&gt;% \n  ggplot(aes(x = apache, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) + \n  geom_point(data = d, aes(x = apache, y = proportion), inherit.aes = FALSE)\n\n\n\n\nThat does a reasonably good job, but we can do better. We observe now that most of the points are reasonably close to the curve, except for that one in the bottom right, a patient who had the highest apache score of all, but who happened to survive. The observed proportions that are 1 or 0 over on the right might have been based on only one patient, and the others might have been based on more. So it would be better to encode how many patients each point is based on, for example by the size of the point (you may be able to think of other ways like colour, and you can experiment to see what gets the message across best in your opinion).\nA reminder that the inherit.aes is to tell ggplot not to take anything for the geom_point from the plot_cap graph (which has additional things that it would be a pain to override).\nAll right, we can make the points different sizes according to the number of patients they are based on, thus:\n\ncbind(predictions(apache.1, newdata = new)) %&gt;% \n  ggplot(aes(x = apache, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) + \n  geom_point(data = d, aes(x = apache, y = proportion, size = patients), \n             inherit.aes = FALSE)\n\n\n\n\nThis shows clearly that the points on the left and right of the graph are based on very few patients, maybe only one each time. But most of the scores between about 10 and 20 were represented by more patients, maybe up to 30. The bigger circles seem to follow the trend of the predictions pretty well, which is what we were hoping for; the proportions based on small numbers of patients might be further away from the predictions, and that’s all right.\nExtra: you can see that we want the proportions to be based on reasonable numbers of patients, but the other end of the issue is that we don’t want to combine patients with very different apache scores, because then you wouldn’t get much of a picture of how well the data follow the trend. This is very much the same idea as choosing the number of bins on a histogram; if you have too many bins, each one will contain very few observations and you won’t see the pattern very clearly, but if you have too few bins, you’ll have lots of observations in each bin but you still won’t see the shape.\nWith that in mind, perhaps we can try binning the observations in our data here. Let’s see what that does. The starting point is to redefine what I called d before which had the proportion in it:\n\nbreak_points &lt;- seq(-1, 45, 4)\nbreak_points\n\n [1] -1  3  7 11 15 19 23 27 31 35 39 43\n\nicu %&gt;% mutate(bins = cut(apache, breaks = break_points))\n\n\n\n  \n\n\n\nMy bins are 4 apache points wide. I have no idea yet whether that’s any good, but I want to get the procedure down so that I can come back and change that later if I want to.\nWhat I am using to make the bins is a base R function called cut. This makes categorical bins out of a quantitative variable (which is normally a bad idea, but we have our reasons here). It takes two inputs: a vector of numbers to bin, and the points that divide one bin from the next, which I defined into a vector called break_points first. The function cut defines bins as “half-open”, meaning that the top end is included but the bottom end is excluded. (Note for example which bin an apache score of 3 goes into.)\nNow we want to total up the patients and deaths within each bin, and, for reasons you’ll see later, we want to know where the middle of each bin is (for which I will use the median of the apache scores within that bin):\n\nicu %&gt;% mutate(bins = cut(apache, breaks = break_points)) %&gt;% \n  group_by(bins) %&gt;% \n  summarize(patients = sum(patients), deaths = sum(deaths), apache = median(apache))\n\n\n\n  \n\n\n\nWe have redefined the names: patients and deaths are now the totals of patients and deaths within each bin, and apache is now something like the middle apache score in each bin.\nNow we can work out the proportion of patients that died within each bin, and save that:\n\nicu %&gt;% mutate(bins = cut(apache, breaks = break_points)) %&gt;% \n  group_by(bins) %&gt;% \n  summarize(patients = sum(patients), deaths = sum(deaths), apache = median(apache)) %&gt;% \n  mutate(proportion = deaths / patients) -&gt; d\nd\n\n\n\n  \n\n\n\nNow we redo our graph, but using the proportions in here as the observed proportions of deaths in the data. You might now realize the reason for those values in the apache column: on the graph, the proportion values will be our \\(y\\) coordinates, but we needed something to be the \\(x\\) coordinates.\nActually redrawing our graph is in fact exactly the same code as before; the thing that has changed is our definition of d in it:\n\ncbind(predictions(apache.1, newdata = new)) %&gt;% \n  ggplot(aes(x = apache, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) + \n  geom_point(data = d, aes(x = apache, y = proportion, size = patients), \n             inherit.aes = FALSE)\n\n\n\n\nAnd now we see, with more patients per bin, that the agreement between data and prediction is very good apart from the very small bins on the right.\nOne modification that you might like to pursue is to have variable-width bins: wider bins at the extremes and narrower bins in the middle, so that each bin has about the same number of patients. You could do this by modifying what I called break_points so that the numbers in it are more spread out at the extremes and closer together in the middle.\nExtra extra: if you’re a football fan, you could imagine doing a very similar analysis by modelling the probability of successfully kicking a field goal as it depends on the distance it is attempted from. This would probably work pretty well for the NFL and (no doubt) for the CFL as well. In both these codes of football, field goals are always attempted from between two side-to-side marks on the field called “hash marks”. In the NFL, the hash marks are close together, so field goals are attempted from more or less directly in front of the goalposts. In the CFL, the hash marks are further apart, so a field goal might be attempted from off to the side of the goalposts, and then the probability of success might also depend on how far off to the side you are.\nIf you happen also to be a rugby fan, you’ll know that kicks at goal might need to be attempted from anywhere on the field, even out near the sidelines, and in that case, the chance of kicking a goal definitely depends on where on the field you are kicking it from as well as how far out it is.\nExtra extra extra: football (NFL and CFL) have an effective formality called the “point after”: after a team scores a touchdown, the kicker kicks at goal from right in front of the posts, which is almost never missed. In rugby, the equivalent to a touchdown is called a “try”, and to score a try the player must literally touch the ball down. (Thus, to a rugby fan, the NFL and CFL “touchdowns” are absurd misnomers.) After a try is scored, there is a kick at goal (called a “conversion”) which is not taken from right in front of the posts like in the NFL, but from the side-to-side position on the field where the try was scored. Thus, if a try is scored near the sidelines, as it often is, the conversion has to be kicked from near the sideline as well. The kicker can go as far back down the field as they like, but as you might imagine, a sideline conversion is still very difficult to kick.\nExtra-to-the-fourth: the reason for those names in rugby. In the old days, if a team scored a try and kicked the conversion as well, it was called a “goal”, and as a rugby player, you wanted your team to score a goal. Scoring a try allowed a team to “try” to “convert” it into a “goal” by kicking the conversion, hence the names. Because the NFL and CFL have their roots in rugby, they have the “point after” a touchdown as well (with the option now of allowing teams to try for two points after a touchdown as well).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#go-away-and-dont-come-back-1",
    "href": "logistic-regression.html#go-away-and-dont-come-back-1",
    "title": "24  Logistic regression",
    "section": "24.20 Go away and don’t come back!",
    "text": "24.20 Go away and don’t come back!\nWhen a person has a heart attack and survives it, the major concern of health professionals is to prevent the person having a second heart attack. Two factors that are believed to be important are anger and anxiety; if a heart attack survivor tends to be angry or anxious, they are believed to put themselves at increased risk of a second heart attack.\nTwenty heart attack survivors took part in a study. Each one was given a test designed to assess their anxiety (a higher score on the test indicates higher anxiety), and some of the survivors took an anger management course. The data are in link; y and n denote “yes” and “no” respectively. The columns denote (i) whether or not the person had a second heart attack, (ii) whether or not the person took the anger management class, (iii) the anxiety score.\n\nRead in and display the data.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ha2.txt\"\nha &lt;- read_delim(my_url, \" \")\n\nRows: 20 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): second, anger\ndbl (1): anxiety\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nha\n\n\n\n  \n\n\n\nThe anxiety scores are numbers; the other two variables are “yes” and “no”, which makes perfect sense.\n\\(\\blacksquare\\)\n\n* Fit a logistic regression predicting whether or not a heart attack survivor has a second heart attack, as it depends on anxiety score and whether or not the person took the anger management class. Display the results.\n\nSolution\nThis:\n\nha.1 &lt;- glm(factor(second) ~ anger + anxiety, family = \"binomial\", data = ha)\nsummary(ha.1)\n\n\nCall:\nglm(formula = factor(second) ~ anger + anxiety, family = \"binomial\", \n    data = ha)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -6.36347    3.21362  -1.980   0.0477 *\nangery      -1.02411    1.17101  -0.875   0.3818  \nanxiety      0.11904    0.05497   2.165   0.0304 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 27.726  on 19  degrees of freedom\nResidual deviance: 18.820  on 17  degrees of freedom\nAIC: 24.82\n\nNumber of Fisher Scoring iterations: 4\n\n\nThere’s no need to worry about two-column responses here, because each row of the data frame refers to only one person, and the response variable second is already a categorical variable with two categories (that needs to be turned into a factor for glm).\n\\(\\blacksquare\\)\n\nIn the previous part, how can you tell that you were predicting the probability of having a second heart attack (as opposed to the probability of not having one)?\n\nSolution\nThe levels of a factor are taken in alphabetical order, with n as the baseline, so we are predicting the probability of the second one y.\n\\(\\blacksquare\\)\n\n* For the two possible values y and n of anger and the anxiety scores 40, 50 and 60, make a data frame containing all six combinations, and use it to obtain predicted probabilities of a second heart attack. Display your predicted probabilities side by side with what they are predictions for.\n\nSolution\nThis time, I give you the values I want the predictions for (as opposed to calculating something like quartiles from the data), so we might as well just type them in.\nFirst step, datagrid:\n\nnew &lt;- datagrid(model = ha.1, anger = c(\"y\", \"n\"), anxiety = c(40, 50, 60))\nnew\n\n\n\n  \n\n\n\nThen, the predictions:\n\ncbind(predictions(ha.1, newdata = new))\n\n\n\n  \n\n\n\nExtra: In the help file for predictions, you will see it done in one step, with code like\npredictions(ha.1, newdata = datagrid(anger = c(\"y\", \"n\"), ...))\nThis saves constructing new first, but the code for datagrid is to my mind usually rather long to fit on one line inside the predictions(...).\n\\(\\blacksquare\\)\n\nUse your predictions from the previous part to describe the effect of changes in anxiety and anger on the probability of a second heart attack.\n\nSolution\nThe idea is to change one variable while leaving the other fixed. (This is the common refrain of “all else equal”.)\nPick a level of anger, say n (it doesn’t matter which) and look at the effect of anxiety. The probability of a second heart attack increases sharply from 0.17 to 0.40 to 0.69. So an increased anxiety score is associated with an increased probability of second heart attack (all else equal).\nTo assess the effect of taking the anger management course, pick an anxiety value, say 40, and compare the probabilities for anger n and y. For someone who has not taken the anger management course, the probability is 0.17, but for someone who has, it drops all the way to 0.07. (The pattern is the same at the other anxiety scores.)\nExtra: the reason it doesn’t matter what value of the other variable you look at (as long as you keep it fixed) is that the model is “additive” with no interaction, so that the effect of one variable does not depend on the value of the other one. If we wanted to see whether the effect of anxiety was different according to whether or not the person had taken the anger management course, we would add an interaction between anxiety and anger. But we won’t be doing this kind of thing until we get to analysis of variance, so you don’t need to worry about it yet.\n\\(\\blacksquare\\)\n\nAre the effects you described in the previous part consistent with the summary output from glm that you obtained in (here)? Explain briefly how they are, or are not. (You need an explanation for each of anxiety and anger, and you will probably get confused if you look at the P-values, so don’t.)\n\nSolution\nIn the previous part, we found that increased anxiety went with an increased probability of second heart attack. Back in (here), we found a positive slope of 0.11904 for anxiety, which also means that a higher anxiety score goes with a higher probability of second heart attack. That was not too hard. The other one is a little more work. anger is categorical with two categories n and y. The first one, n, is the baseline, so angery shows how y compares to n. The slope \\(-1.04211\\) is negative, which means that someone who has taken the anger management course has a lower probability of a second heart attack than someone who hasn’t (all else equal). This is the same story that we got from the predictions. That’s it for that, but I suppose I should talk about those P-values. anxiety is significant, so it definitely has an effect on the probability of a second heart attack. The pattern is clear enough even with this small data set. Here’s a visual: ::: {.cell}\nggplot(ha, aes(x = second, y = anxiety)) + geom_boxplot()\n\n\n\n:::\nThis is the wrong way around in terms of cause and effect, but it shows pretty clearly that people who suffer a second heart attack have a higher level of anxiety than those who don’t.\nThat’s clear enough, but what about anger? That is not significant, but there seems to be a visible effect of anger on the predicted probabilities of (here): there we saw that if you had done the anger management course, your probability of a second heart attack was lower. But that’s only the predicted probability, and there is also uncertainty about that, probably quite a lot because we don’t have much data. So if we were to think about confidence intervals for the predicted probabilities, they would be wide, and for the two levels of anger at a fixed anxiety they would almost certainly overlap.\nAnother way of seeing that is a visual, which would be a side-by-side bar chart:\n\nggplot(ha, aes(x = anger, fill = second)) + geom_bar(position = \"dodge\")\n\n\n\n\nA small majority of people who took the anger management did not have a second heart attack, while a small minority of those who did not, did.17 But with these small numbers, the difference, even though it points the way we would have guessed, is not large enough to be significant:\n\nwith(ha, table(anger, second))\n\n     second\nanger n y\n    n 4 7\n    y 6 3\n\n\nThis is not nearly far enough from an equal split to be significant. (It’s the same kind of idea as for Mood’s median test in C32/C33.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#footnotes",
    "href": "logistic-regression.html#footnotes",
    "title": "24  Logistic regression",
    "section": "",
    "text": "I get confused about the difference between morals and ethics. [This is a very short description of that difference:] (http://smallbusiness.chron.com/differences-between-ethical-issues-moral-issues-business-48134.html). The basic idea is that morals are part of who you are, derived from religion, philosophy etc. Ethics are how you act in a particular situation: that is, your morals, what you believe, inform your ethics, what you do. That’s why the students had to play the role of an ethics committee, rather than a morals committee; presumably the researcher had good morals, but an ethics committee had to evaluate what he was planning to do, rather than his character as a person.↩︎\nAs with many of these acronyms, you get the idea that the acronym came first and they devised some words to fit it.↩︎\nMy daughter learned the word pre-empt because we like to play a bridge app on my phone; in the game of bridge, you make a pre-emptive bid when you have no great strength but a lot of cards of one suit, say seven, and it won’t be too bad if that suit is trumps, no matter what your partner has. If you have a weakish hand with a lot of cards in one suit, your opponents are probably going to be able to bid and make something, so you pre-emptively bid first to try and make it difficult for them.↩︎\nThis kind of thing is sometimes called an inverse prediction.↩︎\nThe usual application of this is to combine a number with a vector. If you try to subtract a length-2 vector from a length-6 vector, R will repeat the shorter one three times and do the subtraction, but if you try to subtract a length-2 vector from a length-7 vector, where you’d have to repeat the shorter one a fractional number of times, R will do it, but you’ll get a warning, because this is probably not what you wanted. Try it and see.↩︎\nThis is done in the geom_point on the website, which is where I learned about it.↩︎\nThe test is this way because it’s a generalized linear model rather than a regular regression.↩︎\nSee, I told you it was a little confusing.↩︎\nWhen you have one observation per line, the predictions are of the second of the two levels of the response variable. When you make that two-column response, the predictions are of the probability of being in the first column. That’s what it is. As the young people say, don’t @ me.↩︎\nWhen you have higher-order terms, you have to keep the lower-order ones as well: higher powers, or interactions (as we see in ANOVA later).↩︎\nI get confused about the difference between morals and ethics. [This is a very short description of that difference:] (http://smallbusiness.chron.com/differences-between-ethical-issues-moral-issues-business-48134.html). The basic idea is that morals are part of who you are, derived from religion, philosophy etc. Ethics are how you act in a particular situation: that is, your morals, what you believe, inform your ethics, what you do. That’s why the students had to play the role of an ethics committee, rather than a morals committee; presumably the researcher had good morals, but an ethics committee had to evaluate what he was planning to do, rather than his character as a person.↩︎\nIf you can believe it.↩︎\nIf you took STAB23, you’ll have used PSPP, which is a free version of SPSS.↩︎\nIf it is for you, go right ahead, but for me it wasn’t.↩︎\nI realize I am extrapolating with these values.↩︎\nAs with many of these acronyms, you get the idea that the acronym came first and they devised some words to fit it.↩︎\nRead that carefully.↩︎"
  },
  {
    "objectID": "ordinal-response.html#do-you-like-your-mobile-phone",
    "href": "ordinal-response.html#do-you-like-your-mobile-phone",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.1 Do you like your mobile phone?",
    "text": "25.1 Do you like your mobile phone?\nA phone company commissioned a survey of their customers’ satisfaction with their mobile devices. The responses to the survey were on a so-called Likert scale of “very unsatisfied”, “unsatisfied”, “satisfied”, “very satisfied”. Also recorded were each customer’s gender and age group (under 18, 18–24, 25–30, 31 or older). (A survey of this kind does not ask its respondents for their exact age, only which age group they fall in.) The data, as frequencies of people falling into each category combination, are in link.\n\n* Read in the data and take a look at the format. Use a tool that you know about to arrange the frequencies in just one column, with other columns labelling the response categories that the frequencies belong to. Save the new data frame. (Take a look at it if you like.)\nWe are going to fit ordered logistic models below. To do that, we need our response variable to be a factor with its levels in the right order. By looking at the data frame you just created, determine what kind of thing your intended response variable currently is.\nIf your intended response variable is not a factor, create a factor in your data frame with levels in the right order. Hint: look at the order your levels are in the data.\n* Fit ordered logistic models to predict satisfaction from (i) gender and age group, (ii) gender only, (iii) age group only. (You don’t need to examine the models.) Don’t forget a suitable weights!\nUse drop1 on your model containing both explanatory variables to determine whether you can remove either of them. Use test=\"Chisq\" to obtain P-values.\nUse anova to decide whether we are justified in removing gender from a model containing both gender and age.group. Compare your P-value with the one from drop1.\nUse anova to see whether we are justified in removing age.group from a model containing both gender and age.group. Compare your P-value with the one from drop1 above.\nWhich of the models you have fit so far is the most appropriate one? Explain briefly.\nObtain predicted probabilities of a customer falling in the various satisfaction categories, as it depends on gender and age group. To do that, you need to feed predict three things: the fitted model that contains both age group and gender, the data frame that you read in from the file back in part (here) (which contains all the combinations of age group and gender), and an appropriate type.\n* Describe any patterns you see in the predictions, bearing in mind the significance or not of the explanatory variables."
  },
  {
    "objectID": "ordinal-response.html#finding-non-missing-values",
    "href": "ordinal-response.html#finding-non-missing-values",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.2 Finding non-missing values",
    "text": "25.2 Finding non-missing values\n* This is to prepare you for something in the next question. It’s meant to be easy.\nIn R, the code NA stands for “missing value” or “value not known”. In R, NA should not have quotes around it. (It is a special code, not a piece of text.)\n\nCreate a vector v that contains some numbers and some missing values, using c(). Put those values into a one-column data frame.\nObtain a new column containing is.na(v). When is this true and when is this false?\nThe symbol ! means “not” in R (and other programming languages). What does !is.na(v) do? Create a new column containing that.\nUse filter to display just the rows of your data frame that have a non-missing value of v."
  },
  {
    "objectID": "ordinal-response.html#high-school-and-beyond",
    "href": "ordinal-response.html#high-school-and-beyond",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.3 High School and Beyond",
    "text": "25.3 High School and Beyond\nA survey called High School and Beyond was given to a large number of American high school seniors (grade 12) by the National Center of Education Statistics. The data set at link is a random sample of 200 of those students.\nThe variables collected are:\n\ngender: student’s gender, female or male.\nrace: the student’s race (African-American, Asian,1 Hispanic, White).\nses: Socio-economic status of student’s family (low, middle, or high)\nschtyp: School type, public or private.\nprog: Student’s program, general, academic, or vocational.\nread: Score on standardized reading test.\nwrite: Score on standardized writing test.\nmath: Score on standardized math test.\nscience: Score on standardized science test.\nsocst: Score on standardized social studies test.\n\nOur aim is to see how socio-economic status is related to the other variables.\n\nRead in and display (some of) the data.\nExplain briefly why an ordinal logistic regression is appropriate for our aims.\nFit an ordinal logistic regression predicting socio-economic status from the scores on the five standardized tests. (You don’t need to display the results.) You will probably go wrong the first time. What kind of thing does your response variable have to be?\nRemove any non-significant explanatory variables one at a time. Use drop1 to decide which one to remove next.\nThe quartiles of the science test score are 44 and 58. The quartiles of the socst test score are 46 and 61. Make a data frame that has all combinations of those quartiles. If your best regression had any other explanatory variables in it, also put the means of those variables into this data frame.\nUse the data frame you created in the previous part, together with your best model, to obtain predicted probabilities of being in each ses category. Display these predicted probabilities so that they are easy to read.\nWhat is the effect of an increased science score on the likelihood of a student being in the different socioeconomic groups, all else equal? Explain briefly. In your explanation, state clearly how you are using your answer to the previous part."
  },
  {
    "objectID": "ordinal-response.html#how-do-you-like-your-steak",
    "href": "ordinal-response.html#how-do-you-like-your-steak",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.4 How do you like your steak?",
    "text": "25.4 How do you like your steak?\nWhen you order a steak in a restaurant, the server will ask you how you would like it cooked, or to be precise, how much you would like it cooked: rare (hardly cooked at all), through medium rare, medium, medium well to well (which means “well done”, so that the meat has only a little red to it). Could you guess how a person likes their steak cooked, from some other information about them? The website link commissioned a survey where they asked a number of people how they preferred their steak, along with as many other things as they could think of to ask. (Many of the variables below are related to risk-taking, which was something the people designing the survey thought might have something to do with liking steak rare.) The variables of interest are all factors or true/false:\n\nrespondent_ID: a ten-digit number identifying each person who responded to the survey.\nlottery_a: true if the respondent preferred lottery A with a small chance to win a lot of money, to lottery B, with a larger chance to win less money.\nsmoke: true if the respondent is currently a smoker\nalcohol: true if the respondent at least occasionally drinks alcohol.\ngamble: true if the respondent likes to gamble (eg. betting on horse racing or playing the lottery)\nskydiving: true if the respondent has ever been skydiving.\nspeed: true if the respondent likes to drive fast\ncheated: true if the respondent has ever cheated on a spouse or girlfriend/boyfriend\nsteak: true if the respondent likes to eat steak\nsteak_prep (response): how the respondent likes their steak cooked (factor, as described above, with 5 levels).\nfemale: true if the respondent is female\nage: age group, from 18–29 to 60+.\nhhold_income: household income group, from $0–24,999 to $150,000+.\neduc: highest level of education attained, from “less than high school” up to “graduate degree”\nregion: region (of the US) that the respondent lives in (five values).\n\nThe data are in link. This is the cleaned-up data from a previous question, with the missing values removed.\n\nRead in the data and display the first few lines.\nWe are going to predict steak_prep from some of the other variables. Why is the model-fitting function polr from package MASS the best choice for these data (alternatives being glm and multinom from package nnet)?\nWhat are the levels of steak_prep, in the order that R thinks they are in? If they are not in a sensible order, create an ordered factor where the levels are in a sensible order.\nFit a model predicting preferred steak preparation in an ordinal logistic regression from educ, female and lottery_a. This ought to be easy from your previous work, but you have to be careful about one thing. No need to print out the results.\nRun drop1 on your fitted model, with test=\"Chisq\". Which explanatory variable should be removed first, if any? Bear in mind that the variable with the smallest AIC should come out first, in case your table doesn’t get printed in order.\nRemove the variable that should come out first, using update. (If all the variables should stay, you can skip this part.)\nUsing the best model that you have so far, predict the probabilities of preferring each different steak preparation (method of cooking) for each combination of the variables that remain. (Some of the variables are TRUE and FALSE rather than factors. Bear this in mind.) Describe the effects of each variable on the predicted probabilities, if any. Note that there is exactly one person in the study whose educational level is “less than high school”.\nIs it reasonable to remove all the remaining explanatory variables from your best model so far? Fit a model with no explanatory variables, and do a test. (In R, if the right side of the squiggle is a 1, that means “just an intercept”. Or you can remove whatever remains using update.) What do you conclude? Explain briefly.\nIn the article for which these data were collected, link, does the author obtain consistent conclusions with yours? Explain briefly. (It’s not a very long article, so it won’t take you long to skim through, and the author’s point is pretty clear.)"
  },
  {
    "objectID": "ordinal-response.html#how-do-you-like-your-steak-the-data",
    "href": "ordinal-response.html#how-do-you-like-your-steak-the-data",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.5 How do you like your steak – the data",
    "text": "25.5 How do you like your steak – the data\n This question takes you through the data preparation for one of the other questions. You don’t have to do this* question, but you may find it interesting or useful.\nWhen you order a steak in a restaurant, the server will ask you how you would like it cooked, or to be precise, how much you would like it cooked: rare (hardly cooked at all), through medium rare, medium, medium well to well (which means “well done”, so that the meat has only a little red to it). Could you guess how a person likes their steak cooked, from some other information about them? The website link commissioned a survey where they asked a number of people how they preferred their steak, along with as many other things as they could think of to ask. (Many of the variables below are related to risk-taking, which was something the people designing the survey thought might have something to do with liking steak rare.) The variables of interest are all factors or true/false:\n\nrespondent_ID: a ten-digit number identifying each person who responded to the survey.\nlottery_a: true if the respondent preferred lottery A with a small chance to win a lot of money, to lottery B, with a larger chance to win less money.\nsmoke: true if the respondent is currently a smoker\nalcohol: true if the respondent at least occasionally drinks alcohol.\ngamble: true if the respondent likes to gamble (eg. betting on horse racing or playing the lottery)\nskydiving: true if the respondent has ever been skydiving.\nspeed: true if the respondent likes to drive fast\ncheated: true if the respondent has ever cheated on a spouse or girlfriend/boyfriend\nsteak: true if the respondent likes to eat steak\nsteak_prep (response): how the respondent likes their steak cooked (factor, as described above, with 5 levels).\nfemale: true if the respondent is female\nage: age group, from 18–29 to 60+.\nhhold_income: household income group, from $0–24,999 to $150,000+.\neduc: highest level of education attained, from “less than high school” up to “graduate degree”\nregion: region (of the US) that the respondent lives in (five values).\n\nThe data are in link.\n\nRead in the data and display the first few lines.\nWhat do you immediately notice about your data frame? Run summary on the entire data frame. Would you say you have a lot of missing values, or only a few?\nWhat does the function drop_na do when applied to a data frame with missing values? To find out, pass the data frame into drop_na(), then into summary again. What has happened?\nWrite the data into a .csv file, with a name like steak1.csv. Open this file in a spreadsheet and (quickly) verify that you have the right columns and no missing values.\n\nMy solutions follow:"
  },
  {
    "objectID": "ordinal-response.html#do-you-like-your-mobile-phone-1",
    "href": "ordinal-response.html#do-you-like-your-mobile-phone-1",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.6 Do you like your mobile phone?",
    "text": "25.6 Do you like your mobile phone?\nA phone company commissioned a survey of their customers’ satisfaction with their mobile devices. The responses to the survey were on a so-called Likert scale of “very unsatisfied”, “unsatisfied”, “satisfied”, “very satisfied”. Also recorded were each customer’s gender and age group (under 18, 18–24, 25–30, 31 or older). (A survey of this kind does not ask its respondents for their exact age, only which age group they fall in.) The data, as frequencies of people falling into each category combination, are in link.\n\n* Read in the data and take a look at the format. Use a tool that you know about to arrange the frequencies in just one column, with other columns labelling the response categories that the frequencies belong to. Save the new data frame. (Take a look at it if you like.)\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/mobile.txt\"\nmobile &lt;- read_delim(my_url, \" \")\n\nRows: 8 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): gender, age.group\ndbl (4): very.unsat, unsat, sat, very.sat\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmobile\n\n\n\n  \n\n\n\nWith multiple columns that are all frequencies, this is a job for pivot_longer: ::: {.cell}\nmobile %&gt;% \n  pivot_longer(very.unsat:very.sat, \n               names_to=\"satisfied\", \n               values_to=\"frequency\") -&gt; mobile.long\nmobile.long\n\n\n\n  \n\n\n:::\nYep, all good. See how mobile.long contains what it should? (For those keeping track, the original data frame had 8 rows and 4 columns to collect up, and the new one has \\(8\\times 4=32\\) rows.)\n\\(\\blacksquare\\)\n\nWe are going to fit ordered logistic models below. To do that, we need our response variable to be a factor with its levels in the right order. By looking at the data frame you just created, determine what kind of thing your intended response variable currently is.\n\nSolution\nI looked at mobile.long in the previous part, but if you didn’t, look at it here:\n\nmobile.long\n\n\n\n  \n\n\n\nMy intended response variable is what I called satisfied. This is chr or “text”, not the factor that I want.\n\\(\\blacksquare\\)\n\nIf your intended response variable is not a factor, create a factor in your data frame with levels in the right order. Hint: look at the order your levels are in the data.\n\nSolution\nMy intended response satisfied is text, not a factor, so I need to do this part. The hint is to look at the column satisfied in mobile.long and note that the satisfaction categories appear in the data in the order that we want. This is good news, because we can use fct_inorder like this: ::: {.cell}\nmobile.long %&gt;%\n  mutate(satis = fct_inorder(satisfied)) -&gt; mobile.long\n:::\nIf you check, by looking at the data frame, satis is a factor, and you can also do this to verify that its levels are in the right order:\n\nwith(mobile.long, levels(satis))\n\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n\n\nSuccess.\nExtra: so now you are asking, what if the levels are in the wrong order in the data? Well, below is what you used to have to do, and it will work for this as well. I’ll first find what levels of satisfaction I have. This can be done by counting them, or by finding the distinct ones: ::: {.cell}\nmobile.long %&gt;% count(satisfied)\n\n\n\n  \n\n\n:::\nor\n\nmobile.long %&gt;% distinct(satisfied)\n\n\n\n  \n\n\n\nIf you count them, they come out in alphabetical order. If you ask for the distinct ones, they come out in the order they were in mobile.long, which is the order the columns of those names were in mobile, which is the order we want.\nTo actually grab those satisfaction levels as a vector (that we will need in a minute), use pluck to pull the column out of the data frame as a vector:\n\nv1 &lt;- mobile.long %&gt;%\n  distinct(satisfied) %&gt;%\n  pluck(\"satisfied\")\nv1\n\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n\n\nwhich is in the correct order, or\n\nv2 &lt;- mobile.long %&gt;%\n  count(satisfied) %&gt;%\n  pluck(\"satisfied\")\nv2\n\n[1] \"sat\"        \"unsat\"      \"very.sat\"   \"very.unsat\"\n\n\nwhich is in alphabetical order. The problem with the second one is that we know the correct order, but there isn’t a good way to code that, so we have to rearrange it ourselves. The correct order from v2 is 4, 2, 1, 3, so:\n\nv3 &lt;- c(v2[4], v2[2], v2[1], v2[3])\nv3\n\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n\nv4 &lt;- v2[c(4, 2, 1, 3)]\nv4\n\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n\n\nEither of these will work. The first one is more typing, but is perhaps more obvious. There is a third way, which is to keep things as a data frame until the end, and use slice to pick out the rows in the right order:\n\nv5 &lt;- mobile.long %&gt;%\n  count(satisfied) %&gt;%\n  slice(c(4, 2, 1, 3)) %&gt;%\n  pluck(\"satisfied\")\nv5\n\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n\n\nIf you don’t see how that works, run it yourself, one line at a time.\nThe other way of doing this is to physically type them into a vector, but this carries the usual warnings of requiring you to be very careful and that it won’t be reproducible (eg. if you do another survey with different response categories).\nSo now create the proper response variable thus, using your vector of categories:\n\nmobile.long %&gt;%\n  mutate(satis = ordered(satisfied, v1)) -&gt; mobile.long2\nmobile.long2\n\n\n\n  \n\n\n\nsatis has the same values as satisfied, but its label ord means that it is an ordered factor, as we want.\n\\(\\blacksquare\\)\n\n* Fit ordered logistic models to predict satisfaction from (i) gender and age group, (ii) gender only, (iii) age group only. (You don’t need to examine the models.) Don’t forget a suitable weights!\n\nSolution\n(i):\n\nlibrary(MASS)\nmobile.1 &lt;- polr(satis ~ gender + age.group, weights = frequency, data = mobile.long)\n\nFor (ii) and (iii), update is the thing (it works for any kind of model):\n\nmobile.2 &lt;- update(mobile.1, . ~ . - age.group)\nmobile.3 &lt;- update(mobile.1, . ~ . - gender)\n\nWe’re not going to look at these, because the output from summary is not very illuminating. What we do next is to try to figure out which (if either) of the explanatory variables age.group and gender we need.\n\\(\\blacksquare\\)\n\nUse drop1 on your model containing both explanatory variables to determine whether you can remove either of them. Use test=\"Chisq\" to obtain P-values.\n\nSolution\ndrop1 takes a fitted model, and tests each term in it in turn, and says which (if any) should be removed. Here’s how it goes:\n\ndrop1(mobile.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe possibilities are to remove gender, to remove age.group or to remove nothing. The best one is “remove nothing”, because it’s the one on the output with the smallest AIC. Both P-values are small, so it would be a mistake to remove either of the explanatory variables.\n\\(\\blacksquare\\)\n\nUse anova to decide whether we are justified in removing gender from a model containing both gender and age.group. Compare your P-value with the one from drop1.\n\nSolution\nThis is a comparison of the model with both variables (mobile.1) and the model with gender removed (mobile.3). Use anova for this, smaller (fewer-\\(x\\)) model first: ::: {.cell}\nanova(mobile.3, mobile.1)\n\n\n\n  \n\n\n:::\nThe P-value is (just) less than 0.05, so the models are significantly different. That means that the model with both variables in fits significantly better than the model with only age.group, and therefore that taking gender out is a mistake.\nThe P-value is identical to the one from drop1 (because they are both doing the same test).\n\\(\\blacksquare\\)\n\nUse anova to see whether we are justified in removing age.group from a model containing both gender and age.group. Compare your P-value with the one from drop1 above.\n\nSolution\nExactly the same idea as the last part. In my case, I’m comparing models mobile.2 and mobile.1: ::: {.cell}\nanova(mobile.2, mobile.1)\n\n\n\n  \n\n\n:::\nThis one is definitely significant, so I need to keep age.group for sure. Again, the P-value is the same as the one in drop1.\n\\(\\blacksquare\\)\n\nWhich of the models you have fit so far is the most appropriate one? Explain briefly.\n\nSolution\nI can’t drop either of my variables, so I have to keep them both: mobile.1, with both age.group and gender.\n\\(\\blacksquare\\)\n\nObtain predicted probabilities of a customer falling in the various satisfaction categories, as it depends on gender and age group. To do that, you need to feed predict three things: the fitted model that contains both age group and gender, the data frame that you read in from the file back in part (here) (which contains all the combinations of age group and gender), and an appropriate type.\n\nSolution\nMy model containing both \\(x\\)s was mobile.1, the data frame read in from the file was called mobile, and I need type=\"p\" to get probabilities: ::: {.cell}\nprobs &lt;- predict(mobile.1, mobile, type = \"p\")\nmobile %&gt;%\n  select(gender, age.group) %&gt;%\n  cbind(probs)\n\n\n\n  \n\n\n:::\nThis worked for me, but this might happen to you, with the same commands as above:\n\n\nError in MASS::select(., gender, age.group): unused arguments (gender, age.group)\n\n\nOh, this didn’t work. Why not? There don’t seem to be any errors.\nThis is the kind of thing that can bother you for days. The resolution (that it took me a long time to discover) is that you might have the tidyverse and also MASS loaded, in the wrong order, and MASS also has a select (that takes different inputs and does something different). If you look back at part (here), you might have seen a message there when you loaded MASS that select was “masked”. When you have two packages that both contain a function with the same name, the one that you can see (and that will get used) is the one that was loaded last, which is the MASS select (not the one we actually wanted, which is the tidyverse select). There are a couple of ways around this. One is to un-load the package we no longer need (when we no longer need it). The mechanism for this is shown at the end of part (here). The other is to say explicitly which package you want your function to come from, so that there is no doubt. The tidyverse is actually a collection of packages. The best way to find out which one our select comes from is to go to the Console window in R Studio and ask for the help for select. With both tidyverse and MASS loaded, the help window offers you a choice of both selects; the one we want is “select/rename variables by name”, and the actual package it comes from is dplyr.\nThere is a third choice, which is the one I prefer now: install and load the package conflicted. When you run your code and it calls for something like select that is in two packages that you have loaded, it gives an error, like this:\nError: [conflicted] `select` found in 2 packages.\nEither pick the one you want with `::` \n* MASS::select\n* dplyr::select\nOr declare a preference with `conflict_prefer()`\n* conflict_prefer(\"select\", \"MASS\")\n* conflict_prefer(\"select\", \"dplyr\")\nFixing this costs you a bit of time upfront, but once you have fixed it, you know that the right thing is being run. What I do is to copy-paste one of those conflict_prefer lines, in this case the second one, and put it before the select that now causes the error. Right after the library(conflicted) is a good place. When you use conflicted, you will probably have to run several times to fix up all the conflicts, which will be a bit frustrating, and you will end up with several conflict_prefer lines, but once you have them there, you won’t have to worry about the right function being called because you have explicitly said which one you want.\nThis is a non-standard use of cbind because I wanted to grab only the gender and age group columns from mobile first, and then cbind that to the predicted probabilities. The missing first input to cbind is “whatever came out of the previous step”, that is, the first two columns of mobile.\nI only included the first two columns of mobile in the cbind, because the rest of the columns of mobile were frequencies, which I don’t need to see. (Having said that, it would be interesting to make a plot using the observed proportions and predicted probabilities, but I didn’t ask you for that.)\nThis is an unusual predict, because we didn’t have to make a data frame (with my usual name new) containing all the combinations of things to predict for. We were lucky enough to have those already in the original data frame mobile.\nThe usual way to do this is something like the trick that we did for getting the different satisfaction levels:\n\ngenders &lt;- mobile.long %&gt;% distinct(gender) %&gt;% pluck(\"gender\")\nage.groups &lt;- mobile.long %&gt;%\n  distinct(age.group) %&gt;%\n  pluck(\"age.group\")\n\nThis is getting perilously close to deserving a function written to do it (strictly speaking, we should, since this is three times we’ve used this idea now).\nThen crossing to get the combinations, and then predict:\n\nnew &lt;- crossing(gender = genders, age.group = age.groups)\nnew\n\n\n\n  \n\n\npp &lt;- predict(mobile.1, new, type = \"p\")\ncbind(new, pp)\n\n\n\n  \n\n\n\nThis method is fine if you want to do this question this way; the way I suggested first ought to be easier, but the nice thing about this is its mindlessness: you always do it about the same way.\n\\(\\blacksquare\\)\n\n* Describe any patterns you see in the predictions, bearing in mind the significance or not of the explanatory variables.\n\nSolution\nI had both explanatory variables being significant, so I would expect to see both an age-group effect and a gender effect. For both males and females, there seems to be a decrease in satisfaction as the customers get older, at least until age 30 or so. I can see this because the predicted prob. of “very satisfied” decreases, and the predicted prob. of “very unsatisfied” increases. The 31+ age group are very similar to the 25–30 group for both males and females. So that’s the age group effect. What about a gender effect? Well, for all the age groups, the males are more likely to be very satisfied than the females of the corresponding age group, and also less likely to to be very unsatisfied. So the gender effect is that males are more satisfied than females overall. (Or, the males are less discerning. Take your pick.) When we did the tests above, age group was very definitely significant, and gender less so (P-value around 0.03). This suggests that the effect of age group ought to be large, and the effect of gender not so large. This is about what we observed: the age group effect was pretty clear, and the gender effect was noticeable but small: the females were less satisfied than the males, but there wasn’t all that much difference.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ordinal-response.html#finding-non-missing-values-1",
    "href": "ordinal-response.html#finding-non-missing-values-1",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.7 Finding non-missing values",
    "text": "25.7 Finding non-missing values\n* This is to prepare you for something in the next question. It’s meant to be easy.\nIn R, the code NA stands for “missing value” or “value not known”. In R, NA should not have quotes around it. (It is a special code, not a piece of text.)\n\nCreate a vector v that contains some numbers and some missing values, using c(). Put those values into a one-column data frame.\n\nSolution\nLike this. The arrangement of numbers and missing values doesn’t matter, as long as you have some of each: ::: {.cell}\nv &lt;- c(1, 2, NA, 4, 5, 6, 9, NA, 11)\nmydata &lt;- tibble(v)\nmydata\n\n\n\n  \n\n\n:::\nThis has one column called v.\n\\(\\blacksquare\\)\n\nObtain a new column containing is.na(v). When is this true and when is this false?\n\nSolution\n\nmydata &lt;- mydata %&gt;% mutate(isna = is.na(v))\nmydata\n\n\n\n  \n\n\n\nThis is TRUE if the corresponding element of v is missing (in my case, the third value and the second-last one), and FALSE otherwise (when there is an actual value there).\n\\(\\blacksquare\\)\n\nThe symbol ! means “not” in R (and other programming languages). What does !is.na(v) do? Create a new column containing that.\n\nSolution\nTry it and see. Give it whatever name you like. My name reflects that I know what it’s going to do: ::: {.cell}\nmydata &lt;- mydata %&gt;% mutate(notisna = !is.na(v))\nmydata\n\n\n\n  \n\n\n:::\nThis is the logical opposite of is.na: it’s true if there is a value, and false if it’s missing.\n\\(\\blacksquare\\)\n\nUse filter to display just the rows of your data frame that have a non-missing value of v.\n\nSolution\nfilter takes a column to say which rows to pick, in which case the column should contain something that either is TRUE or FALSE, or something that can be interpreted that way: ::: {.cell}\nmydata %&gt;% filter(notisna)\n\n\n\n  \n\n\n:::\nor you can provide filter something that can be calculated from what’s in the data frame, and also returns something that is either true or false:\n\nmydata %&gt;% filter(!is.na(v))\n\n\n\n  \n\n\n\nIn either case, I only have non-missing values of v.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ordinal-response.html#high-school-and-beyond-1",
    "href": "ordinal-response.html#high-school-and-beyond-1",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.8 High School and Beyond",
    "text": "25.8 High School and Beyond\nA survey called High School and Beyond was given to a large number of American high school seniors (grade 12) by the National Center of Education Statistics. The data set at link is a random sample of 200 of those students.\nThe variables collected are:\n\ngender: student’s gender, female or male.\nrace: the student’s race (African-American, Asian,2 Hispanic, White).\nses: Socio-economic status of student’s family (low, middle, or high)\nschtyp: School type, public or private.\nprog: Student’s program, general, academic, or vocational.\nread: Score on standardized reading test.\nwrite: Score on standardized writing test.\nmath: Score on standardized math test.\nscience: Score on standardized science test.\nsocst: Score on standardized social studies test.\n\nOur aim is to see how socio-economic status is related to the other variables.\n\nRead in and display (some of) the data.\n\nSolution\nThis is a .csv file (I tried to make it easy for you):\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/hsb.csv\"\nhsb &lt;- read_csv(my_url)\n\nRows: 200 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): race, ses, schtyp, prog, gender\ndbl (6): id, read, write, math, science, socst\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhsb\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nExplain briefly why an ordinal logistic regression is appropriate for our aims.\n\nSolution\nThe response variable ses is categorical, with categories that come in order (low less than middle less than high).\n\\(\\blacksquare\\)\n\nFit an ordinal logistic regression predicting socio-economic status from the scores on the five standardized tests. (You don’t need to display the results.) You will probably go wrong the first time. What kind of thing does your response variable have to be?\n\nSolution\nIt has to be an ordered factor, which you can create in the data frame (or outside, if you prefer): ::: {.cell}\nhsb &lt;- hsb %&gt;% mutate(ses = ordered(ses, c(\"low\", \"middle\", \"high\")))\nhsb\n\n\n\n  \n\n\n:::\nses is now ord. Good. Now fit the model:\n\nses.1 &lt;- polr(ses ~ read + write + math + science + socst, data = hsb)\n\nNo errors is good.\n\\(\\blacksquare\\)\n\nRemove any non-significant explanatory variables one at a time. Use drop1 to decide which one to remove next.\n\nSolution\n\ndrop1(ses.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nI would have expected the AIC column to come out in order, but it doesn’t. Never mind. Scan for the largest P-value, which belongs to read. (This also has the lowest AIC.) So, remove read:\n\nses.2 &lt;- update(ses.1, . ~ . - read)\ndrop1(ses.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nNote how the P-value for science has come down a long way.\nA close call, but math goes next. The update doesn’t take long to type:\n\nses.3 &lt;- update(ses.2, . ~ . - math)\ndrop1(ses.3, test = \"Chisq\")\n\n\n\n  \n\n\n\nscience has become significant now (probably because it was strongly correlated with at least one of the variables we removed (at my guess, math). That is, we didn’t need both science and math, but we do need one of them.\nI think we can guess what will happen now: write comes out, and the other two variables will stay, so that’ll be where we stop:\n\nses.4 &lt;- update(ses.3, . ~ . - write)\ndrop1(ses.4, test = \"Chisq\")\n\n\n\n  \n\n\n\nIndeed so. We need just the science and social studies test scores to predict socio-economic status.\nUsing AIC to decide on which variable to remove next will give the same answer here, but I would like to see the test= part in your drop1 to give P-values (expect to lose something, but not too much, if that’s not there).\nExtras: I talked about correlation among the explanatory variables earlier, which I can explore:\n\nhsb %&gt;% select(read:socst) %&gt;% cor()\n\n             read     write      math   science     socst\nread    1.0000000 0.5967765 0.6622801 0.6301579 0.6214843\nwrite   0.5967765 1.0000000 0.6174493 0.5704416 0.6047932\nmath    0.6622801 0.6174493 1.0000000 0.6307332 0.5444803\nscience 0.6301579 0.5704416 0.6307332 1.0000000 0.4651060\nsocst   0.6214843 0.6047932 0.5444803 0.4651060 1.0000000\n\n\nThe first time I did this, I forgot that I had MASS loaded (for the polr), and so, to get the right select, I needed to say which one I wanted.\nAnyway, the correlations are all moderately high. There’s nothing that stands out as being much higher than the others. The lowest two are between social studies and math, and social studies and science. That would be part of the reason that social studies needs to stay. The highest correlation is between math and reading, which surprises me (they seem to be different skills).\nSo there was not as much insight there as I expected.\nThe other thing is that you can use step for the variable-elimination task as well:\n\nses.5 &lt;- step(ses.1, direction = \"backward\", test = \"Chisq\")\n\nStart:  AIC=404.63\nses ~ read + write + math + science + socst\n\n          Df    AIC    LRT Pr(&gt;Chi)   \n- read     1 403.09 0.4620 0.496684   \n- math     1 403.19 0.5618 0.453517   \n- write    1 403.81 1.1859 0.276167   \n&lt;none&gt;       404.63                   \n- science  1 404.89 2.2630 0.132499   \n- socst    1 410.08 7.4484 0.006349 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=403.09\nses ~ write + math + science + socst\n\n          Df    AIC    LRT Pr(&gt;Chi)   \n- math     1 402.04 0.9541 0.328689   \n- write    1 402.10 1.0124 0.314325   \n&lt;none&gt;       403.09                   \n- science  1 404.29 3.1968 0.073782 . \n- socst    1 410.58 9.4856 0.002071 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=402.04\nses ~ write + science + socst\n\n          Df    AIC     LRT  Pr(&gt;Chi)    \n- write    1 400.60  0.5587 0.4547813    \n&lt;none&gt;       402.04                      \n- science  1 405.41  5.3680 0.0205095 *  \n- socst    1 411.07 11.0235 0.0008997 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=400.6\nses ~ science + socst\n\n          Df    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;       400.60                      \n- science  1 403.45  4.8511 0.0276291 *  \n- socst    1 409.74 11.1412 0.0008443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI would accept you doing it this way, again as long as you have the test= there as well.\n\\(\\blacksquare\\)\n\nThe quartiles of the science test score are 44 and 58. The quartiles of the socst test score are 46 and 61. Make a data frame that has all combinations of those quartiles. If your best regression had any other explanatory variables in it, also put the means of those variables into this data frame.\n\nSolution\nThis is what datagrid does by default (from package marginaleffects):\n\nnew &lt;- datagrid(model = ses.5, science = c(44, 58), socst = c(46, 61))\nnew\n\n\n\n  \n\n\n\nThis explicitly fills in mean values or most frequent categories for all the other variables in the dataset, even though those other variables are not in the model. The two variables you actually care about are over on the right.\nSince there are only two variables left, this new data frame has only \\(2^2=4\\) rows.\nThere is a veiled hint here that these are the two variables that should have remained in your regression. If that was not what you got, the means of the other variables in the model will go automatically into your new:\n\ndatagrid(model = ses.1, science = c(44, 58), socst = c(46, 61))\n\n\n\n  \n\n\n\nso you don’t have to do anything extra.\n\\(\\blacksquare\\)\n\nUse the data frame you created in the previous part, together with your best model, to obtain predicted probabilities of being in each ses category. Display these predicted probabilities so that they are easy to read.\n\nSolution\nThis is predictions, and we’ve done the setup. My best model was called ses.4.\n\ncbind(predictions(ses.4, newdata = new)) %&gt;% \n  select(group, estimate, science, socst) \n\n\nRe-fitting to get Hessian\n\n\n\n\n  \n\n\n\npredictions always works by having one column of predictions. That isn’t the best layout here, though; we want to see the three predicted probabilities for a particular value of science and socst all in one row, which means pivoting-wider:\n\ncbind(predictions(ses.4, newdata = new)) %&gt;% \n  select(group, estimate, science, socst) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\nRe-fitting to get Hessian\n\n\n\n\n  \n\n\n\nThe easiest strategy seems to be to run predictions first, see that it comes out long, and then wonder how to fix it. Then pick the columns you care about: the predicted group, the predictions, and the columns for science and social science, and then pivot wider.\n\\(\\blacksquare\\)\n\nWhat is the effect of an increased science score on the likelihood of a student being in the different socioeconomic groups, all else equal? Explain briefly. In your explanation, state clearly how you are using your answer to the previous part.\n\nSolution\nUse your predictions; hold the socst score constant (that’s the all else equal part). So compare the first and third rows (or, if you like, the second and fourth rows) of your predictions and see what happens as the science score goes from 44 to 58. What I see is that the probability of being low goes noticeably down as the science score increases, the probability of middle stays about the same, and the probability of high goes up (by about the same amount as the probability of low went down). In other words, an increased science score goes with an increased chance of high (and a decreased chance of low). If your best model doesn’t have science in it, then you need to say something like “science has no effect on socio-economic status”, consistent with what you concluded before: if you took it out, it’s because you thought it had no effect. Extra: the effect of an increased social studies score is almost exactly the same as an increased science score (so I didn’t ask you about that). From a social-science point of view, this makes perfect sense: the higher the social-economic stratum a student comes from, the better they are likely to do in school. I’ve been phrasing this as “association”, because really the cause and effect is the other way around: a student’s family socioeconomic status is explanatory, and school performance is response. But this was the nicest example I could find of an ordinal response data set.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ordinal-response.html#how-do-you-like-your-steak-1",
    "href": "ordinal-response.html#how-do-you-like-your-steak-1",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.9 How do you like your steak?",
    "text": "25.9 How do you like your steak?\nWhen you order a steak in a restaurant, the server will ask you how you would like it cooked, or to be precise, how much you would like it cooked: rare (hardly cooked at all), through medium rare, medium, medium well to well (which means “well done”, so that the meat has only a little red to it). Could you guess how a person likes their steak cooked, from some other information about them? The website link commissioned a survey where they asked a number of people how they preferred their steak, along with as many other things as they could think of to ask. (Many of the variables below are related to risk-taking, which was something the people designing the survey thought might have something to do with liking steak rare.) The variables of interest are all factors or true/false:\n\nrespondent_ID: a ten-digit number identifying each person who responded to the survey.\nlottery_a: true if the respondent preferred lottery A with a small chance to win a lot of money, to lottery B, with a larger chance to win less money.\nsmoke: true if the respondent is currently a smoker\nalcohol: true if the respondent at least occasionally drinks alcohol.\ngamble: true if the respondent likes to gamble (eg. betting on horse racing or playing the lottery)\nskydiving: true if the respondent has ever been skydiving.\nspeed: true if the respondent likes to drive fast\ncheated: true if the respondent has ever cheated on a spouse or girlfriend/boyfriend\nsteak: true if the respondent likes to eat steak\nsteak_prep (response): how the respondent likes their steak cooked (factor, as described above, with 5 levels).\nfemale: true if the respondent is female\nage: age group, from 18–29 to 60+.\nhhold_income: household income group, from $0–24,999 to $150,000+.\neduc: highest level of education attained, from “less than high school” up to “graduate degree”\nregion: region (of the US) that the respondent lives in (five values).\n\nThe data are in link. This is the cleaned-up data from a previous question, with the missing values removed.\n\nRead in the data and display the first few lines.\n\nSolution\nThe usual:\n\nsteak &lt;- read_csv(\"steak1.csv\")\n\nRows: 331 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): steak_prep, age, hhold_income, educ, region\ndbl (1): respondent_id\nlgl (9): lottery_a, smoke, alcohol, gamble, skydiving, speed, cheated, steak...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsteak\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nWe are going to predict steak_prep from some of the other variables. Why is the model-fitting function polr from package MASS the best choice for these data (alternatives being glm and multinom from package nnet)?\n\nSolution\nIt all depends on the kind of response variable. We have a response variable with five ordered levels from Rare to Well. There are more than two levels (it is more than a “success” and “failure”), which rules out glm, and the levels are ordered, which rules out multinom. As we know, polr handles an ordered response, so it is the right choice.\n\\(\\blacksquare\\)\n\nWhat are the levels of steak_prep, in the order that R thinks they are in? If they are not in a sensible order, create an ordered factor where the levels are in a sensible order.\n\nSolution\nThis is the most direct way to find out: ::: {.cell}\npreps &lt;- steak %&gt;% distinct(steak_prep) %&gt;% pull(steak_prep)\npreps\n\n[1] \"Medium rare\" \"Rare\"        \"Medium\"      \"Medium Well\" \"Well\"       \n\n:::\nThis is almost the right order (distinct uses the order in the data frame). We just need to switch the first two around, and then we’ll be done:\n\npreps1 &lt;- preps[c(2, 1, 3, 4, 5)]\npreps1\n\n[1] \"Rare\"        \"Medium rare\" \"Medium\"      \"Medium Well\" \"Well\"       \n\n\nIf you used count, there’s a bit more work to do:\n\npreps2 &lt;- steak %&gt;% count(steak_prep) %&gt;% pull(steak_prep)\npreps2\n\n[1] \"Medium\"      \"Medium Well\" \"Medium rare\" \"Rare\"        \"Well\"       \n\n\nbecause count puts them in alphabetical order, so:\n\npreps3 &lt;- preps2[c(4, 2, 1, 3, 5)]\npreps3\n\n[1] \"Rare\"        \"Medium Well\" \"Medium\"      \"Medium rare\" \"Well\"       \n\n\nThese use the idea in the attitudes-to-abortion question: create a vector of the levels in the right order, then create an ordered factor with ordered(). If you like, you can type the levels in the right order (I won’t penalize you for that here), but it’s really better to get the levels without typing or copying-pasting, so that you don’t make any silly errors copying them (which will mess everything up later).\nSo now I create my ordered response:\n\nsteak &lt;- steak %&gt;% mutate(steak_prep_ord = ordered(steak_prep, preps1))\n\nor using one of the other preps vectors containing the levels in the correct order. As far as polr is concerned, it doesn’t matter whether I start at Rare and go “up”, or start at Well and go “down”. So if you do it the other way around, that’s fine. As long as you get the levels in a sensible order, you’re good.\n\\(\\blacksquare\\)\n\nFit a model predicting preferred steak preparation in an ordinal logistic regression from educ, female and lottery_a. This ought to be easy from your previous work, but you have to be careful about one thing. No need to print out the results.\n\nSolution\nThe thing you have to be careful about is that you use the ordered factor that you just created as the response: ::: {.cell hash=‘ordinal-response_cache/html/finetti_d7574b2da18db544ba132ea378564b61’}\nsteak.1 &lt;- polr(steak_prep_ord ~ educ + female + lottery_a, data = steak)\n:::\n\\(\\blacksquare\\)\n\nRun drop1 on your fitted model, with test=\"Chisq\". Which explanatory variable should be removed first, if any? Bear in mind that the variable with the smallest AIC should come out first, in case your table doesn’t get printed in order.\n\nSolution\nThis:\n\ndrop1(steak.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nMy table is indeed out of order (which is why I warned you about it, in case that happens to you as well). The smallest AIC goes with female, which also has a very non-significant P-value, so this one should come out first.\n\\(\\blacksquare\\)\n\nRemove the variable that should come out first, using update. (If all the variables should stay, you can skip this part.)\n\nSolution\nYou could type or copy-paste the whole model again, but update is quicker: ::: {.cell}\nsteak.2 &lt;- update(steak.1, . ~ . - female)\n:::\nThat’s all.\nI wanted to get some support for my drop1 above (since I was a bit worried about those out-of-order rows). Now that we have fitted a model with female and one without, we can compare them using anova:\n\nanova(steak.2, steak.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nDon’t get taken in by that “LR stat” that may be on the end of the first row of the output table; the P-value might have wrapped onto the second line, and is in fact exactly the same as in the drop1 output (it is doing exactly the same test). As non-significant as you could wish for.\nExtra: I was curious about whether either of the other \\(x\\)’s could come out now:\n\ndrop1(steak.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nlottery_a should come out, but educ is edging towards significance. We are about to do predictions; in those, the above suggests that there may be some visible effect of education, but there may not be much effect of lottery_a.\nAll right, so what happens when we remove lottery_a? That we find out later.\n\\(\\blacksquare\\)\n\nUsing the best model that you have so far, predict the probabilities of preferring each different steak preparation (method of cooking) for each combination of the variables that remain. (Some of the variables are TRUE and FALSE rather than factors. Bear this in mind.) Describe the effects of each variable on the predicted probabilities, if any. Note that there is exactly one person in the study whose educational level is “less than high school”.\n\nSolution\nAgain, I’m leaving it to you to follow all the steps. My variables remaining are educ and lottery_a, which are respectively categorical and logical.\nThe first step is to get all combinations of their values, along with “typical” values for the others:\n\nnew &lt;- datagrid(model = steak.2, \n                educ = levels(factor(steak$educ)),\n                lottery_a = c(FALSE, TRUE))\nnew\n\n\n\n  \n\n\n\nI wasn’t sure how to handle the logical lottery_a, so I just typed the TRUE and FALSE.\nOn to the predictions, remembering to make them wider:\n\ncbind(predictions(steak.2, newdata = new)) %&gt;% \n  select(rowid, group, estimate, educ, lottery_a) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\nRe-fitting to get Hessian\n\n\n\n\n  \n\n\n\nThere are 5 levels of education, 2 levels of lottery_a, and 5 ways in which you might ask for your steak to be cooked, so the original output from predictions has \\(5 \\times 2 \\times 5 = 50\\) rows, and the output you see above has \\(5 \\times 2 = 10\\) rows.\nI find this hard to read, so I’m going to round off those predictions. Three or four decimals seems to be sensible. The time to do this is while they are all in one column, that is, before the pivot_wider. On my screen, the education levels also came out rather long, so I’m going to shorten them as well:\n\ncbind(predictions(steak.2, newdata = new)) %&gt;% \n  select(rowid, group, estimate, educ, lottery_a) %&gt;% \n  mutate(estimate = round(estimate, 3),\n         educ = abbreviate(educ, 15)) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\nRe-fitting to get Hessian\n\n\n\n\n  \n\n\n\nThat’s about as much as I can shorten the education levels while still having them readable.\nThen, say something about the effect of changing educational level on the predictions, and say something about the effect of favouring Lottery A vs. not. I don’t much mind what: you can say that there is not much effect (of either variable), or you can say something like “people with a graduate degree are slightly more likely to like their steak rare and less likely to like it well done” (for education level) and “people who preferred Lottery A are slightly less likely to like their steak rare and slightly more likely to like it well done” (for effect of Lottery A). You can see these by comparing the odd-numbered rows rows with each other to assess the effect of education while holding attitudes towards lottery_a constant (or the even-numbered rows, if you prefer), and you can compare eg. rows 1 and 2 to assess the effect of Lottery A (compare two lines with the same educational level but different preferences re Lottery A).\nI would keep away from saying anything about education level “less than high school”, since this entire level is represented by exactly one person.\n\\(\\blacksquare\\)\n\nIs it reasonable to remove all the remaining explanatory variables from your best model so far? Fit a model with no explanatory variables, and do a test. (In R, if the right side of the squiggle is a 1, that means “just an intercept”. Or you can remove whatever remains using update.) What do you conclude? Explain briefly.\n\nSolution\nThe fitting part is the challenge, since the testing part is anova again. The direct fit is this: ::: {.cell hash=‘ordinal-response_cache/html/fiorentina_ed4e97251ec455c039297573b7860ac6’}\nsteak.3 &lt;- polr(steak_prep_ord ~ 1, data = steak)\n:::\nand the update version is this, about equally long, starting from steak.2 since that is the best model so far:\n\nsteak.3a &lt;- update(steak.2, . ~ . - educ - lottery_a)\n\nYou can use whichever you like. Either way, the second part is anova, and the two possible answers should be the same:\n\nanova(steak.3, steak.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nor\n\nanova(steak.3a, steak.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nAt the 0.05 level, removing both of the remaining variables is fine: that is, nothing (out of these variables) has any impact on the probability that a diner will prefer their steak cooked a particular way. However, it is a very close call; the P-value is only just bigger than 0.05.\nHowever, with data like this and a rather exploratory analysis, I might think about using a larger \\(\\alpha\\) like 0.10, and at this level, taking out both these two variables is a bad idea. You could say that one or both of them is “potentially useful” or “provocative” or something like that.\nIf you think that removing these two variables is questionable, you might like to go back to that drop1 output I had above:\n\ndrop1(steak.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe smallest AIC goes with lottery_a, so that comes out (it is nowhere near significant):\n\nsteak.4 &lt;- update(steak.2, . ~ . - lottery_a)\ndrop1(steak.4, test = \"Chisq\")\n\n\n\n  \n\n\n\nand what you see is that educational level is right on the edge of significance, so that may or may not have any impact. Make a call. But if anything, it’s educational level that makes a difference.\n\\(\\blacksquare\\)\n\nIn the article for which these data were collected, link, does the author obtain consistent conclusions with yours? Explain briefly. (It’s not a very long article, so it won’t take you long to skim through, and the author’s point is pretty clear.)\n\nSolution\nThe article says that nothing has anything to do with steak preference. Whether you agree or not depends on what you thought above about dropping those last two variables. So say something consistent with what you said in the previous part. Two points for saying that the author said “nothing has any effect”, and one point for how your findings square with that.\nExtra: now that you have worked through this great long question, this is where I tell you that I simplified things a fair bit for you! There were lots of other variables that might have had an impact on how people like their steaks, and we didn’t even consider those. Why did I choose what I did here? Well, I wanted to fit a regression predicting steak preference from everything else, do a big backward elimination, but: ::: {.cell hash=‘ordinal-response_cache/html/udinese_6ddded22f5b8aa1bb87b651e1aab89d5’}\nsteak.5 &lt;- polr(steak_prep_ord ~ ., data = steak)\n\nWarning: glm.fit: algorithm did not converge\n\n\nError in polr(steak_prep_ord ~ ., data = steak): attempt to find suitable starting values failed\n\n:::\nThe . in place of explanatory variables means “all the other variables”, including the nonsensical personal ID. That saved me having to type them all out.\nUnfortunately, however, it didn’t work. The problem is a numerical one. Regular regression has a well-defined procedure, where the computer follows through the steps and gets to the answer, every time. Once you go beyond regression, however, the answer is obtained by a step-by-step method: the computer makes an initial guess, tries to improve it, then tries to improve it again, until it can’t improve things any more, at which point it calls it good. The problem here is that polr cannot even get the initial guess! (It apparently is known to suck at this, in problems as big and complicated as this one.)\nI don’t normally recommend forward selection, but I wonder whether it works here:\n\nsteak.5 &lt;- polr(steak_prep_ord ~ 1, data = steak)\nsteak.6 &lt;- step(steak.5,\n  scope = . ~ lottery_a + smoke + alcohol + gamble + skydiving +\n    speed + cheated + female + age + hhold_income + educ + region,\n  direction = \"forward\", test = \"Chisq\", trace = 0\n)\ndrop1(steak.6, test = \"Chisq\")\n\n\n\n  \n\n\n\nIt does, and it says the only thing to add out of all the variables is education level. So, for you, I picked this along with a couple of other plausible-sounding variables and had you start from there.\nForward selection starts from a model containing nothing and asks “what can we add?”. This is a bit more complicated than backward elimination, because now you have to say what the candidate things to add are. That’s the purpose of that scope piece, and there I had no alternative but to type the names of all the variables. Backward elimination is easier, because the candidate variables to remove are the ones in the model, and you don’t need a scope. The trace=0 says “don’t give me any output” (you can change it to a different value if you want to see what that does), and last, the drop1 looks at what is actually in the final model (with a view to asking what can be removed, but we don’t care about that here).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ordinal-response.html#how-do-you-like-your-steak-the-data-1",
    "href": "ordinal-response.html#how-do-you-like-your-steak-the-data-1",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.10 How do you like your steak – the data",
    "text": "25.10 How do you like your steak – the data\n This question takes you through the data preparation for one of the other questions. You don’t have to do this* question, but you may find it interesting or useful.\nWhen you order a steak in a restaurant, the server will ask you how you would like it cooked, or to be precise, how much you would like it cooked: rare (hardly cooked at all), through medium rare, medium, medium well to well (which means “well done”, so that the meat has only a little red to it). Could you guess how a person likes their steak cooked, from some other information about them? The website link commissioned a survey where they asked a number of people how they preferred their steak, along with as many other things as they could think of to ask. (Many of the variables below are related to risk-taking, which was something the people designing the survey thought might have something to do with liking steak rare.) The variables of interest are all factors or true/false:\n\nrespondent_ID: a ten-digit number identifying each person who responded to the survey.\nlottery_a: true if the respondent preferred lottery A with a small chance to win a lot of money, to lottery B, with a larger chance to win less money.\nsmoke: true if the respondent is currently a smoker\nalcohol: true if the respondent at least occasionally drinks alcohol.\ngamble: true if the respondent likes to gamble (eg. betting on horse racing or playing the lottery)\nskydiving: true if the respondent has ever been skydiving.\nspeed: true if the respondent likes to drive fast\ncheated: true if the respondent has ever cheated on a spouse or girlfriend/boyfriend\nsteak: true if the respondent likes to eat steak\nsteak_prep (response): how the respondent likes their steak cooked (factor, as described above, with 5 levels).\nfemale: true if the respondent is female\nage: age group, from 18–29 to 60+.\nhhold_income: household income group, from $0–24,999 to $150,000+.\neduc: highest level of education attained, from “less than high school” up to “graduate degree”\nregion: region (of the US) that the respondent lives in (five values).\n\nThe data are in link.\n\nRead in the data and display the first few lines.\n\nSolution\nThe usual:\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/steak.csv\"\nsteak0 &lt;- read_csv(my_url)\n\nRows: 550 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): steak_prep, age, hhold_income, educ, region\ndbl (1): respondent_id\nlgl (9): lottery_a, smoke, alcohol, gamble, skydiving, speed, cheated, steak...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsteak0\n\n\n\n  \n\n\n\nI’m using a temporary name for reasons that will become clear shortly.\n\\(\\blacksquare\\)\n\nWhat do you immediately notice about your data frame? Run summary on the entire data frame. Would you say you have a lot of missing values, or only a few?\n\nSolution\nI see missing values, starting in the very first row. Running the data frame through summary gives this, either as summary(steak0) or this way: ::: {.cell}\nsteak0 %&gt;% summary()\n\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:279       FALSE:453       FALSE:125      \n Median :3.235e+09   TRUE :267       TRUE :84        TRUE :416      \n Mean   :3.235e+09   NA's :4         NA's :13        NA's :9        \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.238e+09                                                  \n   gamble        skydiving         speed          cheated       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:280       FALSE:502       FALSE:59        FALSE:447      \n TRUE :257       TRUE :36        TRUE :480       TRUE :92       \n NA's :13        NA's :12        NA's :11        NA's :11       \n                                                                \n                                                                \n   steak          steak_prep          female            age           \n Mode :logical   Length:550         Mode :logical   Length:550        \n FALSE:109       Class :character   FALSE:246       Class :character  \n TRUE :430       Mode  :character   TRUE :268       Mode  :character  \n NA's :11                           NA's :36                          \n                                                                      \n                                                                      \n hhold_income           educ              region         \n Length:550         Length:550         Length:550        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n\n:::\nMake a call about whether you think that’s a lot of missing values or only a few. This might not be all of them, because missing text doesn’t show here (we see later how to make it show up).\n\\(\\blacksquare\\)\n\nWhat does the function drop_na do when applied to a data frame with missing values? To find out, pass the data frame into drop_na(), then into summary again. What has happened?\n\nSolution\nLet’s try it and see.\n\nsteak0 %&gt;% drop_na() %&gt;% summary()\n\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:171       FALSE:274       FALSE:65       \n Median :3.235e+09   TRUE :160       TRUE :57        TRUE :266      \n Mean   :3.235e+09                                                  \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.235e+09                                                  \n   gamble        skydiving         speed          cheated         steak        \n Mode :logical   Mode :logical   Mode :logical   Mode :logical   Mode:logical  \n FALSE:158       FALSE:308       FALSE:28        FALSE:274       TRUE:331      \n TRUE :173       TRUE :23        TRUE :303       TRUE :57                      \n                                                                               \n                                                                               \n                                                                               \n  steak_prep          female            age            hhold_income      \n Length:331         Mode :logical   Length:331         Length:331        \n Class :character   FALSE:174       Class :character   Class :character  \n Mode  :character   TRUE :157       Mode  :character   Mode  :character  \n                                                                         \n                                                                         \n                                                                         \n     educ              region         \n Length:331         Length:331        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nThe missing values, the ones we can see anyway, have all gone. Precisely, drop_na, as its name suggests, drops all the rows that have missing values in them anywhere. This is potentially wasteful, since a row might be missing only one value, and we drop the entire rest of the row, throwing away the good data as well. If you check, we started with 550 rows, and we now have only 311 left. Ouch.\nSo now we’ll save this into our “good” data frame, which means doing it again (now that we know it works):\n\nsteak0 %&gt;% drop_na() -&gt; steak\n\nExtra: another way to handle missing data is called “imputation”: what you do is to estimate a value for any missing data, and then use that later on as if it were the truth. One way of estimating missing values is to do a regression (of appropriate kind: regular or logistic) to predict a column with missing values from all the other columns.\nExtra extra: below we see how we used to have to do this, for your information.\nFirst, we run complete.cases on the data frame:\n\ncomplete.cases(steak0)\n\n  [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [37]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n [61] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n [73] FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [85] FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n [97]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n[109] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n[133] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[145]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[157]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n[169]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n[181] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n[205]  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[217] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[253] FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n[265] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n[277]  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[289]  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n[301] FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[313]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[325] FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n[337]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[349] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n[361] FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n[373] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n[385]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[397]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[409]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE\n[421]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[433]  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n[445]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n[457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[469]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n[481] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n[493] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[505]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n[517]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n[529] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n[541] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\nYou might be able to guess what this does, in the light of what we just did, but if not, you can investigate. Let’s pick three rows where complete.cases is TRUE and three where it’s FALSE, and see what happens.\nI’ll pick rows 496, 497, and 498 for the TRUE rows, and 540, 541 and 542 for the FALSE ones. Let’s assemble these rows into a vector and use slice to display the rows with these numbers:\n\nrows &lt;- c(496, 497, 498, 540, 541, 542)\nrows\n\n[1] 496 497 498 540 541 542\n\n\nLike this:\n\nsteak0 %&gt;% slice(rows)\n\n\n\n  \n\n\n\nWhat’s the difference? The rows where complete.cases is FALSE have one (or more) missing values in them; where complete.cases is TRUE the rows have no missing values. (Depending on the rows you choose, you may not see the missing value(s), as I didn’t.) Extra (within “extra extra”: I hope you are keeping track): this is a bit tricky to investigate more thoroughly, because the text variables might have missing values in them, and they won’t show up unless we turn them into a factor first: ::: {.cell}\nsteak0 %&gt;%\n  mutate(across(where(is.character), \\(x) factor(x))) %&gt;%\n  summary()\n\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:279       FALSE:453       FALSE:125      \n Median :3.235e+09   TRUE :267       TRUE :84        TRUE :416      \n Mean   :3.235e+09   NA's :4         NA's :13        NA's :9        \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.238e+09                                                  \n                                                                    \n   gamble        skydiving         speed          cheated       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:280       FALSE:502       FALSE:59        FALSE:447      \n TRUE :257       TRUE :36        TRUE :480       TRUE :92       \n NA's :13        NA's :12        NA's :11        NA's :11       \n                                                                \n                                                                \n                                                                \n   steak               steak_prep    female           age     \n Mode :logical   Medium     :132   Mode :logical   &gt;60  :131  \n FALSE:109       Medium rare:166   FALSE:246       18-29:110  \n TRUE :430       Medium Well: 75   TRUE :268       30-44:133  \n NA's :11        Rare       : 23   NA's :36        45-60:140  \n                 Well       : 36                   NA's : 36  \n                 NA's       :118                              \n                                                              \n              hhold_income                               educ    \n $0 - $24,999       : 51   Bachelor degree                 :174  \n $100,000 - $149,999: 76   Graduate degree                 :133  \n $150,000+          : 54   High school degree              : 39  \n $25,000 - $49,999  : 77   Less than high school degree    :  2  \n $50,000 - $99,999  :172   Some college or Associate degree:164  \n NA's               :120   NA's                            : 38  \n                                                                 \n                region   \n Pacific           : 91  \n South Atlantic    : 88  \n East North Central: 86  \n Middle Atlantic   : 72  \n West North Central: 42  \n (Other)           :133  \n NA's              : 38  \n\n:::\nThere are missing values everywhere. What the where does is to do something for each column where the first thing is true: here, if the column is text, then replace it by the factor version of itself. This makes for a better summary, one that shows how many observations are in each category, and, more important for us, how many are missing (a lot).\nAll right, so there are 15 columns, so let’s investigate missingness in our rows by looking at the columns 1 through 8 and then 9 through 15, so they all fit on the screen. Recall that you can select columns by number:\n\nsteak0 %&gt;% select(1:8) %&gt;% slice(rows)\n\n\n\n  \n\n\n\nand\n\nsteak0 %&gt;% select(9:15) %&gt;% slice(rows)\n\n\n\n  \n\n\n\nIn this case, the first three rows have no missing values anywhere, and the last three rows have exactly one missing value. This corresponds to what we would expect, with complete.cases identifying rows that have any missing values.\nWhat we now need to do is to obtain a data frame that contains only the rows with non-missing values. This can be done by saving the result of complete.cases in a variable first; filter can take anything that produces a true or a false for each row, and will return the rows for which the thing it was fed was true. ::: {.cell}\ncc &lt;- complete.cases(steak0)\nsteak0 %&gt;% filter(cc) -&gt; steak.complete\n:::\nA quick check that we got rid of the missing values:\n\nsteak.complete\n\n\n\n  \n\n\n\nThere are no missing values there. Of course, this is not a proof, and there might be some missing values further down, but at least it suggests that we might be good.\nFor proof, this is the easiest way I know:\n\nsteak.complete %&gt;%\n  mutate(across(where(is.character), \\(x) factor(x))) %&gt;%\n  summary()\n\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:171       FALSE:274       FALSE:65       \n Median :3.235e+09   TRUE :160       TRUE :57        TRUE :266      \n Mean   :3.235e+09                                                  \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.235e+09                                                  \n                                                                    \n   gamble        skydiving         speed          cheated         steak        \n Mode :logical   Mode :logical   Mode :logical   Mode :logical   Mode:logical  \n FALSE:158       FALSE:308       FALSE:28        FALSE:274       TRUE:331      \n TRUE :173       TRUE :23        TRUE :303       TRUE :57                      \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n       steak_prep    female           age                  hhold_income\n Medium     :109   Mode :logical   &gt;60  :82   $0 - $24,999       : 37  \n Medium rare:128   FALSE:174       18-29:70   $100,000 - $149,999: 66  \n Medium Well: 56   TRUE :157       30-44:93   $150,000+          : 39  \n Rare       : 18                   45-60:86   $25,000 - $49,999  : 55  \n Well       : 20                              $50,000 - $99,999  :134  \n                                                                       \n                                                                       \n                               educ                    region  \n Bachelor degree                 :120   South Atlantic    :68  \n Graduate degree                 : 86   Pacific           :57  \n High school degree              : 20   East North Central:48  \n Less than high school degree    :  1   Middle Atlantic   :46  \n Some college or Associate degree:104   West North Central:29  \n                                        Mountain          :24  \n                                        (Other)           :59  \n\n\nIf there were any missing values, they would be listed on the end of the counts of observations for each level, or on the bottom of the five-number sumamries. But there aren’t. So here’s your proof.\n\\(\\blacksquare\\)\n\nWrite the data into a .csv file, with a name like steak1.csv. Open this file in a spreadsheet and (quickly) verify that you have the right columns and no missing values.\n\nSolution\nThis is write_csv, using my output from drop_na: ::: {.cell}\nwrite_csv(steak, \"steak1.csv\")\n:::\nOpen up Excel, or whatever you have, and take a look. You should have all the right columns, and, scrolling down, no visible missing values.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ordinal-response.html#footnotes",
    "href": "ordinal-response.html#footnotes",
    "title": "25  Logistic regression with ordinal response",
    "section": "",
    "text": "I’m always amused at how Americans put all Asians into one group.↩︎\nI’m always amused at how Americans put all Asians into one group.↩︎"
  },
  {
    "objectID": "nominal-response.html#finding-non-missing-values",
    "href": "nominal-response.html#finding-non-missing-values",
    "title": "26  Logistic regression with nominal response",
    "section": "26.1 Finding non-missing values",
    "text": "26.1 Finding non-missing values\n* This is to prepare you for something in the next question. It’s meant to be easy.\nIn R, the code NA stands for “missing value” or “value not known”. In R, NA should not have quotes around it. (It is a special code, not a piece of text.)\n\nCreate a vector v that contains some numbers and some missing values, using c(). Put those values into a one-column data frame.\nObtain a new column containing is.na(v). When is this true and when is this false?\nThe symbol ! means “not” in R (and other programming languages). What does !is.na(v) do? Create a new column containing that.\nUse filter to display just the rows of your data frame that have a non-missing value of v."
  },
  {
    "objectID": "nominal-response.html#european-social-survey-and-voting",
    "href": "nominal-response.html#european-social-survey-and-voting",
    "title": "26  Logistic regression with nominal response",
    "section": "26.2 European Social Survey and voting",
    "text": "26.2 European Social Survey and voting\nThe European Social Survey is a giant survey carried out across Europe covering demographic information, attitudes to and amount of education, politics and so on. In this question, we will investigate what might make British people vote for a certain political party.\nThe information for this question is in a (large) spreadsheet at link. There is also a “codebook” at link that tells you what all the variables are. The ones we will use are the last five columns of the spreadsheet, described on pages 11 onwards of the codebook. (I could have given you more, but I didn’t want to make this any more complicated than it already was.)\n\nRead in the .csv file, and verify that you have lots of rows and columns.\n* Use the codebook to find out what the columns prtvtgb, gndr, agea, eduyrs and inwtm are. What do the values 1 and 2 for gndr mean? (You don’t, at this point, have to worry about the values for the other variables.)\nThe three major political parties in Britain are the Conservative, Labour and Liberal Democrat. (These, for your information, correspond roughly to the Canadian Progressive Conservative, NDP and Liberal parties.) For the variable that corresponds to “political party voted for at the last election”, which values correspond to these three parties?\nNormally, I would give you a tidied-up data set. But I figure you could use some practice tidying this one up. As the codebook shows, there are some numerical codes for missing values, and we want to omit those. We want just the columns prtvtgb through inwtm from the right side of the spreadsheet. Use dplyr or tidyr tools to (i) select only these columns, (ii) include the rows that correspond to people who voted for one of the three major parties, (iii) include the rows that have an age at interview less than 999, (iv) include the rows that have less than 40 years of education, (v) include the rows that are not missing on inwtm (use the idea from Question~here for (v)). The last four of those (the inclusion of rows) can be done in one go.\nWhy is my response variable nominal rather than ordinal? How can I tell? Which R function should I use, therefore, to fit my model?\n* Take the political party voted for, and turn it into a factor, by feeding it into factor. Fit an appropriate model to predict political party voted for at the last election (as a factor) from all the other variables. Gender is really a categorical variable too, but since there are only two possible values it can be treated as a number.\nWe have a lot of explanatory variables. The standard way to test whether we need all of them is to take one of them out at a time, and test which ones we can remove. This is a lot of work. We won’t do that. Instead, the R function step does what you want. You feed step two things: a fitted model object, and the option trace=0 (otherwise you get a lot of output). The final part of the output from step tells you which explanatory variables you need to keep. Run step on your fitted model. Which explanatory variables need to stay in the model here?\nFit the model indicated by step (in the last part).\nI didn’t think that interview length could possibly be relevant to which party a person voted for. Test whether interview length can be removed from your model of the last part. What do you conclude? (Note that step and this test may disagree.)\nUse your best model to obtain predictions from some suitably chosen combinations of values of the explanatory variables that remain. (If you have quantitative explanatory variables left, you could use their first and third quartiles as values to predict from. Running summary on the data frame will get summaries of all the variables.)\nWhat is the effect of increasing age? What is the effect of an increase in years of education?"
  },
  {
    "objectID": "nominal-response.html#alligator-food",
    "href": "nominal-response.html#alligator-food",
    "title": "26  Logistic regression with nominal response",
    "section": "26.3 Alligator food",
    "text": "26.3 Alligator food\nWhat do alligators most like to eat? 219 alligators were captured in four Florida lakes. Each alligator’s stomach contents were observed, and the food that the alligator had eaten was classified into one of five categories: fish, invertebrates (such as snails or insects), reptiles (such as turtles), birds, and “other” (such as amphibians, plants or rocks). The researcher noted for each alligator what that alligator had most of in its stomach, as well as the gender of each alligator and whether it was “large” or “small” (greater or less than 2.3 metres in length). The data can be found in link. The numbers in the data set (apart from the first column) are all frequencies. (You can ignore that first column “profile”.)\nOur aim is to predict food type from the other variables.\n\nRead in the data and display the first few lines. Describe how the data are not “tidy”.\nUse pivot_longer to arrange the data suitably for analysis (which will be using multinom). Demonstrate (by looking at the first few rows of your new data frame) that you now have something tidy.\nWhat is different about this problem, compared to Question here, that would make multinom the right tool to use?\nFit a suitable multinomial model predicting food type from gender, size and lake. Does each row represent one alligator or more than one? If more than one, account for this in your modelling.\nDo a test to see whether Gender should stay in the model. (This will entail fitting another model.) What do you conclude?\nPredict the probability that an alligator prefers each food type, given its size, gender (if necessary) and the lake it was found in, using the more appropriate of the two models that you have fitted so far. This means (i) making a data frame for prediction, and (ii) obtaining and displaying the predicted probabilities in a way that is easy to read.\nWhat do you think is the most important way in which the lakes differ? (Hint: look at where the biggest predicted probabilities are.)\nHow would you describe the major difference between the diets of the small and large alligators?"
  },
  {
    "objectID": "nominal-response.html#crimes-in-san-francisco",
    "href": "nominal-response.html#crimes-in-san-francisco",
    "title": "26  Logistic regression with nominal response",
    "section": "26.4 Crimes in San Francisco",
    "text": "26.4 Crimes in San Francisco\nThe data in link is a subset of a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are:\n\nDates: the date and time of the crime\nCategory: the category of crime, eg. “larceny” or “vandalism” (response).\nDescript: detailed description of crime.\nDayOfWeek: the day of the week of the crime.\nPdDistrict: the name of the San Francisco Police Department district in which the crime took place.\nResolution: how the crime was resolved\nAddress: approximate street address of crime\nX: longitude\nY: latitude\n\nOur aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which are the ones included in this data file.\nSome of the model-fitting takes a while (you’ll see why below). If you’re using R Markdown, you can wait for the models to fit each time you re-run your document, or insert cache=T in the top line of your code chunk (the one with r in curly brackets in it, above the actual code). Put a comma and the cache=T inside the curly brackets. What that does is to re-run that code chunk only if it changes; if it hasn’t changed it will use the saved results from last time it was run. That can save you a lot of waiting around.\n\nRead in the data and display the dataset (or, at least, part of it).\nFit a multinomial logistic regression that predicts crime category from day of week and district. (You don’t need to look at it.) The model-fitting produces some output. (If you’re using R Markdown, that will come with it.)\nFit a model that predicts Category from only the district. Hand in the output from the fitting process as well.\nUse anova to compare the two models you just obtained. What does the anova tell you?\nUsing your preferred model, obtain predicted probabilities that a crime will be of each of these four categories for each day of the week in the TENDERLOIN district (the name is ALL CAPS). This will mean constructing a data frame to predict from, obtaining the predictions and then displaying them suitably.\nDescribe briefly how the weekend days Saturday and Sunday differ from the rest."
  },
  {
    "objectID": "nominal-response.html#crimes-in-san-francisco-the-data",
    "href": "nominal-response.html#crimes-in-san-francisco-the-data",
    "title": "26  Logistic regression with nominal response",
    "section": "26.5 Crimes in San Francisco – the data",
    "text": "26.5 Crimes in San Francisco – the data\nThe data in link is a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are:\n\nDates: the date and time of the crime\nCategory: the category of crime, eg. “larceny” or “vandalism” (response).\nDescript: detailed description of crime.\nDayOfWeek: the day of the week of the crime.\nPdDistrict: the name of the San Francisco Police Department district in which the crime took place.\nResolution: how the crime was resolved\nAddress: approximate street address of crime\nX: longitude\nY: latitude\n\nOur aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which we will have to discover.\n\nRead in the data and verify that you have these columns and a lot of rows. (The data may take a moment to read in. You will see why.)\nHow is the response variable here different to the one in the question about steak preferences (and therefore why would multinom from package nnet be the method of choice)?\nFind out which crime categories there are, and arrange them in order of how many crimes there were in each category.\nWhich are the four most frequent “interesting” crime categories, that is to say, not including “other offenses” and “non-criminal”? Get them into a vector called my.crimes. See if you can find a way of doing this that doesn’t involve typing them in (for full marks).\n(Digression, but needed for the next part.) The R vector letters contains the lowercase letters from a to z. Consider the vector ('a','m',3,'Q'). Some of these are found amongst the lowercase letters, and some not. Type these into a vector v and explain briefly why v %in% letters produces what it does.\nWe are going to filter only the rows of our data frame that have one of the crimes in my.crimes as their Category. Also, select only the columns Category, DayOfWeek and PdDistrict. Save the resulting data frame and display its structure. (You should have a lot fewer rows than you did before.)\nSave these data in a file sfcrime1.csv."
  },
  {
    "objectID": "nominal-response.html#what-sports-do-these-athletes-play",
    "href": "nominal-response.html#what-sports-do-these-athletes-play",
    "title": "26  Logistic regression with nominal response",
    "section": "26.6 What sports do these athletes play?",
    "text": "26.6 What sports do these athletes play?\nThe data at link are physical and physiological measurements of 202 male and female Australian elite athletes. The data values are separated by tabs. We are going to see whether we can predict the sport an athlete plays from their height and weight.\nThe sports, if you care, are respectively basketball, “field athletics” (eg. shot put, javelin throw, long jump etc.), gymnastics, netball, rowing, swimming, 400m running, tennis, sprinting (100m or 200m running), water polo.\n\nRead in the data and display the first few rows.\nMake a scatterplot of height vs. weight, with the points coloured by what sport the athlete plays. Put height on the \\(x\\)-axis and weight on the \\(y\\)-axis.\nExplain briefly why a multinomial model (multinom from nnet) would be the best thing to use to predict sport played from the other variables.\nFit a suitable model for predicting sport played from height and weight. (You don’t need to look at the results.) 100 steps isn’t quite enough, so set maxit equal to a larger number to allow the estimation to finish.\nDemonstrate using anova that Wt should not be removed from this model.\nMake a data frame consisting of all combinations of Ht 160, 180 and 200 (cm), and Wt 50, 75, and 100 (kg), and use it to obtain predicted probabilities of athletes of those heights and weights playing each of the sports. Display the results. You might have to display them smaller, or reduce the number of decimal places1 to fit them on the page.\nFor an athlete who is 180 cm tall and weighs 100 kg, what sport would you guess they play? How sure are you that you are right? Explain briefly.\n\nMy solutions follow:"
  },
  {
    "objectID": "nominal-response.html#finding-non-missing-values-1",
    "href": "nominal-response.html#finding-non-missing-values-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.7 Finding non-missing values",
    "text": "26.7 Finding non-missing values\n* This is to prepare you for something in the next question. It’s meant to be easy.\nIn R, the code NA stands for “missing value” or “value not known”. In R, NA should not have quotes around it. (It is a special code, not a piece of text.)\n\nCreate a vector v that contains some numbers and some missing values, using c(). Put those values into a one-column data frame.\n\nSolution\nLike this. The arrangement of numbers and missing values doesn’t matter, as long as you have some of each: ::: {.cell}\nv &lt;- c(1, 2, NA, 4, 5, 6, 9, NA, 11)\nmydata &lt;- tibble(v)\nmydata\n\n\n\n  \n\n\n:::\nThis has one column called v.\n\\(\\blacksquare\\)\n\nObtain a new column containing is.na(v). When is this true and when is this false?\n\nSolution\n\nmydata &lt;- mydata %&gt;% mutate(isna = is.na(v))\nmydata\n\n\n\n  \n\n\n\nThis is TRUE if the corresponding element of v is missing (in my case, the third value and the second-last one), and FALSE otherwise (when there is an actual value there).\n\\(\\blacksquare\\)\n\nThe symbol ! means “not” in R (and other programming languages). What does !is.na(v) do? Create a new column containing that.\n\nSolution\nTry it and see. Give it whatever name you like. My name reflects that I know what it’s going to do: ::: {.cell}\nmydata &lt;- mydata %&gt;% mutate(notisna = !is.na(v))\nmydata\n\n\n\n  \n\n\n:::\nThis is the logical opposite of is.na: it’s true if there is a value, and false if it’s missing.\n\\(\\blacksquare\\)\n\nUse filter to display just the rows of your data frame that have a non-missing value of v.\n\nSolution\nfilter takes a column to say which rows to pick, in which case the column should contain something that either is TRUE or FALSE, or something that can be interpreted that way: ::: {.cell}\nmydata %&gt;% filter(notisna)\n\n\n\n  \n\n\n:::\nor you can provide filter something that can be calculated from what’s in the data frame, and also returns something that is either true or false:\n\nmydata %&gt;% filter(!is.na(v))\n\n\n\n  \n\n\n\nIn either case, I only have non-missing values of v.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#european-social-survey-and-voting-1",
    "href": "nominal-response.html#european-social-survey-and-voting-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.8 European Social Survey and voting",
    "text": "26.8 European Social Survey and voting\nThe European Social Survey is a giant survey carried out across Europe covering demographic information, attitudes to and amount of education, politics and so on. In this question, we will investigate what might make British people vote for a certain political party.\nThe information for this question is in a (large) spreadsheet at link. There is also a “codebook” at link that tells you what all the variables are. The ones we will use are the last five columns of the spreadsheet, described on pages 11 onwards of the codebook. (I could have given you more, but I didn’t want to make this any more complicated than it already was.)\n\nRead in the .csv file, and verify that you have lots of rows and columns.\n\nSolution\nThe obvious way. Printing it out will display some of the data and tell you how many rows and columns you have: ::: {.cell}\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ess.csv\"\ness &lt;- read_csv(my_url)\n\nRows: 2286 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): cntry, cname, cproddat, name\ndbl (13): cedition, cseqno, essround, edition, idno, dweight, pspwght, pweig...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ness\n\n\n\n  \n\n\n:::\n2286 rows and 17 columns.\n\\(\\blacksquare\\)\n\n* Use the codebook to find out what the columns prtvtgb, gndr, agea, eduyrs and inwtm are. What do the values 1 and 2 for gndr mean? (You don’t, at this point, have to worry about the values for the other variables.)\n\nSolution\nRespectively, political party voted for at last election, gender (of respondent), age at interview, years of full-time education, length of interview (in minutes). For gndr, male is 1 and female is 2.\n\\(\\blacksquare\\)\n\nThe three major political parties in Britain are the Conservative, Labour and Liberal Democrat. (These, for your information, correspond roughly to the Canadian Progressive Conservative, NDP and Liberal parties.) For the variable that corresponds to “political party voted for at the last election”, which values correspond to these three parties?\n\nSolution\n1, 2 and 3 respectively. (That was easy!)\n\\(\\blacksquare\\)\n\nNormally, I would give you a tidied-up data set. But I figure you could use some practice tidying this one up. As the codebook shows, there are some numerical codes for missing values, and we want to omit those. We want just the columns prtvtgb through inwtm from the right side of the spreadsheet. Use dplyr or tidyr tools to (i) select only these columns, (ii) include the rows that correspond to people who voted for one of the three major parties, (iii) include the rows that have an age at interview less than 999, (iv) include the rows that have less than 40 years of education, (v) include the rows that are not missing on inwtm (use the idea from Question~here for (v)). The last four of those (the inclusion of rows) can be done in one go.\n\nSolution\nThe major parties are numbered 1, 2 and 3, so we can select the ones less than 4 (or &lt;=3). The reference back to the last question is a hint to use !is.na(). It also works to use drop_na, if you are familiar with that.\n\ness %&gt;%\n  select(prtvtgb:inwtm) %&gt;%\n  filter(prtvtgb &lt; 4, agea &lt; 999, eduyrs &lt; 40, !is.na(inwtm)) -&gt; ess.major\n\nYou might get a weird error in select, something about “unused argument”. If this happens to you, it’s not because you used select wrong, it’s because you used the wrong select! There is one in MASS, and you need to make sure that this package is “detached” so that you use the select you want, namely the one in dplyr, loaded with the tidyverse. Use the instructions at the end of the mobile phones question or the abortion question to do this.\nThe other way around this is to say, instead of select, dplyr::select with two colons. This means “the select that lives in dplyr, no other”, and is what Wikipedia calls “disambiguation”: out of several things with the same name, you say which one you mean.\nIf you do the pipeline, you will probably not get it right the first time. (I didn’t.) For debugging, try out one step at a time, and summarize what you have so far, so that you can check it for correctness. A handy trick for that is to make the last piece of your pipeline summary(), which produces a summary of the columns of the resulting data frame. For example, I first did this (note that my filter is a lot simpler than the one above):\n\ness %&gt;%\n  select(prtvtgb:inwtm) %&gt;%\n  filter(prtvtgb &lt; 4, !is.na(inwtm)) %&gt;%\n  summary()\n\n    prtvtgb           gndr            agea            eduyrs     \n Min.   :1.000   Min.   :1.000   Min.   : 18.00   Min.   : 0.00  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.: 44.00   1st Qu.:11.00  \n Median :2.000   Median :2.000   Median : 58.00   Median :13.00  \n Mean   :1.803   Mean   :1.572   Mean   : 61.74   Mean   :14.23  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 71.00   3rd Qu.:16.00  \n Max.   :3.000   Max.   :2.000   Max.   :999.00   Max.   :88.00  \n     inwtm       \n Min.   :  7.00  \n 1st Qu.: 35.00  \n Median : 41.00  \n Mean   : 43.54  \n 3rd Qu.: 50.00  \n Max.   :160.00  \n\n\nThe mean of a categorical variable like party voted for or gender doesn’t make much sense, but it looks as if all the values are sensible ones (1 to 3 and 1, 2 respectively). However, the maximum values of age and years of education look like missing value codes, hence the other requirements I put in the question.2\nDisplaying as the last step of your pipeline also works, but the advantage of summary is that you get to see whether there are any unusual values, in this case unusually large values that are missing value codes.\n\\(\\blacksquare\\)\n\nWhy is my response variable nominal rather than ordinal? How can I tell? Which R function should I use, therefore, to fit my model?\n\nSolution\nThe response variable is political party voted for. There is no (obvious) ordering to this (unless you want to try to place the parties on a left-right spectrum), so this is nominal, and you’ll need multinom from package nnet.\nIf I had included the minor parties and you were working on a left-right spectrum, you would have had to decide where to put the somewhat libertarian Greens3 or the parties that exist only in Northern Ireland.4\n\\(\\blacksquare\\)\n\n* Take the political party voted for, and turn it into a factor, by feeding it into factor. Fit an appropriate model to predict political party voted for at the last election (as a factor) from all the other variables. Gender is really a categorical variable too, but since there are only two possible values it can be treated as a number.\n\nSolution\nThis, or something like it. multinom lives in package nnet, which you’ll have to install first if you haven’t already:\n\nlibrary(nnet)\ness.1 &lt;- multinom(factor(prtvtgb) ~ gndr + agea + eduyrs + inwtm, data = ess.major)\n\n# weights:  18 (10 variable)\ninitial  value 1343.602829 \niter  10 value 1256.123798\nfinal  value 1247.110080 \nconverged\n\n\nOr create a factor version of your response in the data frame first:\n\ness.major &lt;- ess.major %&gt;% mutate(party = factor(prtvtgb))\n\nand then:\n\ness.1a &lt;- multinom(party ~ gndr + agea + eduyrs + inwtm, data = ess.major)\n\n# weights:  18 (10 variable)\ninitial  value 1343.602829 \niter  10 value 1256.123798\nfinal  value 1247.110080 \nconverged\n\n\n\\(\\blacksquare\\)\n\nWe have a lot of explanatory variables. The standard way to test whether we need all of them is to take one of them out at a time, and test which ones we can remove. This is a lot of work. We won’t do that. Instead, the R function step does what you want. You feed step two things: a fitted model object, and the option trace=0 (otherwise you get a lot of output). The final part of the output from step tells you which explanatory variables you need to keep. Run step on your fitted model. Which explanatory variables need to stay in the model here?\n\nSolution\nI tried to give you lots of hints here:\n\ness.2a &lt;- step(ess.1, trace = 0)\n\ntrying - gndr \ntrying - agea \ntrying - eduyrs \ntrying - inwtm \n# weights:  15 (8 variable)\ninitial  value 1343.602829 \niter  10 value 1248.343563\nfinal  value 1248.253638 \nconverged\ntrying - agea \ntrying - eduyrs \ntrying - inwtm \n\ness.2a\n\nCall:\nmultinom(formula = factor(prtvtgb) ~ agea + eduyrs + inwtm, data = ess.major)\n\nCoefficients:\n  (Intercept)        agea     eduyrs       inwtm\n2    1.632266 -0.02153694 -0.0593757 0.009615167\n3   -1.281031 -0.01869263  0.0886487 0.009337084\n\nResidual Deviance: 2496.507 \nAIC: 2512.507 \n\n\nIf you didn’t save your output in a variable, you’ll get my last bit automatically.\nThe end of the output gives us coefficients for (and thus tells us we need to keep) age, years of education and interview length.\nThe actual numbers don’t mean much; it’s the indication that the variable has stayed in the model that makes a difference.5\nIf you’re wondering about the process: first step tries to take out each explanatory variable, one at a time, from the starting model (the one that contains all the variables). Then it finds the best model out of those and fits it. (It doesn’t tell us which model this is, but evidently it’s the one without gender.) Then it takes that model and tries to remove its explanatory variables one at a time (there are only three of them left). Having decided it cannot remove any of them, it stops, and shows us what’s left.\nLeaving out the trace=0 shows more output and more detail on the process, but I figured this was enough (and this way, you don’t have to wade through all of that output). Try values like 1 or 2 for trace and see what you get.\n\\(\\blacksquare\\)\n\nFit the model indicated by step (in the last part).\n\nSolution\nCopy and paste, and take out the variables you don’t need. Or, better, save the output from step in a variable. This then becomes a fitted model object and you can look at it any of the ways you can look at a model fit. I found that gender needed to be removed, but if yours is different, follow through with whatever your step said to do.\n\ness.2 &lt;- multinom(party ~ agea + eduyrs + inwtm, data = ess.major)\n\n# weights:  15 (8 variable)\ninitial  value 1343.602829 \niter  10 value 1248.343563\nfinal  value 1248.253638 \nconverged\n\n\nIf you saved the output from step, you’ll already have this and you don’t need to do it again:\n\nanova(ess.2, ess.2a)\n\n\n\n  \n\n\n\nSame model.\n\\(\\blacksquare\\)\n\nI didn’t think that interview length could possibly be relevant to which party a person voted for. Test whether interview length can be removed from your model of the last part. What do you conclude? (Note that step and this test may disagree.)\n\nSolution\nFit the model without inwtm:\n\ness.3 &lt;- multinom(party ~ agea + eduyrs, data = ess.major)\n\n# weights:  12 (6 variable)\ninitial  value 1343.602829 \niter  10 value 1250.418281\nfinal  value 1250.417597 \nconverged\n\n\nand then use anova to compare them:\n\nanova(ess.3, ess.2)\n\n\n\n  \n\n\n\nThe P-value, 0.1149, is not small, which says that the smaller model is good, ie. the one without interview length.\nI thought drop1 would also work here, but it appears not to:\n\ndrop1(ess.1, test = \"Chisq\")\n\ntrying - gndr \n\n\nError in if (trace) {: argument is not interpretable as logical\n\n\nI think that’s a bug in multinom, since normally if step works, then drop1 will work too (normally step uses drop1).\nThe reason for the disagreement between step and anova is that step will tend to keep marginal explanatory variables, that is, ones that are “potentially interesting” but whose P-values might not be less than 0.05. There is still no substitute for your judgement in figuring out what to do! step uses a thing called AIC to decide what to do, rather than actually doing a test. If you know about “adjusted R-squared” in choosing explanatory variables for a regression, it’s the same idea: a variable can be not quite significant but still make the adjusted R-squared go up (typically only a little).\n\\(\\blacksquare\\)\n\nUse your best model to obtain predictions from some suitably chosen combinations of values of the explanatory variables that remain. (If you have quantitative explanatory variables left, you could use their first and third quartiles as values to predict from. Running summary on the data frame will get summaries of all the variables.)\n\nSolution\nFirst make our new data frame of values to predict from. You can use quantile or summary to find the quartiles. I only had agea and eduyrs left, having decided that interview time really ought to come out:\n\nsummary(ess.major)\n\n    prtvtgb           gndr            agea           eduyrs     \n Min.   :1.000   Min.   :1.000   Min.   :18.00   Min.   : 0.00  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:44.00   1st Qu.:11.00  \n Median :2.000   Median :2.000   Median :58.00   Median :13.00  \n Mean   :1.803   Mean   :1.574   Mean   :57.19   Mean   :13.45  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:71.00   3rd Qu.:16.00  \n Max.   :3.000   Max.   :2.000   Max.   :94.00   Max.   :33.00  \n     inwtm       party  \n Min.   :  7.0   1:484  \n 1st Qu.: 35.0   2:496  \n Median : 41.0   3:243  \n Mean   : 43.7          \n 3rd Qu.: 50.0          \n Max.   :160.0          \n\n\nQuartiles for age are 44 and 71, and for years of education are 11 and 16.\nThis time, instead of predicting for variable values that predictions chooses for us (like a five-number summary), we are predicting for “custom” values, ones that we chose. To set that up, the marginaleffects way is to use datagrid like this:\n\nnew &lt;- datagrid(model = ess.3, agea = c(44, 71), eduyrs = c(11, 16))\nnew\n\n\n\n  \n\n\n\nWhat datagrid does is to make all combinations of your variable values, and along with that, to use “typical” values for the others: the mean, in the case of quantitative variables like inwtm, and the most common category for categorical ones like party. If you feed datagrid a model first, it only includes variables in that model, which is easier to make sense of:\n\ndatagrid(newdata = ess.major, agea = c(44, 71), eduyrs = c(11, 16))\n\n\n\n  \n\n\n\nThe other variables don’t make much sense, since they are really categorical but expressed as numbers, but they are not in the best model, so that doesn’t do any harm. (In other cases, you might need to be more careful.)\nNext, we feed this into predictions, using the above dataframe as newdata, and with our best model, ess.3 (the one without interview length). The results might confuse you at first, since you will probably get an error:\n\ncbind(predictions(ess.3, newdata = new))\n\n\n\n  \n\n\n\nThe error message gives you a hint about what to do: add a type = \"probs\" to predictions (which is a consequence of how multinom works):\n\ncbind(predictions(ess.3, newdata = new, type = \"probs\"))\n\n\n\n  \n\n\n\nThere are twelve rows for our four predictions, because there are three predictions for each of our four “people”: the probabilities of each one voting for each of the three parties. The party predicted for is in the column group, and the probability of each person (labelled by rowid) voting for that party is in predicted. Let’s simplify things by keeping only those columns and the ones we are predicting for:\n\ncbind(predictions(ess.3, newdata = new, type = \"probs\")) %&gt;% \n  select(rowid, group, estimate, agea, eduyrs)\n\n\n\n  \n\n\n\nand then pivot wider to get all three predictions for each person on one line:\n\ncbind(predictions(ess.3, newdata = new, type = \"probs\")) %&gt;% \n  select(rowid, group, estimate, agea, eduyrs) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nWhat is the effect of increasing age? What is the effect of an increase in years of education?\n\nSolution\nTo assess the effect of age, hold years of education constant. Thus, compare lines 1 and 3 (or 2 and 4): increasing age tends to increase the chance that a person will vote Conservative (party 1), and decrease the chance that a person will vote Labour (party 2). There doesn’t seem to be much effect of age on the chance that a person will vote Liberal Democrat.\nTo assess education, hold age constant, and thus compare rows 1 and 2 (or rows 3 and 4). This time, there isn’t much effect on the chances of voting Conservative, but as education increases, the chance of voting Labour goes down, and the chance of voting Liberal Democrat goes up.\nA little history: back 150 or so years ago, Britain had two political parties, the Tories and the Whigs. The Tories became the Conservative party (and hence, in Britain and in Canada, the Conservatives are nicknamed Tories6). The Whigs became Liberals. At about the same time as working people got to vote (not women, yet, but working men) the Labour Party came into existence. The Labour Party has always been affiliated with working people and trades unions, like the NDP here. But power has typically alternated between Conservative and Labour goverments, with the Liberals as a third party. In the 1980s a new party called the Social Democrats came onto the scene, but on realizing that they couldn’t make much of a dent by themselves, they merged with the Liberals to form the Liberal Democrats, which became a slightly stronger third party.\nI was curious about what the effect of interview length would be. Presumably, the effect is small, but I have no idea which way it would be. To assess this, this is predictions again, but this time we can let predictions pick some values for inwtm for us, and leave everything else at their mean. We have to remember to use the model ess.2 that contained interview length, this time:\n\ncbind(predictions(ess.2, variables = \"inwtm\", type = \"probs\")) %&gt;% \n  select(rowid, group, estimate, agea, eduyrs, inwtm) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\nAs interview length goes up (for a respondent with average age and years of education, though the pattern would be the same for people of different ages and different amounts of education), the respondent is less likely to vote Conservative (party 1), and more likely to vote for one of the other two parties.\nBut, as we suspected, the effect is small (except for that very long interview length) and not really worth worrying about.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#alligator-food-1",
    "href": "nominal-response.html#alligator-food-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.9 Alligator food",
    "text": "26.9 Alligator food\nWhat do alligators most like to eat? 219 alligators were captured in four Florida lakes. Each alligator’s stomach contents were observed, and the food that the alligator had eaten was classified into one of five categories: fish, invertebrates (such as snails or insects), reptiles (such as turtles), birds, and “other” (such as amphibians, plants or rocks). The researcher noted for each alligator what that alligator had most of in its stomach, as well as the gender of each alligator and whether it was “large” or “small” (greater or less than 2.3 metres in length). The data can be found in link. The numbers in the data set (apart from the first column) are all frequencies. (You can ignore that first column “profile”.)\nOur aim is to predict food type from the other variables.\n\nRead in the data and display the first few lines. Describe how the data are not “tidy”.\n\nSolution\nSeparated by exactly one space: ::: {.cell}\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/alligator.txt\"\ngators.orig &lt;- read_delim(my_url, \" \")\n\nRows: 16 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (3): Gender, Size, Lake\ndbl (6): profile, Fish, Invertebrate, Reptile, Bird, Other\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngators.orig\n\n\n\n  \n\n\n:::\nThe last five columns are all frequencies. Or, one of the variables (food type) is spread over five columns instead of being contained in one. Either is good.\nMy choice of “temporary” name reflects that I’m going to obtain a “tidy” data frame called gators in a moment.\n\\(\\blacksquare\\)\n\nUse pivot_longer to arrange the data suitably for analysis (which will be using multinom). Demonstrate (by looking at the first few rows of your new data frame) that you now have something tidy.\n\nSolution\nI’m creating my “official” data frame here:\n\ngators.orig %&gt;% \n  pivot_longer(Fish:Other, names_to = \"Food.type\", values_to = \"Frequency\") -&gt; gators\ngators\n\n\n\n  \n\n\n\nI gave my column names Capital Letters to make them consistent with the others (and in an attempt to stop infesting my brain with annoying variable-name errors when I fit models later).\nLooking at the first few lines reveals that I now have a column of food types and one column of frequencies, both of which are what I wanted. I can check that I have all the different food types by finding the distinct ones:\n\ngators %&gt;% distinct(Food.type)\n\n\n\n  \n\n\n\n(Think about why count would be confusing here.)\nNote that Food.type is text (chr) rather than being a factor. I’ll hold my breath and see what happens when I fit a model where it is supposed to be a factor.\n\\(\\blacksquare\\)\n\nWhat is different about this problem, compared to Question here, that would make multinom the right tool to use?\n\nSolution\nLook at the response variable Food.type (or whatever you called it): this has multiple categories, but they are not ordered in any logical way. Thus, in short, a nominal response.\n\\(\\blacksquare\\)\n\nFit a suitable multinomial model predicting food type from gender, size and lake. Does each row represent one alligator or more than one? If more than one, account for this in your modelling.\n\nSolution\nEach row of the tidy gators represents as many alligators as are in the Frequency column. That is, if you look at female small alligators in Lake George that ate mainly fish, there are three of those.7 This to remind you to include the weights piece, otherwise multinom will assume that you have one observation per line and not as many as the number in Frequency.\nThat is the reason that count earlier would have been confusing: it would have told you how many rows contained each food type, rather than how many alligators, and these would have been different: ::: {.cell}\ngators %&gt;% count(Food.type)\n\n\n\n  \n\n\ngators %&gt;% count(Food.type, wt = Frequency)\n\n\n\n  \n\n\n:::\nEach food type appears on 16 rows, but is the favoured diet of very different numbers of alligators. Note the use of wt= to specify a frequency variable.8\nYou ought to understand why those are different.\nAll right, back to modelling:\n\nlibrary(nnet)\ngators.1 &lt;- multinom(Food.type ~ Gender + Size + Lake,\n  weights = Frequency, data = gators\n)\n\n# weights:  35 (24 variable)\ninitial  value 352.466903 \niter  10 value 270.228588\niter  20 value 268.944257\nfinal  value 268.932741 \nconverged\n\n\nThis worked, even though Food.type was actually text. I guess it got converted to a factor. The ordering of the levels doesn’t matter here anyway, since this is not an ordinal model.\nNo need to look at it, since the output is kind of confusing anyway: ::: {.cell}\nsummary(gators.1)\n\nCall:\nmultinom(formula = Food.type ~ Gender + Size + Lake, data = gators, \n    weights = Frequency)\n\nCoefficients:\n             (Intercept)     Genderm   Size&gt;2.3 Lakehancock Lakeoklawaha\nFish           2.4322304  0.60674971 -0.7308535  -0.5751295    0.5513785\nInvertebrate   2.6012531  0.14378459 -2.0671545  -2.3557377    1.4645820\nOther          1.0014505  0.35423803 -1.0214847   0.1914537    0.5775317\nReptile       -0.9829064 -0.02053375 -0.1741207   0.5534169    3.0807416\n             Laketrafford\nFish          -1.23681053\nInvertebrate  -0.08096493\nOther          0.32097943\nReptile        1.82333205\n\nStd. Errors:\n             (Intercept)   Genderm  Size&gt;2.3 Lakehancock Lakeoklawaha\nFish           0.7706940 0.6888904 0.6523273   0.7952147     1.210229\nInvertebrate   0.7917210 0.7292510 0.7084028   0.9463640     1.232835\nOther          0.8747773 0.7623738 0.7250455   0.9072182     1.374545\nReptile        1.2827234 0.9088217 0.8555051   1.3797755     1.591542\n             Laketrafford\nFish            0.8661187\nInvertebrate    0.8814625\nOther           0.9589807\nReptile         1.3388017\n\nResidual Deviance: 537.8655 \nAIC: 585.8655 \n\n:::\nYou get one coefficient for each variable (along the top) and for each response group (down the side), using the first group as a baseline everywhere. These numbers are hard to interpret; doing predictions is much easier.\n\\(\\blacksquare\\)\n\nDo a test to see whether Gender should stay in the model. (This will entail fitting another model.) What do you conclude?\n\nSolution\nThe other model to fit is the one without the variable you’re testing: ::: {.cell}\ngators.2 &lt;- update(gators.1, . ~ . - Gender)\n\n# weights:  30 (20 variable)\ninitial  value 352.466903 \niter  10 value 272.246275\niter  20 value 270.046891\nfinal  value 270.040139 \nconverged\n\n:::\nI did update here to show you that it works, but of course there’s no problem in just writing out the whole model again and taking out Gender, preferably by copying and pasting:\n\ngators.2x &lt;- multinom(Food.type ~ Size + Lake,\n  weights = Frequency, data = gators\n)\n\n# weights:  30 (20 variable)\ninitial  value 352.466903 \niter  10 value 272.246275\niter  20 value 270.046891\nfinal  value 270.040139 \nconverged\n\n\nand then you compare the models with and without Gender using anova:\n\nanova(gators.2, gators.1)\n\n\n\n  \n\n\n\nThe P-value is not small, so the two models fit equally well, and therefore we should go with the smaller, simpler one: that is, the one without Gender.\nSometimes drop1 works here too (and sometimes it doesn’t, for reasons I haven’t figured out):\n\ndrop1(gators.1, test = \"Chisq\")\n\ntrying - Gender \n\n\nError in if (trace) {: argument is not interpretable as logical\n\n\nI don’t even know what this error message means, never mind what to do about it.\n\\(\\blacksquare\\)\n\nPredict the probability that an alligator prefers each food type, given its size, gender (if necessary) and the lake it was found in, using the more appropriate of the two models that you have fitted so far. This means (i) making a data frame for prediction, and (ii) obtaining and displaying the predicted probabilities in a way that is easy to read.\n\nSolution\nOur best model gators.2 contains size and lake, so we need to predict for all combinations of those.\nFirst, get hold of those combinations, which you can do this way:\n\nnew &lt;- datagrid(model = gators.2, \n                Size = levels(factor(gators$Size)),\n                Lake = levels(factor(gators$Lake)))\nnew\n\n\n\n  \n\n\n\nThere are four lakes and two sizes, so we should (and do) have eight rows.\nNext, use this to make predictions, not forgetting the type = \"probs\" that you need for this kind of model. The last step puts the “long” predictions “wider” so that you can eyeball them:\n\ncbind(predictions(gators.2, newdata = new)) %&gt;% \n  select(group, estimate, Size, Lake) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate) -&gt; preds1\npreds1\n\n\n\n  \n\n\n\nI saved these to look at again later (you don’t need to).\nIf you thought that the better model was the one with Gender in it, or you otherwise forgot that you didn’t need Gender then you needed to include Gender in new also.\n\\(\\blacksquare\\)\n\nWhat do you think is the most important way in which the lakes differ? (Hint: look at where the biggest predicted probabilities are.)\n\nSolution\nHere are the predictions again: ::: {.cell}\npreds1\n\n\n\n  \n\n\n:::\nFollowing my own hint: the preferred diet in George and Hancock lakes is fish, but the preferred diet in Oklawaha and Trafford lakes is (at least sometimes) invertebrates. That is to say, the preferred diet in those last two lakes is less likely to be invertebrates than it is in the first two (comparing for alligators of the same size). This is true for both large and small alligators, as it should be, since there is no interaction in the model.\nThat will do, though you can also note that reptiles are more commonly found in the last two lakes, and birds sometimes appear in the diet in Hancock and Trafford but rarely in the other two lakes.\nAnother way to think about this is to hold size constant and compare lakes (and then check that it applies to the other size too). In this case, you’d find the biggest predictions among the first four rows, and then check that the pattern persists in the second four rows. (It does.)\nI think looking at predicted probabilities like this is the easiest way to see what the model is telling you.\n\\(\\blacksquare\\)\n\nHow would you describe the major difference between the diets of the small and large alligators?\n\nSolution\nSame idea: hold lake constant, and compare small and large, then check that your conclusion holds for the other lakes as it should. For example, in George Lake, the large alligators are more likely to eat fish, and less likely to eat invertebrates, compared to the small ones. The other food types are not that much different, though you might also note that birds appear more in the diets of large alligators than small ones. Does that hold in the other lakes? I think so, though there is less difference for fish in Hancock lake than the others (where invertebrates are rare for both sizes). Birds don’t commonly appear in any alligator’s diets, but where they do, they are commoner for large alligators than small ones.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#crimes-in-san-francisco-1",
    "href": "nominal-response.html#crimes-in-san-francisco-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.10 Crimes in San Francisco",
    "text": "26.10 Crimes in San Francisco\nThe data in link is a subset of a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are:\n\nDates: the date and time of the crime\nCategory: the category of crime, eg. “larceny” or “vandalism” (response).\nDescript: detailed description of crime.\nDayOfWeek: the day of the week of the crime.\nPdDistrict: the name of the San Francisco Police Department district in which the crime took place.\nResolution: how the crime was resolved\nAddress: approximate street address of crime\nX: longitude\nY: latitude\n\nOur aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which are the ones included in this data file.\nSome of the model-fitting takes a while (you’ll see why below). If you’re using R Markdown, you can wait for the models to fit each time you re-run your document, or insert cache=T in the top line of your code chunk (the one with r in curly brackets in it, above the actual code). Put a comma and the cache=T inside the curly brackets. What that does is to re-run that code chunk only if it changes; if it hasn’t changed it will use the saved results from last time it was run. That can save you a lot of waiting around.\n\nRead in the data and display the dataset (or, at least, part of it).\n\nSolution\nThe usual: ::: {.cell hash=‘nominal-response_cache/html/napoli_70a037c743494c5b0a130364332126c8’}\nmy_url &lt;- \"http://utsc.utoronto.ca/~butler/d29/sfcrime1.csv\"\nsfcrime &lt;- read_csv(my_url)\n\nRows: 359528 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Category, DayOfWeek, PdDistrict\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsfcrime\n\n# A tibble: 359,528 × 3\n   Category      DayOfWeek PdDistrict\n   &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;     \n 1 LARCENY/THEFT Wednesday NORTHERN  \n 2 LARCENY/THEFT Wednesday PARK      \n 3 LARCENY/THEFT Wednesday INGLESIDE \n 4 VEHICLE THEFT Wednesday INGLESIDE \n 5 VEHICLE THEFT Wednesday BAYVIEW   \n 6 LARCENY/THEFT Wednesday RICHMOND  \n 7 LARCENY/THEFT Wednesday CENTRAL   \n 8 LARCENY/THEFT Wednesday CENTRAL   \n 9 LARCENY/THEFT Wednesday NORTHERN  \n10 ASSAULT       Wednesday INGLESIDE \n# ℹ 359,518 more rows\n\n:::\nThis is a tidied-up version of the data, with only the variables we’ll look at, and only the observations from one of the “big four” crimes, a mere 300,000 of them. This is the data set we created earlier.\n\\(\\blacksquare\\)\n\nFit a multinomial logistic regression that predicts crime category from day of week and district. (You don’t need to look at it.) The model-fitting produces some output, which will at least convince you that it is working, since it takes some time.\n\nSolution\nThe modelling part is easy enough, as long as you can get the uppercase letters in the right places:\n\nsfcrime.1 &lt;- multinom(Category ~ DayOfWeek + PdDistrict, data=sfcrime)\n\n# weights:  68 (48 variable)\ninitial  value 498411.639069 \niter  10 value 430758.073422\niter  20 value 430314.270403\niter  30 value 423303.587698\niter  40 value 420883.528523\niter  50 value 418355.242764\nfinal  value 418149.979622 \nconverged\n\n\n\\(\\blacksquare\\)\n\nFit a model that predicts Category from only the district.\n\nSolution\nSame idea. Write it out, or use update: ::: {.cell}\nsfcrime.2 &lt;- update(sfcrime.1, . ~ . - DayOfWeek)\n\n# weights:  44 (30 variable)\ninitial  value 498411.639069 \niter  10 value 426003.543845\niter  20 value 425542.806828\niter  30 value 421715.787609\nfinal  value 418858.235297 \nconverged\n\n:::\n\\(\\blacksquare\\)\n\nUse anova to compare the two models you just obtained. What does the anova tell you?\n\nSolution\nThis:\n\nanova(sfcrime.2, sfcrime.1)\n\n\n\n  \n\n\n\nThis is a very small P-value. The null hypothesis is that the two models are equally good, and this is clearly rejected. We need the bigger model: that is, we need to keep DayOfWeek in there, because the pattern of crimes (in each district) differs over day of week.\nOne reason the P-value came out so small is that we have a ton of data, so that even a very small difference between days of the week could come out very strongly significant. The Machine Learning people (this is a machine learning dataset) don’t worry so much about tests for that reason: they are more concerned with predicting things well, so they just throw everything into the model and see what comes out.\n\\(\\blacksquare\\)\n\nUsing your preferred model, obtain predicted probabilities that a crime will be of each of these four categories for each day of the week in the TENDERLOIN district (the name is ALL CAPS). This will mean constructing a data frame to predict from, obtaining the predictions and then displaying them suitably.\n\nSolution\nUse datagrid first to get the combinations you want (and only those), namely all the days of the week, but only the district called TENDERLOIN.\nSo, let’s get the days of the week. The easiest way is to count them and ignore the counts:9\n\nsfcrime %&gt;% count(DayOfWeek) %&gt;% \n  pull(DayOfWeek) -&gt; daysofweek\ndaysofweek\n\n[1] \"Friday\"    \"Monday\"    \"Saturday\"  \"Sunday\"    \"Thursday\"  \"Tuesday\"  \n[7] \"Wednesday\"\n\n\nAnother way is the levels(factor()) thing you may have seen before:\n\nlevels(factor(sfcrime$DayOfWeek))\n\n[1] \"Friday\"    \"Monday\"    \"Saturday\"  \"Sunday\"    \"Thursday\"  \"Tuesday\"  \n[7] \"Wednesday\"\n\n\nNow we can use these in datagrid:10\n\nnew &lt;- datagrid(model = sfcrime.1, DayOfWeek = daysofweek, PdDistrict = \"TENDERLOIN\")\nnew\n\n\n\n  \n\n\n\nGood. And then predict for just these. This is slow, but not as slow as predicting for all districts. I’m saving the result of this slow part, so that it doesn’t matter if I change my mind later about what to do with it. I want to make sure that I don’t have to do it again, is all:11\n\np &lt;- cbind(predictions(sfcrime.1, newdata = new, type = \"probs\"))\np\n\n\n\n  \n\n\n\nThis, as you remember, is long format, so grab the columns you need from it and pivot wider. The columns you want to make sure you have are estimate, group (the type of crime), and the day of week:\n\np %&gt;% \n  select(group, estimate, DayOfWeek) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\nSuccess. (If you don’t get rid of enough, you still have 28 rows and a bunch of missing values; in that case, pivot_wider will infer that everything should be in its own row.)\n\\(\\blacksquare\\)\n\nDescribe briefly how the weekend days Saturday and Sunday differ from the rest.\n\nSolution\nThe days ended up in some quasi-random order, but Saturday and Sunday are still together, so we can still easily compare them with the rest. My take is that the last two columns don’t differ much between weekday and weekend, but the first two columns do: the probability of a crime being an assault is a bit higher on the weekend, and the probability of a crime being drug-related is a bit lower. I will accept anything reasonable supported by the predictions you got. We said there was a strongly significant day-of-week effect, but the changes from weekday to weekend are actually pretty small (but the changes from one weekday to another are even smaller). This supports what I guessed before, that with this much data even a small effect (the one shown here) is statistically significant.12\nExtra: I want to compare another district. What districts do we have?\n\nsfcrime %&gt;% count(PdDistrict)\n\n\n\n  \n\n\n\nThis is the number of our “big four” crimes committed in each district. Let’s look at the lowest-crime district RICHMOND. I copy and paste my code. Since I want to compare two districts, I include both:\n\nnew &lt;- datagrid(model = sfcrime.1, PdDistrict = c(\"TENDERLOIN\", \"RICHMOND\"), DayOfWeek = daysofweek)\nnew\n\n\n\n  \n\n\n\nand then as we just did. I’m going to be a bit more selective about the columns I keep this time, since the display will be a bit wider and I don’t want it to be too big for the page:\n\np &lt;- cbind(predictions(sfcrime.1, newdata = new, type = \"probs\"))\np\n\n\n\n  \n\n\n\n\np %&gt;% \n  select(group, estimate, DayOfWeek, PdDistrict) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\nRichmond is obviously not a drug-dealing kind of place; most of its crimes are theft of one kind or another. But the predicted effect of weekday vs. weekend is the same: Richmond doesn’t have many assaults or drug crimes, but it also has more assaults and fewer drug crimes on the weekend than during the week. There is not much effect of day of the week on the other two crime types in either place.\nThe consistency of pattern, even though the prevalence of the different crime types differs by location, is a feature of the model: we fitted an additive model, that says there is an effect of weekday, and independently there is an effect of location. The pattern over weekday is the same for each location, implied by the model. This may or may not be supported by the actual data.\nThe way to assess this is to fit a model with interaction (we will see more of this when we revisit ANOVA later), and compare the fit. This one takes longer to fit:\n\nsfcrime.3 &lt;- update(sfcrime.1, . ~ . + DayOfWeek * PdDistrict)\n\n# weights:  284 (210 variable)\ninitial  value 498411.639069 \niter  10 value 429631.807781\niter  20 value 429261.427210\niter  30 value 428111.625547\niter  40 value 423807.031450\niter  50 value 421129.496196\niter  60 value 420475.833895\niter  70 value 419523.235916\niter  80 value 418621.612920\niter  90 value 418147.629782\niter 100 value 418036.670485\nfinal  value 418036.670485 \nstopped after 100 iterations\n\n# anova(sfcrime.1,sfcrime.3)\n\nThis one didn’t actually complete the fitting process: it got to 100 times around and stopped (since that’s the default limit). We can make it go a bit further thus:\n\nsfcrime.3 &lt;- update(sfcrime.1, .~.+DayOfWeek*PdDistrict, maxit=300)\n\n# weights:  284 (210 variable)\ninitial  value 498411.639069 \niter  10 value 429631.807781\niter  20 value 429261.427210\niter  30 value 428111.625547\niter  40 value 423807.031450\niter  50 value 421129.496196\niter  60 value 420475.833895\niter  70 value 419523.235916\niter  80 value 418621.612920\niter  90 value 418147.629782\niter 100 value 418036.670485\niter 110 value 417957.337016\niter 120 value 417908.465189\niter 130 value 417890.580843\niter 140 value 417874.839492\niter 150 value 417867.449342\niter 160 value 417862.626213\niter 170 value 417858.654628\nfinal  value 417858.031854 \nconverged\n\nanova(sfcrime.1, sfcrime.3)\n\n\n\n  \n\n\n\nThis time, we got to the end. (The maxit=300 gets passed on to multinom, and says “go around up to 300 times if needed”.) As you will see if you try it, this takes a bit of time to run.\nThis anova is also strongly significant, but in the light of the previous discussion, the differential effect of day of week in different districts might not be very big. We can even assess that; we have all the machinery for the predictions, and we just have to apply them to this model. The only thing is waiting for it to finish!\n\np &lt;- cbind(predictions(sfcrime.3, newdata = new, type = \"probs\"))\np\n\n\n\n  \n\n\n\n\np %&gt;% \n  select(group, estimate, DayOfWeek, PdDistrict) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\nIt doesn’t look much different. Maybe the Tenderloin has a larger weekend increase in assaults than Richmond does.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#crimes-in-san-francisco-the-data-1",
    "href": "nominal-response.html#crimes-in-san-francisco-the-data-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.11 Crimes in San Francisco – the data",
    "text": "26.11 Crimes in San Francisco – the data\nThe data in link is a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are:\n\nDates: the date and time of the crime\nCategory: the category of crime, eg. “larceny” or “vandalism” (response).\nDescript: detailed description of crime.\nDayOfWeek: the day of the week of the crime.\nPdDistrict: the name of the San Francisco Police Department district in which the crime took place.\nResolution: how the crime was resolved\nAddress: approximate street address of crime\nX: longitude\nY: latitude\n\nOur aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which we will have to discover.\n\nRead in the data and verify that you have these columns and a lot of rows. (The data may take a moment to read in. You will see why.)\n\nSolution\n\nmy_url &lt;- \"http://utsc.utoronto.ca/~butler/d29/sfcrime.csv\"\nsfcrime &lt;- read_csv(my_url)\n\nRows: 878049 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): Category, Descript, DayOfWeek, PdDistrict, Resolution, Address\ndbl  (2): X, Y\ndttm (1): Dates\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsfcrime\n\n\n\n  \n\n\n\nThose columns indeed, and pushing a million rows! That’s why it took so long!\nThere are also 39 categories of crime, so we need to cut that down some. There are only ten districts, however, so we should be able to use that as is.\n\\(\\blacksquare\\)\n\nHow is the response variable here different to the one in the question about steak preferences (and therefore why would multinom from package nnet be the method of choice)?\n\nSolution\nSteak preferences have a natural order, while crime categories do not. Since they are unordered, multinom is better than polr.\n\\(\\blacksquare\\)\n\nFind out which crime categories there are, and arrange them in order of how many crimes there were in each category.\n\nSolution\nThis can be the one you know, a group-by and summarize, followed by arrange to sort:\n\nsfcrime %&gt;% group_by(Category) %&gt;%\nsummarize(count=n()) %&gt;%\narrange(desc(count))\n\n\n\n  \n\n\n\nor this one does the same thing and saves a step:\n\nsfcrime %&gt;% count(Category) %&gt;%\narrange(desc(n))\n\n\n\n  \n\n\n\nFor this one, do the count step first, to see what you get. It produces a two-column data frame with the column of counts called n. So now you know that the second line has to be arrange(desc(n)), whereas before you tried count, all you knew is that it was arrange-desc-something.\nYou need to sort in descending order so that the categories you want to see actually do appear at the top.\n\\(\\blacksquare\\)\n\nWhich are the four most frequent “interesting” crime categories, that is to say, not including “other offenses” and “non-criminal”? Get them into a vector called my.crimes. See if you can find a way of doing this that doesn’t involve typing them in (for full marks).\n\nSolution\nThe most frequent interesting ones are, in order, larceny-theft, assault, drug-narcotic and vehicle theft. The fact that “other offenses” is so big indicates that there are a lot of possible crimes out there, and even 39 categories of crime isn’t enough. “Non-criminal” means, I think, that the police were called, but on arriving at the scene, they found that no law had been broken.\nI think the easy way to get the “top four” crimes out is to pull them out of the data frame that count produces. They are rows 1, 4, 5 and 6, so add a slice to your pipeline:\n\nmy.rows &lt;- c(1,4,5,6)\nsfcrime %&gt;% count(Category) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice(my.rows) %&gt;% pull(Category) -&gt; my.crimes\nmy.crimes\n\n[1] \"LARCENY/THEFT\" \"ASSAULT\"       \"DRUG/NARCOTIC\" \"VEHICLE THEFT\"\n\n\nI just want the Category column (as a vector), and pull is the way to get that. (If I don’t do pull, I get a data frame.)\nIf you can’t think of anything, just type them into a vector with c, or better, copy-paste them from the console. But that’s a last resort, and would cost you a point. If you do this, they have to match exactly, UPPERCASE and all.\n\\(\\blacksquare\\)\n\n(Digression, but needed for the next part.) The R vector letters contains the lowercase letters from a to z. Consider the vector ('a','m',3,'Q'). Some of these are found amongst the lowercase letters, and some not. Type these into a vector v and explain briefly why v %in% letters produces what it does.\n\nSolution\nThis is the ultimate “try it and see”:\n\nv=c('a','m',3,'Q')\nv %in% letters\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nThe first two elements of the answer are TRUE because lowercase-a and lowercase-m can be found in the lowercase letters somewhere. The other two are false because the number 3 and the uppercase-Q cannot be found anywhere in the lowercase letters.\nThe name is %in% because it’s asking whether each element of the first vector (one at a time) is in the set defined by the second thing: “is a a lowercase letter?” … is “Q a lowercase letter?” and getting the answers “yes, yes, no, no”.\n\\(\\blacksquare\\)\n\nWe are going to filter only the rows of our data frame that have one of the crimes in my.crimes as their Category. Also, select only the columns Category, DayOfWeek and PdDistrict. Save the resulting data frame and display its structure. (You should have a lot fewer rows than you did before.)\n\nSolution\nThe hard part about this is to get the inputs to %in% the right way around. We are testing the things in Category one at a time for membership in the set in my.crimes, so this:\n\nsfcrime %&gt;% filter(Category %in% my.crimes) %&gt;%\nselect(c(Category,DayOfWeek,PdDistrict)) -&gt; sfcrimea\nsfcrimea\n\n\n\n  \n\n\n\nI had trouble thinking of a good name for this one, so I put an “a” on the end. (I would have used a number, but I prefer those for models.)\nDown to a “mere” 359,000 rows. Don’t be stressed that the Category factor still has 39 levels (the original 39 crime categories); only four of them have any data in them:\n\nsfcrimea %&gt;% count(Category)\n\n\n\n  \n\n\n\nSo all of the crimes that are left are one of the four Categories we want to look at.\n\\(\\blacksquare\\)\n\nSave these data in a file sfcrime1.csv.\n\nSolution\nThis is write_csv again:\n\nwrite_csv(sfcrimea,\"sfcrime1.csv\")\n\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#what-sports-do-these-athletes-play-1",
    "href": "nominal-response.html#what-sports-do-these-athletes-play-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.12 What sports do these athletes play?",
    "text": "26.12 What sports do these athletes play?\nThe data at link are physical and physiological measurements of 202 male and female Australian elite athletes. The data values are separated by tabs. We are going to see whether we can predict the sport an athlete plays from their height and weight.\nThe sports, if you care, are respectively basketball, “field athletics” (eg. shot put, javelin throw, long jump etc.), gymnastics, netball, rowing, swimming, 400m running, tennis, sprinting (100m or 200m running), water polo.\n\nRead in the data and display the first few rows.\n\nSolution\nThe data values are separated by tabs, so read_tsv is the thing: ::: {.cell}\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ais.txt\"\nathletes &lt;- read_tsv(my_url)\n\nRows: 202 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nathletes\n\n\n\n  \n\n\n:::\nIf you didn’t remember that, this also works:\n\nathletes &lt;- read_delim(my_url, \"\\t\")\n\nRows: 202 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n(this is the R way of expressing “tab”.)\n\\(\\blacksquare\\)\n\nMake a scatterplot of height vs. weight, with the points coloured by what sport the athlete plays. Put height on the \\(x\\)-axis and weight on the \\(y\\)-axis.\n\nSolution\nI’m doing this to give you a little intuition for later:\n\nggplot(athletes, aes(x = Ht, y = Wt, colour = Sport)) + geom_point()\n\n\n\n\nThe reason for giving you the axes to use is (i) neither variable is really a response, so it doesn’t actually matter which one goes on which axis, and (ii) I wanted to give the grader something consistent to look at.\n\\(\\blacksquare\\)\n\nExplain briefly why a multinomial model (multinom from nnet) would be the best thing to use to predict sport played from the other variables.\n\nSolution\nThe categories of Sport are not in any kind of order, and there are more than two of them. That’s really all you needed, for which two marks was kind of generous.\n\\(\\blacksquare\\)\n\nFit a suitable model for predicting sport played from height and weight. (You don’t need to look at the results.) 100 steps isn’t quite enough, so set maxit equal to a larger number to allow the estimation to finish.\n\nSolution\n120 steps is actually enough, but any number larger than 110 is fine. It doesn’t matter if your guess is way too high. Like this:\n\nlibrary(nnet)\nsport.1 &lt;- multinom(Sport ~ Ht + Wt, data = athletes, maxit = 200)\n\n# weights:  40 (27 variable)\ninitial  value 465.122189 \niter  10 value 410.089598\niter  20 value 391.426721\niter  30 value 365.829150\niter  40 value 355.326457\niter  50 value 351.255395\niter  60 value 350.876479\niter  70 value 350.729699\niter  80 value 350.532323\niter  90 value 350.480130\niter 100 value 350.349271\niter 110 value 350.312029\nfinal  value 350.311949 \nconverged\n\n\nAs long as you see the word converged at the end, you’re good.\n\\(\\blacksquare\\)\n\nDemonstrate using anova that Wt should not be removed from this model.\n\nSolution\nThe idea is to fit a model without Wt, and then show that it fits significantly worse. This converges in less than 100 iterations, so you can have maxit or not as you prefer:\n\nsport.2 &lt;- update(sport.1, . ~ . - Wt)\n\n# weights:  30 (18 variable)\ninitial  value 465.122189 \niter  10 value 447.375728\niter  20 value 413.597441\niter  30 value 396.685596\niter  40 value 394.121380\niter  50 value 394.116993\niter  60 value 394.116434\nfinal  value 394.116429 \nconverged\n\nanova(sport.2, sport.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe P-value is very small indeed, so the bigger model sport.1 is definitely better (or the smaller model sport.2 is significantly worse, however you want to say it). So taking Wt out is definitely a mistake.\nThis is what I would have guessed (I actually wrote the question in anticipation of this being the answer) because weight certainly seems to help in distinguishing the sports. For example, the field athletes seem to be heavy for their height compared to the other athletes (look back at the graph you made).\ndrop1, the obvious thing, doesn’t work here:\n\ndrop1(sport.1, test = \"Chisq\", trace = T)\n\ntrying - Ht \n\n\nError in if (trace) {: argument is not interpretable as logical\n\n\nI gotta figure out what that error is. Does step?\n\nstep(sport.1, direction = \"backward\", test = \"Chisq\")\n\nStart:  AIC=754.62\nSport ~ Ht + Wt\n\ntrying - Ht \n# weights:  30 (18 variable)\ninitial  value 465.122189 \niter  10 value 441.367394\niter  20 value 381.021649\niter  30 value 380.326030\nfinal  value 380.305003 \nconverged\ntrying - Wt \n# weights:  30 (18 variable)\ninitial  value 465.122189 \niter  10 value 447.375728\niter  20 value 413.597441\niter  30 value 396.685596\niter  40 value 394.121380\niter  50 value 394.116993\niter  60 value 394.116434\nfinal  value 394.116429 \nconverged\n       Df      AIC\n&lt;none&gt; 27 754.6239\n- Ht   18 796.6100\n- Wt   18 824.2329\n\n\nCall:\nmultinom(formula = Sport ~ Ht + Wt, data = athletes, maxit = 200)\n\nCoefficients:\n        (Intercept)         Ht          Wt\nField      59.98535 -0.4671650  0.31466413\nGym       112.49889 -0.5027056 -0.57087657\nNetball    47.70209 -0.2947852  0.07992763\nRow        35.90829 -0.2517942  0.14164007\nSwim       36.82832 -0.2444077  0.10544986\nT400m      32.73554 -0.1482589 -0.07830622\nTennis     41.92855 -0.2278949 -0.01979877\nTSprnt     51.43723 -0.3359534  0.12378285\nWPolo      23.35291 -0.2089807  0.18819526\n\nResidual Deviance: 700.6239 \nAIC: 754.6239 \n\n\nCuriously enough, it does. The final model is the same as the initial one, telling us that neither variable should be removed.\n\\(\\blacksquare\\)\n\nMake a data frame consisting of all combinations of Ht 160, 180 and 200 (cm), and Wt 50, 75, and 100 (kg), and use it to obtain predicted probabilities of athletes of those heights and weights playing each of the sports. Display the results. You might have to display them smaller, or reduce the number of decimal places13 to fit them on the page.\n\nSolution\nTo get all combinations of those heights and weights, use datagrid:\n\nnew &lt;- datagrid(model = sport.1, Ht = c(160, 180, 200), Wt = c(50, 75, 100))\nnew\n\n\n\n  \n\n\n\n(check: \\(3 \\times 3 = 9\\) rows.)\nThen predict:\n\np &lt;- cbind(predictions(sport.1, newdata = new, type = \"probs\"))\np\n\n\n\n  \n\n\n\nI saved these, because I want to talk about what to do next. The normal procedure is to say that there is a prediction for each group (sport), so we want to grab only the columns we need and pivot wider. But, this time there are 10 sports (see how there are \\(9 \\times 10 = 90\\) rows, with a predicted probability for each height-weight combo for each sport). I suggested reducing the number of decimal places in the predictions; the time to do that is now, while they are all in one column (rather than trying to round a bunch of columns all at once, which is doable but more work). Hence:\n\np %&gt;% \n  mutate(estimate = round(estimate, 2)) %&gt;% \n  select(group, estimate, Ht, Wt) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\nThis works (although even then it might not all fit onto the page and you’ll have to scroll left and right to see everything).\nIf you forget to round until the end, you’ll have to do something like this:\n\np %&gt;% \n  select(group, estimate, Ht, Wt) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate) %&gt;% \n  mutate(across(BBall:WPolo, \\(x) round(x, 2)))\n\n\n\n  \n\n\n\nIn words, “for each column from BBall through WPolo, replace it by itself rounded to 2 decimals”.\nThere’s nothing magic about two decimals; three or maybe even four would be fine. A small enough number that you can see most or all of the columns at once, and easily compare the numbers for size (which is hard when some of them are in scientific notation).\nExtra: you can even abbreviate the sport names, like this:\n\np %&gt;% \n  mutate(estimate = round(estimate, 2),\n         group = abbreviate(group, 3)) %&gt;% \n  select(group, estimate, Ht, Wt) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate) \n\n\n\n  \n\n\n\nOnce again, the time to do this is while everything is long, so that you only have one column to work with.14 Again, you can do this at the end, but it’s more work, because you are now renaming columns:\n\np %&gt;% \n  mutate(estimate = round(estimate, 2)) %&gt;% \n  select(group, estimate, Ht, Wt) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate) %&gt;% \n  rename_with(\\(x) abbreviate(x, 3), everything())\n\n\n\n  \n\n\n\nrename doesn’t work with across (you will get an error if you try it), so you need to use rename_with15. This has syntax “how to rename” first (here, with an abbreviated version of itself), and “what to rename” second (here, everything, or BBall through WPolo if you prefer).\n\\(\\blacksquare\\)\n\nFor an athlete who is 180 cm tall and weighs 100 kg, what sport would you guess they play? How sure are you that you are right? Explain briefly.\n\nSolution\nFind this height and weight in your predictions (it’s row 6). Look along the line for the highest probability, which is 0.85 for Field (that is, field athletics). All the other probabilities are much smaller (the biggest of the others is 0.06). So this means we would guess the athlete to be a field athlete, and because the predicted probability is so big, we are very likely to be right. This kind of thought process is characteristic of discriminant analysis, which we’ll see more of later in the course. Compare that with the scatterplot you drew earlier: the field athletes do seem to be distinct from the rest in terms of height-weight combination. Some of the other height-weight combinations are almost equally obvious: for example, very tall people who are not very heavy are likely to play basketball. 400m runners are likely to be of moderate height but light weight. Some of the other sports, or height-weight combinations, are difficult to judge. Consider also that we have mixed up males and females in this data set. We might gain some clarity by including Sex in the model, and also in the predictions. But I wanted to keep this question relatively simple for you, and I wanted to stop from getting unreasonably long. (You can decide whether you think it’s already too long.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#footnotes",
    "href": "nominal-response.html#footnotes",
    "title": "26  Logistic regression with nominal response",
    "section": "",
    "text": "For this, use round.↩︎\nIf you do not take out the NA values, they are shown separately on the end of the summary for that column.↩︎\nThe American Green party is more libertarian than Green parties elsewhere.↩︎\nNorthern Ireland’s political parties distinguish themselves by politics and religion. Northern Ireland has always had political tensions between its Protestants and its Catholics.↩︎\nThere are three political parties; using the first as a baseline, there are therefore \\(3-1=2\\) coefficients for each variable.↩︎\nIt amuses me that Toronto’s current (2021) mayor, named Tory, is politically a Tory.↩︎\nWhen you have variables that are categories, you might have more than one individual with exactly the same categories; on the other hand, if they had measured Size as, say, length in centimetres, it would have been very unlikely to get two alligators of exactly the same size.↩︎\nDiscovered by me two minutes ago.↩︎\nI know I just said not to do this kind of thing, but counting is really fast, while unnecessary predictions are really slow.↩︎\nThere is only one district, so we can just put that in here.↩︎\nThe same thing applies if you are doing something like webscraping, that is to say downloading stuff from the web that you then want to do something with. Download it once and save it, then you can take as long as you need to decide what you’re doing with it.↩︎\nStatistical significance as an idea grew up in the days before “big data”.↩︎\nFor this, use round.↩︎\nabbreviate comes from base R; it returns an abbreviated version of its (text) input, with a minimum length as its second input. If two abbreviations come out the same, it makes them longer until they are different. It has some heuristics to get things short enough: remove vowels, then remove letters on the end.↩︎\nWhich I learned about today.↩︎"
  }
]