[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Problems and Solutions in Applied Statistics (2nd ed)",
    "section": "",
    "text": "Introduction\nThis book contains a collection of problems, and my solutions to them, in applied statistics with R. These come from my courses STAC32, STAC33, and STAD29 at the University of Toronto Scarborough.\nYou will occasionally see question parts beginning with a *; this means that other question parts refer back to this one. (One of my favourite question strategies is to ask how two different approaches lead to the same answer, or more generally to demonstrate that there are different ways to see the same thing.)\nThanks to Dann Sioson for spotting some errors and making some useful suggestions.\nIf you see anything, file an issue on the Github page for now. Likely problems include:\nAs I read through looking for problems like these, I realize that there ought to be a textbook that reflects my way of doing things. There isn’t one (yet), though there are lecture notes. Current versions of these are at:\nA little background:\nSTAC32 is an introduction to R as applied to statistical methods that have (mostly) been learned in previous courses. This course is designed for students who have a second non-mathematical applied statistics course such as this. The idea is that students have already seen a little of regression and analysis of variance (and the things that precede them), and need mainly an introduction of how to run them in R.\nSTAC33 is an introduction to R, and applied statistics in general, for students who have a background in mathematical statistics. The way our courses are structured, these students have a strong mathematical background, but not very much experience in applications, which this course is designed to provide. The material covered is similar to STAC32, with a planned addition of some ideas in bootstrap and practical Bayesian statistics. There are some questions on these here.\nSTAD29 is an overview of a number of advanced statistical methods. I start from regression and proceed to some regression-like methods (logistic regression, survival analysis, log-linear frequency table analysis), then I go a little further with analysis of variance and proceed with MANOVA and repeated measures. I finish with a look at classical multivariate methods such as discriminant analysis, cluster analysis, principal components and factor analysis. I cover a number of methods in no great depth; my aim is to convey an understanding of what these methods are for, how to run them and how to interpret the results. Statistics majors and specialists cannot take this course for credit (they have separate courses covering this material with the proper mathematical background). D29 is intended for students in other disciplines who find themselves wanting to learn more statistics; we have an Applied Statistics Minor program for which C32 and D29 are two of the last courses."
  },
  {
    "objectID": "index.html#packages-used-somewhere-in-this-book",
    "href": "index.html#packages-used-somewhere-in-this-book",
    "title": "Problems and Solutions in Applied Statistics (2nd ed)",
    "section": "Packages used somewhere in this book",
    "text": "Packages used somewhere in this book\nThe bottom lines are below used with the conflicted package: if a function by the name shown is in two or more packages, prefer the one from the package shown.\n\nlibrary(tidyverse)\nlibrary(smmr)\nlibrary(MASS)\nlibrary(nnet)\nlibrary(survival)\nlibrary(survminer)\nlibrary(car)\nlibrary(lme4)\nlibrary(ggbiplot)\nlibrary(ggrepel)\nlibrary(broom)\nlibrary(rpart)\nlibrary(bootstrap)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(tmaptools)\nlibrary(leaflet)\nlibrary(conflicted)\nconflict_prefer(\"summarize\", \"dplyr\")\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"count\", \"dplyr\")\nconflict_prefer(\"arrange\", \"dplyr\")\nconflict_prefer(\"rename\", \"dplyr\")\nconflict_prefer(\"id\", \"dplyr\")\n\nAll of these packages are on CRAN, and may be installed via the usual install.packages, with the exceptions of:\n\nsmmr on Github: install with\n\n\ndevtools::install_github(\"nxskok/smmr\")\n\n\nggbiplot on Github: install with\n\n\ndevtools::install_github(\"vqv/ggbiplot\")\n\n\ncmdstanr, posterior, and bayesplot: install with\n\n\ninstall.packages(\"cmdstanr\", \n                 repos = c(\"https://mc-stan.org/r-packages/\", \n                           getOption(\"repos\")))\ninstall.packages(\"posterior\", \n                 repos = c(\"https://mc-stan.org/r-packages/\", \n                            getOption(\"repos\")))\ninstall.packages(\"bayesplot\", \n                 repos = c(\"https://mc-stan.org/r-packages/\", \n                            getOption(\"repos\")))"
  },
  {
    "objectID": "getting_used.html#using-r-studio-online",
    "href": "getting_used.html#using-r-studio-online",
    "title": "1  Getting used to R and R Studio",
    "section": "1.1 Using R Studio online",
    "text": "1.1 Using R Studio online\n\nPoint your web browser at http://r.datatools.utoronto.ca. Click on the button to the left of “R Studio” (it will show blue), click the orange Log in to Start, and log in using your UTorID and password.\n\nSolution\nThis is about what you should see first, before you click the orange thing:\n\nYou will see a progress bar as things start up, and then you should see something like this:\n\nThis is R Studio, ready to go.\nIf you are already logged in to something else on the same browser that uses your UTorID and password, you may come straight here without needing to log in again.\n\\(\\blacksquare\\)\n\nTake a look around, and create a new Project. Give the new project any name you like.\n\nSolution\nSelect File and New Project to get this:\n\nClick on New Directory (highlighted blue on mine). This will create a new folder to put your new project in, which is usually what you want to do. The idea is that a project is a container for a larger collection of work, such as all your assignments in this course. That brings you to this:\n\nwhere you click on New Project (highlighted on mine), and:\n\nGive your project a name, as I did. Then click Create Project. At this point, R Studio will be restarted in your new project. You can tell which project you are in by looking top right, and you’ll see the name of your project next to the R symbol:\n\n\\(\\blacksquare\\)\n\nOne last piece of testing: find the Console window (which is probably on the left). Click next to the blue &gt;, and type library(tidyverse). Press Enter.\n\nSolution\nIt may think a bit, and then you’ll see something like this:\n\nAside: I used to use a cloud R Studio called rstudio.cloud. If you see or hear any references to that, it means the same thing as R Studio on r.datatools or jupyter. (You can still use rstudio.cloud if you want; it used to be completely free, but now the free tier won’t last you very long; the utoronto.calink is free as long as you are at U of T.) I’m trying to get rid of references to R Studio Cloud as I see them, but I am bound to miss some, and in the lecture videos they are rather hard to find.\nNow we can get down to some actual work.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "getting_used.html#using-r-studio-on-your-own-computer",
    "href": "getting_used.html#using-r-studio-on-your-own-computer",
    "title": "1  Getting used to R and R Studio",
    "section": "1.2 Using R Studio on your own computer",
    "text": "1.2 Using R Studio on your own computer\nThis is not required now, but you may wish to do this now or later so that you are not fighting for resources on the r.datatools server at busy times (eg. when an assignment is due).\nFollow the instructions here to install R Studio on your computer, then start R Studio (which itself starts R).\nOnce you have this working, you can use it for any of the following questions, in almost exactly the same way as the online R (I will explain any differences)."
  },
  {
    "objectID": "getting_used.html#getting-started",
    "href": "getting_used.html#getting-started",
    "title": "1  Getting used to R and R Studio",
    "section": "1.3 Getting started",
    "text": "1.3 Getting started\nThis question is to get you started using R.\n\nStart R Studio on r.datatools (or on your computer), in some project. (If you started up a new project in the previous question and are still logged in, use that; if not, create a new project with File, New Project, and New Directory. Then select New Project and give it a name. Click Create Project. This will give you an empty workspace to start from.)\n\nSolution\nYou ought to see something like this:\n\nThere should be one thing on the left half, and at the top right it’ll say “Environment is empty”.\nExtra: if you want to tweak things, select Tools (at the top of the screen) and from it Global Options, then click Appearance. You can make the text bigger or smaller via Editor Font Size, and choose a different colour scheme by picking one of the Editor Themes (which previews on the right). My favourite is Tomorrow Night Blue. Click Apply or OK when you have found something you like. (I spend a lot of time in R Studio, and I like having a dark background to be easier on my eyes.)\n\\(\\blacksquare\\)\n\nWe’re going to do some stuff in R here, just to get used to it. First, make a Quarto document by selecting File, New File and Quarto Document.\n\nSolution\nIn the first box that pops up, you’ll be invited to give your document a title. Make something up for now.\nThe first time, you might be invited to “install some packages” to make the document thing work.1 Let it do that by clicking Yes. After that, you’ll have this:\n\nA couple of technical notes:\n\nthis should be in the top left pane of your R Studio now, with the Console below it.\nAt the top of the file, between the two lines with three hyphens (minus signs, whatever), is some information about the document, known in the jargon as a YAML block, any of which you can change:\n\nthe title is whatever title you gave your document\nthe formatis what the output is going to be (in this case, HTML like a webpage, which is mostly what we’ll be using)\nthere is a visual editor that looks like Notion or a bit like a Google doc (the default), and also a Source editor which gives you more control, and shows that underlying the document is a thing called R Markdown (which is a code for writing documents).\n\nMy document is called “My awesome title”, but the file in which the document lives is still untitled because I haven’t saved it yet. See right at the top.\n\n\\(\\blacksquare\\)\n\nYou can delete the template code below the YAML block now (that is, everything from the title “Quarto” to the end). Somewhere in the space opened up below the YAML block (it might say “Heading 2”, greyed out), type a /. This, like Notion, gives you a list of things to choose from to insert there. Pressing Enter will insert a “code chunk”, sometimes known as a “code cell”. We are going to use this in a moment.\n\nSolution\nSomething like this:\n\nThe {r} at the top of the code chunk means that the code that will go in there will be R code (you can also have a Python code chunk, among others).\n\\(\\blacksquare\\)\n\nOn the line below the {r}, type these two lines of code into the chunk in the Quarto document:\n\nlibrary(tidyverse)\nmtcars\nSolution\nWhat this will do: get hold of a built-in data set with information about some different models of car, and display it.\n\nIn approximately five seconds, you’ll be demonstrating that for yourself.\n\\(\\blacksquare\\)\n\nRun this command. To do that, look at the top right of your code chunk block (shaded in a slightly different colour). You should see a down arrow and a green “play button”. Click the play button. This will run the code, and show the output below the code chunk.\n\nSolution\nHere’s what I get (yours should be the same):\n\nThis is a rectangular array of rows and columns, with individuals (here, cars) in rows and variables in columns, known as a “dataframe”. When you display a dataframe in an Quarto document, you see 10 rows and as many columns as will fit on the screen. At the bottom, it says how many rows and columns there are altogether (here 32 rows and 11 columns), and which ones are being displayed.\nYou can see more rows by clicking on Next, and if there are more columns, you’ll see a little arrow next to the rightmost column (as here next to am) that you can click on to see more columns. Try it and see. Or if you want to go to a particular collection of rows, click one of the numbers between Previous and Next: 1 is rows 1–10, 2 is rows 11–20, and so on.\nThe column on the left without a header (containing the names of the cars) is called “row names”. These have a funny kind of status, kind of a column and kind of not a column; usually, if we need to use the names, we have to put them in a column first.\nIn future solutions, rather than showing you a screenshot, expect me to show you something like this:\n\nlibrary(tidyverse)\nmtcars\n\n\n\n\n\n  \n\n\n\nThe top bit is the code, the bottom bit the output. In this kind of display, you only see the first ten rows (by default).2\nIf you don’t see the “play button”, make sure that what you have really is a code chunk. (I often accidentally delete one of the special characters above or below the code chunk). If you can’t figure it out, delete this code chunk and make a new one. Sometimes R Studio gets confused.\nOn the code chunk, the other symbols are the settings for this chunk (you have the choice to display or not display the code or the output or to not actually run the code). The second one, the down arrow, runs all the chunks prior to this one (but not this one).\nYour output has its own little buttons (as seen on the screenshot). The first one pops the output out into its own window; the second one shows or hides the output, and the third one deletes the output (so that you have to run the chunk again to get it back). Experiment. You can’t do much damage here.\n\\(\\blacksquare\\)\n\nSomething a little more interesting: summary obtains a summary of whatever you feed it (the five-number summary plus the mean for numerical variables). Obtain this for our data frame. To do this, create a new code chunk below the previous one, type summary(mtcars) into the code chunk, and run it.\n\nSolution\nThis is what you should see:\n\nor the other way:\n\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nFor the gas mileage column mpg, the mean is bigger than the median, and the largest value is unusually large compared with the others, suggesting a distribution that is skewed to the right.\nThere are 11 numeric (quantitative) variables, so we get the five-number summary plus mean for each one. Categorical variables, if we had any here, would be displayed a different way.\n\\(\\blacksquare\\)\n\nLet’s make a histogram of the gas mileage data. Type the code below into another new code chunk, and run it:\n\nggplot(mtcars, aes(x = mpg)) + geom_histogram(bins = 8)\nThe code looks a bit wordy, but we’ll see what all those pieces do later in the course (like, maybe tomorrow).\nSolution\nThis is what you should see:\n\nggplot(mtcars, aes(x = mpg)) + geom_histogram(bins = 8)\n\n\n\n\nThe long right tail supports our guess from before that the distribution is right-skewed.\n\\(\\blacksquare\\)\n\nSome aesthetics: Add some narrative text above and below your code chunks. Above the code chunk is where you say what you are going to do (and maybe why you are doing it), and below is where you say what you conclude from the output you just obtained. I find it looks better if you have a blank line above and below each code chunk.\n\nSolution\nThis is what I wrote (screenshot), with none of the code run yet. My library(tidyverse) line seems to have disappeared, but yours should still be there:\n\n\\(\\blacksquare\\)\n\nSave your Quarto document (the usual way with File and Save). This saves it on the jupyter servers (and not on your computer). This means that when you come back to it later, even from another device, this document will still be available to you. (If you are running R Studio on your own computer, it is much simpler: the Quarto document is on that computer, in the folder associated with the project you created.)\n\nNow click Render. This produces a pretty HTML version of your Quarto document. This will appear in a new tab of your web browser, which you might need to encourage to appear (if you have a pop-up blocker) by clicking a Try Again.\nSolution\nIf there are any errors in the rendering process, these will appear in the Render tab. The error message will tell you where in your document your error was. Find it and correct it.3 Otherwise, you should see your document.\nExtra 1: the rendering process as you did it doesn’t produce that nice display of a dataframe that I had in one of my screenshots. To get that, alter the YAML block to read:\nformat: \n  html:\n     df-print: paged\nThis way, anyone reading your document can actually page through the dataframes you display in the same way that you did, to check that they contain the right things.\nExtra 2: you might prefer to have a preview of your document within R Studio. To make this happen, look for the gear wheel to the right of Render. Click the arrow beside it, and in the drop-down, click on Preview in Viewer Pane. Render again, and you’ll see the rendered version of your document in a Viewer pane on the right. This puts the thing you’re writing and what it will look like side by side.\nExtra 3: you might be annoyed at having to remember to save things. If you are, you can enable auto-saving. To do this, go to Tools and select Global Options. Select Code (on the left) and Saving (at the top). Click on Automatically Save when editor loses focus, to put a check mark in the box on the left of it. Change the pull-down below that to Save and Write Changes. Click OK. Now, as soon as you pause for a couple of seconds, everything unsaved will be saved.\n\\(\\blacksquare\\)\n\nPractice handing in your rendered Quarto document, as if it were an assignment that was worth something. (It is good to get the practice in a low-stakes situation, so that you’ll know what to do next week.)\n\nSolution\nThere are two steps: download the HTML file onto your computer, and then handing it in on Quercus. To download: find the HTML file that you want to download in the Files pane on the right. You might need to click on Files at the top, especially if you had a Viewer open there before:\n\nI called my Quarto document awesomeand the file I was working on was called awesome.qmd(which stands for “Quarto Markdown”). That’s the file I had to render to produce the output. My output file itself is called awesome.html.That’s the file I want to hand in. If you called your file something different when you saved it, that’s the thing to look for: there should be something ending in .qmd and something with the same first part ending in .html.\nClick the checkbox to the left of the HTML file. Now click on More above the bottom-right pane. This pops up a menu from which you choose Export. This will pop up another window called Export Files, where you put the name that the file will have on your computer. (I usually leave the name the same.) Click Download. The file will go to your Downloads folder, or wherever things you download off the web go.\nNow, to hand it in. Open up Quercus at q.utoronto.ca, log in and navigate to this course. Click Assignments. Click (the title of) Assignment 0. There is a big blue Start Assignment button top right. Click it. You’ll get a File Upload at the bottom of the screen. Click Choose File and find the HTML file that you downloaded. Click Open (or equivalent on your system). The name of the file should appear next to Choose File. Click Submit Assignment. You’ll see Submitted at the top right, and below that is a Submission Details window and the file you uploaded.\n\nYou should be in the habit of always checking what you hand in, by downloading it again and looking at it to make sure it’s what you thought you had handed in.\nIf you want to try this again, you can try again as many times as you like, by making a New Attempt. (For the real thing, you can use this if you realize you made a mistake in something you submitted. The graders’ instructions, for the real thing, are to grade the last file submitted, so in that case you need to make sure that the last thing submitted before the due date includes everything that you want graded. Here, though, it doesn’t matter.)\n\\(\\blacksquare\\)\n\nOptional extra. Something more ambitious: make a scatterplot of gas mileage mpg, on the \\(y\\) axis, against horsepower, hp, on the \\(x\\)-axis.\n\nSolution\nThat goes like this. I’ll explain the steps below.\n\nlibrary(tidyverse)\nggplot(mtcars, aes(x=hp, y=mpg)) + geom_point()\n\n\n\n\nThis shows a somewhat downward trend, which is what you’d expect, since a larger hp value means a more powerful engine, which will probably consume more gas and get fewer miles per gallon. As for the code: to make a ggplot plot, as we will shortly see in class, you first need a ggplot statement that says what to plot. The first thing in a ggplot is a data frame (mtcars here), and then the aes says that the plot will have hp on the \\(x\\)-axis and mpg on the \\(y\\)-axis, taken from the data frame that you specified. That’s all of the what-to-plot. The last thing is how to plot it; geom_point() says to plot the data values as points.\nYou might like to add a regression line to the plot. That is a matter of adding this to the end of the plotting command:\n\nggplot(mtcars, aes(x=hp, y=mpg)) + geom_point() + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe line definitely goes downhill. Decide for yourself how well you think a line fits these data.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "getting_used.html#reading-data-from-a-file",
    "href": "getting_used.html#reading-data-from-a-file",
    "title": "1  Getting used to R and R Studio",
    "section": "1.4 Reading data from a file",
    "text": "1.4 Reading data from a file\nIn this question, we read a file from the web and do some descriptive statistics and a graph. This is very like what you will be doing on future assignments, so it’s good to practice it now.\nTake a look at the data file at http://ritsokiguess.site/datafiles/jumping.txt. These are measurements on 30 rats that were randomly made to do different amounts of jumping by group (we’ll see the details later in the course). The control group did no jumping, and the other groups did “low jumping” and “high jumping”. The first column says which jumping group each rat was in, and the second is the rat’s bone density (the experimenters’ supposition was that more jumping should go with higher bone density).\n\nWhat are the two columns of data separated by? (The fancy word is “delimited”).\n\nSolution\nExactly one space. This is true all the way down, as you can check.\n\\(\\blacksquare\\)\n\nMake a new Quarto document. Leave the YAML block, but get rid of the rest of the template document. Start with a code chunk containing library(tidyverse). Run it.\n\nSolution\nYou will get either the same message as before or nothing. (I got nothing because I had already loaded the tidyverse in this session.)\n\\(\\blacksquare\\)\n\nPut the URL of the data file in a variable called my_url. Then use read_delim to read in the file. (See solutions for how.) read_delim reads data files where the data values are always separated by the same single character, here a space. Save the data frame in a variable rats.\n\nSolution\nLike this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/jumping.txt\"\nrats &lt;- read_delim(my_url,\" \")\n\nRows: 30 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): group\ndbl (1): density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe second thing in read_delim is the thing that separates the data values. Often when you use read_delim it’ll be a space.\nHint: to get the file name into my_url, the best way is to right-click on the link, and select Copy Link Address (or equivalent in your browser). That puts in on your clipboard. Then make a code chunk and put this in it (you’ll probably only need to type one quote symbol, because R Studio will supply the other one):\n\nmy_url &lt;- \"\"\n\nthen put the cursor between the two quote symbols and paste. This is better than selecting the URL in my text and then copy-pasting that because odd things happen if it happens to span two lines on your screen. (URLs tend to be rather long, so this is not impossible.)\n\\(\\blacksquare\\)\n\nTake a look at your data frame, by making a new code chunk and putting the data frame’s name in it (as we did with mtcars).\n\nSolution\n\nrats\n\n\n\n  \n\n\n\nThere are 30 rows and two columns, as there should be.\n\\(\\blacksquare\\)\n\nFind the mean bone density for rats that did each amount of jumping.\n\nSolution\nThis is something you’ll see a lot: group_by followed by summarize. Reminder: to get that funny thing with the percent signs (called the “pipe symbol”), type control-shift-M (or equivalent on a Mac):\n\nrats %&gt;% group_by(group) %&gt;%\nsummarize(m = mean(density))\n\n\n\n  \n\n\n\nThe mean bone density is clearly highest for the high jumping group, and not much different between the low-jumping and control groups.\n\\(\\blacksquare\\)\n\nMake a boxplot of bone density for each jumping group.\n\nSolution\nOn a boxplot, the groups go across and the values go up and down, so the right syntax is this:\n\nggplot(rats, aes(x=group, y=density)) + geom_boxplot()\n\n\n\n\nGiven the amount of variability, the control and low-jump groups are very similar (with the control group having a couple of outliers), but the high-jump group seems to have a consistently higher bone density than the others.\nThis is more or less in line with what the experimenters were guessing, but it seems that it has to be high jumping to make a difference.\nYou might recognize that this is the kind of data where we would use analysis of variance, which we will do later on in the course: we are comparing several (here three) groups.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "getting_used.html#reading-files-different-ways",
    "href": "getting_used.html#reading-files-different-ways",
    "title": "1  Getting used to R and R Studio",
    "section": "1.5 Reading files different ways",
    "text": "1.5 Reading files different ways\nThis question is about different ways of reading data files. If you’re working online (using r.datatools or similar), start at the beginning. If you’re using R Studio running on your own computer, start at part (here).\n\nLog in to r.datatools.utoronto.ca. Open up a project (or start a new one), and watch the spinning circles for a few minutes. When that’s done, create a new Quarto Document with File, New File, Quarto Document. Delete the “template” document, but not the top lines with title: and output: in them. Add a code chunk that contains library(tidyverse) and run it.\n\nSolution\nSo far you (with luck) have something that looks like this:\n\nIf you have an error rather than that output, you probably need to install the tidyverse first. Make another code chunk, containing\n\ninstall.packages(\"tidyverse\")\n\nand run it. Wait for it to finish. It may take a while. If it completes successfully (you might see the word DONE at the end), delete that code chunk (you don’t need it any more) and try again with the library(tidyverse) chunk. It should run properly this time.\n\\(\\blacksquare\\)\n\n* The easiest kind of files to read in are ones on the Internet, with a URL address that begins with http or https. I have a small file at link. Click the link to see it, and keep the tab open for the next part of this question. This is a text file with three things on each line, each separated by exactly one space. Read the data file into a data frame, and display your data frame.\n\nSolution\nData values separated by exactly one space is the kind of thing that read_delim reads, so make another code chunk and fill it with this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/testing.txt\"\ntesting &lt;- read_delim(my_url, \" \")\n\nRows: 6 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): g\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntesting\n\n\n\n  \n\n\n\nWhen you run that, you’ll see something like my output. The first part is read_delim telling you what it saw in the file: two columns of (whole) numbers and one column of text. The top line of the file is assumed to contain names, which are used as the names of the columns of your data frame. The bottom part of the output, obtained by putting the name of the data frame on a line by itself in your code chunk, is what the data frame actually looks like. You ought to get into the habit of eyeballing it and checking that it looks like the values in the data file.\nThe things on the left side of the &lt;- symbol (that is meant to look like an arrow pointing left) are variables that you are creating in R. You get to choose the names for them. My habit is to use my_url for URLs of files that I am going to read in, and (usually) to give my data frames names that say something about what they contain, but this is your choice to make.\n\\(\\blacksquare\\)\n\nYou might have data in a file on your own computer. To read data from such a file, you first have to upload it to r.datatools, and then read it from there. To practice this: open a text editor (like Notepad or TextEdit). Go back to the web browser tab containing the data you used in the previous part. Copy the data from there and paste it into the text editor. Save it somewhere on your computer (like the Desktop). Upload that file, read in the data and verify that you get the right thing. (For this last part, see the Solution.)\n\nSolution\nI copied and pasted the data, and saved it in a file called testing.txt on my computer. I’m assuming that you’ve given it a similar name. Then go back to r.datatools. You should have a Files pane bottom right. If you don’t see a pane bottom right at all, press Control-Shift-0 to show all the panes. If you see something bottom right but it’s not Files (for example a plot), click the Files tab, and you should see a list of things that look like files, like this:\n\nClick the Upload button (next to New Folder), click Choose File. Use the file finder to track down the file you saved on your computer, then click OK. The file should be uploaded to the same folder on r.datatools that your project is, and appear in the Files pane bottom right.\nTo read it in, you supply the file name to read_delim thus:\n\ntesting2 &lt;- read_delim(\"testing.txt\", \" \")\n\nRows: 6 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): g\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nand then you look at it in the same way as before:\n\ntesting2\n\n\n\n  \n\n\n\nCheck.\n\\(\\blacksquare\\)\n\nYou might have a spreadsheet on your computer. Create a .csv file from it, and use the ideas of the last part to read it into R Studio.\n\nSolution\nOpen the spreadsheet containing the data you want to read into R. If there are several sheets in the workbook, make sure you’re looking at the right one. Select File, Save As, select “CSV” or “comma-separated values” and give it a name. Save the resulting file somewhere.\nThen follow the same steps as the previous part to upload it to your project on R Studio Cloud. (If you look at the actual file, it will be plain text with the data values having commas between them, as the name suggests. You can open the file in R Studio by clicking on it in the Files pane; it should open top left.)\nThe final step is to read it into an R data frame. This uses read_csv; there are several read_ functions that read in different types of file, and you need to use an appropriate one.\nMy spreadsheet got saved as cars.csv, so:\n\ncars &lt;- read_csv(\"cars.csv\")\n\nRows: 38 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Car, Country\ndbl (4): MPG, Weight, Cylinders, Horsepower\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncars\n\n\n\n  \n\n\n\nSome information about different types of cars.\nYou are now done with this question.\n\\(\\blacksquare\\)\n\n* Start here if you downloaded R and R Studio and they are running on your own computer. Open a web browser and point it at link. Click the link to see it, and keep the tab open for the next part of this question. This is a text file with three things on each line, each separated by exactly one space. Read the data file into a data frame, and display your data frame.\n\nSolution\nData values separated by exactly one space is the kind of thing that read_delim reads, so make another code chunk and fill it with this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/testing.txt\"\ntesting &lt;- read_delim(my_url, \" \")\n\nRows: 6 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): g\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntesting\n\n\n\n  \n\n\n\nWhen you run that, you’ll see something like my output. The first part is read_delim telling you what it saw in the file: two columns of (whole) numbers and one column of text. The top line of the file is assumed to contain names, which are used as the names of the columns of your data frame. The bottom part of the output, obtained by putting the name of the data frame on a line by itself in your code chunk, is what the data frame actually looks like. You ought to get into the habit of eyeballing it and checking that it looks like the values in the data file.\nThe things on the left side of the &lt;- symbol (that is meant to look like an arrow pointing left) are variables that you are creating in R. You get to choose the names for them. My habit is to use my_url for URLs of files that I am going to read in, and (usually) to give my data frames names that say something about what they contain, but this is your choice to make.\n(This part is exactly the same whether you are running R Studio on r.datatools or have R Studio running on your computer. A remote file is obtained in exactly the same way regardless.)\n\\(\\blacksquare\\)\n\nYou might have data in a file on your own computer. To read data from such a file, R has to know where to find it. Each R project lives in a folder, and one way of specifying where a data file is is to give its complete path relative to the folder that R Studio is running its current project in. This is rather complicated, so we will try a simpler way. To set this up, open a text editor (like Notepad or TextEdit). Go back to the web browser tab containing the data you used in the previous part. Copy the data from there and paste it into the text editor. Save it somewhere on your computer (like the Desktop). Follow the steps in the solution below to read the data into R.\n\nSolution\nI copied and pasted the data, and saved it in a file called testing.txt on my computer. I’m assuming that you’ve given it a similar name. Go back to R Studio. Create a new code chunk containing this:\n\nf &lt;- file.choose()\n\nRun this code chunk. You’ll see a file chooser. Find the file you saved on your computer, and click Open (or OK or whatever you see). This saves what R needs to access the file in the variable f. If you want to, you can look at it:\n\nf\n\nand you’ll see what looks like a file path in the appropriate format for your system (Windows, Mac, Linux). To read the data in, you supply the file path to read_delim thus:\n\ntesting2 &lt;- read_delim(f, \" \")\n\nand then you look at it in the same way as before:\n\ntesting2\n\n\n\n  \n\n\n\nCheck.\n\\(\\blacksquare\\)\n\nYou might have a spreadsheet on your computer. Create a .csv file from it, and use the ideas of the last part to read it into R Studio.\n\nSolution\nOpen the spreadsheet containing the data you want to read into R. If there are several sheets in the workbook, make sure you’re looking at the right one. Select File, Save As, select “CSV” or “comma-separated values” and give it a name. Save the resulting file somewhere.\nThen read it into an R data frame. This uses read_csv; there are several read_ functions that read in different types of file, and you need to use an appropriate one. Before that, though, again run\n\nf &lt;- file.choose()\n\nto find the .csv file on your computer, and then\n\ncars &lt;- read_csv(f)\n\nto read it in. My spreadsheet was\n\ncars\n\n\n\n  \n\n\n\nSome information about different types of cars.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "getting_used.html#footnotes",
    "href": "getting_used.html#footnotes",
    "title": "1  Getting used to R and R Studio",
    "section": "",
    "text": "Especially if you are on your own computer.↩︎\nThis document was actually produced by literally running this code, a process known as “rendering”, which we will learn about shortly.↩︎\nA big part of coding is dealing with errors. You will forget things, and it is fine. In the same way that it doesn’t matter how many times you get knocked down, it’s key that you get up again each time: it doesn’t matter how many errors you made, it’s key that you fix them. If you want something to sing along with while you do this, I recommend this.↩︎"
  },
  {
    "objectID": "reading-in.html#orange-juice",
    "href": "reading-in.html#orange-juice",
    "title": "2  Reading in data",
    "section": "2.1 Orange juice",
    "text": "2.1 Orange juice\nThe quality of orange juice produced by a manufacturer (identity unknown) is constantly being monitored. The manufacturer has developed a “sweetness index” for its orange juice, for which a higher value means sweeter juice. Is the sweetness index related to a chemical measure such as the amount of water-soluble pectin (parts per million) in the orange juice? Data were obtained from 24 production runs, and the sweetness and pectin content were measured for each run. The data are in link. Open that link up now. You can click on that link just above to open the file.\n\nThe data values are separated by a space. Use the appropriate Tidyverse function to read the data directly from the course website into a “tibble”.\nTake a look at what got read in. Do you have data for all 24 runs?\nIn your data frame, where did the column (variable) names come from? How did R know where to get them from?"
  },
  {
    "objectID": "reading-in.html#making-soap",
    "href": "reading-in.html#making-soap",
    "title": "2  Reading in data",
    "section": "2.2 Making soap",
    "text": "2.2 Making soap\nA company operates two production lines in a factory for making soap bars. The production lines are labelled A and B. A production line that moves faster may produce more soap, but may possibly also produce more “scrap” (that is, bits of soap that can no longer be made into soap bars and will have to be thrown away).\nThe data are in link.\n\nRead the data into R. Display the data.\nThere should be 27 rows. Are there? What columns are there?"
  },
  {
    "objectID": "reading-in.html#handling-shipments",
    "href": "reading-in.html#handling-shipments",
    "title": "2  Reading in data",
    "section": "2.3 Handling shipments",
    "text": "2.3 Handling shipments\nA company called Global Electronics from time to time imports shipments of a certain large part used as a component in several of its products. The size of the shipment varies each time. Each shipment is sent to one of two warehouses (labelled A and B) for handling. The data in link show the size of each shipment (in thousands of parts) and the direct cost of handling it, in thousands of dollars. Also shown is the warehouse (A or B) that handled each shipment.\n\nRead the data into R and display your data frame.\nDescribe how many rows and columns your data frame has, and what they contain.\n\nMy solutions follow:"
  },
  {
    "objectID": "reading-in.html#orange-juice-1",
    "href": "reading-in.html#orange-juice-1",
    "title": "2  Reading in data",
    "section": "2.4 Orange juice",
    "text": "2.4 Orange juice\nThe quality of orange juice produced by a manufacturer (identity unknown) is constantly being monitored. The manufacturer has developed a “sweetness index” for its orange juice, for which a higher value means sweeter juice. Is the sweetness index related to a chemical measure such as the amount of water-soluble pectin (parts per million) in the orange juice? Data were obtained from 24 production runs, and the sweetness and pectin content were measured for each run. The data are in link. Open that link up now. You can click on that link just above to open the file.\n\nThe data values are separated by a space. Use the appropriate Tidyverse function to read the data directly from the course website into a “tibble”.\n\nSolution\nStart with this (almost always):\n\nlibrary(tidyverse)\n\nThe appropriate function, the data values being separated by a space, will be read_delim. Put the URL as the first thing in read_delim, or (better) define it into a variable first:1\n\nurl &lt;- \"http://ritsokiguess.site/datafiles/ojuice.txt\"\njuice &lt;- read_delim(url, \" \")\n\nRows: 24 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): run, sweetness, pectin\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nread_delim (or read_csv or any of the others) tell you what variables were read in, and also tell you about any “parsing errors” where it couldn’t work out what was what. Here, we have three variables, which is entirely consistent with the three columns of data values in the file.\nread_delim can handle data values separated by any character, not just spaces, but the separating character, known as a “delimiter”, does not have a default, so you have to say what it is, every time.\n\\(\\blacksquare\\)\n\nTake a look at what got read in. Do you have data for all 24 runs?\n\nSolution\nType the name of the data frame in a code chunk (a new one, or add it to the end of the previous one). Because this is actually a “tibble”, which is what read_delim reads in, you’ll only actually see the first 10 lines, but it will tell you how many lines there are altogether, and you can click on the appropriate thing to see the rest of it.\n\njuice\n\n\n\n  \n\n\n\nI appear to have all the data. If you want further convincing, click Next a couple of times to be sure that the runs go down to number 24.\n\\(\\blacksquare\\)\n\nIn your data frame, where did the column (variable) names come from? How did R know where to get them from?\n\nSolution\nThey came from the top line of the data file, so we didn’t have to specify them. This is the default behaviour of all the read_ functions, so we don’t have to ask for it specially.\nExtra: in fact, if the top line of your data file is not variable names, that’s when you have to say something special. The read_ functions have an option col_names which can either be TRUE (the default), which means “read them in from the top line”, FALSE (“they are not there, so make some up”) or a list of column names to use. You might use the last alternative when the column names that are in the file are not the ones you want to use; in that case, you would also say skip=1 to skip the first line. For example, with file a.txt thus:\na b\n1 2\n3 4\n5 6\nyou could read the same data but call the columns x and y thus:\n\nread_delim(\"a.txt\", \" \", col_names = c(\"x\", \"y\"), skip = 1)\n\nRows: 3 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "reading-in.html#making-soap-1",
    "href": "reading-in.html#making-soap-1",
    "title": "2  Reading in data",
    "section": "2.5 Making soap",
    "text": "2.5 Making soap\nA company operates two production lines in a factory for making soap bars. The production lines are labelled A and B. A production line that moves faster may produce more soap, but may possibly also produce more “scrap” (that is, bits of soap that can no longer be made into soap bars and will have to be thrown away).\nThe data are in link.\n\nRead the data into R. Display the data.\n\nSolution\nRead directly from the URL, most easily:\n\nurl &lt;- \"http://ritsokiguess.site/datafiles/soap.txt\"\nsoap &lt;- read_delim(url, \" \")\n\nRows: 27 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): line\ndbl (3): case, scrap, speed\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsoap\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nThere should be 27 rows. Are there? What columns are there?\n\nSolution\nThere are indeed 27 rows, one per observation. The column called case identifies each particular run of a production line (scroll down to see that it gets to 27 as well). Though it is a number, it is an identifier variable and so should not be treated quantitatively. The other columns (variables) are scrap and speed (quantitative) and line (categorical). These indicate which production line was used for each run, the speed it was run at, and the amount of scrap produced.\nThis seems like an odd place to end this question, but later we’ll be using these data to draw some graphs.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "reading-in.html#handling-shipments-1",
    "href": "reading-in.html#handling-shipments-1",
    "title": "2  Reading in data",
    "section": "2.6 Handling shipments",
    "text": "2.6 Handling shipments\nA company called Global Electronics from time to time imports shipments of a certain large part used as a component in several of its products. The size of the shipment varies each time. Each shipment is sent to one of two warehouses (labelled A and B) for handling. The data in link show the size of each shipment (in thousands of parts) and the direct cost of handling it, in thousands of dollars. Also shown is the warehouse (A or B) that handled each shipment.\n\nRead the data into R and display your data frame.\n\nSolution\nIf you open the data file in your web browser, it will probably open as a spreadsheet, which is not really very helpful, since then it is not clear what to do with it. You could, I suppose, save it and upload it to R Studio Cloud, but it requires much less brainpower to open it directly from the URL:\n\nurl &lt;- \"http://ritsokiguess.site/datafiles/global.csv\"\nshipments &lt;- read_csv(url)\n\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): warehouse\ndbl (2): size, cost\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf you display your data frame and it looks like this, you are good (you can give the data frame any name):\n\nshipments\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nDescribe how many rows and columns your data frame has, and what they contain.\n\nSolution\nIt has 10 rows and 3 columns. You need to say this.\nThat is, there were 10 shipments recorded, and for each of them, 3 variables were noted: the size and cost of the shipment, and the warehouse it was handled at.\nWe will also be making some graphs of these data later.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "reading-in.html#footnotes",
    "href": "reading-in.html#footnotes",
    "title": "2  Reading in data",
    "section": "",
    "text": "I say “better” because otherwise the read line gets rather long. This way you read it as “the URL is some long thing that I don’t care about especially, and I what I need to do is to read the data from that URL, separated by spaces.”↩︎"
  },
  {
    "objectID": "data-summaries.html#north-carolina-births",
    "href": "data-summaries.html#north-carolina-births",
    "title": "3  Data exploration",
    "section": "3.1 North Carolina births",
    "text": "3.1 North Carolina births\nThe data in file link are about 500 randomly chosen births of babies in North Carolina. There is a lot of information: not just the weight at birth of the baby, but whether the baby was born prematurely, the ages of the parents, whether the parents are married, how long (in weeks) the pregnancy lasted (this is called the “gestation”) and so on.\n\nRead in the data from the file into R, bearing in mind what type of file it is.\nFrom your output, verify that you have the right number of observations and that you have several variables. Which of your variables correspond to birthweight, prematureness and length of pregnancy? (You might have to make guesses based on the names of the variables.)\nThe theory behind the \\(t\\)-test (which we do later) says that the distribution of birth weights should be (approximately) normally distributed. Obtain a histogram of the birth weights. Does it look approximately normal? Comment briefly. (You’ll have to pick a number of bins for your histogram first. I don’t mind very much what you pick, as long as it’s not obviously too many or too few bins.)"
  },
  {
    "objectID": "data-summaries.html#more-about-the-nc-births",
    "href": "data-summaries.html#more-about-the-nc-births",
    "title": "3  Data exploration",
    "section": "3.2 More about the NC births",
    "text": "3.2 More about the NC births\nThis is an exploration of some extra issues around the North Carolina births data set.\n\nHow short does a pregnancy have to be, for the birth to be classified as “premature”? Deduce this from the data, by drawing a suitable graph or otherwise.\nExplore the relationship between birth weight and length of pregancy (“gestation”) using a suitable graph. What do you see?\nDo a web search to find the standard (North American) definition of a premature birth. Does that correspond to what you saw in the data? Cite the website you used, for example by saying “according to URL, …”, with URL replaced by the address of the website you found."
  },
  {
    "objectID": "data-summaries.html#nenana-alaska",
    "href": "data-summaries.html#nenana-alaska",
    "title": "3  Data exploration",
    "section": "3.3 Nenana, Alaska",
    "text": "3.3 Nenana, Alaska\nNenana, Alaska, is about 50 miles west of Fairbanks. Every spring, there is a contest in Nenana. A wooden tripod is placed on the frozen river, and people try to guess the exact minute when the ice melts enough for the tripod to fall through the ice. The contest started in 1917 as an amusement for railway workers, and has taken place every year since. Now, hundreds of thousands of people enter their guesses on the Internet and the prize for the winner can be as much as $300,000.\nBecause so much money is at stake, and because the exact same tripod is placed at the exact same spot on the ice every year, the data are consistent and accurate. The data are in link.\n\nRead the data into R. Note that the values are separated by tabs rather than spaces, so you’ll need an appropriate read_ to read it in.\nFind a way of displaying how many rows and columns your data frame has, and some of the values. Describe the first and last of the variables that you appear to have.\nDates and times are awkward to handle with software. (We see more ways later in the course.) The column JulianDate expresses the time that the tripod fell through the ice as a fractional number of days since December 31. This enables the time (as a fraction of the way through the day) to be recorded as well, the whole thing being an ordinary number. Make a histogram of the Julian dates. Comment briefly on its shape.\nPlot JulianDate against Year on a scatterplot. What recent trends, if any, do you see? Comment briefly."
  },
  {
    "objectID": "data-summaries.html#computerized-accounting",
    "href": "data-summaries.html#computerized-accounting",
    "title": "3  Data exploration",
    "section": "3.4 Computerized accounting",
    "text": "3.4 Computerized accounting\nBeginning accounting students need to learn to learn to audit in a computerized environment. A sample of beginning accounting students took each of two tests: the Computer Attitude Scale (CAS) and the Computer Anxiety Rating Scale (CARS). A higher score in each indicates greater anxiety around computers. The test scores are scaled to be between 0 and 5. Also noted was each student’s gender. The data are in http://ritsokiguess.site/datafiles/compatt.txt. The data values are separated by spaces.\n\nRead the data into R. Do you have what you expected? Explain briefly.\nHow many males and females were there in the sample?\nDo the CAS scores tend to be higher for females or for males? Draw a suitable graph to help you decide, and come to a conclusion.\nFind the median CAS scores for each gender. Does this support what you saw on your plot? Explain briefly.\nFind the mean and standard deviation of both CAS and CARS scores (for all the students combined, ie. not separated by gender) without naming those columns explicitly."
  },
  {
    "objectID": "data-summaries.html#test-scores-in-two-classes",
    "href": "data-summaries.html#test-scores-in-two-classes",
    "title": "3  Data exploration",
    "section": "3.5 Test scores in two classes",
    "text": "3.5 Test scores in two classes\nOpen R Studio. Create a new Text File by selecting File, New File and Text File. You should see a new empty, untitled window appear at the top left. In that window, type or copy the data below (which are scores on a test for students in two different classes):\n\nclass score\nken 78\nken 62\nken 59\nken 69\nken 81\nthomas 83\nthomas 77\nthomas 63\nthomas 61\nthomas 79\nthomas 72\n\nSave the file, using a filename of your choice (with, perhaps, extension .txt). Or, if you prefer, use the one at link.\n\nRead the data into a data frame called marks, using read_delim, and list the data frame (by typing its name) to confirm that you read the data values properly. Note that the top line of the data file contains the names of the variables, as it ought to.\n* Obtain side-by-side boxplots of the scores for each class.\nDo the two classes appear to have similar or different scores, on average? Explain briefly.\nObtain a boxplot of all the scores together, regardless of which class they came from.\nCompute the median score (of all the scores together). Does this seem about right, looking at the boxplot? Explain briefly."
  },
  {
    "objectID": "data-summaries.html#unprecedented-rainfall",
    "href": "data-summaries.html#unprecedented-rainfall",
    "title": "3  Data exploration",
    "section": "3.6 Unprecedented rainfall",
    "text": "3.6 Unprecedented rainfall\nIn 1997, a company in Davis, California, had problems with odour in its wastewater facility. According to a company official, the problems were caused by “unprecedented weather conditions” and “because rainfall was at 170 to 180 percent of its normal level, the water in the holding ponds took longer to exit for irrigation, giving it more time to develop an odour.”\nAnnual rainfall data for the Davis area is here. Note that clicking on the link will display the file, and right-clicking on the link will give you some options, one of which is Copy Link Address, which you can then paste into your R Notebook.\nThe rainfall is measured in inches.\n\nRead in and display (some of) the data.\nSummarize the data frame.\nMake a suitable plot of the rainfall values. (We are not, for the time being, concerned about the years.)\nHow would you describe the shape of the distribution of rainfall values?\nIn the quote at the beginning of the question, where do you think the assertion that the 1997 rainfall was “at 170 to 180 percent of its normal level” came from? Explain briefly.\nDo you think the official’s calculation was reasonable? Explain briefly. (Note that this is not the same as asking whether the official’s calculation was correct. This is an important distinction for you to make.)\nDo you think that the official was right to use the word “unprecedented” to describe the 1997 rainfall? Justify your answer briefly."
  },
  {
    "objectID": "data-summaries.html#learning-algebra",
    "href": "data-summaries.html#learning-algebra",
    "title": "3  Data exploration",
    "section": "3.7 Learning algebra",
    "text": "3.7 Learning algebra\nAt a high school in New Jersey, teachers were interested in what might help students to learn algebra. One idea was laptops as a learning aid, to see whether having access to one helped with algebra scores. (This was some time ago.) The 20 students in one class were given laptops to use in school and at home, while the 27 students in another class were not given laptops. For all of these students, the final exam score in algebra was recorded. The data are in http://ritsokiguess.site/datafiles/algebra.txt, with two columns, one indicating whether the student received a laptop or not, and the other giving their score on the algebra final exam.\n\nRead in and display (some of) the data. Do you have (i) the correct number of observations, and (ii) the correct type of columns? Explain briefly.\nMake a suitable graph of these data.\nComment briefly on your graph, thinking about what the teachers would like to know.\nWork out the median and inter-quartile range for the students who did and who did not have laptops, and compare with the boxplot. (In R, the inter-quartile range is IQR in uppercase.)\n\nMy solutions follow:"
  },
  {
    "objectID": "data-summaries.html#north-carolina-births-1",
    "href": "data-summaries.html#north-carolina-births-1",
    "title": "3  Data exploration",
    "section": "3.8 North Carolina births",
    "text": "3.8 North Carolina births\nThe data in file link are about 500 randomly chosen births of babies in North Carolina. There is a lot of information: not just the weight at birth of the baby, but whether the baby was born prematurely, the ages of the parents, whether the parents are married, how long (in weeks) the pregnancy lasted (this is called the “gestation”) and so on.\n\nRead in the data from the file into R, bearing in mind what type of file it is.\n\nSolution\nThis is a .csv file (it came from a spreadsheet), so it needs reading in accordingly. Work directly from the URL:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/ncbirths2.csv\"\nbw &lt;- read_csv(myurl)\n\nRows: 500 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): father_age, mother_age, weeks_gestation, pre_natal_visits, marital...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis shows you which variables the data set has (some of the names got a bit mangled), and it shows you that they are all integers except for the birth weight (a decimal number).\nThe easiest way to find out how many rows and columns there are is simply to list the data frame:\n\nbw\n\n\n\n  \n\n\n\nor you can take a “glimpse” of it, which is good if you have a lot of columns:\n\nglimpse(bw)\n\nRows: 500\nColumns: 10\n$ father_age           &lt;dbl&gt; 27, 35, 34, NA, 35, 32, 33, 38, 28, NA, 28, 34, N…\n$ mother_age           &lt;dbl&gt; 26, 33, 22, 16, 33, 24, 33, 35, 29, 19, 26, 31, 1…\n$ weeks_gestation      &lt;dbl&gt; 38, 40, 37, 38, 39, 36, 38, 38, 40, 34, 39, 39, 3…\n$ pre_natal_visits     &lt;dbl&gt; 14, 11, 10, 9, 12, 12, 15, 16, 5, 10, 15, 15, 0, …\n$ marital_status       &lt;dbl&gt; 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2…\n$ mother_weight_gained &lt;dbl&gt; 32, 23, 50, NA, 15, 12, 60, 2, 20, NA, 45, 22, 20…\n$ low_birthweight      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ weight_pounds        &lt;dbl&gt; 6.8750, 6.8125, 7.2500, 8.8125, 8.8125, 5.8125, 6…\n$ premie               &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0…\n$ few_visits           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0…\n\n\nEither of these displays show that there are 500 rows (observations, here births) and 10 columns (variables), and they both show what the variables are called. So they’re both good as an answer to the question.\nExtra: As is rather too often the way, the original data weren’t like this, and I had to do some tidying first. Here’s the original:\n\nmy_old_url &lt;- \"http://ritsokiguess.site/datafiles/ncbirths_original.csv\"\nbw0 &lt;- read_csv(my_old_url)\n\nRows: 500 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): Father Age, Mother Age, Weeks Gestation, Pre-natal Visits, Marital...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbw0\n\n\n\n  \n\n\n\nWhat you’ll notice is that the variables have spaces in their names, which would require special handling later. The glimpse output shows you what to do about those spaces in variable names:\n\nglimpse(bw0)\n\nRows: 500\nColumns: 10\n$ `Father Age`           &lt;dbl&gt; 27, 35, 34, NA, 35, 32, 33, 38, 28, NA, 28, 34,…\n$ `Mother Age`           &lt;dbl&gt; 26, 33, 22, 16, 33, 24, 33, 35, 29, 19, 26, 31,…\n$ `Weeks Gestation`      &lt;dbl&gt; 38, 40, 37, 38, 39, 36, 38, 38, 40, 34, 39, 39,…\n$ `Pre-natal Visits`     &lt;dbl&gt; 14, 11, 10, 9, 12, 12, 15, 16, 5, 10, 15, 15, 0…\n$ `Marital Status`       &lt;dbl&gt; 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2,…\n$ `Mother Weight Gained` &lt;dbl&gt; 32, 23, 50, NA, 15, 12, 60, 2, 20, NA, 45, 22, …\n$ `Low Birthweight?`     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ `Weight (pounds)`      &lt;dbl&gt; 6.8750, 6.8125, 7.2500, 8.8125, 8.8125, 5.8125,…\n$ `Premie?`              &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,…\n$ `Few Visits?`          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,…\n\n\nWhat you have to do is to surround the variable name with “backticks”. (On my keyboard, that’s on the key to the left of number 1, where the squiggle is, that looks like a backwards apostrophe. Probably next to Esc, depending on the layout of your keyboard.) For example, to get the mean mother’s age, you have to do this:\n\nbw0 %&gt;% summarize(mom_mean = mean(`Mother Age`))\n\n\n\n  \n\n\n\nAlthough almost all of the variables are stored as integers, the ones that have a question mark in their name are actually “logical”, true or false, with 1 denoting true and 0 false. We could convert them later if we want to. A question mark is not a traditional character to put in a variable name either, so we have to surround these variables with backticks too.\nIn fact, all the variables have “illegal” names in one way or another: they contain spaces, or question marks, or brackets. So they all need backticks, which, as you can imagine, is rather awkward. The Capital Letters at the start of each word are also rather annoying to type every time.\nPeople who collect data are not always the people who analyze it, so there is not always a lot of thought given to column names in spreadsheets.\nSo how did I get you a dataset with much more sane variable names? Well, I used the janitor package, which has a function in it called clean_names. This is what it does:\n\nlibrary(janitor)\nbw0 %&gt;% clean_names() %&gt;% glimpse()\n\nRows: 500\nColumns: 10\n$ father_age           &lt;dbl&gt; 27, 35, 34, NA, 35, 32, 33, 38, 28, NA, 28, 34, N…\n$ mother_age           &lt;dbl&gt; 26, 33, 22, 16, 33, 24, 33, 35, 29, 19, 26, 31, 1…\n$ weeks_gestation      &lt;dbl&gt; 38, 40, 37, 38, 39, 36, 38, 38, 40, 34, 39, 39, 3…\n$ pre_natal_visits     &lt;dbl&gt; 14, 11, 10, 9, 12, 12, 15, 16, 5, 10, 15, 15, 0, …\n$ marital_status       &lt;dbl&gt; 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2…\n$ mother_weight_gained &lt;dbl&gt; 32, 23, 50, NA, 15, 12, 60, 2, 20, NA, 45, 22, 20…\n$ low_birthweight      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ weight_pounds        &lt;dbl&gt; 6.8750, 6.8125, 7.2500, 8.8125, 8.8125, 5.8125, 6…\n$ premie               &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0…\n$ few_visits           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0…\n\n\nAll the spaces have been replaced by underscores, the question marks and brackets have been removed, and all the uppercase letters have been made lowercase. The spaces have been replaced by underscores because an underscore is a perfectly legal thing to have in a variable name. I saved this dataset into the file you read in.\n\\(\\blacksquare\\)\n\nFrom your output, verify that you have the right number of observations and that you have several variables. Which of your variables correspond to birthweight, prematureness and length of pregnancy? (You might have to make guesses based on the names of the variables.)\n\nSolution\nAs a reminder:\n\nglimpse(bw)\n\nRows: 500\nColumns: 10\n$ father_age           &lt;dbl&gt; 27, 35, 34, NA, 35, 32, 33, 38, 28, NA, 28, 34, N…\n$ mother_age           &lt;dbl&gt; 26, 33, 22, 16, 33, 24, 33, 35, 29, 19, 26, 31, 1…\n$ weeks_gestation      &lt;dbl&gt; 38, 40, 37, 38, 39, 36, 38, 38, 40, 34, 39, 39, 3…\n$ pre_natal_visits     &lt;dbl&gt; 14, 11, 10, 9, 12, 12, 15, 16, 5, 10, 15, 15, 0, …\n$ marital_status       &lt;dbl&gt; 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2…\n$ mother_weight_gained &lt;dbl&gt; 32, 23, 50, NA, 15, 12, 60, 2, 20, NA, 45, 22, 20…\n$ low_birthweight      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ weight_pounds        &lt;dbl&gt; 6.8750, 6.8125, 7.2500, 8.8125, 8.8125, 5.8125, 6…\n$ premie               &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0…\n$ few_visits           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0…\n\n\nI do indeed have 500 observations (rows) on 10 variables (columns; “several”). (If you don’t have several variables, check to see that you didn’t use read_delim or something by mistake.) After that, you see all the variables by name, with what type of values they have,1 and the first few of the values.2\nThe variable weight_pounds is the birthweight (in pounds), premie is 1 for a premature baby and 0 for a full-term baby, and weeks_gestation is the number of weeks the pregnancy lasted.\n\\(\\blacksquare\\)\n\nThe theory behind the \\(t\\)-test (which we do later) says that the birth weights should be (approximately) normally distributed. Obtain a histogram of the birth weights. Does it look approximately normal? Comment briefly. (You’ll have to pick a number of bins for your histogram first. I don’t mind very much what you pick, as long as it’s not obviously too many or too few bins.)\n\nSolution\nYou’ll have seen that I often start with 10 bins, or maybe not quite that many if I don’t have much data, and this is a decent general principle. That would give\n\nggplot(bw, aes(x = weight_pounds)) + geom_histogram(bins = 10)\n\n\n\n\nwhich is perfectly acceptable with 500 observations. You can try something a bit more or a bit less, and see how you like it in comparison. What you are looking for is a nice clear picture of shape. If you have too few bins, you’ll lose the shape:\n\nggplot(bw, aes(x = weight_pounds)) + geom_histogram(bins = 4)\n\n\n\n\n(is that leftmost bin an indication of skewness or some observations that happen to be smallish?)\nAnd if you have too many, the shape will be there, but it will be hard to make out in all the noise, with frequencies going up and down:\n\nggplot(bw, aes(x = weight_pounds)) + geom_histogram(bins = 30)\n\n\n\n\nI generally am fairly relaxed about the number of bins you use, as long as it’s not clearly too few or too many. You might have done exercises in the past that illustrate that the choice of number of bins (or the class intervals where you move from one bin to the next, which is another issue that I won’t explore here) can make an appreciable difference to how a histogram looks.\nExtra: I had some thoughts about this issue that I put in a blog post, that you might like to read: link. The nice thing about Sturges’ rule, mentioned there, is that you can almost get a number of bins for your histogram in your head (as long as you know the powers of 2, that is). What you do is to start with your sample size, here \\(n=500\\). You find the next power of 2 above that, which is here \\(512=2^9\\). You then take that power and add 1, to get 10 bins. If you don’t like that, you can get R to calculate it for you:\n\nnclass.Sturges(bw$weight_pounds)\n\n[1] 10\n\n\nThe place where Sturges’ rule comes from is an assumption of normal data (actually a binomial approximation to the normal, backwards though that sounds). If you have less than 30 observations, you’ll get fewer than 6 bins, which won’t do much of a job of showing the shape. Rob Hyndman wrote a critical note about Sturges’ rule in which he asserts that it is just plain wrong (if you have taken B57, this note is very readable).\nSo what to use instead? Well, judgment is still better than something automatic, but if you want a place to start from, something with a better foundation than Sturges is the Freedman-Diaconis rule. This, in its original formulation, gives a bin width rather than a number of bins:\n\\[\nw=2(IQR)n^{-1/3}\n\\]\nThe nice thing about this is that it uses the interquartile range, so it won’t be distorted by outliers. geom_histogram can take a bin width, so we can use it as follows:\n\nw &lt;- 2 * IQR(bw$weight_pounds) * 500^(-1 / 3)\nw\n\n[1] 0.4094743\n\nggplot(bw, aes(x = weight_pounds)) + geom_histogram(binwidth = w)\n\n\n\n\nR also has\n\nnc &lt;- nclass.FD(bw$weight_pounds)\nnc\n\n[1] 26\n\n\nwhich turns the Freedman-Diaconis rule into a number of bins rather than a binwidth; using that gives the same histogram as we got with binwidth.\nIn my opinion, Freedman-Diaconis tends to give too many bins (here there are 26 rather than the 10 of Sturges). But I put it out there for you to make your own call.\nAnother way to go is a “density plot”. This is a smoothed-out version of a histogram that is not obviously frequencies in bins, but which does have a theoretical basis. It goes something like this:\n\nggplot(bw, aes(x = weight_pounds)) + geom_density()\n\n\n\n\ngeom_density has an optional parameter that controls how smooth or wiggly the picture is, but the default is usually good.\nAlright, before we got distracted, we were assessing normality. What about that?\nIt is mostly normal-looking, but I am suspicious about those very low birth weights, the ones below about 4 pounds. There are a few too many of those, as I see it.\nIf you think this is approximately normal, you need to make some comment along the lines of “the shape is approximately symmetric with no outliers”. I think my first answer is better, but this answer is worth something, since it is a not completely unreasonable interpretation of the histogram.\nI have been making the distinction between a histogram (for one quantitative variable) and side-by-side boxplots (for one quantitative variable divided into groups by one categorical variable). When you learned the boxplot, you probably learned it in the context of one quantitative variable. You can draw a boxplot for that, too, but the ggplot boxplot has an x as well as a y. What you do to make a single boxplot is to set the x equal 1, which produces a weird \\(x\\)-axis (that you ignore):\n\nggplot(bw, aes(x = 1, y = weight_pounds)) + geom_boxplot()\n\n\n\n\nThe high weight is actually an outlier, but look at all those outliers at the bottom!3\nI think the reason for those extra very low values is that they are the premature births (that can result in very small babies). Which leads to the additional question coming up later.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#more-about-the-nc-births-1",
    "href": "data-summaries.html#more-about-the-nc-births-1",
    "title": "3  Data exploration",
    "section": "3.9 More about the NC births",
    "text": "3.9 More about the NC births\nThis is an exploration of some extra issues around the North Carolina births data set.\n\nHow short does a pregnancy have to be, for the birth to be classified as “premature”? Deduce this from the data, by drawing a suitable graph or otherwise.\n\nSolution\nTo figure it out from the data, we can see how weeks_gestation depends on premie. Some possibilities are boxplots or a scatterplot. Either of the first two graphs would get full credit (for the graphing part: you still have to do the explanation) if this were being marked:\n\nggplot(bw,aes(x=factor(premie), y=weeks_gestation)) + geom_boxplot()\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThe warning is because the prematurity of one of the babies is not known. Or\n\nggplot(bw,aes(x=premie, y=weeks_gestation)) + geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe same warning again, for the same reason.\nNotice how the graphs are similar in syntax, because the what-to-plot is the same (apart from the factor thing) and we just make a small change in how-to-plot-it. In the boxplot, the thing on the \\(x\\)-scale needs to be categorical, and premie is actually a number, so we’d better make it into a factor, which is R’s version of a categorical variable. premie is actually a categorical variable (“premature” or “not premature”) masquerading as a quantitative one (1 or 0). It is an “indicator variable”, if you’re familiar with that term.\nIt looks as if the breakpoint is 37 weeks: a pregnancy at least that long is considered normal, but a shorter one ends with a premature birth. Both plots show the same thing: the premie=1 births all go with short pregnancies, shorter than 37 weeks. This is completely clear cut.\nAnother way to attack this is to use summarize, finding the max and min:\n\nbw %&gt;% summarize( n=n(),\nmin=min(weeks_gestation),\nmax=max(weeks_gestation))\n\n\n\n  \n\n\n\nonly this is for all the babies, premature or not.4 So we want it by prematurity, which means a group_by first:\n\nbw %&gt;% group_by(premie) %&gt;%\nsummarize( n=n(),\nmin=min(weeks_gestation),\nmax=max(weeks_gestation))\n\n\n\n  \n\n\n\ngroup_by with a number works, even though using the number in premie in a boxplot didn’t. group_by just uses the distinct values, whether they are numbers, text or factor levels.\nAny of these graphs or summaries will help you answer the question, in the same way. The ultimate issue here is “something that will get the job done”: it doesn’t matter so much what.\nExtra: In R, NA means “missing”. When you try to compute something containing a missing value, the answer is usually missing (since you don’t know what the missing value is). That’s why the first summarize gave us missing values: there was one missing weeks of gestation in with all the ones for which we had values, so the max and min had to be missing as well. In the second summarize, the one by whether a baby was born prematurely or not, we learn a bit more about that missing premie: evidently its weeks of gestation was missing as well, since the min and max of that were missing.5\nHere’s that baby. I’m doing a bit of fiddling to show all the columns (as rows, since there’s only one actual row). Don’t worry about the second line of code below; we will investigate that later in the course. Its job here is to show the values nicely:\n\nbw %&gt;% \n  filter(is.na(premie)) %&gt;% \n  pivot_longer(everything(), names_to=\"name\", values_to=\"value\")\n\n\n\n  \n\n\n\nThe only thing that was missing was its weeks of gestation, but that prevented anyone from figuring out whether it was premature or not.\n\\(\\blacksquare\\)\n\nExplore the relationship between birth weight and length of pregancy (“gestation”) using a suitable graph. What do you see?\n\nSolution\nThis needs to be a scatterplot because these are both quantitative variables:\n\nggplot(bw,aes(x=weeks_gestation, y=weight_pounds)) + geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nYou see a rather clear upward trend. Those very underweight babies came from very short pregnancies, but the vast majority of pregnancies were of more or less normal length (40 weeks is normal) and resulted in babies of more or less normal birth weight.\nExtra: I want to illustrate something else: how about colouring the births that were premature? Piece of cake with ggplot:\n\nggplot(bw,aes(x=weeks_gestation, y=weight_pounds, colour = premie)) + \n  geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThat was rather silly because ggplot treated prematureness as a continuous variable, and plotted the values on a dark blue-light blue scale. This is the same issue as on the boxplot above, and has the same solution:\n\nggplot(bw,aes(x=weeks_gestation, y=weight_pounds, \n              colour = factor(premie))) + geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nBetter.\nWith the normal-length pregnancies (red), there seems to be no relationship between length of pregnancy and birth weight, just a random variation. But with the premature births, a shorter pregnancy typically goes with a lower birth weight. This would be why the birth weights for the premature births were more variable.\n\\(\\blacksquare\\)\n\nDo a web search to find the standard (North American) definition of a premature birth. Does that correspond to what you saw in the data? Cite the website you used, for example by saying “according to URL, …”, with URL replaced by the address of the website you found.\n\nSolution\nThe website http://www.mayoclinic.org/diseases-conditions/premature-birth/basics/definition/con-20020050 says that “a premature birth is one that occurs before the start of the 37th week of pregnancy”, which is exactly what we found. (Note that I am citing the webpage on which I found this, and I even made it into a link so that you can check it.) The Mayo Clinic is a famous hospital system with locations in several US states, so I think we can trust what its website says.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#nenana-alaska-1",
    "href": "data-summaries.html#nenana-alaska-1",
    "title": "3  Data exploration",
    "section": "3.10 Nenana, Alaska",
    "text": "3.10 Nenana, Alaska\nNenana, Alaska, is about 50 miles west of Fairbanks. Every spring, there is a contest in Nenana. A wooden tripod is placed on the frozen river, and people try to guess the exact minute when the ice melts enough for the tripod to fall through the ice. The contest started in 1917 as an amusement for railway workers, and has taken place every year since. Now, hundreds of thousands of people enter their guesses on the Internet and the prize for the winner can be as much as $300,000.\nBecause so much money is at stake, and because the exact same tripod is placed at the exact same spot on the ice every year, the data are consistent and accurate. The data are in link.\n\nRead the data into R. Note that the values are separated by tabs rather than spaces, so you’ll need an appropriate read_ to read it in.\n\nSolution\nThese are “tab-separated values”, so read_tsv is the thing, as for the Australian athletes:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/nenana.txt\"\nnenana &lt;- read_tsv(myurl)\n\nRows: 87 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Date&Time\ndbl (2): Year, JulianDate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nUse whatever name you like for the data frame. One that is different from any of the column headers is smart; then it is clear whether you mean the whole data frame or one of its columns. ice or melt or anything like that would also be good.\nI haven’t asked you to display or check the data (that’s coming up), but if you look at it and find that it didn’t work, you’ll know to come back and try this part again. R usually gets it right or gives you an error.\nIf you look at the data, they do appear to be separated by spaces, but the text version of the date and time also have spaces in them, so things might go astray if you try and read the values in without recognizing that the actual separator is a tab:\n\nx &lt;- read_delim(myurl, \" \")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 87 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): Year   JulianDate  Date&Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOuch! A hint as to what went wrong comes from looking at the read-in data frame:\n\nx\n\n\n\n  \n\n\n\nThose t symbols mean “tab character”, which is our hint that the values were separated by tabs rather than spaces.\nMore detail (if you can bear to see it) is here:\n\nproblems(x)\n\n\n\n  \n\n\n\nThe first line of the data file (with the variable names in it) had no spaces, only tabs, so read_delim thinks there is one column with a very long name, but in the actual data, there are five space-separated columns. The text date-times are of the form “April 30 at 11:30 AM”, which, if you think it’s all separated by spaces, is actually 5 things: April, 30, at and so on. These are the only things that are separated by spaces, so, from that point of view, there are five columns.\n\\(\\blacksquare\\)\n\nFind a way of displaying how many rows and columns your data frame has, and some of the values. Describe the first and last of the variables that you appear to have.\n\nSolution\nThe easiest is just to display the tibble:\n\nnenana\n\n\n\n  \n\n\n\nAlternatively, you can take a glimpse of it:\n\nglimpse(nenana)\n\nRows: 87\nColumns: 3\n$ Year        &lt;dbl&gt; 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926…\n$ JulianDate  &lt;dbl&gt; 120.4795, 131.3983, 123.6066, 132.4490, 131.2795, 132.5559…\n$ `Date&Time` &lt;chr&gt; \"April 30 at 11:30 AM\", \"May 11 at 9:33 AM\", \"May 3 at 2:3…\n\n\nThere are 87 years, and 3 columns (variables). The first column is year, and the last column is the date and time that the tripod fell into the river, written as a piece of text. I explain the second column in a moment.\n\\(\\blacksquare\\)\n\nDates and times are awkward to handle with software. (We see more ways later in the course.) The column JulianDate expresses the time that the tripod fell through the ice as a fractional number of days since December 31. This enables the time (as a fraction of the way through the day) to be recorded as well, the whole thing being an ordinary number. Make a histogram of the Julian dates. Comment briefly on its shape.\n\nSolution\nWith a ggplot histogram, we need a number of bins first. I can do Sturges’ rule in my head: the next power of 2 up from 87 (our \\(n\\)) is 128, which is \\(2^7\\), so the base 2 log of 87 rounds up to 7. That plus one is 8, so we need 8 bins. For you, any not-insane number of bins will do, or any not-insane bin width, if you want to go that way:\n\nggplot(nenana, aes(x = JulianDate)) + geom_histogram(bins = 8)\n\n\n\n\nNote that you need to type JulianDate exactly as it appears, capital letters and all. R is case-sensitive.\nThis histogram looks more or less symmetric (and, indeed, normal). I really don’t think you can justify an answer other than “symmetric” here. (Or “approximately normal”: that’s good too.) If your histogram is different, say so. I think that “hole” in the middle is not especially important.\nWe haven’t done normal quantile plots yet, but looking ahead:\n\nggplot(nenana, aes(sample = JulianDate)) + stat_qq() + stat_qq_line()\n\n\n\n\nThat hugs the line pretty well, so I would call it close to normally-distributed. It bulges away from the line because there are more values just below 120 than you would expect for a normal. This corresponds to the histogram bar centred just below 120 being taller than you would have expected.6\nExtra: looking way ahead (to almost the end of the R stuff), this is how you handle the dates and times:\n\nlibrary(lubridate)\nnenana %&gt;%\n  mutate(longdt = str_c(Year, \" \", `Date&Time`)) %&gt;%\n  mutate(datetime = ymd_hm(longdt, tz = \"America/Anchorage\"))\n\n\n\n  \n\n\n\nI am not doing any further analysis with these, so just displaying them is good.\nI have to do a preliminary step to get the date-times with their year in one place. str_c glues pieces of text together: in this case, the year, a space, and then the rest of the Date&Time. I stored this in longdt. The second mutate is the business end of it: ymd_hm takes a piece of text containing a year, month (by name or number), day, hours, minutes in that order, and extracts those things from it, storing the whole thing as an R date-time. Note that the AM/PM was handled properly. The benefit of doing that is we can extract anything from the dates, such as the month or day of week, or take differences between the dates. Or even check that the Julian dates were calculated correctly (the lubridate function is called yday for “day of year”):\n\nnenana %&gt;%\n  mutate(longdt = str_c(Year, \" \", `Date&Time`)) %&gt;%\n  mutate(datetime = ymd_hm(longdt, tz = \"America/Anchorage\")) %&gt;%\n  mutate(jd = yday(datetime)) -&gt;\nnenana2\nnenana2 %&gt;% select(JulianDate, jd, datetime)\n\n\n\n  \n\n\n\nThe Julian days as calculated are the same. Note that these are not rounded; the Julian day begins at midnight and lasts until the next midnight. Thus Julian day 132 is May 12 (in a non-leap year like 1922) and the reason that the Julian date given in the file for that year would round to 133 is that it is after noon (1:20pm as you see).\n\\(\\blacksquare\\)\n\nPlot JulianDate against Year on a scatterplot. What recent trends, if any, do you see? Comment briefly.\n\nSolution\ngeom_point:\n\nggplot(nenana, aes(x = Year, y = JulianDate)) + geom_point()\n\n\n\n\nThis is actually a small-but-real downward trend, especially since about 1960, but the large amount of variability makes it hard to see, so I’m good with either “no trend” or “weak downward trend” or anything roughly like that. There is definitely not much trend before 1960, but most of the really early break-ups (less than about 118) have been since about 1990.\nYou can even add to the ggplot, by putting a smooth trend on it:\n\nggplot(nenana, aes(x = Year, y = JulianDate)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis is R’s version of a trend that is not constrained to be linear (so that it “lets the data speak for itself”).\nNow there is something obvious to see: after about 1960, there is a clear downward trend: the ice is breaking up earlier on average every year. Even though there is a lot of variability, the overall trend, viewed this way, is clear.\nWhat does this mean, in practice? This notion of the ice melting earlier than it used to is consistent all over the Arctic, and is one more indication of climate change. Precisely, it is an indication that climate change is happening, but we would have to delve further to make any statements about the cause of that climate change.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#computerized-accounting-1",
    "href": "data-summaries.html#computerized-accounting-1",
    "title": "3  Data exploration",
    "section": "3.11 Computerized accounting",
    "text": "3.11 Computerized accounting\nBeginning accounting students need to learn to learn to audit in a computerized environment. A sample of beginning accounting students took each of two tests: the Computer Attitude Scale (CAS) and the Computer Anxiety Rating Scale (CARS). A higher score in each indicates greater anxiety around computers. The test scores are scaled to be between 0 and 5. Also noted was each student’s gender. The data are in http://ritsokiguess.site/datafiles/compatt.txt. The data values are separated by spaces.\n\nRead the data into R. Do you have what you expected? Explain briefly.\n\nSolution\nRead in and display the data. This, I think, is the easiest way.\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/compatt.txt\"\nanxiety=read_delim(my_url,\" \")\n\nRows: 35 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): gender\ndbl (2): CAS, CARS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nanxiety\n\n\n\n  \n\n\n\nThere is a total of 35 students with a CAS score, a CARS score and a gender recorded for each. This is in line with what I was expecting. (You can also note that the genders appear to be a mixture of males and females.)\n\\(\\blacksquare\\)\n\nHow many males and females were there in the sample?\n\nSolution\nMost easily count:\n\nanxiety %&gt;% count(gender)\n\n\n\n  \n\n\n\nThis also works (and is therefore good):\n\nanxiety %&gt;% group_by(gender) %&gt;% summarize(count=n())\n\n\n\n  \n\n\n\nI want you to use R to do the counting (that is, don’t just go through the whole data set and count the males and females yourself). This is because you might have thousands of data values and you need to learn how to get R to count them for you.\n15 females and 20 males, which you should say. I made a point of not saying that it is enough to get the output with the answers on it, so you need to tell me what the answer is.\n\\(\\blacksquare\\)\n\nDo the CAS scores tend to be higher for females or for males? Draw a suitable graph to help you decide, and come to a conclusion.\n\nSolution\nGender is categorical and CAS score is quantitative, so a boxplot would appear to be the thing:\n\nggplot(anxiety,aes(x=gender,y=CAS))+geom_boxplot()\n\n\n\n\nThe median for males is slightly higher, so male accountants are more anxious around computers than female accountants are.\nIf you wish, side-by-side (or, better, above-and-below) histograms would also work:\n\nggplot(anxiety,aes(x=CAS))+geom_histogram(bins=6)+\nfacet_wrap(~gender,ncol=1)\n\n\n\n\nIf you go this way, you have to make a call about where the centres of the histograms are. I guess the male one is slightly further to the right, but it’s not so easy to tell. (Make a call.)\n\\(\\blacksquare\\)\n\nFind the median CAS scores for each gender. Does this support what you saw on your plot? Explain briefly.\n\nSolution\nGroup-by and summarize:\n\nanxiety %&gt;% group_by(gender) %&gt;% summarize(med=median(CAS))\n\n\n\n  \n\n\n\nThe median is a bit higher for males, which is what I got on my boxplot (and is apparently the same thing as is on the histograms, but it’s harder to be sure there).\n\\(\\blacksquare\\)\n\nFind the mean and standard deviation of both CAS and CARS scores (for all the students combined, ie. not separated by gender) without naming those columns explicitly.\n\nSolution\nWithout naming them explicitly means using some other way to pick them out of the data frame, summarize with across.\nThe basic across comes from asking yourself what the names of those columns have in common: they start with C and the gender column doesn’t:\n\nanxiety %&gt;% summarize(across(starts_with(\"C\"), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nAnother way is to ask what property these two columns have in common: they are the only two numeric (quantitative) columns. This means using an across with a where inside it, thus:\n\nanxiety %&gt;% summarize(across(where(is.numeric), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nRead the first one as “across all the columnns whose names start with S, find the mean and SD of them.” The second one is a little clunkier: “acrosss all the columns for which is.numeric is true, find the mean and SD of them”. A shorter way for the second one is “across all the numeric (quantitative) columns, find their mean and SD”, but then you have to remember exactly how to code that. The reason for the list is that we are calculating two statistics for each column that we find. I am using a “named list” so that the mean gets labelled with an m on the end of the column name, and the SD gets an s on the end.\nEither of these is good, or anything equivalent (like noting that the two anxiety scales both ends\\_with S):\n\nanxiety %&gt;% summarize(across(ends_with(\"S\"), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nBecause I didn’t say otherwise, you should tell me what the means and SDs are, rounding off suitably: the CAS scores have mean 2.82 and SD 0.48, and the CARS scores have mean 2.77 and SD 0.67.\nYet another way to do it is to select the columns you want first (which you can do by number so as not to name them), and then find the mean and SD of all of them:\n\nanxiety %&gt;% select(2:3) %&gt;% \n    summarize(across(everything(), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nThis doesn’t work:\n\nsummary(anxiety)\n\n    gender               CAS             CARS      \n Length:35          Min.   :1.800   Min.   :1.000  \n Class :character   1st Qu.:2.575   1st Qu.:2.445  \n Mode  :character   Median :2.800   Median :2.790  \n                    Mean   :2.816   Mean   :2.771  \n                    3rd Qu.:3.150   3rd Qu.:3.290  \n                    Max.   :3.750   Max.   :4.000  \n\n\nbecause, although it gets the means, it does not get the standard deviations. (I added the SD to the original question to make you find a way other than this.)\nIn summary, find a way to get those answers without naming those columns in your code, and I’m good.\nIn case you were wondering about how to do this separately by gender, well, put the group\\_by in like you did before:\n\nanxiety %&gt;% group_by(gender) %&gt;%\nsummarize(across(where(is.numeric), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nor\n\nanxiety %&gt;% group_by(gender) %&gt;%\nsummarize(across(starts_with(\"C\"), list(m = ~mean(.), s = ~sd(.))))\n\n\n\n  \n\n\n\nThe male means are slightly higher on both tests, but the male standard deviations are a little smaller. You might be wondering whether the test scores are related. They are both quantitative, so the obvious way to find out is a scatterplot:\n\nggplot(anxiety,aes(x=CAS,y=CARS))+geom_point()\n\n\n\n\nThe two variables can be on either axis, since there is no obvious response or explanatory variable. A higher score on one scale goes with a higher score on the other, suggesting that the two scales are measuring the same thing.\nThis plot mixes up the males and females, so you might like to distinguish them, which goes like this:\n\nggplot(anxiety,aes(x=CAS,y=CARS,colour=gender))+geom_point()\n\n\n\n\nThere is a slight (but only slight) tendency for the males to be up and to the right, and for the females to be down and to the left. This is about what you would expect, given that the male means are slightly bigger on both scores, but the difference in means is not that big compared to the SD.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#test-scores-in-two-classes-1",
    "href": "data-summaries.html#test-scores-in-two-classes-1",
    "title": "3  Data exploration",
    "section": "3.12 Test scores in two classes",
    "text": "3.12 Test scores in two classes\nOpen R Studio. Create a new Text File by selecting File, New File and Text File. You should see a new empty, untitled window appear at the top left. In that window, type or copy the data below (which are scores on a test for students in two different classes):\n\nclass score\nken 78\nken 62\nken 59\nken 69\nken 81\nthomas 83\nthomas 77\nthomas 63\nthomas 61\nthomas 79\nthomas 72\n\nSave the file, using a filename of your choice (with, perhaps, extension .txt). Or, if you prefer, use the one at link.\n\nRead the data into a data frame called marks, using read_delim, and list the data frame (by typing its name) to confirm that you read the data values properly. Note that the top line of the data file contains the names of the variables, as it ought to.\n\nSolution\nI was lazy and used the one on the web, the values being separated (“delimited”) by exactly one space:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/marks.txt\"\nmarks &lt;- read_delim(my_url, \" \")\n\nRows: 11 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): class\ndbl (1): score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmarks\n\n\n\n  \n\n\n\nIf you copied and pasted, or typed in, the data values yourself, use the local file name (such as marks.txt) in place of the URL.\nExtra: in the old days, when we used read.table (which actually also works here), we needed to also say header=T to note that the top line of the data file was variable names. With read_delim, that’s the default, and if the top line is not variable names, that’s when you have to say so. If I cheat, by skipping the first line and saying that I then have no column names, I get:\n\nread_delim(my_url, \" \", col_names = F, skip = 1)\n\nRows: 11 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): X1\ndbl (1): X2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n  \n\n\n\nColumn names are supplied (X1 and X2). I could also supply my own column names, in which case the file needs not to have any, so I need the skip again:\n\nread_delim(my_url, \" \", col_names = c(\"instructor\", \"mark\"), skip = 1)\n\nRows: 11 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): instructor\ndbl (1): mark\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\n* Obtain side-by-side boxplots of the scores for each class.\n\nSolution\n\nlibrary(tidyverse)\nggplot(marks, aes(x = class, y = score)) + geom_boxplot()\n\n\n\n\nRemember: on a regular boxplot,7 the groups go across (\\(x\\)), the variable measured goes up (\\(y\\)).\nExtra: this might work:\n\nggplot(marks, aes(x = class, y = score)) + geom_boxplot() +\n  coord_flip()\n\n\n\n\nIt does. That was a guess. So if you want sideways boxplots, this is how you can get them. Long group names sometimes fit better on the \\(y\\)-axis, in which case flipping the axes will help. (The x and y happen before the coordinate-flip, so they are the same as above, not the same way they come out.)\n\\(\\blacksquare\\)\n\nDo the two classes appear to have similar or different scores, on average? Explain briefly.\n\nSolution\nThe median for Thomas’s class appears to be quite a bit higher than for Ken’s class (the difference is actually about 6 marks). It’s up to you whether you think this is a big difference or not: I want you to have an opinion, but I don’t mind so much what that opinion is. Having said that the medians are quite a bit different, note that the boxes overlap substantially, so that the distributions of scores are pretty similar (or, the quartiles of scores are similar, or, the IQR of scores is similar for the two groups). If you say that, it’s good, but I’m not insisting that you do.\n\\(\\blacksquare\\)\n\nObtain a boxplot of all the scores together, regardless of which class they came from.\n\nSolution\nReplace your \\(x\\)-coordinate by some kind of dummy thing like 1 (factor(1) also works):\n\nggplot(marks, aes(x = 1, y = score)) + geom_boxplot()\n\n\n\n\nThe \\(x\\)-axis is kind of dopey, so you just ignore it. It is possible to remove it, but that is more work than it’s worth, and I didn’t get rid of the ticks below:\n\nggplot(marks, aes(x = 1, y = score)) + geom_boxplot() +\n  theme(\n    axis.text.x = element_blank(),\n    axis.title.x = element_blank()\n  )\n\n\n\n\n\\(\\blacksquare\\)\n\nCompute the median score (of all the scores together). Does this seem about right, looking at the boxplot? Explain briefly.\n\nSolution\nThree ways to get the median score. I like the first one best:\n\nmarks %&gt;% summarize(med = median(score))\n\n\n\n  \n\n\nwith(marks, median(score))\n\n[1] 72\n\nmedian(marks$score)\n\n[1] 72\n\n\nsummarize is the tidyverse “verb” that does what you want here. (The same idea gets the mean score for each class, below.)\nThe other ways use the basic function median. To make that work, you need to say that the variable score whose median you want lives in the data frame marks. These are two ways to do that.\nExtra: if you wanted median by group, this is the approved (tidyverse) way:\n\nmarks %&gt;%\n  group_by(class) %&gt;%\n  summarize(med = median(score))\n\n\n\n  \n\n\n\nTo get something by group, the extra step is group_by, and then whatever you do after that is done for each group.\nYou can now go back and compare these medians with the ones on the boxplots in (here). They should be the same. Or you can even do this:\n\nmarks %&gt;%\n  group_by(class) %&gt;%\n  summarize(\n    q1 = quantile(score, 0.25),\n    med = median(score),\n    q3 = quantile(score, 0.75)\n  )\n\n\n\n  \n\n\n\nYou can calculate as many summaries as you like. These ones should match up with the top and bottom of the boxes on the boxplots. The only restriction is that the things on the right side of the equals should return a single number. If you have a function like quantile without anything extra that returns more than one number:\n\nquantile(marks$score)\n\n  0%  25%  50%  75% 100% \n59.0 62.5 72.0 78.5 83.0 \n\n\nyou’re in trouble. Only read on if you really want to know how to handle this. Here’s step 1:\n\nmarks %&gt;%\n  nest_by(class)\n\n\n\n  \n\n\n\nThis is kind of a funky group_by. The things in the data column are the whole rest of the data frame: there were 5 students in Ken’s class and 6 in Thomas’s, and they each had a score, so 5 or 6 rows and 1 column. The column data is known in the trade as a “list-column”.\nNow, for each of those mini-data-frames, we want to calculate the quantiles of score. This is rowwise: for each of our mini-data-frames data, calculate the five-number summary of the column called score in it:\n\nmarks %&gt;%\n  nest_by(class) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(quantile(data$score)))\n\n\n\n  \n\n\n\nI have to be a little bit careful about which data frame I want the score to come from: the ones hidden in data, which are the things we’re for-eaching over.\nThis obtains a new list-column called qq, with the five-number summary for each group.8\nNow we want to display the quantiles. This is the easiest way:\n\nmarks %&gt;%\n  nest_by(class) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(quantile(data$score))) %&gt;% \n  unnest(qq)\n\n\n\n  \n\n\n\nThe unnest turns the list-column back into actual data, so we get the five quantiles for each class.\nThe only thing this doesn’t do is to show us which quantile is which (we know, of course, that the first one is the minimum, the last one is the max and the quartiles and median are in between). It would be nice to see which is which, though. A trick to do that is to use enframe, thus:\n\nquantile(marks$score) %&gt;% enframe()\n\n\n\n  \n\n\n\nor thus:\n\nenframe(quantile(marks$score))\n\n\n\n  \n\n\n\nI don’t normally like the second way with all the brackets, but we’ll be using it later.\nThe idea here is that the output from a quantile is a vector, but one with “names”, namely the percentiles themselves. enframe makes a two-column data frame with the names and the values. (You can change the names of the columns it creates, but here we’ll keep track of which is which.)\nSo we have a two-column data frame with a column saying which quantile is which. So let’s rewrite our code to use this:\n\nmarks %&gt;%\n  nest_by(class) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(enframe(quantile(data$score)))) \n\n\n\n  \n\n\n\nNote that the qq data frames in the list-column now themselves have two columns.\nAnd finally unnest qq:\n\nmarks %&gt;%\n  nest_by(class) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(enframe(quantile(data$score)))) %&gt;% \n  unnest(qq)\n\n\n\n  \n\n\n\nSuccess! Or even:\n\nmarks %&gt;%\n  nest_by(class) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(enframe(quantile(data$score)))) %&gt;% \n  unnest(qq) %&gt;% \n  mutate(qn = parse_number(name)) %&gt;%\n  select(-name) %&gt;%\n  pivot_wider(names_from = qn, values_from = value)\n\n\n\n  \n\n\n\nThis deliberately untidies the final answer to make it nicer to look at. (The lines before that create a numeric quantile, so that it sorts into the right order, and then get rid of the original quantile percents. Investigate what happens if you do a similar pivot_wider without doing that.)"
  },
  {
    "objectID": "data-summaries.html#unprecedented-rainfall-1",
    "href": "data-summaries.html#unprecedented-rainfall-1",
    "title": "3  Data exploration",
    "section": "3.13 Unprecedented rainfall",
    "text": "3.13 Unprecedented rainfall\nIn 1997, a company in Davis, California, had problems with odour in its wastewater facility. According to a company official, the problems were caused by “unprecedented weather conditions” and “because rainfall was at 170 to 180 percent of its normal level, the water in the holding ponds took longer to exit for irrigation, giving it more time to develop an odour.”\nAnnual rainfall data for the Davis area is here. Note that clicking on the link will display the file, and right-clicking on the link will give you some options, one of which is Copy Link Address, which you can then paste into your R Notebook.\nThe rainfall is measured in inches.\n\nRead in and display (some of) the data.\n\nSolution\nLook at the data file, and see that the values are separated by a single space, so will do it. Read straight from the URL; the hint above tells you how to copy it, which would even work if the link spans two lines.\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/rainfall.txt\"\nrain &lt;- read_delim(my_url, \" \")\n\nRows: 47 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): Year, Rainfall\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nrain\n\n\n\n  \n\n\n\nNote for later that the and the have Capital Letters. You can call the data frame whatever you like, but I think something descriptive is better than eg. .\nExtra: this works because there is exactly one space between the year and the rainfall amount. But the year is always four digits, so the columns line up, and there is a space all the way down between the year and the rainfall. That means that this will also work:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/rainfall.txt\"\nrain &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Year = col_double(),\n  Rainfall = col_double()\n)\n\nrain\n\n\n\n  \n\n\n\nThis is therefore also good.\nIt also looks as if it could be tab-separated values, since the rainfall column always starts in the same place, but if you try it, you’ll find that it doesn’t work:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/rainfall.txt\"\nrain_nogood &lt;- read_tsv(my_url)\n\nRows: 47 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Year Rainfall\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nrain_nogood\n\n\n\n  \n\n\n\nThis looks as if it worked, but it didn’t, because there is only one column, of years and rainfalls smooshed together as text, and if you try to do anything else with them later it won’t work.\nHence those values that might have been tabs actually were not. There’s no way to be sure about this; you have to try something and see what works. An indication, though: if you have more than one space, and the things in the later columns are left-justified, that could be tab-separated; if the things in the later columns are right-justified, so that they finish in the same place but don’t start in the same place, that is probably aligned columns.\n\\(\\blacksquare\\)\n\nSummarize the data frame.\n\nSolution\nI almost gave the game away: this is summary.\n\nsummary(rain)\n\n      Year         Rainfall    \n Min.   :1951   Min.   : 6.14  \n 1st Qu.:1962   1st Qu.:12.30  \n Median :1974   Median :16.72  \n Mean   :1974   Mean   :18.69  \n 3rd Qu.:1986   3rd Qu.:25.21  \n Max.   :1997   Max.   :37.42  \n\n\nThe summary of the years may not be very helpful, but the summary of the annual rainfall values might be. It’s not clear yet why I asked you to do this, but it will become clearer later.\n\\(\\blacksquare\\)\n\nMake a suitable plot of the rainfall values. (We are not, for the time being, concerned about the years.)\n\nSolution\nThis is one quantitative variable, so a histogram is your first thought. This means picking a number of bins. Not too many, since you want a picture of the shape:\n\nggplot(rain, aes(x=Rainfall)) + geom_histogram(bins=8)\n\n\n\n\nIf you picked fewer bins, you’ll get a different picture:\n\nggplot(rain, aes(x=Rainfall)) + geom_histogram(bins=6)\n\n\n\n\nThe choice of the number of bins depends on what you think the story about shape is that you want to tell (see next part). You will probably need to try some different numbers of bins to see which one you like best. You can say something about what you tried, for example “I also tried 8 bins, but I like the histogram with 6 bins better.”\n\\(\\blacksquare\\)\n\nHow would you describe the shape of the distribution of rainfall values?\n\nSolution\nThis will depend on the histogram you drew in the previous part. If it looks like the first one, the best answer is “bimodal”: that is, it has two peaks with a gap between them. If it looks like the second one, you have an easier time; this is ordinary right-skewness.\n\\(\\blacksquare\\)\n\nIn the quote at the beginning of the question, where do you think the assertion that the 1997 rainfall was “at 170 to 180 percent of its normal level” came from? Explain briefly.\n\nSolution\nFirst we need the 1997 rainfall. Go back and find it in the data. I am borrowing an idea from later in the course (because I am lazy):\n\nrain %&gt;% filter(Year==1997)\n\n\n\n  \n\n\n\n29.7 inches.\nNow, what would be a “normal level” of rainfall? Some kind of average, like a mean or a median, maybe. But we have those, from our summary that we made earlier, repeated here for (my) convenience:\n\nsummary(rain)\n\n      Year         Rainfall    \n Min.   :1951   Min.   : 6.14  \n 1st Qu.:1962   1st Qu.:12.30  \n Median :1974   Median :16.72  \n Mean   :1974   Mean   :18.69  \n 3rd Qu.:1986   3rd Qu.:25.21  \n Max.   :1997   Max.   :37.42  \n\n\nThe mean is 18.69 and the median is 16.72 inches.\nSo divide the 1997 rainfall by each of the summaries, and see what happens, using your calculator, or using R as a calculator:\n\n29.7/18.69\n\n[1] 1.589085\n\n29.7/16.72\n\n[1] 1.776316\n\n\nThe 1997 rainfall was about 178 percent of the normal level if the normal level was the median.\n\\(\\blacksquare\\)\n\nDo you think the official’s calculation was reasonable? Explain briefly. (Note that this is not the same as asking whether the official’s calculation was correct. This is an important distinction for you to make.)\n\nSolution\nThere are several approaches to take. Argue for yours.\nIf you came to the conclusion that the distribution was right-skewed, you can say that the sensible “normal level” is the median, and therefore the official did the right thing. Using the mean would have been the wrong thing.\nIf you thought the distribution was bimodal, you can go a couple of ways: (i) it makes no sense to use any measure of location for “normal” (in fact, the mean rainfall is almost in that low-frequency bar, and so is not really a “normal level” at all). Or, (ii) it looks as if the years split into two kinds: low-rainfall years with around 15 inches, and high-rainfall years with more than 25 inches. Evidently 1997 was a high-rainfall year, but 29.7 inches was not especially high for a high-rainfall year, so the official’s statement was an exaggeration. (I think (ii) is more insightful than (i), so ought to get more points.)\nYou could even also take a more conspiratorial approach and say that the official was trying to make 1997 look like a freak year, and picked the measure of location that made 1997 look more unusual.\n“Normal level” here has nothing to do with a normal distribution; for this to make sense, the official would have needed to say something like “normal shape”. This is why language skills are also important for a statistician to have.\n\\(\\blacksquare\\)\n\nDo you think that the official was right to use the word “unprecedented” to describe the 1997 rainfall? Justify your answer briefly.\n\nSolution\n“Unprecedented” means “never seen before” or “never having happened or existed in the past”.9 That came out of my head; this link has a very similar “never before known or experienced”).\nIf you look back at your histogram, there are several years that had over about 30 inches of rain: five or six, depending on your histogram. One of them was 1997, but there were others too, so 1997 was in no way “unprecedented”.\nAnother approach that you have seen is to View your dataframe:\n\nView(rain)\n\nThat will come out as a separate tab in your R Studio and you can look at it (yourself; it won’t appear in the Preview). You can look at the 1997 rainfall (29.69 inches) and count how many were bigger than that, 4 of them. Or, save yourself some effort10 and sort the rainfall values in descending order (with the biggest one first), by clicking on the little arrows next to Rainfall (twice). Mine looks like this:\n\nLater, we learn how to sort in code, which goes like this (to sort highest first):\n\nrain %&gt;% arrange(desc(Rainfall))\n\n\n\n  \n\n\n\nA more sophisticated way that we learn later:\n\nrain %&gt;% summarize(max=max(Rainfall))\n\n\n\n  \n\n\n\nThis is greater than the rainfall for 1997, ruling out “unprecedented”.\n1997 was only the fifth highest rainfall, and two of the higher ones were also in the 1990s. Definitely not “unprecedented”. The official needs to get a new dictionary!\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#learning-algebra-1",
    "href": "data-summaries.html#learning-algebra-1",
    "title": "3  Data exploration",
    "section": "3.14 Learning algebra",
    "text": "3.14 Learning algebra\nAt a high school in New Jersey, teachers were interested in what might help students to learn algebra. One idea was laptops as a learning aid, to see whether having access to one helped with algebra scores. (This was some time ago.) The 20 students in one class were given laptops to use in school and at home, while the 27 students in another class were not given laptops. For all of these students, the final exam score in algebra was recorded. The data are in http://ritsokiguess.site/datafiles/algebra.txt, with two columns, one indicating whether the student received a laptop or not, and the other giving their score on the algebra final exam.\n\nRead in and display (some of) the data. Do you have (i) the correct number of observations, and (ii) the correct type of columns? Explain briefly.\n\nSolution\nTake a look at the data file first: the data values are aligned in columns with variable numbers of spaces between, so read_table is the thing. Read directly from the URL, rather than trying to copy the data from the website:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/algebra.txt\"\nalgebra &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  laptop = col_character(),\n  score = col_double()\n)\n\nalgebra\n\n\n\n  \n\n\n\nThere were \\(20+27=47\\) students altogether in the two classes, and we do indeed have 47 rows, one per student. So we have the right number of rows. This is two independent samples; each student was in only one of the two classes, either the class whose students got laptops or not. The values in the laptop column are text (see the chr at the top), and the values in the score column are numbers (dbl at the top). Alternatively, you can look at the R Console output in which you see that laptop is col_character() (text) and score is col_double() (numerical, strictly a decimal number).\nExtra 1: read.table also works but it is wrong in this course (because it is not what I taught you in class).\nExtra 2: with more than one space between the values, read_delim will not work. Or, perhaps more confusing, it will appear to work and then fail later, which means that you need to pay attention:\n\nd &lt;- read_delim(my_url, \" \")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 47 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): laptop, score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nd\n\n\n\n  \n\n\n\nThis looks all right, but look carefully: the laptop column is correctly text, but the score column, which should be numbers (dbl), is actually text as well. An easier way to see this is to look at the output from the console, which is the descriptions of the columns: they are both col_character or text, while score should be numbers. You might be able to see exactly what went wrong: with more than one space separating the values, the remaining spaces went into score, which then becomes a piece of text with some spaces at the front and then numbers.\nThis will actually work for a while, as you go through the question, but will come back to bite you the moment you need score to be numerical (eg. when you try to draw a boxplot), because it is actually not numerical at all.\nExtra 3: this is the standard R way to lay out this kind of data, with all the outcome values in one column and a second (categorical) column saying which group each observation was in. In other places you might see two separate columns of scores, one for the students with laptops and one for the students without, as below (you won’t understand the code below now, but you will by the end of the course):\n\nalgebra %&gt;% \nmutate(row = c(1:20, 1:27)) %&gt;% \npivot_wider(names_from = laptop, values_from = score)\n\n\n\n  \n\n\n\nA column of yes and a column of no. The classes were of different sizes, so the yes column, with only 20 observations, has some NA (“missing”) observations at the end (scroll down to see them) to enable the dataframe to keep a rectangular shape.\nWe will learn later what to call these layouts of data: “longer” and “wider” (respectively), and how to convert between them. R usually likes “longer” data, as in the data file, but you will often see data sets displayed wider because it takes up less space.\n\\(\\blacksquare\\)\n\nMake a suitable graph of these data.\n\nSolution\nThe teachers were hoping to see how the laptop-yes and the laptop-no groups compared in terms of algebra scores, so side-by-side boxplots would be helpful. More simply, we have one quantitative and one categorical variable, which is a boxplot according to the table in the notes:\n\nggplot(algebra, aes(x = laptop, y = score)) + geom_boxplot()\n\n\n\n\nExtra: as you will note below, the median score for the students with laptops is a little higher for the students who had laptops. This is easy to see on a boxplot because that is what a boxplot does. (That was what Tukey, who we will meet later, designed the boxplot to do.)\nAnother plot you might have drawn is a histogram for each group, side by side, or, as they come out here, above and below. This works using facets:\n\nggplot(algebra, aes(x = score)) + \ngeom_histogram(bins = 10) +\nfacet_wrap(~laptop, ncol = 1)\n\n\n\n\nLooking at those, can you really say that the median is slightly higher for the yes group? I really don’t think you can. Certainly it is clear from the histograms that the spread for the yes group is less, but comparing the medians is much more easily done from the boxplot. The teachers were interested in whether the laptops were associated with higher scores on average, so the kind of comparison that the boxplot affords is clearly preferred here.\nIf you are interested in the code: you imagine you’re going to make a histogram of scores regardless of group, and then at the end you facet by your grouping variable. I added the ncol = 1 to make the plots come out in one column (that is, one above the other). If you don’t do this, they come out left and right, which makes the distributions even harder to compare.\n\\(\\blacksquare\\)\n\nComment briefly on your graph, thinking about what the teachers would like to know.\n\nSolution\nThere are three things to say something about, the first two of which would probably interest the teachers:\n\ncomparison of centre: the median score for the group that had laptops is (slightly) higher than for the group that did not.\ncomparison of spread: the scores for the group that had laptops are less spread out (have smaller interquartile range) than for the group that did not.\nassessment of shape: both groups have low outliers, or are skewed to the left in shape.\n\nSome comments from me:\n\nboxplots say nothing about mean and standard deviation, so don’t mention those here. You should say something about the measures of centre (median) and spread (IQR) that they do use.\nI think of skewness as a property of a whole distribution, but outlierness as a property of individual observations. So, when you’re looking at this one, think about where the evidence about shape is coming from: is it coming from those one or two low values that are different from the rest (which would be outliers), or is it coming from the whole distribution (would you get the same story if those maybe-outliers are taken away)? My take is that if you take the outliers away, both distributions are close to symmetric, and therefore what you see here is outliers rather than skewness. If you see something different, make the case for it.\n\nOne reason to suspect skewness or something like it is that test scores have an upper limit (100) that some of the scores got close to, and no effective lower limit (the lower limit is 0 but no-one got very close to that). In this sort of situation, you’d expect the scores to be skewed away from the limit: that is, to the left. Or to have low outliers rather than high ones.\n\\(\\blacksquare\\)\n\nWork out the median and inter-quartile range for the students who did and who did not have laptops, and compare with the boxplot. (In R, the inter-quartile range is IQR in uppercase.)\n\nSolution\nThis is easy to make way harder than it needs to be: group_by and summarize will do it. Put the two summaries in one summarize:\n\nalgebra %&gt;% \ngroup_by(laptop) %&gt;% \nsummarize(med = median(score), iqr = IQR(score))\n\n\n\n  \n\n\n\nThen relate these to the information on the boxplot: the centre line of the box is the median. For the no group this is just above 80, so 81 makes sense; for the yes group this is not quite halfway between 80 and 90, so 84 makes sense.\nThe inter-quartile range is the height of the box for each group. Estimate the top and bottom of the two boxes from the boxplot scale, and subtract. For the no group this is something like \\(88-68\\) which is 20, and for the yes group it is something like \\(93-80\\) which is indeed 13.\nExtra: I didn’t ask you here about whether the difference was likely meaningful. The focus here was on getting the graph and summaries. If I had done so, you would then need to consider things like whether a three-point difference in medians could have been chance, and whether we really had random allocation of students to groups.\nTo take the second point first: these are students who chose to take two different classes, rather than being randomly allocated to classes as would be the case in a true experiment. What we have is really in between an experiment and an observational study; yes, there was a treatment (laptop or not) that was (we hope) randomly allocated to one class and not the other, but the classes could have been different for any number of other reasons that had nothing to do with having laptops or not, such as time of day, teacher, approach to material, previous ability at algebra, etc.\nSo even if we are willing to believe that the students were as-if randomized to laptop or not, the question remains as to whether that three-point difference in medians is reproducible or indicative of a real difference or not. This is the kind of thing we would try a two-sample \\(t\\)-test with. In this case, we might doubt whether it will come out significant (because of the small difference in medians and presumably means, compared to the amount of variability present), and, even then, there is the question of whether we should be doing a \\(t\\)-test at all, given the outliers.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "data-summaries.html#footnotes",
    "href": "data-summaries.html#footnotes",
    "title": "3  Data exploration",
    "section": "",
    "text": "these are mostly int, that is, integer.↩︎\nOther possible variable types are num for (real, decimal) numbers such as birth weight, chr for text, and Factor (with the number of levels) for factors/categorical variables. We don’t have any of the last two here. There is also lgl for logical, things that were actually recorded as TRUE or FALSE. We have some variables that are actually logical ones, but they are recorded as integer values.↩︎\nWhen Tukey, a name we will see again, invented the boxplot in the 1950s, 500 observations would have been considered a big data set. He designed the boxplot to produce a sensible number of outliers for the typical size of data set of his day, but a boxplot of a large data set tends to have a lot of outliers that are probably not really outliers at all.↩︎\nI explain the missing values below.↩︎\nIf there had been a weeks of gestation, we could have figured out whether it was premature or not, according to whether the weeks of gestation was less than 37.↩︎\nThat is to say, the principal deviation from normality is not the hole on the histogram, the bar centred around 123 being too short, but that the bar centred just below 120 is too tall.↩︎\nBoxplots can also go across the page, but for us, they don’t.↩︎\nIt’s actually a coincidence that the five-number summary and Ken’s class both have five values in them.↩︎\nSearching for “define” followed by a word is a good way to find out exactly what that word means, if you are not sure, but you should at least say where you got the definition from if you had to look it up.↩︎\nWhen you have a computer at your disposal, it’s worth taking a few minutes to figure out how to use it to make your life easier.↩︎"
  },
  {
    "objectID": "one-sample-inference.html#hunter-gatherers-in-australia",
    "href": "one-sample-inference.html#hunter-gatherers-in-australia",
    "title": "4  One-sample inference",
    "section": "4.1 Hunter-gatherers in Australia",
    "text": "4.1 Hunter-gatherers in Australia\nA hunter-gatherer society is one where people get their food by hunting, fishing or foraging rather than by agriculture or by raising animals. Such societies tend to move from place to place. Anthropologists have studied hunter-gatherer societies in forest ecosystems across the world. The average population density of these societies is 7.38 people per 100 km\\(^2\\). Hunter-gatherer societies on different continents might have different population densities, possibly because of large-scale ecological constraints (such as resource availability), or because of other factors, possibly social and/or historic, determining population density.\nSome hunter-gatherer societies in Australia were studied, and the population density per 100 km\\(^2\\) recorded for each. The data are in http://ritsokiguess.site/datafiles/hg.txt.\n\nRead the data into R. Do you have the correct variables? How many hunter-gatherer societies in Australia were studied? Explain briefly.\nThe question of interest is whether these Australian hunter-gatherer societies are like the rest of the world in terms of mean population density. State suitable null and alternative hypotheses. Define any symbols you use: that is, if you use a symbol, you also have to say what it means.\nTest your hypotheses using a suitable test. What do you conclude, in the context of the data?\nDo you have any doubts about the validity of your test? Explain briefly, using a suitable graph to support your explanation."
  },
  {
    "objectID": "one-sample-inference.html#buses-to-boulder",
    "href": "one-sample-inference.html#buses-to-boulder",
    "title": "4  One-sample inference",
    "section": "4.2 Buses to Boulder",
    "text": "4.2 Buses to Boulder\nA bus line operates a route from Denver to Boulder (these places are in Colorado). The schedule says that the journey time should be 60 minutes. 11 randomly chosen journey times were recorded, and these are in the file link, with journey times shown in minutes.\n\nRead the data into R, and display the data frame that you read in.\nRun a suitable test to see whether there is evidence that the mean journey time differs from 60 minutes. What do you conclude? (I want a conclusion that says something about journey times of buses.)\nGive a 95% confidence interval for the mean journey time. (No R code is needed here.)\nDo you draw consistent conclusions from your test and confidence interval? Explain briefly.\nDraw a boxplot of the journey times. Do you see a reason to doubt the test that you did above?"
  },
  {
    "objectID": "one-sample-inference.html#length-of-gestation-in-north-carolina",
    "href": "one-sample-inference.html#length-of-gestation-in-north-carolina",
    "title": "4  One-sample inference",
    "section": "4.3 Length of gestation in North Carolina",
    "text": "4.3 Length of gestation in North Carolina\nThe data in file link are about 500 randomly chosen births of babies in North Carolina. There is a lot of information: not just the weight at birth of the baby, but whether the baby was born prematurely, the ages of the parents, whether the parents are married, how long (in weeks) the pregnancy lasted (this is called the “gestation”) and so on. We have seen these data before.\n\nRead in the data from the file into R, bearing in mind what type of file it is.\nFind a 95% confidence interval for the mean birth weight of all babies born in North Carolina (of which these babies are a sample). At the end, you should state what the confidence interval is. Giving some output is necessary, but not enough by itself.\nBirth weights of babies born in the United States have a mean of 7.3 pounds. Is there any evidence that babies born in North Carolina are less heavy on average? State appropriate hypotheses, do your test, obtain a P-value and state your conclusion, in terms of the original data.\nThe theory behind the \\(t\\)-test says that the distribution of birth weights should be (approximately) normally distributed. Obtain a histogram of the birth weights. Does it look approximately normal? Comment briefly. (You’ll have to pick a number of bins for your histogram first. I don’t mind very much what you pick, as long as it’s not obviously too many or too few bins.)"
  },
  {
    "objectID": "one-sample-inference.html#inferring-ice-break-up-in-nenana",
    "href": "one-sample-inference.html#inferring-ice-break-up-in-nenana",
    "title": "4  One-sample inference",
    "section": "4.4 Inferring ice break-up in Nenana",
    "text": "4.4 Inferring ice break-up in Nenana\nNenana, Alaska, is about 50 miles west of Fairbanks. Every spring, there is a contest in Nenana. A wooden tripod is placed on the frozen river, and people try to guess the exact minute when the ice melts enough for the tripod to fall through the ice. The contest started in 1917 as an amusement for railway workers, and has taken place every year since. Now, hundreds of thousands of people enter their guesses on the Internet and the prize for the winner can be as much as $300,000.\nBecause so much money is at stake, and because the exact same tripod is placed at the exact same spot on the ice every year, the data are consistent and accurate. The data are in link.\nYes, we saw these data before.\n\nRead the data into R, as before, or use the data frame that you read in before. Note that the values are separated by tabs rather than spaces, so you’ll need an appropriate read_ to read it in.\nObtain a 90% confidence interval for the mean JulianDate. What interval do you get? Looking back at your histogram, do you have any doubts about the validity of what you have just done?\nAn old-timer in Nenana strokes his grey beard and says “When I were young, I remember the tripod used to fall into the water around May 10”. In a non-leap year, May 10 is Julian day 130. Test the null hypothesis that the mean JulianDay is 130, against the alternative that it is less. What do you conclude? What practical implication does that have (assuming that the old-timer has a good memory)?\nPlot JulianDate against Year on a scatterplot. What recent trends, if any, do you see? Comment briefly. (You did this before, but I have some extra comments on the graph this time, so feel free to just read this part.)"
  },
  {
    "objectID": "one-sample-inference.html#diameters-of-trees",
    "href": "one-sample-inference.html#diameters-of-trees",
    "title": "4  One-sample inference",
    "section": "4.5 Diameters of trees",
    "text": "4.5 Diameters of trees\nThe Wade Tract in Thomas County, Georgia, is an old-growth forest of longleaf pine trees. It has survived in a relatively undisturbed state since before settlements of the area by Europeans. For each tree in the tract, researchers measured the diameter at breast height. This is a standard measure in forestry: it is defined as the diameter of the tree at 4.5 feet above the ground.1 They are interested in the mean diameter at breast height of the trees in this tract. These values are in http://ritsokiguess.site/datafiles/treediameter.csv. The diameters are measured in centimetres. The easiest way to get the URL is to right-click on the blue text and select Copy URL. (If you copy and paste the actual text you might end up with extra spaces, especially if the printed URL goes over two lines.)\n\nRead in and display (some of) the data.\nMake a suitable plot of your dataframe.\nObtain a 95% confidence interval for the mean diameter.\nBased on what you have seen so far, would you expect to reject a null hypothesis that the population mean diameter (of all longleaf pines like these) is 35 cm? Explain briefly. Then, carry out the test (against a two-sided alternative) and explain briefly whether you were right.\nWould you expect 35 cm to be in a 99% confidence interval for the mean diameter? Explain briefly, and then see if you were right."
  },
  {
    "objectID": "one-sample-inference.html#one-sample-cholesterol",
    "href": "one-sample-inference.html#one-sample-cholesterol",
    "title": "4  One-sample inference",
    "section": "4.6 One-sample cholesterol",
    "text": "4.6 One-sample cholesterol\nThe data set here contains cholesterol measurements for heart attack patients (at several different times) as well as for a group of control patients. We will focus on the control patients in this question.\n\nRead in and display (some of) the data.\nMake a suitable plot of the cholesterol levels of the control patients, and comment briefly on the shape of the distribution.\nIt is recommended that people in good health, such as the Control patients here, keep their cholesterol level below 200. Is there evidence that the mean cholesterol level of the population of people of which the Control patients are a sample is less than 200? Show that you understand the process, and state your conclusion in the context of the data.\nWhat values could the population mean cholesterol level take? You might need to get some more output to determine this.\nExplain briefly why you would be reasonably happy to trust the \\(t\\) procedures in this question. (There are two points you need to make.)\n\nMy solutions follow:"
  },
  {
    "objectID": "one-sample-inference.html#hunter-gatherers-in-australia-1",
    "href": "one-sample-inference.html#hunter-gatherers-in-australia-1",
    "title": "4  One-sample inference",
    "section": "4.7 Hunter-gatherers in Australia",
    "text": "4.7 Hunter-gatherers in Australia\nA hunter-gatherer society is one where people get their food by hunting, fishing or foraging rather than by agriculture or by raising animals. Such societies tend to move from place to place. Anthropologists have studied hunter-gatherer societies in forest ecosystems across the world. The average population density of these societies is 7.38 people per 100 km\\(^2\\). Hunter-gatherer societies on different continents might have different population densities, possibly because of large-scale ecological constraints (such as resource availability), or because of other factors, possibly social and/or historic, determining population density.\nSome hunter-gatherer societies in Australia were studied, and the population density per 100 km\\(^2\\) recorded for each. The data are in http://ritsokiguess.site/datafiles/hg.txt.\n\nRead the data into R. Do you have the correct variables? How many hunter-gatherer societies in Australia were studied? Explain briefly.\n\nSolution\nThe data values are separated by (single) spaces, so read_delim is the thing:\n\nurl=\"http://ritsokiguess.site/datafiles/hg.txt\"\nsocieties=read_delim(url,\" \")\n\nRows: 13 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): name\ndbl (1): density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nI like to put the URL in a variable first, because if I don’t, the read_delim line can be rather long. But if you want to do it in one step, that’s fine, as long as it’s clear that you are doing the right thing.\nLet’s look at the data frame:\n\nsocieties\n\n\n\n  \n\n\n\nI have the name of each society and its population density, as promised (so that is correct). There were 13 societies that were studied. For me, they were all displayed. For you, you’ll probably see only the first ten, and you’ll have to click Next to see the last three.\n\\(\\blacksquare\\)\n\nThe question of interest is whether these Australian hunter-gatherer societies are like the rest of the world in terms of mean population density. State suitable null and alternative hypotheses. Define any symbols you use: that is, if you use a symbol, you also have to say what it means.\n\nSolution\nThe mean for the world as a whole (“average”, as stated earlier) is 7.38. Let \\(\\mu\\) denote the population mean for Australia (of which these societies are a sample). Then our hypotheses are: \\[ H_0: \\mu=7.38\\] and \\[ H_a: \\mu \\ne 7.38.\\] There is no reason for a one-sided alternative here, since all we are interested in is whether Australia is different from the rest of the world. Expect to lose a point if you use the symbol \\(\\mu\\) without saying what it means.\n\\(\\blacksquare\\)\n\nTest your hypotheses using a suitable test. What do you conclude, in the context of the data?\n\nSolution\nA \\(t\\)-test, since we are testing a mean:\n\nt.test(societies$density,mu=7.38)\n\n\n    One Sample t-test\n\ndata:  societies$density\nt = 3.8627, df = 12, p-value = 0.002257\nalternative hypothesis: true mean is not equal to 7.38\n95 percent confidence interval:\n 15.59244 36.84449\nsample estimates:\nmean of x \n 26.21846 \n\n\nThe P-value is 0.0023, less than the usual \\(\\alpha\\) of 0.05, so we reject the null hypothesis and conclude that the mean population density is not equal to 7.38. That is to say, Australia is different from the rest of the world in this sense.\nAs you know, “reject the null hypothesis” is only part of the answer, so gets only part of the marks.\n\\(\\blacksquare\\)\n\nDo you have any doubts about the validity of your test? Explain briefly, using a suitable graph to support your explanation.\n\nSolution\nThe assumption behind the \\(t\\)-test is that the data are approximately normal. We can assess that in several ways, but the simplest (which is perfectly acceptable at this point) is a histogram. You’ll need to pick a suitable number of bins. This one comes from Sturges’ rule:\n\nggplot(societies,aes(x=density))+geom_histogram(bins=5)\n\n\n\n\nYour conclusion might depend on how many bins you chose for your histogram. Here’s 8 bins (which is really too many with only 13 observations, but it actually shows the shape well):\n\nggplot(societies,aes(x=density))+geom_histogram(bins=8)\n\n\n\n\nor you can get a number of bins from one of the built-in functions, such as:\n\nmybins=nclass.FD(societies$density)\nmybins\n\n[1] 3\n\n\nThis one is small. The interquartile range is large and \\(n\\) is small, so the binwidth will be large and therefore the number of bins will be small.\nOther choices: a one-group boxplot:\n\nggplot(societies,aes(x=1,y=density))+geom_boxplot()\n\n\n\n\nThis isn’t the best for assessing normality as such, but it will tell you about lack of symmetry and outliers, which are the most important threats to the \\(t\\)-test, so it’s fine here. Or, a normal quantile plot:\n\nggplot(societies,aes(sample=density))+\nstat_qq()+stat_qq_line()\n\n\n\n\nThis is actually the best way to assess normality, but I’m not expecting you to use this plot here, because we may not have gotten to it in class yet. (If you have read ahead and successfully use the plot, it’s fine.)\nAfter you have drawn your chosen plot (you need one plot), you need to say something about normality and thus whether you have any doubts about the validity of your \\(t\\)-test. This will depend on the graph you drew: if you think your graph is symmetric and outlier-free, you should have no doubts about your \\(t\\)-test; if you think it has something wrong with it, you should say what it is and express your doubts. My guess is that you will think this distribution is skewed to the right. Most of my plots are saying that.2\nOn the website where I got these data, they were using the data as an example for another test, precisely because they thought the distribution was right-skewed. Later on, we’ll learn about the sign test for the median, which I think is actually a better test here.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#buses-to-boulder-1",
    "href": "one-sample-inference.html#buses-to-boulder-1",
    "title": "4  One-sample inference",
    "section": "4.8 Buses to Boulder",
    "text": "4.8 Buses to Boulder\nA bus line operates a route from Denver to Boulder (these places are in Colorado). The schedule says that the journey time should be 60 minutes. 11 randomly chosen journey times were recorded, and these are in the file link, with journey times shown in minutes.\n\nRead the data into R, and display the data frame that you read in.\n\nSolution\nSince you can read the data directly from the URL, do that (if you are online) rather than having to copy and paste and save, and then find the file you saved. Also, there is only one column, so you can pretend that there were multiple columns, separated by whatever you like. It’s least typing to pretend that they were separated by commas like a .csv file:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/buses.txt\"\njourney.times &lt;- read_csv(my_url)\n\nRows: 11 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): minutes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\njourney.times\n\n\n\n  \n\n\n\nUsing read_delim with any delimiter (such as \" \") will also work, and is thus also good.\nVariable names in R can have a dot (or an underscore, but not a space) in them. I have grown accustomed to using dots to separate words. This works in R but not other languages, but is seen by some as old-fashioned, with underscores being the modern way.3 You can also use what is called “camel case” by starting each “word” after the first with an uppercase letter like this:\n\njourneyTimes &lt;- read_csv(my_url)\n\nYou have to get the capitalization and punctuation right when you use your variables, no matter what they’re called. In any of the cases above, there is no variable called journeytimes. As Jenny Bryan (in link) puts it, boldface in original: Implicit contract with the computer / scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Get better at typing.\n\\(\\blacksquare\\)\n\nRun a suitable test to see whether there is evidence that the mean journey time differs from 60 minutes. What do you conclude? (I want a conclusion that says something about journey times of buses.)\n\nSolution\nt.test doesn’t take a data= to say which data frame to use. Wrap it in a with:\n\nwith(journey.times, t.test(minutes, mu = 60))\n\n\n    One Sample t-test\n\ndata:  minutes\nt = 1.382, df = 10, p-value = 0.1971\nalternative hypothesis: true mean is not equal to 60\n95 percent confidence interval:\n 57.71775 69.73680\nsample estimates:\nmean of x \n 63.72727 \n\n\nWe are testing that the mean journey time is 60 minutes, against the two-sided alternative (default) that the mean is not equal to 60 minutes. The P-value, 0.1971, is a lot bigger than the usual \\(\\alpha\\) of 0.05, so we cannot reject the null hypothesis. That is, there is no evidence that the mean journey time differs from 60 minutes.\nAs you remember, we have not proved that the mean journey time is 60 minutes, which is what “accepting the null hypothesis” would be. We have only failed to reject it, in a shoulder-shrugging kind of way: “the mean journey time could be 60 minutes”. The other acceptable word is “retain”; when you say “we retain the null hypothesis”, you imply something like “we act as if the mean is 60 minutes, at least until we find something better.”\n\\(\\blacksquare\\)\n\nGive a 95% confidence interval for the mean journey time. (No R code is needed here.)\n\nSolution\nJust read it off from the output: 57.72 to 69.74 minutes.\n\\(\\blacksquare\\)\n\nDo you draw consistent conclusions from your test and confidence interval? Explain briefly.\n\nSolution\nThe test said that we should not reject a mean of 60 minutes. The confidence interval says that 60 minutes is inside the interval of plausible values for the population mean, which is another way of saying the same thing. (If we had rejected 60 as a mean, 60 would have been outside the confidence interval.)\n\\(\\blacksquare\\)\n\nDraw a boxplot of the journey times. Do you see a reason to doubt the test that you did above?\n\nSolution\nThe grouping variable is a “nothing” as in the Ken and Thomas question (part (d)):\n\nggplot(journey.times, aes(x = 1, y = minutes)) + geom_boxplot()\n\n\n\n\nThe assumption behind the \\(t\\)-test is that the population from which the data come has a normal distribution: ie. symmetric with no outliers. A small sample (here we have 11 values) even from a normal distribution might look quite non-normal (as in Assignment 0 from last week), so I am not hugely concerned by this boxplot. However, it’s perfectly all right to say that this distribution is skewed, and therefore we should doubt the \\(t\\)-test, because the upper whisker is longer than the lower one. In fact, the topmost value is very nearly an outlier:4\n\nggplot(journey.times, aes(x = minutes)) + geom_histogram(bins = 5)\n\n\n\n\nand there might be skewness as well, so maybe I should have been concerned.\nI would be looking for some intelligent comment on the boxplot: what it looks like vs. what it ought to look like. I don’t so much mind what that comment is, as long as it’s intelligent enough.\nPerhaps I should draw a normal quantile plot:\n\nggplot(journey.times, aes(sample = minutes)) + stat_qq() + stat_qq_line()\n\n\n\n\nThe normal quantile plot is saying that the problem is actually at the bottom of the distribution: the lowest value is not low enough, but the highest value is actually not too high. So this one seems to be on the edge between OK and being right-skewed (too bunched up at the bottom). My take is that with this small sample this is not too bad. But you are free to disagree.\nIf you don’t like the normality, you’d use a sign test and test that the median is not 60 minutes, which you would (at my guess) utterly fail to reject:\n\nlibrary(smmr)\nsign_test(journey.times, minutes, 60)\n\n$above_below\nbelow above \n    4     7 \n\n$p_values\n  alternative   p_value\n1       lower 0.8867187\n2       upper 0.2744141\n3   two-sided 0.5488281\n\nci_median(journey.times, minutes)\n\n[1] 54.00195 71.99023\n\n\nand so we do. The median could easily be 60 minutes.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#length-of-gestation-in-north-carolina-1",
    "href": "one-sample-inference.html#length-of-gestation-in-north-carolina-1",
    "title": "4  One-sample inference",
    "section": "4.9 Length of gestation in North Carolina",
    "text": "4.9 Length of gestation in North Carolina\nThe data in file link are about 500 randomly chosen births of babies in North Carolina. There is a lot of information: not just the weight at birth of the baby, but whether the baby was born prematurely, the ages of the parents, whether the parents are married, how long (in weeks) the pregnancy lasted (this is called the “gestation”) and so on. We have seen these data before.\n\nRead in the data from the file into R, bearing in mind what type of file it is.\n\nSolution\nThis is a .csv file (it came from a spreadsheet), so it needs reading in accordingly. Work directly from the URL (rather than downloading the file):\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/ncbirths2.csv\"\nbw &lt;- read_csv(myurl)\n\nRows: 500 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): father_age, mother_age, weeks_gestation, pre_natal_visits, marital...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\\(\\blacksquare\\)\n\nFind a 95% confidence interval for the mean birth weight of all babies born in North Carolina (of which these babies are a sample). At the end, you should state what the confidence interval is. Giving some output is necessary, but not enough by itself.\n\nSolution\nThis:\n\nt.test(bw$weight_pounds)\n\n\n    One Sample t-test\n\ndata:  bw$weight_pounds\nt = 104.94, df = 499, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.936407 7.201093\nsample estimates:\nmean of x \n  7.06875 \n\n\nor (the same, but remember to match your brackets):\n\nwith(bw, t.test(weight_pounds))\n\n\n    One Sample t-test\n\ndata:  weight_pounds\nt = 104.94, df = 499, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.936407 7.201093\nsample estimates:\nmean of x \n  7.06875 \n\n\nThe confidence interval goes from 6.94 to 7.20 pounds.\nThere is an annoyance about t.test. Sometimes you can use data= with it, and sometimes not. When we do a two-sample \\(t\\)-test later, there is a “model formula” with a squiggle in it, and there we can use data=, but here not, so you have to use the dollar sign or the with to say which data frame to get things from. The distinction seems to be that if you are using a model formula, you can use data=, and if not, not.\nThis is one of those things that is a consequence of R’s history. The original t.test was without the model formula and thus without the data=, but the model formula got “retro-fitted” to it later. Since the model formula comes from things like regression, where data= is legit, that had to be retro-fitted as well. Or, at least, that’s my understanding.\n\\(\\blacksquare\\)\n\nBirth weights of babies born in the United States have a mean of 7.3 pounds. Is there any evidence that babies born in North Carolina are less heavy on average? State appropriate hypotheses, do your test, obtain a P-value and state your conclusion, in terms of the original data.\n\nSolution\nLet \\(\\mu\\) be the population mean (the mean weight of all babies born in North Carolina). Null hypothesis is \\(H_0: \\mu=7.3\\) pounds, and the alternative is that the mean is less: \\(H_a: \\mu&lt;7.3\\) pounds.\nNote that I defined \\(\\mu\\) first before I used it.\nThis is a one-sided alternative, which we need to feed into t.test:\n\nt.test(bw$weight_pounds, mu = 7.3, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  bw$weight_pounds\nt = -3.4331, df = 499, p-value = 0.0003232\nalternative hypothesis: true mean is less than 7.3\n95 percent confidence interval:\n     -Inf 7.179752\nsample estimates:\nmean of x \n  7.06875 \n\n\nOr with with. If you see what I mean.\nThe P-value is 0.0003, which is less than any \\(\\alpha\\) we might have chosen: we reject the null hypothesis in favour of the alternative, and thus we conclude that the mean birth weight of babies in North Carolina is indeed less than 7.3 pounds.\n“Reject the null hypothesis” is not a complete answer. You need to say something about what rejecting the null hypothesis means in this case: that is, you must make a statement about birth weights of babies.\n\\(\\blacksquare\\)\n\nThe theory behind the \\(t\\)-test says that the distribution of birth weights should be (approximately) normally distributed. Obtain a histogram of the birth weights. Does it look approximately normal? Comment briefly. (You’ll have to pick a number of bins for your histogram first. I don’t mind very much what you pick, as long as it’s not obviously too many or too few bins.)\n\nSolution\nWe did this before (and discussed the number of bins before), so I’ll just reproduce my 10-bin histogram (which is what I preferred, but this is a matter of taste):\n\nggplot(bw, aes(x = weight_pounds)) + geom_histogram(bins = 10)\n\n\n\n\nSo, we were assessing normality. What about that?\nIt is mostly normal-looking, but I am suspicious about those very low birth weights, the ones below about 4 pounds. There are too many of those, as I see it.\nIf you think this is approximately normal, you need to make some comment along the lines of “the shape is approximately symmetric with no outliers”. I think my first answer is better, but this answer is worth something, since it is a not completely unreasonable interpretation of the histogram.\nA normal quantile plot is better for assessing normality than a histogram is, but I won’t make you do one until we have seen the idea in class. Here’s the normal quantile plot for these data:\n\nggplot(bw, aes(sample = weight_pounds)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis is rather striking: the lowest birthweights (the ones below 5 pounds or so) are way too low for a normal distribution to apply. The top end is fine (except perhaps for that one very heavy baby), but there are too many low birthweights for a normal distribution to be believable. Note how much clearer this story is than on the histogram.\nHaving said that, the \\(t\\)-test, especially with a sample size as big as this (500), behaves very well when the data are somewhat non-normal (because it takes advantage of the Central Limit Theorem: that is, it’s the sampling distribution of the sample mean whose shape matters). So, even though the data are definitely not normal, I wouldn’t be too worried about our test.\nThis perhaps gives some insight as to why Freedman-Diaconis said we should use so many bins for our histogram. We have a lot of low-end outliers, so that the IQR is actually small compared to the overall spread of the data (as measured, say, by the SD or the range) and so FD thinks we need a lot of bins to describe the shape. Sturges is based on data being approximately normal, so it will tend to produce a small number of bins for data that have outliers.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#inferring-ice-break-up-in-nenana-1",
    "href": "one-sample-inference.html#inferring-ice-break-up-in-nenana-1",
    "title": "4  One-sample inference",
    "section": "4.10 Inferring ice break-up in Nenana",
    "text": "4.10 Inferring ice break-up in Nenana\nNenana, Alaska, is about 50 miles west of Fairbanks. Every spring, there is a contest in Nenana. A wooden tripod is placed on the frozen river, and people try to guess the exact minute when the ice melts enough for the tripod to fall through the ice. The contest started in 1917 as an amusement for railway workers, and has taken place every year since. Now, hundreds of thousands of people enter their guesses on the Internet and the prize for the winner can be as much as $300,000.\nBecause so much money is at stake, and because the exact same tripod is placed at the exact same spot on the ice every year, the data are consistent and accurate. The data are in link.\nYes, we saw these data before.\n\nRead the data into R, as before, or use the data frame that you read in before. Note that the values are separated by tabs rather than spaces, so you’ll need an appropriate read_ to read it in.\n\nSolution\nThese are “tab-separated values”, so read_tsv is the thing, as for the Australian athletes:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/nenana.txt\"\nnenana &lt;- read_tsv(myurl)\n\nRows: 87 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Date&Time\ndbl (2): Year, JulianDate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nUse whatever name you like for the data frame. One that is different from any of the column headers is smart; then it is clear whether you mean the whole data frame or one of its columns. ice or melt or anything like that would also be good.\n\\(\\blacksquare\\)\n\nObtain a 90% confidence interval for the mean JulianDate. What interval do you get? Looking back at your histogram, do you have any doubts about the validity of what you have just done?\n\nSolution\nThis is a matter of using t.test and pulling out the interval. Since we are looking for a non-standard interval, we have to remember conf.level as the way to get the confidence level that we want. I’m going with with this time, though the dollar-sign thing is equally as good:\n\nwith(nenana, t.test(JulianDate, conf.level = 0.90))\n\n\n    One Sample t-test\n\ndata:  JulianDate\nt = 197.41, df = 86, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 124.4869 126.6018\nsample estimates:\nmean of x \n 125.5443 \n\n\nBetween 124.5 and 126.6 days into the year. Converting that into something we can understand (because I want to), there are \\(31+28+31+30=120\\) days in January through April (in a non-leap year), so this says that the mean breakup date is between about May 4 and May 6.\nThe \\(t\\)-test is based on an assumption of data coming from a normal distribution. The histogram we made earlier looks pretty much normal, so there are no doubts about normality and thus no doubts about the validity of what we have done, on the evidence we have seen so far. (I have some doubts on different grounds, based on another of the plots we did earlier, which I’ll explain later, but all I’m expecting you to do is to look at the histogram and say “Yep, that’s normal enough”. Bear in mind that the sample size is 87, which is large enough for the Central Limit Theorem to be pretty helpful, so that we don’t need the data to be more than “approximately normal” for the sampling distribution of the sample mean to be very close to \\(t\\) with the right df.)\n\\(\\blacksquare\\)\n\nAn old-timer in Nenana strokes his grey beard and says “When I were young, I remember the tripod used to fall into the water around May 10”. In a non-leap year, May 10 is Julian day 130. Test the null hypothesis that the mean JulianDay is 130, against the alternative that it is less. What do you conclude? What practical implication does that have (assuming that the old-timer has a good memory)?\n\nSolution\nThe test is t.test again, but this time we have to specify a null mean and a direction of alternative:\n\nwith(nenana, t.test(JulianDate, mu = 130, alternative = \"less\"))\n\n\n    One Sample t-test\n\ndata:  JulianDate\nt = -7.0063, df = 86, p-value = 2.575e-10\nalternative hypothesis: true mean is less than 130\n95 percent confidence interval:\n     -Inf 126.6018\nsample estimates:\nmean of x \n 125.5443 \n\n\nFor a test, look first at the P-value, which is 0.0000000002575: that is to say, the P-value is very small, definitely smaller than 0.05 (or any other \\(\\alpha\\) you might have chosen). So we reject the null hypothesis, and conclude that the mean JulianDate is actually less than 130.\nNow, this is the date on which the ice breaks up on average, and we have concluded that it is earlier than it used to be, since we are assuming the old-timer’s memory is correct.\nThis is evidence in favour of global warming; a small piece of evidence, to be sure, but the ice is melting earlier than it used to all over the Arctic, so it’s not just in Nenana that it is happening. You don’t need to get to the “global warming” part, but I do want you to observe that the ice is breaking up earlier than it used to.\n\\(\\blacksquare\\)\n\nPlot JulianDate against Year on a scatterplot. What recent trends, if any, do you see? Comment briefly. (You did this before, but I have some extra comments on the graph this time, so feel free to just read this part.)\n\nSolution\nI liked the ggplot with a smooth trend on it:\n\nggplot(nenana, aes(x = Year, y = JulianDate)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThere was something obvious to see: after about 1960, there is a clear downward trend: the ice is breaking up earlier on average every year. Even though there is a lot of variability, the overall trend, viewed this way, is clear (and consistent with the test we did earlier). Note that the old-timer’s value of 130 is the kind of JulianDate we would typically observe around 1920, which would make the old-timer over 90 years old.\nAll right, why did I say I had some doubts earlier? Well, because of this downward trend, the mean is not actually the same all the way through, so it doesn’t make all that much sense to estimate it, which is what we were doing earlier by doing a confidence interval or a hypothesis test. What would actually make more sense is to estimate the mean JulianDate for a particular year. This could be done by a regression: predict JulianDate from Year, and then get a “confidence interval for the mean response” (as you would have seen in B27 or will see in C67). The trend isn’t really linear, but is not that far off. I can modify the previous picture to give you an idea. Putting in method=\"lm\" fits a line; as we see later, lm does regressions in R:\n\nggplot(nenana, aes(x = Year, y = JulianDate)) + geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCompare the confidence interval for the mean JulianDate in 1920: 126 to 131 (the shaded area on the graph), with 2000: 121 to 125. A change of about 5 days over 80 years. And with the recent trend that we saw above, it’s probably changing faster than that now. Sobering indeed.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#diameters-of-trees-1",
    "href": "one-sample-inference.html#diameters-of-trees-1",
    "title": "4  One-sample inference",
    "section": "4.11 Diameters of trees",
    "text": "4.11 Diameters of trees\nThe Wade Tract in Thomas County, Georgia, is an old-growth forest of longleaf pine trees. It has survived in a relatively undisturbed state since before settlements of the area by Europeans. For each tree in the tract, researchers measured the diameter at breast height. This is a standard measure in forestry: it is defined as the diameter of the tree at 4.5 feet above the ground.5 They are interested in the mean diameter at breast height of the trees in this tract. These values are in http://ritsokiguess.site/datafiles/treediameter.csv. The diameters are measured in centimetres. The easiest way to get the URL is to right-click on the blue text and select Copy URL. (If you copy and paste the actual text you might end up with extra spaces, especially if the printed URL goes over two lines.)\n\nRead in and display (some of) the data.\n\nSolution\nThe obvious way is this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/treediameter.csv\"\ntrees &lt;- read_csv(my_url)\n\nRows: 40 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): diameter\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntrees\n\n\n\n  \n\n\n\nCall the data frame what you like, though it is better to use a name that tells you what the dataframe contains (rather than something like mydata).\nExtra 1: there is only one column, so you can pretend the columns are separated by anything at all. Thus you could use this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/treediameter.csv\"\ntrees &lt;- read_delim(my_url, \" \")\n\nRows: 40 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (1): diameter\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntrees\n\n\n\n  \n\n\n\nor even this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/treediameter.csv\"\ntrees &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  diameter = col_double()\n)\n\ntrees\n\n\n\n  \n\n\n\nExtra 2: you might be wondering how they measure the diameter without doing something like drilling a hole through the tree. They don’t actually measure the diameter at all. What they measure is the circumference of the tree, which is easy enough to do with a tape measure. Longleaf pines are usually near circular, so you get the diameter by taking the circumference and dividing by \\(\\pi\\). This City of Portland website shows you how it’s done.\n\\(\\blacksquare\\)\n\nMake a suitable plot of your dataframe.\n\nSolution\nOne quantitative variable, so a histogram. Choose a sensible number of bins. There are 40 observations, so a number of bins up to about 10 is good. Sturges’ rule says 6 since \\(2^6=64\\):\n\nggplot(trees, aes(x=diameter)) + geom_histogram(bins=6)\n\n\n\n\nExtra 1: comments come later, but you might care to note (if only for yourself) that the distribution is a little skewed to the right, or, perhaps better, has no left tail at all. You might even observe that diameters cannot be less than 0 (they are measurements), and so you might expect a skew away from the limit.\nAfter you’ve looked at the \\(t\\) procedures for these data, we’ll get back to the shape.\nExtra 2: later we look at a more precise tool for assessing normality, the normal quantile plot, which looks like this:\n\nggplot(trees, aes(sample=diameter)) + stat_qq() + stat_qq_line()\n\n\n\n\nIf the data come from a normal distribution, the points should follow the straight line, at least approximately. Here, most of the points do, except for the points on the left, which veer away upwards from the line: that is, the highest values, on the right, are about right for a normal distribution, but the lowest values, on the left, don’t go down low enough.6 Thus, the problem with normality is not the long tail on the right, but the short one on the left. It is hard to get this kind of insight from the histogram, but at the moment, it’s the best we have.\nThe big problems, for things like \\(t\\)-tests that depend on means, is stuff like outliers, or long tails, with extreme values that might distort the mean. Having short tails, as the left tail here, will make the distribution look non-normal but won’t cause any problems for the \\(t\\)-tests.\n\\(\\blacksquare\\)\n\nObtain a 95% confidence interval for the mean diameter.\n\nSolution\nThis is t.test, but with conf.level to get the interval (and then you ignore the P-value):\n\nwith(trees, t.test(diameter))\n\n\n    One Sample t-test\n\ndata:  diameter\nt = 9.748, df = 39, p-value = 5.245e-12\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.6274 32.9526\nsample estimates:\nmean of x \n    27.29 \n\n\nThe mean diameter of a longleaf pine (like the ones in this tract) is between 21.6 and 33.0 centimetres.\nIf you prefer, do it this way:\n\nt.test(trees$diameter)\n\n\n    One Sample t-test\n\ndata:  trees$diameter\nt = 9.748, df = 39, p-value = 5.245e-12\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.6274 32.9526\nsample estimates:\nmean of x \n    27.29 \n\n\nYou need to state the answer and round it off suitably. The actual diameters in the data have one decimal place, so you can give the same accuracy for the CI, or at most two decimals (so 21.63 to 32.95 cm would also be OK).7 Giving an answer with more decimals is something you cannot possibly justify. Worse even than giving too many decimals is not writing out the interval at all. Never make your reader find something in output. If they want it, tell them what it is.\nThus, here (if this were being graded), one mark for the output, one more for saying what the interval is, and the third if you give the interval with a sensible number of decimals.\n\\(\\blacksquare\\)\n\nBased on what you have seen so far, would you expect to reject a null hypothesis that the population mean diameter (of all longleaf pines like these) is 35 cm? Explain briefly. Then, carry out the test (against a two-sided alternative) and explain briefly whether you were right.\n\nSolution\nThe logic is that “plausible” values for the population mean, ones you believe, are inside the interval, and implausible ones that you don’t believe are outside. Remember that the interval is your best answer to “what is the population mean”, and 35 is outside the interval so you don’t think the population mean is 35, and thus you would reject it.\nAre we right? Take out the conf.level and put in a mu:\n\nwith(trees, t.test(diameter, mu = 35))\n\n\n    One Sample t-test\n\ndata:  diameter\nt = -2.754, df = 39, p-value = 0.008895\nalternative hypothesis: true mean is not equal to 35\n95 percent confidence interval:\n 21.6274 32.9526\nsample estimates:\nmean of x \n    27.29 \n\n\nThe P-value is less than our \\(\\alpha\\) of 0.05, so we would indeed reject a mean of 35 cm (in favour of the mean being different from 35).\n\\(\\blacksquare\\)\n\nWould you expect 35 cm to be in a 99% confidence interval for the mean diameter? Explain briefly, and then see if you were right.\n\nSolution\nThe P-value is less than 0.01 (as well as being less than 0.05), so, in the same way that 35 was outside the 95% interval, it should be outside the 99% CI also. Maybe not by much, though, since the P-value is only just less than 0.01:\n\nwith(trees, t.test(diameter, conf.level = 0.99))\n\n\n    One Sample t-test\n\ndata:  diameter\nt = 9.748, df = 39, p-value = 5.245e-12\nalternative hypothesis: true mean is not equal to 0\n99 percent confidence interval:\n 19.70909 34.87091\nsample estimates:\nmean of x \n    27.29 \n\n\nIndeed so, outside, but only just.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#one-sample-cholesterol-1",
    "href": "one-sample-inference.html#one-sample-cholesterol-1",
    "title": "4  One-sample inference",
    "section": "4.12 One-sample cholesterol",
    "text": "4.12 One-sample cholesterol\nThe data set here contains cholesterol measurements for heart attack patients (at several different times) as well as for a group of control patients. We will focus on the control patients in this question.\n\nRead in and display (some of) the data.\n\nSolution\nThis is (as you might guess) a .csv, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/cholest.csv\"\ncholest &lt;- read_csv(my_url)\n\nRows: 30 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): 2-Day, 4-Day, 14-Day, control\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncholest\n\n\n\n  \n\n\n\nNote for yourself that there are 30 observations (and some missing ones), and a column called that is the one we’ll be working with.\nExtra: the 2-day, 4-day and 14-day columns need to be referred to with funny “backticks” around their names, because a column name cannot contain a - or start with a number. This is not a problem here, since we won’t be using those columns, but if we wanted to, this would not work:\n\ncholest %&gt;% summarize(xbar = mean(2-Day))\n\nError in `summarize()`:\nℹ In argument: `xbar = mean(2 - Day)`.\nCaused by error in `h()`:\n! error in evaluating the argument 'x' in selecting a method for function 'mean': object 'Day' not found\n\n\nbecause it is looking for a column called Day, which doesn’t exist. The meaning of 2-Day is “take the column called Day and subtract it from 2”. To make this work, we have to supply the backticks ourselves:\n\ncholest %&gt;% summarize(xbar = mean(`2-Day`, na.rm = TRUE))\n\n\n\n  \n\n\n\nThis column also has missing values (at the bottom), so here I’ve asked to remove the missing values8 before working out the mean. Otherwise the mean is, unhelpfully, missing as well.\nYou might imagine that dealing with column names like this would get annoying. There is a package called janitor that has a function called clean_names to save you the trouble. Install it first, then load it:\n\nlibrary(janitor)\n\nand then pipe your dataframe into clean_names and see what happens:\n\ncholest %&gt;% clean_names() -&gt; cholest1\ncholest1\n\n\n\n  \n\n\n\nThese are all legit column names; the - has been replaced by an underscore, and each of the first three column names has gained an x on the front so that it no longer starts with a number. This then works:\n\ncholest1 %&gt;% summarize(xbar = mean(x2_day, na.rm = TRUE))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake a suitable plot of the cholesterol levels of the control patients, and comment briefly on the shape of the distribution.\n\nSolution\nThere is one quantitative variable, so a histogram, as ever:\n\nggplot(cholest, aes(x=control)) + geom_histogram(bins=6)\n\n\n\n\nPick a number of bins that shows the shape reasonably well. Too many or too few won’t. (Sturges’ rule says 6, since there are 30 observations and \\(2^5=32\\).) Seven bins also works, but by the time you get to 8 bins or more, you are starting to lose a clear picture of the shape. Four bins is, likewise, about as low as you can go before getting too crude a picture.\nChoosing one of these numbers of bins will make it clear that the distribution is somewhat skewed to the right.\n\\(\\blacksquare\\)\n\nIt is recommended that people in good health, such as the Control patients here, keep their cholesterol level below 200. Is there evidence that the mean cholesterol level of the population of people of which the Control patients are a sample is less than 200? Show that you understand the process, and state your conclusion in the context of the data.\n\nSolution\nThe word “evidence” means to do a hypothesis test and get a P-value. Choose an \\(\\alpha\\) first, such as 0.05.\nTesting a mean implies a one-sample \\(t\\)-test. We are trying to prove that the mean is less than 200, so that’s our alternative: \\(H_a: \\mu &lt; 200\\), and therefore the null is that the mean is equal to 200: \\(H_0: \\mu = 200\\). (You might think it makes more logical sense to have \\(H_0: \\mu \\ge 200\\), which is also fine. As long as the null hypothesis has an equals in it in a logical place, you are good.)\n\nwith(cholest, t.test(control, mu=200, alternative = \"less\"))\n\n\n    One Sample t-test\n\ndata:  control\nt = -1.6866, df = 29, p-value = 0.05121\nalternative hypothesis: true mean is less than 200\n95 percent confidence interval:\n     -Inf 200.0512\nsample estimates:\nmean of x \n 193.1333 \n\n\nThis is also good:\n\nt.test(cholest$control, mu=200, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  cholest$control\nt = -1.6866, df = 29, p-value = 0.05121\nalternative hypothesis: true mean is less than 200\n95 percent confidence interval:\n     -Inf 200.0512\nsample estimates:\nmean of x \n 193.1333 \n\n\nI like the first version better because a lot of what we do later involves giving a data frame, and then working with things in that data frame. This is more like that.\nThis test is one-sided because we are looking for evidence of less; if the mean is actually more than 200, we don’t care about that. For a one-sided test, R requires you to say which side you are testing.\nThe P-value is not (quite) less than 0.05, so we cannot quite reject the null. Therefore, there is no evidence that the mean cholesterol level (of the people of which the control group are a sample) is less than 200. Or, this mean is not significantly less than 200. Or, we conclude that this mean is equal to 200. Or, we conclude that this mean could be 200. Any of those.\nIf you chose a different \\(\\alpha\\), draw the right conclusion for the \\(\\alpha\\) you chose. For example, with \\(\\alpha=0.10\\), we do have evidence that the mean is less than 200. Being consistent is more important than getting the same answer as me.\nWriting out all the steps correctly shows that you understand the process. Anything less doesn’t.\n\\(\\blacksquare\\)\n\nWhat values could the population mean cholesterol level take? You might need to get some more output to determine this.\n\nSolution\nThis is not quoting the sample mean, giving that as your answer, and then stopping. The sample mean should, we hope, be somewhere the population mean, but it is almost certainly not the same as the population mean, because there is variability due to random sampling. (This is perhaps the most important thing in all of Statistics: recognizing that variability exists and dealing with it.)\nWith that in mind, the question means to get a range of values that the population mean could be: that is to say, a confidence interval. The one that came out of the previous output is one-sided, to go with the one-sided test, but confidence intervals for us are two-sided, so we have to run the test again, but two-sided, to get it. To do that, take out the “alternative”, thus (you can also take out the null mean, since a confidence interval has no null hypothesis):\n\nwith(cholest, t.test(control))\n\n\n    One Sample t-test\n\ndata:  control\nt = 47.436, df = 29, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 184.8064 201.4603\nsample estimates:\nmean of x \n 193.1333 \n\n\nWith 95% confidence, the population mean cholesterol level is between 184.8 and 201.5.\nYou need to state the interval, and you also need to round off the decimal places to something sensible. This is because in your statistical life, you are providing results to someone else in a manner that they can read and understand. They do not have time to go searching in some output, or to fish through some excessive number of decimal places. If that’s what you give them, they will ask you to rewrite your report, wasting everybody’s time when you could have done it right the first time.\nHow many decimal places is a good number? Look back at your data. In this case, the cholesterol values are whole numbers (zero decimal places). A confidence interval is talking about a mean. In this case, we have a sample size of 30, which is between 10 and 100, so we can justify one extra decimal place beyond the data, here one decimal altogether, or two at the absolute outside. (Two is more justifiable if the sample size is bigger than 100.) See, for example, this, in particular the piece at the bottom.\n\\(\\blacksquare\\)\n\nExplain briefly why you would be reasonably happy to trust the \\(t\\) procedures in this question. (There are two points you need to make.)\n\nSolution\nThe first thing is to look back at the graph you made earlier. This was skewed to the right (“moderately” or “somewhat” or however you described it). This would seem to say that the \\(t\\) procedures were not very trustworthy, since the population distribution doesn’t look very normal in shape.\nHowever, the second thing is to look at the sample size. We have the central limit theorem, which says (for us) that the larger the sample is, the less the normality matters, when it comes to estimating the mean. Here, the sample size is 30, which, for the central limit theorem, is large enough to overcome moderate non-normality in the data.\nMy take, which I was trying to guide you towards, is that our non-normality was not too bad, and so our sample size is large enough to trust the \\(t\\) procedures we used.\nExtra 1: What matters is the tradeoff between sample size and the extent of the non-normality. If your data is less normal, you need a larger sample size to overcome it. Even a sample size of 500 might not be enough if your distribution is very skewed, or if you have extreme outliers.\nThe place \\(n=30\\) comes from is back from the days when we only ever used printed tables. In most textbooks, if you printed the \\(t\\)-table on one page in a decent-sized font, you’d get to about 29 df before running out of space. Then they would say “\\(\\infty\\) df” and put the normal-distribution \\(z\\) numbers in. If the df you needed was bigger than what you had in the table, you used this last line: that is, you called the sample “large”. Try it in your stats textbooks: I bet the df go up to 30, then you get a few more, then the \\(z\\) numbers.\nExtra 2: By now you are probably thinking that this is very subjective, and so it is. What actually matters is the shape of the thing called the sampling distribution of the sample mean. That is to say, what kind of sample means you might get in repeated samples from your population. The problem is that you don’t know what the population looks like.9 But we can fake it up, in a couple of ways: we can play what-if and pretend we know what the population looks like (to get some understanding for “populations like that”), or we can use a technique called the “bootstrap” that will tell us what kind of sample means we might get from the population that our sample came from (this seems like magic and, indeed, is).\nThe moral of the story is that the central limit theorem is more powerful than you think.\nTo illustrate my first idea, let’s pretend the population looks like this, with a flat top:\n\n\n\n\n\nOnly values between 0 and 1 are possible, and each of those is equally likely. Not very normal in shape. So let’s take some random samples of size three, not in any sense a large sample, from this “uniform” population, and see what kind of sample means we get. This technique is called simulation: rather than working out the answer by math, we’re letting the computer approximate the answer for us. Here’s one simulated sample:\n\nu &lt;- runif(3)\nu\n\n[1] 0.9475841 0.1245953 0.2277288\n\nmean(u)\n\n[1] 0.4333027\n\n\nand here’s the same thing 1000 times, including a histogram of the sample means:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(runif(3))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12)\n\n\n\n\nThis is our computer-generated assessment of what the sampling distribution of the sample mean looks like. Isn’t this looking like a normal distribution?\nLet’s take a moment to realize what this is saying. If the population looks like the flat-topped uniform distribution, the central limit theorem kicks in for a sample of size three, and thus if your population looks like this, \\(t\\) procedures will be perfectly good for \\(n=3\\) or bigger, even though the population isn’t normal.\nThus, when you’re thinking about whether to use a \\(t\\)-test or something else (that we’ll learn about later), the distribution shape matters, but so does the sample size.\nI should say a little about my code. I’m not expecting you to figure out details now (we see the ideas properly in simulating power of tests), but in words, one line at a time:\n\ngenerate 1000 (“many”) samples each of 3 observations from a uniform distribution\nfor each sample, work out the mean of it\nturn those sample means into a data frame with a column called value\nmake a histogram of those.\n\nNow, the central limit theorem doesn’t always work as nicely as this, but maybe a sample size of 30 is large enough to overcome the skewness that we had:\n\nggplot(cholest, aes(x=control)) + geom_histogram(bins=6)\n\n\n\n\nThat brings us to my second idea above.\nThe sample that we had is in some sense an “estimate of the population”. To think about the sampling distribution of the sample mean, we need more estimates of the population. How might we get those? The curious answer is to sample from the sample. This is the idea behind the bootstrap. (This is what Lecture 3c is about.) The name comes from the expression “pulling yourself up by your own bootstraps”, meaning “to begin an enterprise or recover from a setback without any outside help” (from here), something that should be difficult or impossible. How is it possible to understand a sampling distribution with only one sample?\nWe have to be a bit careful. Taking a sample from the sample would give us the original sample back. So, instead, we sample with replacement, so that each bootstrap sample is different:\n\nsort(cholest$control)\n\n [1] 160 162 164 166 170 176 178 178 182 182 182 182 182 184 186 188 196 198 198\n[20] 198 200 200 204 206 212 218 230 232 238 242\n\nsort(sample(cholest$control, replace=TRUE))\n\n [1] 164 166 166 166 166 176 178 178 182 182 182 182 182 188 198 198 198 200 200\n[20] 200 200 204 206 206 218 218 230 232 232 242\n\n\nA bootstrap sample contains repeats of the original data values, and misses some of the others. Here, the original data had values 160 and 162 that are missing in the bootstrap sample; the original data had one value 166, but the bootstrap sample has four! I sorted the data and the bootstrap sample to make this clearer; you will not need to sort. This is a perfectly good bootstrap sample:\n\nsample(cholest$control, replace = TRUE)\n\n [1] 242 232 198 160 242 182 182 182 198 162 212 198 242 204 242 242 170 198 182\n[20] 206 232 170 218 188 166 178 164 160 218 196\n\n\nSo now we know what to do: take lots of bootstrap samples, work out the mean of each, plot the means, and see how normal it looks. The only new idea here is the sampling with replacement:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(cholest$control, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12)\n\n\n\n\nThat looks pretty normal, not obviously skewed, and so the \\(t\\) procedures we used will be reliable enough.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "one-sample-inference.html#footnotes",
    "href": "one-sample-inference.html#footnotes",
    "title": "4  One-sample inference",
    "section": "",
    "text": "The height of a typical human breast off the ground. Men have a breast too, you know.↩︎\nThe normal quantile plot is rather interesting: it says that the uppermost values are approximately normal, but the smallest eight or so values are too bunched up to be normal. That is, normality fails not because of the long tail on the right, but the bunching on the left. Still right-skewed, though.↩︎\nIn some languages, a dot is used to concatenate bits of text, or as a way of calling a method on an object. But in R, a dot has no special meaning, and is used in function names like t.test. Or p.value.↩︎\nWhether you think it is or not may depend on how many bins you have on your histogram. With 5 bins it looks like an outlier, but with 6 it does not. Try it and see.↩︎\nThe height of a typical human breast off the ground. Men have a breast too, you know.↩︎\nThey cannot go down far enough, because they can’t go below zero.↩︎\nOne more decimal place than the data is the maximum you give in a CI.↩︎\nIn R, missing values are labelled NA, and rm is Unix/C shorthand for remove.↩︎\nIf you did, all your problems would be over.↩︎"
  },
  {
    "objectID": "two-sample-inference.html#children-and-electronic-devices",
    "href": "two-sample-inference.html#children-and-electronic-devices",
    "title": "5  Two-sample inference",
    "section": "5.1 Children and electronic devices",
    "text": "5.1 Children and electronic devices\nDo children (aged 8–17) spend more time on electronic devices now than they did 10 years ago? Samples of 15 children aged 8–17 were taken in each of two years, 1999 and 2009, and the children (with their parents’ help) were asked to keep a diary of the number of hours they spent using electronic devices on a certain day. The data are in the file http://ritsokiguess.site/datafiles/pluggedin.txt.\n\nRead in the data and verify that you have 30 rows of data from two different years.\nDraw side-by-side boxplots of the number of hours for each year. year is a numeric variable that we want to treat as a factor, so we need to make it into a factor.\nTest whether the mean number of hours has increased since 1999. Which test did R do?\nObtain a 99% confidence interval for the difference in means."
  },
  {
    "objectID": "two-sample-inference.html#parking-close-to-the-curb",
    "href": "two-sample-inference.html#parking-close-to-the-curb",
    "title": "5  Two-sample inference",
    "section": "5.2 Parking close to the curb",
    "text": "5.2 Parking close to the curb\nIn 2009, the Toronto Star commissioned a survey to address the issue of who is better at parking a car: men or women. The researchers recorded 93 drivers who were parallel-parking their car in downtown Toronto, and for each driver, recorded the distance between the car and the curb, in inches, when the driver was finished parking their car. The data are in an Excel spreadsheet, link. Click on the link. The data will probably download automatically. Check the folder on your computer where things get downloaded.1 If the spreadsheet is just displayed and not downloaded, save it somewhere on your computer.\n\nThere are two sheets in this spreadsheet workbook. They are of the same data in two different formats. Take a look at Sheet 1 and Sheet 2. Describe the format of the data in each case. Which will be the most suitable data layout, bearing in mind that one of the first things we do is to make side-by-side boxplots of parking distances for males and females? Explain briefly.\nRead your preferred sheet directly into R, without using a .csv file. (There is a clue in the lecture notes, in the section about reading in files.) If you get stuck, make a .csv file and read that in.\nObtain side-by-side boxplots of parking distances for males and females. Does one gender seem to be better at parking than the other? Explain briefly.\nExplain briefly why this is two independent samples rather than matched pairs.\nRun a suitable \\(t\\)-test for comparing parking distances for males and females. What do you conclude, in the context of the data?\nWhy might you have some doubts about the \\(t\\)-test that you just did? Explain briefly.\nThe Toronto Star in its report said that females are more accurate at parking their cars. Why do you think they concluded that, and do you think they were right to do so? Explain briefly."
  },
  {
    "objectID": "two-sample-inference.html#bell-peppers-and-too-much-water",
    "href": "two-sample-inference.html#bell-peppers-and-too-much-water",
    "title": "5  Two-sample inference",
    "section": "5.3 Bell peppers and too much water",
    "text": "5.3 Bell peppers and too much water\nA pathogen called Phytophthora capsici causes bell peppers to wilt and die. It is thought that too much water aids in the spread of the pathogen. Two fields are under study, labelled a and b. The first step in the research project is to compare the mean soil water content of the two fields. There is a suspicion that field a will have a higher water content than field b. The data are in the file link.\n\nRead the file in using read_csv, and list the resulting data frame.\nMake side-by-side boxplots of the water content values for the two fields. How do the fields seem to compare?\nDo a two-sample \\(t\\)-test to test whether there is evidence that the mean water content in field a is higher than that of field b. What do you conclude? Explain briefly. (You’ll need to figure out a way of doing a one-sided test, or how to adapt the results from a two-sided test.)\nIs the result of your test consistent with the boxplot, or not? Explain briefly."
  },
  {
    "objectID": "two-sample-inference.html#exercise-and-anxiety-and-bullying-mice",
    "href": "two-sample-inference.html#exercise-and-anxiety-and-bullying-mice",
    "title": "5  Two-sample inference",
    "section": "5.4 Exercise and anxiety and bullying mice",
    "text": "5.4 Exercise and anxiety and bullying mice\nDoes exercise help to reduce anxiety? To assess this, some researchers randomly assigned mice to either an enriched environment where there was an exercise wheel available, or a standard environment with no exercise options. After three weeks in the specified environment, for five minutes a day for two weeks, the mice were each exposed to a “mouse bully” — a mouse who was very strong, aggressive, and territorial. One measure of mouse anxiety is amount of time hiding in a dark compartment, with mice who are more anxious spending more time in darkness. The amount of time spent in darkness is recorded for each of the mice.\nThe data can be found at link.\n\nRead the data into R, and display your data frame. Count the number of mice in each group.\nDraw side-by-side boxplots of time spent in darkness for each group of mice.\nDo the boxplots support the hypothesis about exercise and anxiety? Explain briefly.\nCarry out a \\(t\\)-test for comparing the mean time spent in darkness for the mice in the two groups. Think carefully about the details of the \\(t\\)-test (and what you need evidence in favour of).\nWhat do you conclude, in terms of anxiety and exercise (at least for mice)? Explain briefly.\nDoes anything in the previous parts suggest any problems with the analysis you just did? Explain briefly."
  },
  {
    "objectID": "two-sample-inference.html#diet-and-growth-in-boys",
    "href": "two-sample-inference.html#diet-and-growth-in-boys",
    "title": "5  Two-sample inference",
    "section": "5.5 Diet and growth in boys",
    "text": "5.5 Diet and growth in boys\nA dietician is studying the effect of different diets on children’s growth. In part of the study, the dietician is investigating two religious sects, labelled a and b in our data set. Both sects are vegetarian; the difference between them is that people in Sect A only eat vegetables from below the ground, and Sect B only eats vegetables from above the ground. The height and weight of the boys2 are measured at regular intervals. The data in link are the heights of the boys at age 12.\n\nRead in the data and find out how many observations you have and which variables.\nObtain side-by-side boxplots of the heights for boys from each sect. Does it look as if the heights of the boys in each sect are different? Comment briefly.\nLooking at your boxplots, do you see any problems with doing a two-sample \\(t\\)-test? Explain briefly.\nRun a \\(t\\)-test to determine whether the mean heights differ significantly. What do you conclude? Explain briefly. (Run the \\(t\\)-test even if your previous work suggests that it is not the right thing to do.)"
  },
  {
    "objectID": "two-sample-inference.html#handspans-of-males-and-females",
    "href": "two-sample-inference.html#handspans-of-males-and-females",
    "title": "5  Two-sample inference",
    "section": "5.6 Handspans of males and females",
    "text": "5.6 Handspans of males and females\nTake your right hand, and stretch the fingers out as far as you can. The distance between the tip of your thumb and the tip of your little (pinky) finger is your handspan. The students in a Statistics class at Penn State measured their handspans and also whether they identified as male or female. The data are at http://ritsokiguess.site/datafiles/handspan.txt, with handspans measured in inches. Thinking of these as a random sample of all possible students, is it true that males have a larger mean handspan than females? This is what we will explore.\n\nRead in and display (some of) the data.\nMake a suitable graph of the two columns.\nRun a suitable two-sample \\(t\\)-test to address the question of interest. What do you conclude, in the context of the data?\nObtain a 90% confidence interval for the difference in mean handspan between males and females. Do you need to run any more code? Explain briefly.\nExplain briefly why you might have some concerns about the validity of the \\(t\\)-tests you ran in this question. Or, if you don’t have any concerns, explain briefly why that is."
  },
  {
    "objectID": "two-sample-inference.html#the-anchoring-effect-australia-vs-us",
    "href": "two-sample-inference.html#the-anchoring-effect-australia-vs-us",
    "title": "5  Two-sample inference",
    "section": "5.7 The anchoring effect: Australia vs US",
    "text": "5.7 The anchoring effect: Australia vs US\nTwo groups of students (in a class at a American university) were asked what they thought the population of Canada was. (The correct answer at the time was just over 30 million.) Some of the students, before having to answer this, were told that the population of the United States was about 270 million. The other students in the class were told that the population of Australia was about 18 million. The data are in http://ritsokiguess.site/datafiles/anchoring.csv. The first column contains the country whose population the student was told, and the second contains the student’s guess at the population of Canada.\nYou might wonder how being told the population of an unrelated country would have any impact on a student’s guess at the population of Canada. Psychology says it does: it’s called the anchoring effect, and the idea is that the number mentioned first acts as an “anchor”: a person’s guess will be closer to the anchor than it would have been otherwise. In this case, that would mean that the guesses for the students given the US as an anchor will be higher than for the students given Australia as an anchor. We are interested in seeing whether there is evidence for that here.\n\nRead in and display (some of) the data.\nDraw a suitable graph of these data.\nExplain briefly why a Welch \\(t\\)-test would be better than a pooled \\(t\\)-test in this case.\nRun a suitable Welch \\(t\\)-test and display the output.\nWhat do you conclude from your test, in the context of the data?\n\nMy solutions follow:"
  },
  {
    "objectID": "two-sample-inference.html#children-and-electronic-devices-1",
    "href": "two-sample-inference.html#children-and-electronic-devices-1",
    "title": "5  Two-sample inference",
    "section": "5.8 Children and electronic devices",
    "text": "5.8 Children and electronic devices\nDo children (aged 8–17) spend more time on electronic devices now than they did 10 years ago? Samples of 15 children aged 8–17 were taken in each of two years, 1999 and 2009, and the children (with their parents’ help) were asked to keep a diary of the number of hours they spent using electronic devices on a certain day. The data are in the file http://ritsokiguess.site/datafiles/pluggedin.txt.\n\nRead in the data and verify that you have 30 rows of data from two different years.\n\nSolution\nI see this:\n\nmyurl=\"http://ritsokiguess.site/datafiles/pluggedin.txt\"\nplugged=read_delim(myurl,\" \")\n\nRows: 30 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): year, hours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nplugged\n\n\n\n  \n\n\n\nI see only the first ten rows (with an indication that there are 20 more, so 30 altogether). In your notebook, it’ll look a bit different: again, you’ll see the first 10 rows, but you’ll see exactly how many rows and columns there are, and there will be buttons “Next” and “Previous” to see earlier and later rows, and a little right-arrow to see more columns to the right (to which is added a little left-arrow if there are previous columns to scroll back to). If you want to check for yourself that there are 30 rows, you can click Next a couple of times to get down to row 30, and then see that the Next button cannot be clicked again, and therefore that 30 rows is how many there are.\nOr, you can summarize the years by counting how many there are of each:\n\nplugged %&gt;% count(year)\n\n\n\n  \n\n\n\nor the more verbose form of the same thing:\n\nplugged %&gt;% group_by(year) %&gt;% summarize(rows=n())\n\n\n\n  \n\n\n\nAny of those says that it looks good. 30 rows, 1999 and 2009, 15 measurements for each.\n\\(\\blacksquare\\)\n\nDraw side-by-side boxplots of the number of hours for each year. year is a numeric variable that we want to treat as a factor, so we need to make it into a factor.\n\nSolution\n\nggplot(plugged,aes(x=factor(year),y=hours))+geom_boxplot()\n\n\n\n\nThe fct_inorder trick will also work, since the years are in the data in the order we want them to be displayed. (An option in case you have seen this.)\nThe median for 2009 is noticeably higher, and there is no skewness or outliers worth worrying about.\nThe measurements for the two years have a very similar spread, so there would be no problem running the pooled test here.\nYou might be bothered by the factor(year) on the \\(x\\)-axis. To get around that, you can define year-as-factor first, using mutate, then feed your new column into the boxplot. That goes like this. There is a wrinkle that I explain afterwards:\n\nplugged %&gt;% mutate(the_year=factor(year)) %&gt;%\nggplot(aes(x=the_year, y=hours))+geom_boxplot()\n\n\n\n\nYou could even redefine year to be the factor version of itself (if you don’t need the year-as-number anywhere else). The wrinkle I mentioned above is that in the ggplot you do not name the data frame first; the data frame used is the (nameless) data frame that came out of the previous step, not plugged but plugged with a new column the_year.\nNote how the \\(x\\)-axis now has the name of the new variable.\nIf you forget to make year into a factor, this happens:\n\nggplot(plugged, aes(x = year, y = hours)) + geom_boxplot()\n\nWarning: Continuous x aesthetic\nℹ did you forget `aes(group = ...)`?\n\n\n\n\n\nYou get one boxplot, for all the hours, without distinguishing by year, and a warning message that tries (and fails) to read our mind: yes, we have a continuous, quantitative x, but geom_boxplot doesn’t take a group. Or maybe it does. Try it and see:\n\nggplot(plugged,aes(x = year, y = hours, group = year)) + geom_boxplot()\n\n\n\n\nThe \\(x\\)-axis treats the year as a number, which looks a little odd, but adding a group correctly gets us two boxplots side by side, so this is also a good way to do it. So maybe the error message did read our mind after all.\n\\(\\blacksquare\\)\n\nTest whether the mean number of hours has increased since 1999. Which test did R do?\n\nSolution\nThe hard part to remember is how you specify a one-sided test in R; it’s alternative=\"less\" (rather than “greater”) because 1999 is “before” 2009:\n\nt.test(hours~year,data=plugged,alternative=\"less\")  \n\n\n    Welch Two Sample t-test\n\ndata:  hours by year\nt = -3.3323, df = 24.861, p-value = 0.001348\nalternative hypothesis: true difference in means between group 1999 and group 2009 is less than 0\n95 percent confidence interval:\n       -Inf -0.8121415\nsample estimates:\nmean in group 1999 mean in group 2009 \n          5.933333           7.600000 \n\n\nThe P-value is 0.0013. R does the Welch-Satterthwaite test by default (the unequal-variances one). Since we didn’t change that, that’s what we got. (The pooled test is below.)\nThis is the cleanest way to do it, because this version of t.test, with a “model formula” (the thing with the squiggle) allows a data= to say which data frame to get things from. The other ways, using (for example) with, also work:\n\nwith(plugged,t.test(hours~year,alternative=\"less\"))\n\n\n    Welch Two Sample t-test\n\ndata:  hours by year\nt = -3.3323, df = 24.861, p-value = 0.001348\nalternative hypothesis: true difference in means between group 1999 and group 2009 is less than 0\n95 percent confidence interval:\n       -Inf -0.8121415\nsample estimates:\nmean in group 1999 mean in group 2009 \n          5.933333           7.600000 \n\n\nThis also works, but is ugly:\n\nt.test(plugged$hours~plugged$year,alternative=\"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  plugged$hours by plugged$year\nt = -3.3323, df = 24.861, p-value = 0.001348\nalternative hypothesis: true difference in means between group 1999 and group 2009 is less than 0\n95 percent confidence interval:\n       -Inf -0.8121415\nsample estimates:\nmean in group 1999 mean in group 2009 \n          5.933333           7.600000 \n\n\nUgly because you’ve just typed the name of the data frame and the dollar sign twice for no reason. As a general principle, if you as a programmer are repeating yourself, you should stop and ask yourself how you can avoid the repeat.\nIf you want the pooled test in R, you have to ask for it:\n\nt.test(hours~year, alternative = \"less\", data = plugged, var.equal = TRUE)    \n\n\n    Two Sample t-test\n\ndata:  hours by year\nt = -3.3323, df = 28, p-value = 0.001216\nalternative hypothesis: true difference in means between group 1999 and group 2009 is less than 0\n95 percent confidence interval:\n       -Inf -0.8158312\nsample estimates:\nmean in group 1999 mean in group 2009 \n          5.933333           7.600000 \n\n\nAs is often the case, the P-values for the pooled and Welch-Satterthwaite tests are very similar, so from that point of view it doesn’t matter much which one you use. If you remember back to the boxplots, the number of hours had about the same spread for the two years, so if you used the pooled test instead of the Welch-Satterthwaite test, that would have been just fine.\nThere is a school of thought that says we should learn the Welch-Satterthwaite test and use that always. This is because W-S (i) works when the populations from which the groups are sampled have different SDs and (ii) is pretty good even when those SDs are the same.\nThe pooled test can go badly wrong if the groups have very different SDs. The story is this: if the larger sample is from the population with the larger SD, the probability of a type I error will be smaller than \\(\\alpha\\), and if the larger sample is from the population with the smaller SD, the probability of a type I error will be larger than \\(\\alpha\\). This is why you see S-W in STAB22. You see the pooled test in STAB57 because the logic of its derivation is so much clearer, not because it’s really the better test in practice. The theory says that if your data are normal in both groups with the same variance, then the pooled test is best, but it says nothing about the quality of the pooled test if any of that goes wrong. The usual approach to assessing things like this is via simulation, as we do for estimating power (later): generate some random data eg. from normal distributions with the same means, SDs 10 and 20 and sample sizes 15 and 30, run the pooled \\(t\\)-test, see if you reject, then repeat lots of times and see whether you reject about 5% of the time. Then do the same thing again with the sample sizes switched around. Or, do the same thing with Welch-Satterthwaite.\n\\(\\blacksquare\\)\n\nObtain a 99% confidence interval for the difference in means.\n\nSolution\nTake off the thing that made it one-sided, and put in a thing that gets the right CI:\n\nt.test(hours~year,data=plugged,conf.level=0.99)  \n\n\n    Welch Two Sample t-test\n\ndata:  hours by year\nt = -3.3323, df = 24.861, p-value = 0.002696\nalternative hypothesis: true difference in means between group 1999 and group 2009 is not equal to 0\n99 percent confidence interval:\n -3.0614628 -0.2718705\nsample estimates:\nmean in group 1999 mean in group 2009 \n          5.933333           7.600000 \n\n\n\\(-3.06\\) to \\(-0.27\\). The interval contains only negative values, which is consistent with our having rejected a null hypothesis of no difference in means.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#parking-close-to-the-curb-1",
    "href": "two-sample-inference.html#parking-close-to-the-curb-1",
    "title": "5  Two-sample inference",
    "section": "5.9 Parking close to the curb",
    "text": "5.9 Parking close to the curb\nIn 2009, the Toronto Star commissioned a survey to address the issue of who is better at parking a car: men or women. The researchers recorded 93 drivers who were parallel-parking their car in downtown Toronto, and for each driver, recorded the distance between the car and the curb, in inches, when the driver was finished parking their car. The data are in an Excel spreadsheet, link. Click on the link. The data will probably download automatically. Check the folder on your computer where things get downloaded.3 If the spreadsheet is just displayed and not downloaded, save it somewhere on your computer.\n\nThere are two sheets in this spreadsheet workbook. They are of the same data in two different formats. Take a look at Sheet 1 and Sheet 2. Describe the format of the data in each case. Which will be the most suitable data layout, bearing in mind that one of the first things we do is to make side-by-side boxplots of parking distances for males and females? Explain briefly.\n\nSolution\nThe data in Sheet 1 has one column of parking distances for males, and another for females. This is often how you see data of this sort laid out. Sheet 2 has one column of parking distances, all combined together, and a second column indicating the gender of the driver whose distance is in the first column. If you look back at the kind of data we’ve used to make side-by-side boxplots, it’s always been in the format of Sheet 2: one column containing all the values of the variable we’re interested in, with a second column indicating which group each observation belongs to (“group” here being “gender of driver”). So we need to use the data in Sheet 2, because the data in Sheet 1 are not easy to handle with R. The layout of Sheet 2 is the way R likes to do most things: so-called “long format” with a lot of rows and not many columns. This is true for descriptive stuff: side-by-side boxplots or histograms or means by group, as well as modelling such as (here) a two-sample \\(t\\)-test, or (in other circumstances, as with several groups) a one-way analysis of variance. Hadley Wickham, the guy behind the tidyverse, likes to talk about “tidy data” (like Sheet 2), with each column containing a variable, and “untidy data” (like Sheet 1), where the two columns are the same thing (distances), but under different circumstances (genders). As we’ll see later, it is possible to convert from one format to the other. Usually you want to make untidy data tidy (the function for this is called pivot_longer).\n\\(\\blacksquare\\)\n\nRead your preferred sheet directly into R, without using a .csv file. (There is a clue in the lecture notes, in the section about reading in files.) If you get stuck, make a .csv file and read that in.\n\nSolution\nThe direct way is to use the package readxl. This has a read_excel that works the same way as any of the other read_ functions. You’ll have to make sure that you read in sheet 2, since that’s the one you want. There is some setup first. There are a couple of ways you can do that:\n\nDownload the spreadsheet to your computer, and upload it to your project on R Studio Cloud (or, if you are running R Studio on your computer, use something like file.choose to get the file from wherever it got downloaded to).\nUse the function download.file to get the file from the URL and store it in your project folder directly. This also works in R Studio Cloud, and completely by-passes the download-upload steps that you would have to do otherwise. (I am grateful to Rose Gao for this idea.) Here is how you can use download.file here:\n\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/parking.xlsx\"\nlocal &lt;- \"parking.xlsx\"\ndownload.file(my_url, local, mode = \"wb\")\n\nWhen you’ve gotten the spreadsheet into your project folder via one of those two ways, you go ahead and do this:\n\nlibrary(readxl)\nparking &lt;- read_excel(\"parking.xlsx\", sheet = 2)\nparking\n\n\n\n  \n\n\n\nYou have to do it this way, using the version of the spreadsheet on your computer, since read_excel won’t take a URL, or if it does, I can’t make it work.4 I put the spreadsheet in R Studio’s current folder, so I could read it in by name, or you can do the f &lt;- file.choose() thing, find it, then read it in. The sheet= thing can take either a number (as here: the second sheet in the workbook), or a name (whatever name the sheet has on its tab in the workbook).\nExtra: Rose actually came up with a better idea, which I will show you and explain:\n\ntf &lt;- tempfile()\ndownload.file(my_url, tf, mode = \"wb\")\np &lt;- read_excel(tf, sheet = 2)\n\nWhat tempfile() does is to create a temporary file to hold the spreadsheet that you are about to download. After downloading the spreadsheet to the temporary file, you then use read_excel to read from the temporary file into the data frame.\nThe advantage of this approach is that the temporary file disappears as soon as you close R, and so you don’t have a copy of the spreadsheet lying around that you don’t need (once you have created the dataframe that I called parking, anyway).\nIf you are wondering about that mode thing on download.file: files are of two different types, “text” (like the text of an email, that you can open and look at in something like Notepad), and “binary” that you can’t look at directly, but for which you need special software like Word or Excel to decode it for you.5\nThe first character in mode is either w for “write a new file”, which is what we want here, or a for “append”, which would mean adding to the end of a file that already exists. Thus mode=\"wb\" means “create a new binary file”. End of Extra.\nIf you can’t make any of this work, then do it in two steps: save the appropriate sheet as a .csv file, and then read the .csv file using read_csv. If you experiment, you’ll find that saving a spreadsheet workbook as .csv only saves the sheet you’re looking at, so make sure you are looking at sheet 2 before you Save As .csv. I did that, and called my saved .csv parking2.csv (because it was from sheet 2, but you can use any name you like). Then I read this into R thus:\n\nparking2 &lt;- read_csv(\"parking2.csv\")\n\nRows: 93 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): gender\ndbl (1): distance\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nparking2\n\n\n\n  \n\n\n\nThe read-in data frame parking has 93 rows (\\(47+46=93\\) drivers) and two columns: the distance from the curb that the driver ended up at, and the gender of the driver. This is as the spreadsheet Sheet 2 was, and the first few distances match the ones in the spreadsheet.\nIf I were grading this, you’d get some credit for the .csv route, but I really wanted you to figure out how to read the Excel spreadsheet directly, so that’s what would be worth full marks.\nYou might want to check that you have some males and some females, and how many of each, which you could do this way:\n\nparking %&gt;% count(gender)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nObtain side-by-side boxplots of parking distances for males and females. Does one gender seem to be better at parking than the other? Explain briefly.\n\nSolution\nWith the right data set, this is a piece of cake:\n\nggplot(parking, aes(x = gender, y = distance)) + geom_boxplot()\n\n\n\n\nThe outcome variable is distance from the curb, so smaller should be better (more accurate parking). With that in mind, the median for females is a little smaller than for males (about 8.5 vs. about 10), so it seems that on average females are more accurate parkers than males are. The difference is small, however (and so you might be wondering at this point whether it’s a statistically significant difference — don’t worry, that’s coming up).\nBefore I leave this one, I want to show you something else: above-and-below histograms, as another way of comparing males and females (two or more groups, in general). First, we make a histogram of all the distances, without distinguishing by gender:\n\nggplot(parking, aes(x = distance)) + geom_histogram(bins = 8)\n\n\n\n\nThat big outlier is the very inaccurate male driver.\nNow, how do we get a separate histogram for each gender? In ggplot, separate plots for each “something” are called facets, and the way to get facets arranged as you want them is called facet_grid.6 Let me show you the code first, and then explain how it works:\n\nggplot(parking, aes(x = distance)) +\n  geom_histogram(bins = 7) +\n  facet_grid(gender ~ .)\n\n\n\n\nfacet_grid takes a “model formula” with a squiggle, with \\(y\\) on the left and \\(x\\) on the right. We want to compare our two histograms, one for males and one for females, and I think the best way to compare histograms is to have one on top of the other. Note that the same distance scale is used for both histograms, so that it is a fair comparison. The above-and-below is accomplished by having gender as the \\(y\\) in the arrangement of the facets, so it goes before the squiggle. We don’t have any \\(x\\) in the arrangement of the facets, and we tell ggplot this by putting a dot where the \\(x\\) would be.7\nYou can also use facet_wrap for this, but you have to be more careful since you don’t have any control over how the histograms come out (you probably get them side by side, which is not so helpful for comparing distributions). You can make it work by using ncol=1 to arrange all the histograms in one column:\n\nggplot(parking, aes(x = distance)) +\n  geom_histogram(bins = 7) +\n  facet_wrap(~gender, ncol = 1)\n\n\n\n\nThe centres of both histograms are somewhere around 10, so it’s hard to see any real difference between males and females here. Maybe this is further evidence that the small difference we saw between the boxplots is really not worth getting excited about.\nYou might be concerned about how you know what to put with the squiggle-thing in facet_grid and facet_wrap. The answer is that facet_wrap only has something to the right of the squiggle (which ggplot then decides how to arrange), but facet_grid must have something on both sides of the squiggle (how to arrange in the \\(y\\) direction on the left, how to arrange in the \\(x\\) direction on the right), and if you don’t have anything else to put there, you put a dot. Here’s my facet_grid code from above, again:\n\nggplot(parking, aes(x = distance)) +\n  geom_histogram(bins = 7) +\n  facet_grid(gender ~ .)\n\nWe wanted gender to go up and down, and we had nothing to go left and right, hence the dot. Contrast that with my facet_wrap code:8\n\nggplot(parking, aes(x = distance)) +\n  geom_histogram(bins = 7) +\n  facet_wrap(~gender)\n\nThis says “make a separate facet for each gender”, but it doesn’t say anything about how to arrange them. The choice of bins for my histogram(s) came from Sturges’ rule: with \\(n\\) being the number of observations, you use \\(k\\) bins where \\(k=\\log_2(n)+1\\), rounded up. If we were to make a histogram of all the parking distances combined together, we would have \\(n=47+48=95\\) observations, so we should use this many bins:\n\nsturges &lt;- log(95, 2) + 1\nsturges\n\n[1] 7.569856\n\n\nRound this up to 8. (The second thing in log is the base of the logs, if you specify it, otherwise it defaults to \\(e\\) and gives you “natural” logs.) I seem to have the powers of 2 in my head, so I can do it mentally by saying “the next power of 2 is 128, which is \\(2^7\\), so I need \\(7+1=8\\) bins.”\nOr:\n\nwith(parking, nclass.Sturges(distance))\n\n[1] 8\n\n\nSturges’ rule tends to produce not enough bins if \\(n\\) is small, so be prepared to increase it a bit if you don’t have much data. I think that gives a fairly bare-bones picture of the shape: skewed to the right with outlier.\nThe other rule we saw was Freedman-Diaconis:\n\nwith(parking, nclass.FD(distance))\n\n[1] 14\n\n\nand that leads to this histogram:\n\nggplot(parking, aes(x = distance)) + geom_histogram(bins = 14)\n\n\n\n\nThat gives rather more detail (a lot more bars: the binwidth in the Sturges-rule histogram is about 7, or twice what you see here), but in this case the overall story is about the same.\nIn the case of faceted histograms, you would want to apply a rule that uses the number of observations in each histogram. The facets might have quite different numbers of observations, but you can only use one binwidth (or bins), so you may have to compromise. For example, using Sturges’ rule based on 47 observations (the number of males; the number of females is one more):\n\nlog(47, 2) + 1\n\n[1] 6.554589\n\n\nand so each facet should have that many bins, rounded up. That’s where I got my 7 for the facetted histogram from. This one doesn’t work immediately with nclass.Sturges, because we do not have one column whose length is the number of observations we want: we have one column of distances that are males and females mixed up. To do that, filter one of the genders first:\n\nparking %&gt;%\n  filter(gender == \"female\") %&gt;%\n  with(., nclass.Sturges(distance))\n\n[1] 7\n\n\nI used the “dot” trick again, which you can read as “it”: “from parking, take only the rows for the females, and with it, give me the number of bins for a histogram by Sturges’ rule.”\n\\(\\blacksquare\\)\n\nExplain briefly why this is two independent samples rather than matched pairs.\n\nSolution\nThere is no way to pair any male with a corresponding female, because they are unrelated people. You might also notice that there are not even the same number of males and females, so there can be no way of pairing them up without leaving one over. (In general, if the two samples are paired, there must be the same number of observations in each; if there are different numbers in each, as here, they cannot be paired.) If you want that more mathematically, let \\(n_1\\) and \\(n_2\\) be the two sample sizes; then:\n\\[\n\\mbox{Paired} \\Longrightarrow n_1=n_2\n\\]\nfrom which it follows logically (the “contrapositive”) that\n\\[\nn_1 \\ne n_2 \\Longrightarrow \\mbox{not paired}\n\\]\nYou’ll note from the logic that if the two sample sizes are the same, that tells you nothing about whether it’s paired or independent samples: it could be either, and in that case you have to look at the description of the data to decide between them.\nHere, anything that gets at why the males and females cannot be paired up is good.\n\\(\\blacksquare\\)\n\nRun a suitable \\(t\\)-test for comparing parking distances for males and females. What do you conclude, in the context of the data?\n\nSolution\nA two-sample \\(t\\)-test. I think either the Welch or the pooled one can be justified (and I would expect them to give similar answers). You can do the Welch one either without comment or by asserting that the boxplots show different spreads; if you are going to do the pooled one, you need to say that the spreads are “about equal”, by comparing the heights of the boxes on the boxplots:\n\nt.test(distance ~ gender, data = parking)\n\n\n    Welch Two Sample t-test\n\ndata:  distance by gender\nt = -1.3238, df = 79.446, p-value = 0.1894\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -4.5884103  0.9228228\nsample estimates:\nmean in group female   mean in group male \n            9.308511            11.141304 \n\n\nThis is the Welch-Satterthwaite version of the test, the one that does not assume equal SDs in the two groups. The P-value of 0.1894 is not small, so there is no evidence of any difference in parking accuracy between males and females.\nOr, this being the pooled one:\n\nt.test(distance ~ gender, data = parking, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  distance by gender\nt = -1.329, df = 91, p-value = 0.1872\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -4.5722381  0.9066506\nsample estimates:\nmean in group female   mean in group male \n            9.308511            11.141304 \n\n\nYou might have thought, looking at the boxplots, that the groups had about the same SD (based, for example, on noting that the two boxes were about the same height, so the IQRs were about the same). In that case, you might run a pooled \\(t\\)-test, which here gives an almost identical P-value of 0.1872, and the exact same conclusion.\n\\(\\blacksquare\\)\n\nWhy might you have some doubts about the \\(t\\)-test that you just did? Explain briefly.\n\nSolution\nThe two-sample \\(t\\)-test is based on an assumption of normally-distributed data within each group. If you go back and look at the boxplots, you’ll see either (depending on your point of view) that both groups are right-skewed, or that both groups have outliers, neither of which fits a normal distribution. The outlier in the male group is particularly egregious.9 So I think we are entitled to question whether a two-sample \\(t\\)-test is the right thing to do. Having said that, we should go back and remember that the \\(t\\)-tests are “robust to departures from normality” (since we are working with the Central Limit Theorem here), and therefore that this test might be quite good even though the data are not normal, because the sample sizes of 40-plus are large (by the standards of what typically makes the Central Limit Theorem work for us). So it may not be as bad as it seems. A common competitor for the two-sample \\(t\\)-test is the Mann-Whitney test. This doesn’t assume normality, but it does assume symmetric distributions, which it’s not clear that we have here. I like a test called Mood’s Median Test, which is kind of the two-sample equivalent of the sign test (which we will also see later). It goes like this: Work out the overall median of all the distances, regardless of gender:\n\nparking %&gt;% summarize(med = median(distance))\n\n\n\n  \n\n\n\nThe overall median is 9.\nCount up how many distances of each gender were above or below the overall median. (Strictly, I’m supposed to throw away any values that are exactly equal to the overall median, but I won’t here for clarity of exposition.)\n\ntab &lt;- with(parking, table(gender, distance &lt; 9))\ntab\n\n        \ngender   FALSE TRUE\n  female    23   24\n  male      27   19\n\n\nFor example, 19 of the male drivers had a distance (strictly) less than 9. Both genders are pretty close to 50–50 above and below the overall median, which suggests that the males and females have about the same median. This can be tested (it’s a chi-squared test for independence, if you know that):\n\nchisq.test(tab, correct = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  tab\nX-squared = 0.89075, df = 1, p-value = 0.3453\n\n\nThis is even less significant (P-value 0.3453) than the two-sample \\(t\\)-test, and so is consistent with our conclusion from before that there is actually no difference between males and females in terms of average parking distance. The Mood’s median test is believable because it is not affected by outliers or distribution shape.\n\\(\\blacksquare\\)\n\nThe Toronto Star in its report said that females are more accurate at parking their cars. Why do you think they concluded that, and do you think they were right to do so? Explain briefly.\n\nSolution\nThe conclusion from the boxplots was that the female median distance was less than the males, slightly, in this sample. That is probably what the Star seized on. Were they right? Well, that was why we did the test of significance. We were trying to see whether this observed difference between males and females was “real” (would hold up if you looked at “all” male and female drivers) or “reproducible” (you would expect to see it again if you did another study like this one). The large, non-significant P-values in all our tests tell us that the difference observed here was nothing more than chance. So it was not reasonable to conclude that females generally are more accurate at parallel-parking than males are.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#bell-peppers-and-too-much-water-1",
    "href": "two-sample-inference.html#bell-peppers-and-too-much-water-1",
    "title": "5  Two-sample inference",
    "section": "5.10 Bell peppers and too much water",
    "text": "5.10 Bell peppers and too much water\nA pathogen called Phytophthora capsici causes bell peppers to wilt and die. It is thought that too much water aids in the spread of the pathogen. Two fields are under study, labelled a and b. The first step in the research project is to compare the mean soil water content of the two fields. There is a suspicion that field a will have a higher water content than field b. The data are in the file link.\n\nRead the file in using read_csv, and list the resulting data frame.\n\nSolution\nReading directly from the URL is easiest:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/bellpepper.csv\"\npepper &lt;- read_csv(my_url)\n\nRows: 30 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): field\ndbl (1): water\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npepper\n\n\n\n  \n\n\n\nIf you like, find out how many observations you have from each field, thus:\n\npepper %&gt;% count(field)\n\n\n\n  \n\n\n\nFourteen and sixteen.\n\\(\\blacksquare\\)\n\nMake side-by-side boxplots of the water content values for the two fields. How do the fields seem to compare?\n\nSolution\nThis kind of thing:\n\nggplot(pepper, aes(x = field, y = water)) + geom_boxplot()\n\n\n\n\nThis one is rather interesting: the distribution of water contents for field a is generally higher than that for field b, but the median for a is actually lower.\nThe other reasonable plot is a facetted histogram, something like this:\n\nggplot(pepper, aes(x = water)) + geom_histogram(bins = 6) +\n  facet_grid(field ~ .)\n\n\n\n\nThe distribution of water content in field b is actually bimodal, which is probably the explanation of the funny thing with the median. What actually seems to be happening (at least for these data) is that the water content in field B is either about the same as field A, or a lot less (nothing in between). I can borrow an idea from earlier to find the five-number summaries for each field:\n\npepper %&gt;%\n  nest(-field) %&gt;%\n  rowwise() %&gt;% \n  mutate(qq = list(enframe(quantile(data$water))))%&gt;%\n  unnest(qq) %&gt;%\n  select(-data) %&gt;% \n  pivot_wider(names_from=name, values_from=value)\n\nWarning: Supplying `...` without names was deprecated in tidyr 1.0.0.\nℹ Please specify a name for each selection.\nℹ Did you want `data = -field`?\n\n\n\n\n  \n\n\n\nThis is a weird one: all the quantiles are greater for field A except for the median.\n\\(\\blacksquare\\)\n\nDo a two-sample \\(t\\)-test to test whether there is evidence that the mean water content in field a is higher than that of field b. What do you conclude? Explain briefly. (You’ll need to figure out a way of doing a one-sided test, or how to adapt the results from a two-sided test.)\n\nSolution\n\nt.test(water ~ field, alternative = \"greater\", data = pepper)\n\n\n    Welch Two Sample t-test\n\ndata:  water by field\nt = 2.0059, df = 27.495, p-value = 0.0274\nalternative hypothesis: true difference in means between group a and group b is greater than 0\n95 percent confidence interval:\n 0.2664399       Inf\nsample estimates:\nmean in group a mean in group b \n       12.52857        10.76875 \n\n\nNote the use of alternative to specify that the first group mean (that of field a) is bigger than the second, field b, under the alternative hypothesis.\nThe P-value, 0.0274, is less than 0.05, so we reject the null (equal means) in favour of the a mean being bigger than the b mean: field a really does have a higher mean water content.\nAnother way to tackle this is to do a two-sided test and adapt the P-value:\n\nt.test(water ~ field, data = pepper)\n\n\n    Welch Two Sample t-test\n\ndata:  water by field\nt = 2.0059, df = 27.495, p-value = 0.0548\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.03878411  3.55842696\nsample estimates:\nmean in group a mean in group b \n       12.52857        10.76875 \n\n\nThis time we do not go straight to the P-value. First we check that we are on the correct side, which we are since the sample mean for field a is bigger than for field b. Then we are entitled to take the two-sided P-value 0.0548 and halve it to get the same 0.0274 that we did before.\n\\(\\blacksquare\\)\n\nIs the result of your test consistent with the boxplot, or not? Explain briefly.\n\nSolution\nThe test said that field a had a greater mean water content. Looking at the boxplot, this is consistent with where the boxes sit (a’s box is higher up than b’s). However, it is not consistent with the medians, where b’s median is actually bigger. You have two possible right answers here: comparing the boxes with the test result (they agree) or comparing the medians with the test result (they disagree). Either is good. If you like, you could also take the angle that the two boxes overlap a fair bit, so it is surprising that the test came out significant. (The resolution of this one is that we have 30 measurements altogether, 14 and 16 in the two groups, so the sample size is not tiny. With smaller samples, having overlapping boxes would probably lead to a non-significant difference.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#exercise-and-anxiety-and-bullying-mice-1",
    "href": "two-sample-inference.html#exercise-and-anxiety-and-bullying-mice-1",
    "title": "5  Two-sample inference",
    "section": "5.11 Exercise and anxiety and bullying mice",
    "text": "5.11 Exercise and anxiety and bullying mice\nDoes exercise help to reduce anxiety? To assess this, some researchers randomly assigned mice to either an enriched environment where there was an exercise wheel available, or a standard environment with no exercise options. After three weeks in the specified environment, for five minutes a day for two weeks, the mice were each exposed to a “mouse bully” — a mouse who was very strong, aggressive, and territorial. One measure of mouse anxiety is amount of time hiding in a dark compartment, with mice who are more anxious spending more time in darkness. The amount of time spent in darkness is recorded for each of the mice.\nThe data can be found at link.\n\nRead the data into R, and display your data frame. Count the number of mice in each group.\n\nSolution\nThese are aligned columns with spaces in between, so we need read_table:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/stressedmice.txt\"\nmice &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Time = col_double(),\n  Environment = col_character()\n)\n\nmice\n\n\n\n  \n\n\n\nYou can call the data frame whatever you like.\nIf you must, you can physically count the number of mice in each group, but you ought to get in the habit of coding this kind of thing:\n\nmice %&gt;% count(Environment)\n\n\n\n  \n\n\n\nSeven in each.\n\\(\\blacksquare\\)\n\nDraw side-by-side boxplots of time spent in darkness for each group of mice.\n\nSolution\nThis:\n\nggplot(mice, aes(x = Environment, y = Time)) + geom_boxplot()\n\n\n\n\nYou did remember to put capital letters on the variable names, didn’t you?\n\\(\\blacksquare\\)\n\nDo the boxplots support the hypothesis about exercise and anxiety? Explain briefly.\n\nSolution\nThe hypothesis about exercise and anxiety is that mice who exercise more should be less anxious. How does that play out in this study? Well, mice in the enriched environment at least have the opportunity to exercise, which the mice in the standard environment do not, and anxiety is measured by the amount of time spent in darkness (more equals more anxious). So we’d expect the mice in the standard environment to spend more time in darkness, if that hypothesis is correct. That’s exactly what the boxplots show, with very little doubt.10 Your answer needs to make two points: (i) what you would expect to see, if the hypothesis about anxiety and exercise is true, and (ii) whether you actually did see it. You can do this either way around: for example, you can say what you see in the boxplot, and then make the case that this does support the idea of more exercise corresponding with less anxiety.\n\\(\\blacksquare\\)\n\nCarry out a \\(t\\)-test for comparing the mean time spent in darkness for the mice in the two groups. Think carefully about the details of the \\(t\\)-test (and what you need evidence in favour of).\n\nSolution\nWe are trying to prove that exercise goes with less anxiety, so a one-sided test is called for. The other thing to think about is how R organizes the groups for Environment: in alphabetical order. Thus Enriched is first (like on the boxplot). We’re trying to prove that the mean Time is less for Enriched than for Standard, so we need alternative=\"less\":\n\nwith(mice, t.test(Time ~ Environment, alternative = \"less\"))\n\n\n    Welch Two Sample t-test\n\ndata:  Time by Environment\nt = -6.7966, df = 9.1146, p-value = 3.734e-05\nalternative hypothesis: true difference in means between group Enriched and group Standard is less than 0\n95 percent confidence interval:\n      -Inf -151.2498\nsample estimates:\nmean in group Enriched mean in group Standard \n              231.7143               438.7143 \n\n\nA common clue that you have the wrong alternative hypothesis is a P-value coming out close to 1, which is what you would have gotten from something like this:\n\nwith(mice, t.test(Time ~ Environment, alternative = \"greater\"))\n\n\n    Welch Two Sample t-test\n\ndata:  Time by Environment\nt = -6.7966, df = 9.1146, p-value = 1\nalternative hypothesis: true difference in means between group Enriched and group Standard is greater than 0\n95 percent confidence interval:\n -262.7502       Inf\nsample estimates:\nmean in group Enriched mean in group Standard \n              231.7143               438.7143 \n\n\nHere, we looked at the pictures and expected to find a difference, so we expected to find a P-value close to 0 rather than close to 1.\n\\(\\blacksquare\\)\n\nWhat do you conclude, in terms of anxiety and exercise (at least for mice)? Explain briefly.\n\nSolution\nThe P-value (from the previous part) is 0.000037, which is way less than 0.05 (or 0.01 or whatever \\(\\alpha\\) you chose). So the null hypothesis (equal means) is resoundingly rejected in favour of the one-sided alternative that the mean anxiety (as measured by time spent in darkness) is less for the mice who (can) exercise. You need to end up by doing a one-sided test. An alternative to what I did is to do a two-sided test in the previous part. Then you can fix it up by recognizing that the means are the right way around for the research hypothesis (the mean time in darkness is way less for Enriched), and then dividing the two-sided P-value by 2. But you need to do the “correct side” thing: just halving the two-sided P-value is not enough, because the sample mean for Enriched might have been more than for Standard.\n\\(\\blacksquare\\)\n\nDoes anything in the previous parts suggest any problems with the analysis you just did? Explain briefly.\n\nSolution\nLook at the side-by-side boxplots. The strict assumptions hiding behind the \\(t\\)-tests are that the data in each group come from normal distributions (equal standard deviations are not required). Are the data symmetric? Are there any outliers? Well, I see a high outlier in the Enriched group, so I have some doubts about the normality. On the other hand, I only have seven observations in each group, so there is no guarantee even if the populations from which they come are normal that the samples will be. So maybe things are not so bad. This is one of those situations where you make a case and defend it. I don’t mind so much which case you make, as long as you can defend it. Thus, something like either of these two is good:\n\nI see an outlier in the Enriched group. The data within each group are supposed to be normally distributed, and the Enriched group is not. So I see a problem.\nI see an outlier in the Enriched group. But the sample sizes are small, and an apparent outlier could arise by chance. So I do not see a problem.\n\nExtra: another way to think about this is normal quantile plots to assess normality within each group. This uses the facetting trick to get a separate normal quantile plot for each Environment:\n\nggplot(mice, aes(sample = Time)) + stat_qq() + stat_qq_line() +\n  facet_wrap(~Environment, scales = \"free\")\n\n\n\n\nFor the Enriched group, the upper-end outlier shows up. In a way this plot is no more illuminating than the boxplot, because you still have to make a call about whether this is “too big”. Bear in mind also that these facetted normal quantile plots, with two groups, come out tall and skinny, so vertical deviations from the line are exaggerated. On this plot, the lowest value also looks too low.\nFor the Standard group, there are no problems with normality at all.\nWhat happens if we change the shape of the plots?\n\nggplot(mice, aes(sample = Time)) + stat_qq() + stat_qq_line() +\n  facet_wrap(~Environment, scales = \"free\", ncol = 1)\n\n\n\n\nThis makes the plots come out in one column, that is, short and squat. I prefer these. I’d still call the highest value in Enriched an outlier, but the lowest value now looks pretty close to what you’d expect.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#diet-and-growth-in-boys-1",
    "href": "two-sample-inference.html#diet-and-growth-in-boys-1",
    "title": "5  Two-sample inference",
    "section": "5.12 Diet and growth in boys",
    "text": "5.12 Diet and growth in boys\nA dietician is studying the effect of different diets on children’s growth. In part of the study, the dietician is investigating two religious sects, labelled a and b in our data set. Both sects are vegetarian; the difference between them is that people in Sect A only eat vegetables from below the ground, and Sect B only eats vegetables from above the ground. The height and weight of the boys11 are measured at regular intervals. The data in link are the heights of the boys at age 12.\n\nRead in the data and find out how many observations you have and which variables.\n\nSolution\nThe data values are separated by one space, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/kids-diet.txt\"\ndiet &lt;- read_delim(my_url, \" \")\n\nRows: 21 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): sect\ndbl (1): height\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndiet\n\n\n\n  \n\n\n\n21 observations on two variables, sect and height. (You should state this; it is not enough to make the reader figure it out for themselves.)\nThe heights are evidently in centimetres.\nYou can call the data frame whatever you like.\n\\(\\blacksquare\\)\n\nObtain side-by-side boxplots of the heights for boys from each sect. Does it look as if the heights of the boys in each sect are different? Comment briefly.\n\nSolution\nThe boxplot is the kind of thing we’ve seen before:\n\nggplot(diet, aes(x = sect, y = height)) + geom_boxplot()\n\n\n\n\nIt looks to me as if the boys in Sect B are taller on average.\n\\(\\blacksquare\\)\n\nLooking at your boxplots, do you see any problems with doing a two-sample \\(t\\)-test? Explain briefly.\n\nSolution\nThe assumption is that the data in each group are “approximately normal”. Boxplots don’t tell you about normality specifically, but they tell you whether there are any outliers (none here) and something about the shape (via the lengths of the whiskers). I’d say the Sect A values are as symmetric as we could hope for. For Sect B, you can say either that they’re skewed to the left (and that therefore we have a problem), or that the heights are close enough to symmetric (and that therefore we don’t). For me, either is good. As ever, normal quantile plots can offer more insight. With data in this form, the two samples are mixed up, but using facets is the way to go. Philosophically, we draw a normal quantile plot of all the heights, and then say at the end that we would actually like a separate plot for each sect:\n\ndiet %&gt;%\n  ggplot(aes(sample = height)) + stat_qq() + stat_qq_line() +\n  facet_wrap(~sect, ncol = 1)\n\n\n\n\nI decided that I wanted short squat plots rather than tall skinny ones.\nWith the sizes of the samples, I really don’t see any problems here. Most of the evidence for the left skewness in Sect B is actually coming from that largest value being too small. Sect A is as good as you could wish for. Having extreme values being not extreme enough is not a problem, since it won’t be distorting the mean.\nThe other way of doing this is to use filter to pull out the rows you want and then feed that into the plot:\n\nsecta &lt;- filter(diet, sect == \"a\") %&gt;%\n  ggplot(aes(sample = sect)) + stat_qq() + stat_qq_line()\n\nand the same for sect B. This is the usual ggplot-in-pipeline thing where you don’t have a named data frame in the ggplot because it will use whatever came out of the previous step of the pipeline.\n\\(\\blacksquare\\)\n\nRun a \\(t\\)-test to determine whether the mean heights differ significantly. What do you conclude? Explain briefly. (Run the \\(t\\)-test even if your previous work suggests that it is not the right thing to do.)\n\nSolution\nThe wording states that a two-sided test is correct, which is the default, so you don’t need anything special:\n\nt.test(height ~ sect, data = diet)\n\n\n    Welch Two Sample t-test\n\ndata:  height by sect\nt = -1.7393, df = 14.629, p-value = 0.103\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -12.007505   1.229728\nsample estimates:\nmean in group a mean in group b \n       144.8333        150.2222 \n\n\nThis is a two-sample test, so it takes a data=.\nOur null hypothesis is that the two sects have equal mean height. The P-value of 0.103 is larger than 0.05, so we do not reject that null hypothesis. That is, there is no evidence that the sects differ in mean height. (That is, our earlier thought that the boys in Sect B were taller is explainable by chance.)\nYou must end up with a statement about mean heights, and when you do a test, you must state the conclusion in the context of the problem, whether I ask you to or not. “Don’t reject the null hypothesis” is a step on the way to an answer, not an answer in itself. If you think it’s an answer in itself, you won’t be of much use to the world as a statistician.\nYou might have been thinking that Mood’s median test was the thing, if you were worried about that skewness in Sect B. My guess is that the \\(t\\)-test is all right, so it will be the better test (and give the smaller P-value) here, but if you want to do it, you could do it this way:\n\nlibrary(smmr)\nmedian_test(diet, height, sect)\n\n$table\n     above\ngroup above below\n    a     4     7\n    b     6     3\n\n$test\n       what     value\n1 statistic 1.8181818\n2        df 1.0000000\n3   P-value 0.1775299\n\n\nMy suspicion (that I wrote before doing the test) is correct: there is even less evidence of a difference in median height between the sects. The table shows that both sects are pretty close to 50–50 above and below the overall median, and with sample sizes this small, they are certainly not significantly different from an even split. The small frequencies bring a warning about the chi-squared approximation possibly not working (that smmr suppresses). We had one like this elsewhere, but there the result was very significant, and this one is very non-significant. However, the implication is the same: even if the P-value is not very accurate (because the expected frequencies for sect B are both 4.5), the conclusion is unlikely to be wrong because the P-value is so far from 0.05.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#handspans-of-males-and-females-1",
    "href": "two-sample-inference.html#handspans-of-males-and-females-1",
    "title": "5  Two-sample inference",
    "section": "5.13 Handspans of males and females",
    "text": "5.13 Handspans of males and females\nTake your right hand, and stretch the fingers out as far as you can. The distance between the tip of your thumb and the tip of your little (pinky) finger is your handspan. The students in a Statistics class at Penn State measured their handspans and also whether they identified as male or female. The data are at http://ritsokiguess.site/datafiles/handspan.txt, with handspans measured in inches. Thinking of these as a random sample of all possible students, is it true that males have a larger mean handspan than females? This is what we will explore.\n\nRead in and display (some of) the data.\n\nSolution\nThis is a delimited (by spaces) file, so:\n\nlibrary(tidyverse)\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/handspan.txt\"\nspan &lt;- read_delim(my_url, \" \")\n\nRows: 190 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): sex\ndbl (1): handspan\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspan\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake a suitable graph of the two columns.\n\nSolution\nOne quantitative variable and one categorical one, so a boxplot:\n\nggplot(span, aes(x=sex, y=handspan)) + geom_boxplot()\n\n\n\n\n\\(\\blacksquare\\)\n\nRun a suitable two-sample \\(t\\)-test to address the question of interest. What do you conclude, in the context of the data?\n\nSolution\nWe are trying to show that males have a larger mean handspan, so we need an alternative. To see which: there are two sexes, F and M in that order, and we are trying to show that F is less than M:\n\nt.test(handspan~sex, data=span, alternative=\"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  handspan by sex\nt = -10.871, df = 187.92, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group F and group M is less than 0\n95 percent confidence interval:\n      -Inf -2.154173\nsample estimates:\nmean in group F mean in group M \n       20.01699        22.55747 \n\n\nThe P-value is very small, so there is no doubt that males have larger average handspans than females.\n\\(\\blacksquare\\)\n\nObtain a 90% confidence interval for the difference in mean handspan between males and females. Do you need to run any more code? Explain briefly.\n\nSolution\nA confidence interval is two-sided, so we have to re-run the test without the to make it two-sided. Note also that we need a 90% interval, which is different from the default 95%, so we have to ask for that too:\n\nt.test(handspan~sex, data=span, conf.level=0.90)\n\n\n    Welch Two Sample t-test\n\ndata:  handspan by sex\nt = -10.871, df = 187.92, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n90 percent confidence interval:\n -2.926789 -2.154173\nsample estimates:\nmean in group F mean in group M \n       20.01699        22.55747 \n\n\nThe interval is \\(-2.93\\) to \\(-2.15\\), which you should say. It would be even better to say that males have a mean handspan between 2.15 and 2.93 inches larger than that of females. You also need to round off your answer: the data are given to 0 or 1 decimals, so your interval should be given to 1 or 2 decimals (since the confidence interval is for a mean).\nOn a question like this, the grader is looking for three things:\n\ngetting the output\nsaying what the interval is\nrounding it to a suitable number of decimals.\n\nThus, getting the output alone is only one out of three things.\n\\(\\blacksquare\\)\n\nExplain briefly why you might have some concerns about the validity of the \\(t\\)-tests you ran in this question. Or, if you don’t have any concerns, explain briefly why that is.\n\nSolution\nThe major assumption here is that the male and female handspans have (approximate) normal distributions. The boxplots we drew earlier both had low-end outliers, so the normality is questionable.\nAlso, say something about the sample sizes and whether or not you think they are large enough to be helpful.\nHow big are our sample sizes?\n\nspan %&gt;% count(sex)\n\n\n\n  \n\n\n\nMy suspicion is that we are saved by two things: the sample sizes are large enough for the central limit theorem to help us, and in any case, the conclusion is so clear that the assumptions can afford to be off by a bit.\nExtra: one way to think about whether we should be concerned about the lack of normality is to use the bootstrap to see what the sampling distribution of the sample mean might look like for males and for females. (This is the stuff in Lecture 5a.) The way this works is to sample from each distribution with replacement, work out the mean of each sample, then repeat many times, once for the females and once for the males.\nTo start with the females, the first thing to do is to grab only the rows containing the females. This, using an idea from Lecture 5a that we see again properly later, is filter:\n\nspan %&gt;% filter(sex==\"F\") -&gt; females\nfemales\n\n\n\n  \n\n\n\nThere are 103 females. From these we need to take a “large” number of bootstrap samples to get a sense of how the mean handspan of the females varies:\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(the_sample = list(sample(females$handspan, replace = TRUE))) %&gt;% \n  mutate(the_mean = mean(the_sample)) -&gt; d\n\nThen we make a histogram of the bootstrap sampling distribution of the sample mean for the females:\n\nggplot(d, aes(x = the_mean)) + geom_histogram(bins = 10)\n\n\n\n\nI don’t know what you think of this. There are a few more extreme values than I would like, and it looks otherwise a bit left-skewed to me. But maybe I am worrying too much.\nThe males one works exactly the same way:\n\nspan %&gt;% filter(sex==\"M\") -&gt; males\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(the_sample = list(sample(males$handspan, replace = TRUE))) %&gt;% \n  mutate(the_mean = mean(the_sample)) -&gt; d\n\n\nggplot(d, aes(x = the_mean)) + geom_histogram(bins = 10)\n\n\n\n\nThere is a similar story here. I think these are good enough overall, and so I am happy with the two-sample \\(t\\)-test, but it is not as clear-cut as I was expecting.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#the-anchoring-effect-australia-vs-us-1",
    "href": "two-sample-inference.html#the-anchoring-effect-australia-vs-us-1",
    "title": "5  Two-sample inference",
    "section": "5.14 The anchoring effect: Australia vs US",
    "text": "5.14 The anchoring effect: Australia vs US\nTwo groups of students (in a class at a American university) were asked what they thought the population of Canada was. (The correct answer at the time was just over 30 million.) Some of the students, before having to answer this, were told that the population of the United States was about 270 million. The other students in the class were told that the population of Australia was about 18 million. The data are in http://ritsokiguess.site/datafiles/anchoring.csv. The first column contains the country whose population the student was told, and the second contains the student’s guess at the population of Canada.\nYou might wonder how being told the population of an unrelated country would have any impact on a student’s guess at the population of Canada. Psychology says it does: it’s called the anchoring effect, and the idea is that the number mentioned first acts as an “anchor”: a person’s guess will be closer to the anchor than it would have been otherwise. In this case, that would mean that the guesses for the students given the US as an anchor will be higher than for the students given Australia as an anchor. We are interested in seeing whether there is evidence for that here.\n\nRead in and display (some of) the data.\n\nSolution\nI made it as easy as I could:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/anchoring.csv\"\ncanada &lt;- read_csv(my_url)\n\nRows: 21 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): anchor\ndbl (1): estimate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncanada\n\n\n\n  \n\n\n\nYou might need to scroll down to see that both “anchor” countries are indeed represented.\n\\(\\blacksquare\\)\n\nDraw a suitable graph of these data.\n\nSolution\nOne categorical variable and one quantitative one, so a boxplot:\n\nggplot(canada, aes(x = anchor, y = estimate)) + geom_boxplot()\n\n\n\n\n\\(\\blacksquare\\)\n\nExplain briefly why a Welch \\(t\\)-test would be better than a pooled \\(t\\)-test in this case.\n\nSolution\nThe decision between these two tests lies in whether you think the two groups have equal spread (variance, strictly). Here, the spread for the US group is much larger than for the Australia group, even taking into account the big outlier in the latter group. Since the spreads are different, we should do a Welch \\(t\\)-test rather than a pooled one.\nMake sure you answer the question I asked, not the one you think I should have asked.\nThere is a separate question about whether the groups are close enough to normal, but I wasn’t asking about that here. I was asking: given that we have decided to do some kind of \\(t\\)-test, why is the Welch one better than the pooled one? I am not asking whether we should be doing any kind of \\(t\\)-test at all; if I had, you could then reasonably talk about the outlier in the Australia group, and other possible skewness in its distribution, but that’s not what I asked about.\n\\(\\blacksquare\\)\n\nRun a suitable Welch \\(t\\)-test and display the output.\n\nSolution\nThe word “suitable” is a hint that you may have to think a bit about how you run the test. If the anchoring effect is real, the mean of the guesses for the students told the population of the US will be higher on average than for those told the population of Australia, so we want a one-sided alternative. Australia is before the US alphabetically, so the alternative has to be less:\n\nt.test(estimate~anchor, data = canada, alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  estimate by anchor\nt = -3.0261, df = 10.558, p-value = 0.006019\nalternative hypothesis: true difference in means between group australia and group US is less than 0\n95 percent confidence interval:\n      -Inf -26.63839\nsample estimates:\nmean in group australia        mean in group US \n               22.45455                88.35000 \n\n\nNote that the Welch test is the default, so you don’t have to do anything special to get it. Your output will tell you that a Welch test is what you have. It’s if you want a pooled test that you have to ask for it specifically (with var.equal = TRUE).\nIf you get a P-value close to 1, this is often an indication that you have the alternative the wrong way around.\n\\(\\blacksquare\\)\n\nWhat do you conclude from your test, in the context of the data?\n\nSolution\nThe P-value is definitely less than 0.05, so we reject the null hypothesis (which says that the mean guess is the same regardless of the anchor the student was given). So we have evidence that the mean guess is higher for the students who were given the US population first.\nExtra 1: this is perhaps the place to think about what effect that outlier in the australia group might have had. Since it is a high outlier, its effect will be to make the the australia mean higher than it would have been otherwise, and therefore to make the two group means closer together. Despite this, the difference still came out strongly significant, so that we can be even more sure than the P-value says that there is a real difference between the means of estimates of the population of Canada. (To say it differently, if the outlier had not been there, the difference in means would have been even bigger and thus even more significant.)\nExtra 2: if you are still worried about doing a two-sample \\(t\\)-test here, you might consider looking at the bootstrapped sampling distribution of the sample mean of the australia group:\n\ncanada %&gt;% filter(anchor == \"australia\") -&gt; oz\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(the_sample = list(sample(oz$estimate, replace = TRUE))) %&gt;% \n  mutate(the_mean = mean(the_sample)) %&gt;% \n  ggplot(aes(x = the_mean)) + geom_histogram(bins=10)\n\n\n\n\nThis is indeed skewed to the right (though, with 11 observations, not nearly so non-normal as the original data), and so the P-value we got from the \\(t\\)-test may not be reliable. But, as discussed in Extra 1, the “correct” P-value is, if anything, even smaller than the one we got, and so the conclusion we drew earlier (that there is a significant anchoring effect) is not going to change.\nExtra 3: looking even further ahead, there is a test that definitely does apply here, called Mood’s Median Test. You won’t have installed the package yet, so this won’t work for you just yet (read ahead if you want to learn more), but here’s how it goes:\n\nlibrary(smmr)\nmedian_test(canada, estimate, anchor)\n\n$table\n           above\ngroup       above below\n  australia     2     5\n  US            7     1\n\n$test\n       what      value\n1 statistic 5.40178571\n2        df 1.00000000\n3   P-value 0.02011616\n\n\nThis does (as it is written) a two-sided test, because it can also be used for comparing more than two groups. Since we want a one-sided test here, you can (i) check that we are on the correct side (we are)12 (ii) halve the P-value to get 0.010.\nThis is a P-value you can trust. It is not smaller than the \\(t\\)-test one, perhaps because this test is less powerful than the \\(t\\)-test in most cases.13\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "two-sample-inference.html#footnotes",
    "href": "two-sample-inference.html#footnotes",
    "title": "5  Two-sample inference",
    "section": "",
    "text": "Mine is rather prosaically called Downloads.↩︎\nThis was not sexism, but a recognition that boys and girls will be of different heights for reasons unrelated to diet. Doing it this way makes the analysis simpler.↩︎\nMine is rather prosaically called Downloads.↩︎\nLet me know if you have more success than I did.↩︎\nA Word or Excel document has all kinds of formatting information hidden in the file as well as the text that you see on the screen.↩︎\nI wrote this question a long time ago, back when I thought that facet_grid was the only way to do facets. Now, I would use facet_wrap. See the discussion about facet_wrap near the bottom.↩︎\nYou might have a second categorical variable by which you want to arrange the facets left and right, and that would go where the dot is.↩︎\nI took out the ncol since that confuses the explanation here.↩︎\nGoogle defines this as meaning “outstandingly bad, shocking”.↩︎\nThis means that I would expect to reject a null hypothesis of equal means, but I get ahead of myself.↩︎\nThis was not sexism, but a recognition that boys and girls will be of different heights for reasons unrelated to diet. Doing it this way makes the analysis simpler.↩︎\nThe test works by comparing the data values in each group to the overall median. The students who were given Australia as an anchor mostly guessed below the overall median, and the students given the US as an anchor mostly guessed above.↩︎\nIt uses the data less efficiently than the t-test; it just counts the number of values above and below the overall median in each group, rather than using the actual numbers to compute means.↩︎"
  },
  {
    "objectID": "power.html#simulating-power",
    "href": "power.html#simulating-power",
    "title": "6  Power and sample size",
    "section": "6.1 Simulating power",
    "text": "6.1 Simulating power\nThis question investigates power by simulation.\n\nUse rnorm to generate 10 random values from a normal distribution with mean 20 and SD 2. Do your values look reasonable? Explain briefly. (You don’t need to draw a graph.)\nEstimate by simulation the power of a \\(t\\)-test to reject a null hypothesis of 20 when the true mean is also 20, the population SD is 2, and the sample size is 10, against a (default) two-sided alternative. Remember the steps: (i) generate a lot of random samples from the true distribution, (ii) run the \\(t\\)-test with the required null mean, (iii) pull out the P-values, (iv) count how many of them are 0.05 or less.\nIn the simulation you just did, was the null hypothesis true or false? Do you want to reject the null hypothesis or not? Explain briefly why the simulation results you got were (or were not) about what you would expect.\nBy copying, pasting and editing your code from the previous part, estimate the power of the test of \\(H_0: \\mu=20\\) (against a two-sided alternative) when the true population mean is 22 (rather than 20).\nUse R to calculate this power exactly (without simulation). Compare the exact result with your simulation."
  },
  {
    "objectID": "power.html#calculating-power-and-sample-size-for-estimating-mean",
    "href": "power.html#calculating-power-and-sample-size-for-estimating-mean",
    "title": "6  Power and sample size",
    "section": "6.2 Calculating power and sample size for estimating mean",
    "text": "6.2 Calculating power and sample size for estimating mean\nWe are planning a study to estimate a population mean. The population standard deviation is believed to be 20, and the population distribution is believed to be approximately normal. We will be testing the null hypothesis that the population mean is 100. Suppose the population mean is actually 110, and we want to determine how likely we are to (correctly) reject the null hypothesis in this case, using a two-sided (but one-sample) test with \\(\\alpha=0.05\\).\n\nWe will take a sample of size \\(n=30\\). Calculate the power of this test.\nFind the sample size necessary to obtain a power of at least 0.80 under these conditions. What sample size do you need? Explain briefly how your answer is consistent with (a)."
  },
  {
    "objectID": "power.html#simulating-power-for-proportions",
    "href": "power.html#simulating-power-for-proportions",
    "title": "6  Power and sample size",
    "section": "6.3 Simulating power for proportions",
    "text": "6.3 Simulating power for proportions\nIn opinion surveys (and other places), we are testing for a proportion \\(p\\) (for example, the proportion of people agreeing with some statement). Often, we want to know whether the proportion is “really” greater than 0.5.1\nThat would entail testing a null \\(H_0: p=0.5\\) against an alternative \\(H_a: p&gt;0.5\\). This is usually done by calculating the test statistic \\[ z = { \\hat{p} - 0.5 \\over \\sqrt{0.25/n}},\\] where \\(\\hat{p}\\) is the observed proportion in the sample, and getting a P-value from the upper tail of a standard normal distribution. (The 0.25 is \\(p(1-p)\\) where \\(p=0.5\\).) This is what prop.test does, as we investigate shortly.\n\nUse rbinom to generate a random value from a binomial distribution with \\(n=100\\) and \\(p=0.6\\). There are three inputs to rbinom: the first one should be the number 1, and the second and third are the \\(n\\) and \\(p\\) of the binomial distribution.\nUsing the random binomial that you generated just above, use prop.test to test whether it could reasonably have come from a binomial population with \\(n=100\\) and \\(p=0.5\\), or whether \\(p\\) is actually bigger than 0.5. (Of course, you know it actually did not come from a population with \\(p=0.5\\).) prop.test has, for us, four inputs, thus:\n\n\nthe observed number of successes\nthe n of the binomial distribution\nthe null-hypothesis p of the binomial distribution\nthe alternative hypothesis, here “greater”\n\n\nRun prop.test again, just as you did before, but this time save the result, and extract the piece of it called p.value. Is that the P-value from your test?\nEstimate the power of a test of \\(H_0: p=0.5\\) against \\(H_a: p&gt;0.5\\) when \\(n=500\\) and \\(p=0.56\\), using \\(\\alpha=0.05\\). There are three steps:\n\n\ngenerate random samples from binomial distributions with \\(n=500\\) and \\(p=0.56\\), repeated “many” times (something like 1000 or 10,000 is good)\nrun prop.test on each of those random samples\nextract the P-value for each test and save the results (in a column called, perhaps, pvals).\n\nSo I lied: the fourth and final step is to count how many of those P-values are 0.05 or less."
  },
  {
    "objectID": "power.html#designing-a-study-to-have-enough-power",
    "href": "power.html#designing-a-study-to-have-enough-power",
    "title": "6  Power and sample size",
    "section": "6.4 Designing a study to have enough power",
    "text": "6.4 Designing a study to have enough power\nYou are designing a study to test the null hypothesis that a population mean is 0 against the alternative hypothesis that it is greater than 0. Assume that the population SD is \\(\\sigma=15\\). It is important to detect the alternative \\(\\mu=2\\); that is, we want to design the study so that most of the time the null hypothesis would be (correctly) rejected if in fact \\(\\mu=2\\). A one-sample \\(t\\)-test will be used, and the data values are assumed to have a normal distribution.\n\nUse simulation to estimate the power of this test when the sample size is 100. Use \\(\\alpha=0.05\\).\nAgain by using simulation, estimate how large a sample size would be needed to obtain a power of 0.80. Show and briefly explain your process."
  },
  {
    "objectID": "power.html#power-and-alpha-in-a-skewed-population",
    "href": "power.html#power-and-alpha-in-a-skewed-population",
    "title": "6  Power and sample size",
    "section": "6.5 Power and \\(\\alpha\\) in a skewed population",
    "text": "6.5 Power and \\(\\alpha\\) in a skewed population\nA population of a large number of values v is at http://ritsokiguess.site/datafiles/pop.csv, in a CSV file.\n\nRead in the population and display some of the values.\nObtain a suitable plot of your population. What do you notice?\nIf you take a sample of 10 observations from this population and run a \\(t\\)-test, how likely are you to (correctly) reject the null hypothesis \\(H_0: \\mu = 4\\), against the alternative \\(H_a: \\mu &gt; 4\\)? Investigate by simulation.\nTry again with a sample size of 50 (leaving everything else the same). Explain briefly why the results so far are as you’d expect.\nAgain by simulation, estimate the probability that the null hypothesis \\(H_0: \\mu=5\\) will be rejected when a sample of size 10 is taken from this population, in favour of the alternative \\(H_a: \\mu &gt; 5\\). Explain briefly why the answer is not what you would have expected, and why that happened here. (Hint: what is the population mean?)\n\nMy solutions follow:"
  },
  {
    "objectID": "power.html#simulating-power-1",
    "href": "power.html#simulating-power-1",
    "title": "6  Power and sample size",
    "section": "6.6 Simulating power",
    "text": "6.6 Simulating power\nThis question investigates power by simulation.\n\nUse rnorm to generate 10 random values from a normal distribution with mean 20 and SD 2. Do your values look reasonable? Explain briefly. (You don’t need to draw a graph.)\n\nSolution\nrnorm with the number of values first, then the mean, then the SD:\n\nx=rnorm(10,20,2)\nx\n\n [1] 21.59476 18.64044 21.83231 18.76556 18.64861 21.81889 21.62614 20.18249\n [9] 16.91266 20.63490\n\n\n95% of the sampled values should be within 2 SDs of the mean, that is, between 16 and 24 (or 99.7% should be within 3 SDs of the mean, between 14 and 26). None of my values are even outside the interval 16 to 24, though yours may be different.\nI saved mine in a variable and then displayed them, which you don’t need to do. I did because there’s another way of assessing them for reasonableness: turn the sample into \\(z\\)-scores and see whether the values you get look like \\(z\\)-scores (that is, most of them are between \\(-2\\) and 2, for example):\n\n(x-20)/2\n\n [1]  0.79738130 -0.67977910  0.91615386 -0.61722168 -0.67569291  0.90944266\n [7]  0.81307163  0.09124563 -1.54367207  0.31744905\n\n\nThese ones look very much like \\(z\\)-scores. This, if you think about it, is really the flip-side of 68–95–99.7, so it’s another way of implementing the same idea.\nYou might also think of finding the sample mean and SD, and demonstrating that they are close to the right answers. Mine are:\n\nmean(x)\n\n[1] 20.06568\n\nsd(x)\n\n[1] 1.731305\n\n\nThe sample SD is more variable than the sample mean, so it can get further away from the population SD than the sample mean does from the population mean.\nThe downside to this idea is that it doesn’t get at assessing the normality, which looking at \\(z\\)-scores or equivalent does. Maybe coupling the above with a boxplot would have helped, had I not said “no graphs”, since then you’d (hopefully) see no outliers and a roughly symmetric shape.\nThis is old-fashioned “base R” technology; you could do it with a data frame like this:\n\nd &lt;- tibble(x=rnorm(10,20,2))\nd\n\n\n\n  \n\n\nd %&gt;% summarize(m=mean(x), s=sd(x))\n\n\n\n  \n\n\n\nThese are different random numbers, but are about equally what you’d expect. (These ones are a bit less variable than you’d expect, but with only ten values, don’t expect perfection.)\nSome discussion about the kind of values you should get, and whether or not you get them, is what is called for here. I want you to say something convincing about how the values you get come from a normal distribution with mean 20 and SD 2. “Close to 20” is not the whole answer here, because that doesn’t get at “how close to 20?”: that is, it talks about the mean but not about the SD.\n\\(\\blacksquare\\)\n\nEstimate by simulation the power of a \\(t\\)-test to reject a null hypothesis of 20 when the true mean is also 20, the population SD is 2, and the sample size is 10, against a (default) two-sided alternative. Remember the steps: (i) generate a lot of random samples from the true distribution, (ii) run the \\(t\\)-test with the required null mean, (iii) pull out the P-values, (iv) count how many of them are 0.05 or less.\n\nSolution\nOnce you get the hang of these, they all look almost the same. This one is easier than some because we don’t have to do anything special to get a two-sided alternative hypothesis. The initial setup is to make a dataframe with a column called something like sim to label the simulations, and then a rowwise to generate one random sample, \\(t\\)-test and P-value for each simulation:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %&gt;% \n  mutate(t_test = list(t.test(norm_sample, mu = 20))) %&gt;% \n  mutate(pval = t_test$p.value) %&gt;% \n  count(pval &lt;= 0.05)\n\n\n\n  \n\n\n\nThe power is about 4.2%. This seems depressingly small, but see the next part. (Are you confused about something in this one? You have a right to be.)\n\\(\\blacksquare\\)\n\nIn the simulation you just did, was the null hypothesis true or false? Do you want to reject the null hypothesis or not? Explain briefly why the simulation results you got were (or were not) about what you would expect.\n\nSolution\nThe null mean and the true mean were both 20: that is, the null hypothesis was correct, and rejecting it would be a mistake, to be precise a type I error. We were doing the test at \\(\\alpha=0.05\\) (by comparing our collection of simulated P-values with 0.05), so we should be making a type I error 5% of the time. This is entirely in line with the 4.2% of (wrong) rejections that I had. Your estimation is likely to be different from mine, but you should be rejecting about 5% of the time. If your result is very different from 5%, that’s an invitation to go back and check your code. On the other hand, if it is about 5%, that ought to give you confidence to go on and use the same ideas for the next part.\n\\(\\blacksquare\\)\n\nBy copying, pasting and editing your code from the previous part, estimate the power of the test of \\(H_0: \\mu=20\\) (against a two-sided alternative) when the true population mean is 22 (rather than 20).\n\nSolution\nHere’s the code we just used:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %&gt;% \n  mutate(t_test = list(t.test(norm_sample, mu = 20))) %&gt;% \n  mutate(pval = t_test$p.value) %&gt;% \n  count(pval &lt;= 0.05)\n\nOne of those 20s needs to become 22. Not the one in the t.test, since the hypotheses have not changed. So we need to change the 20 in the rnorm line to 22, since that’s where we’re generating data from the true distribution. The rest of it stays the same:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %&gt;% \n  mutate(t_test = list(t.test(norm_sample, mu = 22))) %&gt;% \n  mutate(pval = t_test$p.value) %&gt;% \n  count(pval &lt;= 0.05)\n\n\n\n  \n\n\n\nThis time, we want to reject, since the null hypothesis is false. So look at the TRUE count: the power is about \\(80\\%\\). We are very likely to correctly reject a null of 20 when the mean is actually 22.\nExtra: another way to reason that the power should be fairly large is to think about what kind of sample you are likely to get from the true distribution: one with a mean around 22 and an SD around 2. Thus the \\(t\\)-statistic should be somewhere around this (we have a sample size of 10):\n\nt_stat=(22-20)/(2/sqrt(10))\nt_stat\n\n[1] 3.162278\n\n\nand the two-sided P-value should be about\n\n2*(1-pt(t_stat,10-1))\n\n[1] 0.01150799\n\n\nOf course, with your actual data, you will sometimes be less lucky than this (a sample mean nearer 20 or a larger sample SD), but sometimes you will be luckier. But the suggestion is that most of the time, the P-value will be pretty small and you will end up correctly rejecting.\nThe quantity t_stat above, 3.16, is known to some people as an “effect size”, and summarizes how far apart the null and true means are, relative to the amount of variability present (in the sampling distribution of the sample mean). As effect sizes go, this one is pretty large.\n\\(\\blacksquare\\)\n\nUse R to calculate this power exactly (without simulation). Compare the exact result with your simulation.\n\nSolution\nThis is power.t.test. The quantity delta is the difference between true and null means:\n\npower.t.test(n=10,delta=22-20,sd=2,type=\"one.sample\",alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 10\n          delta = 2\n             sd = 2\n      sig.level = 0.05\n          power = 0.8030962\n    alternative = two.sided\n\n\nThis, 0.803, is very close to the value I got from my simulation. Which makes me think I did them both right. This is not a watertight proof, though: for example, I might have made a mistake and gotten lucky somewhere. But it does at least give me confidence.\nExtra: when you estimate power by simulation, what you are doing is rejecting or not with a certain probability (which is the same for all simulations). So the number of times you actually do reject has a binomial distribution with \\(n\\) equal to the number of simulated P-values you got (1000 in my case; you could do more) and a \\(p\\) that the simulation is trying to estimate. This is inference for a proportion, exactly what prop.test does.\nRecall that prop.test has as input:\n\na number of “successes” (rejections of the null in our case)\nthe number of trials (simulated tests)\nthe null-hypothesis value of p (optional if you only want a CI)\n(optional) a confidence level conf.level.\n\nIn part (b), we knew that the probability of (incorrectly) rejecting should have been 0.05 and we rejected 42 times out of 1000:\n\nprop.test(42,1000,0.05)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  42 out of 1000, null probability 0.05\nX-squared = 1.1842, df = 1, p-value = 0.2765\nalternative hypothesis: true p is not equal to 0.05\n95 percent confidence interval:\n 0.03079269 0.05685194\nsample estimates:\n    p \n0.042 \n\n\nLooking at the P-value, we definitely fail to reject that the probability of (incorrectly) rejecting is the 0.05 that it should be. Ouch. That’s true, but unnecessarily confusing. Look at the confidence interval instead, 0.031 to 0.057. The right answer is 0.05, which is inside that interval, so good.\nIn part (c), we didn’t know what the power was going to be (not until we calculated it with power.t.test, anyway), so we go straight for a confidence interval; the default 95% confidence level is fine. We (correctly) rejected 798 times out of 1000:\n\nprop.test(798,1000)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  798 out of 1000, null probability 0.5\nX-squared = 354.02, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.7714759 0.8221976\nsample estimates:\n    p \n0.798 \n\n\nI left out the 3rd input since we’re not doing a test, and ignore the P-value that comes out. (The default null proportion is 0.5, which often makes sense, but not here.)\nAccording to the confidence interval, the estimated power is between 0.771 and 0.822. This interval definitely includes what we now know is the right answer of 0.803.\nThis might be an accurate enough assessment of the power for you, but if not, you can do more simulations, say 10,000:\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %&gt;% \n  mutate(t_test = list(t.test(norm_sample, mu = 22))) %&gt;% \n  mutate(pval = t_test$p.value) %&gt;% \n  count(pval &lt;= 0.05)\n\n\n\n  \n\n\n\nI copied and pasted my code again, which means that I’m dangerously close to turning it into a function, but anyway.\nThe confidence interval for the power is then\n\nprop.test(7996,10000)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  7996 out of 10000, null probability 0.5\nX-squared = 3589.2, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.7915892 0.8073793\nsample estimates:\n     p \n0.7996 \n\n\nthat is, from 0.792 to 0.807, which once again includes the right answer of 0.803. The first interval, based on 1,000 simulations, has length 0.051, while this interval has length 0.015. The first interval is more than three times as long as the second, which is about what you’d expect since the first one is based on 10 times fewer simulations, and thus ought to be a factor of \\(\\sqrt{10}\\simeq 3.16\\) times longer.\nThis means that you can estimate power as accurately as you like by doing a large enough (possibly very large) number of simulations. Provided, that is, that you are prepared to wait a possibly long time for it to finish working!\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "power.html#calculating-power-and-sample-size-for-estimating-mean-1",
    "href": "power.html#calculating-power-and-sample-size-for-estimating-mean-1",
    "title": "6  Power and sample size",
    "section": "6.7 Calculating power and sample size for estimating mean",
    "text": "6.7 Calculating power and sample size for estimating mean\nWe are planning a study to estimate a population mean. The population standard deviation is believed to be 20, and the population distribution is believed to be approximately normal. We will be testing the null hypothesis that the population mean is 100. Suppose the population mean is actually 110, and we want to determine how likely we are to (correctly) reject the null hypothesis in this case, using a two-sided (but one-sample) test with \\(\\alpha=0.05\\).\n\nWe will take a sample of size \\(n=30\\). Calculate the power of this test.\n\nSolution\npower.t.test. Fill in: sample size n, difference in means delta (\\(10=110-100\\)), population SD sd, type of test type (one.sample) and kind of alternative hypothesis alternative (two.sided). Leave out power since that’s what we want:\n\npower.t.test(n=30,delta=10,sd=20,type=\"one.sample\",alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 30\n          delta = 10\n             sd = 20\n      sig.level = 0.05\n          power = 0.7539627\n    alternative = two.sided\n\n\nI meant “calculate” exactly rather than “estimate” (by simulation). Though if you want to, you can do that as well, thus:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(samples = list(rnorm(30, 110, 20))) %&gt;% \n  mutate(ttest = list(t.test(samples, mu= 100))) %&gt;% \n  mutate(pvals = ttest$p.value) %&gt;% \n  count(pvals&lt;=0.05)\n\n\n\n  \n\n\n\nThat came out alarmingly close to the exact answer.\n\\(\\blacksquare\\)\n\nFind the sample size necessary to obtain a power of at least 0.80 under these conditions. What sample size do you need? Explain briefly how your answer is consistent with (a).\n\nSolution\nAgain, the implication is “by calculation”. This time, in power.t.test, put in 0.80 for power and leave out n. The order of things doesn’t matter (since I have named everything that’s going into power.t.test):\n\npower.t.test(delta=10,power=0.80,sd=20,type=\"one.sample\",alternative=\"two.sided\")  \n\n\n     One-sample t test power calculation \n\n              n = 33.3672\n          delta = 10\n             sd = 20\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nTo get sample size for power at least 0.80, we have to round 33.36 up to the next whole number, ie. \\(n=34\\) is needed. (A sample of size 33 wouldn’t quite have enough power.)\nThis answer is consistent with (a) because a sample size of 30 gave a power a bit less than 0.80, and so to increase the power by a little (0.75 to 0.80), we had to increase the sample size by a little (30 to 34).\nExtra: estimating sample sizes by simulation is tricky, because the sample size has to be input to the simulation. That means your only strategy is to try different sample sizes until you find one that gives the right power.\nIn this case, we know that a sample of size 30 doesn’t give quite enough power, so we have to up the sample size a bit. How about we try 40? I copied and pasted my code from above and changed 30 to 40:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(samples = list(rnorm(40, 110, 20))) %&gt;% \n  mutate(ttest = list(t.test(samples, mu= 100))) %&gt;% \n  mutate(pvals = ttest$p.value) %&gt;% \n  count(pvals&lt;=0.05)\n\n\n\n  \n\n\n\nNow the power is a bit too big, so we don’t need a sample size quite as big as 40. So probably our next guess would be 35. But before we copy and paste again, we should be thinking about making a function of it first, with the sample size as input. Copy-paste once more and edit:\n\nsim_power=function(n) {\n  tibble(sim = 1:1000) %&gt;% \n    rowwise() %&gt;% \n    mutate(samples = list(rnorm(n, 110, 20))) %&gt;% \n    mutate(ttest = list(t.test(samples, mu= 100))) %&gt;% \n    mutate(pvals = ttest$p.value) %&gt;% \n    ungroup() %&gt;% \n    count(pvals&lt;=0.05)\n}\n\nIn the grand scheme of things, we might want to have the null and true means, population SD and \\(\\alpha\\) be inputs to the function as well, so that we have a more general tool, but this will do for now.\nLet’s run it with a sample size of 35:\n\nsim_power(35)\n\n\n\n  \n\n\n\nand I’m going to call that good. (Because there is randomness in the estimation of the power, don’t expect to get too close to the right answer. This one came out a fair bit less than the right answer; the power for \\(n=35\\) should be a bit more than 0.80.)\nNow that you have the software to do it, you can see that figuring out a sample size like this, at least roughly, won’t take very long: each one of these simulations takes maybe seconds to run, and all you have to do is copy and paste the previous one, and edit it to contain the new sample size before running it again. You’re making the computer work hard while you lazily sip your coffee, but there’s no harm in that: programmer’s brain cells are more valuable than computer CPU cycles, and you might as well save your brain cells for when you really need them.\nYou might even think about automating this further. The easiest way, now that we have the function, is something like this:\n\ntibble(ns = seq(20, 50, 5)) %&gt;% \n  rowwise() %&gt;% \n  mutate(power_tab = list(sim_power(ns))) %&gt;% \n  unnest(power_tab) %&gt;% \n  pivot_wider(names_from = `pvals &lt;= 0.05`, values_from = n)\n\n\n\n  \n\n\n\nThe business end of this is happening in the first three lines. I wasn’t thinking of this when I originally wrote sim_power to return a dataframe, so there is a bit more fiddling after the simulations are done: I have to unnest to see what the list-column power_tab actually contains, and because of the layout of the output from unnesting sim_power (long format), it looks better if I pivot it wider, so that I can just cast my eye down the TRUE column and see the power increasing as the sample size increases.\nYou might also think of something like bisection to find the sample size that has power 0.8, but it starts getting tricky because of the randomness; just by chance, it may be that sometimes the simulated power goes down as the sample size goes up. With 1000 simulations each time, it seems that the power ought to hit 80% with a sample size between 30 and 35."
  },
  {
    "objectID": "power.html#simulating-power-for-proportions-1",
    "href": "power.html#simulating-power-for-proportions-1",
    "title": "6  Power and sample size",
    "section": "6.8 Simulating power for proportions",
    "text": "6.8 Simulating power for proportions\nIn opinion surveys (and other places), we are testing for a proportion \\(p\\) (for example, the proportion of people agreeing with some statement). Often, we want to know whether the proportion is “really” greater than 0.5.2\nThat would entail testing a null \\(H_0: p=0.5\\) against an alternative \\(H_a: p&gt;0.5\\). This is usually done by calculating the test statistic \\[ z = { \\hat{p} - 0.5 \\over \\sqrt{0.25/n}},\\] where \\(\\hat{p}\\) is the observed proportion in the sample, and getting a P-value from the upper tail of a standard normal distribution. (The 0.25 is \\(p(1-p)\\) where \\(p=0.5\\).) This is what prop.test does, as we investigate shortly.\n\nUse rbinom to generate a random value from a binomial distribution with \\(n=100\\) and \\(p=0.6\\). There are three inputs to rbinom: the first one should be the number 1, and the second and third are the \\(n\\) and \\(p\\) of the binomial distribution.\n\nSolution\nI am doing some preparatory work that you don’t need to do:\n\nset.seed(457299)\n\nBy setting the “seed” for the random number generator, I guarantee that I will get the same answers every time I run my code below (and therefore I can talk about my answers without worrying that they will change). Up to you whether you do this. You can “seed” the random number generator with any number you like. A lot of people use 1. Mahinda seems to like 123. Mine is an old phone number.\nAnd so to work:\n\nrbinom(1, 100, 0.6)\n\n[1] 60\n\n\nI got exactly 60% successes this time. You probably won’t get exactly 60, but you should get somewhere close. (If you use my random number seed and use the random number generator exactly the same way I did, you should get the same values I did.)\nFor fun, you can see what happens if you change the 1:\n\nrbinom(3, 100, 0.6)\n\n[1] 58 57 55\n\n\nThree random binomials, that happened to come out just below 60. We’re going to leave the first input as 1, though, and let rowwise handle “lots of sampled values” later.\n\\(\\blacksquare\\)\n\nUsing the random binomial that you generated just above, use prop.test to test whether it could reasonably have come from a binomial population with \\(n=100\\) and \\(p=0.5\\), or whether \\(p\\) is actually bigger than 0.5. (Of course, you know it actually did not come from a population with \\(p=0.5\\).) prop.test has, for us, four inputs, thus:\n\n\nthe observed number of successes\nthe n of the binomial distribution\nthe null-hypothesis p of the binomial distribution\nthe alternative hypothesis, here “greater”\n\nSolution\nI got exactly 60 successes, so I do this:\n\nprop.test(60, 100, 0.5, alternative = \"greater\")\n\n\n    1-sample proportions test with continuity correction\n\ndata:  60 out of 100, null probability 0.5\nX-squared = 3.61, df = 1, p-value = 0.02872\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.5127842 1.0000000\nsample estimates:\n  p \n0.6 \n\n\nThe P-value should at least be fairly small, since 60 is a bit bigger than 50. (Think about tossing a coin 100 times; would 60 heads make you doubt the coin’s fairness? The above says it should.)\n\\(\\blacksquare\\)\n\nRun prop.test again, just as you did before, but this time save the result, and extract the piece of it called p.value. Is that the P-value from your test?\n\nSolution\nCopying and pasting:\n\np_test &lt;- prop.test(60, 100, 0.5, alternative = \"greater\")\np_test$p.value\n\n[1] 0.02871656\n\n\nYep, the same.\n\\(\\blacksquare\\)\n\nEstimate the power of a test of \\(H_0: p=0.5\\) against \\(H_a: p&gt;0.5\\) when \\(n=500\\) and \\(p=0.56\\), using \\(\\alpha=0.05\\). There are three steps:\n\n\ngenerate random samples from binomial distributions with \\(n=500\\) and \\(p=0.56\\), repeated “many” times (something like 1000 or 10,000 is good)\nrun prop.test on each of those random samples\nextract the P-value for each test and save the results (in a column called, perhaps, pvals).\n\nSo I lied: the fourth and final step is to count how many of those P-values are 0.05 or less.\nSolution\nThe first part of the first step is to create a column called something like sim that labels each simulated sample, and to make sure that everything happens rowwise. After that, you follow the procedure:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = rbinom(1, 500, 0.56)) %&gt;% \n  mutate(test = list(prop.test(sample, 500, 0.5, alternative = \"greater\"))) %&gt;% \n  mutate(pvals = test$p.value) %&gt;% \n  count(pvals &lt;= 0.05)\n\n\n\n  \n\n\n\nThe previous parts, using rbinom and prop.test, were meant to provide you with the ingredients for this part. The first step is to use rbinom. The first input is 1 since we only want one random binomial each time (the rowwise will handle the fact that you actually want lots of them; you only want one per row since you are working rowwise). The second step runs prop.test; the first input to that is each one of the numbers of successes from the first step. The last part is to pull out all the P-values and make a table of them, just like the example in lecture.\nThe estimated power is about 85%. That is, if \\(p\\) is actually 0.56 and we have a sample of size 500, we have a good chance of (correctly) rejecting that \\(p=0.5\\).\nExtra: It turns out that SAS can work out this power by calculation (using proc power). SAS says our power is also about 85%, as our simulation said. I was actually pleased that my simulation came out so close to the right answer.\nIn contrast to power.t.test, SAS’s proc power handles power analyses for a lot of things, including analysis of variance, correlation and (multiple) regression. What these have in common is some normal-based theory that allows you (under assumptions of sufficiently normal-shaped populations) to calculate the exact answer (that is, the distribution of the test statistic when the alternative hypothesis is true). The case we looked at is one of those because of the normal approximation to the binomial: once \\(n\\) gets big, particularly if \\(p\\) is somewhere near 0.5, the binomial is very well approximated by a normal with the right mean and SD.\nThe moral of this story is that when you have a decently large sample, \\(n=500\\) in this case, \\(p\\) doesn’t have to get very far away from 0.5 before you can correctly reject 0.5. Bear in mind that sample sizes for estimating proportions need to be larger than those for estimating means, so \\(n=500\\) is large without being huge. The practical upshot is that if you design a survey and give it to 500 (or more) randomly chosen people, the proportion of people in favour doesn’t have to be much above 50% for you to correctly infer that it is above 50%, most of the time.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "power.html#designing-a-study-to-have-enough-power-1",
    "href": "power.html#designing-a-study-to-have-enough-power-1",
    "title": "6  Power and sample size",
    "section": "6.9 Designing a study to have enough power",
    "text": "6.9 Designing a study to have enough power\nYou are designing a study to test the null hypothesis that a population mean is 0 against the alternative hypothesis that it is greater than 0. Assume that the population SD is \\(\\sigma=15\\). It is important to detect the alternative \\(\\mu=2\\); that is, we want to design the study so that most of the time the null hypothesis would be (correctly) rejected if in fact \\(\\mu=2\\). A one-sample \\(t\\)-test will be used, and the data values are assumed to have a normal distribution.\n\nUse simulation to estimate the power of this test when the sample size is 100. Use \\(\\alpha=0.05\\).\n\nSolution\nUse at least 1000 simulations (more, if you’re willing to wait for it). In rnorm, the sample size is first, then the (true) population mean, then the (assumed) population SD:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(100, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThe power is (estimated as) a disappointing 0.361. Your answer won’t (most likely) be the same as this, but it should be somewhere close. I would like to see you demonstrate that you know what power is, for example “if the population mean is actually 2, the null hypothesis \\(H_0: \\mu = 0\\), which is wrong, will only be rejected about 36% of the time”.3\nThe test we are doing is one-sided, so you need the alternative in there. If you omit it, you’ll have the answer to a different problem:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(100, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThis is the probability that you reject \\(H_0: \\mu=0\\) in favour of \\(H_a: \\mu \\ne 0\\). This is smaller, because the test is “wasting effort” allowing the possibility of rejecting when the sample mean is far enough less than zero, when most of the time the samples drawn from the true distribution have mean greater than zero. (If you get a sample mean of 2.5, say, the P-value for a one-sided test will be smaller than for a two-sided one.)\nExtra 1:\nThis low power of 0.361 is because the population SD is large relative to the kind of difference from the null that we are hoping to find. To get a sense of how big the power might be, imagine you draw a “typical” sample from the true population: it will have a sample mean of 2 and a sample SD of 15, so that \\(t\\) will be about\n\n(2-0)/(15/sqrt(100))\n\n[1] 1.333333\n\n\nYou won’t reject with this (\\(t\\) would have to be bigger than 2), so in the cases where you do reject, you’ll have to be more lucky: you’ll need a sample mean bigger than 2, or a sample SD smaller than 15. So the power won’t be very big, less than 0.5, because about half the time you’ll get a test statistic less than 1.33 and about half the time more, and not all of those will lead to rejection.\nExtra 2:\nThis is exactly the situation where power.t.test works, so we can get the exact answer (you need all the pieces):\n\npower.t.test(n=100, delta=2-0, sd=15, type=\"one.sample\", \nalternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 100\n          delta = 2\n             sd = 15\n      sig.level = 0.05\n          power = 0.3742438\n    alternative = one.sided\n\n\nYour answer, from 1000 simulations, should be within about 3 percentage points of that. (Mine was only about 1 percentage point off.)\n\\(\\blacksquare\\)\n\nAgain by using simulation, estimate how large a sample size would be needed to obtain a power of 0.80. Show and briefly explain your process.\n\nSolution\nThe point of this one is the process as well as the final answer, so you need to show and justify what you are doing. Showing only a final answer does not show that you know how to do it. The whole point of this one is to make mistakes and fix them!\nThe simulation approach does not immediately give you a sample size for fixed power, so what you have to do is to try different sample sizes until you get one that gives a power close enough to 0.80. You have to decide what “close enough” means for you, given that the simulations have randomness in them. I’m going to use 10,000 simulations for each of my attempts, in the hope of getting a more accurate answer.\nFirst off, for a sample size of 100, the power was too small, so the answer had better be bigger than 100. I’ll try 200. For these, copy and paste the code, changing the sample size each time:\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(200, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nA sample size of 200 isn’t big enough yet. I’ll double again to 400:\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(400, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nGetting closer. 400 is too big, but closer than 200. 350?\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(350, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nClose! I reckon you could call that good (see below), or try again with a sample size a bit less than 350:\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(345, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\n340 is definitely too small:\n\ntibble(sim = 1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(340, 2, 15))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThis is actually not as close as I was expecting. I think we are getting close to simulation accuracy for this number of simulations. If we do 10,000 simulations of an event with probability 0.8 (correctly rejecting this null), below are the kind of results we might get.4 This is the middle 95% of that distribution.\n\nqbinom(c(0.025,0.975), 10000, 0.8)\n\n[1] 7921 8078\n\n\nAnything between those limits is the kind of thing we might get by chance, so simulation doesn’t let us distinguish between 347 and 350 as the correct sample size. Unless we do more than 10,000 simulations, of course!\nIf you stuck with 1000 simulations each time, these are the corresponding limits:\n\nqbinom(c(0.025,0.975), 1000, 0.8)\n\n[1] 775 824\n\n\nand any sample sizes that produce an estimated power between these are as accurate as you’ll get. (Here you see the advantage of doing more simulations.)\nIf you’ve been using 10,000 simulations each time like me, you’ll have noticed that these actually take a noticeable time to run. This is why coders always have a coffee or something else to sip on while their code runs; coders, like us, need to see the output to decide what to do next. Or you could install the beepr package, and get some kind of sound when your simulation finishes, so that you’ll know to get off Twitter5 and see what happened. There are also packages that will send you a text message or will send a notification to all your devices.\nWhat I want to see from you here is some kind of trial and error that proceeds logically, sensibly increasing or decreasing the sample size at each trial, until you have gotten reasonably close to power 0.8.\nExtra: once again we can figure out the correct answer:\n\npower.t.test(power = 0.80, delta=2-0, sd=15, type=\"one.sample\", \nalternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 349.1256\n          delta = 2\n             sd = 15\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\n\nThis does not answer the question, though, since you need to do it by simulation with trial and error. If you want to do it this way, do it at the end as a check on your work; if the answer you get this way is very different from the simulation results, that’s an invitation to check what you did.\n350 actually is the correct answer. But you will need to try different sample sizes until you get close enough to a power of 0.8; simply doing it for \\(n=350\\) is not enough, because how did you know to try 350 and not some other sample size?\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "power.html#power-and-alpha-in-a-skewed-population-1",
    "href": "power.html#power-and-alpha-in-a-skewed-population-1",
    "title": "6  Power and sample size",
    "section": "6.10 Power and \\(\\alpha\\) in a skewed population",
    "text": "6.10 Power and \\(\\alpha\\) in a skewed population\nA population of a large number of values v is at http://ritsokiguess.site/datafiles/pop.csv, in a CSV file.\n\nRead in the population and display some of the values.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/pop.csv\"\npop &lt;- read_csv(my_url)\n\nRows: 10000 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): v\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npop\n\n\n\n  \n\n\n\n10,000 values. A large population. (From these few values, v seems to be positive but rather variable.)\n\\(\\blacksquare\\)\n\nObtain a suitable plot of your population. What do you notice?\n\nSolution\nOne quantitative variable, so a histogram. The population is large, so you can use more bins than usual. Sturges’ rule says 14 bins (the logarithm below is base 2, or, the next power of 2 above 10,000 is 16,384 which is \\(2^{14}\\)):\n\nlog(10000, 2)\n\n[1] 13.28771\n\n2^14\n\n[1] 16384\n\n\n\nggplot(pop, aes(x=v)) + geom_histogram(bins=14)\n\n\n\n\nPick a number of bins: the default 30 bins is pretty much always too many. Any number of bins that shows this shape is good as an answer, but you also need to display some thinking about how many bins to use, either starting with a rule as I did, or experimenting with different numbers of bins. Rules are not hard and fast; it so happened that I liked the picture that 14 bins gave, so I stopped there. Thirty bins, the default, is actually not bad here:\n\nggplot(pop, aes(x=v)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nbut if you do this, you need to say something that indicates some conscious thought, such as saying “this number of bins gives a good picture of the shape of the distribution”, which I am OK with. Have a reason for doing what you do.\nThis is skewed to the right, or has a long right tail. This is a better description than “outliers”: there are indeed some very large values (almost invisible on the histogram), but to say that is to imply that the rest of the distribution apart from the outliers has a regular shape, not something you can say here.6\nExtra: The issue that’s coming up is whether this is normally-distributed, which of course it is not. This is a normal quantile plot. (Idea: if the points follow the line, at least approximately, the variable is normally distributed; if not, not.):\n\nggplot(pop, aes(sample=v)) + stat_qq() + stat_qq_line()\n\n\n\n\nThat is your archetypal skewed-to-right. The two highest values are not a lot higher than the rest, again supporting a curved shape overall (skewness) on this plot, rather than the problem being outliers. (The highest values are consistent with the shape of the curve, rather than being unusually high compared to the curve.)\n\\(\\blacksquare\\)\n\nIf you take a sample of 10 observations from this population and run a \\(t\\)-test, how likely are you to (correctly) reject the null hypothesis \\(H_0: \\mu = 4\\), against the alternative \\(H_a: \\mu &gt; 4\\)? Investigate by simulation.\n\nSolution\nAs you noted, this is a one-sided alternative, so make sure your code does the right thing. Take a lot of random samples, run the \\(t\\)-test on each one, grab the P-value each time, count the number of P-values less or equal to your \\(\\alpha\\). This is not a bootstrap, so the sampling needs to be without replacement, and you need to say how big the sample is:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(pop$v, 10))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 4, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThe estimated power is only about 0.19.\nAs to the code, well, the samples and the \\(t\\)-test both consist of more than one thing, so in the mutates that create them, don’t forget the list around the outside, which will create a list-column.\nHere, and elsewhere in this question, use at least 1000 simulations. More will give you more accurate results, but you’ll have to wait longer for it to run. Your choice.\nAs a final remark, you can not do this one by algebra, as you might have done in other courses, because you do not know the functional form of the population distribution. The power calculations you may have done before as calculations typically assume a normal population, because if you don’t, the algebra gets too messy too fast. (You’d need to know the distribution of the test statistic under the alternative hypothesis, which in cases beyond the normal is not usually known.)\n\\(\\blacksquare\\)\n\nTry again with a sample size of 50 (leaving everything else the same). Explain briefly why the results so far are as you’d expect.\n\nSolution\nFor the code, this is copy-paste-edit. Just change the sample size:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(pop$v, 50))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 4, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThe power is now much bigger, around 73%. This is as expected because with a larger sample size we should be more likely to reject a false null hypothesis.\nThe reason for this is that the mean of a bigger sample should be closer to the population mean, because of the Law of Large Numbers, and thus further away from the incorrect null hypothesis and more likely far enough away to reject it. In this case, as you will see shortly, the population mean is 5, and so, with a bigger sample, the sample mean will almost certainly be closer to 5 and further away from 4.\nI have a feeling you could formalize this kind of argument with Chebyshev’s inequality, which would apply to any kind of population.7 I think I’d have to write it down to get it right, though.\n\\(\\blacksquare\\)\n\nAgain by simulation, estimate the probability that the null hypothesis \\(H_0: \\mu=5\\) will be rejected when a sample of size 10 is taken from this population, in favour of the alternative \\(H_a: \\mu &gt; 5\\). Explain briefly why the answer is not what you would have expected, and why that happened here. (Hint: what is the population mean?)\n\nSolution\nTaking the hint first:\n\npop %&gt;% \nsummarize(m = mean(v))\n\n\n\n  \n\n\n\n(I’m hoping that some light dawns at this point), and copy-paste-edit your simulation code again, this time changing the null mean to 5:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(pop$v, 10))) %&gt;% \n  mutate(t_test = list(t.test(my_sample, mu = 5, alternative = \"greater\"))) %&gt;% \n  mutate(p_value = t_test$p.value) %&gt;% \n  count(p_value &lt;= 0.05)\n\n\n\n  \n\n\n\nThe “power” is estimated to be 0.020. (Again, your value won’t be exactly this, most likely, but it should be somewhere close.)\nSo what were we expecting? This time, the null hypothesis, that the population mean is 5, is actually true. So rejecting it is now a type I error, and the probability of that should be \\(\\alpha\\), which was 0.05 here. In our simulation, though, the estimated probability is quite a bit less than 0.05. (Your result will probably differ from mine, but it is not likely to be bigger than 0.05).\nTo think about why that happened, remember that this is a very skewed population, and the sample size of 10 is not big, so this is not really the situation in which we should be using a \\(t\\)-test. The consequence of doing so anyway, which is what we investigated, is that the actual \\(\\alpha\\) of our test is not 0.05, but something smaller: the test is not properly calibrated.\nIf you do this again for a sample of size 50, you’ll find that the simulation tells you that \\(\\alpha\\) is closer to 0.05, but still less. The population is skewed enough that the Central Limit Theorem still hasn’t kicked in yet, and so we still cannot trust the \\(t\\)-test to give us a sensible P-value.\nExtra: a lot more discussion on what is happening here:\nThis test is what is known in the jargon as “conservative”. To a statistician, this means that the probability of making a type I error is smaller than it should be. That is in some sense safe, in that if you reject, you can be pretty sure that this rejection is correct, but it makes it a lot harder than it should to reject in the first place, and thus you can fail to declare a discovery when you have really made one (but the test didn’t say so).\nI did some investigation to see what was going on. First, I ran the simulation again, but this time keeping the mean and SD of each sample, as well as the \\(t\\)-statistic, but not actually doing the \\(t\\)-test:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(pop$v, 10))) %&gt;% \n  mutate(xbar = mean(my_sample),\n         s = sd(my_sample),\n         t_stat = (xbar - 5) / (s / sqrt(10))) -&gt; mean_sd\nmean_sd\n\n\n\n  \n\n\n\nAs for coding, I made a dataframe with a column sim that numbers the individual samples, made sure I said that I wanted to work rowwise, generated a random sample from the population in each row of size 10, and found its mean, SD and the calculated-by-me \\(t\\)-statistic.8\nAfter that, I played around with several things, but I found something interesting when I plotted the sample mean and SD against each other:\n\nggplot(mean_sd, aes(x=xbar, y=s)) + geom_point() + geom_smooth(se=F)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nWhen the sample mean is bigger, so is the sample standard deviation!\nThis actually does make sense, if you stop to think about it. A sample with a large mean will have some of those values from the long right tail in it, and having those values will also make the sample more spread out. The same does not happen at the low end: if the mean is small, all the sample values must be close together and the SD will be small also.9\nIt wasn’t clear to me what that would do to the \\(t\\)-statistic. A larger sample mean would make the top of the test statistic bigger, but a larger sample mean would also go with a larger sample SD, and so the bottom of the test statistic would be bigger as well. That’s why I included this in the simulation too:\n\nggplot(mean_sd, aes(x=t_stat)) + geom_histogram(bins=12)\n\n\n\n\nWell, well. Skewed to the left.\nThis too makes sense with a bit of thought. A small sample mean will also have a small sample SD, so the test statistic could be more negative. But a large sample mean will have a large sample SD, so the test statistic won’t get so positive. Hence, in our simulation, the test statistic won’t get large enough to reject with as often as it should. Thus, the type I error probability that is too small.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "power.html#footnotes",
    "href": "power.html#footnotes",
    "title": "6  Power and sample size",
    "section": "",
    "text": "That would mean assessing whether an observed proportion could be greater than 0.5 just by chance, or whether it is bigger enough than 0.5 to reject chance as a plausible explanation.↩︎\nThat would mean assessing whether an observed proportion could be greater than 0.5 just by chance, or whether it is bigger enough than 0.5 to reject chance as a plausible explanation.↩︎\nThis is why I called my result disappointing. I would like to reject a lot more of the time than this, but, given that the truth was not very far away from the null given the (large) population SD, I can’t. See Extra 1.↩︎\nIf the power is really 0.8, the number of simulated tests that end up rejecting has a binomial distribution with n of 10000 and p of 0.80.↩︎\nOr Reddit or Quora or whatever your favourite time-killer is.↩︎\nThe question to ask yourself is whether the shape comes from the entire distribution, as it does here (skewness), or whether it comes from a few unusual observations (outliers).↩︎\nIt has to have a standard deviation, though, but our population seems well-enough behaved to have a standard deviation.↩︎\nThis is not bootstrapping, but generating ordinary random samples from a presumed-known population, so there is no replace = TRUE here.↩︎\nYou might have something lurking in your mind about the sample mean and sample SD/variance being independent, which they clearly are not here. That is true if the samples come from a normal distribution, and from that comes independence of the top and bottom of the \\(t\\)-statistic. But here is an example of how everything fails once you go away from normality, and how you have to rely on the central limit theorem, or large sample sizes more generally, for most of your theory to be any good.↩︎"
  },
  {
    "objectID": "sign.html#running-a-maze",
    "href": "sign.html#running-a-maze",
    "title": "7  The sign test",
    "section": "7.1 Running a maze",
    "text": "7.1 Running a maze\nA researcher is trying to design a maze that can be run by rats in about 60 seconds. One particular maze was run by a sample of 21 rats, with the times shown in link.\n\nRead the data into R. What (if anything) are the data values delimited by?\nRun a sign test, doing it yourself as we did in class: count the number of values above and below 60, take the smaller of those, and find the probability of a value of that or smaller still on a binomial distribution with \\(n=21\\) and \\(p=0.5\\) (we have 21 data points), doubling the answer because the test is two-sided.\nInstall my package smmr, if you haven’t already. To do this, you first need to install the package devtools (if you haven’t already), by going to the console and typing\n\n\ninstall.packages(\"devtools\")\n\nWhen that’s all done, install smmr thus:\n\nlibrary(devtools)\ninstall_github(\"nxskok/smmr\")\n\nThat all needs to be done only once. Then, each R Studio session where you want to use smmr needs this:\n\nlibrary(smmr)\n\nAs usual, only the library thing only needs to be done every time.\nWhen you have smmr installed, use sign_test from that package to re-run your sign test. Do you get the same P-value?\n\nPackage smmr also has a function pval_sign, which has the same input as sign_test, but with the null median first. Run it on your data and see what it gives.\nObtain a 95% confidence interval for the median based on these data. Do this two ways. First, use the trial and error way from class (either the try-lots-of-values way or the bisection way; either is good). Second, use ci_median from smmr. The latter takes as input a data frame, a column name (unquoted) and optionally a conf.level that defaults to 0.95."
  },
  {
    "objectID": "sign.html#chocolate-chips",
    "href": "sign.html#chocolate-chips",
    "title": "7  The sign test",
    "section": "7.2 Chocolate chips",
    "text": "7.2 Chocolate chips\nA famous cookie manufacturer claims that their bags of chocolate chip cookies contain “more than 1100 chocolate chips on average”. A diligent group of students buys 16 bags of these cookies and counts the number of chocolate chips in each bag. The results are in http://ritsokiguess.site/datafiles/chips.txt.\n\nRead in and display (some of) the data.\nBuild your own sign test in R for testing that the median is 1100 chocolate chips, against the alternative that it is greater. (Do this as in class: count the appropriate thing, compare it with an appropriate binomial distribution, and obtain a P-value.\nUse my R package smmr to reproduce your sign test above, and verify that you get consistent results. (See the maze-design question for instructions on installing this, if you haven’t yet.)\nUse smmr to obtain a 95% confidence interval for the median number of chocolate chips per bag of cookies."
  },
  {
    "objectID": "sign.html#the-power-of-the-sign-test",
    "href": "sign.html#the-power-of-the-sign-test",
    "title": "7  The sign test",
    "section": "7.3 The power of the sign test",
    "text": "7.3 The power of the sign test\nI’ve mentioned several times that the sign test has less power than the \\(t\\)-test. Let’s investigate this with a specific example.\nLet’s suppose we are testing \\(H_0: \\mu=40\\) against \\(H_a: \\mu \\ne 40\\), where \\(\\mu\\) is the population mean (and median, as we shall see). Our population actually has a normal distribution with mean 50 and SD 15, so that the null hypothesis is wrong and we want to reject it most of the time. On the other hand, the population actually is normally-distributed and so the \\(t\\)-test is the right one to use.\n(This is an old question, so I tackle the simulated power differently than I did it in class this time. But see if you can follow what I do here.)\n\nUse power.t.test to find the probability that a \\(t\\)-test correctly rejects the null hypothesis using a sample size of \\(n=10\\).\nWhat code in R would draw a random sample of size 10 from the true population distribution and save the sample in a variable?\nWhat code would count how many of the sampled values are less than 40 and how many are greater (or equal)?\nIt turns out the sign test would reject \\(H_0: M=40\\) against \\(H_a: M \\ne 40\\) at \\(\\alpha=0.05\\) if the smaller of the numbers in the last part is 1 or less. (\\(M\\) is the population median.) Add to your pipeline to obtain TRUE if you should reject the null for your data and FALSE otherwise.\nSimulate the above process 1000 times: draw a random sample from a normal distribution of size 10 with mean 50 and SD 15, count the number of values below 40, reject if the minimum of those is 0, 1, 9, or 10, then count the number of rejections out of 1000.\nWhich is more powerful in this case, the sign test or the \\(t\\)-test? How do you know?"
  },
  {
    "objectID": "sign.html#ben-roethlisberger",
    "href": "sign.html#ben-roethlisberger",
    "title": "7  The sign test",
    "section": "7.4 Ben Roethlisberger",
    "text": "7.4 Ben Roethlisberger\nBen Roethlisberger plays (American) football for the Pittsburgh Steelers. He plays as a quarterback, which means that his job is to throw (pass) the ball so that one of his teammates can catch it. Each time he makes a pass that is caught, this is called a “completion”, and the team coaches are interested in his average number of completions per game (this average could be the mean or the median).\nIn 2010, Roethlisberger was suspended for the first four games of the season, and there was concern that this might affect his performance (in terms of the number of passes completed in the games after he returned). The Pittsburgh Steelers did not play in week 5 of the 2010 season; the season is 17 weeks long (one game per week) and each team has one week in which they do not play.\nThe data are here. There are four columns: the year (always 2010), the week number of the season that the game was played in, the name of the opposing team, and the number of completed passes by Roethlisberger in the game.\n\nRead in and display (some of) the data. Do you have what you were expecting?\nMake a suitable graph of the number of completed passes, and explain briefly why you would have some doubts about using \\(t\\)-procedures in this situation.\nRun a sign test to compare Roethlisberger’s performance in 2010 with his previous average of 22.5 completions per game. What do you conclude?\nWhy might you have expected your sign test to come out significant, even without looking at the P-value? Explain briefly.\nObtain a 90% confidence interval for the median number of completed passes (over “all possible games played by 2010 Ben Roethlisberger”).\nFind a 90% confidence interval for the mean number of passes completed, and explain briefly why it differs from the one for the median in the way that it does."
  },
  {
    "objectID": "sign.html#six-ounces-of-protein",
    "href": "sign.html#six-ounces-of-protein",
    "title": "7  The sign test",
    "section": "7.5 Six ounces of protein",
    "text": "7.5 Six ounces of protein\nA company produces prepackaged diet meals. These meals are advertised as containing “6 ounces of protein per package”. A consumer organization is concerned that this is not accurate. The organization takes a random sample of 20 of these meals, and measures the protein content of each one. The data are in http://ritsokiguess.site/datafiles/protein.txt as one column.\n\nRead in and display (some of) the data.\nMake a suitable graph of your data.\nWhy might a sign test be better than a \\(t\\)-test for assessing the average amount of protein per package? Explain briefly. (“Average” here means any measure of centre.)\nRun a suitable sign test for these data. What do you conclude?\nIn your sign test, how could you have deduced that the P-value was going to be small even without looking at any of the P-values themselves? Explain briefly.\nObtain a 90% confidence interval for the population median protein content. What does this tell you about the reason for the rejection or non-rejection of the null hypothesis above?\n\nMy solutions follow:"
  },
  {
    "objectID": "sign.html#running-a-maze-1",
    "href": "sign.html#running-a-maze-1",
    "title": "7  The sign test",
    "section": "7.6 Running a maze",
    "text": "7.6 Running a maze\nA researcher is trying to design a maze that can be run by rats in about 60 seconds. One particular maze was run by a sample of 21 rats, with the times shown in link.\n\nRead the data into R. What (if anything) are the data values delimited by?\n\nSolution\nTake a look at the data file first. There is only one column of data, so you can treat it as being delimited by anything you like: a space, or a comma (the file can also be treated as a .csv), etc.:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/maze.txt\"\ntimes &lt;- read_delim(myurl, \" \")\n\nRows: 21 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntimes\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nRun a sign test, doing it yourself as we did in class: count the number of values above and below 60, take the smaller of those, and find the probability of a value of that or smaller still on a binomial distribution with \\(n=21\\) and \\(p=0.5\\) (we have 21 data points), doubling the answer because the test is two-sided.\n\nSolution\nCount how many values are above and below 60:\n\ntimes %&gt;% count(time &gt; 60)\n\n\n\n  \n\n\n\n5 above and 16 below. Then find out how likely it is that a binomial with \\(n=21, p=0.5\\) would produce 5 or fewer successes:\n\np &lt;- sum(dbinom(0:5, 21, 0.5))\np\n\n[1] 0.01330185\n\n\nor if you prefer count upwards from 16:\n\nsum(dbinom(16:21, 21, 0.5))\n\n[1] 0.01330185\n\n\nand double it to get a two-sided P-value:\n\n2 * p\n\n[1] 0.0266037\n\n\nWe’ll compare this with smmr in a moment.\n\\(\\blacksquare\\)\n\nInstall my package smmr, if you haven’t already. To do this, you first need to install the package devtools (if you haven’t already), by going to the console and typing\n\n\ninstall.packages(\"devtools\")\n\nWhen that’s all done, install smmr thus:\n\nlibrary(devtools)\ninstall_github(\"nxskok/smmr\")\n\nThat all needs to be done only once. Then, each R Studio session where you want to use smmr needs this:\n\nlibrary(smmr)\n\nAs usual, only the library thing only needs to be done every time.\nWhen you have smmr installed, use sign_test from that package to re-run your sign test. Do you get the same P-value?\nSolution\nThe sign test function takes a data frame, an (unquoted) column name from that data frame of data to test the median of, and a null median (which defaults to 0 if you omit it):\n\nlibrary(smmr)\nsign_test(times, time, 60)\n\n$above_below\nbelow above \n   16     5 \n\n$p_values\n  alternative    p_value\n1       lower 0.01330185\n2       upper 0.99640131\n3   two-sided 0.02660370\n\n\nThis shows you two things: a count of the values below and above the null median, and then the P-values according to the various alternative hypotheses you might have.\nIn our case, we see again the 16 maze-running times below 60 seconds and 5 above (one of which was a long way above, but we don’t care about that here). We were testing whether the median was different from 60, so we look at the two-sided P-value of 0.0266, which is exactly what we had before.\nIf sign_test doesn’t work for you (perhaps because it needs a function enquo that you don’t have), there is an alternative function sign_test0 that doesn’t use it. It requires as input a column of values (extracted from the data frame) and a null median, thus:\n\nwith(times, sign_test0(time, 60))\n\n$above_below\nbelow above \n   16     5 \n\n$p_values\n  alternative    p_value\n1       lower 0.01330185\n2       upper 0.99640131\n3   two-sided 0.02660370\n\n\nThe output should be, and here is, identical.\n\\(\\blacksquare\\)\n\nPackage smmr also has a function pval_sign, which has the same input as sign_test, but with the null median first. Run it on your data and see what it gives.\n\nSolution\nTry it and see:\n\npval_sign(60, times, time)\n\n[1] 0.0266037\n\n\nThe two-sided P-value, and that is all. We’ll be using this in a minute.\nAlternatively, there is also this, which needs a null median and a column as input:\n\nwith(times, pval_sign0(60, time))\n\n[1] 0.0266037\n\n\n\\(\\blacksquare\\)\n\nObtain a 95% confidence interval for the median based on these data. Do this two ways. First, use the trial and error way from class (either the try-lots-of-values way or the bisection way; either is good). Second, use ci_median from smmr. The latter takes as input a data frame, a column name (unquoted) and optionally a conf.level that defaults to 0.95.\n\nSolution\nThe reason for showing you pval_sign in the previous part is that this is a building block for the confidence interval. What we do is to try various null medians and find out which ones give P-values less than 0.05 (outside the interval) and which ones bigger (inside). We know that the value 60 is outside the 95% CI, and the sample median is close to 50 (which we expect to be inside), so sensible values to try for the upper end of the interval would be between 50 and 60:\n\npval_sign(58, times, time)\n\n[1] 0.0266037\n\npval_sign(55, times, time)\n\n[1] 0.6636238\n\n\nSo, 55 is inside the interval and 58 is outside. I could investigate further in similar fashion, but I thought I would try a whole bunch of null medians all at once. That goes like this, rowwise because pval_sign expects one null-hypothesis median, not several all at once:\n\ntibble(meds = seq(55, 58, 0.25)) %&gt;% \n  rowwise() %&gt;% \n  mutate(pvals = pval_sign(meds, times, time))\n\n\n\n  \n\n\n\nSo values for the median all the way up to and including 57.5 are in the confidence interval.\nNow for the other end of the interval. I’m going to do this a different way: more efficient, but less transparent. The first thing I need is a pair of values for the median: one inside the interval and one outside. Let’s try 40 and 50:\n\npval_sign(40, times, time)\n\n[1] 0.00719738\n\npval_sign(50, times, time)\n\n[1] 1\n\n\nOK, so 40 is outside and 50 is inside. So what do I guess for the next value to try? I could do something clever like assuming that the relationship between hypothesized median and P-value is linear, and then guessing where that line crosses 0.05. But I’m going to assume nothing about the relationship except that it goes uphill, and therefore crosses 0.05 somewhere. So my next guess is halfway between the two values I tried before:\n\npval_sign(45, times, time)\n\n[1] 0.07835388\n\n\nSo, 45 is inside the interval, and my (slightly) improved guess at the bottom end of the interval is that it’s between 40 and 45. So next, I try halfway between those:\n\npval_sign(42.5, times, time)\n\n[1] 0.0266037\n\n\n42.5 is outside, so the bottom end of the interval is between 42.5 and 45.\nWhat we are doing is narrowing down where the interval’s bottom end is. We started by knowing it to within 10, and now we know it to within 2.5. So if we keep going, we’ll know it as accurately as we wish.\nThis is called a “bisection” method, because at each step, we’re dividing our interval by 2.\nThere is one piece of decision-making at each step: if the P-value for the median you try is greater than 0.05, that becomes the top end of your interval (as when we tried 45); if it is less, it becomes the bottom end (when we tried 42.5).\nThis all begs to be automated into a loop. It’s not a for-type loop, because we don’t know how many times we’ll be going around. It’s a while loop: keep going while something is true. Here’s how it goes:\n\nlo &lt;- 40\nhi &lt;- 50\nwhile (abs(hi - lo) &gt; 0.1) {\n  try &lt;- (hi + lo) / 2\n  ptry &lt;- pval_sign(try, times, time)\n  print(c(try, ptry))\n  if (ptry &lt; 0.05) {\n    lo &lt;- try\n  } else {\n    hi &lt;- try\n  }\n}\n\n[1] 45.00000000  0.07835388\n[1] 42.5000000  0.0266037\n[1] 43.7500000  0.0266037\n[1] 44.37500000  0.07835388\n[1] 44.0625000  0.0266037\n[1] 44.2187500  0.0266037\n[1] 44.2968750  0.0266037\n\nlo\n\n[1] 44.29688\n\npval_sign(lo, times, time)\n\n[1] 0.0266037\n\nhi\n\n[1] 44.375\n\npval_sign(hi, times, time)\n\n[1] 0.07835388\n\n\nThe loop stopped because 44.297 and 44.375 are less than 0.1 apart. The first of those is outside the interval and the second is inside. So the bottom end of our interval is 44.375, to this accuracy. If you want it more accurately, change 0.1 in the while line to something smaller (but then you’ll be waiting longer for the answer).\nI put the print statement in the loop so that you could see what values were being tried, and what P-values they were producing. What happens with these is that the P-value jumps at each data value, so you won’t get a P-value exactly 0.05; you’ll get one above and one below.\nLikewise, you can use the function with a zero on its name and feed it a column rather than a data frame and a column name:\n\ntibble(meds =  seq(55, 58, 0.25)) %&gt;% \n  rowwise() %&gt;% \n  mutate(pvals =  with(times, pval_sign0(meds, time)))\n\n\n\n  \n\n\n\nOr adapt the idea I had above for bisection. All that was a lot of work, but I wanted you to see it all once, so that you know where the confidence interval is coming from. smmr also has a function ci_median that does all of the above without you having to do it. As I first wrote it, it was using the trial and error thing with rowwise, but I chose to rewrite it with the bisection idea, because I thought that would be more accurate.\n\nci_median(times, time)\n\n[1] 44.30747 57.59766\n\n\nThis is a more accurate interval than we got above. (The while loop for the bisection keeps going until the two guesses at the appropriate end of the interval are less than 0.01 apart, by default.)1\nIf you want some other confidence level, you add conf.level on the end, as you would for t.test:\n\nci_median(times, time, conf.level = 0.75)\n\n[1] 46.20444 55.49473\n\n\nA 75% CI, just for fun. This is a shorter interval than the 95% one, as it should be.\nLikewise there is a ci_median0 that takes a column and an optional confidence level:\n\nwith(times, ci_median0(time))\n\n[1] 44.30747 57.59766\n\nwith(times, ci_median0(time, conf.level = 0.75))\n\n[1] 46.20444 55.49473\n\n\nwith the same results. Try ci_median first, and if it doesn’t work, try ci_median0.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "sign.html#chocolate-chips-1",
    "href": "sign.html#chocolate-chips-1",
    "title": "7  The sign test",
    "section": "7.7 Chocolate chips",
    "text": "7.7 Chocolate chips\nA famous cookie manufacturer claims that their bags of chocolate chip cookies contain “more than 1100 chocolate chips on average”. A diligent group of students buys 16 bags of these cookies and counts the number of chocolate chips in each bag. The results are in http://ritsokiguess.site/datafiles/chips.txt.\n\nRead in and display (some of) the data.\n\nSolution\nI’ll pretend it’s a .csv this time, just for fun. Give the data frame a name different from chips, so that you don’t get confused:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/chips.txt\"\nbags &lt;- read_csv(my_url)\n\nRows: 16 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): chips\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbags\n\n\n\n  \n\n\n\nThat looks sensible.\n\\(\\blacksquare\\)\n\nBuild your own sign test in R for testing that the median is 1100 chocolate chips, against the alternative that it is greater. (Do this as in class: count the appropriate thing, compare it with an appropriate binomial distribution, and obtain a P-value.\n\nSolution\nThe null median is 1100, so we count the number of values above and below:\n\nbags %&gt;% count(chips&lt;1100)\n\n\n\n  \n\n\n\nThe un-standard thing there is that we can put a logical condition directly into the count. If you don’t think of that, you can also do this, which creates a new variable less that is TRUE or FALSE for each bag appropriately:\n\nbags %&gt;% mutate(less=(chips&lt;1100)) %&gt;% count(less)\n\n\n\n  \n\n\n\nor the more verbose\n\nbags %&gt;% mutate(less=(chips&lt;1100)) %&gt;%\ngroup_by(less) %&gt;% summarize(howmany=n())\n\n\n\n  \n\n\n\nJust one value below, with all the rest above. Getting the right P-value, properly, requires some careful thought (but you will probably get the right answer anyway). If the alternative hypothesis is true, and the median is actually bigger than 1100 (say, 1200), you would expect half the data values to be bigger than 1200 and half smaller. So more than half the data values would be bigger than 1100, and fewer than half of them would be less than 1100. So, if we are going to reject the null (as it looks as if we will), that small number of values below 1100 is what we want.\nThe P-value is the probability of a value 1 or less in a binomial distribution with \\(n=16, p=0.5\\):\n\nsum(dbinom(0:1,16,0.5))\n\n[1] 0.0002593994\n\n\nOr, equivalently, count up from 15:\n\nsum(dbinom(15:16,16,0.5))\n\n[1] 0.0002593994\n\n\nThis is correctly one-sided, so we don’t have to do anything with it.\n\\(\\blacksquare\\)\n\nUse my R package smmr to reproduce your sign test above, and verify that you get consistent results. (See the maze-design question for instructions on installing this, if you haven’t yet.)\n\nSolution\nThis will mean reading the output carefully:\n\nlibrary(smmr)\nsign_test(bags,chips,1100)\n\n$above_below\nbelow above \n    1    15 \n\n$p_values\n  alternative      p_value\n1       lower 0.9999847412\n2       upper 0.0002593994\n3   two-sided 0.0005187988\n\n\nThis time, we’re doing a one-sided test, specifically an upper-tail test, since we are looking for evidence that the median is greater than 1100. The results are exactly what we got “by hand”: 15 values above and one below, and a P-value (look along the upper line) of 0.00026. The two-sided P-value of 0.00052 rounds to the same 0.0005 as SAS got.\nAlternatively, you can do this:\n\nsign_test0(bags$chips,1100)\n\n$above_below\nbelow above \n    1    15 \n\n$p_values\n  alternative      p_value\n1       lower 0.9999847412\n2       upper 0.0002593994\n3   two-sided 0.0005187988\n\n\nwith the same result (but only go this way if you need to).\n\\(\\blacksquare\\)\n\nUse smmr to obtain a 95% confidence interval for the median number of chocolate chips per bag of cookies.\n\nSolution\nOnce everything is in place, this is simplicity itself:\n\nci_median(bags,chips)\n\n[1] 1135.003 1324.996\n\n\n1135 to 1325. I would round these off to whole numbers, since the data values are all whole numbers. These values are all above 1100, which supports the conclusion we got above that the median is above 1100. This is as it should be, because the CI is “all those medians that would not be rejected by the sign test”.\nOr,\n\nci_median0(bags$chips)\n\n[1] 1135.003 1324.996\n\n\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "sign.html#the-power-of-the-sign-test-1",
    "href": "sign.html#the-power-of-the-sign-test-1",
    "title": "7  The sign test",
    "section": "7.8 The power of the sign test",
    "text": "7.8 The power of the sign test\nI’ve mentioned several times that the sign test has less power than the \\(t\\)-test. Let’s investigate this with a specific example.\nLet’s suppose we are testing \\(H_0: \\mu=40\\) against \\(H_a: \\mu \\ne 40\\), where \\(\\mu\\) is the population mean (and median, as we shall see). Our population actually has a normal distribution with mean 50 and SD 15, so that the null hypothesis is wrong and we want to reject it most of the time. On the other hand, the population actually is normally-distributed and so the \\(t\\)-test is the right one to use.\n(This is an old question, so I tackle the simulated power differently than I did it in class this time. But see if you can follow what I do here.)\n\nUse power.t.test to find the probability that a \\(t\\)-test correctly rejects the null hypothesis using a sample size of \\(n=10\\).\n\nSolution\n\npower.t.test(delta=50-40,n=10,sd=15,type=\"one.sample\",alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 10\n          delta = 10\n             sd = 15\n      sig.level = 0.05\n          power = 0.4691805\n    alternative = two.sided\n\n\nThe power is 0.469. Not great, but we’ll see how this stacks up against the sign test.\n\\(\\blacksquare\\)\n\nWhat code in R would draw a random sample of size 10 from the true population distribution and save the sample in a variable?\n\nSolution\nThe data actually have a normal distribution with mean 50 and SD 15, so we use rnorm with this mean and SD, obtaining 10 values:\n\nx=rnorm(10,50,15)  \nx\n\n [1] 30.17079 64.00529 49.19231 68.83447 59.37662 57.36909 53.65464 66.93529\n [9] 50.22649 45.52094\n\n\n\\(\\blacksquare\\)\n\nWhat code would count how many of the sampled values are less than 40 and how many are greater (or equal)?\n\nSolution\nThe way we know this is to put x into a data frame first:\n\ntibble(x) %&gt;% count(x&lt;40)\n\n\n\n  \n\n\n\n2 values less (and 8 greater-or-equal).\n\\(\\blacksquare\\)\n\nIt turns out the sign test would reject \\(H_0: M=40\\) against \\(H_a: M \\ne 40\\) at \\(\\alpha=0.05\\) if the smaller of the numbers in the last part is 1 or less. (\\(M\\) is the population median.) Add to your pipeline to obtain TRUE if you should reject the null for your data and FALSE otherwise.\n\nSolution\nThis is actually easier than you might think. The output from count is a data frame with a column called n, whose minimum value you want. I add to my pipeline:\n\ntibble(x) %&gt;% count(x&lt;40) %&gt;%\nsummarize(the_min=min(n)) %&gt;%\nmutate(is_rejected=(the_min&lt;=1))\n\n\n\n  \n\n\n\nThis will fail sometimes. If all 10 of your sample values are greater than 40, which they might turn out to be, you’ll get a table with only one line, FALSE and 10; the minimum of the n values is 10 (since there is only one), and it will falsely say that you should not reject. The fix is\n\ntibble(x) %&gt;% count(x&lt;40) %&gt;%\nsummarize(the_min=min(n)) %&gt;%\nmutate(is_rejected=(the_min&lt;=1 | the_min==10))\n\n\n\n  \n\n\n\nThe above is almost the right thing, but not quite: we only want that value that I called is_rejected, rather than the whole data frame, so a pull will grab it:\n\ntibble(x) %&gt;% count(x&lt;40) %&gt;%\nsummarize(the_min=min(n)) %&gt;%\nmutate(is_rejected=(the_min&lt;=1 | the_min==10)) %&gt;%\npull(is_rejected)\n\n[1] TRUE\n\n\nYou might be wondering where the “1 or less” came from. Getting a P-value for the sign test involves the binomial distribution: if the null is correct, each data value is independently either above or below 40, with probability 0.5 of each, so the number of values below 40 (say) is binomial with \\(n=10\\) and \\(p=0.5\\). The P-value for 1 observed value below 40 and the rest above is\n\n2*pbinom(1,10,0.5)  \n\n[1] 0.02148438\n\n\nwhich is less than 0.05; the P-value for 2 values below 40 and the rest above is\n\n2*pbinom(2,10,0.5)    \n\n[1] 0.109375\n\n\nwhich is bigger than 0.05.\nYou might have encountered the term “critical region” for a test. This is the values of your test statistic that you would reject the null hypothesis for. In this case, the critical region is 1 and 0 observations below 40, along with 1 and 0 observations above 40.\nWhen you’re thinking about power, I think it’s easiest to think in terms of the critical region (rather than directly in terms of P-values) since you have a certain \\(\\alpha\\) in mind all the way through, 0.05 in the power examples that I’ve done. The steps are then:\n\nWork out the critical region for your test, the values of the test statistic (or sample mean or sample count) that would lead to rejecting the null hypothesis.\nUnder your particular alternative hypothesis, find the probability of falling into your critical region.\n\nWhen I say “work out”, I mean either calculating (along the lines of STAB57), or simulating, as we have done here.\n\\(\\blacksquare\\)\n\nSimulate the above process 1000 times: draw a random sample from a normal distribution of size 10 with mean 50 and SD 15, count the number of values below 40, reject if the minimum of those is 0, 1, 9, or 10, then count the number of rejections out of 1000.\n\nSolution\nSet up a dataframe with a column (called, maybe, sim) that counts the number of simulations you are doing, and then use rowwise to take a random sample in each row and extract what you need from it.\nI start with setting the random number seed, so it comes out the same each time. That way, if I rerun the code, my answers are the same (and I don’t have to change my discussion of them.)\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rnorm(10, 50, 15)))\n\n\n\n  \n\n\n\nEach sample has 10 values in it, not just one, so you need the list around the rnorm. Note that sample is labelled as a list-column.\nNow we have to count how many of the sample values are less than 40:\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rnorm(10, 50, 15))) %&gt;% \n  mutate(less = list(sample&lt;40)) %&gt;% \n  mutate(counted = sum(less)) \n\n\n\n  \n\n\n\nThis is a bit of a programmer’s trick. In R, less contains a vector of 10 TRUE or FALSE values, according to whether the corresponding value in sample is less than 40 or not. In R (and many other programming languages), the numeric value of TRUE is 1 and of FALSE is 0, so you count how many TRUE values there are by adding them up. To verify that this worked, we should unnest sample and less:\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rnorm(10, 50, 15))) %&gt;% \n  mutate(less = list(sample&lt;40)) %&gt;% \n  mutate(counted = sum(less)) %&gt;% \n  unnest(c(sample, less))\n\n\n\n  \n\n\n\nIn the first sample, 38.8, 39.5, and 33.8 are less than 40, correctly identified so in less, and the counted column shows that the first sample did indeed have 3 values less than 40. You can check a few of the others as well, enough to convince yourself that this is working.\nNext, the sign test will reject if there are 0, 1, 9 or 10 values less than 40 (you might be guessing that the last two will be pretty unlikely), so make a column called reject that encapsulates that, and then count how many times you rejected in your simulations. I don’t need my unnest any more; that was just to check that everything was working so far:\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rnorm(10, 50, 15))) %&gt;% \n  mutate(less = list(sample&lt;40)) %&gt;% \n  mutate(counted = sum(less)) %&gt;% \n  mutate(reject = (counted&lt;=1 | counted &gt;= 9)) %&gt;% \n  count(reject)\n\n\n\n  \n\n\n\nMy simulated power is 0.243\nThis is all liable to go wrong the first few times, so make sure that each line works before you go on to the next, as I did. While you’re debugging, try it with a small number of random samples like 5. (It is smart to have a variable called nsim which you set to a small number like 5 when you are testing, and than to 1000 when you run the real thing, so that the first line of the pipeline is then tibble(sim = 1:nsim).)\nIf you were handing something like this in, I would only want to see your code for the final pipeline that does everything, though you could and should have some words that describe what you did.\nI’m now thinking a better way to do this is to write a function that takes a sample (in a vector) and returns a TRUE or FALSE according to whether or not a median of 40 would be rejected for that sample:\n\nis_reject=function(x) {\n  tibble(x=x) %&gt;%\n    mutate(counted = (x &lt; 40)) %&gt;% \n    summarize(below = sum(counted)) %&gt;% \n    summarize(is_rejected = (below&lt;=1 | below&gt;=9)) %&gt;% \n    pull(is_rejected)\n}\nis_reject(c(35, 45, 55))\n\n[1] TRUE\n\nis_reject(c(35, 38, 45, 55))\n\n[1] FALSE\n\n\nNow, we have to use that:\n\nset.seed(457299)\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rnorm(10, 50, 15))) %&gt;% \n  mutate(reject = is_reject(sample)) %&gt;% \n  count(reject)\n\n\n\n  \n\n\n\nThis is a bit cleaner because the process of deciding whether each sample leads to a rejection of the median being 40 has been “outsourced” to the function, and the pipeline with the rowwise is a lot cleaner: take a sample, decide whether that sample leads to rejection, and count up the rejections.\n\\(\\blacksquare\\)\n\nWhich is more powerful in this case, the sign test or the \\(t\\)-test? How do you know?\n\nSolution\nThe power of the sign test is estimated as 0.243, which is quite a bit less than the power of the \\(t\\)-test, which we found back in (a) to be 0.469. So the \\(t\\)-test, in this situation where it is valid, is the right test to use: it is (i) valid and (ii) more powerful. So the \\(t\\)-test is more powerful. One way to think about how much more powerful is to ask “how much smaller of a sample size would be needed for the \\(t\\)-test to have the same power as this sign test?” The power of my sign test was 0.243, so in power.t.test we set power equal to that and omit the sample size n:\n\npower.t.test(delta=50-40,power=0.243,sd=15,type=\"one.sample\",alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 5.599293\n          delta = 10\n             sd = 15\n      sig.level = 0.05\n          power = 0.243\n    alternative = two.sided\n\n\nA sample of size 6 gives the same power for the \\(t\\)-test that a sample of size 10 does for the sign test. The ratio of these two sample sizes is called the relative efficiency of the two tests: in this case, the \\(t\\)-test is \\(10/6=1.67\\) times more efficient. The data that you have are being used “more efficiently” by the \\(t\\)-test. It is possible to derive2\nthe limiting relative efficiency of the \\(t\\) test relative to the sign test when the data are actually normal, as the sample size gets larger. This turns out not to depend on how far wrong the null is (as long as it is the same for both the \\(t\\)-test and the sign test). This “asymptotic relative efficiency” is \\(\\pi/2=1.57\\). Our relative efficiency for power 0.243, namely 1.67, was pretty close to this, even though our sample sizes 10 and 6 are not especially close to infinity. This says that, if your data are actually from a normal distribution, you do a lot better to use the \\(t\\)-test than the sign test, because the sign test is wasteful of data (it only uses above/below rather than the actual values).\nExtra: if your data are not from a normal distribution, then the story can be very different. Of course you knew I would investigate this. There is a distribution called the “Laplace” or “double exponential” distribution, that has very long tails.3 The distribution is not in base R, but there is a package called smoothmest that contains a function rdoublex to generate random values from this distribution. So we’re going to do a simulation investigation of the power of the sign test for Laplace data, by the same simulation technique that we did above. Like the normal, the Laplace distribution is symmetric, so its mean and median are the same (which makes our life easier).4\nLet’s test the hypothesis that the median is zero. We’ll suppose that the true median is 0.5 (this is called mu in rdoublex). The first problem we run into is that we can’t use power.t.test because they assume normal data, which we are far from having. So we have to do two simulations: one to simulate the power of the \\(t\\) test, and one to simulate the power of the sign test.\nTo simulate the \\(t\\) test, we first have to generate some Laplace data with the true mean of 0.5. We’ll use a sample size of 50 throughout these simulations.\n\nlibrary(smoothmest)\nrl &lt;- rdoublex(50,mu=0.5)\nrl\n\n [1] -0.33323285  0.70569291 -1.22513053  0.68517708  0.87221482  0.49250051\n [7]  0.26700527  1.90236874  0.53288312  1.37374732  0.72743434  0.46634071\n[13]  0.43581431 -0.01545866  0.18594908 -0.40403202 -0.13540289  0.83862694\n[19] -0.23360644 -0.74050354  2.92089551 -2.72173880  0.51571185  1.23636045\n[25]  0.82921382  1.72456334  0.07903058  0.74789589  0.90487190  2.52310082\n[31]  3.13629814  0.81851434  0.74615575 -0.26068744  2.70683355  1.46981530\n[37]  1.45646489  1.20232517  6.65249860 -0.51575026 -0.07606399  2.11338640\n[43] -1.20427995  1.70986104 -1.66466321  0.55346854  0.33908531  0.72100677\n[49]  0.92025176  0.98922656\n\n\nThis seems to have some unusual values, far away from zero:\n\ntibble(rl) %&gt;%\nggplot(aes(sample=rl))+\nstat_qq()+stat_qq_line()\n\n\n\n\nYou see the long tails compared to the normal.\nNow, we feed these values into t.test and see whether we reject a null median of zero (at \\(\\alpha=0.05\\)):\n\ntt &lt;- t.test(rl)  \ntt\n\n\n    One Sample t-test\n\ndata:  rl\nt = 3.72, df = 49, p-value = 0.0005131\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.3399906 1.1388911\nsample estimates:\nmean of x \n0.7394408 \n\n\nOr we can just pull out the P-value and even compare it to 0.05:\n\npval &lt;- tt$p.value  \npval\n\n[1] 0.0005130841\n\nis.reject &lt;- (pval&lt;=0.05)\nis.reject\n\n[1] TRUE\n\n\nThis one has a small P-value and so the null median of 0 should be (correctly) rejected.\nWe’ll use these ideas to simulate the power of the \\(t\\)-test for these data, testing a mean of 0. This uses the same ideas as for any power simulation; the difference here is the true distribution:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rdoublex(50, mu = 0.5))) %&gt;% \n  mutate(t_test = list(t.test(sample, mu = 0))) %&gt;% \n  mutate(t_pval = t_test$p.value) %&gt;% \n  count(t_pval &lt;= 0.05)\n\n\n\n  \n\n\n\nAnd now we simulate the sign test. Since what we want is a P-value from a vector, the easiest way to do this is to use pval_sign0 from smmr, which returns exactly the two-sided P-value that we want, so that the procedure is a step simpler:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(rdoublex(50, mu = 0.5))) %&gt;% \n  mutate(sign_pval = pval_sign0(0, sample)) %&gt;% \n  count(sign_pval &lt;= 0.05)\n\n\n\n  \n\n\n\nFor data from this Laplace distribution, the power of this \\(t\\)-test is 0.696, but the power of the sign test on the same data is 0.761, bigger. For Laplace-distributed data, the sign test is more powerful than the \\(t\\)-test.\nThis is not to say that you will ever run into data that comes from the Laplace distribution. But the moral of the story is that the sign test can be more powerful than the \\(t\\)-test, under the right circumstances (and the above simulation is the “proof” of that statement). So a blanket statement like “the sign test is not very powerful” needs to be qualified a bit: when your data come from a sufficiently long-tailed distribution, the sign test can be more powerful relative to the \\(t\\)-test than you would think.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "sign.html#ben-roethlisberger-1",
    "href": "sign.html#ben-roethlisberger-1",
    "title": "7  The sign test",
    "section": "7.9 Ben Roethlisberger",
    "text": "7.9 Ben Roethlisberger\nBen Roethlisberger plays (American) football for the Pittsburgh Steelers. He plays as a quarterback, which means that his job is to throw (pass) the ball so that one of his teammates can catch it. Each time he makes a pass that is caught, this is called a “completion”, and the team coaches are interested in his average number of completions per game (this average could be the mean or the median).\nIn 2010, Roethlisberger was suspended for the first four games of the season, and there was concern that this might affect his performance (in terms of the number of passes completed in the games after he returned). The Pittsburgh Steelers did not play in week 5 of the 2010 season; the season is 17 weeks long (one game per week) and each team has one week in which they do not play.\nThe data are here. There are four columns: the year (always 2010), the week number of the season that the game was played in, the name of the opposing team, and the number of completed passes by Roethlisberger in the game.\n\nRead in and display (some of) the data. Do you have what you were expecting?\n\nSolution\nReading in is the usual, noting that this is a .csv:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/roethlisberger.csv\"\nben &lt;- read_csv(my_url)\n\nRows: 12 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): opponent\ndbl (3): season, week, completed\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nben\n\n\n\n  \n\n\n\nSince “Roethlisberger” is a lot to type every time, I called the dataframe by his first name.\nI am showing all 12 rows here; you are probably seeing only 10, and will have to scroll down to see the last two.\nI have the four variables promised, and I also have a sensible number of rows. In particular, there is no data for weeks 1–4 (the suspension) and for week 5 (in which the team did not play), but there is a number of passes completed for all the other weeks of the season up to week 17. (If Roethlisberger had not played in any other games, you can expect that I would have told you about it.)\nExtra: I did some processing to get the data to this point. I wanted to ask you about the 2010 season, and that meant having the 2009 data to compare it with. So I went here, scrolled down to Schedule and Game Results, and clicked on each of the Boxscores to get the player stats by game. Then I made a note of the opponent and the number of passes completed, and did the same for 2010. I put them in a file I called r1.txt, in aligned columns, and read that in. (An alternative would have been to make a spreadsheet and save that as a .csv, but I already had R Studio open.) Thus:\n\nr0 &lt;- read_table(\"r1.txt\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  season = col_double(),\n  week = col_double(),\n  opponent = col_character(),\n  completed = col_double()\n)\n\nr0\n\n\n\n  \n\n\n\nI was curious about the season medians (for reasons you see later), thus:\n\nr0 %&gt;% group_by(season) %&gt;% summarise(med = median(completed))\n\n\n\n  \n\n\n\nYou will realize that my asserted average for “previous seasons” is close to the median for 2009. Here is where I have to admit that I cheated. It actually is the median for 2009, except that there are some games in 2010 where Roethlisberger had 22 completed passes and I didn’t want to mess the sign test up (I talk more about this later). So I made it 22.5, which is a possible value for the median of an even number of whole-number values.\nAnyway, the last thing to do is to grab only the rows for 2010 and save them for you. This uses filter to select only the rows for which something is true:\n\nlibrary(smmr)\nr0 %&gt;% filter(season==2010) -&gt; r1\nwrite_csv(r1, \"roethlisberger.csv\")\n\n\\(\\blacksquare\\)\n\nMake a suitable graph of the number of completed passes, and explain briefly why you would have some doubts about using \\(t\\)-procedures in this situation.\n\nSolution\nDon’t be tempted to think too hard about the choice of graph (though I talk more about this below). One quantitative variable, so a histogram again. There are only 12 observations, so 5 bins is about as high as you should go:\n\nggplot(ben, aes(x=completed)) + geom_histogram(bins=5)\n\n\n\n\nThis one shows an outlier: there is one number of completed passes that is noticeably higher than the rest. A normal distribution doesn’t have outliers, and so this, coupled with a small sample in which normality is important, means that we should not be using a \\(t\\)-test or confidence interval.\nIf you chose a different number of bins, you might get a different look. Here’s 4 bins:\n\nggplot(ben, aes(x=completed)) + geom_histogram(bins=4)\n\n\n\n\nThat looks more like right-skewness, but the conclusion is the same.\nExtra: if you have read, for example, Problem 6.1 in PASIAS, you’ll have seen that another possibility is a one-group boxplot. This might have been the context in which you first saw the boxplot, maybe at about the time you first saw the five-number summary, but I don’t talk about that so much in this course because ggplot boxplots have both an x and a y, and it makes more sense to think about using boxplots to compare groups. But, you can certainly get R to make you a one-sample boxplot. What you do is to set the grouping variable to a “dummy” thing like the number 1:\n\nggplot(ben, aes(x=1, y=completed)) + geom_boxplot()\n\n\n\n\nand then you ignore the \\(x\\)-axis.\nThis really shows off the outlier; it is actually much bigger than the other observations. It didn’t show up so much on the histograms because of where the bin boundaries happened to come. On the four-bin histogram, the highest value 30 was in the 27.5–32.5 bin, and the second-highest value 23 was at the bottom of the 22.5–27.5 bin. So the highest and second-highest values looked closer together than they actually were.\nIf you have been reading ahead, you might also be thinking about a normal quantile plot. That is for specifically assessing normality, and here this is something that interests us, because a \\(t\\)-test will be doubtful if the normality fails:\n\nggplot(ben, aes(sample=completed)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis again shows off the outlier at the high end. It is a reasonable choice of plot here because normality is of specific interest to us.\nA note: you are absolutely not required to read ahead to future lectures. Each assignment can be done using the material in the indicated lectures only. If you want to use something from future lectures, go ahead, but make sure you are using it appropriately.\nDon’t be tempted to plot the number of completed passes against something like week number:\n\nggplot(ben, aes(x=week, y=completed)) + geom_point()\n\n\n\n\nThat is quite interesting (a mostly increasing trend over weeks, with the outlier performance in week 10), but it doesn’t tell us what we want to know here: namely, is a \\(t\\)-test any good?\n\\(\\blacksquare\\)\n\nRun a sign test to compare Roethlisberger’s performance in 2010 with his previous average of 22.5 completions per game. What do you conclude?\n\nSolution\nUse smmr, dataframe, column, null median:\n\nsign_test(ben, completed, 22.5)\n\n$above_below\nbelow above \n   10     2 \n\n$p_values\n  alternative    p_value\n1       lower 0.01928711\n2       upper 0.99682617\n3   two-sided 0.03857422\n\n\nI am looking for any change, so for me, a two-sided test is appropriate. If you think this is one-sided, make a case for your side, and then go ahead.\nMy P-value is 0.039, so I can reject the null hypothesis (that the median number of passes completed is 22.5) and conclude that it has changed in 2010.\n(You might hypothesize that this is the result of a decrease in confidence, that he is either throwing fewer passes, or the ones that he is throwing are harder to catch. If you know about football, you might suspect that Roethlisberger was actually passing too much, including in situations where he should have handing off to the running back, instead of reading the game appropriately.)\nExtra: I said above that I cheated and made the null median 22.5 instead of 22. What happens if we make the null median 22?\n\nsign_test(ben, completed, 22)\n\n$above_below\nbelow above \n    8     2 \n\n$p_values\n  alternative   p_value\n1       lower 0.0546875\n2       upper 0.9892578\n3   two-sided 0.1093750\n\n\nFor one thing, the result is no longer significant. But looking at the table of values above and below reveals something odd: there are only ten values. What happened to the other two? What happened is that two of the data values were exactly equal to 22, so they are neither above nor below. In the sign test, they are thrown away, so that we are left with 8 values below 22 and 2 above.\nI didn’t want to make you wonder what happened, so I made the null median 22.5.\n\\(\\blacksquare\\)\n\nWhy might you have expected your sign test to come out significant, even without looking at the P-value? Explain briefly.\n\nSolution\nThe other ingredient to the sign test is how many data values are above and below the null median. You can look at the output from sign_test (the first part), or count them yourself:\n\nben %&gt;% count(completed&lt;22.5)\n\n\n\n  \n\n\n\nYou can put a logical condition (something that can be true or false) into count, or you can create a new column using ifelse (which I think I showed you somewhere):\n\nben %&gt;% mutate(side = ifelse(completed&lt;22.5, \"below\", \"above\")) %&gt;% \ncount(side)\n\n\n\n  \n\n\n\nWhichever way you do it, there seem to be a lot more values below than above, very different from a 50–50 split. Even with only 12 observations, this turns out to be enough to be significant. (If you tossed a fair coin 12 times, would you be surprised to get only 2 heads or 2 tails?)\n\\(\\blacksquare\\)\n\nObtain a 90% confidence interval for the median number of completed passes (over “all possible games played by 2010 Ben Roethlisberger”).\n\nSolution\nThis is ci_median, but with conf.level since you are not using the default level of 95%:\n\nci_median(ben, completed, conf.level = 0.90)\n\n[1] 17.00244 21.99878\n\n\n17 to 22 completed passes.\nExtra: the P-value of the sign test only changes (as the null median changes) when you get to a data point; otherwise, the number of values above and below will stay the same, and the P-value will stay the same. The data values here were all whole numbers, so the limits of the confidence interval are also whole numbers (to the accuracy of the bisection), so the interval really should be rounded off.\n\\(\\blacksquare\\)\n\nFind a 90% confidence interval for the mean number of passes completed, and explain briefly why it differs from the one for the median in the way that it does.\n\nSolution\nAll right, get the interval for the mean first:\n\nwith(ben, t.test(completed, conf.level = 0.90))\n\n\n    One Sample t-test\n\ndata:  completed\nt = 17.033, df = 11, p-value = 2.971e-09\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 17.89124 22.10876\nsample estimates:\nmean of x \n       20 \n\n\nThe 95% confidence interval for the mean goes from 17.9 to 22.1 (completions per game).\nThis is higher at both ends than the interval for the median, though possibly not as much as I expected. This is because the mean is made higher by the outlier (compared to the median), and so the CI procedure comes to the conclusion that the mean is higher.\nExtra: this is one of those cases where the bootstrap might shed some light on the sampling distribution of the sample mean:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(ben$completed, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 10)\n\n\n\n\nThis is noticeably skewed to the right (it goes further up from the peak than down), which is why the CI for the mean went up a bit higher than the one for the median.\nFinally, bootstrapping the median is not something you’d want to do, since the sign test doesn’t depend on anything being normally-distributed. This is a good thing, since bootstrapping the sample median is weird:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(ben$completed, replace = TRUE))) %&gt;% \n  mutate(my_median = median(my_sample)) %&gt;% \n  ggplot(aes(x = my_median)) + geom_histogram(bins = 30)\n\n\n\n\nThe “holes” in the distribution comes about because there are not all that many different possible sample medians when you sample with replacement. For one thing, the values are all whole numbers, so the median can only be something or something and a half. Even then, the bar heights look kind of irregular.\nI used a large number of bins to emphasize this, but even a more reasonable number looks strange:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(ben$completed, replace = TRUE))) %&gt;% \n  mutate(my_median = median(my_sample)) %&gt;% \n  ggplot(aes(x = my_median)) + geom_histogram(bins = 10)\n\n\n\n\nA sample median of 19 or 20 is more likely than one of 19.5.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "sign.html#six-ounces-of-protein-1",
    "href": "sign.html#six-ounces-of-protein-1",
    "title": "7  The sign test",
    "section": "7.10 Six ounces of protein",
    "text": "7.10 Six ounces of protein\nA company produces prepackaged diet meals. These meals are advertised as containing “6 ounces of protein per package”. A consumer organization is concerned that this is not accurate. The organization takes a random sample of 20 of these meals, and measures the protein content of each one. The data are in http://ritsokiguess.site/datafiles/protein.txt as one column.\n\nRead in and display (some of) the data.\n\nSolution\nThe usual. This is one column only, so you can pretend the columns are separated by anything at all and it will still work:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/protein.txt\"\nmeals &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  protein = col_double()\n)\n\nmeals %&gt;% arrange(protein)\n\n\n\n  \n\n\n\nGet it to work via one of the methods you’ve seen in this class (ie., not read.table); I don’t mind how you manage it.\n\\(\\blacksquare\\)\n\nMake a suitable graph of your data.\n\nSolution\nOne quantitative variable, so a histogram with a sufficiently small number of bins:\n\nggplot(meals, aes(x=protein)) + geom_histogram(bins = 5)\n\n\n\n\n\\(\\blacksquare\\)\n\nWhy might a sign test be better than a \\(t\\)-test for assessing the average amount of protein per package? Explain briefly. (“Average” here means any measure of centre.)\n\nSolution\nThe shape of the above distribution is skewed to the left, and not symmetric like a normal distribution. (If you say “the histogram is not normal”, make sure you also say how you know.) This means that the median would be a better measure of “average” (that is, centre) than the mean is, because the mean would be pulled downwards by the long tail, and the median would not. To complete the story, the sign test is a test of the median, so the sign test would be better than the \\(t\\)-test, which is a test of the mean.\nThe other thing you might consider is the sample size, 20, which might be large enough to overcome this amount of skewness, but then again it might not be. So you could say that we should be cautious and run the sign test here instead.\n\\(\\blacksquare\\)\n\nRun a suitable sign test for these data. What do you conclude?\n\nSolution\nFirst, if you have not already done so, install smmr following the instructions in the lecture notes. (This one is not just install.packages.) Then, make sure you have a library(smmr) somewhere above where you are going to use something from it. Once that is in place, remember what we were interested in: was the median protein content 6 ounces, or is there evidence that it is something different? (The “not accurate” in the question says that the median could be higher or lower, either of which would be a problem, and so we need a two-sided alternative.) Thus, the null median is 6 and we need a two-sided test, which goes this way:\n\nsign_test(meals, protein, 6)\n\n$above_below\nbelow above \n   15     5 \n\n$p_values\n  alternative    p_value\n1       lower 0.02069473\n2       upper 0.99409103\n3   two-sided 0.04138947\n\n\nThe P-value, 0.0414, is less than 0.05, so we reject the null hypothesis and conclude that the median is different from 6 ounces. The advertisement by the company is not accurate.\nMake sure you give the actual P-value you are comparing with 0.05, since otherwise your answer is incomplete. That is, you need to say more than just “the P-value is less than 0.05”; there are three P-values here, and only one of them is the right one.\nExtra: we already decided that a \\(t\\)-test is not the best here, but I am curious as to how different its P-value is:\n\nwith(meals, t.test(protein, mu=6))\n\n\n    One Sample t-test\n\ndata:  protein\nt = -4.2312, df = 19, p-value = 0.000452\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 4.946263 5.643737\nsample estimates:\nmean of x \n    5.295 \n\n\nThe conclusion is the same, but the P-value is a lot smaller. I don’t think it should really be this small; this is probably because the mean is pulled down by the left skew and so really ought not to look so far below 6. I am inclined to think that if the \\(t\\)-test were correct, its P-value ought to be between this and the one from the sign test, because the \\(t\\)-test uses the actual data values, and the sign test uses the data less efficiently (only considering whether each one is above or below the null median).\n\\(\\blacksquare\\)\n\nIn your sign test, how could you have deduced that the P-value was going to be small even without looking at any of the P-values themselves? Explain briefly.\n\nSolution\nLook at the other part of the output, the count of values above and below the null median. (You might have to click on “R Console” to see it.) If the null hypothesis was correct and the median was really 6, you’d expect to see about half the data values above 6 and about half below. But that is not what happened: there were 15 values below and only 5 above. Such an uneven split is rather unlikely if the null hypothesis was correct. So we would guess that our P-value would be small, as indeed it is.\n\\(\\blacksquare\\)\n\nObtain a 90% confidence interval for the population median protein content. What does this tell you about the reason for the rejection or non-rejection of the null hypothesis above?\n\nSolution\nThis is ci_median, but you need to be paying attention: it’s not the default 95% confidence level, so you have to specify that as well:\n\nci_median(meals, protein, conf.level = 0.90)\n\n[1] 4.905273 5.793750\n\n\nThe interval goes from 4.91 to 5.79. (The data values have one decimal place, so you could justify two decimals in the CI for the median, but anything beyond that is noise and you shouldn’t give it in your answer.5)\nThis interval is entirely below 6 (the null median), so evidently the reason that we rejected 6 as the population median is that the actual population median is less than 6.\nExtra: the CI for the median is not that different from the one for the mean, which suggests that maybe the \\(t\\)-test was not so bad after all. If you want to investigate further, you can try finding a bootstrapped sampling distribution of the sample mean, and see how non-normal it looks:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(meals$protein, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) -&gt; d\nggplot(d, aes(x = my_mean)) + geom_histogram(bins = 10)\n\n\n\n\nThat is pretty close to normal. So the \\(t\\)-test would in actual fact have been fine. To confirm, a normal quantile plot of the bootstrapped sampling distribution:\n\nggplot(d, aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nA tiny bit skewed to the left.\nBut I didn’t ask you to do this, because I wanted to give you a chance to do a sign test for what seemed like a good reason.\nExtra 2: I mentioned in an note that the endpoints of the CI for the median are actually data points, only we didn’t see it because of the accuracy to which ci_median was working. You can control this accuracy by an extra input tol. Let’s do something silly here:\n\nci_median(meals, protein, conf.level = 0.90, tol = 0.00000001)\n\n[1] 4.900001 5.799999\n\n\nThis takes a bit longer to run, since it has to get the answer more accurately, but now you can see how the interval goes from “just over 4.9” to “just under 5.8”, and it actually makes the most sense to give the interval as “4.9 to 5.8” without giving any more decimals.\nExtra 3: the reason for the confidence interval endpoints to be data values is that the interval comes from inverting the test: that is, finding the values of the population median that would not be rejected by a sign test run on our data. Recall how the sign test works: it is based on a count of how many data values are above and below the hypothesized population median. These counts are only going to change as the hypothesized median changes if you hit a data point, since that’s the only way you can change how many values are above and below.6 Thus, the only places where changing the null median changes whether or not a value for it is inside or outside the confidence interval are at data values, and thus the ends of the interval must be at (or, perhaps more strictly, just above or below) data values.\nThis is a peculiarity of using the sign test to get a CI for the median. If, say, you were to invert the \\(t\\)-test to get a confidence interval for the mean, you wouldn’t see that. (This is in fact exactly what you do to get a confidence interval for the mean, but this is not the way it is usually introduced.) The reason that the CI for the mean (based on the \\(t\\)-test) is different from the one for the median (based on the sign test) is that if you change the null hypothesis in the \\(t\\)-test, however slightly, you change the P-value (maybe only slightly, but you change it). So the CI for the mean, based on the \\(t\\)-test, is not required to have data points at its ends, and indeed usually does not. The difference is in the kind of distribution the test statistic has; the \\(t\\)-distribution is continuous, while the sign test statistic (a count of the number of values above or below something) is discrete. It’s the discreteness that causes the problems.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "sign.html#footnotes",
    "href": "sign.html#footnotes",
    "title": "7  The sign test",
    "section": "",
    "text": "You can change this by adding something like tol=1e-4 to the end of your ci-median.↩︎\nMeaning, I forget how to do it. But it has something to do with looking at alternatives that are very close to the null.↩︎\nIf you’ve ever run into the exponential distribution, you’ll recall that this is right skewed with a very long tail. The Laplace distribution looks like two of these glued back to back.↩︎\nThis is about the only way in which the normal and Laplace distributions are alike.↩︎\nThere is actually slightly more to it here: the ends of this confidence interval for the median are always data values, because of the way it is constructed, so the actual end points really ought to be given to the same number of decimals as the data, here 4.9 to 5.8. The output given is not exactly 4.9 and 5.8 because of inaccuracy in the bisection.↩︎\nThere is a technicality about what happens when the null median is exactly equal to a data value; see PASIAS for more discussion on this.↩︎"
  },
  {
    "objectID": "mood-median.html#sugar-in-breakfast-cereals",
    "href": "mood-median.html#sugar-in-breakfast-cereals",
    "title": "8  Mood median test",
    "section": "8.1 Sugar in breakfast cereals",
    "text": "8.1 Sugar in breakfast cereals\nThe data for this question are in http://ritsokiguess.site/datafiles/cereal-sugar.txt. The story here is whether breakfast cereals marketed to children have a lot of sugar in them; in particular, whether they have more sugar on average than cereals marketed to adults.\n\nRead in the data (to R) and display the data set. Do you have a variable that distinguishes the children’s cereals from the adults’ cereals, and another that contains the amount of sugar?\nCalculate the mean sugar content for each group of cereals (the adults’ ones and the children’s ones). Do they look similar or different?\nMake side-by-side boxplots of the sugar contents of the two types of cereal. What do you see that is out of the ordinary?\nExplain briefly why you would not trust a two-sample \\(t\\)-test with these data. (That is, say what the problem is, and why it’s a problem.)\nRun a suitable test to see whether the “typical” amount of sugar differs between adult’s and children’s cereals. Justify the test that you run. (You can use the version of your test that lives in a package, if that is easier for you.) What do you conclude, in the context of the data?"
  },
  {
    "objectID": "mood-median.html#fear-of-math",
    "href": "mood-median.html#fear-of-math",
    "title": "8  Mood median test",
    "section": "8.2 Fear of math",
    "text": "8.2 Fear of math\nTwo new short courses have been proposed for helping students who suffer from severe math phobia. The courses are labelled A and B. Ten students were randomly allocated to one of these two courses, and each student’s score on a math phobia test was recorded after they completed their course. The math phobia test produces whole-number scores between 0 and 10, with a higher score indicating a greater fear of mathematics. The data can be found in link. We start with R for this question.\n\nRead in the data and check, however you like, that you have 10 observations, 5 from each course.\nDo a two-sample \\(t\\)-test to assess whether there is a difference in mean phobia scores after the students have taken the two courses. What do you conclude? (You have no 1 reason to suppose that a particular one of the tests will produce a higher mean than the other, so do a two-sided test.)\nDraw boxplots of the math phobia scores for each group (one line of code). What is the most striking thing that you notice?\nExplain briefly why a \\(t\\)-test would not be good for these data. (There are two things that you need to say.)\nRun a suitable test to compare the “typical” scores for the two courses. (You can use the version from a package rather than building your own.) What do you conclude?"
  },
  {
    "objectID": "mood-median.html#medical-instructions",
    "href": "mood-median.html#medical-instructions",
    "title": "8  Mood median test",
    "section": "8.3 Medical instructions",
    "text": "8.3 Medical instructions\nDo people understand medical instructions better at certain times of the day? In a study, students in a grade 12 class are randomly divided into two groups, A and B. All students see a video describing how to use an infant forehead thermometer. The students in Group A see the video at 8:30 am, while the students in Group B see the same video at 3:00 pm (on the same day). The next day, all the students are given a test on the material in the video (graded out of 100). The observed scores are in link (values separated by spaces).\n\nRead the data into R and display the (first ten) values.\nObtain a suitable plot that will enable you to assess the assumptions for a two-sample \\(t\\)-test.\nWhy might you have doubts about using a two-sample \\(t\\)-test here?\nRun Mood’s median test as in class (without using smmr). What do you conclude, in the context of the data? What recommendation would you make about the time of day to see the video? (You might get a warning about “chisquared approximation being incorrect”, which you can ignore here.)\nRun Mood’s median test on these data using my smmr package, and verify that you get the same answer."
  },
  {
    "objectID": "mood-median.html#handspans-revisited",
    "href": "mood-median.html#handspans-revisited",
    "title": "8  Mood median test",
    "section": "8.4 Handspans revisited",
    "text": "8.4 Handspans revisited\nTake your right hand, and stretch the fingers out as far as you can. The distance between the tip of your thumb and the tip of your little (pinky) finger is your handspan. The students in a Statistics class at Penn State measured their handspans and also whether they identified as male or female. The data are at http://ritsokiguess.site/datafiles/handspan.txt, with handspans measured in inches. We want to see whether male students have a larger mean handspan than female students.\n\nRead in and display (some of) the data.\nMake a suitable (facetted) normal quantile plot of the data. (Bear in mind what is supposed to have a normal distribution.)\nDiscuss briefly whether you might prefer to use Mood’s median test to compare the handspans of the male and female students, compared to a two-sample \\(t\\)-test.\nRun Mood’s median test. What do you conclude from the test, in the context of the data?\n\nMy solutions follow:"
  },
  {
    "objectID": "mood-median.html#sugar-in-breakfast-cereals-1",
    "href": "mood-median.html#sugar-in-breakfast-cereals-1",
    "title": "8  Mood median test",
    "section": "8.5 Sugar in breakfast cereals",
    "text": "8.5 Sugar in breakfast cereals\nThe data for this question are in http://ritsokiguess.site/datafiles/cereal-sugar.txt. The story here is whether breakfast cereals marketed to children have a lot of sugar in them; in particular, whether they have more sugar on average than cereals marketed to adults.\n\nRead in the data (to R) and display the data set. Do you have a variable that distinguishes the children’s cereals from the adults’ cereals, and another that contains the amount of sugar?\n\nSolution\n\nmy_url=\"http://ritsokiguess.site/datafiles/cereal-sugar.txt\"\ncereals=read_delim(my_url,\" \")\n\nRows: 40 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): who\ndbl (1): sugar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncereals\n\n\n\n  \n\n\n\nThe variable who is a categorical variable saying who the cereal is intended for, and the variable sugar says how much sugar each cereal has.\n\\(\\blacksquare\\)\n\nCalculate the mean sugar content for each group of cereals (the adults’ ones and the children’s ones). Do they look similar or different?\n\nSolution\ngroup_by and summarize:\n\ncereals %&gt;% group_by(who) %&gt;%\nsummarize(sugar_mean=mean(sugar))\n\n\n\n  \n\n\n\nThese means look very different, though it would be better to look at a boxplot (coming up in a moment).\n\\(\\blacksquare\\)\n\nMake side-by-side boxplots of the sugar contents of the two types of cereal. What do you see that is out of the ordinary?\n\nSolution\nThe usual:\n\nggplot(cereals,aes(x=who,y=sugar))+geom_boxplot()\n\n\n\n\nI see outliers: two high ones on the adults’ cereals, and one high and one low on the children’s cereals.\nMy thought above about the means being very different is definitely supported by the medians being very different on the boxplots. We should have no trouble declaring that the “typical” amounts of sugar in the adults’ and children’s cereals are different.\n\\(\\blacksquare\\)\n\nExplain briefly why you would not trust a two-sample \\(t\\)-test with these data. (That is, say what the problem is, and why it’s a problem.)\n\nSolution\nThe problem is the outliers (which is rather a giveaway), but the reason it’s a problem is that the two-sample \\(t\\)-test assumes (approximately) normal data, and a normal distribution doesn’t have outliers. Not only do you need to note the outliers, but you also need to say why the outliers cause a problem in this case. Anything less than that is not a complete answer.\n\\(\\blacksquare\\)\n\nRun a suitable test to see whether the “typical” amount of sugar differs between adult’s and children’s cereals. Justify the test that you run. (You can use the version of your test that lives in a package, if that is easier for you.) What do you conclude, in the context of the data?\n\nSolution\nHaving ruled out the two-sample \\(t\\)-test, we are left with Mood’s median test. I didn’t need you to build it yourself, so you can use package smmr to run it with:\n\nlibrary(smmr)\nmedian_test(cereals,sugar,who)\n\n$table\n          above\ngroup      above below\n  adults       2    19\n  children    18     1\n\n$test\n       what        value\n1 statistic 2.897243e+01\n2        df 1.000000e+00\n3   P-value 7.341573e-08\n\n\nWe conclude that there is a difference between the median amounts of sugar between the two groups of cereals, the P-value of 0.00000007 being extremely small.\nWhy did it come out so small? Because the amount of sugar was smaller than the overall median for almost all the adult cereals, and larger than the overall median for almost all the children’s ones. That is, the children’s cereals really do have more sugar.\nMood’s median test doesn’t come with a confidence interval (for the difference in population medians), because whether or not a certain difference in medians is rejected depends on what those medians actually are, and the idea of the duality of the test and CI doesn’t carry over as we would like.\nMy daughter likes chocolate Cheerios, but she also likes Shredded Wheat and Bran Flakes. Go figure. (Her current favourite is Raisin Bran, even though she doesn’t like raisins by themselves.)\nMood’s median test is the test we should trust, but you might be curious about how the \\(t\\)-test stacks up here:\n\nt.test(sugar~who,data=cereals)\n\n\n    Welch Two Sample t-test\n\ndata:  sugar by who\nt = -11.002, df = 37.968, p-value = 2.278e-13\nalternative hypothesis: true difference in means between group adults and group children is not equal to 0\n95 percent confidence interval:\n -42.28180 -29.13925\nsample estimates:\n  mean in group adults mean in group children \n              10.90000               46.61053 \n\n\nThe P-value is even smaller, and we have the advantage of getting a confidence interval for the difference in means: from about 30 to about 40 units less sugar in the adult cereals. Whatever the units were.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mood-median.html#fear-of-math-1",
    "href": "mood-median.html#fear-of-math-1",
    "title": "8  Mood median test",
    "section": "8.6 Fear of math",
    "text": "8.6 Fear of math\nTwo new short courses have been proposed for helping students who suffer from severe math phobia. The courses are labelled A and B. Ten students were randomly allocated to one of these two courses, and each student’s score on a math phobia test was recorded after they completed their course. The math phobia test produces whole-number scores between 0 and 10, with a higher score indicating a greater fear of mathematics. The data can be found in link. We start with R for this question.\n\nRead in the data and check, however you like, that you have 10 observations, 5 from each course.\n\nSolution\nThis doesn’t need much comment:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/mathphobia.txt\"\nmath &lt;- read_delim(my_url, \" \")\n\nRows: 10 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): course\ndbl (1): phobia\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmath\n\n\n\n  \n\n\n\nThis will do, counting the a and b. Or, to save yourself that trouble:\n\nmath %&gt;% count(course)\n\n\n\n  \n\n\n\nFive each. The story is to get the computer to do the grunt work for you, if you can make it do so. Other ways:\n\nmath %&gt;% group_by(course) %&gt;% summarize(count = n())\n\n\n\n  \n\n\n\nand this:\n\nwith(math, table(course))\n\ncourse\na b \n5 5 \n\n\ngiving the same answer. Lots of ways.\nExtra: there is an experimental design issue here. You might have noticed that each student did only one of the courses. Couldn’t students do both, in a matched-pairs kind of way? Well, it’s a bit like the kids learning to read in that if the first of the courses reduces a student’s anxiety, the second course won’t appear to do much good (even if it actually would have been helpful had the student done that one first). This is the same idea as the kids learning to read: once you’ve learned to read, you’ve learned to read, and learning to read a second way won’t help much. The place where matched pairs scores is when you can “wipe out” the effect of one treatment before a subject gets the other one. We have an example of kids throwing baseballs and softballs that is like that: if you throw one kind of ball, that won’t affect how far you can throw the other kind.\n\\(\\blacksquare\\)\n\nDo a two-sample \\(t\\)-test to assess whether there is a difference in mean phobia scores after the students have taken the two courses. What do you conclude? (You have no 2 reason to suppose that a particular one of the tests will produce a higher mean than the other, so do a two-sided test.)\n\nSolution\nA two-sided test is the default, so there is not much to do here:\n\nt.test(phobia ~ course, data = math)\n\n\n    Welch Two Sample t-test\n\ndata:  phobia by course\nt = 0.83666, df = 4.4199, p-value = 0.4456\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -3.076889  5.876889\nsample estimates:\nmean in group a mean in group b \n            6.8             5.4 \n\n\nThe P-value of 0.4456 is nowhere near less than 0.05, so there is no evidence at all that the\nmean math phobia scores are different between the two courses.\n\\(\\blacksquare\\)\n\nDraw boxplots of the math phobia scores for each group (one line of code). What is the most striking thing that you notice?\n\nSolution\n\nggplot(math, aes(x = course, y = phobia)) + geom_boxplot()\n\n\n\n\nBoxplot a is just weird. The bar across the middle is actually at the top, and it has no bottom. (Noting something sensible like this is enough.) Boxplot b is hugely spread out.3\nBy way of explanation: the course a scores have a number of values equal so that the 3rd quartile and the median are the name, and also that the first quartile and the minimum value are the same:\n\ntmp &lt;- math %&gt;% filter(course == \"a\")\ntmp %&gt;% count(phobia)\n\n\n\n  \n\n\nsummary(tmp)\n\n    course              phobia   \n Length:5           Min.   :6.0  \n Class :character   1st Qu.:6.0  \n Mode  :character   Median :7.0  \n                    Mean   :6.8  \n                    3rd Qu.:7.0  \n                    Max.   :8.0  \n\n\nThe phobia scores from course A are two 6’s, two 7’s and an 8. The median and third quartile are both 7, and the first quartile is the same as the lowest value, 6.\nTechnique note: I wanted to do two things with the phobia scores from course A: count up how many of each score, and show you what the five-number summary looks like. One pipe won’t do this (the pipe “branches”), so I saved what I needed to use, before it branched, into a data frame tmp and then used tmp twice. Pipes are powerful, but not all-powerful.\n\\(\\blacksquare\\)\n\nExplain briefly why a \\(t\\)-test would not be good for these data. (There are two things that you need to say.)\n\nSolution\nThe easiest way to structure this is to ask yourself first what the \\(t\\)-test needs, and second whether you have it. The \\(t\\)-test assumes (approximately) normal data. The boxplot for group a doesn’t even look symmetric, and the one for group b has an oddly asymmetric box. So I think the normality is in question here, and therefore another test would be better. (This is perhaps a bit glib of an answer, since there are only 5 values in each group, and so they can certainly look non-normal even if they actually are normal, but these values are all integers, so it is perhaps wise to be cautious.) We have the machinery to assess the normality for these, in one shot:\n\nggplot(math, aes(sample = phobia)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~course, ncol = 1, scales = \"free\")\n\n\n\n\nI don’t know what you make of those, but they both look pretty straight to me (and there are only five observations, so it’s hard to judge). Course b maybe has a “hole” in it (three large values and two small ones). Maybe. I dunno. What I would really be worried about is outliers, and at least we don’t have those. I mentioned in class that the \\(t\\)-tests are robust to non-normality. I ought to have expanded on that a bit: what really makes the \\(t\\)-test still behave itself with non-normality is when you have large samples, that is, when the Central Limit Theorem has had a chance to take hold. (That’s what drives the normality not really being necessary in most cases.) But, even with small samples, exact normality doesn’t matter so much. Here, we have two tiny samples, and so we have to insist a bit more, but only a bit more, on a more-or-less normal shape in each group. (It’s kind of a double jeopardy in that the situation where normality matters most, namely with small samples, is where it’s the hardest to judge, because samples of size 5 even from a normal distribution can look very non-normal.) But, the biggest threats to the \\(t\\)-test are big-time skewness and outliers, and we are not suffering too badly from those.\n\\(\\blacksquare\\)\n\nRun a suitable test to compare the “typical” scores for the two courses. (You can use the version from a package rather than building your own.) What do you conclude?\n\nSolution\nThis is an invite to use smmr:\n\nlibrary(smmr)\nmedian_test(math, phobia, course)\n\n$table\n     above\ngroup above below\n    a     1     2\n    b     2     2\n\n$test\n       what     value\n1 statistic 0.1944444\n2        df 1.0000000\n3   P-value 0.6592430\n\n\nWe are nowhere near rejecting equal medians; in fact, both courses are very close to 50–50 above and below the overall median.\nIf you look at the frequency table, you might be confused by something: there were 10 observations, but there are only \\(1+2+2+2=7\\) in the table. This is because three of the observations were equal to the overall median, and had to be thrown away:\n\nmath %&gt;% summarize(med = median(phobia))\n\n\n\n  \n\n\nmath %&gt;% count(phobia)\n\n\n\n  \n\n\n\nThe overall median was 7. Because the actual data were really discrete (the phobia scores could only be whole numbers), we risked losing a lot of our data when we did this test (and we didn’t have much to begin with). The other thing to say is that with small sample sizes, the frequencies in the table have to be very lopsided for you to have a chance of rejecting the null. Something like this is what you’d need:\n\nx &lt;- c(1, 1, 2, 6, 6, 6, 7, 8, 9, 10)\ng &lt;- c(1, 1, 1, 1, 1, 2, 2, 2, 2, 2)\nd &lt;- tibble(x, g)\nmedian_test(d, x, g)\n\n$table\n     above\ngroup above below\n    1     0     3\n    2     4     0\n\n$test\n       what       value\n1 statistic 7.000000000\n2        df 1.000000000\n3   P-value 0.008150972\n\n\nI faked it up so that we had 10 observations, three of which were equal to the overall median. Of the rest, all the small ones were in group 1 and all the large ones were in group 2. This is lopsided enough to reject with, though, because of the small frequencies, there actually was a warning about “chi-squared approximation may be inaccurate”.4\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mood-median.html#medical-instructions-1",
    "href": "mood-median.html#medical-instructions-1",
    "title": "8  Mood median test",
    "section": "8.7 Medical instructions",
    "text": "8.7 Medical instructions\nDo people understand medical instructions better at certain times of the day? In a study, students in a grade 12 class are randomly divided into two groups, A and B. All students see a video describing how to use an infant forehead thermometer. The students in Group A see the video at 8:30 am, while the students in Group B see the same video at 3:00 pm (on the same day). The next day, all the students are given a test on the material in the video (graded out of 100). The observed scores are in link (values separated by spaces).\n\nRead the data into R and display the (first ten) values.\n\nSolution\nSeparated by spaces, so read_delim:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/forehead.txt\"\ninstr &lt;- read_delim(my_url, \" \")\n\nRows: 18 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): group\ndbl (1): score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ninstr\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nObtain a suitable plot that will enable you to assess the assumptions for a two-sample \\(t\\)-test.\n\nSolution\nWe need the values in each group to be approximately normally distributed. Side-by-side boxplots will do it:\n\nggplot(instr, aes(x = group, y = score)) + geom_boxplot()\n\n\n\n\nor, if you like, separate (facetted) normal quantile plots, which I would do this way:\n\nggplot(instr, aes(sample = score)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~group, ncol = 1)\n\n\n\n\n\\(\\blacksquare\\)\n\nWhy might you have doubts about using a two-sample \\(t\\)-test here?\n\nSolution\nWe are looking for non-normality in at least one of the groups. Here, both groups have an outlier at the low end that would be expected to pull the mean downward. I don’t think there is left-skewness here, since there is no particular evidence of the high-end values being bunched up: the problem in both cases with normality is at the low end. One way or another, I’m expecting you to have noticed the outliers. Extra: last year, when I first drew the normal quantile plots, there was no stat_qq_line, so you had to imagine where the line went if you did it this way. Without the line, these plots look somewhat curved, which would have pointed to left-skewness, but now we see that the lowest observation is too low, and maybe the second-lowest one as well, while the other observations are just fine.\n\\(\\blacksquare\\)\n\nRun Mood’s median test as in class (without using smmr). What do you conclude, in the context of the data? What recommendation would you make about the time of day to see the video? (You might get a warning about “chisquared approximation being incorrect”, which you can ignore here.)\n\nSolution\nThe overall median first:\n\ninstr %&gt;% summarize(med = median(score))\n\n\n\n  \n\n\n\n87.5, which is not equal to any of the data values (they are all integers). This will avoid any issues with values-equal-to-median later.\nThen, create and save a table of the value by group and above/below median. You can count either above or below (it comes out equivalently either way):\n\ntab &lt;- with(instr, table(group, score &gt; 87.5))\ntab\n\n     \ngroup FALSE TRUE\n    A     2    8\n    B     7    1\n\n\nThen, chi-squared test for independence (the null) or association of some kind (the alternative). The correct=F is saying not to do Yates’s correction, so that it would come out the same if you were doing it by hand (“observed minus expected, squared, divided by expected” and all that stuff).\n\nchisq.test(tab, correct = F)\n\nWarning in chisq.test(tab, correct = F): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tab\nX-squared = 8.1, df = 1, p-value = 0.004427\n\n\nThe P-value is 0.0044, which is (much) smaller than 0.05, and therefore you can reject independence and conclude association: that is, whether a student scores above or below the median depends on which group they are in, or, that the median scores are different for the two groups.\nThe warning is because the expected frequencies are on the small side (if you have done this kind of problem by hand, you might remember something about “expected frequencies less than 5”. This is that.) Here, the P-value is so small that we can afford to have it be inaccurate by a bit and still not affect the conclusion, so I think we are safe.\nAs for which group is better, well, the easiest way is to go back to your boxplots and see that the median for group A (8:30 am) is substantially higher than for group B (3:00pm). But you can also see it from your frequency table, if you displayed it:\n\ntab\n\n     \ngroup FALSE TRUE\n    A     2    8\n    B     7    1\n\n\nMost of the people in the 8:30 am group scored above the median, and most of the people in the 3:00 pm group scored below the median. So the scores at 8:30 am were better overall.\nAs I write this, it is just after 3:00 pm and I am about to make myself a pot of tea!\nExtra: about that correct=F thing. There was a point of view for a long time that when you are dealing with a \\(2 \\times 2\\) table, you can get better P-values by, before squaring “observed minus expected”, taking 0.5 away from the absolute value of the difference. This is called Yates’s correction. It is in the same spirit as the “continuity correction” that you might have encountered in the normal approximation to the binomial, where in the binomial you have to have a whole number of successes, but the normal allows fractional values as well. In about the 1960s, the usefulness of Yates’s correction was shot down, for general contingency tables. There is, however, one case where it is useful, and that is the case where the row totals and column totals are fixed.\nWhat do I mean by that? Well, first let’s look at a case where the totals are not all fixed. Consider a survey in which you want to see whether males and females agree or disagree on some burning issue of the day. You collect random samples of, say, 500 males and 500 females, and you count how many of them say Yes or No to your statement.5 You might get results like this:\n\nYes  No  Total\nMales    197 303   500\nFemales  343 157   500\nTotal    540 460  1000\n\nIn this table, the row totals must be 500, because you asked this many males and this many females, and each one must have answered something. The column totals, however, are not fixed: you didn’t know, ahead of time, that 540 people would answer “yes”. That was just the way the data turned out, and if you did another survey with the same design, you’d probably get a different number of people saying “yes”.\nFor another example, let’s go back to Fisher (yes, that Fisher). A “lady” of his acquaintance claimed to be able, by drinking a cup of tea with milk and sugar in it, whether the milk or the sugar had been added first. Fisher, or, more likely, his housekeeper, prepared 8 cups of tea, 4 with milk first and 4 with sugar first. The lady knew that four of the cups had milk first, and her job was to say which four. The results might have been like this:\n\nActual \nMilk first  sugar first  Total\nLady   Milk first        3            1         4\nsays   sugar first       1            3         4\nTotal             4            4         8\n\nThis time, all of the row totals and all of the column totals must be 4, regardless of what the lady thinks. Even if she thinks 5 of the cups of tea actually had milk first, she is going to pick 4 of them to say that they have milk first, since she knows there are only 4. In this case, all of the row and column totals are fixed at 4, and the right analysis is called Fisher’s Exact Test, based on the hypergeometric distribution. In a \\(2\\times 2\\) table like this one, there is only one “degree of freedom”, since as soon as you specify one of the frequencies, say the number of cups where the lady said milk first and they actually were milk first, you can work out the others. But, leaving that aside, the usual chi-squared analysis is a perfectly good approximation, especially if the frequencies are large, and especially if you use Yates’s correction.\nIt is clear that Fisher must have been English, since he was able to get a publication out of drinking tea.\nHow does that apply to Mood’s median test? Well, let’s remind ourselves of the table we had:\n\ntab\n\n     \ngroup FALSE TRUE\n    A     2    8\n    B     7    1\n\n\nWe know how many students were in each group: 10 in group A and 8 in B. So the row totals are fixed. What about the columns? These are whether each observation was above or below the overall median. There were 18 observations altogether, so there must be 9 above and 9 below.6 So the column totals are fixed as well. All totals fixed, so we should be using Yates’s correction. I didn’t, because I wanted to keep things simple, but I should have done.\nR’s chisq.test by default always uses Yates’s correction, and if you don’t want it, you have to say correct=F. Which is why I have been doing so all through.\n\\(\\blacksquare\\)\n\nRun Mood’s median test on these data using my smmr package, and verify that you get the same answer.\n\nSolution\nNot much to it, since the data is already read in:\n\nlibrary(smmr)\nmedian_test(instr, score, group)\n\n$table\n     above\ngroup above below\n    A     8     2\n    B     1     7\n\n$test\n       what       value\n1 statistic 8.100000000\n2        df 1.000000000\n3   P-value 0.004426526\n\n\nIdentical, test statistic, degrees of freedom and P-value. The table of frequencies is also the same, just with columns rearranged. (In smmr I counted the number of values below the overall median, whereas in my build-it-yourself I counted the number of values above.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mood-median.html#handspans-revisited-1",
    "href": "mood-median.html#handspans-revisited-1",
    "title": "8  Mood median test",
    "section": "8.8 Handspans revisited",
    "text": "8.8 Handspans revisited\nTake your right hand, and stretch the fingers out as far as you can. The distance between the tip of your thumb and the tip of your little (pinky) finger is your handspan. The students in a Statistics class at Penn State measured their handspans and also whether they identified as male or female. The data are at http://ritsokiguess.site/datafiles/handspan.txt, with handspans measured in inches. We want to see whether male students have a larger mean handspan than female students.\n\nRead in and display (some of) the data.\n\nSolution\nDelimited by a single space, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/handspan.txt\"\nspan &lt;- read_delim(my_url, \" \")\n\nRows: 190 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): sex\ndbl (1): handspan\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspan\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake a suitable (facetted) normal quantile plot of the data. (Bear in mind what is supposed to have a normal distribution.)\n\nSolution\nHere, we need each group to be approximately normal, so make normal quantile plots of handspan, facetted by sex:\n\nggplot(span, aes(sample=handspan)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~sex)\n\n\n\n\n\\(\\blacksquare\\)\n\nDiscuss briefly whether you might prefer to use Mood’s median test to compare the handspans of the male and female students, compared to a two-sample \\(t\\)-test.\n\nSolution\nA two-sample \\(t\\)-test assumes that each of the two samples comes from a (approximately) normal distribution (“the data are normal” is not precise enough). The female values, on the left, definitely have some outliers at the low end (or a long lower tail), so these are definitely not normal. The male values (on the right) are slightly skewed to the left, or there are some mild outliers at the low end, or, if you prefer, these are approximately normal. (You need discussion of each of the males and females, or of why looking at one group is enough.) Because the males are not close enough to normal (or, because neither group is close enough to normal), we would prefer to use Mood’s median test. (Say this.) You do yourself a favour by making it clear that you know that both groups have to be normal enough; if one is good but the other is not, that is not enough.\nThe other relevant issue is sample size. The best answer discusses that as well, even though you have a lot to think about already. This data set has 190 observations in it, so the samples must be pretty big:\n\nspan %&gt;% count(sex)\n\n\n\n  \n\n\n\nWith these sample sizes, we can expect a lot of help from the central limit theorem. The apparent outliers in the males won’t be a problem, and maybe we could even get away with those outliers in the females.\nExtra: you could also think about bootstrapped sampling distributions of the sample mean here. The one we are most concerned about is the females; if it turns out that they are all right, then the males must be all right too, since the plot for them is showing less non-normality (or, without the double negative, is closer to being normal). So let’s do the females:\n\nspan %&gt;% \n  filter(sex == \"F\") -&gt; females \ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(females$handspan, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nMy take is that the sampling distribution of the sample mean for the females is normal enough, therefore the one for the males is also normal enough, therefore the two-sample \\(t\\)-test is actually fine.\nThe reason that this one is close to normal is different from the other one, though. In the other question, we had milder non-normality but a smaller sample; in this one, the data distribution is less normal, but we had a much larger sample size to compensate.\n\\(\\blacksquare\\)\n\nRun Mood’s median test. What do you conclude from the test, in the context of the data?\n\nSolution\n\nlibrary(smmr)\nmedian_test(span, handspan, sex)\n\n$table\n     above\ngroup above below\n    F    17    82\n    M    65    11\n\n$test\n       what       value\n1 statistic 8.06725e+01\n2        df 1.00000e+00\n3   P-value 2.66404e-19\n\n\nThe P-value of \\(2.66 \\times 10^{-19}\\) is extremely small, so we can conclude that males and females have different median handspans. Remember that we are now comparing medians, and that this test is two-sided.\nYou can stop here, or you can go on and note that most of the males have a handspan bigger than the median, and most of the females have a handspan smaller than the median, so that males have on average a larger handspan. But you have to make the case that males have a larger handspan; you cannot just assert this from the P-value.\nA more formal way to do this is to make the same observation as above, then note that this is “on the correct side” (for males to have a larger handspan), and thus that you can halve the P-value, and conclude that males’ handspans are indeed larger in terms of median.\nExtra: you are probably expecting a confidence interval now for the difference in medians. I haven’t talked about that in lecture, because the ideas are a bit trickier than they were for the confidence interval for the sign test. The sign test could be used for testing any median, so we could try a bunch of medians and see whether each one was rejected or not. The problem with Mood’s median test is that it only tests that the medians are the same. If you could easily test that the difference in medians was 3, say, you would know whether 3 was inside or outside the confidence interval for the difference in medians.\nWhat were the actual sample medians, anyway?\n\nspan %&gt;% group_by(sex) %&gt;% \nsummarize(med = median(handspan))\n\n\n\n  \n\n\n\nHere’s an idea: if we shift all the female handspans up by 2.5 inches, the medians would be the same:\n\nspan %&gt;% mutate(x = ifelse(sex==\"F\", handspan+2.5, handspan)) -&gt; d\nd\n\n\n\n  \n\n\n\nDataframe d has a new column x that is the handspan plus 2.5 inches for females, and the unchanged handspan for males. So the median of x should be the same for males and females:\n\nd %&gt;% group_by(sex) %&gt;% \nsummarize(med_x = median(x))\n\n\n\n  \n\n\n\nand also the medians of x cannot possibly be significantly different:\n\nmedian_test(d, x, sex)\n\n$table\n     above\ngroup above below\n    F    46    36\n    M    41    35\n\n$test\n       what      value\n1 statistic 0.07369901\n2        df 1.00000000\n3   P-value 0.78602526\n\n\nQuite a lot of the values of x are exactly equal to the overall median (and are discarded), so the P-value is not exactly 1 as you would expect. But it is definitely not significant, and so a difference of 2.5 inches smaller for females is going to be in a confidence interval for the difference in medians.\nThe strategy now is to try shifting the female handspans by different amounts, run Mood’s median test for each one, and see which shifts are not rejected. These are the ones for which that difference in medians would be in the confidence interval. Before we get to that, though, I want to simplify the procedure we have, so that it is easier to run it lots of times. First, let’s get just the P-value out of the median test:\n\nd.1 &lt;- median_test(d, x, sex)\nd.1 %&gt;% pluck(\"test\", \"value\", 3)\n\n[1] 0.7860253\n\n\nThat’s the P-value. pluck pulls individual things out of bigger things. The variable I called d.1 has two things in it. The one called table has the numbers of data values above and below the overall median; the one called test has the test statistic and P-value in it. test is a dataframe; inside that is a column called what and a column called value with the number we want in it, and we want the third thing in that (the other two are the chi-squared test statistic and its degrees of freedom). Hence the pluck statement got the right thing.\nLet’s think strategy: we want to shift the female handspans by a bunch of different amounts, run the test on each one, and get the P-value each time. When you’re running a big for-each like this, you want the thing you do each time to be as simple as possible. So let’s write a function that takes the shift as input, works out the new x, runs the test, and returns the P-value. We have all the ingredients, so it’s a matter of putting them together:\n\nshift_pval &lt;- function(shift) {\n  span %&gt;% mutate(x = ifelse(sex == \"F\", handspan + shift, handspan)) -&gt; d\n  d.1 &lt;- median_test(d, x, sex)\n  d.1 %&gt;% pluck(\"test\", \"value\", 3)\n}\n\nIn the function, the shift is input. The first line computes the handspans shifted by the input amount, whatever it is; the second line runs the median test on the shifted data; the last line pulls out, and returns, the P-value.\nI am being a little sloppy here (but R is letting me get away with it): the function is also using a dataframe called span, which is the one we read in from the file earlier. That was not input to the function, so, if you have experience with other programming languages, you might be wondering whether that is “in the scope” of inside the function: that is, whether R will know about it. R does; anything the function needs that is not part of the input, it will take from your workspace. This is, you might imagine, dangerous; if the input to your function is called, say, x, you might easily have an x lying around in your workspace from some other analysis that has nothing to do with the x you want as the input to your function. The safe way to do it, and what I should have done, is to have span be input to my function as well. However, that clutters up the discussion below, so we’ll leave things as I did them here.\nLet’s test this on a shift of 2.5 inches, and on the original data (a shift of zero):\n\nshift_pval(2.5)\n\n[1] 0.7860253\n\nshift_pval(0)\n\n[1] 2.66404e-19\n\n\nThose are the same P-values we got before, so good.\nNow, let’s get a bunch of shifts, say from 0 to 5 in steps of 0.5:\n\ntibble(shift = seq(0, 5, 0.5))\n\n\n\n  \n\n\n\nwork out the P-value for each one, rowwise:\n\ntibble(shift = seq(0, 5, 0.5)) %&gt;%\n  rowwise() %&gt;% \n  mutate(p_value = shift_pval(shift))\n\n\n\n  \n\n\n\nand finally decide whether each shift is inside or outside the CI (because I am too lazy to figure out the scientific notation):\n\ntibble(shift = seq(0, 5, 0.5)) %&gt;%\n  rowwise() %&gt;% \n  mutate(p_value = shift_pval(shift)) %&gt;% \n  mutate(where = ifelse(p_value&lt;0.05, \"outside\", \"inside\"))\n\n\n\n  \n\n\n\nThe confidence interval goes from 2 inches to 2.5 inches on this scale. I checked and it goes up to 3 really, except that 3 itself is outside the interval. So let’s call it 2 to 3 inches. This means that the median female handspan is between 2 and 3 inches smaller than the median male handspan, because we had to shift the female handspans up by that much to make them not significantly different.\nYou, of course, would do just the last pipeline; I showed you the steps so you could see what was going on.\nThe final observation is that this interval is a long way from containing zero, because the P-value was so tiny.\nLet’s see how the \\(t\\)-interval looks in comparison (two-sided, because we want the confidence interval):\n\nt.test(handspan~sex, data = span)\n\n\n    Welch Two Sample t-test\n\ndata:  handspan by sex\nt = -10.871, df = 187.92, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -3.001496 -2.079466\nsample estimates:\nmean in group F mean in group M \n       20.01699        22.55747 \n\n\nAlmost exactly the same (except that F is before M). So it made no difference at all whether we did a \\(t\\)-test or a Mood’s median test, as the bootstrapped sampling distribution suggested.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mood-median.html#footnotes",
    "href": "mood-median.html#footnotes",
    "title": "8  Mood median test",
    "section": "",
    "text": "That is, before looking at the data. This is Latin. It’s also the place that the Bayesian “prior distribution” comes from. The “posterior distribution” comes from the Latin a posteriori, which means afterwards, that is, after you have looked at the data.↩︎\nThat is, before looking at the data. This is Latin. It’s also the place that the Bayesian “prior distribution” comes from. The “posterior distribution” comes from the Latin a posteriori, which means afterwards, that is, after you have looked at the data.↩︎\nThe two groups have very different spreads, but that is not a problem as long as we remember to do the Welch-Satterthwaite test that does not assume equal spreads. This is the default in R, so we are good, at least with that.↩︎\nThere was, in the chisq.test inside median_test, but in smmr I didn’t pass that warning back to the outside world.↩︎\nTo simplify things, we’ll assume that everyone gave a Yes or a No answer, though you could add a column like “No answer” if you wanted to make it more realistic.↩︎\nExcept in the case of the previous problem, where there were multiple observations equal to the overall median. Which we ignore for the moment.↩︎"
  },
  {
    "objectID": "matched-pairs-sign.html#measuring-body-fat",
    "href": "matched-pairs-sign.html#measuring-body-fat",
    "title": "9  Matched pairs t and sign test",
    "section": "9.1 Measuring body fat",
    "text": "9.1 Measuring body fat\nAthletes are concerned with measuring their body fat percentage. Two different methods are available: one using ultrasound, and the other using X-ray technology. We are interested in whether there is a difference in the mean body fat percentage as measured by these two methods, and if so, how big that difference is. Data on 16 athletes are at link.\n\nExplain briefly why a matched pairs analysis is more suitable for these data than a two-independent-samples analysis (using a two-sample \\(t\\)-test). You might find that looking at the data (clicking on the link) helps you figure this out.\nRead in the data and check that you have a sensible number of rows and columns.\nCarry out a suitable test to determine whether the means are the same or different. (At this point, obtain the R output including a P-value.)\nWhat do you conclude from the test?\nObtain a 95% confidence interval for the population mean difference. How is the interval consistent with your test?\nCalculate the differences, and make a normal quantile plot of them. Is there any evidence that normality of differences fails? Explain briefly."
  },
  {
    "objectID": "matched-pairs-sign.html#throwing-baseballs-and-softballs",
    "href": "matched-pairs-sign.html#throwing-baseballs-and-softballs",
    "title": "9  Matched pairs t and sign test",
    "section": "9.2 Throwing baseballs and softballs",
    "text": "9.2 Throwing baseballs and softballs\nCan students throw a baseball farther than a softball? A statistics class, containing 24 students, went out to a football field to try to answer this question. Each student warmed up and then threw each type of ball as far as they could. The order of ball types was randomized: some students threw the baseball first, and some threw the softball first. (A softball is bigger than a baseball, so we might expect that a softball would be harder to throw a long way than a baseball.) The data are in http://ritsokiguess.site/datafiles/throw.txt in three columns: the first is a number identifying the student, the second is the distance thrown with the baseball (in yards) and the third is the distance thrown with the softball (also in yards).\n\nRead the data into R. You’ll need to supply some names to the columns.\nCalculate a column of differences, baseball minus softball, in the data frame.\nCarry out a sign test in R, testing the null hypothesis that the median difference is zero, against the alternative that it is greater than zero. Obtain a P-value. Your option whether you use smmr or not."
  },
  {
    "objectID": "matched-pairs-sign.html#throwing-baseballs-and-softballs-again",
    "href": "matched-pairs-sign.html#throwing-baseballs-and-softballs-again",
    "title": "9  Matched pairs t and sign test",
    "section": "9.3 Throwing baseballs and softballs, again",
    "text": "9.3 Throwing baseballs and softballs, again\nPreviously, you carried out a sign test to determine whether students could throw a baseball farther than a softball. This time, we will calculate a confidence interval for the median difference baseball minus softball, using the results of sign tests.\n\nRead the data into R from link, giving appropriate names to the columns, and add a column of differences.\nUse smmr to find a 95% confidence interval for the median difference.\nWhat function in smmr will run a two-sided sign test and return only the P-value? Check that it works by testing whether the median difference for your data is zero or different from zero.\nBased on your P-value, do you think 0 is inside the confidence interval or not? Explain briefly.\nObtain a 95% confidence interval for the population median difference, baseball minus softball, using a trial-and-error procedure that determines whether a number of possible medians are inside or outside the CI."
  },
  {
    "objectID": "matched-pairs-sign.html#changes-in-salary",
    "href": "matched-pairs-sign.html#changes-in-salary",
    "title": "9  Matched pairs t and sign test",
    "section": "9.4 Changes in salary",
    "text": "9.4 Changes in salary\nA company is growing and would like to attract more employees. The company would like to advertise that salaries there are increasing. To do this, the company randomly samples 20 employees that have been working there since January 2016, and for each of these employees, records their salary in January 2016 and January 2017. The data, with salaries in thousands of dollars, are in link.\n\nRead the data into R and demonstrate that you have two salaries for each of 20 employees.\nTo compare the salaries, explain briefly why a matched-pairs test would be better than a two-sample test.\nMake a suitable graph to assess the assumptions for a matched-pairs \\(t\\)-test. What does your graph tell you?\nCarry out a suitable matched-pairs \\(t\\)-test on these data. (If you thought in the previous part that this was the wrong thing to do, do it anyway for the purposes of this assignment.) What do you conclude?\nThe company would like to estimate\nhow much salaries are increasing, on average. Obtain some output that will enable the company to assess this, and tell the CEO which piece of the output they should look at."
  },
  {
    "objectID": "matched-pairs-sign.html#body-fat-revisited",
    "href": "matched-pairs-sign.html#body-fat-revisited",
    "title": "9  Matched pairs t and sign test",
    "section": "9.5 Body fat revisited",
    "text": "9.5 Body fat revisited\nAthletes are concerned with measuring their body fat percentage. Two different methods are available: one using ultrasound, and the other using X-ray technology. We are interested in whether there is a difference in the mean body fat percentage as measured by these two methods, and if so, how big that difference is. Data on 16 athletes are at link.\nWe saw this data set before.\n\nRead in the data again.\nCalculate the differences, and make a normal quantile plot of them. Is there any evidence that normality of differences fails? Explain briefly.\nPreviously, we did a matched-pairs \\(t\\)-test for these data. In the light of your normal quantile plot, do you think that was a good idea? Explain briefly.\nUse the sign test appropriately to compare the two methods for measuring body fat. (Use smmr if you wish.) What do you conclude, as ever in the context of the data?"
  },
  {
    "objectID": "matched-pairs-sign.html#the-dentist-and-blood-pressure",
    "href": "matched-pairs-sign.html#the-dentist-and-blood-pressure",
    "title": "9  Matched pairs t and sign test",
    "section": "9.6 The dentist and blood pressure",
    "text": "9.6 The dentist and blood pressure\nGoing to the dentist is scary for a lot of people. One way in which this might show up is that people might have higher blood pressure on average before their dentist’s appointment than an hour after the appointment is done. Ten randomly-chosen individuals have their (systolic) blood pressure measured while they are in a dentist’s waiting room, and then again one hour after their appointment is finished.\nThe data are in http://ritsokiguess.site/datafiles/blood_pressure1.csv.\n\nRead in and display the data.\nWhat kind of experimental design is this? How do you know? Explain briefly.\nRun a suitable \\(t\\)-test on these data. What do you conclude, in the context of the data?\nRun a suitable sign test on these data. What do you conclude, in the context of the data?\nDraw a suitable normal quantile plot of these data, one that will enable you to decide between the tests you ran in the previous two parts.\nDiscuss briefly which of your two tests is the more appropriate one to run."
  },
  {
    "objectID": "matched-pairs-sign.html#french-teachers",
    "href": "matched-pairs-sign.html#french-teachers",
    "title": "9  Matched pairs t and sign test",
    "section": "9.7 French teachers",
    "text": "9.7 French teachers\nTwenty high-school French teachers attended a summer institute to improve their French skills. At the beginning of their session, each teacher took a listening test (to test their understanding of spoken French). After 4 weeks of immersion in French, each teacher took a similar listening test again. (The actual French spoken in the two tests was different, so simply taking the first test should not improve the score in the second one; the tests were otherwise similar.) The maximum score on each test was 36, and a higher score is better. The data are here. (Right-click on the link, select “copy link address”, and then paste that URL into R Studio.) The data values are separated by tabs.\nThe data file has three columns:\n\nan identification for each teacher\nthe teacher’s score in the first test\nthe teacher’s score in the second test\n\n\nRead in and display (some of) the data.\nExplain briefly why this is a matched-pairs study.\nRun a suitable matched-pairs \\(t\\)-test to see whether the teachers’ scores have on average improved over the four weeks.\nWhat do you conclude from your test, in the context of the data?\nHow much is the teachers’ listening skill improving, on average? Give a suitable interval to support your answer.\nMake a suitable plot to assess any assumptions for this test.\nDo you trust the result of your matched-pairs \\(t\\)-test? Explain briefly.\nRun a suitable sign test, and obtain a suitable (95%) confidence interval. Comment briefly on your results.\nComment briefly on the comparison between your inferences for the mean and the median.\n\nMy solutions follow:"
  },
  {
    "objectID": "matched-pairs-sign.html#measuring-body-fat-1",
    "href": "matched-pairs-sign.html#measuring-body-fat-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.8 Measuring body fat",
    "text": "9.8 Measuring body fat\nAthletes are concerned with measuring their body fat percentage. Two different methods are available: one using ultrasound, and the other using X-ray technology. We are interested in whether there is a difference in the mean body fat percentage as measured by these two methods, and if so, how big that difference is. Data on 16 athletes are at link.\n\nExplain briefly why a matched pairs analysis is more suitable for these data than a two-independent-samples analysis (using a two-sample \\(t\\)-test). You might find that looking at the data (clicking on the link) helps you figure this out.\n\nSolution\nThe data file looks like this:\n\nathlete xray ultrasound\n1 5.00 4.75\n2 7 3.75\n3 9.25 9\n4 12 11.75\n5 17.25 17\n6 29.5 27.5\n7 5.5 6.5\n8 6 6.75\n9 8 8.75\n10 8.5 9.5\n11 9.25 9.5\n12 11 12\n13 12 12.25\n14 14 15.5\n15 17 18\n16 18 18.25\n\nThe data are two measurements for each of the 16 athletes: that is, each athlete had their body fat percentage measured using both of the two methods. Extra: a two-sample \\(t\\) approach would be reasonable if one set of 16 athletes had been measured by X-ray and another different set of 16 athletes had been measured by ultrasound. (That is, if there had been 32 athletes in total, with each one randomly assigned to one of the measurement methods.) But that’s not what happened. It is easy to measure one athlete’s body fat percentage using both of the two methods, so a matched pairs design is easy to implement (as well as being better). If you use two independent samples (each athlete doing only one measurement method), you introduce an extra source of variability: athletes differ one from another in body fat, as well as differing possibly by measurement method. If you use a matched-pairs design, you remove the athlete-to-athlete differences, leaving only the differences due to measurement method.\n\\(\\blacksquare\\)\n\nRead in the data and check that you have a sensible number of rows and columns.\n\nSolution\nThis kind of thing. Since you looked at the data (didn’t you?), you’ll know that the values are separated by single spaces:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/bodyfat.txt\"\nbodyfat &lt;- read_delim(myurl, \" \")\n\nRows: 16 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): athlete, xray, ultrasound\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbodyfat\n\n\n\n  \n\n\n\n16 rows (athletes) and 3 columns, one for each measurement method and one labelling the athletes. All good.\nSince 16 is not too much bigger than 10, I got the whole data frame here. (At least, I think that’s the reason I got more than 10 rows.) In an R Notebook, you’ll see the first ten rows as normal, with a button to click to see the other six.\n\\(\\blacksquare\\)\n\nCarry out a suitable test to determine whether the means are the same or different. (At this point, obtain the R output including a P-value.)\n\nSolution\nFeed the two columns into t.test along with paired=T. This is a two-sided test, so we don’t have to take any special steps for that. Note that we’re back to the “old-fashioned” version of t.test that does not allow data=, so we have to go the with way:\n\nwith(bodyfat, t.test(xray, ultrasound, paired = TRUE))\n\n\n    Paired t-test\n\ndata:  xray and ultrasound\nt = -0.30801, df = 15, p-value = 0.7623\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.7425068  0.5550068\nsample estimates:\nmean difference \n       -0.09375 \n\n\n\\(\\blacksquare\\)\n\nWhat do you conclude from the test?\n\nSolution\nThe P-value of 0.7623 is not at all small, so there is no way we can reject the null hypothesis.1 There is no evidence of a difference in means; we can act as if the two methods produce the same mean body fat percentage. That is to say, on this evidence we can use either method, whichever one is cheaper or more convenient.\n\\(\\blacksquare\\)\n\nObtain a 95% confidence interval for the population mean difference. How is the interval consistent with your test?\n\nSolution\nYou don’t even need to do any more coding: the test was two-sided, so just pick the confidence interval off the output above: \\(-0.74\\) to 0.56. The interval includes both positive and negative values (or, 0 is inside the interval), so the difference could go either way. This is entirely consistent with not being able to reject the null.\n\\(\\blacksquare\\)\n\nCalculate the differences, and make a normal quantile plot of them. Is there any evidence that normality of differences fails? Explain briefly.\n\nSolution\nThe smoothest2 way to do this is to use a pipeline: use a mutate to create the column of differences, and then pipe that into ggplot, omitting the data frame that would normally go first (the input data frame here is the new one with the differences in it, which doesn’t have a name). I’ll make a normal quantile plot in a moment, but if you haven’t seen that yet, the plot to make is a histogram:\n\nbodyfat %&gt;%\n  mutate(diff = xray - ultrasound) %&gt;%\n  ggplot(aes(x = diff)) + geom_histogram(bins = 6)\n\n\n\n\nI don’t know whether you’d call that “approximately normal” or not. We are in kind of a double-bind with this one: the sample size is small, so normality matters, but with a small sample, the data might not look very normal. It’s kind of skewed right, but most of the evidence for the skewness is contained in those two observations with difference 2 and above, which is pretty flimsy evidence for anything. (In the normal quantile plot below, the suggestion is that those two observations really are a bit too large. It’s easier to tell there.)\nBelow, I’m repeating the calculation of the differences, which is inefficient. If I’m going to draw two graphs of the differences, the right way is to calculate the differences and save the data frame, then use that new data frame twice. But you’re probably only going to draw either the histogram or the normal quantile plot, not both, so you can use the appropriate one of my two bits of code. The normal quantile plot:\n\nbodyfat %&gt;%\n  mutate(diff = xray - ultrasound) %&gt;%\n  ggplot(aes(sample = diff)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis is showing a little evidence of skewness or outliers (depending on your point of view: either is good). The lowest and highest values are both too high, and the pattern of points on the plot is kind of curved (which would be evidence of skewness). Or you could say that the two highest values are too high, with the other values being more or less in line (that would be evidence of outliers at the upper end). I like outliers better than skewness, since those bottom-end points are not far off the line. I would also accept “no substantial problems”, if you can make the case that those two highest points are not too far off the line. With only 16 observations as we have here, even truly normal data would stray off the line a bit.\nAs ever, your explanation is more important than your conclusion. Can you justify what you think?\nIf you took your differences the other way around, as ultrasound minus xray, your plot will also be the other way around, with the “outliers” at the bottom. That’s good too.\nWhere this is going (which I didn’t ask you) is whether or not we trust the result of the matched pairs test. I would say that the test is so far from being significant, and the failure of normality is not gross, that it is hard to imagine any alternative test coming up with a significant result. So I would be happy to trust this paired \\(t\\)-test.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#throwing-baseballs-and-softballs-1",
    "href": "matched-pairs-sign.html#throwing-baseballs-and-softballs-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.9 Throwing baseballs and softballs",
    "text": "9.9 Throwing baseballs and softballs\nCan students throw a baseball farther than a softball? A statistics class, containing 24 students, went out to a football field to try to answer this question. Each student warmed up and then threw each type of ball as far as they could. The order of ball types was randomized: some students threw the baseball first, and some threw the softball first. (A softball is bigger than a baseball, so we might expect that a softball would be harder to throw a long way than a baseball.) The data are in http://ritsokiguess.site/datafiles/throw.txt in three columns: the first is a number identifying the student, the second is the distance thrown with the baseball (in yards) and the third is the distance thrown with the softball (also in yards).\n\nRead the data into R. You’ll need to supply some names to the columns.\n\nSolution\nThis kind of thing:\n\nmyurl=\"http://ritsokiguess.site/datafiles/throw.txt\"\nthrows=read_delim(myurl,\" \",col_names=c(\"student\",\"baseball\",\"softball\"))\n\nRows: 24 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): student, baseball, softball\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nthrows\n\n\n\n  \n\n\n\nThis is one of those times where we have to tell R what names to give the columns. Or you can put col_names=F and leave the columns called X1, X2, X3 or whatever they end up as.\n\\(\\blacksquare\\)\n\nCalculate a column of differences, baseball minus softball, in the data frame.\n\nSolution\nAdd it to the data frame using mutate. Use the right-arrow assignment to create what I called throws2 below, or put something like throws2 &lt;- on the beginning of the line. Your choice.\n\nthrows %&gt;% mutate(diff=baseball-softball) -&gt;\n  throws2\n\n\\(\\blacksquare\\)\n\nCarry out a sign test in R, testing the null hypothesis that the median difference is zero, against the alternative that it is greater than zero. Obtain a P-value. Your option whether you use smmr or not.\n\nSolution\nI think using smmr is way easier, so I’ll do that first. There is even a shortcut in that the null median defaults to zero, which is exactly what we want here:\n\nlibrary(smmr)\nsign_test(throws2,diff)\n\n$above_below\nbelow above \n    2    21 \n\n$p_values\n  alternative      p_value\n1       lower 9.999971e-01\n2       upper 3.302097e-05\n3   two-sided 6.604195e-05\n\n\nWe want, this time, the upper-tailed one-sided test, since we want to prove that students can throw a baseball a longer distance than a softball. Thus the P-value we want is 0.000033.\nTo build it yourself, you know the steps by now. First step is to count how many differences are greater and less than zero:\n\ntable(throws2$diff&gt;0)\n\n\nFALSE  TRUE \n    3    21 \n\n\nor\n\ntable(throws2$diff&lt;0)\n\n\nFALSE  TRUE \n   22     2 \n\n\nor, since we have things in a data frame,\n\nthrows2 %&gt;% count(diff&gt;0)\n\n\n\n  \n\n\n\nor count those less than zero. I’d take any of those.\nNote that these are not all the same. One of the differences is in fact exactly zero. The technically right thing to do with the zero difference is to throw it away (leaving 23 differences with 2 negative and 21 positive). I would take that, or 2 or 3 negative differences out of 24 (depending on whether you count “greater than zero” or “less than zero”). We hope that this won’t make a material difference to the P-value; it’ll make some difference, but won’t (we hope) change the conclusion about whether to reject.\nSecond step is to get a P-value for whichever one of those you got, from the appropriate binomial distribution.\nThe P-value is the probability of getting 21 (or 22) positive differences out of 24 (or 23) or more, since this is the end of the distribution we should be at if the alternative hypothesis is correct. Thus any of these will get you a defensible P-value:\n\nsum(dbinom(21:23,23,0.5))\n\n[1] 3.302097e-05\n\nsum(dbinom(22:24,24,0.5))\n\n[1] 1.7941e-05\n\nsum(dbinom(21:24,24,0.5))\n\n[1] 0.0001385808\n\nsum(dbinom(0:2,23,0.5))\n\n[1] 3.302097e-05\n\nsum(dbinom(0:2,24,0.5))\n\n[1] 1.7941e-05\n\nsum(dbinom(0:3,24,0.5))\n\n[1] 0.0001385808\n\n\nThe first and fourth of those are the same as smmr (throwing away the exactly-median value).\nAs we hoped, there is no material difference here: there is no doubt with any of these possibilities that we will reject a median difference of zero in favour of a median difference greater than zero.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#throwing-baseballs-and-softballs-again-1",
    "href": "matched-pairs-sign.html#throwing-baseballs-and-softballs-again-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.10 Throwing baseballs and softballs, again",
    "text": "9.10 Throwing baseballs and softballs, again\nPreviously, you carried out a sign test to determine whether students could throw a baseball farther than a softball. This time, we will calculate a confidence interval for the median difference baseball minus softball, using the results of sign tests.\n\nRead the data into R from link, giving appropriate names to the columns, and add a column of differences.\n\nSolution\nI did it this way, combining the reading of the data with the calculation of the differences in one pipe:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/throw.txt\"\nthrows &lt;- read_delim(myurl, \" \", col_names = c(\"student\", \"baseball\", \"softball\")) %&gt;%\n  mutate(diff = baseball - softball)\n\nRows: 24 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): student, baseball, softball\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nthrows\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nUse smmr to find a 95% confidence interval for the median difference.\n\nSolution\nci_median, with 95% being the default confidence level:\n\nci_median(throws, diff)\n\n[1] 2.002930 8.999023\n\n\n2 to 9. The ends of a CI for the median will be data values, which are all whole numbers, so round off that 8.999.\n\\(\\blacksquare\\)\n\nWhat function in smmr will run a two-sided sign test and return only the P-value? Check that it works by testing whether the median difference for your data is zero or different from zero.\n\nSolution\nThe rest of the way, we are trying to reproduce that confidence interval by finding it ourselves. The function is called pval_sign. If you haven’t run into it before, in R Studio click on Packages, find smmr, and click on its name. This will bring up package help, which includes a list of all the functions in the package, along with a brief description of what each one does. (Clicking on a function name brings up the help for that function.) Let’s check that it works properly by repeating the previous sign_test and verifying that pval_sign gives the same thing:\n\nsign_test(throws, diff, 0)\n\n$above_below\nbelow above \n    2    21 \n\n$p_values\n  alternative      p_value\n1       lower 9.999971e-01\n2       upper 3.302097e-05\n3   two-sided 6.604195e-05\n\npval_sign(0, throws, diff)\n\n[1] 6.604195e-05\n\n\nThe P-values are the same (for the two-sided test) and both small, so the median difference is not zero.\n\\(\\blacksquare\\)\n\nBased on your P-value, do you think 0 is inside the confidence interval or not? Explain briefly.\n\nSolution\nAbsolutely not. The median difference is definitely not zero, so zero cannot be in the confidence interval. Our suspicion, from the one-sided test from earlier, is that the differences were mostly positive (people could throw a baseball farther than a softball, in most cases). So the confidence interval ought to contain only positive values. I ask this because it drives what happens below.\n\\(\\blacksquare\\)\n\nObtain a 95% confidence interval for the population median difference, baseball minus softball, using a trial-and-error procedure that determines whether a number of possible medians are inside or outside the CI.\n\nSolution\nI’ve given you a fair bit of freedom to tackle this as you wish. Anything that makes sense is good: whatever mixture of mindlessness, guesswork and cleverness that you want to employ. The most mindless way to try some values one at a time and see what you get, eg.:\n\npval_sign(1, throws, diff)\n\n[1] 0.001489639\n\npval_sign(5, throws, diff)\n\n[1] 1.168188\n\n\nSo median 1 is outside and median 5 is inside the 95% interval. Keep trying values until you’ve figured out where the lower and upper ends of the interval are: where the P-values cross from below 0.05 to above, or vice versa.\nSomething more intelligent is to make a long list of potential medians, and get the P-value for each of them, eg.:\n\nd &lt;- tibble(my.med = seq(0, 20, 2))\nd %&gt;% rowwise() %&gt;% \n  mutate(pvals = pval_sign(my.med, throws, diff))\n\n\n\n  \n\n\n\n2 is just inside the interval, 8 is also inside, and 10 is outside. Some closer investigation:\n\nd &lt;- tibble(my.med = seq(0, 2, 0.5))\nd %&gt;% rowwise() %&gt;% \n  mutate(pvals = pval_sign(my.med, throws, diff))\n\n\n\n  \n\n\n\nThe bottom end of the interval actually is 2, since 2 is inside and 1.5 is outside.\n\nd &lt;- tibble(my.med = seq(8, 10, 0.5))\nd %&gt;% rowwise() %&gt;% \n  mutate(pvals = pval_sign(my.med, throws, diff))\n\n\n\n  \n\n\n\nThe top end is 9, 9 being inside and 9.5 outside.\nSince the data values are all whole numbers, I think this is accurate enough. The most sophisticated way is the “bisection” idea we saw before. We already have a kickoff for this, since we found, mindlessly, that 1 is outside the interval on the low end and 5 is inside, so the lower limit has to be between 1 and 5. Let’s try halfway between, ie. 3:\n\npval_sign(3, throws, diff)\n\n[1] 0.3833103\n\n\nInside, so lower limit is between 1 and 3. This can be automated, thus:\n\nlo &lt;- 1\nhi &lt;- 3\nwhile (abs(hi - lo) &gt; 0.1) {\n  try &lt;- (lo + hi) / 2\n  ptry &lt;- pval_sign(try, throws, diff)\n  if (ptry &gt; 0.05) {\n    hi &lt;- try\n  } else {\n    lo &lt;- try\n  }\n}\nc(lo, hi)\n\n[1] 1.9375 2.0000\n\n\nThe difficult bit is to decide whether the value try becomes the new lo or the new hi. If the P-value for the median of try is greater than 0.05, try is inside the interval, and it becomes the new hi; otherwise it’s outside and becomes the new lo. Whatever the values are, lo is always outside the interval and hi is always inside, and they move closer and closer to each other.\nAt the other end of the interval, lo is inside and hi is outside, so there is a little switching around within the loop. For starting values, you can be fairly mindless: for example, we know that 5 is inside and something big like 20 must be outside:\n\nlo &lt;- 5\nhi &lt;- 20\nwhile (abs(hi - lo) &gt; 0.1) {\n  try &lt;- (lo + hi) / 2\n  ptry &lt;- pval_sign(try, throws, diff)\n  if (ptry &gt; 0.05) {\n    lo &lt;- try\n  } else {\n    hi &lt;- try\n  }\n}\nc(lo, hi)\n\n[1] 8.984375 9.042969\n\n\nThe interval goes from 2 to (as calculated here) about 9. (This is apparently the same as ci_median in smmr got.) ci_median uses the bisection method with a smaller “tolerance” than we did, so its answer is more accurate. It looks as if the interval goes from 2 to 9: that is, students can throw a baseball on average between 2 and 9 feet further than they can throw a softball.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#changes-in-salary-1",
    "href": "matched-pairs-sign.html#changes-in-salary-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.11 Changes in salary",
    "text": "9.11 Changes in salary\nA company is growing and would like to attract more employees. The company would like to advertise that salaries there are increasing. To do this, the company randomly samples 20 employees that have been working there since January 2016, and for each of these employees, records their salary in January 2016 and January 2017. The data, with salaries in thousands of dollars, are in link.\n\nRead the data into R and demonstrate that you have two salaries for each of 20 employees.\n\nSolution\nLooking at the file, we see that the values are separated by exactly one space:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/salaryinc.txt\"\nsalaries &lt;- read_delim(my_url, \" \")\n\nRows: 20 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): employee\ndbl (2): jan2016, jan2017\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsalaries\n\n\n\n  \n\n\n\nThere are 20 employees (rows), and two columns of salaries: for each employee in the data set, their salary in January 2016 and in January 2017 (thus, two salaries for each employee).\n\\(\\blacksquare\\)\n\nTo compare the salaries, explain briefly why a matched-pairs test would be better than a two-sample test.\n\nSolution\nA matched-pairs test would be better because we have two observations (salaries) for each subject (employee). A two-sample test would be appropriate if we had two separate sets of employees, one set with their salaries recorded in 2016 and the other with their salaries recorded in 2017. That is not what we have here. You can go after this either way: why a matched-pairs approach is appropriate, or why a two-sample approach is not (or a bit of both).\n\\(\\blacksquare\\)\n\nMake a suitable graph to assess the assumptions for a matched-pairs \\(t\\)-test. What does your graph tell you?\n\nSolution\nThis requires thought first before you do any coding (and this is the reason for this one being four points). What has to be at least approximately normally distributed is the set of differences, salary at one time point minus the salary at the other, for each employee. The individual salaries don’t have to be normally distributed at all. We don’t have the differences here, so we have to calculate them first. The smoothest way is to make a pipeline:\n\nsalaries %&gt;%\n  mutate(diff = jan2017 - jan2016) %&gt;%\n  ggplot(aes(sample = diff)) + stat_qq() + stat_qq_line()\n\n\n\n\nA couple of coding notes: (i) you can take the differences 2016 minus 2017 if you like (then they will tend to be negative), (ii) ggplot used in a pipeline like this does not have a data frame first (the data frame used is the nameless output from the mutate, with the differences in it).\nAlso, there’s no problem doing the mutate, saving that, and then feeding the saved data frame into ggplot. If you find that clearer, go for it.\nAs for what I see: I think those points get a bit far from the line at the high and low ends: the high values are too high and the low values are too low, which is to say that we have outliers at both ends, or the distribution has long tails (either way of saying it is good).\nThe important conclusion here is whether these differences are normal enough to trust a matched pairs \\(t\\)-test here. We have a sample of size 20, so the central limit theorem will help us some, but you can reasonably say that these tails are too long and that we should not trust a matched-pairs \\(t\\)-test.\nI actually wanted you to practice doing a matched-pairs \\(t\\)-test anyway, hence my comment in the next part, but the result is probably not so trustworthy.\n\\(\\blacksquare\\)\n\nCarry out a suitable matched-pairs \\(t\\)-test on these data. (If you thought in the previous part that this was the wrong thing to do, do it anyway for the purposes of this assignment.) What do you conclude?\n\nSolution\nThe company is trying to prove that salaries are increasing over time, so we need a one-sided alternative. Following through the procedure, even though you may not trust it much:\n\nwith(salaries, t.test(jan2016, jan2017, alternative = \"less\", paired = TRUE))\n\n\n    Paired t-test\n\ndata:  jan2016 and jan2017\nt = -10.092, df = 19, p-value = 2.271e-09\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n      -Inf -5.125252\nsample estimates:\nmean difference \n         -6.185 \n\n\nYou could also have the years the other way around, in which case the alternative has to be the other way around as well:\n\nwith(salaries, t.test(jan2017, jan2016, alternative = \"greater\", paired = TRUE))\n\n\n    Paired t-test\n\ndata:  jan2017 and jan2016\nt = 10.092, df = 19, p-value = 2.271e-09\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 5.125252      Inf\nsample estimates:\nmean difference \n          6.185 \n\n\nOr, if you saved the data frame with the differences in it, do a one-sample test on those, again making sure that you get the alternative right. I didn’t save it, so I’m calculating the differences again:\n\nsalaries %&gt;%\n  mutate(diff = jan2017 - jan2016) %&gt;%\n  with(., t.test(diff, mu = 0, alternative = \"greater\"))\n\n\n    One Sample t-test\n\ndata:  diff\nt = 10.092, df = 19, p-value = 2.271e-09\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 5.125252      Inf\nsample estimates:\nmean of x \n    6.185 \n\n\nWhichever way you do it, the P-value is the same \\(2.271 \\times 10^{-9}\\), which is a whole lot less than 0.05, so there is no doubt at all that salaries are increasing.\n(Your intuition ought to have expected something like this, because everyone’s 2017 salary appears to be greater than their 2016 salary.)\nExtra: you might be feeling that we ought to be doing a matched-pairs sign test, which you could do this way:\n\nlibrary(smmr)\nsalaries %&gt;%\n  mutate(diff = jan2017 - jan2016) %&gt;%\n  sign_test(diff, 0)\n\n$above_below\nbelow above \n    0    20 \n\n$p_values\n  alternative      p_value\n1       lower 1.000000e+00\n2       upper 9.536743e-07\n3   two-sided 1.907349e-06\n\n\nand then take the “upper” P-value, which is in the same ballpark as the one from the \\(t\\)-test. So the salaries really are increasing, whether you believe the \\(t\\)-test or not. And note that every single employee’s salary increased.\n(Again, the “missing” data frame in sign_test is the nameless one with the differences in it.)\n\\(\\blacksquare\\)\n\nThe company would like to estimate how much salaries are increasing, on average. Obtain some output that will enable the company to assess this, and tell the CEO which piece of the output they should look at.\n\nSolution\nA confidence interval. 95% is fine. As before, we have to run t.test again because we ran a one-sided test and a confidence interval for us is two-sided:\n\nwith(salaries, t.test(jan2017, jan2016, paired = TRUE))\n\n\n    Paired t-test\n\ndata:  jan2017 and jan2016\nt = 10.092, df = 19, p-value = 4.542e-09\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 4.902231 7.467769\nsample estimates:\nmean difference \n          6.185 \n\n\nBetween about $5,000 and about $7,500. This is what to tell the CEO.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#body-fat-revisited-1",
    "href": "matched-pairs-sign.html#body-fat-revisited-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.12 Body fat revisited",
    "text": "9.12 Body fat revisited\nAthletes are concerned with measuring their body fat percentage. Two different methods are available: one using ultrasound, and the other using X-ray technology. We are interested in whether there is a difference in the mean body fat percentage as measured by these two methods, and if so, how big that difference is. Data on 16 athletes are at link.\nWe saw this data set before.\n\nRead in the data again.\n\nSolution\nThis kind of thing. Since you looked at the data (didn’t you?), you’ll know that the values are separated by single spaces:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/bodyfat.txt\"\nbodyfat &lt;- read_delim(myurl, \" \")\n\nRows: 16 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): athlete, xray, ultrasound\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbodyfat\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nCalculate the differences, and make a normal quantile plot of them. Is there any evidence that normality of differences fails? Explain briefly.\n\nSolution\nThis is a good place to look ahead. We’ll need the differences in two places, most likely: first for the normal quantile plot, and second for the matched-pairs sign test. So we should calculate and save them first:\n\nbodyfat %&gt;% mutate(diff = xray - ultrasound) -&gt; bodyfat2\n\nI seem to be using a 2 on the end to name my dataframe-with-differences, but you can use whatever name you like.\nThen, not forgetting to use the data frame that we just made:\n\nggplot(bodyfat2, aes(sample = diff)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis is showing a little evidence of skewness or outliers (depending on your point of view: either is good). The lowest and highest values are both too high, and the pattern of points on the plot is kind of curved (which would be evidence of skewness). Or you could say that the two highest values are too high, with the other values being more or less in line (that would be evidence of outliers at the upper end). I like outliers better than skewness, since those bottom-end points are not far off the line. I would also accept “no substantial problems”, if you can make the case that those two highest points are not too far off the line. With only 16 observations as we have here, even truly normal data would stray off the line a bit.\nAs ever, your explanation is more important than your conclusion. Can you justify what you think?\nIf you took your differences the other way around, as ultrasound minus xray, your plot will also be the other way around, with the “outliers” at the bottom. That’s good too.\n\\(\\blacksquare\\)\n\nPreviously, we did a matched-pairs \\(t\\)-test for these data. In the light of your normal quantile plot, do you think that was a good idea? Explain briefly.\n\nSolution\nWe are looking for the differences to be approximately normal, bearing in mind that we have a sample of size 16, which is not that large. Say what you think here; the points, if I were giving any here, would be for the way in which you support it. The comment I made before when we did a matched-pairs \\(t\\)-test was that the P-value was so large and non-significant that it was hard to imagine any other test giving a significant result. Another way of saying that is that I considered these differences to be “normal enough”, given the circumstances. You might very well take a different view. You could say that these differences are clearly not normal, and that the sample size of 16 is not large enough to get any substantial help from the Central Limit Theorem. From that point of view, running the \\(t\\)-test is clearly not advisable.\n\\(\\blacksquare\\)\n\nUse the sign test appropriately to compare the two methods for measuring body fat. (Use smmr if you wish.) What do you conclude, as ever in the context of the data?\n\nSolution\nThat means using a sign test to test the null hypothesis that the median difference is zero, against the alternative that it is not zero. (I don’t see anything here to indicate that we are looking only for positive or only for negative differences, so I think two-sided is right. You need some reason to do a one-sided test, and there isn’t one here.)\nRemembering again to use the data frame that has the differences in it:\n\nsign_test(bodyfat2, diff, 0)\n\n$above_below\nbelow above \n   10     6 \n\n$p_values\n  alternative   p_value\n1       lower 0.2272491\n2       upper 0.8949432\n3   two-sided 0.4544983\n\n\nThe two-sided P-value is 0.4545, so we are nowhere near rejecting the null hypothesis that the median difference is zero. There is no evidence that the two methods for measuring body fat show any difference on average.\nThe table of aboves and belows says that there were 6 positive differences and 10 negative ones. This is not far from an even split, so the lack of significance is entirely what we would expect.\nExtra: this is the same conclusion that we drew the last time we looked at these data (with a matched-pairs \\(t\\)-test). That supports what I said then, which is that the \\(t\\)-test was so far from being significant, that it could be very wrong without changing the conclusion. That is what seems to have happened.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#the-dentist-and-blood-pressure-1",
    "href": "matched-pairs-sign.html#the-dentist-and-blood-pressure-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.13 The dentist and blood pressure",
    "text": "9.13 The dentist and blood pressure\nGoing to the dentist is scary for a lot of people. One way in which this might show up is that people might have higher blood pressure on average before their dentist’s appointment than an hour after the appointment is done. Ten randomly-chosen individuals have their (systolic) blood pressure measured while they are in a dentist’s waiting room, and then again one hour after their appointment is finished.\nThe data are in http://ritsokiguess.site/datafiles/blood_pressure1.csv.\n\nRead in and display the data.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/blood_pressure1.csv\"\nblood_pressure &lt;- read_csv(my_url)\n\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): person\ndbl (2): before, after\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nblood_pressure\n\n\n\n  \n\n\n\nAside: A blood pressure is usually given as two numbers, like ``120 over 80’’. The first number, which is the one shown in our data, is called the systolic blood pressure. It is the pressure in the arteries when the heart is pumping. The second is called the diastolic blood pressure, and it is the pressure in the arteries when the heart is resting.\n\\(\\blacksquare\\)\n\nWhat kind of experimental design is this? How do you know? Explain briefly.\n\nSolution\nThis is a matched pairs design. We know this because we have two measurements on each person, or the same people were measured before and after seeing the dentist. (The thing that it is not is one group of people measured before seeing the dentist, and a different group of people measured afterwards, so a two-sample test is not the right thing.)\n\\(\\blacksquare\\)\n\nRun a suitable \\(t\\)-test on these data. What do you conclude, in the context of the data?\n\nSolution\nA matched-pairs \\(t\\)-test, then. Remember, we want to see whether blood pressure is lower afterwards (that is, before is greater than after), so this needs to be one-sided:\n\nwith(blood_pressure, t.test(before, after, alternative = \"greater\", paired = TRUE))\n\n\n    Paired t-test\n\ndata:  before and after\nt = 2.9945, df = 9, p-value = 0.007545\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 2.210659      Inf\nsample estimates:\nmean difference \n            5.7 \n\n\nThere are some variations possible here: before and after could be switched (in which case alternative must be reversed also).\nOr, you can do a one-sample \\(t\\) on the differences, with the right alternative corresponding to the way you took differences. If you are looking ahead, you might realize that working out the differences now and adding them to the dataframe will be a good idea:\n\nblood_pressure %&gt;% \nmutate(difference = before - after) -&gt; blood_pressure\nblood_pressure\n\n\n\n  \n\n\n\nI took the differences this way around since I was expecting, if anything, the before numbers to be bigger than the after ones. And then:\n\nwith(blood_pressure, t.test(difference, mu = 0, alternative = \"greater\"))\n\n\n    One Sample t-test\n\ndata:  difference\nt = 2.9945, df = 9, p-value = 0.007545\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 2.210659      Inf\nsample estimates:\nmean of x \n      5.7 \n\n\nIf you did the differences the other way around, your alternative will need to be the other way around also.\nThe P-value (either way) is 0.008,3 so we have evidence that the mean blood pressure before is greater than the mean blood pressure after.\n\\(\\blacksquare\\)\n\nRun a suitable sign test on these data. What do you conclude, in the context of the data?\n\nSolution\nA sign test on the differences. By this point, you will realize that you will need to have obtained the differences. Get them here if you did not already get them:\n\nsign_test(blood_pressure, difference, 0)\n\n$above_below\nbelow above \n    2     8 \n\n$p_values\n  alternative   p_value\n1       lower 0.9892578\n2       upper 0.0546875\n3   two-sided 0.1093750\n\n\nThis one gives us all three P-values. The way around I found the differences, the one we want is “upper”, 0.055. There is not quite evidence that median blood pressure before is higher.\n\\(\\blacksquare\\)\n\nDraw a suitable normal quantile plot of these data, one that will enable you to decide between the tests you ran in the previous two parts.\n\nSolution\nThe differences are supposed to be approximately normal if a matched-pairs \\(t\\)-test is the thing:\n\nggplot(blood_pressure, aes(sample=difference)) + stat_qq() + stat_qq_line()\n\n\n\n\n\\(\\blacksquare\\)\n\nDiscuss briefly which of your two tests is the more appropriate one to run.\n\nSolution\nMake a call about whether the differences are normal enough. You have a couple of angles you can take:\n\nthe lowest two values are too low, so we have two outliers at the low end\nthe lowest and highest values are too extreme, so that we have a long-tailed distribution\n\nEither of these would suggest a non-normal distribution, which I think you have to conclude from this plot.\nThe best answer also considers the sample size: there are only 10 differences, a small sample size, and so we will not get much help from the Central Limit Theorem (the sample size is likely not enough4 to overcome those two outliers or the long tails). Thus, we should not trust the \\(t\\)-test and should prefer the sign test.\nExtra: you might be disappointed to go through this and come to the conclusion that there was not a decrease in blood pressure between before and after.\nWhat has happened, I think, is that we have only a small sample (10 people), and having 8 positive differences and 2 negative ones is not quite unbalanced enough (with such a small sample) to rule out chance: that is to say, a median difference of zero. The \\(t\\)-test accounted for the size of the differences, and if you believed the normality was satisfactory, you could demonstrate a difference between before and after. But if you didn’t like the normality, you were out of luck: the only test you have is an apparently not very powerful one.\nIf you wanted to, you could bootstrap the sampling distribution of the sample mean and see how normal it looks:\n\ntibble(sim =  1:10000) %&gt;% \n  rowwise() %&gt;% \n  mutate(the_sample = list(sample(blood_pressure$difference, replace = TRUE))) %&gt;% \n  mutate(the_mean = mean(the_sample)) %&gt;% \n  ggplot(aes(sample = the_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\n(Code note: you can do anything with the result of a simulation, and you can use anything that might need to be normal as input to a normal quantile plot. Now that we have the normal quantile plot as a tool, we can use it wherever it might be helpful.)\nThis is actually not nearly as bad as I was expecting. Even a sample size of 10 is providing some help. The bootstrapped sampling distribution is somewhat left-skewed, which is not a surprise given the two low outliers. However, it is rather close to normal, suggesting that the \\(t\\)-test is not as bad as we thought.\n(I did 10,000 simulations because I was having trouble seeing how non-normal it was. With this many, I can be pretty sure that this distribution is somewhat left-skewed.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#french-teachers-1",
    "href": "matched-pairs-sign.html#french-teachers-1",
    "title": "9  Matched pairs t and sign test",
    "section": "9.14 French teachers",
    "text": "9.14 French teachers\nTwenty high-school French teachers attended a summer institute to improve their French skills. At the beginning of their session, each teacher took a listening test (to test their understanding of spoken French). After 4 weeks of immersion in French, each teacher took a similar listening test again. (The actual French spoken in the two tests was different, so simply taking the first test should not improve the score in the second one; the tests were otherwise similar.) The maximum score on each test was 36, and a higher score is better. The data are here. (Right-click on the link, select “copy link address”, and then paste that URL into R Studio.) The data values are separated by tabs.\nThe data file has three columns:\n\nan identification for each teacher\nthe teacher’s score in the first test\nthe teacher’s score in the second test\n\n\nRead in and display (some of) the data.\n\nSolution\nSeparated by tabs means read_tsv:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/frenchtest.txt\"\nfrench &lt;- read_tsv(my_url)\n\nRows: 20 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (3): id, pre, post\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfrench\n\n\n\n  \n\n\n\nAs promised. The score on the first test is called pre and on the second is called post.\n\\(\\blacksquare\\)\n\nExplain briefly why this is a matched-pairs study.\n\nSolution\nThere are two measurements for each teacher, or, the 20 pre measurements and the 20 post measurements are paired up, namely, the ones that come from the same teacher. Or, if it were two independent samples, our 40 measurements would come from 40 different teachers, but there are only 20 teachers, so the 40 measurements must be paired up.\n\\(\\blacksquare\\)\n\nRun a suitable matched-pairs \\(t\\)-test to see whether the teachers’ scores have on average improved over the four weeks.\n\nSolution\nSeeing whether the scores have improved implies a one-sided test that post is bigger than pre. There are three ways you might do that, any of which is good. Remember that if you are running a test with paired = TRUE, the alternative is relative to the column that is input first, not the first one in alphabetical order or anything like that:\n(i):\n\nwith(french, t.test(pre, post, paired = TRUE, alternative = \"less\"))\n\n\n    Paired t-test\n\ndata:  pre and post\nt = -3.8649, df = 19, p-value = 0.0005216\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n      -Inf -1.381502\nsample estimates:\nmean difference \n           -2.5 \n\n\n(ii):\n\nwith(french, t.test(post, pre, paired = T, alternative = \"greater\"))\n\n\n    Paired t-test\n\ndata:  post and pre\nt = 3.8649, df = 19, p-value = 0.0005216\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 1.381502      Inf\nsample estimates:\nmean difference \n            2.5 \n\n\nYour choice between these two might be influenced by whether you think pre comes first, or whether you think it’s easier to decide how post compares to pre. It’s all down to what seems natural to you.\n\nworking out the differences and testing those (but look ahead in the question to see whether you need the differences for anything else: you do):\n\n\nfrench %&gt;% mutate(gain = post - pre) -&gt; french1\nwith(french1, t.test(gain, mu=0, alternative = \"greater\"))\n\n\n    One Sample t-test\n\ndata:  gain\nt = 3.8649, df = 19, p-value = 0.0005216\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 1.381502      Inf\nsample estimates:\nmean of x \n      2.5 \n\n\nThis last is an ordinary one-sample test, which saves you having to learn anything new, but requires you to calculate the differences first. You will need the differences for a plot anyway, so this may not be as much extra work as it appears. The right thing to do here is to save the data frame with the differences in it, so that you don’t need to calculate them again later.\nA fourth alternative is to calculate the differences as pre minus post, and then switch the alternative around (since if going to the French institute helps, the differences this way will be mostly negative):\n\nfrench %&gt;% mutate(gain = pre - post) -&gt; french2\nwith(french2, t.test(gain, mu=0, alternative = \"less\"))\n\n\n    One Sample t-test\n\ndata:  gain\nt = -3.8649, df = 19, p-value = 0.0005216\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n      -Inf -1.381502\nsample estimates:\nmean of x \n     -2.5 \n\n\n\\(\\blacksquare\\)\n\nWhat do you conclude from your test, in the context of the data?\n\nSolution\nThe P-value of 0.0005 is much less than 0.05, so we reject the null hypothesis that the mean scores before and after are the same, in favour of the alternative that the mean score afterwards is higher. That is to say, the four-week program is helping the teachers improve their understanding of spoken French.\n\\(\\blacksquare\\)\n\nHow much is the teachers’ listening skill improving, on average? Give a suitable interval to support your answer.\n\nSolution\nA 95% (or other level) confidence interval for the mean difference. A one-sided test doesn’t give that, so you need to do the test again without the alternative (to make it two-sided), via any of the methods above, such as:\n\nwith(french, t.test(post, pre, paired = TRUE))\n\n\n    Paired t-test\n\ndata:  post and pre\nt = 3.8649, df = 19, p-value = 0.001043\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.146117 3.853883\nsample estimates:\nmean difference \n            2.5 \n\n\nThis says that, with 95% confidence, the mean test score afterwards is between about 1.1 and 3.9 points higher than before. So that’s how much listening skill is improving on average. Give the suitably rounded interval; the test scores are whole numbers, and there are 20 differences making up the mean, so one decimal is the most you should give.\nIf you did it the first way:\n\nwith(french, t.test(pre, post, paired = TRUE))\n\n\n    Paired t-test\n\ndata:  pre and post\nt = -3.8649, df = 19, p-value = 0.001043\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.853883 -1.146117\nsample estimates:\nmean difference \n           -2.5 \n\n\nyou have given yourself a bit of work to do, because this is before minus after, so you have to strip off the minus signs and switch the numbers around. Giving the answer with the minus signs is wrong, because I didn’t ask about before minus after. Disentangle it, though, and you’re good.\n\\(\\blacksquare\\)\n\nMake a suitable plot to assess any assumptions for this test.\n\nSolution\nThe key assumption here is that the differences are approximately normally distributed.\nFirst calculate and save the differences (since you will need them later for a sign test; otherwise you would have to find them again). If you found the differences to make your \\(t\\)-test, use the ones you saved there.\n\nfrench %&gt;% mutate(gain = post - pre) -&gt; french1\n\nAssess that with a histogram (with suitable number of bins):\n\nggplot(french1, aes(x=gain)) + geom_histogram(bins=6)\n\n\n\n\nor, better, a normal quantile plot (since the normality is our immediate concern):\n\nggplot(french1, aes(sample=gain)) + stat_qq() + stat_qq_line()\n\n\n\n\n(note that the horizontal lines of points are because the test scores were whole numbers, therefore the differences between them are whole numbers also, and some of the teachers had the same difference in scores as others.)\n\\(\\blacksquare\\)\n\nDo you trust the result of your matched-pairs \\(t\\)-test? Explain briefly.\n\nSolution\nThere are about three considerations here:\n\nthe plot shows an outlier at the low end, but no other real problems.\nthe sample size is 20, so we should get some help from the Central Limit Theorem.\nthe P-value was really small.\n\nI expect you to mention the first two of those. Make a call about whether you think that outlier is too much of a problem, given the sample size. You could, I think, go either way with this one.\nThe third of my points says that even if the distribution of differences is not normal enough, and so the P-value is off by a bit, it would take a lot to change it enough to stop it being significant. So I don’t think we need to worry, for myself.\nExtra:\nWe can assess the \\(t\\)-test by obtaining a bootstrap distribution of the sample mean, by sampling from the differences with replacement:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(french1$gain, replace = T))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nThe bootstrapped sampling distribution of the sample mean difference is about as normal as you could reasonably wish for, so there was no need to worry. Only a very few of the most extreme samples were at all off the line.\nA histogram would be almost as good, but now that you know about the normal quantile plot, the time to use it is when you are specifically interested in normality, as you are here. (If you were interested in shape generally, then a histogram or, if appropriate, a boxplot, would also work.)\nThe code: the first line takes 1000 bootstrap samples, and the second finds the mean of each one. Instead of saving the sample means, since I was only going to be using them once, I made them into a dataframe, and then made a normal quantile plot of them. The enframe creates a dataframe with a column called value with the means in it, which I use in the plot.\n\\(\\blacksquare\\)\n\nRun a suitable sign test, and obtain a suitable (95%) confidence interval. Comment briefly on your results.\n\nSolution\nThis works with the differences, that you calculated for the plot, so use the data frame that you saved them in:\n\nsign_test(french1, gain, 0)\n\n$above_below\nbelow above \n    1    16 \n\n$p_values\n  alternative      p_value\n1       lower 0.9999923706\n2       upper 0.0001373291\n3   two-sided 0.0002746582\n\nci_median(french1, gain)\n\n[1] 1.007812 3.000000\n\n\nThe P-value is 0.00014, again very small, saying that the median difference is greater than zero, that is, that the test scores after are greater than the test scores before on average. The confidence interval is from 1 to 3 points, indicating that this is how much test scores are increasing on average.\nA technique thing: the first time you are going through this, you probably got to this point and realized that you were calculating the differences for the second (or third) time. This is the place to stop and think that you don’t really need to do that, and to go back to the plot you did and save the differences after you have calculated them. Then you edit the code here to use the differences you got before and saved. It doesn’t matter whether you see this the first time you do it or not, but it does matter that you see it before you hand it in. It’s like editing an essay; you need to go back through work that you will be handing in and make sure you did it the best way you could.\n\\(\\blacksquare\\)\n\nComment briefly on the comparison between your inferences for the mean and the median.\n\nSolution\nThe upper-tail P-value is 0.0001, in the same ballpark as the \\(t\\)-test (0.0005). The 95% confidence interval for the median difference is from 1 to 3,5 again much like the \\(t\\)-interval (1.1 to 3.9).\nThis suggests that it doesn’t matter much which test we do, and therefore that the \\(t\\)-test ought to be better because it uses the data better.6 This is more evidence that the outlier didn’t have that big of an effect.\nExtra: choosing a test on the basis of its P-value is wrong, because as soon as you introduce a choice on that basis, your P-value looks lower than it should; a P-value is based on you doing one test and only that one test. It is reasonable to note, as I did, that the two P-values are about the same and then choose between the tests on some other basis, such as that the \\(t\\)-test uses the data better.7\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "matched-pairs-sign.html#footnotes",
    "href": "matched-pairs-sign.html#footnotes",
    "title": "9  Matched pairs t and sign test",
    "section": "",
    "text": "My hat stays on my head.↩︎\nI learned yesterday that the Welsh word for “ironing” is smwddio, which seems weird until you say it out loud: it sounds like “smoothio”.↩︎\nGive the P-value, and round it off to about this accuracy so that your reader can see easily (i) how it compares to 0.05, and (ii) about how big it is. More than two decimal places is too many.↩︎\nBut see the Extra.↩︎\nI think I mentioned elsewhere that the P-value of the sign test, as it depends on the null median for a fixed data set, only changes at a data point. Therefore, the ends of a CI for the median must be data points.↩︎\nIt uses the actual data values, not just whether each one is positive or negative.↩︎\nIf the P-values had come out very different, that would be telling you that it matters which one you use, and you would need to go back and look at your plot to decide. Often, this happens when there is something wrong with the \\(t\\)-test, but not necessarily.↩︎"
  },
  {
    "objectID": "normal-quantile.html#lengths-of-heliconia-flowers",
    "href": "normal-quantile.html#lengths-of-heliconia-flowers",
    "title": "10  Normal quantile plots",
    "section": "10.1 Lengths of heliconia flowers",
    "text": "10.1 Lengths of heliconia flowers\nThe tropical flower Heliconia is fertilized by hummingbirds, a different species for each variety of Heliconia. Over time, the lengths of the flowers and the form of the hummingbirds’ beaks have evolved to match each other. The length of the Heliconia flower is therefore an important measurement. Does it have a normal distribution for each variety?\nThe data set at http://ritsokiguess.site/datafiles/heliconia.csv contains the lengths (in millimetres) of samples of flowers from each of three varieties of Heliconia: bihai, caribaea red, and caribaea yellow.\n\nRead the data into R. There are different numbers of length measurements for each variety. How does this show up in the data frame? (Look at all the rows, not just the first ten.)\nMake a normal quantile plot for the variety bihai.\nMake a normal quantile plot for the variety Caribaea red (note that the variable name in the data frame has an underscore in it).\nMake a normal quantile plot for the variety Caribaea yellow (this also has an underscore in it).\nWhich of the three varieties is closest to having a normal distribution? Explain (very) briefly.\nFor each of the two other varieties, apart from the one you mentioned in the last part, describe briefly how their distributions fail to be normal."
  },
  {
    "objectID": "normal-quantile.html#ferritin-and-normality",
    "href": "normal-quantile.html#ferritin-and-normality",
    "title": "10  Normal quantile plots",
    "section": "10.2 Ferritin and normality",
    "text": "10.2 Ferritin and normality\nIn the lecture notes, we looked at some data on different athletes from the Australian Institute of Sport. This data set can be found at http://ritsokiguess.site/datafiles/ais.txt. Recall that the values are separated by tabs. In this question, we will assess one of the variables in the data set for normality.\n\nRead the data set into R.\nOne of the variables, Ferr, is a measurement of Ferritin for each athlete. Obtain a normal quantile plot of the Ferritin values, for all the athletes together. What do you conclude about the shape of the distribution? Explain briefly.\nIt is possible that the shape you found in the previous part is because the athletes from all the different sports were mixed together. Use ggplot to obtain one normal quantile plot for each sport, collected together on one plot.\nLooking at the plots in the previous part, would you say that the Ferritin values for each sport individually have a more normal shape than they do for all the sports together? Explain briefly.\n\nMy solutions follow:"
  },
  {
    "objectID": "normal-quantile.html#lengths-of-heliconia-flowers-1",
    "href": "normal-quantile.html#lengths-of-heliconia-flowers-1",
    "title": "10  Normal quantile plots",
    "section": "10.3 Lengths of heliconia flowers",
    "text": "10.3 Lengths of heliconia flowers\nThe tropical flower Heliconia is fertilized by hummingbirds, a different species for each variety of Heliconia. Over time, the lengths of the flowers and the form of the hummingbirds’ beaks have evolved to match each other. The length of the Heliconia flower is therefore an important measurement. Does it have a normal distribution for each variety?\nThe data set at http://ritsokiguess.site/datafiles/heliconia.csv contains the lengths (in millimetres) of samples of flowers from each of three varieties of Heliconia: bihai, caribaea red, and caribaea yellow.\n\nRead the data into R. There are different numbers of length measurements for each variety. How does this show up in the data frame? (Look at all the rows, not just the first ten.)\n\nSolution\nThe usual read_csv:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/heliconia.csv\"\nheliconia &lt;- read_csv(my_url)\n\nRows: 23 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): bihai, caribaea_red, caribaea_yellow\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nI suggested to look at all the rows. Here’s why:\n\nheliconia \n\n\n\n  \n\n\n\nThe varieties with fewer values have missings (NAs) attached to the end. This is because all the columns in a data frame have to have the same number of values. (The missings won’t impact what we do below — we get a warning but not an error, and the plots are the same as they would be without the missings — but you might be aesthetically offended by them, in which case you can read what I do later on.)\n\\(\\blacksquare\\)\n\nMake a normal quantile plot for the variety bihai.\n\nSolution\nThere’s a certain amount of repetitiveness here (that we work around later):\n\nggplot(heliconia,aes(sample=bihai))+stat_qq()+stat_qq_line()\n\nWarning: Removed 7 rows containing non-finite values (`stat_qq()`).\n\n\nWarning: Removed 7 rows containing non-finite values (`stat_qq_line()`).\n\n\n\n\n\nI’m saving the comments until we’ve seen all three.\n\\(\\blacksquare\\)\n\nMake a normal quantile plot for the variety Caribaea red (note that the variable name in the data frame has an underscore in it).\n\nSolution\nSame idea again:\n\nggplot(heliconia,aes(sample=caribaea_red))+stat_qq()+stat_qq_line()\n\n\n\n\n\\(\\blacksquare\\)\n\nMake a normal quantile plot for the variety Caribaea yellow (this also has an underscore in it).\n\nSolution\nAnd, one more time:\n\nggplot(heliconia,aes(sample=caribaea_yellow))+stat_qq()+stat_qq_line()\n\nWarning: Removed 8 rows containing non-finite values (`stat_qq()`).\n\n\nWarning: Removed 8 rows containing non-finite values (`stat_qq_line()`).\n\n\n\n\n\nI did a lot of copying and pasting there.\n\\(\\blacksquare\\)\n\nWhich of the three varieties is closest to having a normal distribution? Explain (very) briefly.\n\nSolution\nLook at the three plots, and see which one stays closest to the line. To my mind, this is clearly the last one, Caribaea yellow. So your answer ought to be “Caribaea yellow, because the points are closest to the line”. This, I would say, is acceptably close to normal, so using a \\(t\\)-test here would be fine. The answer “the last one” is not quite complete, because I asked you which variety, so your answer needs to name a variety.\n\\(\\blacksquare\\)\n\nFor each of the two other varieties, apart from the one you mentioned in the last part, describe briefly how their distributions fail to be normal.\n\nSolution\nLet’s look at bihai first. I see this one as an almost classic curve: the points are above the line, then below, then above again. If you look at the data scale (\\(y\\)-axis), the points are too bunched up to be normal at the bottom, and too spread out at the top: that is, skewed to the right. You might also (reasonably) take the view that the points at the bottom are close to the line (not sure about the very smallest one, though), but the points at the top are farther away, so that what we have here is two outliers at the top. I’m OK with that. It’s often difficult to distinguish between skewness and outliers (at the end of the long tail). What you conclude can often depend on how you look. We also had to look at the second plot, caribaea red. This is a rather strange one: the points veer away from the line at the ends, but look carefully: it is not outliers at both ends, but rather the points are too bunched up to be normal at both ends: that is, the distribution has short tails compared to the normal. It is something more like a uniform distribution, which has no tails at all, than a normal distribution, which won’t have outliers but it does have some kind of tails. So, “short tails”.\nExtra: that’s all you needed, but I mentioned above that you might have been offended aesthetically by those missing values that were not really missing. Let’s see if we can do this aesthetically. As you might expect, it uses several of the tools from the “tidyverse”. First, tidy the data. The three columns of the data frame are all lengths, just lengths of different things, which need to be labelled. This is pivot_longer from tidyr:\n\nheliconia %&gt;% \n  pivot_longer(everything(), names_to=\"variety\", values_to=\"length\", values_drop_na = T) -&gt; heliconia.long\nheliconia.long  \n\n\n\n  \n\n\n\nThis is now aesthetic as well as tidy: all those NA lines have gone (you can check that there are now \\(16+23+15=54\\) rows of actual data, as there should be). This was accomplished by the last thing in the pivot_longer: “in the values (that is, the lengths), drop any missing values.”\nNow, how to get a normal quantile plot for each variety? This is facet_wrap on the end of the ggplot again.\n\nggplot(heliconia.long,aes(sample=length))+\nstat_qq()+stat_qq_line()+\nfacet_wrap(~variety,scale=\"free\")\n\n\n\n\nThese are a bit elongated vertically. The scale=\"free\" allows a different vertical scale for each plot (otherwise there would be one vertical scale for all three plots); I decided that was best here since the typical lengths for the three varieties are different. Caribaea yellow is more or less straight, bihai has outliers (and may also be curved), caribaea red has that peculiar S-bend shape.\nI didn’t really like the vertical elongation. I’d rather have the plots be almost square, which they would be if we put them in three cells of a \\(2 \\times 2\\) grid. facet_wrap has nrow and ncol which you can use one or both of to make this happen. This creates an array of plots with two columns and as many rows as needed:\n\nggplot(heliconia.long,aes(sample=length))+\nstat_qq()+stat_qq_line()+\nfacet_wrap(~variety,scale=\"free\",ncol=2)\n\n\n\n\nI think the squarer plots make it easier to see the shape of these: curved, S-bend, straightish. Almost the same code will get a histogram for each variety, which I’ll also make squarish:\n\nggplot(heliconia.long,aes(x=length))+\ngeom_histogram(bins=5)+facet_wrap(~variety,scale=\"free\",ncol=2)\n\n\n\n\nbihai has those two outliers, caribaea red has no tails to speak of (or you might say “it’s bimodal”, which would be another explanation of the pattern on the normal quantile plot1) and caribaea yellow is shoulder-shruggingly normal (I looked at that and said, “well, I guess it’s normal”.) After you’ve looked at the normal quantile plots, you see what a crude tool a histogram is for assessing normality.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "normal-quantile.html#ferritin-and-normality-1",
    "href": "normal-quantile.html#ferritin-and-normality-1",
    "title": "10  Normal quantile plots",
    "section": "10.4 Ferritin and normality",
    "text": "10.4 Ferritin and normality\nIn the lecture notes, we looked at some data on different athletes from the Australian Institute of Sport. This data set can be found at http://ritsokiguess.site/datafiles/ais.txt. Recall that the values are separated by tabs. In this question, we will assess one of the variables in the data set for normality.\n\nRead the data set into R.\n\nSolution\nread_tsv is the right thing:\n\nmy_url=\"http://ritsokiguess.site/datafiles/ais.txt\"\nathletes=read_tsv(my_url)\n\nRows: 202 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nathletes\n\n\n\n  \n\n\n\nI listed the data to check that I had it right, but I didn’t ask you to. (If you didn’t have it right, that will show up soon enough.)\n\\(\\blacksquare\\)\n\nOne of the variables, Ferr, is a measurement of Ferritin for each athlete. Obtain a normal quantile plot of the Ferritin values, for all the athletes together. What do you conclude about the shape of the distribution? Explain briefly.\n\nSolution\nAs you would expect:\n\nggplot(athletes, aes(sample=Ferr))+\nstat_qq()+stat_qq_line()\n\n\n\n\nThis is almost a classic right skew: the values are too bunched up at the bottom and too spread out at the top. The curved shape should be making you think “skewed” and then you can work out which way it’s skewed.\n\\(\\blacksquare\\)\n\nIt is possible that the shape you found in the previous part is because the athletes from all the different sports were mixed together. Use ggplot to obtain one normal quantile plot for each sport, collected together on one plot.\n\nSolution\nYour previous plot had all the sports mixed together. To that you add something that will put each sport in its own facet:\n\nggplot(athletes,aes(sample=Ferr))+stat_qq()+stat_qq_line()+\nfacet_wrap(~Sport)\n\n\n\n\n\\(\\blacksquare\\)\n\nLooking at the plots in the previous part, would you say that the Ferritin values for each sport individually have a more normal shape than they do for all the sports together? Explain briefly.\n\nSolution\nThere are a couple of ways you can go, and as ever I’m looking mostly for consistency of argument. The two major directions you can go are (i) most of these plots are still curved the same way as the previous one, and (ii) they are mostly straighter than they were before. Possible lines of argument include that pretty much all of these plots are right-skewed still, with the same upward-opening curve. Pretty much the only one that doesn’t is Gymnastics, for which there are only four observations, so you can’t really tell. So, by this argument, Ferritin just does have a right-skewed distribution, and breaking things out by sport doesn’t make much difference to that. Or, you could go another way and say that the plot of all the data together was very curved, and these plots are much less curved, that is to say, much less skewed. Some of them, such as basketball and netball, are almost straight, and they are almost normally distributed. Some of the distributions, such as track sprinting (TSprnt), are definitely still right-skewed, but not as seriously so as before. Decide what you think and then discuss how you see it.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "normal-quantile.html#footnotes",
    "href": "normal-quantile.html#footnotes",
    "title": "10  Normal quantile plots",
    "section": "",
    "text": "If you have studied a thing called kurtosis, the fourth moment about the mean, you’ll know that this measures both tail length and peakedness, so a short-tailed distribution also has a strong peak. Or, maybe, in this case, two strong peaks.↩︎"
  },
  {
    "objectID": "analysis-of-variance.html#movie-ratings-and-lengths",
    "href": "analysis-of-variance.html#movie-ratings-and-lengths",
    "title": "11  Analysis of variance",
    "section": "11.1 Movie ratings and lengths",
    "text": "11.1 Movie ratings and lengths\nBefore a movie is shown in theatres, it receives a “rating” that says what kind of material it contains. link explains the categories, from G (suitable for children) to R (anyone under 17 must be accompanied by parent/guardian). In 2011, two students collected data on the length (in minutes) and the rating category, for 15 movies of each rating category, randomly chosen from all the movies released that year. The data are at link.\n\nRead the data into R, and display (some of) what you read in.\nCount how many movies there are of each rating.\nCarry out an ANOVA and a Tukey analysis (if warranted).\nMake a graph to assess whether this ANOVA is trustworthy. Discuss your graph and its implications briefly."
  },
  {
    "objectID": "analysis-of-variance.html#deer-and-how-much-they-eat",
    "href": "analysis-of-variance.html#deer-and-how-much-they-eat",
    "title": "11  Analysis of variance",
    "section": "11.2 Deer and how much they eat",
    "text": "11.2 Deer and how much they eat\nDo adult deer eat different amounts of food at different times of the year? The data in link are the weights of food (in kilograms) consumed by randomly selected adult deer observed at different times of the year (in February, May, August and November). We will assume that these were different deer observed in the different months. (If the same animals had been observed at different times, we would have been in the domain of “repeated measures”, which would require a different analysis, beyond the scope of this course.)\n\nRead the data into R, and calculate numbers of observations and the median amounts of food eaten each month.\nMake side-by-side boxplots of the amount of food eaten each month. Comment briefly on what you see.\nRun a Mood’s median test as in lecture (ie. not using smmr). What do you conclude, in the context of the data?\nRun a Mood’s median test using smmr, and compare the results with the previous part.\nHow is it that Mood’s median test does not completely answer the question you really want to answer? How might you get an answer to the question you really want answered? Explain briefly, and obtain the answer you really want, discussing your results briefly."
  },
  {
    "objectID": "analysis-of-variance.html#movie-ratings-again",
    "href": "analysis-of-variance.html#movie-ratings-again",
    "title": "11  Analysis of variance",
    "section": "11.3 Movie ratings again",
    "text": "11.3 Movie ratings again\nThis question again uses the movie rating data at link.\n\nRead the data into R and obtain the number of movies of each rating and the median length of movies of each rating.\nObtain a suitable graph that assesses the assumptions for ANOVA. Why do you think it is not reasonable to run ANOVA here? Explain briefly.\nRun a Mood’s median test (use smmr if you like). What do you conclude, in the context of the data?"
  },
  {
    "objectID": "analysis-of-variance.html#atomic-weight-of-carbon",
    "href": "analysis-of-variance.html#atomic-weight-of-carbon",
    "title": "11  Analysis of variance",
    "section": "11.4 Atomic weight of carbon",
    "text": "11.4 Atomic weight of carbon\nThe atomic weight of the chemical element carbon is 12. Two methods of measuring the atomic weight of samples of carbon were compared. The results are shown in link. The methods are labelled 1 and 2. The first task is to find out whether the two methods have different “typical” measures (mean or median, as appropriate) of the atomic weight of carbon.\nFor this question, compose a report in R Markdown. (R Markdown is what you use in an R Notebook, but you can also have a separate R Markdown document from which you can produce HTML, Word etc. output.) See part (a) for how to get this started.\nYour report should read like an actual report, not just the answers to some questions that I set you. To help with that, write some text that links the parts of the report together smoothly, so that it reads as a coherent whole. The grader had 3 discretionary marks to award for the overall quality of your writing. The scale for this was:\n\n3 points: excellent writing. The report flows smoothly, is easy to read, and contains everything it should (and nothing it shouldn’t).\n2 points: satisfactory writing. Not the easiest to read, but says what it should, and it looks at least somewhat like a report rather than a string of answers to questions.\n1 point: writing that is hard to read or to understand. If you get this (or 0), you should consider what you need to do to improve when you write your project.\n0 points: you answered the questions, but you did almost nothing to make it read like a report.\n\n\nCreate a new R Markdown document. To do this, in R Studio, select File, New File, R Markdown. Type the report title and your name in the boxes, and leave the output on the default HTML. Click OK.\nWrite an introduction that explains the purpose of this study and the data collected in your own words.\nBegin an appropriately-titled new section in your report, read the data into R and display the results.\nMake an appropriate plot to compare the measurements obtained by the two methods. You might need to do something about the two methods being given as numbers even though they are really only identifiers. (If you do, your report ought to say what you did and why.)\nComment briefly on what you see in your plot.\nCarry out the most appropriate \\(t\\)-test. (You might like to begin another new section in your report here.)\nDo the most appropriate test you know that does not assume normally-distributed data.\nDiscuss the results of your tests and what they say about the two methods for measuring the atomic weight of carbon. If it seems appropriate, put the discussion into a section called Conclusions."
  },
  {
    "objectID": "analysis-of-variance.html#can-caffeine-improve-your-performance-on-a-test",
    "href": "analysis-of-variance.html#can-caffeine-improve-your-performance-on-a-test",
    "title": "11  Analysis of variance",
    "section": "11.5 Can caffeine improve your performance on a test?",
    "text": "11.5 Can caffeine improve your performance on a test?\nDoes caffeine help students do better on a certain test? To find out, 36 students were randomly allocated to three groups (12 in each group). Each student received a fixed number of cups of coffee while they were studying, but the students didn’t know whether they were receiving all full-strength coffee (“high”), all decaf coffee (“low”) or a 50-50 mixture of the two (“moderate”). For each subject, their group was recorded as well as their score on the test. The data are in link, as a .csv file.\n\nRead in and examine the data. How are the values laid out?\nExplain briefly how the data are not “tidy”.\nUse a suitable tool from the tidyverse to create one column of test scores and and one column of group labels. Call your column of group labels amount. Is it a factor?\nObtain side-by-side boxplots of test scores by amount of caffeine.\nDoes caffeine amount seem to have an effect? If so, what kind of effect?\nRun a suitable analysis of variance to determine whether the mean test score is equal or unequal for the three groups. What do you conclude?\nWhy is it a good idea to run Tukey’s method here?\nRun Tukey’s method. What do you conclude?"
  },
  {
    "objectID": "analysis-of-variance.html#reggae-music",
    "href": "analysis-of-variance.html#reggae-music",
    "title": "11  Analysis of variance",
    "section": "11.6 Reggae music",
    "text": "11.6 Reggae music\nReggae is a music genre that originated in Jamaica in the late 1960s. One of the most famous reggae bands was Bob Marley and the Wailers. In a survey, 729 students were asked to rate reggae music on a scale from 1, “don’t like it at all” to 6, “like it a lot”. We will treat the ratings as quantitative. Each student was also asked to classify their home town as one of “big city”, “suburban”, “small town”, “rural”. Does a student’s opinion of reggae depend on the kind of home town they come from? The data are in http://ritsokiguess.site/datafiles/reggae.csv.\n\nRead in and display (some of) the data.\nHow many students are from each different size of town?\nMake a suitable graph of the two variables in this data frame.\nDiscuss briefly why you might prefer to run Mood’s median test to compare ratings among home towns.\nSuppose that somebody wanted to run Welch ANOVA on these data. What would be a reasonable argument to support that?\nRun Mood’s median test and display the output.\nExplain briefly why running pairwise median tests is a good idea, run them, and display the results.\nSummarize, as concisely as possible, how the home towns differ in terms of their students’ ratings of reggae music."
  },
  {
    "objectID": "analysis-of-variance.html#watching-tv-and-education",
    "href": "analysis-of-variance.html#watching-tv-and-education",
    "title": "11  Analysis of variance",
    "section": "11.7 Watching TV and education",
    "text": "11.7 Watching TV and education\nThe General Social Survey is a large survey of a large number of people. One of the questions on the survey is “how many hours of TV do you watch in a typical day?” Another is “what is your highest level of education attained”, on this scale:\n\nHSorLess: completed no more than high h school\nCollege: completed some form of college, either a community college (like Centennial) or a four-year university (like UTSC)\nGraduate: completed a graduate degree such as an MSc.\n\nDo people with more education tend to watch more TV? We will be exploring this. The data are in http://ritsokiguess.site/datafiles/gss_tv.csv.\n\nRead in and display (some of) the data.\nFor each level of education, obtain the number of observations, the mean and the median of the number of hours of TV watched.\nWhat does your answer to the previous part tell you about the shapes of the distributions of the numbers of hours of TV watched? Explain briefly.\nObtain a suitable graph of your data frame.\nDoes your plot indicate that your guess about the distribution shape was correct? Explain briefly.\nRun a suitable test to compare the average number of hours of TV watched for people with each amount of education. (“Average” could be mean or median, whichever you think is appropriate.)\nWhat do you conclude from your test, in the context of the data?\nWhy might you now want to run some kind of follow-up test? Run the appropriate thing and explain briefly what you conclude from it, in the context of the data."
  },
  {
    "objectID": "analysis-of-variance.html#death-of-poets",
    "href": "analysis-of-variance.html#death-of-poets",
    "title": "11  Analysis of variance",
    "section": "11.8 Death of poets",
    "text": "11.8 Death of poets\nSome people believe that poets, especially female poets, die younger than other types of writer. William Butler Yeats1 wrote:\n\nShe is the Gaelic2 muse, for she gives inspiration to those she persecutes. The Gaelic poets die young, for she is restless, and will not let them remain long on earth.\n\nA literature student wanted to investigate this, and so collected a sample of 123 female writers (of three different types), and noted the age at death of each writer.\nThe data are in http://ritsokiguess.site/datafiles/writers.csv.\n\nRead in and display (some of) the data.\nMake a suitable plot of the ages and types of writing.\nObtain a summary table showing, for each type of writing, the number of writers of that type, along with the mean, median and standard deviation of their ages at death.\nRun a complete analysis, starting with an ordinary (not Welch) analysis of variance, that ends with a conclusion in the context of the data and an assessment of assumptions."
  },
  {
    "objectID": "analysis-of-variance.html#religion-and-studying",
    "href": "analysis-of-variance.html#religion-and-studying",
    "title": "11  Analysis of variance",
    "section": "11.9 Religion and studying",
    "text": "11.9 Religion and studying\nMany students at a certain university were asked about the importance of religion in their lives (categorized as “not”, “fairly”, or “very” important), and also about the number of hours they spent studying per week. (This was part of a much larger survey.) We want to see whether there is any kind of relationship between these two variables. The data are in here.\n\nRead in and display (some of) the data.\nObtain the number of observations and the mean and standard deviation of study hours for each level of importance.\nComment briefly on how the groups compare in terms of study hours.\nMake a suitable graph of this data set.\nThe statistician in this study decided that the data were sufficiently normal in shape given the (very large) sample sizes, but was concerned about unequal spreads among the three groups. Given this, run a suitable analysis and display the output. (This includes a suitable follow-up test, if warranted.)\nWhat do you conclude from your analysis of the previous part, in the context of the data?\n\nMy solutions follow:"
  },
  {
    "objectID": "analysis-of-variance.html#movie-ratings-and-lengths-1",
    "href": "analysis-of-variance.html#movie-ratings-and-lengths-1",
    "title": "11  Analysis of variance",
    "section": "11.10 Movie ratings and lengths",
    "text": "11.10 Movie ratings and lengths\nBefore a movie is shown in theatres, it receives a “rating” that says what kind of material it contains. link explains the categories, from G (suitable for children) to R (anyone under 17 must be accompanied by parent/guardian). In 2011, two students collected data on the length (in minutes) and the rating category, for 15 movies of each rating category, randomly chosen from all the movies released that year. The data are at link.\n\nRead the data into R, and display (some of) what you read in.\n\nSolution\nread_csv:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/movie-lengths.csv\"\nmovies &lt;- read_csv(my_url)\n\nRows: 60 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): rating\ndbl (1): length\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmovies\n\n\n\n  \n\n\n\nSomething that looks like a length in minutes, and a rating.\n\\(\\blacksquare\\)\n\nCount how many movies there are of each rating.\n\nSolution\n\nmovies %&gt;% count(rating)\n\n\n\n  \n\n\n\nFifteen of each rating. (It’s common to have the same number of observations in each group, but not necessary for a one-way ANOVA.)\n\\(\\blacksquare\\)\n\nCarry out an ANOVA and a Tukey analysis (if warranted).\n\nSolution\nANOVA first:\n\nlength.1 &lt;- aov(length ~ rating, data = movies)\nsummary(length.1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nrating       3  14624    4875   11.72 4.59e-06 ***\nResiduals   56  23295     416                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis P-value is 0.00000459, which is way less than 0.05.\nHaving rejected the null (which said “all means equal”), we now need to do Tukey, thus:\n\nTukeyHSD(length.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = length ~ rating, data = movies)\n\n$rating\n               diff        lwr       upr     p adj\nPG-G      26.333333   6.613562 46.053104 0.0044541\nPG-13-G   42.800000  23.080229 62.519771 0.0000023\nR-G       30.600000  10.880229 50.319771 0.0007379\nPG-13-PG  16.466667  -3.253104 36.186438 0.1327466\nR-PG       4.266667 -15.453104 23.986438 0.9397550\nR-PG-13  -12.200000 -31.919771  7.519771 0.3660019\n\n\nCast your eye down the p adj column and look for the ones that are significant, here the first three. These are all comparisons with the G (“general”) movies, which are shorter on average than the others (which are not significantly different from each other).\nIf you like, you can make a table of means to verify that:\n\nmovies %&gt;%\n  group_by(rating) %&gt;%\n  summarize(mean = mean(length))\n\n\n\n  \n\n\n\nWhen we do this problem in SAS, you’ll see the Tukey get handled a different way, one that you might find more appealing.\n\\(\\blacksquare\\)\n\nMake a graph to assess whether this ANOVA is trustworthy. Discuss your graph and its implications briefly.\n\nSolution\nThe obvious graph is a boxplot:\n\nggplot(movies, aes(x = rating, y = length)) + geom_boxplot()\n\n\n\n\nFor ANOVA, we are looking for approximately normal distributions within each group and approximately equal spreads. Without the outliers, I would be more or less happy with that, but the G movies have a low outlier that would pull the mean down and the PG and PG-13 movies have outliers that would pull the mean up. So a comparison of means might make the differences look more significant than they should. Having said that, you could also say that the ANOVA is very significant, so even considering the effect of the outliers, the differences between G and the others are still likely to be significant.\nExtra: the way to go if you don’t trust the ANOVA is (as for the two-sample \\(t\\)) the Mood’s median test. This applies to any number of groups, and works in the same way as before:\n\nlibrary(smmr)\nmedian_test(movies, length, rating)\n\n$table\n       above\ngroup   above below\n  G         2    13\n  PG        7     7\n  PG-13    12     3\n  R         8     6\n\n$test\n       what        value\n1 statistic 13.752380952\n2        df  3.000000000\n3   P-value  0.003262334\n\n\nStill significant, though not quite as small a P-value as before (which echoes our thoughts about what the outliers might do to the means). If you look at the table above the test results, you see that the G movies are mostly shorter than the overall median, but now the PG-13 movies are mostly longer. So the picture is a little different.\nMood’s median test does not naturally come with something like Tukey. What you can do is to do all the pairwise Mood’s median tests, between each pair of groups, and then adjust to allow for your having done several tests at once. I thought this was generally useful enough that I put it into smmr under the name pairwise_median_test:\n\npairwise_median_test(movies, length, rating)\n\n\n\n  \n\n\n\nYou can ignore those (adjusted) P-values rather stupidly bigger than 1. These are not significant.\nThere are two significant differences in median length: between G movies and the two flavours of PG movies. The G movies are significantly shorter (as you can tell from the boxplot), but the difference between G and R movies is no longer significant (a change from the regular ANOVA).\nYou may be puzzled by something in the boxplot: how is it that the G movies are significantly shorter than the PG movies, but not significantly shorter than the R movies, when the difference in medians between G and R movies is bigger? In Tukey, if the difference in means is bigger, the P-value is smaller.3 The resolution to this puzzle, such as it is, is that Mood’s median test is not directly comparing the medians of the groups (despite its name); it’s counting values above and below a joint median, which might be a different story.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#deer-and-how-much-they-eat-1",
    "href": "analysis-of-variance.html#deer-and-how-much-they-eat-1",
    "title": "11  Analysis of variance",
    "section": "11.11 Deer and how much they eat",
    "text": "11.11 Deer and how much they eat\nDo adult deer eat different amounts of food at different times of the year? The data in link are the weights of food (in kilograms) consumed by randomly selected adult deer observed at different times of the year (in February, May, August and November). We will assume that these were different deer observed in the different months. (If the same animals had been observed at different times, we would have been in the domain of “repeated measures”, which would require a different analysis, beyond the scope of this course.)\n\nRead the data into R, and calculate numbers of observations and the median amounts of food eaten each month.\n\nSolution\nThe usual stuff for data values separated by (single) spaces:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/deer.txt\"\ndeer &lt;- read_delim(myurl, \" \")\n\nRows: 22 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): month\ndbl (1): food\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nand then, recalling that n() is the handy way of getting the number of observations in each group:\n\ndeer %&gt;%\n  group_by(month) %&gt;%\n  summarize(n = n(), med = median(food))\n\n\n\n  \n\n\n\nWhen you want the number of observations plus some other summaries, as here, the group-by and summarize idea is the way, using n() to get the number of observations in each group. count counts the number of observations per group when you only have grouping variables.\nThe medians differ a bit, but it’s hard to judge without a sense of spread, which the boxplots (next) provide. November is a bit higher and May a bit lower.\n\\(\\blacksquare\\)\n\nMake side-by-side boxplots of the amount of food eaten each month. Comment briefly on what you see.\n\nSolution\n\nggplot(deer, aes(x = month, y = food)) + geom_boxplot()\n\n\n\n\nThis offers the suggestion that maybe November will be significantly higher than the rest and May significantly lower, or at least they will be significantly different from each other.\nThis is perhaps getting ahead of the game: we should be thinking about spread and shape. Bear in mind that there are only 5 or 6 observations in each group, so you won’t be able to say much about normality. In any case, we are going to be doing a Mood’s median test, so any lack of normality doesn’t matter (eg. perhaps that 4.4 observation in August). Given the small sample sizes, I actually think the spreads are quite similar.\nAnother way of looking at the data, especially with these small sample sizes, is a “dot plot”: instead of making a boxplot for each month, we plot the actual points for each month as if we were making a scatterplot:\n\nggplot(deer, aes(x = month, y = food)) + geom_point()\n\n\n\n\nWait a minute. There were five deer in February and six in August. Where did they go?\nThe problem is overplotting: more than one of the deer plotted in the same place on the plot, because the amounts of food eaten were only given to one decimal place and there were some duplicated values. One way to solve this is to randomly move the points around so that no two of them plot in the same place. This is called jittering, and is done like this:\n\nggplot(deer, aes(x = month, y = food)) + geom_jitter(width = 0, height = 0.05)\n\n\n\n\nNow you see all the deer, and you can see that two pairs of points in August and one pair of points in February are close enough on the jittered plot that they would have been the same to one decimal place.\nI wanted to keep the points above the months they belong to, so I only allowed vertical jitter (that’s the width and height in the geom_jitter; the width is zero so there is no horizontal jittering). If you like, you can colour the months; it’s up to you whether you think that’s making the plot easier to read, or is overkill (see my point on the facetted plots on the 2017 midterm).\nThis way you see the whole distribution for each month. Normally it’s nicer to see the summary made by the boxplots, but here there are not very many points. The value of 4.4 in August does look quite a bit lower than the rest, but the other months look believably normal given the small sample sizes. I don’t know about equal spreads (November looks more spread out), but normality looks believable. Maybe this is the kind of situation in which Welch’s ANOVA is a good idea. (If you believe that the normality-with-unequal-spreads is a reasonable assumption to make, then the Welch ANOVA will be more powerful than the Mood’s median test, and so should be preferred.)\n\\(\\blacksquare\\)\n\nRun a Mood’s median test as in lecture (ie. not using smmr). What do you conclude, in the context of the data?\n\nSolution\nTo give you some practice with the mechanics, first find the overall median:\n\ndeer %&gt;% summarize(med = median(food))\n\n\n\n  \n\n\n\nor\n\nmedian(deer$food)\n\n[1] 4.7\n\n\nI like the first way because it’s the same idea as we did before, just not differentiating by month. I think there are some observations exactly equal to the median, which will mess things up later:\n\ndeer %&gt;% filter(food == 4.7)\n\n\n\n  \n\n\n\nThere are, two in February and two in August.\nNext, make (and save) a table of the observations within each month that are above and below this median:\n\ntab1 &lt;- with(deer, table(month, food &lt; 4.7))\ntab1\n\n     \nmonth FALSE TRUE\n  Aug     4    2\n  Feb     5    0\n  May     0    6\n  Nov     5    0\n\n\nor\n\ntab2 &lt;- with(deer, table(month, food &gt; 4.7))\ntab2\n\n     \nmonth FALSE TRUE\n  Aug     4    2\n  Feb     2    3\n  May     6    0\n  Nov     0    5\n\n\nEither of these is good, but note that they are different. Two of the February observations (the ones that were exactly 4.7) have “switched sides”, and (look carefully) two of the August ones also. Hence the test results will be different, and smmr (later) will give different results again:\n\nchisq.test(tab1, correct = F)\n\nWarning in chisq.test(tab1, correct = F): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tab1\nX-squared = 16.238, df = 3, p-value = 0.001013\n\nchisq.test(tab2, correct = F)\n\nWarning in chisq.test(tab2, correct = F): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tab2\nX-squared = 11.782, df = 3, p-value = 0.008168\n\n\nThe warnings are because of the small frequencies. If you’ve done these by hand before (which you will have if you took PSYC08), you’ll remember that thing about “expected frequencies less than 5”. This is that. It means “don’t take those P-values too seriously.”\nThe P-values are different, but they are both clearly significant, so the median amounts of food eaten in the different months are not all the same. (This is the same “there are differences” that you get from an ANOVA, which you would follow up with Tukey.) Despite the injunction not to take the P-values too seriously, I think these are small enough that they could be off by a bit without affecting the conclusion.\nThe first table came out with a smaller P-value because it looked more extreme: all of the February measurements were taken as higher than the overall median (since we were counting “strictly less” and “the rest”). In the second table, the February measurements look more evenly split, so the overall P-value is not quite so small.\nYou can make a guess as to what smmr will come out with (next), since it throws away any data values exactly equal to the median.\n\\(\\blacksquare\\)\n\nRun a Mood’s median test using smmr, and compare the results with the previous part.\n\nSolution\nOff we go:\n\nlibrary(smmr)\nmedian_test(deer, food, month)\n\n$table\n     above\ngroup above below\n  Aug     2     2\n  Feb     3     0\n  May     0     6\n  Nov     5     0\n\n$test\n       what        value\n1 statistic 13.950000000\n2        df  3.000000000\n3   P-value  0.002974007\n\n\nThe P-value came out in between the other two, but the conclusion is the same all three ways: the months are not all the same in terms of median food eaten. The researchers can then go ahead and try to figure out why the animals eat different amounts in the different months.\nYou might be wondering how you could get rid of the equal-to-median values in the build-it-yourself way. This is filter from dplyr, which you use first:\n\ndeer2 &lt;- deer %&gt;% filter(food != 4.7)\ntab3 &lt;- with(deer2, table(month, food &lt; 4.7))\ntab3\n\n     \nmonth FALSE TRUE\n  Aug     2    2\n  Feb     3    0\n  May     0    6\n  Nov     5    0\n\nchisq.test(tab3)\n\nWarning in chisq.test(tab3): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  tab3\nX-squared = 13.95, df = 3, p-value = 0.002974\n\n\nwhich is exactly what smmr does, so the answer is identical.4 How would an ANOVA come out here? My guess is, very similarly:\n\ndeer.1 &lt;- aov(food ~ month, data = deer)\nsummary(deer.1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmonth        3 2.3065  0.7688   22.08 2.94e-06 ***\nResiduals   18 0.6267  0.0348                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(deer.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = food ~ month, data = deer)\n\n$month\n              diff         lwr        upr     p adj\nFeb-Aug  0.1533333 -0.16599282  0.4726595 0.5405724\nMay-Aug -0.3333333 -0.63779887 -0.0288678 0.0290758\nNov-Aug  0.5733333  0.25400718  0.8926595 0.0004209\nMay-Feb -0.4866667 -0.80599282 -0.1673405 0.0021859\nNov-Feb  0.4200000  0.08647471  0.7535253 0.0109631\nNov-May  0.9066667  0.58734052  1.2259928 0.0000013\n\n\nThe conclusion is the same, but the P-value on the \\(F\\)-test is much smaller. I think this is because the \\(F\\)-test uses the actual values, rather than just whether they are bigger or smaller than 4.7. The Tukey says that all the months are different in terms of (now) mean, except for February and August, which were those two very similar ones on the boxplot.\n\\(\\blacksquare\\)\n\nHow is it that Mood’s median test does not completely answer the question you really want to answer? How might you get an answer to the question you really want answered? Explain briefly, and obtain the answer you really want, discussing your results briefly.\n\nSolution\nThat’s rather a lot, so let’s take those things one at a time.5\nMood’s median test is really like the \\(F\\)-test in ANOVA: it’s testing the null hypothesis that the groups (months) all have the same median (of food eaten), against the alternative that the null is not true. We rejected this null, but we don’t know which months differ significantly from which. To resolve this in ANOVA, we do Tukey (or Games-Howell if we did the Welch ANOVA). The corresponding thing here is to do all the possible two-group Mood tests on all the pairs of groups, and, after adjusting for doing (here) six tests at once, look at the adjusted P-values to see how the months differ in terms of food eaten.\nThis is accomplished in smmr via pairwise_median_test, thus:\n\npairwise_median_test(deer, food, month)\n\n\n\n  \n\n\n\nThis compares each month with each other month. Looking at the last column, there are only three significant differences: August-November, February-May and May-November. Going back to the table of medians we made in (a), November is significantly higher (in terms of median food eaten) than August and May (but not February), and February is significantly higher than May. The other differences are not big enough to be significant.\nExtra: Pairwise median tests done this way are not likely to be very sensitive (that is, powerful), for a couple of reasons: (i) the usual one that the median tests don’t use the data very efficiently, and (ii) the way I go from the unadjusted to the adjusted P-values is via Bonferroni (here, multiply the P-values by 6), which is known to be safe but conservative. This is why the Tukey produced more significant differences among the months than the pairwise median tests did.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#movie-ratings-again-1",
    "href": "analysis-of-variance.html#movie-ratings-again-1",
    "title": "11  Analysis of variance",
    "section": "11.12 Movie ratings again",
    "text": "11.12 Movie ratings again\nThis question again uses the movie rating data at link.\n\nRead the data into R and obtain the number of movies of each rating and the median length of movies of each rating.\n\nSolution\nReading in is as in the other question using these data (just copy your code, or mine).\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/movie-lengths.csv\"\nmovies &lt;- read_csv(my_url)\n\nRows: 60 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): rating\ndbl (1): length\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmovies\n\n\n\n  \n\n\n\nNow, the actual for-credit part, which is a group_by and summarize:\n\nmovies %&gt;%\n  group_by(rating) %&gt;%\n  summarize(count = n(), med = median(length))\n\n\n\n  \n\n\n\nThe G movies have a smaller median than the others, but also the PG-13 movies seem to be longer on average (not what we found before).\n\\(\\blacksquare\\)\n\nObtain a suitable graph that assesses the assumptions for ANOVA. Why do you think it is not reasonable to run ANOVA here? Explain briefly.\n\nSolution\nThe graph would seem to be a boxplot, side by side for each group:\n\nggplot(movies, aes(x = rating, y = length)) + geom_boxplot()\n\n\n\n\nWe are looking for approximate normal distributions with approximately equal spreads, which I don’t think we have: there are outliers, at the low end for G movies, and at the high end for PG and PG-13 movies. Also, you might observe that the distribution of lengths for R movies is skewed to the right. (Noting either the outliers or skewness as a reason for not believing normality is enough, since all we need is one way that normality fails.)\nI think the spreads (as measured by the interquartile ranges) are acceptably similar, but since we have rejected normality, it is a bit late for that.\nSo I think it is far from reasonable to run an ANOVA here. In my opinion 15 observations in each group is not enough to gain much from the Central Limit Theorem either.\nExtra: since part of the assumption for ANOVA is (approximate) normality, it would also be entirely reasonable to make normal quantile plots, one for each movie type, facetted. Remember the process: you pretend that you are making a normal quantile plot for all the data together, regardless of group, and then at the last minute, you throw in a facet_wrap. I’ve written the code out on three lines, so that you can see the pieces: the “what to plot”, then the normal quantile plot part, then the facetting:\n\nggplot(movies, aes(sample = length)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~rating)\n\n\n\n\nSince there are four movie ratings, facet_wrap has arranged them into a \\(2\\times 2\\) grid, which satisfyingly means that each normal quantile plot is more or less square and thus easy to interpret.\nThe principal problem unveiled by these plots is outliers. It looks as if the G movies have one low outlier, the PG movies have two high outliers, the PG-13 movies have one or maybe three high outliers (depending on how you count them), and the R movies have none. Another way to look at the last two is you could call them curved, with too much bunching up at the bottom and (on PG-13) too much spread-out-ness at the top, indicating right-skewed distributions. The distribution of lengths of the R-rated movies is too bunched up at the bottom, but as you would expect for a normal at the top. The R movies show the right-skewedness in an odd way: usually this skewness shows up by having too many high values, but this time it’s having too few low values.\nThe assumption for ANOVA is that all four of these are at least approximately normal (with the same spread). We found problems with the normality on at least three of them, so we definitely have doubts about trusting ANOVA here.\nI could have used scales=free here to get a separate \\(y\\)-axis for each plot, but since the \\(y\\)-axis is movie length each time, and all four groups would be expected to have at least roughly similar movie lengths, I left it as it was. (The other advantage of leaving the scales the same is that you can compare spread by comparing the slopes of the lines on these graphs; since the lines connect the observed and theoretical quartiles, a steeper slope means a larger IQR. Here, the R line is steepest and the PG line is flattest. Compare this with the spreads of the boxplots.)\nExtra extra: if you want, you can compare the normal quantile plots with the boxplots to see whether you get the same conclusion from both. For the G movies, the low outlier shows up both ways, and the rest of the distribution is at least more or less normal. For the PG movies, I’d say the distribution is basically normal except for the highest two values (on both plots). For the PG-13 movies, only the highest value shows up as an outlier, but the next two apparent outliers on the normal quantile plot are at the upper end of the long upper whisker, so the boxplot is saying “right-skewed with one upper outlier” rather than “three upper outliers”. The distribution of the R movies is skewed right, with the bunching at the bottom showing up as the very small lower whisker.\nThe boxplots and the normal quantile plots are basically telling the same story in each case, but they are doing it in a slightly different way.\n\\(\\blacksquare\\)\n\nRun a Mood’s median test (use smmr if you like). What do you conclude, in the context of the data?\n\nSolution\nThe smart way is to use smmr, since it is much easier:\n\nlibrary(smmr)\nmedian_test(movies, length, rating)\n\n$table\n       above\ngroup   above below\n  G         2    13\n  PG        7     7\n  PG-13    12     3\n  R         8     6\n\n$test\n       what        value\n1 statistic 13.752380952\n2        df  3.000000000\n3   P-value  0.003262334\n\n\nThe movies do not all have the same median length, or at least one of the rating types has movies of different median length from the others. Or something equivalent to that. It’s the same conclusion as for ANOVA, only with medians instead of means.\nYou can speculate about why the test came out significant. My guess is that the G movies are shorter than average, and that the PG-13 movies are longer than average. (We had the first conclusion before, but not the second. This is where medians are different from means.)\nThe easiest way to see which movie types really differ in length from which is to do all the pairwise median tests, which is in smmr thus:\n\npairwise_median_test(movies, length, rating)\n\n\n\n  \n\n\n\nThe inputs for this are the same ones in the same order as for median_test. (A design decision on my part, since otherwise I would never have been able to remember how to run these!) Only the first two of these are significant (look in the last column). We can remind ourselves of the sample medians:\n\nmovies %&gt;%\n  group_by(rating) %&gt;%\n  summarize(count = n(), med = median(length))\n\n\n\n  \n\n\n\nThe G movies are significantly shorter than the PG and PG-13 movies, but not quite significantly different from the R movies. This is a little odd, since the difference in sample medians between G and PG, significant, is less than for G and R (not significant). There are several Extras here, which you can skip if you don’t care about the background. First, we can do the median test by hand: This has about four steps: (i) find the median of all the data, (ii) make a table tabulating the number of values above and below the overall median for each group, (iii) test the table for association, (iv) draw a conclusion. Thus (i):\n\nmedian(movies$length)\n\n[1] 100\n\n\nor\n\nmovies %&gt;% summarize(med = median(length))\n\n\n\n  \n\n\n\nor store it in a variable, and then (ii):\n\ntab1 &lt;- with(movies, table(length &lt; 100, rating))\ntab1\n\n       rating\n         G PG PG-13  R\n  FALSE  2  8    12  9\n  TRUE  13  7     3  6\n\n\nor\n\ntab2 &lt;- with(movies, table(length &gt; 100, rating))\ntab2\n\n       rating\n         G PG PG-13  R\n  FALSE 13  8     3  7\n  TRUE   2  7    12  8\n\n\nThese differ because there are evidently some movies of length exactly 100 minutes, and it matters whether you count \\(&lt;\\) and \\(\\ge\\) (as in tab1) or \\(&gt;\\) and \\(le\\) (tab2). Either is good.\nWas I right about movies of length exactly 100 minutes?\n\nmovies %&gt;% filter(length == 100)\n\n\n\n  \n\n\n\nOne PG and one R. It makes a difference to the R movies, but if you look carefully, it makes a difference to the PG movies as well, because the False and True switch roles between tab1 and tab2 (compare the G movies, for instance). You need to store your table in a variable because it has to get passed on to chisq.test below, (iii):\n\nchisq.test(tab1, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  tab1\nX-squared = 14.082, df = 3, p-value = 0.002795\n\n\nor\n\nchisq.test(tab2, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  tab2\nX-squared = 13.548, df = 3, p-value = 0.003589\n\n\nEither is correct, or, actually, without the correct=FALSE.6\nThe conclusion (iv) is the same either way: the null of no association is clearly rejected (with a P-value of 0.0028 or 0.0036 as appropriate), and therefore whether a movie is longer or shorter than median length depends on what rating it has: that is, the median lengths do differ among the ratings. The same conclusion, in other words, as the \\(F\\)-test gave, though with not quite such a small P-value.\nSecond, you might be curious about how we might do something like Tukey having found some significant differences (that is, what’s lurking in the background of pairwise_median_test).\nLet’s first suppose we are comparing G and PG movies. We need to pull out just those, and then compare them using smmr. Because the first input to median_test is a data frame, it fits neatly into a pipe (with the data frame omitted):\n\nmovies %&gt;%\n  filter(rating == \"G\" | rating == \"PG\") %&gt;%\n  median_test(length, rating)\n\n$table\n     above\ngroup above below\n   G      4    11\n   PG    10     3\n\n$test\n       what       value\n1 statistic 7.035897436\n2        df 1.000000000\n3   P-value 0.007989183\n\n\nWe’re going to be doing this about six times — \\({4 \\choose 2}=6\\) choices of two rating groups to compare out of the four — so we should have a function to do it. I think the input to the function should be a data frame that has a column called rating, and two names of ratings to compare:\n\ncomp2 &lt;- function(rat_1, rat_2, d) {\n  d %&gt;%\n    filter(rating == rat_1 | rating == rat_2) %&gt;%\n    median_test(length, rating)\n}\n\nThe way I wrote this function is that you have to specify the movie ratings in quotes. It is possible to write it in such a way that you input them without quotes, tidyverse style, but that gets into “non-standard evaluation” and enquo() and !!, which (i) I have to look up every time I want to do it, and (ii) I am feeling that the effort involved in explaining it to you is going to exceed the benefit you will gain from it. I mastered it enough to make it work in smmr (note that you specify column names without quotes there). There are tutorials on this kind of thing if you’re interested.\nAnyway, testing:\n\ncomp2(\"G\", \"PG\", movies)\n\n$table\n     above\ngroup above below\n   G      4    11\n   PG    10     3\n\n$test\n       what       value\n1 statistic 7.035897436\n2        df 1.000000000\n3   P-value 0.007989183\n\n\nThat works, but I really only want to pick out the P-value, which is in the list item test in the column value, the third entry. So let’s rewrite the function to return just that:\n\ncomp2 &lt;- function(rat_1, rat_2, d) {\n  d %&gt;%\n    filter(rating == rat_1 | rating == rat_2) %&gt;%\n    median_test(length, rating) %&gt;%\n    pluck(\"test\", \"value\", 3)\n}\ncomp2(\"G\", \"PG\", movies)\n\n[1] 0.007989183\n\n\nGosh.\nWhat median_test returns is an R list that has two things in it, one called table and one called test. The thing called test is a data frame with a column called value that contains the P-values. The third of these is the two-sided P-value that we want.\nYou might not have seen pluck before. This is a way of getting things out of complicated data structures. This one takes the output from median_test and from it grabs the piece called test. This is a data frame. Next, we want the column called value, and from that we want the third row. These are specified one after the other to pluck and it pulls out the right thing.\nSo now our function returns just the P-value.\nI have to say that it took me several goes and some playing around in R Studio to sort this one out. Once I thought I understood pluck, I wondered why my function was not returning a value. And then I realized that I was saving the value inside the function and not returning it. Ooops. The nice thing about pluck is that I can put it on the end of the pipeline and and it will pull out (and return) whatever I want it to.\nLet’s grab a hold of the different rating groups we have:\n\nthe_ratings &lt;- unique(movies$rating)\nthe_ratings\n\n[1] \"G\"     \"PG-13\" \"PG\"    \"R\"    \n\n\nThe Pythonisti among you will know how to finish this off: do a loop-inside-a-loop over the rating groups, and get the P-value for each pair. You can do that in R, if you must. It’s not pretty at all, but it works:\n\nii &lt;- character(0)\njj &lt;- character(0)\npp &lt;- numeric(0)\nfor (i in the_ratings) {\n  for (j in the_ratings) {\n    pval &lt;- comp2(i, j, movies)\n    ii &lt;- c(ii, i)\n    jj &lt;- c(jj, j)\n    pp &lt;- c(pp, pval)\n  }\n}\ntibble(ii, jj, pp)\n\n\n\n  \n\n\n\nThis is a lot of fiddling about, since you have to initialize three vectors, and then update them every time through the loop. It’s hard to read, because the actual business part of the loop is the calculation of the P-value, and that’s almost hidden by all the book-keeping. (It’s also slow and inefficient, though the slowness doesn’t matter too much here since it’s not a very big problem.)\nLet’s try another way:\n\ncrossing(first = the_ratings, second = the_ratings)\n\n\n\n  \n\n\n\nThis does “all possible combinations” of one rating with another. We don’t actually need all of that; we just need the ones where the first one is (alphabetically) strictly less than the second one. This is because we’re never comparing a rating with itself, and each pair of ratings appears twice, once in alphabetical order, and once the other way around. The ones we need are these:\n\ncrossing(first = the_ratings, second = the_ratings) %&gt;%\n  filter(first &lt; second)\n\n\n\n  \n\n\n\nA technique thing to note: instead of asking “how do I pick out the distinct pairs of ratings?”, I use two simpler tools: first I make all the combinations of pairs of ratings, and then out of those, pick the ones that are alphabetically in ascending order, which we know how to do.\nNow we want to call our function comp2 for each of the things in first and each of the things in second, and make a new column called pval that contains exactly that. comp2 expects single movie ratings for each of its inputs, not a vector of each, so the way to go about this is rowwise:\n\ncrossing(first = the_ratings, second = the_ratings) %&gt;%\n  filter(first &lt; second) %&gt;%\n  rowwise() %&gt;% \n  mutate(pval = comp2(first, second, movies))\n\n\n\n  \n\n\n\nOne more thing: we’re doing 6 tests at once here, so we’re giving ourselves 6 chances to reject a null (all medians equal) that might have been true. So the true probability of a type I error is no longer 0.05 but something bigger.\nThe easiest way around that is to do a so-called Bonferroni adjustment: instead of rejecting if the P-value is less than 0.05, we only reject if it is less than \\(0.05/6\\), since we are doing 6 tests. This is a fiddly calculation to do by hand, but it’s easy to build in another mutate, thus:7\n\ncrossing(first = the_ratings, second = the_ratings) %&gt;%\n  filter(first &lt; second) %&gt;%\n  rowwise() %&gt;% \n  mutate(pval = comp2(first, second, movies)) %&gt;% \n  mutate(reject = (pval &lt; 0.05 / 6))\n\n\n\n  \n\n\n\nAnd not a loop in sight.\nThis is how I coded it in pairwise_median_test. If you want to check it, it’s on Github: link. The function median_test_pair is the same as comp2 above.\nSo the only significant differences are now G compared to PG and PG-13. There is not a significant difference in median movie length between G and R, though it is a close call. We thought the PG-13 movies might have a significantly different median from other rating groups beyond G, but they turn out not to have. (The third and fourth comparisons would have been significant had we not made the Bonferroni adjustment to compensate for doing six tests at once; with that adjustment, we only reject if the P-value is less than \\(0.05/6=0.0083\\), and so 0.0106 is not quite small enough to reject with.)\nListing the rating groups sorted by median would give you an idea of how far different the medians have to be to be significantly different:\n\nmedians &lt;- movies %&gt;%\n  group_by(rating) %&gt;%\n  summarize(med = median(length)) %&gt;%\n  arrange(desc(med))\nmedians\n\n\n\n  \n\n\n\nSomething rather interesting has happened: even though the comparison of G and PG (18 apart) is significant, the comparison of G and R (21 apart) is not significant. This seems very odd, but it happens because the Mood median test is not actually literally comparing the sample medians, but only assessing the splits of values above and below the median of the combined sample. A subtlety, rather than an error, I’d say.\nHere’s something extremely flashy to finish with:\n\ncrossing(first = the_ratings, second = the_ratings) %&gt;%\n  filter(first &lt; second) %&gt;%\n  rowwise() %&gt;% \n  mutate(pval = comp2(first, second, movies)) %&gt;% \n  mutate(reject = (pval &lt; 0.05 / 6)) %&gt;% \n  left_join(medians, by = c(\"first\" = \"rating\")) %&gt;%\n  left_join(medians, by = c(\"second\" = \"rating\"))\n\n\n\n  \n\n\n\nThe additional two lines look up the medians of the rating groups in first, then second, so that you can see the actual medians of the groups being compared each time. You see that medians different by 30 are definitely different, ones differing by 15 or less are definitely not different, and ones differing by about 20 could go either way.\nI think that’s quite enough of that.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#atomic-weight-of-carbon-1",
    "href": "analysis-of-variance.html#atomic-weight-of-carbon-1",
    "title": "11  Analysis of variance",
    "section": "11.13 Atomic weight of carbon",
    "text": "11.13 Atomic weight of carbon\nThe atomic weight of the chemical element carbon is 12. Two methods of measuring the atomic weight of samples of carbon were compared. The results are shown in link. The methods are labelled 1 and 2. The first task is to find out whether the two methods have different “typical” measures (mean or median, as appropriate) of the atomic weight of carbon.\nFor this question, compose a report in R Markdown. (R Markdown is what you use in an R Notebook, but you can also have a separate R Markdown document from which you can produce HTML, Word etc. output.) See part (a) for how to get this started.\nYour report should read like an actual report, not just the answers to some questions that I set you. To help with that, write some text that links the parts of the report together smoothly, so that it reads as a coherent whole. The grader had 3 discretionary marks to award for the overall quality of your writing. The scale for this was:\n\n3 points: excellent writing. The report flows smoothly, is easy to read, and contains everything it should (and nothing it shouldn’t).\n2 points: satisfactory writing. Not the easiest to read, but says what it should, and it looks at least somewhat like a report rather than a string of answers to questions.\n1 point: writing that is hard to read or to understand. If you get this (or 0), you should consider what you need to do to improve when you write your project.\n0 points: you answered the questions, but you did almost nothing to make it read like a report.\n\n\nCreate a new R Markdown document. To do this, in R Studio, select File, New File, R Markdown. Type the report title and your name in the boxes, and leave the output on the default HTML. Click OK.\n\nSolution\nYou’ll see the title and your name in a section at the top of the document, and below that you’ll see a template document, as you would for an R Notebook. The difference is that where you are used to seeing Preview, it now says “knit”, but this has the same effect of producing the formatted version of your report.\n\\(\\blacksquare\\)\n\nWrite an introduction that explains the purpose of this study and the data collected in your own words.\n\nSolution\nSomething like this:\n\nThis study is intended to compare two different methods (labelled 1 and 2) for measuring the atomic weight of carbon (which is known in actual fact to be 12). Fifteen samples of carbon were used; ten of these were assessed using method 1 and the remaining five using method 2. The primary interest in this particular study is to see whether there is a difference in the mean or median atomic weight as measured by the two methods.\n\nBefore that, start a new section like this: ## Introduction. Also, get used to expressing your understanding in your words, not mine. Using my words, in my courses, is likely to be worth very little.\n\\(\\blacksquare\\)\n\nBegin an appropriately-titled new section in your report, read the data into R and display the results.\n\nSolution\nValues separated by spaces:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/carbon.txt\"\ncarbon &lt;- read_delim(my_url, \" \")\n\nRows: 15 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): method, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncarbon\n\n\n\n  \n\n\n\nI would expect you to include, without being told to include it, some text in your report indicating that you have sensible data: two methods labelled 1 and 2 as promised, and a bunch8 of atomic weights close to the nominal figure of 12.\n\\(\\blacksquare\\)\n\nMake an appropriate plot to compare the measurements obtained by the two methods. You might need to do something about the two methods being given as numbers even though they are really only identifiers. (If you do, your report ought to say what you did and why.)\n\nSolution\nThe appropriate plot, with a categorical method and quantitative weight, is something like a boxplot. If you’re not careful, method will get treated as a quantitative variable, which you don’t want; the easiest way around that, for a boxplot at least, is to turn it into a factor like this:\n\nggplot(carbon, aes(x = factor(method), y = weight)) + geom_boxplot()\n\n\n\n\nIf you insist, you could do a faceted histogram (above and below, for preference):\n\nggplot(carbon, aes(x = weight)) + geom_histogram(bins = 5) +\n  facet_wrap(~method, ncol = 1)\n\n\n\n\nThere are really not enough data values for a histogram to be of much help, so I don’t like this as much.\nIf you are thinking ahead (we are going to be doing a \\(t\\)-test), then you’ll realize that normality is the kind of thing we’re looking for, in which case normal quantile plots would be the thing. However, we might have to be rather forgiving for method 2 since there are only 5 observations:\n\nggplot(carbon, aes(sample = weight)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~method)\n\n\n\n\nI don’t mind these coming out side by side, though I would rather have them squarer.\nI would say, boxplots are the best, normal quantile plots are also acceptable, but expect to lose something for histograms because they offer only a rather crude comparison in this case.\n\\(\\blacksquare\\)\n\nComment briefly on what you see in your plot.\n\nSolution\nIn boxplots, if that’s what you drew, there are several things that deserve comment: the medians, the spreads and the shapes. The median for method 1 is a little bit lower than for method 2 (the means are probably more different, given the shapes of the boxes). The spread for method 2 is a lot bigger. (Looking forward, that suggests a Welch-Satterthwaite rather than a pooled test.) As for shape, the method 2 measurements seem more or less symmetric (the whiskers are equal anyway, even if the position of the median in the box isn’t), but the method 1 measurements have a low outlier. The histograms are hard to compare. Try to say something about centre and spread and shape. I think the method 2 histogram has a slightly higher centre and definitely bigger spread. On my histogram for method 1, the distribution looks skewed left. If you did normal quantile plots, say something sensible about normality for each of the two methods. For method 1, I would say the low value is an outlier and the rest of the values look pretty straight. Up to you whether you think there is a curve on the plot (which would indicate skewness, but then that highest value is too high: it would be bunched up with the other values below 12.01 if there were really skewness). For method 2, it’s really hard to say anything since there are only five values. Given where the line goes, there isn’t much you can say to doubt normality. Perhaps the best you can say here is that in a sample of size 5, it’s difficult to assess normality at all.\n\\(\\blacksquare\\)\n\nCarry out the most appropriate \\(t\\)-test. (You might like to begin another new section in your report here.)\n\nSolution\nThis would be the Welch-Satterthwaite version of the two-sample \\(t\\)-test, since the two groups do appear to have different spreads:\n\nt.test(weight ~ method, data = carbon)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by method\nt = -1.817, df = 5.4808, p-value = 0.1238\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.027777288  0.004417288\nsample estimates:\nmean in group 1 mean in group 2 \n       12.00260        12.01428 \n\n\nImagining that this is a report that would go to your boss, you ought to defend your choice of the Welch-Satterthwaite test (as I did above), and not just do the default \\(t\\)-test without comment.\nIf, in your discussion above, you thought the spreads were equal enough, then you should do the pooled \\(t\\)-test here, which goes like this:\n\nt.test(weight ~ method, data = carbon, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  weight by method\nt = -2.1616, df = 13, p-value = 0.04989\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -2.335341e-02 -6.588810e-06\nsample estimates:\nmean in group 1 mean in group 2 \n       12.00260        12.01428 \n\n\nThe point here is that you should do the right test based on your conclusion. Being consistent is the most important thing. (In this case, note that the P-values are very different. We’ll get to that shortly.)\nIf we were doing this in SAS, as we see later, we’d get a test at the bottom of the output that compares the two variances. I feel that it’s just as good to eyeball the spreads and make a call about whether they are “reasonably close”. Or even, to always do the Welch-Satterthwaite test on the basis that it is pretty good even if the two populations have the same variance. (If this last point of view is one that you share, you ought to say something about that when you do your \\(t\\)-test.)\nExtra: I guess this is a good place to say something about tests for comparing variances, given that you might be pondering that. There are several that I can think of, that R can do, of which I mention two.\nThe first is the \\(F\\)-test for variances that you might have learned in B57 (that is the basis for the ANOVA \\(F\\)-test):\n\nvar.test(weight ~ method, data = carbon)\n\n\n    F test to compare two variances\n\ndata:  weight by method\nF = 0.35768, num df = 9, denom df = 4, p-value = 0.1845\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.04016811 1.68758230\nsample estimates:\nratio of variances \n         0.3576842 \n\n\nThis, unfortunately, is rather dependent on the data in the two groups being approximately normal. Since we are talking variances rather than means, there is no Central Limit Theorem to rescue us for large samples (quite aside from the fact that these samples are not large). Since the ANOVA \\(F\\)-test is based on the same theory, this is why normality is also more important in ANOVA than it is in a \\(t\\)-test.\nThe second is Levene’s test. This doesn’t depend on normality (at least, not nearly so much), so I like it better in general:\n\nlibrary(car)\nleveneTest(weight ~ factor(method), data = carbon)\n\n\n\n  \n\n\n\nLevene’s test takes a different approach: first the absolute differences from the group medians are calculated, and then an ANOVA is run on the absolute differences. If, say, one of the groups has a larger spread than the other(s), its absolute differences from the median will tend to be bigger.9 As for what we conclude here, well, neither of the variance tests show any significance at all, so from that point of view there is no evidence against using the pooled \\(t\\)-test. Having said that, the samples are small, and so it would be difficult to prove that the two methods have different variance, even if they actually did.10\nThings are never as clear-cut as you would like. In the end, it all comes down to making a call and defending it.\n\\(\\blacksquare\\)\n\nDo the most appropriate test you know that does not assume normally-distributed data.\n\nSolution\nThat would be Mood’s median test. Since I didn’t say anything about building it yourself, feel free to use smmr:\n\nlibrary(smmr)\nmedian_test(carbon, weight, method)\n\n$table\n     above\ngroup above below\n    1     3     6\n    2     4     1\n\n$test\n       what      value\n1 statistic 2.80000000\n2        df 1.00000000\n3   P-value 0.09426431\n\n\nAs an aside, if you have run into a non-parametric test such as Mann-Whitney or Kruskal-Wallis that applies in this situation, be careful about using it here, because they have additional assumptions that you may not want to trust. Mann-Whitney started life as a test for “equal distributions”.11 This means that the null is equal location and equal spread, and if you reject the null, one of those has failed. But here, we suspect that equal spread will fail, so that the Mann-Whitney test may end up rejecting whether or not the medians are different, so it won’t answer the question you want an answer to. Mood’s median test doesn’t have that problem; all it’s saying if the null is true is that the medians are equal; the spreads could be anything at all.\nThe same kind of issues apply to the signed-rank test vs. the sign test. In the case of the signed-rank test, the extra assumption is of a symmetric distribution — to my mind, if you don’t believe normality, you probably don’t have much confidence in symmetry either. That’s why I like the sign test and Mood’s median test: in the situation where you don’t want to be dealing with assumptions, these tests don’t make you worry about that.\nAnother comment that you don’t need to make is based on the not-quite-significance of the Mood test. The P-value is less than 0.10 but not less than 0.05, so it doesn’t quite reach significance by the usual standard. But if you look up at the table, the frequencies seem rather unbalanced: 6 out of the remaining 9 weights in group 1 are below the overall median, but 4 out of 5 weights in group 2 are above. This seems as if it ought to be significant, but bear in mind that the sample sizes are small, and thus Mood’s median test needs very unbalanced frequencies, which we don’t quite have here.\n\\(\\blacksquare\\)\n\nDiscuss the results of your tests and what they say about the two methods for measuring the atomic weight of carbon. If it seems appropriate, put the discussion into a section called Conclusions.\n\nSolution\nBegin by pulling out the P-values for your preferred test(s) and say what they mean. The P-value for the Welch-Satterthwaite \\(t\\)-test is 0.1238, which indicates no difference in mean atomic weights between the two methods. The Mood median test gives a similarly non-significant 0.0943, indicating no difference in the median weights. If you think both tests are plausible, then give both P-values and do a compare-and-contrast with them; if you think that one of the tests is clearly preferable, then say so (and why) and focus on that test’s results.\nIf you thought the pooled test was the right one, then you’ll have a bit more discussion to do, since its P-value is 0.0499, and at \\(\\alpha=0.05\\) this test disagrees with the others. If you are comparing this test with the Mood test, you ought to make some kind of reasoned recommendation about which test to believe.\nAs ever, be consistent in your reasoning.\nExtra: this dataset, where I found it, was actually being used to illustrate a case where the pooled and the Welch-Satterthwaite tests disagreed. The authors of the original paper that used this dataset (a 1987 paper by Best and Rayner;12 the data come from 1924!) point out that the pooled \\(t\\)-test can be especially misleading when the smaller sample is also the one with the larger variance. This is what happened here.\nIn the Best and Rayner paper, the Mood (or the Mann-Whitney) test was not being considered, but I think it’s good practice to draw a picture and make a call about which test is appropriate.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#can-caffeine-improve-your-performance-on-a-test-1",
    "href": "analysis-of-variance.html#can-caffeine-improve-your-performance-on-a-test-1",
    "title": "11  Analysis of variance",
    "section": "11.14 Can caffeine improve your performance on a test?",
    "text": "11.14 Can caffeine improve your performance on a test?\nDoes caffeine help students do better on a certain test? To find out, 36 students were randomly allocated to three groups (12 in each group). Each student received a fixed number of cups of coffee while they were studying, but the students didn’t know whether they were receiving all full-strength coffee (“high”), all decaf coffee (“low”) or a 50-50 mixture of the two (“moderate”). For each subject, their group was recorded as well as their score on the test. The data are in link, as a .csv file.\n\nRead in and examine the data. How are the values laid out?\n\nSolution\nread_csv because it’s a .csv file:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/caffeine.csv\"\ncaffeine.untidy &lt;- read_csv(my_url)\n\nRows: 12 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Sub, High, Moderate, None\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncaffeine.untidy\n\n\n\n  \n\n\n\nThe first column is the number of the subject (actually within each group, since each student only tried one amount of caffeine). Then follow the test scores for the students in each group, one group per column.\nI gave the data frame a kind of dumb name, since (looking ahead) I could see that I would need a less-dumb name for the tidied-up data, and it seemed sensible to keep caffeine for that.\n\\(\\blacksquare\\)\n\nExplain briefly how the data are not “tidy”.\n\nSolution\nThe last three columns are all scores on the test: that is, they all measure the same thing, so they should all be in the same column. Or, there should be a column of scores, and a separate column naming the groups. Or, there were 36 observations in the data, so there should be 36 rows. You always have a variety of ways to answer these, any of which will do.\n\\(\\blacksquare\\)\n\nUse a suitable tool from the tidyverse to create one column of test scores and and one column of group labels. Call your column of group labels amount. Is it a factor?\n\nSolution\nWe are combining several columns into one, so this is pivot_longer:\n\ncaffeine.untidy %&gt;% \n  pivot_longer(-Sub, names_to = \"amount\", values_to = \"score\") -&gt; caffeine\n\nI didn’t ask you to list the resulting data frame, but it is smart to at least look for yourself, to make sure pivot_longer has done what you expected.\n\ncaffeine\n\n\n\n  \n\n\n\nA column of amounts of caffeine, and a column of test scores. This is what we expected. There should be 12 each of the amounts, which you can check if you like:\n\ncaffeine %&gt;% count(amount)\n\n\n\n  \n\n\n\nIndeed.\nNote that amount is text, not a factor. Does this matter? We’ll see.\nThis is entirely the kind of situation where you need pivot_longer, so get used to seeing where it will be useful.\n\\(\\blacksquare\\)\n\nObtain side-by-side boxplots of test scores by amount of caffeine.\n\nSolution\n\nggplot(caffeine, aes(x = amount, y = score)) + geom_boxplot()\n\n\n\n\nNote that this is much more difficult if you don’t have a tidy data frame. (Try it and see.)\n\\(\\blacksquare\\)\n\nDoes caffeine amount seem to have an effect? If so, what kind of effect?\n\nSolution\nOn average, exam scores seem to be higher when the amount of caffeine is higher (with the effect being particularly pronounced for High caffeine). If you want to, you can also say the the effect of caffeine seems to be small, relative to the amount of variability there is (there is a lot). The point is that you say something supported by the boxplot.\n\\(\\blacksquare\\)\n\nRun a suitable analysis of variance to determine whether the mean test score is equal or unequal for the three groups. What do you conclude?\n\nSolution\nSomething like this:\n\ncaff.1 &lt;- aov(score ~ amount, data = caffeine)\nsummary(caff.1)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \namount       2  477.7  238.86   3.986 0.0281 *\nResiduals   33 1977.5   59.92                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe P-value on the \\(F\\)-test is less than 0.05, so we reject the null hypothesis (which says that all the groups have equal means) in favour of the alternative: the group means are not all the same (one or more of them is different from the others).\nNotice that the boxplot and the aov are quite happy for amount to be text rather than a factor (they actually do want a factor, but if the input is text, they’ll create one).\n\\(\\blacksquare\\)\n\nWhy is it a good idea to run Tukey’s method here?\n\nSolution\nThe analysis of variance \\(F\\)-test is significant, so that the groups are not all the same. Tukey’s method will tell us which group(s) differ(s) from the others. There are three groups, so there are differences to find that we don’t know about yet.\n\\(\\blacksquare\\)\n\nRun Tukey’s method. What do you conclude?\n\nSolution\nThis kind of thing:\n\ncaff.3 &lt;- TukeyHSD(caff.1)\ncaff.3\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = score ~ amount, data = caffeine)\n\n$amount\n                   diff       lwr       upr     p adj\nModerate-High -4.750000 -12.50468  3.004679 0.3025693\nNone-High     -8.916667 -16.67135 -1.161987 0.0213422\nNone-Moderate -4.166667 -11.92135  3.588013 0.3952176\n\n\nThe high-caffeine group definitely has a higher mean test score than the no-caffeine group. (The Moderate group is not significantly different from either of the other groups.) Both the comparisons involving Moderate could go either way (the interval for the difference in means includes zero). The None-High comparison, however, is away from zero, so this is the significant one. As is usual, we are pretty sure that the difference in means (this way around) is negative, but we are not at all clear about how big it is, because the confidence interval is rather long.13\nExtra: the normality and equal spreads assumptions look perfectly good, given the boxplots, and I don’t think there’s any reason to consider any other test. You might like to assess that with normal quantile plots:\n\nggplot(caffeine, aes(sample=score)) + stat_qq() +\n  stat_qq_line() + facet_wrap(~amount, ncol=2)\n\n\n\n\nThere’s nothing to worry about there normality-wise. If anything, there’s a little evidence of short tails (in the None group especially), but you’ll recall that short tails don’t affect the mean and thus pose no problems for the ANOVA. Those three lines also have pretty much the same slope, indicating very similar spreads. Regular ANOVA is the best test here. (Running eg. Mood’s median test would be a mistake here, because it doesn’t use the data as efficiently (counting only aboves and belows) as the ANOVA does, and so the ANOVA will give a better picture of what differs from what.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#reggae-music-1",
    "href": "analysis-of-variance.html#reggae-music-1",
    "title": "11  Analysis of variance",
    "section": "11.15 Reggae music",
    "text": "11.15 Reggae music\nReggae is a music genre that originated in Jamaica in the late 1960s. One of the most famous reggae bands was Bob Marley and the Wailers. In a survey, 729 students were asked to rate reggae music on a scale from 1, “don’t like it at all” to 6, “like it a lot”. We will treat the ratings as quantitative. Each student was also asked to classify their home town as one of “big city”, “suburban”, “small town”, “rural”. Does a student’s opinion of reggae depend on the kind of home town they come from? The data are in http://ritsokiguess.site/datafiles/reggae.csv.\n\nRead in and display (some of) the data.\n\nSolution\nThis is (evidently) a .csv, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/reggae.csv\"\nreggae &lt;- read_csv(my_url)\n\nRows: 729 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): home\ndbl (1): rating\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nreggae\n\n\n\n  \n\n\n\nThe students shown are all from big cities, but there are others, as you can check by scrolling down.\n\\(\\blacksquare\\)\n\nHow many students are from each different size of town?\n\nSolution\nThis is the usual kind of application of count:\n\nreggae %&gt;% count(home)\n\n\n\n  \n\n\n\nAnother, equally good, way (you can ignore the warning):\n\nreggae %&gt;% group_by(home) %&gt;% \nsummarize(n=n())\n\n\n\n  \n\n\n\nMost of the students in this data set are from suburbia.\n\\(\\blacksquare\\)\n\nMake a suitable graph of the two variables in this data frame.\n\nSolution\nOne quantitative, one categorical: a boxplot, as ever:\n\nggplot(reggae, aes(x=home, y=rating)) + geom_boxplot()\n\n\n\n\nExtra 1: the last three boxplots really are identical, because the medians, means, quartiles and extreme values are all equal. However, the data values are not all the same, as you see below.\nExtra 2: I said that the ratings should be treated as quantitative, to guide you towards this plot. You could otherwise have taken the point of view that the ratings were (ordered) categorical, in which case the right graph would have been a grouped bar chart, as below. There is a question about which variable should be x and which should be fill. I am taking the point of view that we want to compare ratings within each category of home, which I think makes sense here (see discussion below), which breaks my “rule” that the categorical variable with fewer categories should be x.14\n\nggplot(reggae, aes(x=home, fill=factor(rating))) + geom_bar(position = \"dodge\")\n\n\n\n\n\\(\\blacksquare\\)\n\nDiscuss briefly why you might prefer to run Mood’s median test to compare ratings among home towns.\n\nSolution\nThe issue here is whether all of the rating distributions (within each category of home) are sufficiently close to normal in shape. The “big city” group is clearly skewed to the left. This is enough to make us favour Mood’s median test over ANOVA.\nA part-marks answer is to note that the big-city group has smaller spread than the other groups (as measured by the IQR). This is answering the wrong question, though. Remember the process: first we assess normality. If that fails, we use Mood’s median test. Then, with normality OK, we assess equal spreads. If that fails, we use Welch ANOVA, and if both normality and equal spreads pass, we use regular ANOVA.\n\\(\\blacksquare\\)\n\nSuppose that somebody wanted to run Welch ANOVA on these data. What would be a reasonable argument to support that?\n\nSolution\nThe argument would have to be that normality is all right, given the sample sizes. We found earlier that there are between 89 and 368 students in each group. These are large samples, and might be enough to overcome the non-normality we see.\nThe only real concern I have is with the big city group. This is the least normal, and also the smallest sample. The other groups seem to have the kind of non-normality that will easily be taken care of by the sample sizes we have.\nExtra: the issue is really about the sampling distribution of the mean within each group. Does that look normal enough? This could be assessed by looking at each group, one at a time, and taking bootstrap samples. Here’s the big-city group:\n\nreggae %&gt;% filter(home==\"big city\") -&gt; bigs\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(bigs$rating, replace = T))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12)\n\n\n\n\nNot too much wrong with that. This shows that the sample size is indeed big enough to cope with the skewness.\nYou can do any of the others the same way.\nIf you’re feeling bold, you can get hold of all three bootstrapped sampling distributions at once, like this:\n\nreggae %&gt;% \n  nest_by(home) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$rating, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12) +\n  facet_wrap(~home, scales = \"free\")\n\n\n\n\nAll of these distributions look very much normal, so there is no cause for concern anywhere.\nThis was rather a lot of code, so let me take you through it. The first thing is that we want to treat the different students’ homes separately, so the first step is this:\n\nreggae %&gt;% \n  nest_by(home) \n\n\n\n  \n\n\n\nThis subdivides the students’ reggae ratings according to where their home is. The things in data are data frames containing a column rating for in each case the students who had the home shown.\nNormally, we would start by making a dataframe with a column called sim that labels the 1000 or so simulations. This time, we want four sets of simulations, one for each home, which we can set up this way:\n\nreggae %&gt;% \n  nest_by(home) %&gt;% \n  mutate(sim = list(1:1000)) \n\n\n\n  \n\n\n\nThe definition of sim happens by group, or rowwise, by home (however you want to look at it). Next, we need to spread out those sim values so that we’ll have one row per bootstrap sample:\n\nreggae %&gt;% \n  nest_by(home) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) \n\n\n\n  \n\n\n\n\\(4 \\times 1000 = 4000\\) rows. Note that the data column now contains multiple copies of all the ratings for the students with that home, which seems wasteful, but it makes our life easier because what we want is a bootstrap sample from the right set of students, namely the rating column from the dataframe data in each row. Thus, from here out, everything is the same as we have done before: work rowwise, get a bootstrap sample , find its mean, plot it. The one thing we need to be careful of is to make a separate histogram for each home, since each of the four distributions need to look normal. I used different scales for each one, since they are centred in different places; this has the side benefit of simplifying the choice of the number of bins. (See what happens if you omit the scales = \"free\".)\nIn any case, all is absolutely fine. We’ll see how this plays out below.\n\\(\\blacksquare\\)\n\nRun Mood’s median test and display the output.\n\nSolution\nData frame, quantitative column, categorical column:\n\nmedian_test(reggae, rating, home)\n\n$table\n            above\ngroup        above below\n  big city      51    21\n  rural         25    49\n  small town    64    89\n  suburban     120   187\n\n$test\n       what        value\n1 statistic 2.733683e+01\n2        df 3.000000e+00\n3   P-value 5.003693e-06\n\n\n\\(\\blacksquare\\)\n\nExplain briefly why running pairwise median tests is a good idea, run them, and display the results.\n\nSolution\nThe Mood’s median test is significant, with a P-value of 0.000005, so the median ratings are not all the same. We want to find out how they differ.\n(The table of aboves and belows, and for that matter the boxplot earlier, suggest that big-city will be different from the rest, but it is not clear whether there will be any other significant differences.)\n\npairwise_median_test(reggae, rating, home)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nSummarize, as concisely as possible, how the home towns differ in terms of their students’ ratings of reggae music.\n\nSolution\nThe students from big cities like reggae more than students from other places. The other kinds of hometown do not differ significantly.\nExtra 1: Given the previous discussion, you might be wondering how Welch ANOVA (and maybe even regular ANOVA) compare. Let’s find out:\n\noneway.test(rating~home,data=reggae)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  rating and home\nF = 16.518, num df = 3.00, denom df = 257.07, p-value = 7.606e-10\n\n\nand\n\ngamesHowellTest(rating~factor(home),data=reggae)\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: rating by factor(home)\n\n\n           big city rural small town\nrural      1.1e-07  -     -         \nsmall town 2.9e-06  0.74  -         \nsuburban   4.9e-09  0.91  0.94      \n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nThe conclusions are identical with Mood’s median test, and the P-values are not that different, either.\nThis makes me wonder how an ordinary ANOVA with Tukey would have come out:\n\nreggae %&gt;% \naov(rating~home, data=.) %&gt;% \nTukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = rating ~ home, data = .)\n\n$home\n                           diff        lwr        upr     p adj\nrural-big city      -1.20681180 -1.8311850 -0.5824386 0.0000048\nsmall town-big city -1.00510725 -1.5570075 -0.4532070 0.0000194\nsuburban-big city   -1.09404006 -1.5952598 -0.5928203 0.0000002\nsmall town-rural     0.20170455 -0.3366662  0.7400753 0.7695442\nsuburban-rural       0.11277174 -0.3735106  0.5990540 0.9329253\nsuburban-small town -0.08893281 -0.4778062  0.2999406 0.9354431\n\n\nAgain, almost identical.\nExtra 2: some Bob Marley and the Wailers for you:\n\nfrom 1980\nfrom 1973\n\nReggae music at its finest.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#watching-tv-and-education-1",
    "href": "analysis-of-variance.html#watching-tv-and-education-1",
    "title": "11  Analysis of variance",
    "section": "11.16 Watching TV and education",
    "text": "11.16 Watching TV and education\nThe General Social Survey is a large survey of a large number of people. One of the questions on the survey is “how many hours of TV do you watch in a typical day?” Another is “what is your highest level of education attained”, on this scale:\n\nHSorLess: completed no more than high h school\nCollege: completed some form of college, either a community college (like Centennial) or a four-year university (like UTSC)\nGraduate: completed a graduate degree such as an MSc.\n\nDo people with more education tend to watch more TV? We will be exploring this. The data are in http://ritsokiguess.site/datafiles/gss_tv.csv.\n\nRead in and display (some of) the data.\n\nSolution\nExactly the usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/gss_tv.csv\"\ngss &lt;- read_csv(my_url)\n\nRows: 905 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): degree\ndbl (1): tvhours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngss\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nFor each level of education, obtain the number of observations, the mean and the median of the number of hours of TV watched.\n\nSolution\ngroup_by and summarize, using n() to get the number of observations (rather than count because you want some numerical summaries as well):\n\ngss %&gt;% group_by(degree) %&gt;% \nsummarise(n=n(), mean=mean(tvhours), med=median(tvhours))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nWhat does your answer to the previous part tell you about the shapes of the distributions of the numbers of hours of TV watched? Explain briefly.\n\nSolution\nIn each of the three groups, the mean is greater than the median, so I think the distributions are skewed to the right. Alternatively, you could say that you expect to see some outliers at the upper end.\n\\(\\blacksquare\\)\n\nObtain a suitable graph of your data frame.\n\nSolution\nOne quantitative variable and one categorical one, so a boxplot. (I hope you are getting the hang of this by now.)\n\nggplot(gss, aes(x=degree, y=tvhours)) + geom_boxplot()\n\n\n\n\n\\(\\blacksquare\\)\n\nDoes your plot indicate that your guess about the distribution shape was correct? Explain briefly.\n\nSolution\nI guessed before that the distributions would be right-skewed, and they indeed are, with the long upper tails. Or, if you suspected upper outliers, they are here as well.\nSay what you guessed before, and how your graph confirms it (or doesn’t, if it doesn’t.)\n\\(\\blacksquare\\)\n\nRun a suitable test to compare the average number of hours of TV watched for people with each amount of education. (“Average” could be mean or median, whichever you think is appropriate.)\n\nSolution\nFrom the boxplot, the distributions are definitely not all normal; in fact, none of them are. So we should use Mood’s median test, thus:\n\nmedian_test(gss, tvhours, degree)\n\n$table\n          above\ngroup      above below\n  College     67    70\n  Graduate    18    36\n  HSorLess   355   126\n\n$test\n       what        value\n1 statistic 5.608269e+01\n2        df 2.000000e+00\n3   P-value 6.634351e-13\n\n\n\\(\\blacksquare\\)\n\nWhat do you conclude from your test, in the context of the data?\n\nSolution\nThe P-value of \\(6.6\\times 10^{-13}\\) is extremely small, so we conclude that not all of the education groups watch the same median amount of TV. Or, there are differences in the median amount of TV watched among the three groups.\nAn answer of “the education groups are different” is wrong, because you don’t know that they are all different. It might be that some of them are different and some of them are the same. The next part gets into that.\n\\(\\blacksquare\\)\n\nWhy might you now want to run some kind of follow-up test? Run the appropriate thing and explain briefly what you conclude from it, in the context of the data.\n\nSolution\nThe overall Mood test is significant, so there are some differences between the education groups, but we don’t know where they are. Pairwise median tests will reveal where any differences are:\n\npairwise_median_test(gss, tvhours, degree)\n\n\n\n  \n\n\n\nThe people whose education is high school or less are significantly different from the other two education levels. The boxplot reveals that this is because they watch more TV on average. The college and graduate groups are not significantly different (in median TV watching).\nExtra 1:\nYou might have been surprised that the College and Graduate medians were not significantly different. After all, they look quite different on the boxplot. Indeed, the P-value for comparing just those two groups is 0.0512, only just over 0.05. But remember that we are doing three tests at once, so the Bonferroni adjustment is to multiply the P-values by 3, so this P-value is “really” some way from being significant. I thought I would investigate this in more detail:\n\ngss %&gt;% filter(degree != \"HSorLess\") %&gt;% \nmedian_test(tvhours, degree)\n\n$table\n          above\ngroup      above below\n  College     67    70\n  Graduate    18    36\n\n$test\n       what     value\n1 statistic 3.8027625\n2        df 1.0000000\n3   P-value 0.0511681\n\n\nThe College group are about 50-50 above and below the overall median, but the Graduate group are two-thirds below. This suggests that the Graduate group watches less TV, and with these sample sizes I would have expected a smaller P-value. But it didn’t come out that way.\nYou might also be concerned that there are in total more values below the grand median (106) than above (only 85). This must mean that there are a lot of data values equal to the grand median:\n\ngss %&gt;% filter(degree != \"HSorLess\") -&gt; gss1\ngss1 %&gt;% summarize(med=median(tvhours))\n\n\n\n  \n\n\n\nand\n\ngss1 %&gt;% count(tvhours)\n\n\n\n  \n\n\n\nEverybody gave a whole number of hours, and there are not too many different ones; in addition, a lot of them are equal to the grand median of 2.\nExtra 2:\nRegular ANOVA and Welch ANOVA should be non-starters here because of the non-normality, but you might be curious about how they would perform:\n\ngss.1 &lt;- aov(tvhours~degree, data=gss)\nsummary(gss.1)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndegree        2    267  133.30   25.18 2.27e-11 ***\nResiduals   902   4774    5.29                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(gss.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = tvhours ~ degree, data = gss)\n\n$degree\n                        diff        lwr       upr     p adj\nGraduate-College  -0.4238095 -1.1763372 0.3287181 0.3831942\nHSorLess-College   1.0598958  0.6181202 1.5016715 0.0000001\nHSorLess-Graduate  1.4837054  0.8037882 2.1636225 0.0000011\n\n\nand\n\noneway.test(tvhours~degree, data=gss)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  tvhours and degree\nF = 37.899, num df = 2.00, denom df = 206.22, p-value = 9.608e-15\n\ngamesHowellTest(tvhours~factor(degree), data=gss)\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: tvhours by factor(degree)\n\n\n         College Graduate\nGraduate 0.12    -       \nHSorLess 2.4e-10 1.7e-10 \n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nThe conclusions are actually identical to our Mood test, and the P-values are actually not all that much different. Which makes me wonder just how bad the sampling distributions of the sample means are. Bootstrap to the rescue:\n\ngss %&gt;% \n  nest_by(degree) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$tvhours, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12) +\n  facet_wrap(~degree, scales = \"free\")\n\n\n\n\nCoding this made my head hurt, but building it one line at a time, I pretty much got it right first time. In words:\n\n“compress” the dataframe to get one row per degree and a list-column called data with the number of hours of TV watched for each person with that degree\ngenerate 1000 sims for each degree (to guide the taking of bootstrap samples shortly)\norganize into one row per sim\nthen take bootstrap samples as normal and work out the mean of each one\nmake histograms for each degree, using a different scale for each one. (This has the advantage that the normal number of bins will work for all the histograms.)\n\nIf you are not sure about what happened, run it one line at a time and see what the results look like after each one.\nAnyway, even though the data was very much not normal, these sampling distributions are very normal-looking, suggesting that something like Welch ANOVA would have been not nearly as bad as you would have guessed. This is evidently because of the big sample sizes. (This also explains why the two other flavours of ANOVA gave results very similar to Mood’s median test.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#death-of-poets-1",
    "href": "analysis-of-variance.html#death-of-poets-1",
    "title": "11  Analysis of variance",
    "section": "11.17 Death of poets",
    "text": "11.17 Death of poets\nSome people believe that poets, especially female poets, die younger than other types of writer. William Butler Yeats15 wrote:\n\nShe is the Gaelic16 muse, for she gives inspiration to those she persecutes. The Gaelic poets die young, for she is restless, and will not let them remain long on earth.\n\nA literature student wanted to investigate this, and so collected a sample of 123 female writers (of three different types), and noted the age at death of each writer.\nThe data are in http://ritsokiguess.site/datafiles/writers.csv.\n\nRead in and display (some of) the data.\n\nSolution\nThe usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/writers.csv\"\nwriters &lt;- read_csv(my_url)\n\nRows: 123 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Type\ndbl (2): Type1, Age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwriters\n\n\n\n  \n\n\n\nThere are indeed 123 writers. The second column shows the principal type of writing each writer did, and the third column shows their age at death. The first column is a numerical code for the type of writing, which we ignore (since we can handle the text writing type).\n\\(\\blacksquare\\)\n\nMake a suitable plot of the ages and types of writing.\n\nSolution\nAs usual, one quantitative and one categorical, so a boxplot:\n\nggplot(writers, aes(x=Type, y=Age)) + geom_boxplot()\n\n\n\n\nAt this point, a boxplot is best, since right now you are mostly after a general sense of what is going on, rather than assessing normality in particular (that will come later).\n\\(\\blacksquare\\)\n\nObtain a summary table showing, for each type of writing, the number of writers of that type, along with the mean, median and standard deviation of their ages at death.\n\nSolution\nThe customary group_by and summarize:\n\nwriters %&gt;% group_by(Type) %&gt;% \nsummarize(n=n(), mean=mean(Age), med=median(Age), sd=sd(Age))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nRun a complete analysis, starting with an ordinary (not Welch) analysis of variance, that ends with a conclusion in the context of the data and an assessment of assumptions.\n\nSolution\nI’ve left this fairly open-ended, to see how well you know what needs to be included and what it means. There is a lot of room here for explanatory text to show that you know what you are doing. One output followed by another without any explanatory text suggests that you are just copying what I did without any idea about why you are doing it.\nThe place to start is the ordinary (not Welch) ANOVA. You may not think that this is the best thing to do (you’ll have a chance to talk about that later), but I wanted to make sure that you practiced the procedure:\n\nwriters.1 &lt;- aov(Age~Type, data=writers)\nsummary(writers.1)\n\n             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nType          2   2744  1372.1   6.563 0.00197 **\nResiduals   120  25088   209.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis says that the mean ages at death of the three groups of writers are not all the same, or that there are differences among those writers (in terms of mean age at death). “The mean ages of the types of writer are different” is not accurate enough, because it comes too close to saying that all three groups are different, which is more than you can say right now.\nThe \\(F\\)-test is significant, meaning that there are some differences among17 the means, and Tukey’s method will enable us to see which ones differ:\n\nTukeyHSD(writers.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Age ~ Type, data = writers)\n\n$Type\n                        diff       lwr       upr     p adj\nNovels-Nonfiction  -5.427239 -13.59016  2.735681 0.2591656\nPoems-Nonfiction  -13.687500 -22.95326 -4.421736 0.0018438\nPoems-Novels       -8.260261 -15.63375 -0.886772 0.0240459\n\n\nThere is a significant difference in mean age at death between the poets and both the other types of writer. The novelists and the nonfiction writers do not differ significantly in mean age at death.\nWe know from the boxplots (or the summary table) that this significant difference was because the poets died younger on average, which is exactly what the literature student was trying to find out. Thus, female poets really do die younger on average than female writers of other types. It is best to bring this point out, since this is the reason we (or the literature student) were doing this analysis in the first place. See Extra 1 for more.\nSo now we need to assess the assumptions on which the ANOVA depends.\nThe assumption we made is that the ages at death of the authors of each different type had approximately a normal distribution (given the sample sizes) with approximately equal spread. The boxplots definitely look skewed to the left (well, not the poets so much, but the others definitely). So now consider the sample sizes: 24, 67, and 32 for the three groups (respectively), and make a call about whether you think the normality is good enough. You are certainly entitled to declare the two outliers on the nonfiction writers to be too extreme given a sample size of only 24. Recall that once one sample fails normality, that’s all you need.\nNow, since you specifically want normality, you could reasonably look at normal quantile plots instead of the boxplots. Don’t just get normal quantile plots, though; say something about why you want them instead of the boxplots you drew earlier:\n\nggplot(writers, aes(sample = Age)) +\nstat_qq() + stat_qq_line() + \nfacet_wrap(~Type)\n\n\n\n\nI see that the Nonfiction writers have two outliers at the low end (and are otherwise not bad); the writers of Novels don’t go up high enough (it’s almost as if there is some magic that stops them living beyond 90!); the writers of Poems have a short-tailed distribution. You’ll remember that short tails are not a problem, since the mean is still descriptive of such a distribution; it’s long tails or outliers or skewness that you need to be worried about. The outliers in the Nonfiction writers are the biggest concern.\nAre you concerned that these outliers are a problem, given the sample size? There are only 24 nonfiction writers (from your table of means earlier), so the Central Limit Theorem will help a bit. Make a call about whether these outliers are a big enough problem. You can go either way on this, as long as you raise the relevant issues.\nAnother approach you might take is to look at the P-values. The one in the \\(F\\)-test is really small, and so is one of the ones in the Tukey. So even if you think the analysis is a bit off, those conclusions are not likely to change. The 0.02 P-value in the Tukey, however, is another story. This could become non-significant in actual fact if the P-value is not to be trusted.\nYet another approach (looking at the bootstrapped sampling distributions of the sample means) is in Extra 3. This gets more than a little hairy with three groups, especially doing it the way I do.\nIf you think that the normality is not good enough, it’s a good idea to suggest that we might do a Mood’s Median Test instead, and you could even do it (followed up with pairwise median tests). If you think that normality is all right, you might then look at the spreads. I think you ought to conclude that these are close enough to equal (the SDs from the summary table or the heights of the boxes on the boxplots), and so there is no need to do a Welch ANOVA. (Disagree if you like, but be prepared to make the case.)\nI have several Extras:\nExtra 1: having come to that tidy conclusion, we really ought to back off a bit. These writers were (we assume) a random sample of some population, but they were actually mostly Americans, with a few Canadian and Mexican writers. So this appears to be true at least for North American writers. But this is (or might be) a different thing to the Yeats quote about female Gaelic poets.\nThere is a more prosaic reason. It is harder (in most places, but especially North America) to get poetry published than it is to find a market for other types of writing. (A would-be novelist, say, can be a journalist or write for magazines to pay the bills while they try to find a publisher for their novel.) Thus a poet is living a more precarious existence, and that might bring about health problems.\nExtra 2: with the non-normality in mind, maybe Mood’s median test is the thing:\n\nmedian_test(writers, Age, Type)\n\n$table\n            above\ngroup        above below\n  Nonfiction    17     6\n  Novels        33    30\n  Poems         10    22\n\n$test\n       what       value\n1 statistic 9.872664561\n2        df 2.000000000\n3   P-value 0.007180888\n\n\nThe P-value here is a bit bigger than for the \\(F\\)-test, but it is still clearly significant. Hence, we do the pairwise median tests to find out which medians differ:\n\npairwise_median_test(writers, Age, Type)\n\n\n\n  \n\n\n\nThe conclusion here is exactly the same as for the ANOVA. The P-values have moved around a bit, though: the first one is a little closer to significance (remember, look at the last column since we are doing three tests at once) and the last one is now only just significant.\n\nwriters %&gt;% group_by(Type) %&gt;% \nsummarize(n=n(), mean=mean(Age), med=median(Age), sd=sd(Age))\n\n\n\n  \n\n\n\nIn both of these two cases (Nonfiction-Novels and Novels-Poems), the medians are closer together than the means are. That would explain why the Novels-Poems P-value would increase, but not why the Nonfiction-Novels one would decrease.\nI would have no objection in general to your running a Mood’s Median Test on these data, but the point of this problem was to give you practice with aov.\nExtra 3: the other way to assess if the normality is OK given the sample sizes is to obtain bootstrap sampling distributions of the sample means for each Type. The sample size for the novelists is 67, so I would expect the skewness there to be fine, but the two outliers among the Nonfiction writers may be cause for concern, since there are only 24 of those altogether.\nLet’s see if we can do all three at once (I like living on the edge). I take things one step at a time, building up a pipeline as I go. Here’s how it starts:\n\nwriters %&gt;% nest_by(Type)\n\n\n\n  \n\n\n\nThe thing data is a so-called list-column. The dataframes we have mostly seen so far are like spreadsheets, in that each “cell” or “entry” in a dataframe has something like a number or a piece of text in it (or, occasionally, a thing that is True or False, or a date). Tibble-type dataframes are more flexible than that, however: each cell of a dataframe could contain anything.\nIn this one, the three things in the column data are each dataframes,18 containing the column called Age from the original dataframe. These are the ages at death of the writers of that particular Type. These are the things we want bootstrap samples of.\nI’m not at all sure how this is going to go, so let’s shoot for just 5 bootstrap samples to start with. If we can get it working, we can scale up the number of samples later, but having a smaller number of samples is easier to look at:\n\nwriters %&gt;% nest_by(Type) %&gt;% \n  mutate(sim = list(1:5))\n\n\n\n  \n\n\n\nLet me break off at this point to say that we want 1000 bootstrap samples for the writers of each type, so this is the kind of thing we need to start with. nest_by has an implied rowwise, so we get three lots of values in sim; the list is needed since each one is five values rather than just one. The next stage is to unnest these, and then do another rowwise to work with all the (more) rows of the dataframe we now have. After that, the process should look more or less familiar:\n\nwriters %&gt;% nest_by(Type) %&gt;% \n  mutate(sim = list(1:5)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$Age, replace = TRUE)))\n\n\n\n  \n\n\n\nThat seems to be about the right thing; the bootstrap samples appear to be the right size, considering how many writers of each type our dataset had. From here, work out the mean of each sample:\n\nwriters %&gt;% nest_by(Type) %&gt;% \n  mutate(sim = list(1:5)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$Age, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample))\n\n\n\n  \n\n\n\nand then you could plot those means. This seems to be working, so let’s scale up to 1000 simulations, and make normal quantile plots of the bootstrapped sampling distributions, one for each Type of writer:\n\nwriters %&gt;% nest_by(Type) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$Age, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + \n  stat_qq_line() + facet_wrap(~Type, scales = \"free\")\n\n\n\n\nThese three normal quantile plots are all acceptable, to my mind, although the Nonfiction one, with the two outliers and the smallest sample size, is still a tiny bit skewed to the left. Apart from that, the three sampling distributions of the sample means are close to normal, so our aov is much better than you might have thought from looking at the boxplots. That’s the result of having large enough samples to get help from the Central Limit Theorem.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#religion-and-studying-1",
    "href": "analysis-of-variance.html#religion-and-studying-1",
    "title": "11  Analysis of variance",
    "section": "11.18 Religion and studying",
    "text": "11.18 Religion and studying\nMany students at a certain university were asked about the importance of religion in their lives (categorized as “not”, “fairly”, or “very” important), and also about the number of hours they spent studying per week. (This was part of a much larger survey.) We want to see whether there is any kind of relationship between these two variables. The data are in here.\n\nRead in and display (some of) the data.\n\nSolution\nThe usual. This is a straightforward one:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/student_relig.csv\"\nstudent &lt;- read_csv(my_url)\n\nRows: 686 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ReligImp\ndbl (1): StudyHrs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstudent\n\n\n\n  \n\n\n\n686 students, with columns obviously named for religious importance and study hours.\nExtra:\nI said this came from a bigger survey, actually this one:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/student0405.csv\"\nstudent0 &lt;- read_csv(my_url)\n\nRows: 690 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Sex, ReligImp, Seat\ndbl (4): GPA, MissClass, PartyDays, StudyHrs\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstudent0\n\n\n\n  \n\n\n\nThere are four extra rows here. Why? Let’s look at a summary of the dataframe:\n\nsummary(student0) \n\n     Sex                 GPA          ReligImp           MissClass     \n Length:690         Min.   :1.500   Length:690         Min.   :0.0000  \n Class :character   1st Qu.:2.930   Class :character   1st Qu.:0.0000  \n Mode  :character   Median :3.200   Mode  :character   Median :1.0000  \n                    Mean   :3.179                      Mean   :0.9064  \n                    3rd Qu.:3.515                      3rd Qu.:1.0000  \n                    Max.   :4.000                      Max.   :6.0000  \n                    NA's   :3                          NA's   :1       \n     Seat             PartyDays         StudyHrs    \n Length:690         Min.   : 0.000   Min.   : 0.00  \n Class :character   1st Qu.: 3.000   1st Qu.: 6.25  \n Mode  :character   Median : 7.000   Median :10.00  \n                    Mean   : 7.501   Mean   :13.16  \n                    3rd Qu.:11.000   3rd Qu.:16.00  \n                    Max.   :31.000   Max.   :70.00  \n                                     NA's   :4      \n\n\nYou get information about each variable. For the text variables, you don’t learn much, only how many there are. (See later for more on this.) For each of the four quantitative variables, you see some stats about each one, along with a count of missing values. The study hours variable is evidently skewed to the right (mean bigger than median), which we will have to think about later.\nR also has a “factor” variable type, which is the “official” way to handle categorical variables in R. Sometimes it matters, but most of the time leaving categorical variables as text is just fine. summary handles these differently. My second line of code below says “for each variable that is text, make it into a factor”:\n\nstudent0 %&gt;% \nmutate(across(where(is.character), ~factor(.))) %&gt;% \nsummary()\n\n     Sex           GPA          ReligImp     MissClass          Seat    \n Female:382   Min.   :1.500   Fairly:319   Min.   :0.0000   Back  :134  \n Male  :308   1st Qu.:2.930   Not   :222   1st Qu.:0.0000   Front :151  \n              Median :3.200   Very  :149   Median :1.0000   Middle:404  \n              Mean   :3.179                Mean   :0.9064   NA's  :  1  \n              3rd Qu.:3.515                3rd Qu.:1.0000               \n              Max.   :4.000                Max.   :6.0000               \n              NA's   :3                    NA's   :1                    \n   PartyDays         StudyHrs    \n Min.   : 0.000   Min.   : 0.00  \n 1st Qu.: 3.000   1st Qu.: 6.25  \n Median : 7.000   Median :10.00  \n Mean   : 7.501   Mean   :13.16  \n 3rd Qu.:11.000   3rd Qu.:16.00  \n Max.   :31.000   Max.   :70.00  \n                  NA's   :4      \n\n\nFor factors, you also get how many observations there are in each category, and the number of missing values, which we didn’t get before. However, ReligImp does not have any missing values.\nI said there were four missing values for study hours, that is, four students who left that blank on their survey. We want to get rid of those students (that is, remove those whole rows), and, to simplify things for you, let’s keep only the study hours and importance of religion columns. That goes like this:\n\nstudent0 %&gt;% drop_na(StudyHrs) %&gt;% \nselect(ReligImp, StudyHrs)\n\n\n\n  \n\n\n\nThen I saved that for you. 686 rows instead of 690, having removed the four rows with missing StudyHrs.\nAnother (better, but more complicated) option is to use the package pointblank, which produces much more detailed data validation reports. You would start that by piping your data into scan_data() to get a (very) detailed report of missingness and data values, and then you can check your data for particular problems, such as missing values, or values bigger or smaller than they should be, for the variables you care about. See here for more.\n\\(\\blacksquare\\)\n\nObtain the number of observations and the mean and standard deviation of study hours for each level of importance.\n\nSolution\ngroup_by and summarize (spelling the latter with s or z as you prefer):\n\nstudent %&gt;% group_by(ReligImp) %&gt;% \nsummarize(n=n(), mean_sh=mean(StudyHrs), sd_sh=sd(StudyHrs))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nComment briefly on how the groups compare in terms of study hours.\n\nSolution\nThe students who think religion is very important have a higher mean number of study hours. The other two groups seem similar.\nAs far as the SDs are concerned, make a call. You could say that the very-important group also has a (slightly) larger SD, or you could say that the SDs are all very similar.\nI would actually favour the second one, but this is going to be a question about Welch ANOVA, so go whichever way you like.\n\\(\\blacksquare\\)\n\nMake a suitable graph of this data set.\n\nSolution\nThis kind of data is one quantitative and one categorical variable, so once again a boxplot:\n\nggplot(student, aes(x=ReligImp, y=StudyHrs)) + geom_boxplot()\n\n\n\n\n\\(\\blacksquare\\)\n\nThe statistician in this study decided that the data were sufficiently normal in shape given the (very large) sample sizes, but was concerned about unequal spreads among the three groups. Given this, run a suitable analysis and display the output. (This includes a suitable follow-up test, if warranted.)\n\nSolution\nNormal-enough data (in the statistician’s estimation) and unequal spreads means a Welch ANOVA:\n\noneway.test(StudyHrs~ReligImp, data=student)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  StudyHrs and ReligImp\nF = 7.9259, num df = 2.0, denom df = 350.4, p-value = 0.0004299\n\ngamesHowellTest(StudyHrs~factor(ReligImp), data=student)\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: StudyHrs by factor(ReligImp)\n\n\n     Fairly  Not    \nNot  0.26035 -      \nVery 0.00906 0.00026\n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nGames-Howell is the suitable follow-up here, to go with the Welch ANOVA. It is warranted because the Welch ANOVA was significant.\nMake sure you have installed and loaded PMCMRplus before trying the second half of this.\nExtra: for large data sets, boxplots make it look as if the outlier problem is bad, because a boxplot of a large amount of data will almost certainly contain some outliers (according to Tukey’s definition). Tukey envisaged a boxplot as something you could draw by hand for a smallish data set, and couldn’t foresee something like R and the kind of data we might be able to deal with. To show you the kind of thing I mean, let’s draw some random samples of varying sizes from normal distributions, which should not have outliers, and see how their boxplots look:\n\ntibble(n=c(10, 30, 100, 300)) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(rnorm(n))) %&gt;% \n  unnest(my_sample) %&gt;% \n  ggplot(aes(x = factor(n), y = my_sample)) + geom_boxplot()\n\n\n\n\nAs the sample size gets bigger, the number of outliers gets bigger, and the whiskers get longer. All this means is that in a larger sample, you are more likely to see a small number of values that are further out, and that is not necessarily a reason for concern. Here, the outliers are only one value out of 100 and two out of 300, but they have what looks like an outsize influence on the plot. In the boxplot for our data, the distributions were a bit skewed, but the outliers may not have been as much of a problem as they looked.\n\\(\\blacksquare\\)\n\nWhat do you conclude from your analysis of the previous part, in the context of the data?\n\nSolution\nThe Welch ANOVA was significant, so the religious-importance groups are not all the same in terms of mean study hours, and we need to figure out which groups differ from which. (Or say this in the previous part if you wish.)\nThe students for whom religion was very important had a significantly different mean number of study hours than the other students; the Fairly and Not groups were not significantly different from each other. Looking back at the means (or the boxplots), the significance was because the Very group studied for more hours than the other groups. It seems that religion has to be very important to a student to positively affect how much they study.\nExtra: you might have been concerned that the study hours within the groups were not nearly normal enough to trust the Welch ANOVA. But the groups were large, so there is a lot of help from the Central Limit Theorem. Enough? Well, that is hard to judge.\nMy take on this is to bootstrap the sampling distribution of the sample mean for each group. If that looks normal, then we ought to be able to trust the \\(F\\)-test (regular or Welch, as appropriate). The code is complicated (I’ll explain the ideas below):\n\nstudent %&gt;% \n  nest_by(ReligImp) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(data$StudyHrs, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line() +\n  facet_wrap(~ReligImp, scales = \"free\")\n\n\n\n\nTo truly understand what’s going on, you probably need to run this code one line at a time.\nAnyway, these normal quantile plots are very normal. This says that the sampling distributions of the sample means are very much normal in shape, which means that the sample sizes are definitely large enough to overcome the apparently bad skewness that we saw on the boxplots. In other words, using a regular or Welch ANOVA will be perfectly good; there is no need to reach for Mood’s median test here, despite what you might think from looking at the boxplots, because the sample sizes are so large.\nThe code, line by line:\n\ncreate mini-data-frames called data, containing one column called StudyHrs, for each ReligImp group\nset up for 1000 bootstrap samples for each group, and (next line) arrange for one row per bootstrap sample\nwork rowwise\ngenerate the bootstrap samples\nwork out the mean of each bootstrap sample\nplot normal quantile plots of them, using different facets for each group.\n\nFinally, you might have wondered whether we needed to do Welch:\n\nstudent.1 &lt;- aov(StudyHrs~ReligImp, data=student)\nsummary(student.1)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nReligImp      2   1721   860.7   9.768 6.57e-05 ***\nResiduals   683  60184    88.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(student.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = StudyHrs ~ ReligImp, data = student)\n\n$ReligImp\n                 diff        lwr       upr     p adj\nNot-Fairly  -1.195917 -3.1267811 0.7349462 0.3135501\nVery-Fairly  3.143047  0.9468809 5.3392122 0.0023566\nVery-Not     4.338964  1.9991894 6.6787385 0.0000454\n\n\nIt didn’t make much difference, and the conclusions are identical. So I think either way would have been defensible.\nThe value of doing Tukey is that we get confidence intervals for the difference of means between each group, and this gives us an “effect size”: the students for whom religion was very important studied on average three or four hours per week more than the other students, and you can look at the confidence intervals to see how much uncertainty there is in those estimates. Students vary a lot in how much they study, but the sample sizes are large, so the intervals are not that long.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "analysis-of-variance.html#footnotes",
    "href": "analysis-of-variance.html#footnotes",
    "title": "11  Analysis of variance",
    "section": "",
    "text": "An Irish, that is to say, Gaelic, poet (see below), but a male one.↩︎\nGaelic is a language of Scotland and Ireland, and the culture of the people who speak it.↩︎\nActually, this doesn’t always work if the sample sizes in each group are different. If you’re comparing two small groups, it takes a very large difference in means to get a small P-value. But in this case the sample sizes are all the same.↩︎\nThe computer scientists among you will note that I should not use equals or not-equals to compare a decimal floating-point number, since decimal numbers are not represented exactly in the computer. R, however, is ahead of us here, since when you try to do “food not equal to 4.7”, it tests whether food is more than a small distance away from 4.7, which is the right way to do it. In R, therefore, code like my food !=  4.7 does exactly what I want, but in a language like C, it does not, and you have to be more careful: abs(food-4.7)&gt;1e-8, or something like that. The small number 1e-8 (\\(10^{-8}\\)) is typically equal to machine epsilon, the smallest number on a computer that is distinguishable from zero.↩︎\nMost of these parts are old from assignment questions that I actually asked a previous class to do, but not this part. I added it later.↩︎\nSee discussion elsewhere about Yates’ Correction and fixed margins.↩︎\nIn the pairwise median test in smmr, I did this backwards: rather than changing the alpha that you compare each P-value with from 0.05 to 0.05/6, I flip it around so that you adjust the P-values by multiplying them by 6, and then comparing the adjusted P-values with the usual 0.05. It comes to the same place in the end, except that this way you can get adjusted P-values that are greater than 1, which makes no sense. You read those as being definitely not significant.↩︎\nIt’s probably better in a report to use language a bit more formal than a bunch. Something like a number would be better.↩︎\nThe use of absolute differences, and the median, downplays the influence of outliers. The assumption here is that the absolute differences from the medians are approximately normal, which seems a less big assumption than assuming the actual data are approximately normal.↩︎\nThis is coming back to the power of something like Levene’s test; the power of any test is not going to be very big if the sample sizes are small.↩︎\nThe test goes back to the 1940s.↩︎\nBest, D. J., and J. C. W. Rayner. “Welch’s Approximate Solution for the Behrens–Fisher Problem.” Technometrics 29, no. 2 (May 1, 1987): 205–10. doi:10.1080/00401706.1987.10488211. The data set is near the end.↩︎\nWe’d need a lot more students to make it narrower, but this is not surprising since students vary in a lot of other ways that were not measured here.↩︎\nPerhaps a better word here would be principle, to convey the idea that you can do something else if it works better for your purposes.↩︎\nAn Irish, that is to say, Gaelic, poet (see below), but a male one.↩︎\nGaelic is a language of Scotland and Ireland, and the culture of the people who speak it.↩︎\nThere might be differences between two things, but among three or more.↩︎\nLike those Russian dolls.↩︎"
  },
  {
    "objectID": "reports.html#atomic-weight-of-carbon",
    "href": "reports.html#atomic-weight-of-carbon",
    "title": "12  Writing reports",
    "section": "12.1 Atomic weight of carbon",
    "text": "12.1 Atomic weight of carbon\nThe atomic weight of the chemical element carbon is 12. Two methods of measuring the atomic weight of samples of carbon were compared. The results are shown in link. The methods are labelled 1 and 2. The first task is to find out whether the two methods have different “typical” measures (mean or median, as appropriate) of the atomic weight of carbon.\nFor this question, compose a report in R Markdown. (R Markdown is what you use in an R Notebook, but you can also have a separate R Markdown document from which you can produce HTML, Word etc. output.) See part (a) for how to get this started.\nYour report should read like an actual report, not just the answers to some questions that I set you. To help with that, write some text that links the parts of the report together smoothly, so that it reads as a coherent whole. The grader had 3 discretionary marks to award for the overall quality of your writing. The scale for this was:\n\n3 points: excellent writing. The report flows smoothly, is easy to read, and contains everything it should (and nothing it shouldn’t).\n2 points: satisfactory writing. Not the easiest to read, but says what it should, and it looks at least somewhat like a report rather than a string of answers to questions.\n1 point: writing that is hard to read or to understand. If you get this (or 0), you should consider what you need to do to improve when you write your project.\n0 points: you answered the questions, but you did almost nothing to make it read like a report.\n\n\nCreate a new R Markdown document. To do this, in R Studio, select File, New File, R Markdown. Type the report title and your name in the boxes, and leave the output on the default HTML. Click OK.\nWrite an introduction that explains the purpose of this study and the data collected in your own words.\nBegin an appropriately-titled new section in your report, read the data into R and display the results.\nMake an appropriate plot to compare the measurements obtained by the two methods. You might need to do something about the two methods being given as numbers even though they are really only identifiers. (If you do, your report ought to say what you did and why.)\nComment briefly on what you see in your plot.\nCarry out the most appropriate \\(t\\)-test. (You might like to begin another new section in your report here.)\nDo the most appropriate test you know that does not assume normally-distributed data.\nDiscuss the results of your tests and what they say about the two methods for measuring the atomic weight of carbon. If it seems appropriate, put the discussion into a section called Conclusions."
  },
  {
    "objectID": "reports.html#sparrowhawks",
    "href": "reports.html#sparrowhawks",
    "title": "12  Writing reports",
    "section": "12.2 Sparrowhawks",
    "text": "12.2 Sparrowhawks\n(This is a report-writing question, but it also uses some regression techniques from later in the course.)\nOne of nature’s patterns is the relationship between the percentage of adult birds in a colony that return from the previous year, and the number of new adults that join the colony. Data for 13 colonies of sparrowhawks can be found at link. The columns are the percentage of adults returning from the previous year, and the number of new adults that join the colony.\n\nCreate a new R Markdown report, give it a suitable title, and ask for HTML output. Answer the questions that follow in your report. At any stage, you can Knit HTML to see how the report looks so far.\nRead in the data and display the first few values. Add some text saying how many rows of data there are.\nCreate a new section entitled “Exploratory analysis”, and create a scatterplot for predicting number of new adults from the percentage of returning adults. Describe what you see, adding some suitable text to your report.\nObtain the correlation between the two variables. Is this consistent with the scatterplot? Explain briefly. (The R function you need is cor. You can feed it a data frame.)\nObtain the regression line for predicting the number of new adults from the percentage of returning adults.\nWhat are the intercept and slope of your regression line? Is the slope significant? What does that mean, in the context of the data?\nCreate a scatterplot of the data with the regression line on it.\nFor short-lived birds, the association between these two variables is positive: changes in weather and food supply cause the populations of new and returning birds to increase together. For long-lived territorial birds, however, the association is negative because returning birds claim their territories in the colony and do not leave room for new recruits. Which type of species is the sparrowhawk? Add a short Conclusions section to your report with discussion of this issue."
  },
  {
    "objectID": "reports.html#learning-to-code",
    "href": "reports.html#learning-to-code",
    "title": "12  Writing reports",
    "section": "12.3 Learning to code",
    "text": "12.3 Learning to code\nA programming course, Comp Sci 101, can be taken either in-person, by attending a class at fixed days and times, or online, by doing sessions that can be taken at times the student chooses. The course coordinator wants to know whether students taking the course in these two different ways learn a different amount, as measured by their scores on the final exam for the course. This example comes from the before-times, so the final exam was taken in person by all students. The final exam was out of 45 marks. A total of 18 students took part in the study. Each student was allowed to choose the section they preferred. The data are in http://ritsokiguess.site/datafiles/proggo.csv.\nWrite a report of a complete and appropriate analysis of these data. Your report should include a description of the data in your own words, any necessary pre-processing steps, appropriate graphs, statistical analysis, assessment of assumptions for your preferred analysis, and a statement of conclusions. Imagine that your report will be read by the department Chair, who does not know about this study, and who still remembers some of their first-year Statistics course.\n(My example report is in a later chapter, the one called Learning to Code.)"
  },
  {
    "objectID": "reports.html#treating-dandruff",
    "href": "reports.html#treating-dandruff",
    "title": "12  Writing reports",
    "section": "12.4 Treating dandruff",
    "text": "12.4 Treating dandruff\nAccording to the Mayo Clinic, dandruff is “a common condition that causes the skin on the scalp to flake. It isn’t contagious or serious. But it can be embarrassing and difficult to treat.” Shampoos often claim to be effective in treating dandruff. In a study, four shampoos were compared:\n\nPyrI: 1% pyrithione zinc shampoo\nPyrII: the same as PyrI but with instructions to shampoo two times at each wash. The labels for these are Pyr with a Roman numeral I or II attached.\nKeto: 2% ketoconazole shampoo\nPlacebo: a placebo shampoo\n\nEach subject was randomly assigned to a shampoo. After six weeks of treatment, eight sections of the scalp were examined for each subject. Each section of the scalp was given a score that measured the amount of flaking on a scale of 0-10, less flaking being better. The response variable, called Flaking, was the sum of these eight scores, and is a whole number for each subject.\nThe data are in http://ritsokiguess.site/datafiles/dandruff.txt, with the data values separated by tabs.\nYour task is to write a report on your analysis of this data set, and to make a recommendation for the best shampoo(s) out of the four studied here. The target audience for your report is the principal investigator of the study described above, who knows a lot about shampoo, but not so much about statistics. (They took a course some time ago that covered the material you’ve seen in this course so far, at about the level of STAB22 or STA 220.) Some things you might want to consider, in no particular order (you need to think about where and if to include these things):\n\nan Introduction, written in your own words as much as possible\na Conclusion that summarizes what you found\na suitable and complete piece of statistical inference\na numerical summary of the data\ngraph(s) of the data\nan assessment of the assumptions of your analysis\ncitation of external sources\nanything else that you can make the case for including\n\nIn R Markdown (the text of an R Notebook), you can use ## to make a heading (you can experiment with more or fewer # symbols).\nYour aim is to produce a report, suitable for the intended audience, with all the important elements and no irrelevant ones, that is well-written and easy to follow. There is credit for good writing. For this report, you should include your code in with your report. (In a real report, you would probably show the output and not the code, but we are interested in your code here as well.)\n(My example report is in a later chapter.)\nMy solutions follow. The example reports on coding and treating dandruff are separate chapters."
  },
  {
    "objectID": "reports.html#atomic-weight-of-carbon-1",
    "href": "reports.html#atomic-weight-of-carbon-1",
    "title": "12  Writing reports",
    "section": "12.5 Atomic weight of carbon",
    "text": "12.5 Atomic weight of carbon\nThe atomic weight of the chemical element carbon is 12. Two methods of measuring the atomic weight of samples of carbon were compared. The results are shown in link. The methods are labelled 1 and 2. The first task is to find out whether the two methods have different “typical” measures (mean or median, as appropriate) of the atomic weight of carbon.\nFor this question, compose a report in R Markdown. (R Markdown is what you use in an R Notebook, but you can also have a separate R Markdown document from which you can produce HTML, Word etc. output.) See part (a) for how to get this started.\nYour report should read like an actual report, not just the answers to some questions that I set you. To help with that, write some text that links the parts of the report together smoothly, so that it reads as a coherent whole. The grader had 3 discretionary marks to award for the overall quality of your writing. The scale for this was:\n\n3 points: excellent writing. The report flows smoothly, is easy to read, and contains everything it should (and nothing it shouldn’t).\n2 points: satisfactory writing. Not the easiest to read, but says what it should, and it looks at least somewhat like a report rather than a string of answers to questions.\n1 point: writing that is hard to read or to understand. If you get this (or 0), you should consider what you need to do to improve when you write your project.\n0 points: you answered the questions, but you did almost nothing to make it read like a report.\n\n\nCreate a new R Markdown document. To do this, in R Studio, select File, New File, R Markdown. Type the report title and your name in the boxes, and leave the output on the default HTML. Click OK.\n\nSolution\nYou’ll see the title and your name in a section at the top of the document, and below that you’ll see a template document, as you would for an R Notebook. The difference is that where you are used to seeing Preview, it now says “knit”, but this has the same effect of producing the formatted version of your report.\n\\(\\blacksquare\\)\n\nWrite an introduction that explains the purpose of this study and the data collected in your own words.\n\nSolution\nSomething like this:\n\nThis study is intended to compare two different methods (labelled 1 and 2) for measuring the atomic weight of carbon (which is known in actual fact to be 12). Fifteen samples of carbon were used; ten of these were assessed using method 1 and the remaining five using method 2. The primary interest in this particular study is to see whether there is a difference in the mean or median atomic weight as measured by the two methods.\n\nBefore that, start a new section like this: ## Introduction. Also, get used to expressing your understanding in your words, not mine. Using my words, in my courses, is likely to be worth very little.\n\\(\\blacksquare\\)\n\nBegin an appropriately-titled new section in your report, read the data into R and display the results.\n\nSolution\nValues separated by spaces:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/carbon.txt\"\ncarbon &lt;- read_delim(my_url, \" \")\n\nRows: 15 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): method, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncarbon\n\n\n\n  \n\n\n\nI would expect you to include, without being told to include it, some text in your report indicating that you have sensible data: two methods labelled 1 and 2 as promised, and a bunch1 of atomic weights close to the nominal figure of 12.\n\\(\\blacksquare\\)\n\nMake an appropriate plot to compare the measurements obtained by the two methods. You might need to do something about the two methods being given as numbers even though they are really only identifiers. (If you do, your report ought to say what you did and why.)\n\nSolution\nThe appropriate plot, with a categorical method and quantitative weight, is something like a boxplot. If you’re not careful, method will get treated as a quantitative variable, which you don’t want; the easiest way around that, for a boxplot at least, is to turn it into a factor like this:\n\nggplot(carbon, aes(x = factor(method), y = weight)) + geom_boxplot()\n\n\n\n\nIf you insist, you could do a faceted histogram (above and below, for preference):\n\nggplot(carbon, aes(x = weight)) + geom_histogram(bins = 5) +\n  facet_wrap(~method, ncol = 1)\n\n\n\n\nThere are really not enough data values for a histogram to be of much help, so I don’t like this as much.\nIf you are thinking ahead (we are going to be doing a \\(t\\)-test), then you’ll realize that normality is the kind of thing we’re looking for, in which case normal quantile plots would be the thing. However, we might have to be rather forgiving for method 2 since there are only 5 observations:\n\nggplot(carbon, aes(sample = weight)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~method)\n\n\n\n\nI don’t mind these coming out side by side, though I would rather have them squarer.\nI would say, boxplots are the best, normal quantile plots are also acceptable, but expect to lose something for histograms because they offer only a rather crude comparison in this case.\n\\(\\blacksquare\\)\n\nComment briefly on what you see in your plot.\n\nSolution\nIn boxplots, if that’s what you drew, there are several things that deserve comment: the medians, the spreads and the shapes. The median for method 1 is a little bit lower than for method 2 (the means are probably more different, given the shapes of the boxes). The spread for method 2 is a lot bigger. (Looking forward, that suggests a Welch-Satterthwaite rather than a pooled test.) As for shape, the method 2 measurements seem more or less symmetric (the whiskers are equal anyway, even if the position of the median in the box isn’t), but the method 1 measurements have a low outlier. The histograms are hard to compare. Try to say something about centre and spread and shape. I think the method 2 histogram has a slightly higher centre and definitely bigger spread. On my histogram for method 1, the distribution looks skewed left. If you did normal quantile plots, say something sensible about normality for each of the two methods. For method 1, I would say the low value is an outlier and the rest of the values look pretty straight. Up to you whether you think there is a curve on the plot (which would indicate skewness, but then that highest value is too high: it would be bunched up with the other values below 12.01 if there were really skewness). For method 2, it’s really hard to say anything since there are only five values. Given where the line goes, there isn’t much you can say to doubt normality. Perhaps the best you can say here is that in a sample of size 5, it’s difficult to assess normality at all.\n\\(\\blacksquare\\)\n\nCarry out the most appropriate \\(t\\)-test. (You might like to begin another new section in your report here.)\n\nSolution\nThis would be the Welch-Satterthwaite version of the two-sample \\(t\\)-test, since the two groups do appear to have different spreads:\n\nt.test(weight ~ method, data = carbon)\n\n\n    Welch Two Sample t-test\n\ndata:  weight by method\nt = -1.817, df = 5.4808, p-value = 0.1238\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.027777288  0.004417288\nsample estimates:\nmean in group 1 mean in group 2 \n       12.00260        12.01428 \n\n\nImagining that this is a report that would go to your boss, you ought to defend your choice of the Welch-Satterthwaite test (as I did above), and not just do the default \\(t\\)-test without comment.\nIf, in your discussion above, you thought the spreads were equal enough, then you should do the pooled \\(t\\)-test here, which goes like this:\n\nt.test(weight ~ method, data = carbon, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  weight by method\nt = -2.1616, df = 13, p-value = 0.04989\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -2.335341e-02 -6.588810e-06\nsample estimates:\nmean in group 1 mean in group 2 \n       12.00260        12.01428 \n\n\nThe point here is that you should do the right test based on your conclusion. Being consistent is the most important thing. (In this case, note that the P-values are very different. We’ll get to that shortly.)\nIf we were doing this in SAS, as we see later, we’d get a test at the bottom of the output that compares the two variances. I feel that it’s just as good to eyeball the spreads and make a call about whether they are “reasonably close”. Or even, to always do the Welch-Satterthwaite test on the basis that it is pretty good even if the two populations have the same variance. (If this last point of view is one that you share, you ought to say something about that when you do your \\(t\\)-test.)\nExtra: I guess this is a good place to say something about tests for comparing variances, given that you might be pondering that. There are several that I can think of, that R can do, of which I mention two.\nThe first is the \\(F\\)-test for variances that you might have learned in B57 (that is the basis for the ANOVA \\(F\\)-test):\n\nvar.test(weight ~ method, data = carbon)\n\n\n    F test to compare two variances\n\ndata:  weight by method\nF = 0.35768, num df = 9, denom df = 4, p-value = 0.1845\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.04016811 1.68758230\nsample estimates:\nratio of variances \n         0.3576842 \n\n\nThis, unfortunately, is rather dependent on the data in the two groups being approximately normal. Since we are talking variances rather than means, there is no Central Limit Theorem to rescue us for large samples (quite aside from the fact that these samples are not large). Since the ANOVA \\(F\\)-test is based on the same theory, this is why normality is also more important in ANOVA than it is in a \\(t\\)-test.\nThe second is Levene’s test. This doesn’t depend on normality (at least, not nearly so much), so I like it better in general:\n\nlibrary(car)\nleveneTest(weight ~ factor(method), data = carbon)\n\n\n\n  \n\n\n\nLevene’s test takes a different approach: first the absolute differences from the group medians are calculated, and then an ANOVA is run on the absolute differences. If, say, one of the groups has a larger spread than the other(s), its absolute differences from the median will tend to be bigger.2 As for what we conclude here, well, neither of the variance tests show any significance at all, so from that point of view there is no evidence against using the pooled \\(t\\)-test. Having said that, the samples are small, and so it would be difficult to prove that the two methods have different variance, even if they actually did.3\nThings are never as clear-cut as you would like. In the end, it all comes down to making a call and defending it.\n\\(\\blacksquare\\)\n\nDo the most appropriate test you know that does not assume normally-distributed data.\n\nSolution\nThat would be Mood’s median test. Since I didn’t say anything about building it yourself, feel free to use smmr:\n\nlibrary(smmr)\nmedian_test(carbon, weight, method)\n\n$table\n     above\ngroup above below\n    1     3     6\n    2     4     1\n\n$test\n       what      value\n1 statistic 2.80000000\n2        df 1.00000000\n3   P-value 0.09426431\n\n\nAs an aside, if you have run into a non-parametric test such as Mann-Whitney or Kruskal-Wallis that applies in this situation, be careful about using it here, because they have additional assumptions that you may not want to trust. Mann-Whitney started life as a test for “equal distributions”.4 This means that the null is equal location and equal spread, and if you reject the null, one of those has failed. But here, we suspect that equal spread will fail, so that the Mann-Whitney test may end up rejecting whether or not the medians are different, so it won’t answer the question you want an answer to. Mood’s median test doesn’t have that problem; all it’s saying if the null is true is that the medians are equal; the spreads could be anything at all.\nThe same kind of issues apply to the signed-rank test vs. the sign test. In the case of the signed-rank test, the extra assumption is of a symmetric distribution — to my mind, if you don’t believe normality, you probably don’t have much confidence in symmetry either. That’s why I like the sign test and Mood’s median test: in the situation where you don’t want to be dealing with assumptions, these tests don’t make you worry about that.\nAnother comment that you don’t need to make is based on the not-quite-significance of the Mood test. The P-value is less than 0.10 but not less than 0.05, so it doesn’t quite reach significance by the usual standard. But if you look up at the table, the frequencies seem rather unbalanced: 6 out of the remaining 9 weights in group 1 are below the overall median, but 4 out of 5 weights in group 2 are above. This seems as if it ought to be significant, but bear in mind that the sample sizes are small, and thus Mood’s median test needs very unbalanced frequencies, which we don’t quite have here.\n\\(\\blacksquare\\)\n\nDiscuss the results of your tests and what they say about the two methods for measuring the atomic weight of carbon. If it seems appropriate, put the discussion into a section called Conclusions.\n\nSolution\nBegin by pulling out the P-values for your preferred test(s) and say what they mean. The P-value for the Welch-Satterthwaite \\(t\\)-test is 0.1238, which indicates no difference in mean atomic weights between the two methods. The Mood median test gives a similarly non-significant 0.0943, indicating no difference in the median weights. If you think both tests are plausible, then give both P-values and do a compare-and-contrast with them; if you think that one of the tests is clearly preferable, then say so (and why) and focus on that test’s results.\nIf you thought the pooled test was the right one, then you’ll have a bit more discussion to do, since its P-value is 0.0499, and at \\(\\alpha=0.05\\) this test disagrees with the others. If you are comparing this test with the Mood test, you ought to make some kind of reasoned recommendation about which test to believe.\nAs ever, be consistent in your reasoning.\nExtra: this dataset, where I found it, was actually being used to illustrate a case where the pooled and the Welch-Satterthwaite tests disagreed. The authors of the original paper that used this dataset (a 1987 paper by Best and Rayner;5 the data come from 1924!) point out that the pooled \\(t\\)-test can be especially misleading when the smaller sample is also the one with the larger variance. This is what happened here.\nIn the Best and Rayner paper, the Mood (or the Mann-Whitney) test was not being considered, but I think it’s good practice to draw a picture and make a call about which test is appropriate.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "reports.html#sparrowhawks-1",
    "href": "reports.html#sparrowhawks-1",
    "title": "12  Writing reports",
    "section": "12.6 Sparrowhawks",
    "text": "12.6 Sparrowhawks\n(This is a report-writing question, but it also uses some regression techniques from later in the course.)\nOne of nature’s patterns is the relationship between the percentage of adult birds in a colony that return from the previous year, and the number of new adults that join the colony. Data for 13 colonies of sparrowhawks can be found at link. The columns are the percentage of adults returning from the previous year, and the number of new adults that join the colony.\n\nCreate a new R Markdown report, give it a suitable title, and ask for HTML output. Answer the questions that follow in your report. At any stage, you can Knit HTML to see how the report looks so far.\n\nSolution\n(Note: this is the previous version of Quarto, called R Markdown. The two are fairly similar.)\nIn R Studio, select File, New File, R Markdown. Fill in the Title, Author and leave the Default Output Format at HTML. You’ll see a template report with the document info at the top. This is my document info:\n\nThis is known in the jargon as a “YAML block”.6 Below that is the template R Markdown document, which you can delete now or later.\n\\(\\blacksquare\\)\n\nRead in the data and display the first few values. Add some text saying how many rows of data there are.\n\nSolution\nRead the data into a data frame. In your report, add some text like “we read in the data”, perhaps after a section heading like “The data”. Then add a code chunk by selecting Chunks and Insert Chunk, or by pressing control-alt-I. So far you have something like this.\n\nInside the code chunk, that is, in the bit between the backtick characters, put R code, just as you would type it at the Console or put in an R notebook. In this case, that would be the following code, minus the message that comes out of read_delim:\n\nlibrary(tidyverse)\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/sparrowhawk.txt\"\nsparrowhawks &lt;- read_delim(my_url, \" \")\n\nRows: 13 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): returning, newadults\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsparrowhawks\n\nFor you, it looks like this:\n\nWe don’t know how many rows of data there are yet, so I’ve left a “placeholder” for it, when we figure it out. The file is annoyingly called sparrowhawk.txt, singular. Sorry about that. If you knit this (click on “Knit HTML” next to the ball of wool, or press control-shift-K), it should run, and you’ll see a viewer pop up with the HTML output. Now you can see how many rows there are, and you can go back and edit the R Markdown and put in 13 in place of the x’s, and knit again. You might be worried about how hard R is working with all this knitting. Don’t worry about that. R can take it. Mine looked like this:\n\nThere is a better way of adding values that come from the output, which I mention here in case you are interested (if you are not, feel free to skip this). What you do is to make what is called an “inline code chunk”. Where you want a number to appear in the text, you have some R Markdown that looks like this:\n\nThe piece inside the backticks is the letter r, a space, and then one line of R code. The one line of code will be run, and all of the stuff within the backticks will be replaced in the output by the result of running the R code, in this case the number 13. Typically, you are extracting a number from the data, like the number of rows or a mean of something. If it’s a decimal number, it will come out with a lot of decimal places unless you explicitly round it. OK, let me try it: the data frame has 13 rows altogether. I didn’t type that number; it was calculated from the data frame. Woo hoo!\n\\(\\blacksquare\\)\n\nCreate a new section entitled “Exploratory analysis”, and create a scatterplot for predicting number of new adults from the percentage of returning adults. Describe what you see, adding some suitable text to your report.\n\nSolution\nThe R code you add should look like this, with the results shown (when you knit the report again):\n\nlibrary(tidyverse)\nggplot(sparrowhawks, aes(x = returning, y = newadults)) +\n  geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\nThe piece of report that I added looks like this:\n\nNote (i) that you have to do nothing special to get the plot to appear, and (ii) that I put “smaller” in italics, and you see how.\n\\(\\blacksquare\\)\n\nObtain the correlation between the two variables. Is this consistent with the scatterplot? Explain briefly. (The R function you need is cor. You can feed it a data frame.)\n\nSolution\nThe appropriate R code is this, in another code chunk:\n\nwith(sparrowhawks, cor(newadults, returning))\n\n[1] -0.7484673\n\n\nOr you can ask for the correlations of the whole data frame:\n\ncor(sparrowhawks)\n\n           returning  newadults\nreturning  1.0000000 -0.7484673\nnewadults -0.7484673  1.0000000\n\n\nThis latter is a “correlation matrix” with a correlation between each column and each other column. Obviously the correlation between a column and itself is 1, and that is not the one we want.\nI added this to the report (still in the Exploratory Analysis section, since it seems to belong there):\n\n\\(\\blacksquare\\)\n\nObtain the regression line for predicting the number of new adults from the percentage of returning adults.\n\nSolution\nThis R code, in another code chunk:\n\nnewadults.1 &lt;- lm(newadults ~ returning, data = sparrowhawks)\nsummary(newadults.1)\n\n\nCall:\nlm(formula = newadults ~ returning, data = sparrowhawks)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8687 -1.2532  0.0508  2.0508  5.3071 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 31.93426    4.83762   6.601 3.86e-05 ***\nreturning   -0.30402    0.08122  -3.743  0.00325 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.667 on 11 degrees of freedom\nMultiple R-squared:  0.5602,    Adjusted R-squared:  0.5202 \nF-statistic: 14.01 on 1 and 11 DF,  p-value: 0.003248\n\n\n\\(\\blacksquare\\)\n\nWhat are the intercept and slope of your regression line? Is the slope significant? What does that mean, in the context of the data?\n\nSolution\nSee the output in the previous part. That’s what we need to talk about. I added this to the report. I thought we deserved a new section here:\n\n\\(\\blacksquare\\)\n\nCreate a scatterplot of the data with the regression line on it.\n\nSolution\nThis code. Using geom_smooth with method=\"lm\" will add the regression line to the plot:\n\nggplot(sparrowhawks, aes(x = returning, y = newadults)) +\n  geom_point() + geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nI added a bit of text to the report, which I will show in a moment.\n\\(\\blacksquare\\)\n\nFor short-lived birds, the association between these two variables is positive: changes in weather and food supply cause the populations of new and returning birds to increase together. For long-lived territorial birds, however, the association is negative because returning birds claim their territories in the colony and do not leave room for new recruits. Which type of species is the sparrowhawk? Add a short Conclusions section to your report with discussion of this issue.\n\nSolution\nMy addition to the report looks like this:\n\nI think that rounds off the report nicely.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "reports.html#footnotes",
    "href": "reports.html#footnotes",
    "title": "12  Writing reports",
    "section": "",
    "text": "It’s probably better in a report to use language a bit more formal than a bunch. Something like a number would be better.↩︎\nThe use of absolute differences, and the median, downplays the influence of outliers. The assumption here is that the absolute differences from the medians are approximately normal, which seems a less big assumption than assuming the actual data are approximately normal.↩︎\nThis is coming back to the power of something like Levene’s test; the power of any test is not going to be very big if the sample sizes are small.↩︎\nThe test goes back to the 1940s.↩︎\nBest, D. J., and J. C. W. Rayner. “Welch’s Approximate Solution for the Behrens–Fisher Problem.” Technometrics 29, no. 2 (May 1, 1987): 205–10. doi:10.1080/00401706.1987.10488211. The data set is near the end.↩︎\nYAML stands for Yet Another Markup Language, but we’re only using it in this course as the top bit of an R Markdown document.↩︎"
  },
  {
    "objectID": "coding.html#introduction",
    "href": "coding.html#introduction",
    "title": "13  Learning to code",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nDo students learn programming more or less effectively from an online course, completed on their own time, compared with a regular in-person lecture course that meets at the same times every week? In Comp Sci 101, a study was carried out in which 18 students, 9 each in the online and in-person sections, were assessed for learning by means of the course final exam (out of 45 marks). We compare the mean final exam scores for the students in the two sections."
  },
  {
    "objectID": "coding.html#data-and-pre-processing",
    "href": "coding.html#data-and-pre-processing",
    "title": "13  Learning to code",
    "section": "13.2 Data and pre-processing",
    "text": "13.2 Data and pre-processing\nWe begin by reading in the data:\n\nlibrary(tidyverse)\n\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/proggo.csv\"\nprog0 &lt;- read_csv(my_url)\n\nRows: 9 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): online, classroom\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprog0\n\n\n\n  \n\n\n\nThe nine students in each section are in separate columns. This is not an appropriate layout for analysis because each row contains data for two separate, unrelated students, and we need to have each student’s score in its own row. (See note 1.) This means making the data longer (see note 2):\n\nprog0 %&gt;% pivot_longer(everything(), \n  names_to = \"instruction\", \n  values_to = \"mark\") -&gt; prog\nprog\n\n\n\n  \n\n\n\nNow everything is arranged as we need, and we can proceed to analysis."
  },
  {
    "objectID": "coding.html#analysis",
    "href": "coding.html#analysis",
    "title": "13  Learning to code",
    "section": "13.3 Analysis",
    "text": "13.3 Analysis\nWe begin by visualizing the data. With one quantitative variable mark and one categorical variable instruction, a boxplot will give us an overall picture (see note 3):\n\nggplot(prog, aes(x= instruction, y = mark)) + geom_boxplot()\n\n\n\n\nIt looks as if the average (median) mark is somewhat higher for the students in the online section. In addition, both distributions look reasonably symmetric with no outliers, and they appear to have similar spread. (see note 4.)\nWith that in mind, it seems sensible to compare the mean final exam marks in the two groups by a pooled two-sample \\(t\\)-test, in order to see whether the apparent difference in performance is any more than chance. The aim of the course coordinator was to see whether there was any difference between the two sections, without having any prior idea about which teaching method would be better, so a two-sided test is appropriate: (see note 5.)\n\nt.test(mark~instruction, data = prog, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  mark by instruction\nt = -1.6495, df = 16, p-value = 0.1185\nalternative hypothesis: true difference in means between group classroom and group online is not equal to 0\n95 percent confidence interval:\n -8.379039  1.045706\nsample estimates:\nmean in group classroom    mean in group online \n               31.55556                35.22222 \n\n\nThe P-value of 0.1185 is not smaller than 0.05, so there is no evidence of any difference between the mean final exam marks of the students in the two sections. The difference between the two groups shown on the boxplot is the kind of thing that could be observed by chance if the students were performing equally well under the two methods of instruction. (See notes 6 and 7.)\nThis \\(t\\)-test comes with the assumption that the data in each group come from a normal distribution, at least approximately. With two small samples, this will not be easy to assess, but we can at least look for any gross violations, using a normal quantile plot for each group:\n\nggplot(prog, aes(sample = mark)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~instruction)\n\n\n\n\nThe largest observation in the classroom group and the largest two observations in the online group are a little bigger than expected, but they were not big enough to be flagged as outliers on the boxplots, so I conclude that the normality is good enough to justify the \\(t\\)-test that we did. (See note 8.)"
  },
  {
    "objectID": "coding.html#conclusions-see-note-9",
    "href": "coding.html#conclusions-see-note-9",
    "title": "13  Learning to code",
    "section": "13.4 Conclusions (see note 9)",
    "text": "13.4 Conclusions (see note 9)\nWe found that there is no significant difference in the performance of the students learning in the classroom compared to those learning online. Before leaping to generalize to other classes, however, we should note two limitations of the study (see note 10):\n\nthe sample sizes were very small; if we had observed this size of difference between the two groups in larger samples, we might have been able to show that the difference was significant.\nwe have no information about how the students were allocated to the groups, and it seems likely that the students were allowed to choose their own method of instruction. If the students had been randomly allocated to instruction method, we could have been more confident that any differences observed were due to the instruction method, rather than also having something to do with the relative ability of the students who chose each instruction method.\n\nWe feel that it would be worth running another study of this type, but with larger sample sizes and randomly allocating students to instruction types. This latter, however, risks running into ethical difficulties, since students will normally wish to choose the section they are in.\nThus ends the report.\nNotes:\n\nSay something about the kind of data layout you have, and whether it’s what you want.\nYou can do a \\(t\\)-test without rearranging the data (the method is that of the “all possible \\(t\\)-tests” discussion in the ANOVA section), but if you try to draw plots with the data laid out that way, you will at best be repeating yourself a lot and at worst get stuck (if you try to make side-by-side boxplots). The right format of data should give you no surprises!\n\nI split the pivot-longer onto several lines so that it wouldn’t run off the right side of the page. Recall that R doesn’t mind if you have it on one line or several, as long as it can tell that the current line is incomplete, which it must be until it sees the closing parenthesis on the pivot_longer.\n\nA normal quantile plot is also justifiable here, but I think it would take more justification, because you would have to sell your reader on the need for normality this early in the story. A standard plot like a boxplot needs no such justification; it just describes centre, spread and shape, exactly the kind of thing your first look at the data should be telling you about.\nSay something about what the graph is telling you. It makes sense to do as I did and look forward to the kind of inference you are going to try. If you do a normal quantile plot here, you can formally assess the assumptions before you do the test, which you might argue makes more sense, rather than doing a second plot and assessing the assumptions later as I do.\nYou might be able to justify a one-sided test here, along the lines of “is the online instruction significantly worse?”, but in any case, you need to justify the one-sided or two-sided test that you do.\nConclusion in the context of the data, as ever. Writing “we fail to reject the null hypothesis” and then stopping invites your reader to ask “so what?”.\nYou might have chosen to do a different test, but your choice needs to be properly justified. I think the pooled \\(t\\)-test is the best choice here. If you thought those marks were not normal enough, then you need to do Mood’s median test, explaining why. That comes out like this:\n\n\nmedian_test(prog, mark, instruction)\n\n$table\n           above\ngroup       above below\n  classroom     3     6\n  online        6     3\n\n$test\n       what     value\n1 statistic 2.0000000\n2        df 1.0000000\n3   P-value 0.1572992\n\n\nThe (two-sided) P-value is a little bigger than the one for the pooled \\(t\\)-test, but the conclusion is the same. (If you think the test should be one-sided, justify dividing your P-value by 2, if you can. In the case that your one-sided alternative was that classroom teaching was better, you cannot reject the null in favour of that, because the online students actually have better marks in these samples. In that case, halving the P-value would not be justifiable.)\nIf you thought that the normality was all right, but the equal spreads was not, you will need to justify the unequal spreads. Perhaps the best way here is to say that you are unsure whether the spreads are close enough to equal, so you are doing the Welch test to be safe. That looks like this:\n\nt.test(mark~instruction, data = prog)\n\n\n    Welch Two Sample t-test\n\ndata:  mark by instruction\nt = -1.6495, df = 15.844, p-value = 0.1187\nalternative hypothesis: true difference in means between group classroom and group online is not equal to 0\n95 percent confidence interval:\n -8.382820  1.049486\nsample estimates:\nmean in group classroom    mean in group online \n               31.55556                35.22222 \n\n\nAs you see, the P-values of the pooled and Welch \\(t\\)-tests are almost identical (which is what I was guessing), but to do the Welch test without comment reveals that you are not thinking about which of the two tests is more appropriate here. In the real world, you might get away with it (for these data, the conclusions are the same), but in this course I need to see that your thought process is correct.\nA final observation in this note: all three tests give you similar P-values and the same conclusion, so that in the end it didn’t really matter which one of them you did. When that happens, it’s usually (as here) a sign that a \\(t\\)-test will be best, because it makes the best use of the data (and thus you will get the most power in your test). I cannot stop you doing all three tests behind the scenes and using this to help decide, but strictly speaking your P-value will not be what you say it is, because these tests (really, any tests) are designed so that the test you choose is the only one you do. In any case, your report should include only the one test you thought was most appropriate. Your reader does not have the time or patience for a detailed comparison of the three tests.\n\nYou might have combined the assessment of assumptions with your first plot, particularly if you chose a normal quantile plot instead of a boxplot there. As long as you have assessed normality (and equal spreads, if you are happy with the normality) somewhere, this is fine. In particular, if you ended up doing a Mood median test, you have presumably already decided that the normality was not good enough, and (I hope) you already discussed that somewhere.\n\nAgain, bear in mind who is (in the setup that we have) reading your report: someone who might remember about \\(t\\)-tests and normality. Ideas like the bootstrap are far too advanced to go into a report like this. If you must include something like that, you need to put it in an appendix, not the main body of the report, so that the person reading your report can get the main ideas without having to wade through that.\nThe rest of this note is an Extra:\nHere is a slightly different way to approach the bootstrap in this case, that takes care of assessing the normality of both groups at once.\nThe first step is nest_by, which does two things, one of which is invisible:\n\nprog %&gt;% nest_by(instruction)\n\n\n\n  \n\n\n\nThis (visibly) creates a list-column containing two mini dataframes data. They are the original data with all the data except instruction in each one: that is, the other column mark. The top one is the nine marks for the students in the classroom section, and the bottom one is the nine marks for the students in the online section. The invisible thing is that nest_by includes a rowwise, so that what we do after this is one for each row, one at a time.\nWhat we do next is to generate a lot of bootstrap samples, for each group. First, a vector of simulation numbers, one for each method of instruction. This only needs to be said once because nest_by is like group_by:\n\nprog %&gt;% nest_by(instruction) %&gt;% \n  mutate(sim = list(1:10))\n\n\n\n  \n\n\n\nThen we need to unnest those sims, so that we can put a bootstrap sample next to each one:\n\nprog %&gt;% nest_by(instruction) %&gt;% \n  mutate(sim = list(1:10)) %&gt;% \n  unnest(sim)\n\n\n\n  \n\n\n\nNow we draw a bootstrap sample from the data to the left of each sim (the ones next to classroom are the in-person ones, and the ones next to online are the online ones, so we will be taking bootstrap samples of the right thing). This here is where the rowwise goes:\n\nprog %&gt;% nest_by(instruction) %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(the_sample = list(sample(data$mark, replace = TRUE))) %&gt;% \n  mutate(the_mean = mean(the_sample)) %&gt;% \n  ggplot(aes(sample = the_mean)) + stat_qq() +\n    stat_qq_line() + facet_wrap(~instruction)\n\n\n\n\nThese are close to their lines, which tells me indeed that the two-sample \\(t\\)-test I did was perfectly reasonable.\nAs I say, though, this is very much for you, and not for the report, given who would be reading it. If you were doing a presentation on this, the bootstrap stuff is something you would keep in reserve in case someone asks about the appropriateness of the \\(t\\)-test, and then you could talk about it, but otherwise you wouldn’t mention it at all.\n\nYou definitely need some conclusions. If the department chair is busy (quite likely), this is the only part of the entire report they may be able to read.\nThis is the place to put limitations, and recommendations for next time. The sample size one is (I hope) obvious, but you can also get at the other one by asking yourself how the students were assigned to instruction methods. The classical comparative study assigns them at random; that way, you know that the two groups of students are at least supposed to be about equal on ability (and anything else) before you start. But if the students choose their own groups, it might be that (say) the weaker students choose the classroom instruction, and in that case the reason the online group came out looking better might be that they were stronger students: it could have nothing to do with the effectiveness of the learning environment at all.\n\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dandruff.html#introduction",
    "href": "dandruff.html#introduction",
    "title": "14  Treating dandruff",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nShampoos are often claimed to be effective at treating dandruff. In a study, the dandruff-treating properties of four shampoos were compared. These four shampoos were, as referred to in the dataset:\n\nPyrI: 1% pyrithione zinc shampoo\nPyrII: as PyrI but with instructions to shampoo two times at each wash.1\nKeto: 2% ketoconazole shampoo\nPlacebo: a placebo shampoo\n\nEach of the experimental subjects was randomly given one of the shampoos. After using their shampoo for six weeks, eight sections of the subject’s scalp were examined for each subject. Each section of the scalp was given a score that measured the amount of flaking on a scale of 0-10. The response variable, called Flaking, was the sum of these eight scores, and is a whole number for each subject. A smaller value of Flaking indicates less dandruff.2\nOur aim is to see which shampoo or shampoos are most effective at treating dandruff, that is, have the smallest value of Flaking on average."
  },
  {
    "objectID": "dandruff.html#exploratory-analysis",
    "href": "dandruff.html#exploratory-analysis",
    "title": "14  Treating dandruff",
    "section": "14.2 Exploratory analysis",
    "text": "14.2 Exploratory analysis\nWe begin by reading in the data:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/dandruff.txt\"\ndandruff &lt;- read_tsv(my_url)\n\nRows: 355 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Treatment\ndbl (3): OBS, GroupNum, Flaking\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndandruff\n\n\n\n  \n\n\n\n355 subjects took part in the study altogether. The shampoo used is indicated in the Treatment column. The remaining columns OBS and GroupNum will not be used in this analysis.\nNumerical summaries of the data are as shown:\n\ndandruff %&gt;% group_by(Treatment) %&gt;% \nsummarise(n=n(), fl_mean=mean(Flaking), fl_sd=sd(Flaking))\n\n\n\n  \n\n\n\nThere are approximately 100 observations in each group, apart from the Placebo group, which had only 28. The mean number of flakes is much higher for the Placebo group than for the others, which seem similar. The group standard deviations are fairly similar.\nWith a categorical Treatment and a quantitative Flakes, a suitable graph is a side-by-side boxplot:\n\nggplot(dandruff, aes(x=Treatment, y=Flaking)) + geom_boxplot() \n\n\n\n\nOnce again, we see that the flaking for the Placebo shampoo is much higher than for the others. There are outliers in the PyrI group, but given that the data values are all whole numbers, they are not far different from the rest of the data. Considering these outliers, the spreads of the groups all look fairly similar and the distributions appear more or less symmetric.3"
  },
  {
    "objectID": "dandruff.html#analysis-of-variance",
    "href": "dandruff.html#analysis-of-variance",
    "title": "14  Treating dandruff",
    "section": "14.3 Analysis of Variance",
    "text": "14.3 Analysis of Variance\nFor comparing four groups, we need some kind of analysis of variance. Having seen that the Flaking values within the four groups are more or less normal with more or less equal spreads, we run a standard ANOVA:\n\ndandruff.1 &lt;- aov(Flaking~Treatment, data=dandruff)\nsummary(dandruff.1)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTreatment     3   4151  1383.8   967.8 &lt;2e-16 ***\nResiduals   351    502     1.4                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWith an extremely small P-value, we conclude that the four shampoos do not all have the same mean value of Flaking.\nTo find out which ones are different from which, we use Tukey’s method:\n\nTukeyHSD(dandruff.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Flaking ~ Treatment, data = dandruff)\n\n$Treatment\n                     diff         lwr         upr     p adj\nPlacebo-Keto   13.3645553  12.7086918  14.0204187 0.0000000\nPyrI-Keto       1.3645553   0.9462828   1.7828278 0.0000000\nPyrII-Keto      1.1735330   0.7524710   1.5945950 0.0000000\nPyrI-Placebo  -12.0000000 -12.6521823 -11.3478177 0.0000000\nPyrII-Placebo -12.1910223 -12.8449971 -11.5370475 0.0000000\nPyrII-PyrI     -0.1910223  -0.6063270   0.2242825 0.6352706\n\n\nAll of the shampoo treatments are significantly different from each other except for the two pyrithione ones. To see which shampoos are best and worst, we remind ourselves of the treatment means:\n\ndandruff %&gt;% group_by(Treatment) %&gt;% \nsummarise(n=n(), fl_mean=mean(Flaking), fl_sd=sd(Flaking))\n\n\n\n  \n\n\n\nThe placebo shampoo has significantly more flaking than all the others, and the ketoconazole4 shampoo has significantly less flaking than all the others. From this analysis, therefore, we would recommend the ketoconazole shampoo over all the others."
  },
  {
    "objectID": "dandruff.html#assessment-of-assumptions",
    "href": "dandruff.html#assessment-of-assumptions",
    "title": "14  Treating dandruff",
    "section": "14.4 Assessment of Assumptions",
    "text": "14.4 Assessment of Assumptions\nThe analysis of variance done above requires that the observations within each treatment group (shampoo) be approximately normally distributed, given the sample sizes, with approximately equal spreads. To assess this, we look at normal quantile plots5 for each shampoo:6\n\nggplot(dandruff, aes(sample=Flaking)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~Treatment, scales = \"free\")\n\n\n\n\nGiven that the data are whole numbers, the distributions each appear close to the lines, indicating that the distributions are close to normal in shape.7 The distribution of the PyrI values is slightly long-tailed, but with over 100 observations in that group, this shape is not enough to invalidate the normality assumption.8\nHaving concluded that the normality is sufficient, we need to assess the equality of spreads. Referring back to our summary table:\n\ndandruff %&gt;% group_by(Treatment) %&gt;% \nsummarise(n=n(), fl_mean=mean(Flaking), fl_sd=sd(Flaking))\n\n\n\n  \n\n\n\nwe note that the spreads are not greatly different, and so the equal-spread assumption appears to be satisfied.9\nIn summary, the assumptions for the analysis of variance we did seem to be reasonably well satisfied, and we can have some confidence in our conclusions."
  },
  {
    "objectID": "dandruff.html#conclusions",
    "href": "dandruff.html#conclusions",
    "title": "14  Treating dandruff",
    "section": "14.5 Conclusions",
    "text": "14.5 Conclusions\nWe found that the ketoconazole shampoo produced the smallest mean flaking, and its mean was significantly smaller than that of all the other shampoos. This shampoo can be recommended over the others. There was no significant difference between the two pyrithione treatments; shampooing twice had no benefit over shampooing once.\nThe difference in means between the ketoconazole and the two pyrithione shampoos was only about 1.2. This difference was significant because of the large sample sizes, but it is a separate question as to whether a difference of this size is of practical importance. If it is not, any of the shampoos except for the placebo can be recommended."
  },
  {
    "objectID": "dandruff.html#end",
    "href": "dandruff.html#end",
    "title": "14  Treating dandruff",
    "section": "14.6 End",
    "text": "14.6 End\nThat is the end of my report. You, of course, don’t need the word “end” or any of the footnotes I had. These were to draw your attention to other things that don’t necessarily belong in the report, but I would like you to be aware of them. When you reach the end of your report, you can just stop.10\nSome extra, bigger, thoughts (there are quite a few of these. I hope I don’t repeat things I also say in the “sidenotes”):\nExtra 1: The placebo group is much smaller than the others, but all the groups are pretty big by ANOVA standards. Apparently what happened is that originally the three “real” treatments had 112 subjects each, but the placebo had 28 (ie., a quarter of the subjects that the other groups had), and a few subjects dropped out. There’s no problem, in one-way ANOVAs of this kind, with having groups of unequal sizes; the \\(F\\)-test is fine, and as long as you use a suitable extension of Tukey that deals with unequal sample sizes, you are OK there too. TukeyHSD, according to the help file, “incorporates an adjustment for sample size that produces sensible intervals for mildly unbalanced designs”. In this case, we might be holding our breath a bit, depending on what “mildly unbalanced” actually means. Usually in this kind of study, you have the groups about the same size, because proving that the smallest group differs from any of the others is more difficult. I guess these researchers were pretty confident that the placebo shampoo would be clearly worse than the others! (Additional: TukeyHSD uses the so-called Tukey-Kramer test when sample sizes within groups are unequal. My understanding is that this is good no matter what the sample sizes are.)\nExtra 2: The mean for Placebo is quite a lot bigger than for the other groups, so a plot with different scales for each facet is best. Otherwise you get this kind of thing, which is much harder to read:\n\nggplot(dandruff, aes(sample=Flaking)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~Treatment)\n\n\n\n\nThe points fill less than half their facets, which makes the plots harder to understand. This also makes it look as if the distributions are more normal, because the vertical scale has been compressed. Having a better basis for assessing the normality is a good idea, given that the purpose of the plot is assessing the normality! Hence, using scales = \"free\" is best.\nExtra 3: You might have been wondering why the boxplots, which are the usual thing in these circumstances, look worse than the normal quantile plots.\nLet’s revisit them and see what happened:\n\nggplot(dandruff, aes(x=Treatment, y=Flaking)) + geom_boxplot() \n\n\n\n\nThe Placebo group has the largest IQR, and the PyrI group appears to have two outliers. We need to bear in mind, though, that the data values are whole numbers and there might be repeats; also, what looks like an outlier here might not look quite so much like one when we see all the data.\nWhat you can do is to add a geom_point on to the plot to plot the observations as points:\n\nggplot(dandruff, aes(x=Treatment, y=Flaking)) + \ngeom_boxplot() + geom_point()\n\n\n\n\nBut there are a lot more data points than this! What happened is that a point at, say, 20 is all the observations in that group that were 20, of which there might be a lot, but we cannot see how many, because they are printed on top of each other. To see all the observations, we can jitter them: that is, plot them all not in the same place. In this case, we have the whole width of the boxplot boxes to use; we could also jitter vertically, but I decided not to do that here. There is a geom_jitter that does exactly this:11\n\nggplot(dandruff, aes(x=Treatment, y=Flaking)) + geom_boxplot() + \ngeom_jitter(height = 0)\n\n\n\n\nThe plot is rather messy,12 but now you see everything. The height=0 means not to do any vertical jittering: just spread the points left and right.13 Where the points are exactly on the \\(x\\)-scale is now irrelevant; this is just a device to spread the points out so that you can see them all.\nI left the vertical alone so that you can still see the actual data values. Even though the highest and lowest values in PyrI were shown as outliers on the original boxplot, you can see that they are really not. When the data values are discrete (separated) like this, an apparent outlier may be only one bigger or smaller than the next value, and thus not really an outlier at all.\nTo try the vertical jittering too, use the defaults on geom_jitter:\n\nggplot(dandruff, aes(x=Treatment, y=Flaking)) + geom_boxplot() + geom_jitter()\n\n\n\n\nThis maybe spreads the points out better, so you can be more certain that you’re seeing them all, but you lose the clear picture of the data values being whole numbers.\nWhen Tukey popularized the boxplot, his idea was that it would be drawn by hand with relatively small samples, and when you draw boxplots for large samples, you can get an apparently large number of outliers, that are not in retrospect quite as extreme as they may look at first. This may also have happened here. Tukey, however, did not invent the boxplot; credit for that goes to Mary Eleanor Spear with her “range plot”.14\nExtra 4: More discussion of normality. The assumptions for a standard ANOVA (that is to say, not a Welch ANOVA) are normally-distributed data within each treatment group, with equal spreads. What that means in practice is that you want normal enough data given the sample sizes, and approximately equal spreads. My normal quantile plots are a long way back, so let’s get them again:\n\nggplot(dandruff, aes(sample=Flaking)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~Treatment, scales = \"free\")\n\n\n\n\nThe data values are all whole numbers, so we get those horizontal stripes of Flaking values that are all the same. As long as these more or less hug the line, we are all right. The PyrII values certainly do. In my top row, Keto and Placebo are not quite so good, but they have short tails compared to the normal, so there will be no problem using the means for these groups, as ANOVA does. The only one that is problematic at all is PyrI. That has slightly long tails compared to a normal. (You could, I suppose, call those highest and lowest values “outliers”, but I don’t think they are far enough away from the rest of the data to justify that.) Are these long tails a problem? That depends on how many observations we have:\n\ndandruff %&gt;% group_by(Treatment) %&gt;% \nsummarise(n=n(), mean_flaking=mean(Flaking), sd_flaking=sd(Flaking))\n\n\n\n  \n\n\n\nThere are 112 of them. Easily enough to overcome those long tails. So, to my mind, normality is no problem.\nAside: you might be wondering whether you can make nicer-looking tables in your reports. There are several ways. The gt package is the most comprehensive one I know, and has links to a large number of others (at the bottom of its webpage). The simplest one I know of is kable in the knitr package. You may well have that package already installed, but you’ll need to load it, preferably at the beginning of your report:\n\nlibrary(knitr)\ndandruff %&gt;% group_by(Treatment) %&gt;% \nsummarise(n=n(), mean_flaking=mean(Flaking), sd_flaking=sd(Flaking)) -&gt; summary\nkable(summary)\n\n\n\n\nTreatment\nn\nmean_flaking\nsd_flaking\n\n\n\n\nKeto\n106\n16.02830\n0.9305149\n\n\nPlacebo\n28\n29.39286\n1.5948827\n\n\nPyrI\n112\n17.39286\n1.1418110\n\n\nPyrII\n109\n17.20183\n1.3524999\n\n\n\n\n\nEnd of aside.\nBefore I got distracted, we were talking about whether the distribution of PyrI was normal enough, given the sample size. Another way of thinking about this is to look at the bootstrapped sampling distribution of the sample mean for this group. I set the random number seed so that the results will be the same even if I run this again:\n\nset.seed(457299)\n\n\ndandruff %&gt;% filter(Treatment == \"PyrI\") -&gt; pyri\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(pyri$Flaking, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nOh yes, no problem with the normality there. (The discreteness of the population implies that each sample mean is some number of one-hundred-and-twelfths, so that the sampling distribution is also discrete, just less discrete than the data distribution. This is the reason for the little stair-steps in the plot.) In addition, the fact that the least normal distribution is normal enough means that the other distributions must also be OK. If you wanted to be careful, you would assess the smallest Placebo group as well, though that if anything is short-tailed and so would not be a problem anyway.\nThe other question is whether those spreads are equal enough. The easiest way is to look back at your summary table (that I reproduced above), cast your eye down the SD column, and make a call about whether they are equal enough. The large sample sizes don’t help here, although see the end of the question for more discussion. I would call these “not grossly unequal” and call standard ANOVA good, but you are also entitled to call them different enough, and then you need to say that in your opinion we should have done a Welch ANOVA. Or, if you got your normal quantile plots before you did your ANOVA, you could actually do a Welch ANOVA.\nI am not a fan of doing one test to see whether you can do another test,15 but if you really want to, you can use something like Levene’s test to test the null hypothesis that all the groups have the same variance.16 Levene’s test lives in the package car that you might have to install first:\n\nlibrary(car)\nleveneTest(Flaking~Treatment, data = dandruff)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\n\n\n  \n\n\n\nEqual variances are resoundingly rejected here; the samples here have variances that are less equal than they would be if the populations all had the same variances. But that is really asking the wrong question: the one that matters is “does the inequality of variances that we saw here matter when it comes to doing the ANOVA?”. With samples as big as we had, the variances could be declared unequal even if they were actually quite similar. This is another (different) angle on statistical significance (rather similar variances can be significantly different with large samples) vs. practical importance (does the fact that our sample variances are as different as they are matter to the ANOVA?). I do the Welch ANOVA in Extra 6, and you will see there whether it comes out much different than the regular ANOVA. See also Extra 7.\nIf your normal quantile plots looked like this:\n\nggplot(dandruff, aes(sample=Flaking)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~Treatment)\n\n\n\n\nwith the same scales, you can use the slopes of the lines to judge equal spreads: either equal enough, or the Placebo line is a bit steeper than the others. If you did scales = \"free\", you cannot do this, because you have essentially standardized your data before making the normal quantile plots.\nIt is hugely important to distinguish the null hypothesis (all the means are the same) from the assumptions behind the test (how you know that the P-value obtained from testing your null hypothesis can be trusted). These are separate things, and getting them straight is a vital part of being a good statistician. You might say that this is part of somebody else knowing how they, as someone hiring a statistician, can trust you.\nExtra 5: Several ways to say what you conclude from the ANOVA:\n\nThe null hypothesis, which says that all the shampoos have the same mean amount of flaking, is rejected. (Or say it in two sentences: what the null hypothesis is, and then what you’re doing with it.)\nNot all the shampoos have the same mean amount of flaking.\nThere are shampoos that differ in mean amount of flaking.\n\nSome wrong or incomplete ways to say it:\n\nWe reject the null hypothesis. (Meaning what, about the data?)\nwe reject the null hypothesis that the means are different. (You have confused the null with the conclusion, and come out with something that is backwards.)\nthe mean flaking for the treatments is different (this says that they are all different, but you don’t know that yet.)\n\nExtra 6: You might have been wondering how Welch’s ANOVA would have played out, given that the placebo group measurements looked more variable than the others. Wonder no more:\n\noneway.test(Flaking~Treatment, data=dandruff)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  Flaking and Treatment\nF = 595.03, num df = 3.00, denom df = 105.91, p-value &lt; 2.2e-16\n\ngamesHowellTest(Flaking~factor(Treatment), data=dandruff)\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: Flaking by factor(Treatment)\n\n\n        Keto    Placebo PyrI\nPlacebo 9.2e-14 -       -   \nPyrI    8.7e-14 &lt; 2e-16 -   \nPyrII   2.1e-11 &lt; 2e-16 0.67\n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nThe results are almost exactly the same: the conclusions are identical, and the P-values are even pretty much the same. The place where it would make a difference is when you are close to the boundary between rejecting and not. Here, our Tukey and Games-Howell P-values were all either close to 0 or about 0.6, whichever way we did it. So it didn’t matter which one we did; you could justify using either. The regular ANOVA might have been a better choice for your report, though, because this is something your audience could reasonably be expected to have heard of. The Welch ANOVA deserves to be as well-used as the Welch two-sample \\(t\\)-test, but it doesn’t often appear in Statistics courses. (This course is an exception, of course!)\nExtra 7: the general principle when you are not sure of the choice between two tests is to run them both. If the conclusions agree, as they do here, then it doesn’t matter which one you run. If they disagree, then it matters, and you need to think more carefully about which test is the more appropriate one. (Usually, this is the test with the fewer assumptions, but not always.)\nAnother way to go is to do a simulation (of the ordinary ANOVA). Generate some data that are like what you actually have, and then in your simulation see whether your \\(\\alpha\\) is near to 0.05. Since we are talking about \\(\\alpha\\) here, the simulated data needs to have the same mean in every group, so that the null hypothesis is true, but SDs and sample sizes like the ones in the data (and of a normal shape). Let me build up the process. Let’s start by making a dataframe that contains the sample sizes, means and SDs for the data we want to generate. The treatment names don’t matter:\n\nsim_from &lt;- tribble(\n~trt, ~n, ~mean, ~sd,\n\"A\", 106, 0, 0.93,\n\"B\", 28, 0, 1.59,\n\"C\", 112, 0, 1.14,\n\"D\", 109, 0, 1.35\n)\nsim_from\n\n\n\n  \n\n\n\nStarting from here, we want to set up drawing a lot of random samples from a normal distribution with the mean and SD shown, and the sample size shown. The way I like to do it17 is to set up a list-column called sim that will index the simulations. I’m going to pretend I’m doing just three simulations, while I get my head around this, and then up it later after I have things working:\n\nsim_from %&gt;% \n  mutate(sim = list(1:3))\n\n\n\n  \n\n\n\nThen I unnest sim so that I have a place to draw each sample:\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim)\n\n\n\n  \n\n\n\nNow, working rowwise, I can draw a random sample from each normal distribution. The inputs to rnorm are the sample size, the mean, and the SD, in that order.18\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd)))\n\n\n\n  \n\n\n\nThis is looking similar in procedure to a bootstrap sampling distribution. If we were doing that, we would now make a new column containing something like the mean of each of those samples (and then make a picture of what we had). But this is different: we want to combine all of the random samples for each of the four treatments for one of the simulations, run an ANOVA on it, and get hold of the P-value. So we need to unnest those samples, and then combine them together properly. That goes something like this:\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim)\n\n\n\n  \n\n\n\nWhat this has done is to create three mini-dataframes in the list-column data that have our generated random y and a column called treatment. What we want to do is to run the ordinary ANOVA on each of those dataframes in data, and, in a minute, get hold of the P-value. I think I need another rowwise first, because I want to work with the rows of the new dataframe:\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_anova = list(aov(y ~ trt, data = data)))\n\n\n\n  \n\n\n\nI know the P-value is in there somewhere, but I can’t remember how to get hold of it. The easiest way is to load broom and pass the models into tidy, then take a look at that:\n\nlibrary(broom)\n\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_anova = list(aov(y ~ trt, data = data))) %&gt;% \n  mutate(my_tidy = list(tidy(my_anova))) %&gt;% \n  unnest(my_tidy)\n\n\n\n  \n\n\n\nAlmost there. The rows that have P-values in them are the ones that have trt (the explanatory variable) in the term, so:\n\nsim_from %&gt;% \n  mutate(sim = list(1:3)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_anova = list(aov(y ~ trt, data = data))) %&gt;% \n  mutate(my_tidy = list(tidy(my_anova))) %&gt;% \n  unnest(my_tidy) %&gt;% \n  filter(term == \"trt\") %&gt;% \n  select(sim, p.value)\n\n\n\n  \n\n\n\nThree simulated samples, each time one from each of the four treatments, and three P-values. So this works, and the remaining thing is to change the number of simulations from 3 to 1000 and run it again, saving the result:\n\nsim_from %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_anova = list(aov(y ~ trt, data = data))) %&gt;% \n  mutate(my_tidy = list(tidy(my_anova))) %&gt;% \n  unnest(my_tidy) %&gt;% \n  filter(term == \"trt\") %&gt;% \n  select(sim, p.value) -&gt; sim_pval\nsim_pval\n\n\n\n  \n\n\n\nNow, the reason we were doing this was to see whether regular ANOVA worked properly on data from populations with different SDs. We know that the null hypothesis is true here (because all the true treatment means were equal to 0), so the probability of making a type I error by rejecting the null (that all the means are the same) should be 0.05. How close is it?\n\nsim_pval %&gt;% \n  count(p.value &lt;= 0.05)\n\n\n\n  \n\n\n\n\\(91/1000 = 0.091\\). We are too likely to falsely reject the null. The regular ANOVA does not behave properly for data like ours.\nThat looks rather high. Is the proportion of times I am rejecting significantly different from 0.05? Testing null hypotheses about (single) proportions is done using prop.test. This uses the normal approximation to the binomial, with continuity correction:\n\nprop.test(91, 1000, p = 0.05)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  91 out of 1000, null probability 0.05\nX-squared = 34.532, df = 1, p-value = 4.194e-09\nalternative hypothesis: true p is not equal to 0.05\n95 percent confidence interval:\n 0.07425035 0.11096724\nsample estimates:\n    p \n0.091 \n\n\nAh, now, that’s interesting. A supposed \\(\\alpha = 0.05\\) test is actually rejecting around 9% of the time, which is significantly different from 0.05. This surprises me. So the ANOVA is actually not all that accurate.19\nSo now let’s do the same simulation for the Welch ANOVA to see whether it’s better:\n\nsim_from %&gt;% \n  mutate(sim = list(1:1000)) %&gt;% \n  unnest(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(y = list(rnorm(n, mean, sd))) %&gt;% \n  unnest(y) %&gt;% \n  nest_by(sim) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_anova = list(oneway.test(y ~ trt, data = data))) %&gt;%\n  mutate(pval = my_anova$p.value) -&gt; sim_pval2\nsim_pval2\n\n\n\n  \n\n\n\nThis one is a bit easier because oneway.test has a thing called p.value that you can just pull out. No need to use tidy here.\nHow many of those P-values are less than 0.05?\n\nsim_pval2 %&gt;% \n  count(pval &lt;= 0.05)\n\n\n\n  \n\n\n\nThat couldn’t be much closer to the mark:\n\nprop.test(49, 1000, p = 0.05)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  49 out of 1000, null probability 0.05\nX-squared = 0.0052632, df = 1, p-value = 0.9422\nalternative hypothesis: true p is not equal to 0.05\n95 percent confidence interval:\n 0.03682698 0.06475244\nsample estimates:\n    p \n0.049 \n\n\nThis one is on the money.20 The proportion of times our simulation falsely rejects is not significantly different from 0.05. So this investigation says that the Welch ANOVA is much more trustworthy for data resembling what we observed.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dandruff.html#footnotes",
    "href": "dandruff.html#footnotes",
    "title": "14  Treating dandruff",
    "section": "",
    "text": "The piece in the problem statement about why these two labels were used is clarification for you and doesn’t belong in the report. If you leave it in, you need to at least paraphrase it; simply copying it without having a reason to do so shows that you are not thinking.↩︎\nI’m using a fair few of my own words from the question. This is OK if you think they are clear, but the aim is to write a report that sounds like you rather than me.↩︎\nOffer supported opinions of your own here, which don’t need to be the same as mine. Alternatively, you can get the graph and numerical summaries first and comment on them both at once.↩︎\nUse the full name of the shampoo if you are making a conclusion about it.↩︎\nI’ve used scales = \"free\" to get the plots to fill their boxes, for the best assessment of normality. The downside of doing it this way is that you cannot use the slopes of the lines to compare spreads. I think this way is still better, though, because the mean for placebo is so much bigger than the others that if you use the same scale for each plot, you’ll be wasting a lot of plot real estate that you could use to get a better picture of the normality.↩︎\nIt’s also good, arguably clearer, to use this as your exploratory plot. This enables you to get to a discussion about normality earlier and you might decide in that case that you don’t even need this discussion. You can do the assessment of assumptions first, and then do the corresponding analysis, or you can pick an apparently reasonable analysis and then critique it afterwards. Either way is logical here. In other cases it might be different; for example, in a regression, you might need to fit a model first and improve it after, since it may not be so clear what a good model might be off the top.↩︎\nThis is a way to write it if you suspect your reader won’t remember what a normal quantile plot is, and by writing it this way you won’t insult their intelligence if they do remember after all. The other side benefit of writing it this way is that it shows your understanding as well.↩︎\nIf you have any doubts about sufficient normality, you need to make sure you have also considered the relevant sample size, but if you are already happy with the normality, there is no need. The placebo group, for example, is the smallest, but its shape is if anything short-tailed, so its non-normality will be no problem no matter how small the sample is.↩︎\nI’d rather assess equality of spreads by eyeballing them than by doing a test, but if you really want to, you could use Levene’s test, illustrated elsewhere in PASIAS and in Extra 4 for these data. It works for any number of groups, not just two.↩︎\nI wanted to make it clear where my report ended and where the additional chat began.↩︎\nI explain the height=0 below the plot.↩︎\nIt looks to me as if the boxplot has been attacked by mosquitoes.↩︎\nThe default jittering is up to a maximum of not quite halfway to the next value. Here that means that each observation is nearest to the box it belongs with.↩︎\nI learned this today.↩︎\nBecause the true alpha for the combined procedure in which the test you do second depends on the result of the first test is no longer 0.05; you need to think about what that true alpha is. It might not be too bad here, because regular ANOVA and Welch ANOVA tend to come out similar unless the sample variances are very different, in the same way that the Welch and pooled two-sample tests do. But it is not something to take for granted.↩︎\nThere are other tests you could use here. I like Levene’s test because it works best when the samples are not normal, but the normality is OK here, so this is not so much of an issue. Of course, the time you want to be assessing equality of variances is when you have already seen sufficient normality, but that might have been because the samples were large rather than that they were especially normal in shape themselves.↩︎\nAt this moment, subject to change, etc etc.↩︎\nOr you can remember or look up the names to the inputs, and then you can have them in any order you like. But I know which order they come, which is good enough for me.↩︎\nWhen you get a simulation result that is not what you were expecting, there are two options to explore: either it really is different from your expectation, or there is something wrong with your code. I think my code is OK here, but do let me know if you see a problem with it.↩︎\nYou could redo this with 10,000 simulations to convince yourself further.↩︎"
  },
  {
    "objectID": "tidying-data.html#baseball-and-softball-spaghetti",
    "href": "tidying-data.html#baseball-and-softball-spaghetti",
    "title": "15  Tidying data",
    "section": "15.1 Baseball and softball spaghetti",
    "text": "15.1 Baseball and softball spaghetti\nOn a previous assignment, we found that students could throw a baseball further than they could throw a softball. In this question, we will make a graph called a “spaghetti plot” to illustrate this graphically. (The issue previously was that the data were matched pairs: the same students threw both balls.)\nThis seems to work most naturally by building a pipe, a line or two at a time. See if you can do it that way. (If you can’t make it work, use lots of temporary data frames, one to hold the result of each part.)\n\nRead in the data again from link. The variables had no names, so supply some, as you did before.\nCreate a new column that is the students turned into a factor, adding it to your data frame. The reason for this will become clear later.\nCollect together all the throwing distances into one column, making a second column that says which ball was thrown.\nUsing your new data frame, make a “scatterplot” of throwing distance against type of ball.\nAdd two things to your plot: something that will distinguish the students by colour (this works best if the thing distinguished by colour is a factor),1 and something that will join the two points for the same student by a line.\nThe legend is not very informative. Remove it from the plot, using guides.\nWhat do you see on the final spaghetti plot? What does that tell you about the relative distances a student can throw a baseball vs. a softball? Explain briefly, blah blah blah."
  },
  {
    "objectID": "tidying-data.html#ethanol-and-sleep-time-in-rats",
    "href": "tidying-data.html#ethanol-and-sleep-time-in-rats",
    "title": "15  Tidying data",
    "section": "15.2 Ethanol and sleep time in rats",
    "text": "15.2 Ethanol and sleep time in rats\nA biologist wished to study the effects of ethanol on sleep time in rats. A sample of 20 rats (all the same age) was selected, and each rat was given an injection having a particular concentration (0, 1, 2 or 4 grams per kilogram of body weight) of ethanol. These are labelled e0, e1, e2, e4. The “0” treatment was a control group. The rapid eye movement (REM) sleep time was then recorded for each rat. The data are in link.\n\nRead the data in from the file. Check that you have four rows of observations and five columns of sleep times.\nUnfortunately, the data are in the wrong format. All the sleep times for each treatment group are on one row, and we should have one column containing all the sleep times, and the corresponding row should show which treatment group that sleep time came from. Transform this data frame into one that you could use for modelling or making graphs.\nUsing your new data frame, make side-by-side boxplots of sleep time by treatment group.\nIn your boxplots, how does the median sleep time appear to depend on treatment group?\nThere is an assumption about spread that the analysis of variance needs in order to be reliable. Do your boxplots indicate that this assumption is satisfied for these data, bearing in mind that you have only five observations per group?\nRun an analysis of variance to see whether sleep time differs significantly among treatment groups. What do you conclude?\nWould it be a good idea to run Tukey’s method here? Explain briefly why or why not, and if you think it would be a good idea, run it.\nWhat do you conclude from Tukey’s method? (This is liable to be a bit complicated.) Is there a treatment that is clearly best, in terms of the sleep time being largest?"
  },
  {
    "objectID": "tidying-data.html#growth-of-tomatoes",
    "href": "tidying-data.html#growth-of-tomatoes",
    "title": "15  Tidying data",
    "section": "15.3 Growth of tomatoes",
    "text": "15.3 Growth of tomatoes\nA biology graduate student exposed each of 32 tomato plants to one of four different colours of light (8 plants to each colour). The growth rate of each plant, in millimetres per week, was recorded. The data are in link.\n\nRead the data into R and confirm that you have 8 rows and 5 columns of data.\nRe-arrange the data so that you have one column containing all the growth rates, and another column saying which colour light each plant was exposed to. (The aim here is to produce something suitable for feeding into aov later.)\nSave the data in the new format to a text file. This is most easily done using write_csv, which is the opposite of read_csv. It requires two things: a data frame, and the name of a file to save in, which should have a .csv extension.\nMake a suitable boxplot, and use it to assess the assumptions for ANOVA. What do you conclude? Explain briefly.\nRun (regular) ANOVA on these data. What do you conclude? (Optional extra: if you think that some other variant of ANOVA would be better, run that as well and compare the results.)\nIf warranted, run a suitable follow-up. (If not warranted, explain briefly why not.)"
  },
  {
    "objectID": "tidying-data.html#pain-relief-in-migraine-headaches-again",
    "href": "tidying-data.html#pain-relief-in-migraine-headaches-again",
    "title": "15  Tidying data",
    "section": "15.4 Pain relief in migraine headaches (again)",
    "text": "15.4 Pain relief in migraine headaches (again)\nThe data in link are from a study of pain relief in migraine headaches. Specifically, 27 subjects were randomly assigned to receive one of three pain relieving drugs, labelled A, B and C. Each subject reported the number of hours of pain relief they obtained (that is, the number of hours between taking the drug and the migraine symptoms returning). A higher value is therefore better. Can we make some recommendation about which drug is best for the population of migraine sufferers?\n\nRead in and display the data. Take a look at the data file first, and see if you can say why read_table will work and read_delim will not.\nWhat is it about the experimental design that makes a one-way analysis of variance plausible for data like this?\nWhat is wrong with the current format of the data as far as doing a one-way ANOVA analysis is concerned? (This is related to the idea of whether or not the data are “tidy”.)\n“Tidy” the data to produce a data frame suitable for your analysis.\nGo ahead and run your one-way ANOVA (and Tukey if necessary). Assume for this that the pain relief hours in each group are sufficiently close to normally distributed with sufficiently equal spreads.\nWhat recommendation would you make about the best drug or drugs? Explain briefly."
  },
  {
    "objectID": "tidying-data.html#location-species-and-disease-in-plants",
    "href": "tidying-data.html#location-species-and-disease-in-plants",
    "title": "15  Tidying data",
    "section": "15.5 Location, species and disease in plants",
    "text": "15.5 Location, species and disease in plants\nThe table below is a “contingency table”, showing frequencies of diseased and undiseased plants of two different species in two different locations:\n\nSpecies     Disease present         Disease absent\nLocation X Location Y  Location X Location Y\nA            44         12          38        10\nB            28         22          20        18\n\nThe data were saved as link. In that file, the columns are coded by two letters: a p or an a to denote presence or absence of disease, and an x or a y to denote location X or Y. The data are separated by multiple spaces and aligned with the variable names.\n\nRead in and display the data.\nExplain briefly how these data are not “tidy”.\nUse a suitable tidyr tool to get all the things that are the same into a single column. (You’ll need to make up a temporary name for the other new column that you create.) Show your result.\nExplain briefly how the data frame you just created is still not “tidy” yet.\nUse one more tidyr tool to make these data tidy, and show your result.\nLet’s see if we can re-construct the original contingency table (or something equivalent to it). Use the function xtabs. This requires first a model formula with the frequency variable on the left of the squiggle, and the other variables separated by plus signs on the right. Second it requires a data frame, with data=. Feed your data frame from the previous part into xtabs. Save the result in a variable and display the result.\nTake the output from the last part and feed it into the function ftable. How has the output been changed? Which do you like better? Explain briefly."
  },
  {
    "objectID": "tidying-data.html#mating-songs-in-crickets",
    "href": "tidying-data.html#mating-songs-in-crickets",
    "title": "15  Tidying data",
    "section": "15.6 Mating songs in crickets",
    "text": "15.6 Mating songs in crickets\nMale tree crickets produce “mating songs” by rubbing their wings together to produce a chirping sound. It is hypothesized that female tree crickets identify males of the correct species by how fast (in chirps per second) the male’s mating song is. This is called the “pulse rate”. Some data for two species of crickets are in link. The columns, which are unlabelled, are temperature and pulse rate (respectively) for Oecanthus exclamationis (first two columns) and Oecanthus niveus (third and fourth columns). The columns are separated by tabs. There are some missing values in the first two columns because fewer exclamationis crickets than niveus crickets were measured. The research question is whether males of the different species have different average pulse rates. It is also of interest to see whether temperature has an effect, and if so, what. Before we get to that, however, we have some data organization to do.\n\nRead in the data, allowing for the fact that you have no column names. You’ll see that the columns have names X1 through X4. This is OK.\nTidy these untidy data, going as directly as you can to something tidy. (Some later parts show you how it used to be done.) Begin by: (i) adding a column of row numbers, (ii) rename-ing the columns to species name, an underscore, and the variable contents (keeping pulserate as one word), and then use pivot_longer. Note that the column names encode two things.\nIf you found (b) a bit much to take in, the rest of the way we take a rather more leisurely approach towards the tidying.\n\nThese data are rather far from being tidy. There need to be three variables, temperature, pulse rate and species, and there are \\(14+17=31\\) observations altogether. This one is tricky in that there are temperature and pulse rate for each of two levels of a factor, so I’ll suggest combining the temperature and chirp rate together into one thing for each species, then pivoting them longer (“combining”), then pivoting them wider again (“splitting”). Create new columns, named for each species, that contain the temperature and pulse rate for that species in that order, united together. For the rest of this question, start from the data frame you read in, and build a pipe, one or two steps at a time, to save creating a lot of temporary data frames.\n\nThe two columns exclamationis and niveus that you just created are both temperature-pulse rate combos, but for different species. Collect them together into one column, labelled by species. (This is a straight tidyr pivot_longer, even though the columns contain something odd-looking.)\nNow split up the temperature-pulse combos at the underscore, into two separate columns. This is separate. When specifying what to separate by, you can use a number (“split after this many characters”) or a piece of text, in quotes (“when you see this text, split at it”).\nAlmost there. Temperature and pulse rate are still text (because unite turned them into text), but they should be numbers. Create new variables that are numerical versions of temperature and pulse rate (using as.numeric). Check that you have no extraneous variables (and, if necessary, get rid of the ones you don’t want). (Species is also text and really ought to be a factor, but having it as text doesn’t seem to cause any problems.) You can, if you like, use parse_number instead of as.numeric. They should both work. The distinction I prefer to make is that parse_number is good for text with a number in it (that we want to pull the number out of), while as.numeric is for turning something that looks like a number but isn’t one into a genuine number.2"
  },
  {
    "objectID": "tidying-data.html#number-1-songs",
    "href": "tidying-data.html#number-1-songs",
    "title": "15  Tidying data",
    "section": "15.7 Number 1 songs",
    "text": "15.7 Number 1 songs\nThe data file link contains a lot of information about songs popular in 2000. This dataset is untidy. Our ultimate aim is to answer “which song occupied the #1 position for the largest number of weeks?”. To do that, we will build a pipe that starts from the data frame read in from the URL above, and finishes with an answer to the question. I will take you through this step by step. Each part will involve adding something to the pipe you built previously (possibly after removing a line or two that you used to display the previous result).\n\nRead the data and display what you have.\nThe columns x1st.week through x76th.week contain the rank of each song in the Billboard chart in that week, with week 1 being the first week that the song appeared in the chart. Convert all these columns into two: an indication of week, called week, and of rank, called rank. Most songs appeared in the Billboard chart for a lot less than 76 weeks, so there are missing values, which you want to remove. (I say “indication of week” since this will probably be text at the moment). Display your new data frame. Do you have fewer columns? Why do you have a lot more rows? Explain briefly.\nBoth your week and rank columns are (probably) text. Create new columns that contain just the numeric values, and display just your new columns, again adding onto the end of your pipe. If it so happens that rank is already a number, leave it as it is.\nThe meaning of your week-number column is that it refers to the number of weeks after the song first appeared in the Billboard chart. That is, if a song’s first appearance (in date.entered) is July 24, then week 1 is July 24, week 2 is July 31, week 3 is August 7, and so on. Create a column current by adding the appropriate number of days, based on your week number, to date.entered. Display date.entered, your week number, and current to show that you have calculated the right thing. Note that you can add a number of days onto a date and you will get another date.\nReaching the #1 rank on the Billboard chart is one of the highest accolades in the popular music world. List all the songs that reached rank 1. For these songs, list the artist (as given in the data set), the song title, and the date(s) for which the song was ranked number 1. Arrange the songs in date order of being ranked #1. Display all the songs (I found 55 of them).\nUse R to find out which song held the #1 rank for the largest number of weeks. For this, you can assume that the song titles are all unique (if it’s the same song title, it’s the same song), but the artists might not be (for example, Madonna might have had two different songs reach the #1 rank). The information you need is in the output you obtained for the previous part, so it’s a matter of adding some code to the end of that. The last mark was for displaying only the song that was ranked #1 for the largest number of weeks, or for otherwise making it easy to see which song it was."
  },
  {
    "objectID": "tidying-data.html#bikes-on-college",
    "href": "tidying-data.html#bikes-on-college",
    "title": "15  Tidying data",
    "section": "15.8 Bikes on College",
    "text": "15.8 Bikes on College\nThe City of Toronto collects all kinds of data on aspects of life in the city. See link. One collection of data is records of the number of cyclists on certain downtown streets. The data in link are a record of the cyclists on College Street on the block west from Huron to Spadina on September 24, 2010. In the spreadsheet, each row relates to one cyclist. The first column is the time the cyclist was observed (to the nearest 15 minutes). After that, there are four pairs of columns. The observer filled in (exactly) one X in each pair of columns, according to whether (i) the cyclist was male or female, (ii) was or was not wearing a helmet, (iii) was or was not carrying a passenger on the bike, (iv) was or was not riding on the sidewalk. We want to create a tidy data frame that has the time in each row, and has columns containing appropriate values, often TRUE or FALSE, for each of the four variables measured.\nI will lead you through the process, which will involve developing a (long) pipe, one step at a time.\n\nTake a look at the spreadsheet (using Excel or similar: this may open when you click the link). Are there any obvious header rows? Is there any extra material before the data start? Explain briefly.\nRead the data into an R data frame. Read without headers, and instruct R how many lines to skip over using skip= and a suitable number. When this is working, display the first few lines of your data frame. Note that your columns have names X1 through X9.\nWhat do you notice about the times in your first column? What do you think those “missing” times should be?\nFind something from the tidyverse that will fill3 in those missing values with the right thing. Start a pipe from the data frame you read in, that updates the appropriate column with the filled-in times.\nR’s ifelse function works like =IF in Excel. You use it to create values for a new variable, for example in a mutate. The first input to it is a logical condition (something that is either true or false); the second is the value your new variable should take if the condition is true, and the third is the value of your new variable if the condition is false. Create a new column gender in your data frame that is “male” or “female” depending on the value of your X2 column, using mutate. (You can assume that exactly one of the second and third columns has an X in it.) Add your code to the end of your pipe and display (the first 10 rows of) the result.\nCreate variables helmet, passenger and sidewalk in your data frame that are TRUE if the “Yes” column contains X and FALSE otherwise. This will use mutate again, but you don’t need ifelse: just set the variable equal to the appropriate logical condition. As before, the best way to create these variables is to test the appropriate things for missingness. Note that you can create as many new variables as you like in one mutate. Show the first few lines of your new data frame. (Add your code onto the end of the pipe you made above.)\nFinally (for the data manipulation), get rid of all the original columns, keeping only the new ones that you created. Save the results in a data frame and display its first few rows.\nThe next few parts are a quick-fire analysis of the data set. They can all be solved using count. How many male and how many female cyclists were observed in total?\nHow many male and female cyclists were not wearing helmets?\nHow many cyclists were riding on the sidewalk and carrying a passenger?\nWhat was the busiest 15-minute period of the day, and how many cyclists were there then?"
  },
  {
    "objectID": "tidying-data.html#feeling-the-heat",
    "href": "tidying-data.html#feeling-the-heat",
    "title": "15  Tidying data",
    "section": "15.9 Feeling the heat",
    "text": "15.9 Feeling the heat\nIn summer, the city of Toronto issues Heat Alerts for “high heat or humidity that is expected to last two or more days”. The precise definitions are shown at link. During a heat alert, the city opens Cooling Centres and may extend the hours of operation of city swimming pools, among other things. All the heat alert days from 2001 to 2016 are listed at link.\nThe word “warning” is sometimes used in place of “alert” in these data. They mean the same thing.4\n\nRead the data into R, and display the data frame. Note that there are four columns:\n\n\na numerical id (numbered upwards from the first Heat Alert in 2001; some of the numbers are missing)\nthe date of the heat alert, in year-month-day format with 4-digit years.\na text code for the type of heat alert\ntext describing the kind of heat alert. This can be quite long.\n\n\nIn your data frame, are the dates stored as genuine dates or as text? How can you tell?\nWhich different heat alert codes do you have, and how many of each?\nUse the text in your dataset (or look back at the original data file) to describe briefly in your own words what the various codes represent.\nHow many (regular and extended) heat alert events are there altogether? A heat alert event is a stretch of consecutive days, on all of which there is a heat alert or extended heat alert. Hints: (i) you can answer this from output you already have; (ii) how can you tell when a heat alert event starts?\nWe are going to investigate how many heat alert days there were in each year. To do that, we have to extract the year from each of our dates.\nCount the number of heat alert days for each year, by tabulating the year variable. Looking at this table, would you say that there have been more heat alert days in recent years? Explain (very) briefly."
  },
  {
    "objectID": "tidying-data.html#isoflavones",
    "href": "tidying-data.html#isoflavones",
    "title": "15  Tidying data",
    "section": "15.10 Isoflavones",
    "text": "15.10 Isoflavones\nThe plant called kudzu was imported to the US South from Japan. It is rich in isoflavones, which are believed to be beneficial for bones. In a study, rats were randomly assigned to one of three diets: one with a low dose of isoflavones from kudzu, one with a high dose, and a control diet with no extra isoflavones. At the end of the study, each rat’s bone density was measured, in milligrams per square centimetre. The data as recorded are shown in http://ritsokiguess.site/isoflavones.txt.5 There are 15 observations for each treatment, and hence 45 altogether.\nHere are some code ideas you might need to use later, all part of the tidyverse. You may need to find out how they work.\n\ncol_names (in the read_ functions)\nconvert (in various tidyverse functions)\nfill\nna_if\nrename\nseparate_rows\nskip (in the read_ functions)\nvalues_drop_na (in the pivot_ functions)\n\nIf you use any of these, cite the webpage(s) or other source(s) where you learned about them.\n\nTake a look at the data file. Describe briefly what you see.\nRead in the data, using read_table, and get it into a tidy form, suitable for making a graph. This means finishing with (at least) a column of treatments with a suitable name (the treatments will be text) and a column of bone density values (numbers), one for each rat. You can have other columns as well; there is no obligation to get rid of them. Describe your process clearly enough that someone new to this data set would be able to understand what you have done and reproduce it on another similar dataset. Before you begin, think about whether or not you want to keep the column headers that are in the data file or not. (It can be done either way, but one way is easier than the other.)\nThe statistician on this study is thinking about running an ordinary analysis of variance to compare the bone mineral density for the different treatments. Obtain a plot from your tidy dataframe that will help her decide whether that is a good idea.\nBased on your graph, and any additional graphs you wish to draw, what analysis would you recommend for this dataset? Explain briefly. (Don’t do the analysis.)"
  },
  {
    "objectID": "tidying-data.html#jockos-garage",
    "href": "tidying-data.html#jockos-garage",
    "title": "15  Tidying data",
    "section": "15.11 Jocko’s Garage",
    "text": "15.11 Jocko’s Garage\nInsurance adjusters are concerned that Jocko’s Garage is giving estimates for repairing car damage that are too high. To see whether this is indeed the case, ten cars that had been in collisions were taken to both Jocko’s Garage and another garage, and the two estimates for repair were recorded. The data as recorded are here.\n\nTake a look at the data file (eg. by using your web browser). How are the data laid out? Do there appear to be column headers?\nRead in and display the data file, bearing in mind what you just concluded about it. What names did the columns acquire?\nMake this data set tidy. That is, you need to end up with columns containing the repair cost estimates at each of the two garages and also identifying the cars, with each observation on one row. Describe your thought process. (It needs to be possible for the reader to follow your description and understand why it works.) Save your tidy dataframe.\nMake a suitable graph to assess the comparison of interest, and say briefly what your graph tells you.\nCarry out a test to make an appropriate comparison of the mean estimates. What do you conclude, in the context of the data?"
  },
  {
    "objectID": "tidying-data.html#tidying-electricity-consumption",
    "href": "tidying-data.html#tidying-electricity-consumption",
    "title": "15  Tidying data",
    "section": "15.12 Tidying electricity consumption",
    "text": "15.12 Tidying electricity consumption\nHow does the consumption of electricity depend on temperature? To find out, a short-term study was carried out by a utility company based in a certain area. For a period of two years, the average monthly temperature was recorded (in degrees Fahrenheit), the mean daily demand for electricity per household (in kilowatt hours), and the cost per kilowatt hour of electricity for that year (8 cents for the first year and 10 cents for the second, which it will be easiest to treat as categorical).\nThe data were laid out in an odd way, as shown in http://ritsokiguess.site/datafiles/utils.txt, in aligned columns: the twelve months of temperature were laid out on two lines for the first year, then the twelve months of consumption for the first year on the next two lines, and then four more lines for the second year laid out the same way. Thus the temperature of 31 in the first line goes with the consumption of 55 in the third line, and the last measurements for that year are the 78 at the end of the second line (temperature) and 73 at the end of the fourth line (consumption). Lines 5 through 8 of the data file are the same thing for the second year (when electricity was more expensive).\nThe data seem to have been laid out in order of temperature, rather than in order of months, which I would have thought would make more sense. But this is what we have.\n\nRead in and display the data file, bearing in mind that it has no column names.\nArrange these data tidily, so that there is a column of price (per kilowatt hour), a column of temperatures, and a column of consumptions. Describe your process, including why you got list-columns (if you did) and what you did about them (if necessary).\nMake a suitable graph of temperature, consumption and price in your tidy dataframe. Add smooth trends if appropriate. If you were unable to get the data tidy, use my tidy version here. (If you need the other file, right-click on “here” and Copy Link Address.)\nWhat patterns or trends do you see on your graph? Do they make practical sense? There are two things I would like you to comment on."
  },
  {
    "objectID": "tidying-data.html#tidy-blood-pressure",
    "href": "tidying-data.html#tidy-blood-pressure",
    "title": "15  Tidying data",
    "section": "15.13 Tidy blood pressure",
    "text": "15.13 Tidy blood pressure\nGoing to the dentist is scary for a lot of people. One way in which this might show up is that people might have higher blood pressure on average before their dentist’s appointment than an hour after the appointment is done. Ten randomly-chosen individuals have their (systolic6) blood pressure measured while they are in a dentist’s waiting room, and then again one hour after their appointment is finished.\nYou might have seen a tidy version of this data set before.\nThe data as I originally received it is in http://ritsokiguess.site/datafiles/blood_pressure2.csv.\n\nRead in and display the data as originally received.\nDescribe briefly how the data you read in is not tidy, bearing in mind how the data were collected and how they would be analysed.\nProduce a tidy dataframe from the one you read in from the file. (How many rows should you have?)\nWhat kind of test might you run on these data? Explain briefly.\nDraw a suitable graph of these data.\n\nMy solutions follow:"
  },
  {
    "objectID": "tidying-data.html#baseball-and-softball-spaghetti-1",
    "href": "tidying-data.html#baseball-and-softball-spaghetti-1",
    "title": "15  Tidying data",
    "section": "15.14 Baseball and softball spaghetti",
    "text": "15.14 Baseball and softball spaghetti\nOn a previous assignment, we found that students could throw a baseball further than they could throw a softball. In this question, we will make a graph called a “spaghetti plot” to illustrate this graphically. (The issue previously was that the data were matched pairs: the same students threw both balls.)\nThis seems to work most naturally by building a pipe, a line or two at a time. See if you can do it that way. (If you can’t make it work, use lots of temporary data frames, one to hold the result of each part.)\n\nRead in the data again from link. The variables had no names, so supply some, as you did before.\n\nSolution\nLiteral copy and paste:\n\nmyurl &lt;- \"http://ritsokiguess.site/datafiles/throw.txt\"\nthrows &lt;- read_delim(myurl, \" \", col_names = c(\"student\", \"baseball\", \"softball\"))\n\nRows: 24 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): student, baseball, softball\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nthrows\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nCreate a new column that is the students turned into a factor, adding it to your data frame. The reason for this will become clear later.\n\nSolution\nFeed student into factor, creating a new column with mutate:\n\nthrows %&gt;% mutate(fs = factor(student))\n\n\n\n  \n\n\n\nThis doesn’t look any different from the original student numbers, but note the variable type at the top of the column.\n\\(\\blacksquare\\)\n\nCollect together all the throwing distances into one column, making a second column that says which ball was thrown.\n\nSolution\nUse pivot_longer. It goes like this:\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\")\n\n\n\n  \n\n\n\nThe names_to is the name of a new categorical column whose values will be what is currently column names, and the values_to names a new quantitative (usually) column that will hold the values in those columns that you are making longer.\nIf you want to show off a little, you can use a select-helper, noting that the columns you want to make longer all end in “ball”:\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(ends_with(\"ball\"), names_to=\"ball\", values_to=\"distance\")\n\n\n\n  \n\n\n\nThe same result. Use whichever you like.\n\\(\\blacksquare\\)\n\nUsing your new data frame, make a “scatterplot” of throwing distance against type of ball.\n\nSolution\nThe obvious thing. No data frame in the ggplot because it’s the data frame that came out of the previous part of the pipeline (that doesn’t have a name):\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %&gt;% \n  ggplot(aes(x = ball, y = distance)) + geom_point()\n\n\n\n\nThis is an odd type of scatterplot because the \\(x\\)-axis is actually a categorical variable. It’s really what would be called something like a dotplot. We’ll be using this as raw material for the plot we actually want.\nWhat this plot is missing is an indication of which student threw which ball. As it stands now, it could be an inferior version of a boxplot of distances thrown for each ball (which would imply that they are two independent sets of students, something that is not true).\n\\(\\blacksquare\\)\n\nAdd two things to your plot: something that will distinguish the students by colour (this works best if the thing distinguished by colour is a factor),7 and something that will join the two points for the same student by a line.\n\nSolution\nA colour and a group in the aes, and a geom_line:\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %&gt;% \n  ggplot(aes(x = ball, y = distance, group = fs, colour = fs)) +\n  geom_point() + geom_line()\n\n\n\n\nYou can see what happens if you use the student as a number:\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %&gt;% \n  ggplot(aes(x = ball, y = distance, group = student, colour = student)) +\n  geom_point() + geom_line()\n\n\n\n\nNow the student numbers are distinguished as a shade of blue (on an implied continuous scale: even a nonsensical fractional student number like 17.5 would be a shade of blue). This is not actually so bad here, because all we are trying to do is to distinguish the students sufficiently from each other so that we can see where the spaghetti strands go. But I like the multi-coloured one better.\n\\(\\blacksquare\\)\n\nThe legend is not very informative. Remove it from the plot, using guides.\n\nSolution\nYou may not have seen this before. Here’s what to do: Find what’s at the top of the legend that you want to remove. Here that is fs. Find where fs appears in your aes. It actually appears in two places: in group and colour. I think the legend we want to get rid of is actually the colour one, so we do this:\n\nthrows %&gt;%\n  mutate(fs = factor(student)) %&gt;%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %&gt;% \n  ggplot(aes(x = ball, y = distance, group = fs, colour = fs)) +\n  geom_point() + geom_line() +\n  guides(colour = \"none\")\n\n\n\n\nThat seems to have done it.\n\\(\\blacksquare\\)\n\nWhat do you see on the final spaghetti plot? What does that tell you about the relative distances a student can throw a baseball vs. a softball? Explain briefly, blah blah blah.\n\nSolution\nMost of the spaghetti strands go downhill from baseball to softball, or at least very few of them go uphill. That tells us that most students can throw a baseball further than a softball. That was the same impression that the matched-pairs \\(t\\)-test gave us. But the spaghetti plot tells us something else. If you look carefully, you see that most of the big drops are for students who could throw a baseball a long way. These students also threw a softball further than the other students, but not by as much. Most of the spaghetti strands in the bottom half of the plot go more or less straight across. This indicates that students who cannot throw a baseball very far will throw a softball about the same distance as they threw the baseball. There is an argument you could make here that the difference between distances thrown is a proportional one, something like “a student typically throws a baseball 20% further than a softball”. That could be assessed by comparing not the distances themselves, but the logs of the distances: in other words, making a log transformation of all the distances. (Distances have a lower limit of zero, so you might expect observed distances to be skewed to the right, which is another argument for making some kind of transformation.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#ethanol-and-sleep-time-in-rats-1",
    "href": "tidying-data.html#ethanol-and-sleep-time-in-rats-1",
    "title": "15  Tidying data",
    "section": "15.15 Ethanol and sleep time in rats",
    "text": "15.15 Ethanol and sleep time in rats\nA biologist wished to study the effects of ethanol on sleep time in rats. A sample of 20 rats (all the same age) was selected, and each rat was given an injection having a particular concentration (0, 1, 2 or 4 grams per kilogram of body weight) of ethanol. These are labelled e0, e1, e2, e4. The “0” treatment was a control group. The rapid eye movement (REM) sleep time was then recorded for each rat. The data are in link.\n\nRead the data in from the file. Check that you have four rows of observations and five columns of sleep times.\n\nSolution\nSeparated by single spaces:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ratsleep.txt\"\nsleep1 &lt;- read_delim(my_url, \" \")\n\nRows: 4 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): treatment\ndbl (5): obs1, obs2, obs3, obs4, obs5\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsleep1\n\n\n\n  \n\n\n\nThere are six columns, but one of them labels the groups, and there are correctly five columns of sleep times.\nI used a “temporary” name for my data frame, because I’m going to be doing some processing on it in a minute, and I want to reserve the name sleep for my processed data frame.\n\\(\\blacksquare\\)\n\nUnfortunately, the data are in the wrong format. All the sleep times for each treatment group are on one row, and we should have one column containing all the sleep times, and the corresponding row should show which treatment group that sleep time came from. Transform this data frame into one that you could use for modelling or making graphs.\n\nSolution\nWe will want one column of sleep times, with an additional categorical column saying what observation each sleep time was within its group (or, you might say, we don’t really care about that much, but that’s what we are going to get).\nThe columns obs1 through obs5 are different in that they are different observation numbers (“replicates”, in the jargon). I’ll call that rep. What makes them the same is that they are all sleep times. Columns obs1 through obs5 are the ones we want to combine, thus. Here is where I use the name sleep: I save the result of the pivot_longer into a data frame sleep. Note that I also used the brackets-around-the-outside to display what I had, so that I didn’t have to do a separate display. This is a handy way of saving and displaying in one shot:\n\n(sleep1 %&gt;% \n  pivot_longer(-treatment, names_to=\"rep\", values_to=\"sleeptime\") -&gt; sleep)\n\n\n\n  \n\n\n\nTypically in this kind of work, you have a lot of columns that need to be made longer, and a much smaller number of columns that need to be repeated as necessary. You can either specify all the columns to make longer, or you can specify “not” the other columns. Above, my first input to pivot_longer was “everything but treatment”, but you could also do it like this:\n\nsleep1 %&gt;% \n  pivot_longer(obs1:obs5, names_to=\"rep\", values_to=\"sleeptime\") \n\n\n\n  \n\n\n\nor like this:\n\nsleep1 %&gt;% \n  pivot_longer(starts_with(\"obs\"), names_to=\"rep\", values_to=\"sleeptime\") \n\n\n\n  \n\n\n\nThis one was a little unusual in that usually with these you have the treatments in the columns and the replicates in the rows. It doesn’t matter, though: pivot_longer handles both cases.\nWe have 20 rows of 3 columns. I got all the rows, but you will probably get an output with ten rows as usual, and will need to click Next to see the last ten rows. The initial display will say how many rows (20) and columns (3) you have.\nThe column rep is not very interesting: it just says which observation each one was within its group.8 The interesting things are treatment and sleeptime, which are the two variables we’ll need for our analysis of variance.\n\\(\\blacksquare\\)\n\nUsing your new data frame, make side-by-side boxplots of sleep time by treatment group.\n\nSolution\n\nggplot(sleep, aes(x = treatment, y = sleeptime)) + geom_boxplot()\n\n\n\n\n\\(\\blacksquare\\)\n\nIn your boxplots, how does the median sleep time appear to depend on treatment group?\n\nSolution\nIt appears to decrease as the dose of ethanol increases, and pretty substantially so (in that the differences ought to be significant, but that’s coming up).\n\\(\\blacksquare\\)\n\nThere is an assumption about spread that the analysis of variance needs in order to be reliable. Do your boxplots indicate that this assumption is satisfied for these data, bearing in mind that you have only five observations per group?\n\nSolution\nThe assumption is that the population SDs of each group are all equal. Now, the boxplots show IQRs, which are kind of a surrogate for SD, and because we only have five observations per group to base the IQRs on, the sample IQRs might vary a bit. So we should look at the heights of the boxes on the boxplot, and see whether they are grossly unequal. They appear to be to be of very similar heights, all things considered, so I am happy.\nIf you want the SDs themselves:\n\nsleep %&gt;%\n  group_by(treatment) %&gt;%\n  summarize(stddev = sd(sleeptime))\n\n\n\n  \n\n\n\nThose are very similar, given only 5 observations per group. No problems here.\n\\(\\blacksquare\\)\n\nRun an analysis of variance to see whether sleep time differs significantly among treatment groups. What do you conclude?\n\nSolution\nI use aov here, because I might be following up with Tukey in a minute:\n\nsleep.1 &lt;- aov(sleeptime ~ treatment, data = sleep)\nsummary(sleep.1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntreatment    3   5882    1961   21.09 8.32e-06 ***\nResiduals   16   1487      93                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis is a very small P-value, so my conclusion is that the mean sleep times are not all the same for the treatment groups. Further than that I am not entitled to say (yet).\nThe technique here is to save the output from aov in something, look at that (via summary), and then that same something gets fed into TukeyHSD later.\n\\(\\blacksquare\\)\n\nWould it be a good idea to run Tukey’s method here? Explain briefly why or why not, and if you think it would be a good idea, run it.\n\nSolution\nTukey’s method is useful when (i) we have run an analysis of variance and got a significant result and (ii) when we want to know which groups differ significantly from which. Both (i) and (ii) are true here. So:\n\nTukeyHSD(sleep.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = sleeptime ~ treatment, data = sleep)\n\n$treatment\n        diff       lwr         upr     p adj\ne1-e0 -17.74 -35.18636  -0.2936428 0.0455781\ne2-e0 -31.36 -48.80636 -13.9136428 0.0005142\ne4-e0 -46.52 -63.96636 -29.0736428 0.0000056\ne2-e1 -13.62 -31.06636   3.8263572 0.1563545\ne4-e1 -28.78 -46.22636 -11.3336428 0.0011925\ne4-e2 -15.16 -32.60636   2.2863572 0.1005398\n\n\n\\(\\blacksquare\\)\n\nWhat do you conclude from Tukey’s method? (This is liable to be a bit complicated.) Is there a treatment that is clearly best, in terms of the sleep time being largest?\n\nSolution\nAll the differences are significant except treatment e2 vs. e1 and e4. All the differences involving the control group e0 are significant, and if you look back at the boxplots in (c), you’ll see that the control group e0 had the highest mean sleep time. So the control group is best (from this point of view), or another way of saying it is that any dose of ethanol is significantly reducing mean sleep time. The other comparisons are a bit confusing, because the 1-4 difference is significant, but neither of the differences involving 2 are. That is, 1 is better than 4, but 2 is not significantly worse than 1 nor better than 4. This seems like it should be a logical impossibility, but the story is that we don’t have enough data to decide where 2 fits relative to 1 or 4. If we had 10 or 20 observations per group, we might be able to conclude that 2 is in between 1 and 4 as the boxplots suggest.\nExtra: I didn’t ask about normality here, but like the equal-spreads assumption I’d say there’s nothing controversial about it with these data. With normality good and equal spreads good, aov plus Tukey is the analysis of choice.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#growth-of-tomatoes-1",
    "href": "tidying-data.html#growth-of-tomatoes-1",
    "title": "15  Tidying data",
    "section": "15.16 Growth of tomatoes",
    "text": "15.16 Growth of tomatoes\nA biology graduate student exposed each of 32 tomato plants to one of four different colours of light (8 plants to each colour). The growth rate of each plant, in millimetres per week, was recorded. The data are in link.\n\nRead the data into R and confirm that you have 8 rows and 5 columns of data.\n\nSolution\nThis kind of thing:\n\nmy_url=\"http://ritsokiguess.site/datafiles/tomatoes.txt\"\ntoms1=read_delim(my_url,\" \")\n\nRows: 8 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (5): plant, blue, red, yellow, green\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntoms1\n\n\n\n  \n\n\n\nI do indeed have 8 rows and 5 columns.\nWith only 8 rows, listing the data like this is good.\n\\(\\blacksquare\\)\n\nRe-arrange the data so that you have one column containing all the growth rates, and another column saying which colour light each plant was exposed to. (The aim here is to produce something suitable for feeding into aov later.)\n\nSolution\nThis is a job for pivot_longer:\n\ntoms1 %&gt;% \n   pivot_longer(-plant, names_to=\"colour\", values_to=\"growthrate\") -&gt; toms2\ntoms2\n\n\n\n  \n\n\n\nI chose to specify “everything but plant number”, since there are several colour columns with different names.\nSince the column plant was never mentioned, this gets repeated as necessary, so now it denotes “plant within colour group”, which in this case is not very useful. (Where you have matched pairs, or repeated measures in general, you do want to keep track of which individual is which. But this is not repeated measures because plant number 1 in the blue group and plant number 1 in the red group are different plants.)\nThere were 8 rows originally and 4 different colours, so there should be, and are, \\(8 \\times 4=32\\) rows in the made-longer data set.\n\\(\\blacksquare\\)\n\nSave the data in the new format to a text file. This is most easily done using write_csv, which is the opposite of read_csv. It requires two things: a data frame, and the name of a file to save in, which should have a .csv extension.\n\nSolution\nThe code is easy enough:\n\nwrite_csv(toms2,\"tomatoes2.csv\")\n\nIf no error, it worked. That’s all you need.\nTo verify (for my satisfaction) that it was saved correctly:\n\ncat tomatoes2.csv \n\nplant,colour,growthrate\n1,blue,5.34\n1,red,13.67\n1,yellow,4.61\n1,green,2.72\n2,blue,7.45\n2,red,13.04\n2,yellow,6.63\n2,green,1.08\n3,blue,7.15\n3,red,10.16\n3,yellow,5.29\n3,green,3.97\n4,blue,5.53\n4,red,13.12\n4,yellow,5.29\n4,green,2.66\n5,blue,6.34\n5,red,11.06\n5,yellow,4.76\n5,green,3.69\n6,blue,7.16\n6,red,11.43\n6,yellow,5.57\n6,green,1.96\n7,blue,7.77\n7,red,13.98\n7,yellow,6.57\n7,green,3.38\n8,blue,5.09\n8,red,13.49\n8,yellow,5.25\n8,green,1.87\n\n\nOn my system, that will list the contents of the file. Or you can just open it in R Studio (if you saved it the way I did, it’ll be in the same folder, and you can find it in the Files pane.)\n\\(\\blacksquare\\)\n\nMake a suitable boxplot, and use it to assess the assumptions for ANOVA. What do you conclude? Explain briefly.\n\nSolution\nNothing terribly surprising here. My data frame is called toms2, for some reason:\n\nggplot(toms2,aes(x=colour, y=growthrate))+geom_boxplot()\n\n\n\n\nThere are no outliers, but there is a little skewness (compare the whiskers, not the placement of the median within the box, because what matters with skewness is the tails, not the middle of the distribution; it’s problems in the tails that make the mean unsuitable as a measure of centre). The Red group looks the most skewed. Also, the Yellow group has smaller spread than the others (we assume that the population variances within each group are equal). The thing to bear in mind here, though, is that there are only eight observations per group, so the distributions could appear to have unequal variances or some non-normality by chance.\nMy take is that these data, all things considered, are just about OK for ANOVA. Another option would be to do Welch’s ANOVA as well and compare with the regular ANOVA: if they give more or less the same P-value, that’s a sign that I didn’t need to worry.\nExtra: some people like to run a formal test on the variances to test them for equality. My favourite (for reasons explained elsewhere) is the Levene test, if you insist on going this way. It lives in package car, and does not take a data=, so you need to do the with thing:\n\nlibrary(car)\nwith(toms2,leveneTest(growthrate,colour))\n\nWarning in leveneTest.default(growthrate, colour): colour coerced to factor.\n\n\n\n\n  \n\n\n\nThe warning is because colour was actually text, but the test did the right thing by turning it into a factor, so that’s OK.\nThere is no way we can reject equal variances in the four groups. The \\(F\\)-statistic is less than 1, in fact, which says that if the four groups have the same population variances, the sample variances will be more different than the ones we observed on average, and so there is no way that these sample variances indicate different population variances. (This is because of 8 observations only per group; if there had been 80 observations per group, it would have been a different story.) Decide for yourself whether you’re surprised by this.\nWith that in mind, I think the regular ANOVA will be perfectly good, and we would expect that and the Welch ANOVA to give very similar results.\n\\(\\blacksquare\\)\n\nRun (regular) ANOVA on these data. What do you conclude? (Optional extra: if you think that some other variant of ANOVA would be better, run that as well and compare the results.)\n\nSolution\naov, bearing in mind that Tukey is likely to follow:\n\ntoms.1=aov(growthrate~colour,data=toms2)\nsummary(toms.1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncolour       3  410.5  136.82   118.2 5.28e-16 ***\nResiduals   28   32.4    1.16                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis is a tiny P-value, so the mean growth rate for the different colours is definitely not the same for all colours. Or, if you like, one or more of the colours has a different mean growth rate than the others.\nThis, remember, is as far as we go right now.\nExtra: if you thought that normality was OK but not equal spreads, then Welch ANOVA is the way to go:\n\ntoms.2=oneway.test(growthrate~colour,data=toms2)\ntoms.2\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  growthrate and colour\nF = 81.079, num df = 3.000, denom df = 15.227, p-value = 1.377e-09\n\n\nThe P-value is not quite as small as for the regular ANOVA, but it is still very small, and the conclusion is the same.\nIf you had doubts about the normality (that were sufficiently great, even given the small sample sizes), then go with Mood’s median test for multiple groups:\n\nlibrary(smmr)\nmedian_test(toms2,growthrate,colour)\n\n$table\n        above\ngroup    above below\n  blue       5     3\n  green      0     8\n  red        8     0\n  yellow     3     5\n\n$test\n       what        value\n1 statistic 1.700000e+01\n2        df 3.000000e+00\n3   P-value 7.067424e-04\n\n\nThe P-value is again extremely small (though not quite as small as for the other two tests, for the usual reason that Mood’s median test doesn’t use the data very efficiently: it doesn’t use how far above or below the overall median the data values are.)\nThe story here, as ever, is consistency: whatever you thought was wrong, looking at the boxplots, needs to guide the test you do:\n\nif you are not happy with normality, go with median_test from smmr (Mood’s median test).\nif you are happy with normality and equal variances, go with aov.\nif you are happy with normality but not equal variances, go with oneway.test (Welch ANOVA).\n\nSo the first thing to think about is normality, and if you are OK with normality, then think about equal spreads. Bear in mind that you need to be willing to tolerate a certain amount of non-normality and inequality in the spreads, given that your data are only samples from their populations. (Don’t expect perfection, in short.)\n\\(\\blacksquare\\)\n\nIf warranted, run a suitable follow-up. (If not warranted, explain briefly why not.)\n\nSolution\nWhichever flavour of ANOVA you ran (regular ANOVA, Welch ANOVA, Mood’s median test), you got the same conclusion for these data: that the average growth rates were not all the same for the four colours. That, as you’ll remember, is as far as you go. To find out which colours differ from which in terms of growth rate, you need to run some kind of multiple-comparisons follow-up, the right one for the analysis you did. Looking at the boxplots suggests that red is clearly best and green clearly worst, and it is possible that all the colours are significantly different from each other.) If you did regular ANOVA, Tukey is what you need:\n\nTukeyHSD(toms.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = growthrate ~ colour, data = toms2)\n\n$colour\n                diff       lwr        upr     p adj\ngreen-blue   -3.8125 -5.281129 -2.3438706 0.0000006\nred-blue      6.0150  4.546371  7.4836294 0.0000000\nyellow-blue  -0.9825 -2.451129  0.4861294 0.2825002\nred-green     9.8275  8.358871 11.2961294 0.0000000\nyellow-green  2.8300  1.361371  4.2986294 0.0000766\nyellow-red   -6.9975 -8.466129 -5.5288706 0.0000000\n\n\nAll of the differences are (strongly) significant, except for yellow and blue, the two with middling growth rates on the boxplot. Thus we would have no hesitation in saying that growth rate is biggest in red light and smallest in green light.\nIf you did Welch ANOVA, you need Games-Howell, which you have to get from one of the packages that offers it:\n\nlibrary(PMCMRplus)\ngamesHowellTest(growthrate~factor(colour),data=toms2)\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: growthrate by factor(colour)\n\n\n       blue    green   red    \ngreen  1.6e-05 -       -      \nred    1.5e-06 4.8e-09 -      \nyellow 0.18707 0.00011 5.8e-07\n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nThe conclusions are the same as for the Tukey: all the means are significantly different except for yellow and blue. Finally, if you did Mood’s median test, you need this one:\n\npairwise_median_test(toms2, growthrate, colour)\n\n\n\n  \n\n\n\nSame conclusions again. This is what I would have guessed; the conclusions from Tukey were so clear-cut that it really didn’t matter which way you went; you’d come to the same conclusion.\nThat said, what I am looking for from you is a sensible choice of analysis of variance (ANOVA, Welch’s ANOVA or Mood’s median test) for a good reason, followed by the right follow-up for the test you did. Even though the conclusions are all the same no matter what you do here, I want you to get used to following the right method, so that you will be able to do the right thing when it does matter.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#pain-relief-in-migraine-headaches-again-1",
    "href": "tidying-data.html#pain-relief-in-migraine-headaches-again-1",
    "title": "15  Tidying data",
    "section": "15.17 Pain relief in migraine headaches (again)",
    "text": "15.17 Pain relief in migraine headaches (again)\nThe data in link are from a study of pain relief in migraine headaches. Specifically, 27 subjects were randomly assigned to receive one of three pain relieving drugs, labelled A, B and C. Each subject reported the number of hours of pain relief they obtained (that is, the number of hours between taking the drug and the migraine symptoms returning). A higher value is therefore better. Can we make some recommendation about which drug is best for the population of migraine sufferers?\n\nRead in and display the data. Take a look at the data file first, and see if you can say why read_table will work and read_delim will not.\n\nSolution\nThe key is two things: the data values are lined up in columns, and there is more than one space between values. The second thing is why read_delim will not work. If you look carefully at the data file, you’ll see that the column names are above and aligned with the columns. read_table doesn’t actually need things to be lined up in columns; all it actually needs is for there to be one or more spaces between columns.\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/migraine.txt\"\nmigraine &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  DrugA = col_double(),\n  DrugB = col_double(),\n  DrugC = col_double()\n)\n\nmigraine\n\n\n\n  \n\n\n\nSuccess.\n\\(\\blacksquare\\)\n\nWhat is it about the experimental design that makes a one-way analysis of variance plausible for data like this?\n\nSolution\nEach experimental subject only tested one drug, so that we have 27 independent observations, nine from each drug. This is exactly the setup that a one-way ANOVA requires. Compare that to, for example, a situation where you had only 9 subjects, but they each tested all the drugs (so that each subject produced three measurements). That is like a three-measurement version of matched pairs, a so-called repeated-measures design, which requires its own kind of analysis.9\n\\(\\blacksquare\\)\n\nWhat is wrong with the current format of the data as far as doing a one-way ANOVA analysis is concerned? (This is related to the idea of whether or not the data are “tidy”.)\n\nSolution\nFor our analysis, we need one column of pain relief time and one column labelling the drug that the subject in question took. Or, if you prefer to think about what would make these data “tidy”: there are 27 subjects, so there ought to be 27 rows, and all three columns are measurements of pain relief, so they ought to be in one column.\n\\(\\blacksquare\\)\n\n“Tidy” the data to produce a data frame suitable for your analysis.\n\nSolution\nThis is pivot_longer. The column names are going to be stored in a column drug, and the corresponding values in a column called painrelief (use whatever names you like):\n\nmigraine %&gt;% \n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") -&gt; migraine2\n\nSince I was making all the columns longer, I used the select-helper everything() to do that. Using instead DrugA:DrugC or starts_with(\"Drug\") would also be good. Try them. starts_with is not case-sensitive, as far as I remember, so starts_with(\"drug\") will also work here.\nWe do indeed have a new data frame with 27 rows, one per observation, and 2 columns, one for each variable: the pain relief hours, plus a column identifying which drug that pain relief time came from. Exactly what aov needs.\nYou can probably devise a better name for your new data frame.\n\\(\\blacksquare\\)\n\nGo ahead and run your one-way ANOVA (and Tukey if necessary). Assume for this that the pain relief hours in each group are sufficiently close to normally distributed with sufficiently equal spreads.\n\nSolution\nMy last sentence absolves us from doing the boxplots that we would normally insist on doing.\n\npainrelief.1 &lt;- aov(painrelief ~ drug, data = migraine2)\nsummary(painrelief.1)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndrug         2  41.19   20.59   7.831 0.00241 **\nResiduals   24  63.11    2.63                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere are (strongly) significant differences among the drugs, so it is definitely worth firing up Tukey to figure out where the differences are:\n\nTukeyHSD(painrelief.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = painrelief ~ drug, data = migraine2)\n\n$drug\n                  diff        lwr      upr     p adj\nDrugB-DrugA  2.8888889  0.9798731 4.797905 0.0025509\nDrugC-DrugA  2.2222222  0.3132065 4.131238 0.0203671\nDrugC-DrugB -0.6666667 -2.5756824 1.242349 0.6626647\n\n\nBoth the differences involving drug A are significant, and because a high value of painrelief is better, in both cases drug A is worse than the other drugs. Drugs B and C are not significantly different from each other.\nExtra: we can also use the “pipe” to do this all in one go:\n\nmigraine %&gt;%\n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") %&gt;%\n  aov(painrelief ~ drug, data = .) %&gt;%\n  summary()\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndrug         2  41.19   20.59   7.831 0.00241 **\nResiduals   24  63.11    2.63                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nwith the same results as before. Notice that I never actually created a second data frame by name; it was created by pivot_longer and then immediately used as input to aov.10 I also used the data=. trick to use “the data frame that came out of the previous step” as my input to aov.\nRead the above like this: “take migraine, and then make everything longer, creating new columns drug and painrelief, and then do an ANOVA of painrelief by drug, and then summarize the results.”\nWhat is even more alarming is that I can feed the output from aov straight into TukeyHSD:\n\nmigraine %&gt;%\n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") %&gt;%\n  aov(painrelief ~ drug, data = .) %&gt;%\n  TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = painrelief ~ drug, data = .)\n\n$drug\n                  diff        lwr      upr     p adj\nDrugB-DrugA  2.8888889  0.9798731 4.797905 0.0025509\nDrugC-DrugA  2.2222222  0.3132065 4.131238 0.0203671\nDrugC-DrugB -0.6666667 -2.5756824 1.242349 0.6626647\n\n\nI wasn’t sure whether this would work, since the output from aov is an R list rather than a data frame, but the output from aov is sent into TukeyHSD whatever kind of thing it is.\nWhat I am missing here is to display the result of aov and use it as input to TukeyHSD. Of course, I had to discover that this could be solved, and indeed it can:\n\nmigraine %&gt;%\n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") %&gt;%\n  aov(painrelief ~ drug, data = .) %&gt;%\n  {\n    print(summary(.))\n    .\n  } %&gt;%\n  TukeyHSD()\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndrug         2  41.19   20.59   7.831 0.00241 **\nResiduals   24  63.11    2.63                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = painrelief ~ drug, data = .)\n\n$drug\n                  diff        lwr      upr     p adj\nDrugB-DrugA  2.8888889  0.9798731 4.797905 0.0025509\nDrugC-DrugA  2.2222222  0.3132065 4.131238 0.0203671\nDrugC-DrugB -0.6666667 -2.5756824 1.242349 0.6626647\n\n\nThe odd-looking second-last line of that again uses the . trick for “whatever came out of the previous step”. The thing inside the curly brackets is two commands one after the other; the first is to display the summary of that aov11 and the second is to just pass whatever came out of the previous line, the output from aov, on, unchanged, into TukeyHSD.\nIn the Unix/Linux world this is called tee, where you print something and pass it on to the next step. The name tee comes from a (real physical) pipe that plumbers would use to split water flow into two, which looks like a letter T.\n\\(\\blacksquare\\)\n\nWhat recommendation would you make about the best drug or drugs? Explain briefly.\n\nSolution\nDrug A is significantly the worst, so we eliminate that. But there is no significant difference between drugs B and C, so we have no reproducible reason for preferring one rather than the other. Thus, we recommend “either B or C”. If you weren’t sure which way around the drugs actually came out, then you should work out the mean pain relief score by drug:\n\nmigraine2 %&gt;%\n  group_by(drug) %&gt;%\n  summarize(m = mean(painrelief))\n\n\n\n  \n\n\n\nThese confirm that A is worst, and there is nothing much to choose between B and C. You should not recommend drug B over drug C on this evidence, just because its (sample) mean is higher. The point about significant differences is that they are supposed to stand up to replication: in another experiment, or in real-life experiences with these drugs, the mean pain relief score for drug A is expected to be worst, but between drugs B and C, sometimes the mean of B will come out higher and sometimes C’s mean will be higher, because there is no significant difference between them.12 Another way is to draw a boxplot of pain-relief scores:\n\nggplot(migraine2, aes(x = drug, y = painrelief)) + geom_boxplot()\n\n\n\n\nThe medians of drugs B and C are actually exactly the same. Because the pain relief values are all whole numbers (and there are only 9 in each group), you get that thing where enough of them are equal that the median and third quartiles are equal, actually for two of the three groups.\nDespite the weird distributions, I’m willing to call these groups sufficiently symmetric for the ANOVA to be OK, but I didn’t ask you to draw the boxplot, because I didn’t want to confuse the issue with this. The point of this question was to get the data tidy enough to do an analysis.\nAs I said, I didn’t want you to have to get into this, but if you are worried, you know what the remedy is — Mood’s median test. Don’t forget to use the right data frame:\n\nlibrary(smmr)\nmedian_test(migraine2, painrelief, drug)\n\n$table\n       above\ngroup   above below\n  DrugA     0     8\n  DrugB     5     2\n  DrugC     6     0\n\n$test\n       what        value\n1 statistic 1.527273e+01\n2        df 2.000000e+00\n3   P-value 4.825801e-04\n\n\nBecause the pain relief scores are integers, there are probably a lot of them equal to the overall median. There were 27 observations altogether, but Mood’s median test will discard any that are equal to this value. There must have been 9 observations in each group to start with, but if you look at each row of the table, there are only 8 observations listed for drug A, 7 for drug B and 6 for drug C, so there must have been 1, 2 and 3 (totalling 6) observations equal to the median that were discarded.\nThe P-value is a little bigger than came out of the \\(F\\)-test, but the conclusion is still that there are definitely differences among the drugs in terms of pain relief. The table at the top of the output again suggests that drug A is worse than the others, but to confirm that you’d have to do Mood’s median test on all three pairs of drugs, and then use Bonferroni to allow for your having done three tests:\n\npairwise_median_test(migraine2, painrelief, drug)\n\n\n\n  \n\n\n\nDrug A gives worse pain relief (fewer hours) than both drugs B and C, which are not significantly different from each hour. This is exactly what you would have guessed from the boxplot.\nI adjusted the P-values as per Bonferroni by multiplying them by 3 (so that I could still compare with 0.05), but it makes no sense to have a P-value, which is a probability, greater than 1, so an “adjusted P-value” that comes out greater than 1 is rounded back down to 1. You interpret this as being “no evidence at all of a difference in medians” between drugs B and C.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#location-species-and-disease-in-plants-1",
    "href": "tidying-data.html#location-species-and-disease-in-plants-1",
    "title": "15  Tidying data",
    "section": "15.18 Location, species and disease in plants",
    "text": "15.18 Location, species and disease in plants\nThe table below is a “contingency table”, showing frequencies of diseased and undiseased plants of two different species in two different locations:\n\nSpecies     Disease present         Disease absent\n          Location X Location Y  Location X Location Y\nA            44         12          38        10\nB            28         22          20        18\n\nThe data were saved as link. In that file, the columns are coded by two letters: a p or an a to denote presence or absence of disease, and an x or a y to denote location X or Y. The data are separated by multiple spaces and aligned with the variable names.\n\nRead in and display the data.\n\nSolution\nread_table again. You know this because, when you looked at the data file, which of course you did (didn’t you?), you saw that the data values were aligned by columns with multiple spaces between them:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/disease.txt\"\ntbl &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Species = col_character(),\n  px = col_double(),\n  py = col_double(),\n  ax = col_double(),\n  ay = col_double()\n)\n\ntbl\n\n\n\n  \n\n\n\nI was thinking ahead, since I’ll be wanting to have one of my columns called disease, so I’m not calling the data frame disease.\nYou’ll also have noticed that I simplified the data frame that I had you read in, because the original contingency table I showed you has two header rows, and we have to have one header row. So I mixed up the information in the two header rows into one.\n\\(\\blacksquare\\)\n\nExplain briefly how these data are not “tidy”.\n\nSolution\nThe simple answer is that there are 8 frequencies, that each ought to be in a row by themselves. Or, if you like, there are three variables, Species, Disease status and Location, and each of those should be in a column of its own. Either one of these ideas, or something like it, is good. I need you to demonstrate that you know something about “tidy data” in this context.\n\\(\\blacksquare\\)\n\nUse a suitable tidyr tool to get all the things that are the same into a single column. (You’ll need to make up a temporary name for the other new column that you create.) Show your result.\n\nSolution\npivot_longer is the tool. All the columns apart from Species contain frequencies. They are frequencies in disease-location combinations, so I’ll call the column of “names” disloc. Feel free to call it temp for now if you prefer:\n\ntbl %&gt;% pivot_longer(-Species, names_to=\"disloc\", values_to = \"frequency\") -&gt; tbl.2\ntbl.2\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nExplain briefly how the data frame you just created is still not “tidy” yet.\n\nSolution\nThe column I called disloc actually contains two variables, disease and location, which need to be split up. A check on this is that we have two columns (not including the frequencies), but back in (b) we found three variables, so there ought to be three non-frequency columns.\n\\(\\blacksquare\\)\n\nUse one more tidyr tool to make these data tidy, and show your result.\n\nSolution\nThis means splitting up disloc into two separate columns, splitting after the first character, thus:\n\n(tbl.2 %&gt;% separate(disloc, c(\"disease\", \"location\"), 1) -&gt; tbl.3)\n\n\n\n  \n\n\n\nThis is now tidy: eight frequencies in rows, and three non-frequency columns. (Go back and look at your answer to part (b) and note that the issues you found there have all been resolved now.)\nExtra: my reading of one of the vignettes (the one called pivot) for tidyr suggests that pivot_longer can do both the making longer and the separating in one shot:\n\ntbl %&gt;% pivot_longer(-Species, names_to=c(\"disease\", \"location\"), names_sep=1, values_to=\"frequency\")\n\n\n\n  \n\n\n\nAnd I (amazingly) got that right first time!\nThe idea is that you recognize that the column names are actually two things: a disease status and a location. To get pivot_longer to recognize that, you put two things in the names_to. Then you have to say how the two things in the columns are separated: this might be by an underscore or a dot, or, as here, “after the first character” (just as in separate). Using two names and some indication of what separates them then does a combined pivot-longer-and-separate, all in one shot.\nThe more I use pivot_longer, the more I marvel at the excellence of its design: it seems to be easy to guess how to make things work.\n\\(\\blacksquare\\)\n\nLet’s see if we can re-construct the original contingency table (or something equivalent to it). Use the function xtabs. This requires first a model formula with the frequency variable on the left of the squiggle, and the other variables separated by plus signs on the right. Second it requires a data frame, with data=. Feed your data frame from the previous part into xtabs. Save the result in a variable and display the result.\n\nSolution\n\ntbl.4 &lt;- xtabs(frequency ~ Species + disease + location, data = tbl.3)\ntbl.4\n\n, , location = x\n\n       disease\nSpecies  a  p\n      A 38 44\n      B 20 28\n\n, , location = y\n\n       disease\nSpecies  a  p\n      A 10 12\n      B 18 22\n\n\nThis shows a pair of contingency tables, one each for each of the two locations (in general, the variable you put last on the right side of the model formula). You can check that everything corresponds with the original data layout at the beginning of the question, possibly with some things rearranged (but with the same frequencies in the same places).\n\\(\\blacksquare\\)\n\nTake the output from the last part and feed it into the function ftable. How has the output been changed? Which do you like better? Explain briefly.\n\nSolution\nThis:\n\nftable(tbl.4)\n\n                location  x  y\nSpecies disease               \nA       a                38 10\n        p                44 12\nB       a                20 18\n        p                28 22\n\n\nThis is the same output, but shown more compactly. (Rather like a vertical version of the original data, in fact.) I like ftable better because it displays the data in the smallest amount of space, though I’m fine if you prefer the xtabs output because it spreads things out more. This is a matter of taste. Pick one and tell me why you prefer it, and I’m good.\nThat’s the end of what you had to do, but I thought I would do some modelling and try to find out what’s associated with disease. The appropriate modelling with frequencies is called “log-linear modelling”, and it assumes that the log of the frequencies has a linear relationship with the effects of the other variables. This is not quite as simple as the log transformations we had before, because bigger frequencies are going to be more variable, so we fit a generalized linear model with a Poisson-distributed response and log link. (It’s better if you know what that means, but you ought to be able to follow the logic if you don’t. Chapter 29 has more on this.)\nFirst, fit a model predicting frequency from everything, including all the interactions. (The reason for doing it this way will become clear later):\n\nmodel.1 &lt;- glm(frequency ~ Species * location * disease, data = tbl.3, family = \"poisson\")\ndrop1(model.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe residuals are all zero because this model fits perfectly. The problem is that it is very complicated, so it offers no insight. So what we do is to look at the highest-order interaction Species:location:disease and see whether it is significant. It is not, so we can remove it. This is reminiscent of variable selection in regression, where we pull the least significant thing out of the model in turn until we can go no further. But here, we have additional things to think about: we have to get rid of all the three-way interactions before we can tackle the two-way ones, and all the two-way ones before we can tackle the main effects. There is a so-called “nested” structure happening here that says you don’t look at, say, Species, until you have removed all the higher-order interactions involving Species. Not clear yet? Don’t fret. drop1 allows you to assess what is currently up for grabs (here, only the three-way interaction, which is not significant, so out it comes).\nLet’s get rid of that three-way interaction. This is another use for update that you might have seen in connection with multiple regression (to make small changes to a big model):\n\nmodel.2 &lt;- update(model.1, . ~ . - Species:location:disease)\ndrop1(model.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nNotice how update saved us having to write the whole model out again.\nNow the three two-way interactions are up for grabs: Species:location, Species:disease and location:disease. The last of these is the least significant, so out it comes. I did some copying and pasting, but I had to remember which model I was working with and what I was removing:\n\nmodel.3 &lt;- update(model.2, . ~ . - location:disease)\ndrop1(model.3, test = \"Chisq\")\n\n\n\n  \n\n\n\nSpecies:disease comes out, but it looks as if Species:location will have to stay:\n\nmodel.4 &lt;- update(model.3, . ~ . - Species:disease)\ndrop1(model.4, test = \"Chisq\")\n\n\n\n  \n\n\n\nSpecies:location indeed stays. That means that anything “contained in” it also has to stay, regardless of its main effect. So the only candidate for removal now is disease: not significant, out it comes:\n\nmodel.5 &lt;- update(model.4, . ~ . - disease)\ndrop1(model.5, test = \"Chisq\")\n\n\n\n  \n\n\n\nAnd now we have to stop.\nWhat does this final model mean? Well, frequency depends significantly on the Species:location combination, but not on anything else. To see how, we make a contingency table of species by location (totalling up over disease status, since that is not significant):\n\nxtabs(frequency ~ Species + location, data = tbl.3)\n\n       location\nSpecies  x  y\n      A 82 22\n      B 48 40\n\n\nMost of the species A’s are at location X, but the species B’s are about evenly divided between the two locations. Or, if you prefer (equally good): location X has mostly species A, while location Y has mostly species B. You can condition on either variable and compare the conditional distribution of the other one.\nNow, this is rather interesting, because this began as a study of disease, but disease has completely disappeared from our final model! That means that nothing in our final model has any relationship with disease. Indeed, if you check the original table, you’ll find that disease is present slightly more than it’s absent, for all combinations of species and location. That is, neither species nor location has any particular association with (effect on) disease, since disease prevalence doesn’t change appreciably if you change location, species or the combination of them.\nThe way an association with disease would show up is if a disease:something interaction had been significant and had stayed in the model, that something would have been associated with disease. For example, if the disease:Species table had looked like this:\n\ndisease &lt;- c(\"a\", \"a\", \"p\", \"p\")\nSpecies &lt;- c(\"A\", \"B\", \"A\", \"B\")\nfrequency &lt;- c(10, 50, 30, 30)\nxx &lt;- tibble(disease, Species, frequency)\nxtabs(frequency ~ disease + Species, data=xx)\n\n       Species\ndisease  A  B\n      a 10 50\n      p 30 30\n\n\nFor species A, disease is present 75% of the time, but for species B it’s present less than 40% of the time. So in this one there ought to be a significant association between disease and species:\n\nxx.1 &lt;- glm(frequency ~ disease * Species, data = xx, family = \"poisson\")\ndrop1(xx.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nAnd so there is. Nothing can come out of the model. (This is the same kind of test as a chi-squared test for association.\nThe log-linear model is a multi-variable generalization of that.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#mating-songs-in-crickets-1",
    "href": "tidying-data.html#mating-songs-in-crickets-1",
    "title": "15  Tidying data",
    "section": "15.19 Mating songs in crickets",
    "text": "15.19 Mating songs in crickets\nMale tree crickets produce “mating songs” by rubbing their wings together to produce a chirping sound. It is hypothesized that female tree crickets identify males of the correct species by how fast (in chirps per second) the male’s mating song is. This is called the “pulse rate”. Some data for two species of crickets are in link. The columns, which are unlabelled, are temperature and pulse rate (respectively) for Oecanthus exclamationis (first two columns) and Oecanthus niveus (third and fourth columns). The columns are separated by tabs. There are some missing values in the first two columns because fewer exclamationis crickets than niveus crickets were measured. The research question is whether males of the different species have different average pulse rates. It is also of interest to see whether temperature has an effect, and if so, what. Before we get to that, however, we have some data organization to do.\n\nRead in the data, allowing for the fact that you have no column names. You’ll see that the columns have names X1 through X4. This is OK.\n\nSolution\nTab-separated, so read_tsv; no column names, so col_names=FALSE:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/crickets.txt\"\ncrickets &lt;- read_tsv(my_url, col_names = FALSE)\n\nRows: 17 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (4): X1, X2, X3, X4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncrickets\n\n\n\n  \n\n\n\nAs promised.\nIf you didn’t catch the tab-separated part, this probably happened to you:\n\nd &lt;- read_delim(my_url, \" \", col_names = FALSE)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 17 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): X1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis doesn’t look good:\n\nproblems(d)\n\n\n\n  \n\n\n\nThe “expected columns” being 1 should bother you, since we know there are supposed to be 4 columns. At this point, we take a look at what got read in:\n\nd\n\n\n\n  \n\n\n\nand there you see the \\t or “tab” characters separating the values, instead of spaces. (This is what I tried first, and once I looked at this, I realized that read_tsv was what I needed.)\n\\(\\blacksquare\\)\n\nTidy these untidy data, going as directly as you can to something tidy. (Some later parts show you how it used to be done.) Begin by: (i) adding a column of row numbers, (ii) rename-ing the columns to species name, an underscore, and the variable contents (keeping pulserate as one word), and then use pivot_longer. Note that the column names encode two things.\n\nSolution\nTake this one piece of the pipeline at a time: that is, first check that you got the renaming right and looking at what you have, before proceeding to the pivot_longer. The syntax of rename is new name equals old name, and I like to split this over several lines to make it easier to read:\n\ncrickets %&gt;% \n  mutate(row=row_number()) %&gt;% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) \n\n\n\n  \n\n\n\nThe first part of each column name is the species and the second part is what was measured each time, separated by an underscore. To handle that in pivot_longer, you give two names of new columns to create (in names_to), and say what they’re separated by:\n\ncrickets %&gt;% \n  mutate(row=row_number()) %&gt;% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) %&gt;% \n  pivot_longer(-row, names_to=c(\"species\", \"measurement\"), names_sep=\"_\", values_to = \"obs\")\n\n\n\n  \n\n\n\nThis is tidy now, but we went a step too far: that column measurement should be two columns, called temperature and pulserate, which means it should be made wider. The obvious way is this:\n\ncrickets %&gt;% \n  mutate(row=row_number()) %&gt;% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) %&gt;% \n  pivot_longer(-row, names_to=c(\"species\", \"measurement\"), names_sep=\"_\", values_to = \"obs\") %&gt;% \n  pivot_wider(names_from=measurement, values_from=obs)\n\n\n\n  \n\n\n\nThe row numbers are cricket-within-species, which isn’t very meaningful, but we needed something for the pivot_wider to key on, to recognize what needed to go in which row. The way it works is it uses anything not mentioned in names_from or values_from as a “key”: each unique combination belongs in a row. Here that would be the combination of row and species, which is a good key because each species appears once with each row number.\nThis works, but a better way is to recognize that one of the variants of pivot_longer will do this all at once (something to think about when you have a longer followed by a wider). The key is that temperature and pulse rate need to be column names, so the second thing in names_to has to be that special thing .value, and you remove the values_to since it is now clear where the values are coming from:\n\ncrickets %&gt;% \n  mutate(row=row_number()) %&gt;% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) %&gt;% \n  pivot_longer(-row, names_to=c(\"species\", \".value\"), names_sep=\"_\") \n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nIf you found (b) a bit much to take in, the rest of the way we take a rather more leisurely approach towards the tidying.\n\nThese data are rather far from being tidy. There need to be three variables, temperature, pulse rate and species, and there are \\(14+17=31\\) observations altogether. This one is tricky in that there are temperature and pulse rate for each of two levels of a factor, so I’ll suggest combining the temperature and chirp rate together into one thing for each species, then pivoting them longer (“combining”), then pivoting them wider again (“splitting”). Create new columns, named for each species, that contain the temperature and pulse rate for that species in that order, united together. For the rest of this question, start from the data frame you read in, and build a pipe, one or two steps at a time, to save creating a lot of temporary data frames.\nSolution\nBreathe, and then begin. unite creates new columns by joining together old ones:13\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4)\n\n\n\n  \n\n\n\nNote that the original columns X1:X4 are gone, which is fine, because the information we needed from them is contained in the two new columns. unite by default uses an underscore to separate the joined-together values, which is generally safe since you won’t often find those in data.\nDigression: unite-ing with a space could cause problems if the data values have spaces in them already. Consider this list of names:\n\nnames &lt;- c(\"Cameron McDonald\", \"Durwin Yang\", \"Ole Gunnar Solskjaer\", \"Mahmudullah\")\n\nTwo very former students of mine, a Norwegian soccer player, and a Bangladeshi cricketer. Only one of these has played for Manchester United:\n\nmanu &lt;- c(F, F, T, F)\n\nand let’s make a data frame:\n\nd &lt;- tibble(name = names, manu = manu)\nd\n\n\n\n  \n\n\n\nNow, what happens if we unite those columns, separating them by a space?\n\nd %&gt;% unite(joined, name:manu, sep = \" \")\n\n\n\n  \n\n\n\nIf we then try to separate them again, what happens?\n\nd %&gt;%\n  unite(joined, name:manu, sep = \" \") %&gt;%\n  separate(joined, c(\"one\", \"two\"), \" \")\n\nWarning: Expected 2 pieces. Additional pieces discarded in 3 rows [1, 2, 3].\n\n\n\n\n  \n\n\n\nThings have gotten lost: most of the original values of manu and some of the names. If we use a different separator character, either choosing one deliberately or going with the default underscore, everything works swimmingly:\n\nd %&gt;%\n  unite(joined, name:manu, sep = \":\") %&gt;%\n  separate(joined, c(\"one\", \"two\"), \":\")\n\n\n\n  \n\n\n\nand we are back to where we started.\nIf you run just the unite line (move the pipe symbol to the next line so that the unite line is complete as it stands), you’ll see what happened.\n\\(\\blacksquare\\)\n\nThe two columns exclamationis and niveus that you just created are both temperature-pulse rate combos, but for different species. Collect them together into one column, labelled by species. (This is a straight tidyr pivot_longer, even though the columns contain something odd-looking.)\n\nSolution\nThus, this, naming the new column temp_pulse since it contains both of those things. Add to the end of the pipe you started building in the previous part:\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4) %&gt;%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", values_to = \"temp_pulse\")\n\n\n\n  \n\n\n\nYep. You’ll see both species of crickets, and you’ll see some missing values at the bottom, labelled, at the moment, NA_NA.\nThis is going to get rather long, but don’t fret: we debugged the two unite lines before, so if you get any errors, they must have come from the pivot_longer. So that would be the place to check.\n\\(\\blacksquare\\)\n\nNow split up the temperature-pulse combos at the underscore, into two separate columns. This is separate. When specifying what to separate by, you can use a number (“split after this many characters”) or a piece of text, in quotes (“when you see this text, split at it”).\n\nSolution\nThe text to split by is an underscore (in quotes), since unite by default puts an underscore in between the values it pastes together. Glue the separate onto the end. We are creating two new variables temperature and pulse_rate:\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4) %&gt;%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", values_to = \"temp_pulse\") %&gt;% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\")\n\n\n\n  \n\n\n\nYou’ll note that unite and separate are opposites (“inverses”) of each other, but we haven’t just done something and then undone it, because we have a pivot_longer in between; in fact, arranging it this way has done precisely the tidying we wanted.\n\\(\\blacksquare\\)\n\nAlmost there. Temperature and pulse rate are still text (because unite turned them into text), but they should be numbers. Create new variables that are numerical versions of temperature and pulse rate (using as.numeric). Check that you have no extraneous variables (and, if necessary, get rid of the ones you don’t want). (Species is also text and really ought to be a factor, but having it as text doesn’t seem to cause any problems.) You can, if you like, use parse_number instead of as.numeric. They should both work. The distinction I prefer to make is that parse_number is good for text with a number in it (that we want to pull the number out of), while as.numeric is for turning something that looks like a number but isn’t one into a genuine number.14\n\nSolution\nmutate-ing into a column that already exists overwrites the variable that’s already there (which saves us some effort here).\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4) %&gt;%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", values_to = \"temp_pulse\") %&gt;% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\") %&gt;%\n  mutate(temperature = as.numeric(temperature)) %&gt;%\n  mutate(pulse_rate = as.numeric(pulse_rate)) -&gt; crickets.1\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `temperature = as.numeric(temperature)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `pulse_rate = as.numeric(pulse_rate)`.\nCaused by warning:\n! NAs introduced by coercion\n\ncrickets.1\n\n\n\n  \n\n\n\nI saved the data frame this time, since this is the one we will use for our analysis.\nThe warning message tells us that we got genuine missing-value NAs back, which is probably what we want. Specifically, they got turned from missing text to missing numbers!15 The R word “coercion” means values being changed from one type of thing to another type of thing. (We’ll ignore the missings and see if they cause us any trouble. The same warning messages will show up on graphs later.) So I have 34 rows (including three rows of missings) instead of the 31 rows I would have liked. Otherwise, success.\nThere is (inevitably) another way to do this. We are doing the as.numeric twice, exactly the same on two different columns, and when you are doing the same thing on a number of columns, here a mutate with the same function, you have the option of using across. This is the same idea that we used way back to compute numerical summaries of a bunch of columns:\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4) %&gt;%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", \n               values_to = \"temp_pulse\") %&gt;% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\") %&gt;%\n  mutate(across(c(temperature, pulse_rate), \\(x) as.numeric(x)))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(c(temperature, pulse_rate), function(x) as.numeric(x))`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\n\n  \n\n\n\nCan’t I just say that these are columns 2 and 3?\n\ncrickets %&gt;%\n  unite(exclamationis, X1:X2) %&gt;%\n  unite(niveus, X3:X4) %&gt;%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", \n               values_to = \"temp_pulse\") %&gt;% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\") %&gt;%\n  mutate(across(2:3, \\(x) as.numeric(x)))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(2:3, function(x) as.numeric(x))`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\n\n  \n\n\n\nYes. Equally good. What goes into the across is the same as can go into a select: column numbers, names, or any of those “select helpers” like starts_with.\nYou might think of using across here on the quantitative columns, but remember the reason for doing this: all the columns are text, before you convert temperature and pulse rate to numbers, and so there’s no way to pick out just the two columns you want that way.\nCheck that the temperature and pulse rate columns are now labelled dbl, which means they actually are decimal numbers (and don’t just look like decimal numbers).\nEither way, using unite and then separate means that all the columns we created we want to keep (or, all the ones we would have wanted to get rid of have already been gotten rid of).\nNow we could actually do some statistics. That we do elsewhere.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#number-1-songs-1",
    "href": "tidying-data.html#number-1-songs-1",
    "title": "15  Tidying data",
    "section": "15.20 Number 1 songs",
    "text": "15.20 Number 1 songs\nThe data file link contains a lot of information about songs popular in 2000. This dataset is untidy. Our ultimate aim is to answer “which song occupied the #1 position for the largest number of weeks?”. To do that, we will build a pipe that starts from the data frame read in from the URL above, and finishes with an answer to the question. I will take you through this step by step. Each part will involve adding something to the pipe you built previously (possibly after removing a line or two that you used to display the previous result).\n\nRead the data and display what you have.\n\nSolution\n\nbillboard &lt;- read_csv(\"http://stat405.had.co.nz/data/billboard.csv\")\n\nRows: 317 Columns: 83\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (3): artist.inverted, track, genre\ndbl  (66): year, x1st.week, x2nd.week, x3rd.week, x4th.week, x5th.week, x6th...\nlgl  (11): x66th.week, x67th.week, x68th.week, x69th.week, x70th.week, x71st...\ndate  (2): date.entered, date.peaked\ntime  (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThere are a lot of columns. What does this look like?\n\nbillboard\n\n\n\n  \n\n\n\nOn yours, you will definitely see a little arrow top right saying “there are more columns”, and you will have to click on it several times to see them all. A lot of the ones on the right will be missing.\n\\(\\blacksquare\\)\n\nThe columns x1st.week through x76th.week contain the rank of each song in the Billboard chart in that week, with week 1 being the first week that the song appeared in the chart. Convert all these columns into two: an indication of week, called week, and of rank, called rank. Most songs appeared in the Billboard chart for a lot less than 76 weeks, so there are missing values, which you want to remove. (I say “indication of week” since this will probably be text at the moment). Display your new data frame. Do you have fewer columns? Why do you have a lot more rows? Explain briefly.\n\nSolution\nAs is often the case, the first step is pivot_longer, to reduce all those columns to something easier to deal with. The columns we want to make longer are the ones ending in “week”:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T)\n\n\n\n  \n\n\n\nThe “values” (ranks) have missings in them, which we wanted to get rid of.\nThere are now only 9 columns, a lot fewer than we started with. This is (I didn’t need you to say) because we have collected together all those week columns into one (a column called rank with an indication of which week it came from). The logic of the pivot_longer is that all those columns contain ranks (which is what make them the same), but they are ranks from different weeks (which is what makes them different).\nWhat has actually happened is that we have turned “wide” format into “long” format. This is not very insightful, so I would like you to go a bit further in your explanation. The original data frame encodes the rank of each song in each week, and what the pivot_longer has done is to make that explicit: in the new data frame, each song’s rank in each week appears in one row, so that there are as many rows as there are song-week combinations. The original data frame had 317 songs over 76 weeks, so this many:\n\n317 * 76\n\n[1] 24092\n\n\nsong-week combinations.\nNot every song appeared in the Billboard chart for 76 weeks, so our tidy data frame has a lot fewer rows than this.\nYou need to say that the original data frame had each song appearing once (on one line), but now each song appears on multiple rows, one for each week that the song was in the chart. Or something equivalent to that.\n\\(\\blacksquare\\)\n\nBoth your week and rank columns are (probably) text. Create new columns that contain just the numeric values, and display just your new columns, again adding onto the end of your pipe. If it so happens that rank is already a number, leave it as it is.\n\nSolution\nMy rank is already a number, so I could leave it; for later, I make a copy of it called rakn_numbe. The week has a number in it, which I can extract using parse_number:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nThe meaning of your week-number column is that it refers to the number of weeks after the song first appeared in the Billboard chart. That is, if a song’s first appearance (in date.entered) is July 24, then week 1 is July 24, week 2 is July 31, week 3 is August 7, and so on. Create a column current by adding the appropriate number of days, based on your week number, to date.entered. Display date.entered, your week number, and current to show that you have calculated the right thing. Note that you can add a number of days onto a date and you will get another date.\n\nSolution\nThere is a (small) gotcha here: if you read carefully, you’ll see that “week 1” is actually “week 0” in terms of the number of days to add on to date.entered. So you have to subtract one from the number of weeks before you multiply it by seven to get a number of days. After that thinking, this:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  select(date.entered, week_number, current)\n\n\n\n  \n\n\n\nDon’t forget to use your week-turned-into-number, or else it won’t work! (This bit me too, so you don’t need to feel bad.)\nYou can also combine the three column-definition statements into one mutate. It doesn’t matter; as soon as you have defined a column, you can use it in defining another column, even within the same mutate.\nAnyway, the rows displayed are all week_number 1, so the current date should be the same as date.entered, and is. (These are all the first week that a song is in the Billboard chart).\nYou might be thinking that this is not much of a check, and you would be right. A handy trick is to display a random sample of 10 (say) out of the 5,000-odd rows of the data frame. To do that, add the line sample_n(10) on the end, like this:\n\nbillboard %&gt;%\n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(\n    week_number = parse_number(week),\n    rank_number = rank\n  ) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  select(date.entered, week_number, current) %&gt;%\n  sample_n(10)\n\n\n\n  \n\n\n\nThis gives a variety of rows to check. The first current should be \\(7-1=6\\) weeks, or about a month and a half, after the date the song entered the chart, and so it is; the second and third ones should be \\(18-1=17\\) weeks after entry, which is very close to a third of a year (\\(17 \\times 3 = 51\\)), or four months. November to March is indeed four months. The fourth one is the first week on the charts, so the current date and the date entered should be (and are) the same. And so on.\nYour random selection of rows is likely to be different from mine, but the same kind of thinking will enable you to check whether it makes sense.\n\\(\\blacksquare\\)\n\nReaching the #1 rank on the Billboard chart is one of the highest accolades in the popular music world. List all the songs that reached rank 1. For these songs, list the artist (as given in the data set), the song title, and the date(s) for which the song was ranked number 1. Arrange the songs in date order of being ranked #1. Display all the songs (I found 55 of them).\n\nSolution\nTo the previous pipe, add the last lines below. You can use either rank (text) or what I called rank_number (a number). It doesn’t matter here, since we are only checking for equal-to, not something like “less than”:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  filter(rank == 1) %&gt;%\n  arrange(current) %&gt;%\n  select(artist.inverted, track, current)\n\n\n\n  \n\n\n\nYou’ll see the first ten rows, as here, but with clickable buttons to see the next 10 (and the previous 10 if you have moved beyond 1–10). The “artist” column is called artist.inverted because, if the artist is a single person rather than a group, their last name is listed first. The song title appears in the column track.\nThe song by Destiny’s Child spills into 2001 because it entered the chart in 2000, and the data set keeps a record of all such songs until they drop out of the chart. I’m not sure what happened to the song that was #1 on January 8, 2000; maybe it entered the chart in 199916 and so is not listed here.\n\\(\\blacksquare\\)\n\nUse R to find out which song held the #1 rank for the largest number of weeks. For this, you can assume that the song titles are all unique (if it’s the same song title, it’s the same song), but the artists might not be (for example, Madonna might have had two different songs reach the #1 rank). The information you need is in the output you obtained for the previous part, so it’s a matter of adding some code to the end of that. The last mark was for displaying only the song that was ranked #1 for the largest number of weeks, or for otherwise making it easy to see which song it was.\n\nSolution\nThis is a question of using count, but on the track title:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  filter(rank == 1) %&gt;%\n  arrange(current) %&gt;%\n  select(artist.inverted, track, current) %&gt;%\n  count(track)\n\n\n\n  \n\n\n\nThen you can scan down the n column, find that the biggest number is 11, and say: it’s the song “Independent Women Part I” by Destiny’s Child. This is 3 points (out of 4, when the question was to be handed in).\nBut, this is a data frame, so anything we can do to a data frame we can do to this, like listing out only the row(s) where n is equal to its maximum value:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  filter(rank == 1) %&gt;%\n  arrange(current) %&gt;%\n  select(artist.inverted, track, current) %&gt;%\n  count(track) %&gt;% \n  filter(n == max(n))\n\n\n\n  \n\n\n\nor arranging them in (most logically, descending) order by n to make it easier to pick out the top one:\n\nbillboard %&gt;% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %&gt;% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  filter(rank == 1) %&gt;%\n  arrange(current) %&gt;%\n  select(artist.inverted, track, current) %&gt;%\n  count(track) %&gt;% \n  arrange(desc(n))\n\n\n\n  \n\n\n\nEither of those would have netted you the 4th point.\nIf you want to be a little bit more careful, you can make an artist-track combination as below. This would catch occasions where the same song by two different artists made it to #1, or two different songs that happened to have the same title did. It’s not very likely that the same artist would record two different songs with the same title, though it is possible that the same song by the same artist could appear in the Billboard chart on two different occasions.17\nI think I want to create an artist-song combo fairly early in my pipe, and then display that later, something like this. This means replacing track by my combo later in the pipe, wherever it appears:\n\nbillboard %&gt;%\n  pivot_longer(x1st.week:x76th.week, names_to = \"week\", values_to = \"rank\", values_drop_na = T) %&gt;%\n  mutate(\n    week_number = parse_number(week),\n    rank_number = rank\n  ) %&gt;%\n  mutate(combo = paste(track, artist.inverted, sep = \" by \")) %&gt;%\n  mutate(current = date.entered + (week_number - 1) * 7) %&gt;%\n  filter(rank == 1) %&gt;%\n  arrange(current) %&gt;%\n  select(combo, current) %&gt;%\n  count(combo) %&gt;%\n  arrange(desc(n))\n\n\n\n  \n\n\n\nI don’t think it makes any difference here, but it might in other years, or if you look over several years where you might get cover versions of the same song performed by different artists.\nZero-point bonus: how many of these artists have you heard of? How many have your parents heard of? (I followed popular music quite closely much earlier than this, in the early 1980s in the UK. I remember both Madonna and U2 when they first became famous. U2’s first single was called “Fire” and it just scraped into the UK top 40. Things changed after that.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#bikes-on-college-1",
    "href": "tidying-data.html#bikes-on-college-1",
    "title": "15  Tidying data",
    "section": "15.21 Bikes on College",
    "text": "15.21 Bikes on College\nThe City of Toronto collects all kinds of data on aspects of life in the city. See link. One collection of data is records of the number of cyclists on certain downtown streets. The data in link are a record of the cyclists on College Street on the block west from Huron to Spadina on September 24, 2010. In the spreadsheet, each row relates to one cyclist. The first column is the time the cyclist was observed (to the nearest 15 minutes). After that, there are four pairs of columns. The observer filled in (exactly) one X in each pair of columns, according to whether (i) the cyclist was male or female, (ii) was or was not wearing a helmet, (iii) was or was not carrying a passenger on the bike, (iv) was or was not riding on the sidewalk. We want to create a tidy data frame that has the time in each row, and has columns containing appropriate values, often TRUE or FALSE, for each of the four variables measured.\nI will lead you through the process, which will involve developing a (long) pipe, one step at a time.\n\nTake a look at the spreadsheet (using Excel or similar: this may open when you click the link). Are there any obvious header rows? Is there any extra material before the data start? Explain briefly.\n\nSolution\nThis is what I see (you should see something that looks like this):\n\nThere are really two rows of headers (the rows highlighted in yellow). The actual information that says what the column pair is about is in the first of those two rows, and the second row indicates which category of the information above this column refers to. This is not the usual way that the column headers encode what the columns are about: we are used to having one column gender that would take the values female or male, or a column helmet containing the values yes or no. (You might be sensing pivot_longer here, which may be one way of tackling this, but I lead you into another idea below.) There are also six lines above the highlighted ones that contain background information about this study. (This is where I got the information about the date of the study and which block of which street it is about.) I am looking for two things: the apparent header line is actually two lines (the ones in yellow), and there are extra lines above that which are not data.\n\\(\\blacksquare\\)\n\nRead the data into an R data frame. Read without headers, and instruct R how many lines to skip over using skip= and a suitable number. When this is working, display the first few lines of your data frame. Note that your columns have names X1 through X9.\n\nSolution\nThe actual data start on line 9, so we need to skip 8 lines. col_names=F is the way to say that we have no column names (not ones that we want to use, anyway). Just typing the name of the data frame will display “a few” (that is, 10) lines of it, so that you can check it for plausibleness:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/bikes.csv\"\nbikes &lt;- read_csv(my_url, skip = 8, col_names = FALSE)\n\nRows: 1958 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): X2, X3, X4, X5, X6, X7, X8, X9\ntime (1): X1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbikes\n\n\n\n  \n\n\n\nThis seems to have worked: a column with times in it, and four pairs of columns, with exactly one of each pair having an X in it. The variable names X1 through X9 were generated by read_csv, as it does when you read in data with col_names=FALSE. The times are correctly times, and the other columns are all text. The blank cells in the spreadsheet have appeared in our data frame as “missing” (NA). The notation &lt;NA&gt; means “missing text” (as opposed to a missing number, say).\nThe first line in our data frame contains the first 7:00 (am) cyclist, so it looks as if we skipped the right number of lines.\n\\(\\blacksquare\\)\n\nWhat do you notice about the times in your first column? What do you think those “missing” times should be?\n\nSolution\nThere are some times and some missing values. It seems a reasonable guess that the person recording the data only recorded a time when a new period of 15 minutes had begun, so that the missing times should be the same as the previous non-missing one: For example, the first five rows are cyclists observed at 7:00 am (or, at least, between 7:00 and 7:15). So they should be recorded as 7:00, and the ones in rows 7–10 should be recorded as 7:15, and so on.\n\\(\\blacksquare\\)\n\nFind something from the tidyverse that will fill18 in those missing values with the right thing. Start a pipe from the data frame you read in, that updates the appropriate column with the filled-in times.\n\nSolution\nfill from tidyr fills in the missing times with the previous non-missing value. (This will mean finding the help for fill in R Studio or online.) I told you it was a giveaway. If you look in the help for fill via ?fill (or if you Google tidyr::fill, which is the full name for “the fill that lives in tidyr”), you’ll see that it requires up to two things (not including the data frame): a column to fill, and a direction to fill it (the default of “down” is exactly what we want). Thus:\n\nbikes %&gt;% fill(X1)\n\n\n\n  \n\n\n\nSuccess!\nWe will probably want to rename X1 to something like time, so let’s do that now before we forget. There is a rename that does about what you’d expect:\n\nbikes %&gt;% fill(X1) %&gt;% rename(Time = X1)\n\n\n\n  \n\n\n\nThe only thing I keep forgetting is that the syntax of rename is “new name equals old name”. Sometimes I think it’s the other way around, and then I wonder why it doesn’t work.\nI gave it a capital T so as not to confuse it with other things in R called time.\n\\(\\blacksquare\\)\n\nR’s ifelse function works like =IF in Excel. You use it to create values for a new variable, for example in a mutate. The first input to it is a logical condition (something that is either true or false); the second is the value your new variable should take if the condition is true, and the third is the value of your new variable if the condition is false. Create a new column gender in your data frame that is “male” or “female” depending on the value of your X2 column, using mutate. (You can assume that exactly one of the second and third columns has an X in it.) Add your code to the end of your pipe and display (the first 10 rows of) the result.\n\nSolution\nUnder the assumption we are making, we only have to look at column X2 and we ignore X3 totally:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(X2 == \"X\", \"male\", \"female\"))\n\n\n\n  \n\n\n\nOh, that didn’t work. The gender column is either male or missing; the two missing ones here should say female. What happened? Let’s just look at our logical condition this time:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(isX = (X2 == \"X\"))\n\n\n\n  \n\n\n\nThis is not true and false, it is true and missing. The idea is that if X2 is missing, we don’t (in general) know what its value is: it might even be X! So if X2 is missing, any comparison of it with another value ought to be missing as well.\nThat’s in general. Here, we know where those missing values came from: they were blank cells in the spreadsheet, so we actually have more information.\nPerhaps a better way to go is to test whether X2 is missing (in which case, it’s a female cyclist). R has a function is.na which is TRUE if the thing inside it is missing and FALSE if the thing inside it has some non-missing value. In our case, it goes like this:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\"))\n\n\n\n  \n\n\n\nOr you can test X3 for missingness: if missing, it’s male, otherwise it’s female. That also works.\nThis made an assumption that the person recording the X’s actually did mark an X in exactly one of the columns. For example, the columns could both be missing, or both have an X in them. This gives us more things to check, at least three. ifelse is good for something with only two alternatives, but when you have more, case_when is much better.19 Here’s how that goes. Our strategy is to check for three things: (i) X2 has an X and X3 is missing; (ii) X2 is missing and X3 has an X; (iii) anything else, which is an error:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = case_when(\n    X2 == \"X\" & is.na(X3) ~ \"Male\",\n    is.na(X2) & X3 == \"X\" ~ \"Female\",\n    TRUE                  ~ \"Error!\"\n  ))\n\n\n\n  \n\n\n\nIt seems nicest to format it with the squiggles lining up, so you can see what possible values gender might take.\nThe structure of the case_when is that the thing you’re checking for goes on the left of the squiggle, and the value you want your new variable to take goes on the right. What it does is to go down the list of conditions that you are checking for, and as soon as it finds one that is true, it grabs the value on the right of the squiggle and moves on to the next row. The usual way to write these is to have a catch-all condition at the end that is always true, serving to make sure that your new variable always gets some value. TRUE is, um, always true. If you want an English word for the last condition of your case_when, “otherwise” is a nice one.\nI wanted to check that the observer did check exactly one of V2 and V3 as I asserted, which can be done by gluing this onto the end:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = case_when(\n    X2 == \"X\" & is.na(X3) ~ \"Male\",\n    is.na(X2) & X3 == \"X\" ~ \"Female\",\n    TRUE ~ \"Error!\"\n  )) %&gt;%\n  count(gender)\n\n\n\n  \n\n\n\nThere are only Males and Females, so the observer really did mark exactly one X. (As a bonus, you see that there were slightly more male cyclists than female ones.)\nExtra: I was wondering how pivot_longer would play out here. The way to do it seems to be to rename the columns we want first, and get rid of the others:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  rename(male = X2, female = X3) %&gt;% \n  select(-starts_with(\"X\"))\n\n\n\n  \n\n\n\nEach row should have one X and one missing in it, so we may as well drop the missings as we pivot-longer:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  rename(male = X2, female = X3) %&gt;% \n  select(-starts_with(\"X\")) %&gt;% \n  pivot_longer(-Time, names_to=\"gender\", values_to=\"observed\", \n               values_drop_na = TRUE)\n\n\n\n  \n\n\n\nThe observed column is kind of pointless, since its value is always X. But we do have a check: the previous data frame had 1958 rows, with an X in either the male or the female column. This data frame has the gender of each observed cyclist in the gender column, and it also has 1958 rows. So, either way, that’s how many cyclists were observed in total.\n\\(\\blacksquare\\)\n\nCreate variables helmet, passenger and sidewalk in your data frame that are TRUE if the “Yes” column contains X and FALSE otherwise. This will use mutate again, but you don’t need ifelse: just set the variable equal to the appropriate logical condition. As before, the best way to create these variables is to test the appropriate things for missingness. Note that you can create as many new variables as you like in one mutate. Show the first few lines of your new data frame. (Add your code onto the end of the pipe you made above.)\n\nSolution\nOn the face of it, the way to do this is to go looking for X’s:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %&gt;%\n  mutate(\n    helmet = (X4 == \"X\"),\n    passenger = (X6 == \"X\"),\n    sidewalk = (X8 == \"X\")\n  )\n\n\n\n  \n\n\n\nBut, we run into the same problem that we did with gender: the new variables are either TRUE or missing, never FALSE.\nThe solution is the same: look for the things that are missing if the cyclist is wearing a helmet, carrying a passenger or riding on the sidewalk. These are X5, X7, X9 respectively:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %&gt;%\n  mutate(\n    helmet = is.na(X5),\n    passenger = is.na(X7),\n    sidewalk = is.na(X9)\n  )\n\n\n\n  \n\n\n\nAgain, you can do the mutate all on one line if you want to, or all four variable assignments in one mutate, but I used newlines and indentation to make the structure clear.\nIt is less elegant, though equally good for the purposes of the assignment, to use ifelse for these as well, which would go like this, for example:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(X2 == \"X\", \"male\", \"female\")) %&gt;%\n  mutate(helmet = ifelse(is.na(X5), TRUE, FALSE))\n\n\n\n  \n\n\n\nand the same for passenger and sidewalk. The warning is, whenever you see a TRUE and a FALSE in an ifelse, that you could probably get rid of the ifelse and use the logical condition directly.20\nFor gender, though, you need the ifelse (or a case_when) because the values you want it to take are male and female, something other than TRUE and FALSE.\nI like to put brackets around logical conditions when I am assigning them to a variable or defining new columns containing them. If I don’t, I get something like\n\nhelmet &lt;- V4 == \"X\"\n\nwhich actually works, but is hard to read. Well, I think it works. Let’s check:\n\nexes &lt;- c(\"X\", \"\", \"X\", \"\", \"X\")\ny &lt;- exes == \"X\"\ny\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE\n\n\nYes it does. But I would not recommend writing it this way, because unless you are paying attention, you won’t notice that == is testing for “logically equal” rather than putting something in a column.\nIt works because of a thing called “operator precedence”: the logical-equals is evaluated first, and the result of that is saved in the variable. But unless you or your readers remember that, it’s better to write\n\ny &lt;- (exes == \"X\")\n\nto draw attention to the order of calculation. This is the same reason that\n\n4 + 5 * 6\n\n[1] 34\n\n\nevaluates this way rather than doing the addition first and getting 54. BODMAS and all that.\nThe pivot_longer approach works for these too. Rename the columns as yes and no, and then give the names_to column a name like helmet. Give the values_to column a name like what2, to make it easier to remove later. And then do the same with the others, one pivot_longer at a time. (Keep all the columns, and then discard them at the end if you want. That way you don’t risk deleting something you might need later.)\n\\(\\blacksquare\\)\n\nFinally (for the data manipulation), get rid of all the original columns, keeping only the new ones that you created. Save the results in a data frame and display its first few rows.\n\nSolution\nThis is a breath of fresh air after all the thinking needed above: this is just select, added to the end:\n\nmybikes &lt;- bikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %&gt;%\n  mutate(\n    helmet = is.na(X5),\n    passenger = is.na(X7),\n    sidewalk = is.na(X9)\n  ) %&gt;%\n  select(-(X2:X9))\nmybikes\n\n\n\n  \n\n\n\nYou might not have renamed your X1, in which case, you still have it, but need to keep it (because it holds the times).\nAnother way to do this is to use a “select-helper”, thus:\n\nbikes %&gt;%\n  fill(X1) %&gt;%\n  rename(Time = X1) %&gt;%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %&gt;%\n  mutate(\n    helmet = is.na(X5),\n    passenger = is.na(X7),\n    sidewalk = is.na(X9)\n  ) %&gt;%\n  select(-num_range(\"X\", 2:9))\n\n\n\n  \n\n\n\nThis means “get rid of all the columns whose names are X followed by a number 2 through 9”.\nThe pipe looks long and forbidding, but you built it (and tested it) a little at a time. Which is how you do it.\n\\(\\blacksquare\\)\n\nThe next few parts are a quick-fire analysis of the data set. They can all be solved using count. How many male and how many female cyclists were observed in total?\n\nSolution\nI already got this one when I was checking for observer-notation errors earlier:\n\nmybikes %&gt;% count(gender)\n\n\n\n  \n\n\n\n861 females and 1097 males.\n\\(\\blacksquare\\)\n\nHow many male and female cyclists were not wearing helmets?\n\nSolution\nYou can count two variables at once, in which case you get counts of all combinations of them:\n\nmybikes %&gt;% count(gender, helmet)\n\n\n\n  \n\n\n\n403 females and 604 males were not wearing helmets, picking out what we need.\nThe real question of interest here is “what proportion of male and female cyclists were not wearing helmets?”21 This has a rather elegant solution that I will have to explain. First, let’s go back to the group_by and summarize version of the count here:\n\nmybikes %&gt;%\n  group_by(gender, helmet) %&gt;%\n  summarize(the_count = n())\n\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nThat’s the same table we got just now. Now, let’s calculate a proportion and see what happens:\n\nmybikes %&gt;%\n  group_by(gender, helmet) %&gt;%\n  summarize(the_count = n()) %&gt;%\n  mutate(prop = the_count / sum(the_count))\n\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nWe seem to have the proportions of males and females who were and were not wearing a helmet, and you can check that this is indeed the case, for example:\n\n403 / (403 + 458)\n\n[1] 0.4680604\n\n\n47% of females were not wearing helmets, while 55% of males were helmetless. (You can tell from the original frequencies that a small majority of females wore helmets and a small majority of males did not.)\nNow, we have to ask ourselves: how on earth did that work?\nWhen you calculate a summary (like our sum(count) above), it figures that you can’t want the sum by gender-helmet combination, since you already have those in count. You must want the sum over something. What? What happens is that it goes back to the group_by and “peels off” the last thing there, which in this case is helmet, leaving only gender. It then sums the counts for each gender, giving us what we wanted.\nIt just blows my mind that someone (ie., Hadley Wickham) could (i) think that this would be a nice syntax to have (instead of just being an error), (ii) find a way to implement it and (iii) find a nice logical explanation (“peeling off”) to explain how it worked.\nWhat happens if we switch the order of the things in the group_by?\n\nmybikes %&gt;%\n  group_by(helmet, gender) %&gt;%\n  summarize(the_count = n()) %&gt;%\n  mutate(prop = the_count / sum(the_count))\n\n`summarise()` has grouped output by 'helmet'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nNow we get the proportion of helmeted riders of each gender, which is not the same as what we had before. Before, we had “out of males” and “out of females”; now we have “out of helmeted riders” and “out of helmetless riders”. (The riders with helmets are almost 50–50 males and females, but the riders without helmets are about 60% male.)\nThis is row and column proportions in a contingency table, B22 style.\nNow, I have to see whether the count variant of this works:\n\nmybikes %&gt;%\n  count(gender, helmet) %&gt;%\n  mutate(prop = n / sum(n))\n\n\n\n  \n\n\n\nIt doesn’t. Well, it kind of does, but it divided by the sum of all of them rather than “peeling off”, so these are overall proportions rather than row or column proportions.\nSo I think you have to do this the group_by and summarize way.\n\\(\\blacksquare\\)\n\nHow many cyclists were riding on the sidewalk and carrying a passenger?\n\nSolution\nNot too many, I’d hope. Again:\n\nmybikes %&gt;% count(passenger, sidewalk)\n\n\n\n  \n\n\n\nWe’re looking for the “true”, “true” entry of that table, which seems to have vanished. That means the count is zero: none at all. (There were only 5 passenger-carrying riders, and they were all on the road.)\n\\(\\blacksquare\\)\n\nWhat was the busiest 15-minute period of the day, and how many cyclists were there then?\n\nSolution\nThe obvious way is to list every 15-minute period and eyeball the largest frequency. There are quite a few 15-minute periods, so be prepared to hit Next a few times (or use View):\n\nmybikes %&gt;% count(Time) \n\n\n\n  \n\n\n\n17:15, or 5:15 pm, with 128 cyclists.\nBut, computers are meant to save us that kind of effort. How? Note that the output from count is itself a data frame, so anything you can do to a data frame, you can do to it: for example, display only the rows where the frequency equals the maximum frequency:\n\nmybikes %&gt;%\n  count(Time) %&gt;%\n  filter(n == max(n))\n\n\n\n  \n\n\n\nThat will actually display all the times where the cyclist count equals the maximum, of which there might be more than one.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#feeling-the-heat-1",
    "href": "tidying-data.html#feeling-the-heat-1",
    "title": "15  Tidying data",
    "section": "15.22 Feeling the heat",
    "text": "15.22 Feeling the heat\nIn summer, the city of Toronto issues Heat Alerts for “high heat or humidity that is expected to last two or more days”. The precise definitions are shown at link. During a heat alert, the city opens Cooling Centres and may extend the hours of operation of city swimming pools, among other things. All the heat alert days from 2001 to 2016 are listed at link.\nThe word “warning” is sometimes used in place of “alert” in these data. They mean the same thing.22\n\nRead the data into R, and display the data frame. Note that there are four columns:\n\n\na numerical id (numbered upwards from the first Heat Alert in 2001; some of the numbers are missing)\nthe date of the heat alert, in year-month-day format with 4-digit years.\na text code for the type of heat alert\ntext describing the kind of heat alert. This can be quite long.\n\nSolution\nA .csv, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/heat.csv\"\nheat &lt;- read_csv(my_url)\n\nRows: 200 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): code, text\ndbl  (1): id\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nheat\n\n\n\n  \n\n\n\nYou might get a truncated text as I did, or you might have to click to see more of it. In any case, we won’t be using the text, so you can just forget about it from here on.\n\\(\\blacksquare\\)\n\nIn your data frame, are the dates stored as genuine dates or as text? How can you tell?\n\nSolution\nLook at the top of the column on your display of the data frame. Under the date column it says date rather than chr (which means “text”), so these are genuine dates. This happened because the data file contained the dates in year-month-day order, so read_csv read them in as dates. (If they had been in some other order, they would have been read in as text and we would need to use lubridate to make them into dates.)\n\\(\\blacksquare\\)\n\nWhich different heat alert codes do you have, and how many of each?\n\nSolution\ncount, most easily:\n\nheat %&gt;% count(code)\n\n\n\n  \n\n\n\nAlternatively, group_by and summarize:\n\nheat %&gt;% group_by(code) %&gt;% summarize(count = n())\n\n\n\n  \n\n\n\n(note that n() gives the number of rows, in each group if you have groups.)\nThere are six different codes, but EHAD only appears once.\n\\(\\blacksquare\\)\n\nUse the text in your dataset (or look back at the original data file) to describe briefly in your own words what the various codes represent.\n\nSolution\nYou can check that each time a certain code appears, the text next to it is identical. The six codes and my brief descriptions are:\n\nEHA: (Start of) Extended Heat Alert\nEHAD: Extreme Heat Alert downgraded to Heat Alert\nEHAE: Extended Heat Alert continues\nHA: (Start of) Heat Alert\nHAE: Heat Alert continues\nHAU: Heat Alert upgraded to Extended Heat Alert\n\nI thought there was such a thing as an Extreme Heat Alert, but here the word is (usually) Extended, meaning a heat alert that extends over several days, long in duration rather than extremely hot. The only place Extreme occurs is in EHAD, which only occurs once. I want your answer to say or suggest something about whether a code applies only to continuing heat alerts (ie., that EHAD, EHAE, HAE and HAU are different from the others).\n\\(\\blacksquare\\)\n\nHow many (regular and extended) heat alert events are there altogether? A heat alert event is a stretch of consecutive days, on all of which there is a heat alert or extended heat alert. Hints: (i) you can answer this from output you already have; (ii) how can you tell when a heat alert event starts?\n\nSolution\nThis turned out to be more messed-up than I thought. There is a detailed discussion below. The codes EHAD, EHAE, HAE, HAU all indicate that there was a heat alert on the day before. Only the codes HA and EHA can indicate the start of a heat alert (event). The problem is that HA and EHA sometimes indicate the start of a heat alert event and sometimes one that is continuing. You can check by looking at the data that HA and EHA days can (though they don’t always: see below) have a non-heat-alert day before (below) them in the data file: for example, August 4, 2012 is an HA day, but August 3 of that year was not part of any kind of heat alert. I had intended the answer to be this:\n\nSo we get the total number of heat alert events by totalling up the number of HA and EHA days: \\(59+93=152\\).\n\nThis is not right because there are some consecutive EHA days, eg. 5–8 July 2010, so that EHA sometimes indicates the continuation of an extended heat alert and sometimes the start of one. I was expecting EHA to be used only for the start, and one of the other codes to indicate a continuation. The same is (sometimes) true of HA. So reasonable answers to the question as set include:\n\n93, the number of HAs\n59, the number of EHAs\n152, the number of HAs and EHAs combined\n“152 or less”, “between 93 and 152”, ``between 59 and 152’’ to reflect that not all of these mark the start of a heat alert event.\n\nAny of these, or something similar with an explanation of how you got your answer, are acceptable. In your career as a data scientist, you will often run into this kind of thing, and it will be your job to do something with the data and explain what you did so that somebody else can decide whether they believe you or not. A good explanation, even if it is not correct, will help you get at the truth because it will inspire someone to say “in fact, it goes this way”, and then the two of you can jointly figure out what’s actually going on. Detailed discussion follows. If you have any ambitions of working with data, you should try to follow the paragraphs below, because they indicate how you would get an actual answer to the question.\nI think the key is the number of days between one heat alert day and the next one. dplyr has a function diff that works out exactly this. Building a pipeline, just because:\n\nheat %&gt;%\n  select(-text) %&gt;%\n  mutate(daycount = as.numeric(date)) %&gt;%\n  mutate(daydiff = abs(c(diff(daycount), 0)))\n\n\n\n  \n\n\n\nOof. I have some things to keep track of here:\n\nGet rid of the text, since it serves no purpose here.\nThe date column is a proper Date (we checked).\nThen I want the date as number of days; since it is a number of days internally, I just make it a number with as.numeric.\nThen I use diff to get the difference between each date and the previous one, remembering to glue a 0 onto the end so that I have the right number of differences.\nSince the dates are most recent first, I take the absolute value so that the daydiff values are positive (except for the one that is 0 on the end).\n\nStill with me? All right. You can check that the daydiff values are the number of days between the date on that line and the line below it. For example, there were 24 days between August 13 and September 6.\nNow, when daydiff is 1, there was also a heat alert on the previous day (the line below in the file), but when daydiff is not 1, that day must have been the start of a heat alert event. So if I count the non-1’s, that will count the number of heat alert events there were. (That includes the difference of 0 on the first day, the one at the end of the file.)\nThus my pipeline continues like this:\n\nheat %&gt;%\n  select(-text) %&gt;%\n  mutate(daycount = as.numeric(date)) %&gt;%\n  mutate(daydiff = abs(c(diff(daycount), 0))) %&gt;%\n  count(daydiff != 1)\n\n\n\n  \n\n\n\nAnd that’s how many actual heat alert events there were: 79, less even than the number of HAs. So that tells me that a lot of my HAs and EHAs were actually continuations of heat alert events rather than the start of them. I think I need to have a word with the City of Toronto about their data collection processes.\ncount will count anything that is, or can be made into, a categorical variable. It doesn’t have to be one of the columns of your data frame; here it is something that is either TRUE or FALSE about every row of the data frame.\nOne step further: what is the connection between the codes and the start of heat alert events? We can figure that out now:\n\nheat %&gt;%\n  select(-text) %&gt;%\n  mutate(daycount = as.numeric(date)) %&gt;%\n  mutate(daydiff = abs(c(diff(daycount), 0))) %&gt;%\n  mutate(start = (daydiff != 1)) %&gt;%\n  count(code, start)\n\n\n\n  \n\n\n\nI made a column start that is TRUE at the start of a heat alert event and FALSE otherwise, by comparing the days from the previous heat alert day with 1. Then I can make a table, or, as here, the dplyr equivalent with count.23 Or group_by and summarize. What this shows is that EHAD, EHAE, HAE and HAU never go with the start of a heat alert event (as they shouldn’t). But look at the HAs and EHAs. For the HAs, 73 of them go with the start of an event, but 20 do not. For the EHAs, just 6 of them go with the start, and 53 do not. (Thus, counting just the HAs was very much a reasonable thing to do.)\nThe 79 heat alert events that we found above had 73 of them starting with an HA, and just 6 starting with an EHA. I wasn’t quite sure how this would come out, but I knew it had something to do with the number of days between one heat alert day and the next, so I calculated those first and then figured out what to do with them.\n\\(\\blacksquare\\)\n\nWe are going to investigate how many heat alert days there were in each year. To do that, we have to extract the year from each of our dates.\n\nSolution\nThis will need the lubridate package, but you don’t need to load it specifically because it is now loaded with the tidyverse:\n\nheat %&gt;% select(-text) %&gt;% mutate(year = year(date)) %&gt;% sample_n(10)\n\n\n\n  \n\n\n\nThat seems to have worked. I listed a random sample of rows to get back to previous years. Having convinced myself that it worked, let me save it:\n\nheat %&gt;% select(-text) %&gt;% mutate(year = year(date)) -&gt; heat\n\n\\(\\blacksquare\\)\n\nCount the number of heat alert days for each year, by tabulating the year variable. Looking at this table, would you say that there have been more heat alert days in recent years? Explain (very) briefly.\n\nSolution\nCount them again:\n\nheat %&gt;% count(year)\n\n\n\n  \n\n\n\nThere are various things you could say, most of which are likely to be good. My immediate reaction is that most of the years with a lot of heat-alert days are in the last few years, and most of the years with not many are near the start, so there is something of an upward trend. Having said that, 2014 is unusually low (that was a cool summer), and 2005 was unusually high. (Was that the summer of the big power outage? I forget.24\nYou could also reasonably say that there isn’t much pattern: the number of heat-alert days goes up and down. In fact, anything that’s not obviously nonsense will do.\nI was thinking about making a graph of these frequencies against year, and sticking some kind of smooth trend on it. This uses the output we just got, which is itself a data frame:\n\nheat %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n)) + geom_point() + geom_smooth(se = F)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe pattern is very scattered, as is commonly the case with environmental-science-type data, but there is a very small upward trend. So it seems that either answer is justified, either “there is no trend” or “there is something of an upward trend”.\nThe other thing I notice on this plot is that if there are a lot of heat-alert days one year, there will probably also be a lot in the next year (and correspondingly if the number of heat-alert days is below average: it tends to be below average again in the next year). This pattern is known to time-series people as “autocorrelation” and indicates that the number of heat-alert days in one year and the next is not independent: if you know one year, you can predict the next year. (Assessment of trend and autocorrelation are hard to untangle properly.)\nExtra 1: I learn from Environmental Science grad students (of whom we have a number at UTSC) that the approved measure of association is called the Mann-Kendall correlation, which is the Kendall correlation of the data values with time. In the same way that we use the sign test when we doubt normality, and it uses the data more crudely but safely, the regular (so-called Pearson) correlation assumes normality (of the errors in the regression of one variable on the other), and when you doubt that (as you typically do with this kind of data) you compute a different kind of correlation with time. What the Kendall correlation does is to take each pair of observations and ask whether the trend with time is uphill or downhill. For example, there were 3 heat-alert days in 2009, 16 in 2010 and 12 in 2011. Between 2009 and 2010, the trend is uphill (increasing with time), and also between 2009 and 2011 (there were more heat-alert days in the later year), but between 2010 and 2011 the trend is downhill. The idea of the Kendall correlation is you take all the pairs of points, of which there are typically rather a lot, count up how many pairs are uphill and how many downhill, and apply a formula to get a correlation between \\(-1\\) and 1. (If there are about an equal number of uphills and downhills, the correlation comes out near 0; if they are mostly uphill, the correlation is near 1, and if they are mostly downhill, the correlation is near \\(-1\\).) It doesn’t matter how uphill or downhill the trends are, only the number of each, in the same way that the sign test only counts the number of values above or below the hypothesized median, not how far above or below they are.\nThis can be calculated, and even tested:\n\nheat %&gt;%\n  count(year) %&gt;%\n  with(., cor.test(year, n, method = \"kendall\"))\n\nWarning in cor.test.default(year, n, method = \"kendall\"): Cannot compute exact\np-value with ties\n\n\n\n    Kendall's rank correlation tau\n\ndata:  year and n\nz = 0.31612, p-value = 0.7519\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n0.05907646 \n\n\nThe Mann-Kendall correlation is a thoroughly unremarkable 0.06, and with only 16 data points, a null hypothesis that the correlation is zero is far from being rejected, P-value 0.7519 as shown. So this is no evidence of a time trend at all.\nExtra 2: I’d like to say a word about how I got these data. They came from link. If you take a look there, there are no obvious rows and columns. This format is called JSON. Look a bit more carefully and you’ll see stuff like this, repeated:\n\n{\"id\":\"232\",\"date\":\"2016-09-08\",\"code\":\"HAU\",\n\"text\":\"Toronto's Medical Officer of Health has upgraded the Heat Warning to an Extended Heat Warning\"}\n\none for each heat alert day. These are “keys” (on the left side of the :) and “values” (on the right side).25 The keys are column headers (if the data were in a data frame) and the values are the data values that would be in that column. In JSON generally, there’s no need for the keys to be the same in every row, but if they are, as they are here, the data can be arranged in a data frame. How? Read on.\nI did this in R, using a package called jsonlite, with this code:\n\nlibrary(jsonlite)\nurl &lt;- \"http://app.toronto.ca/opendata/heat_alerts/heat_alerts_list.json\"\nheat &lt;- fromJSON(url, simplifyDataFrame = T)\nhead(heat)\nwrite_csv(heat, \"heat.csv\")\n\nAfter loading the package, I create a variable url that contains the URL for the JSON file. The fromJSON line takes something that is JSON (which could be in text, a file or a URL) and converts it to and saves it in a data frame. Finally, I save the data frame in a .csv file. That’s the .csv file you used. If you run that code, you’ll get a .csv file of heat alerts right up to the present, and you can update my analysis.\nWhy .csv? If I had used write_delim, the values would have been separated by spaces. But, the text is a sentence of several words, which are themselves separated by spaces. I could have had you read in everything else and not the text, and then separated-by-spaces would have been fine, but I wanted you to see the text so that you could understand the code values. So .csv is what it was.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#isoflavones-1",
    "href": "tidying-data.html#isoflavones-1",
    "title": "15  Tidying data",
    "section": "15.23 Isoflavones",
    "text": "15.23 Isoflavones\nThe plant called kudzu was imported to the US South from Japan. It is rich in isoflavones, which are believed to be beneficial for bones. In a study, rats were randomly assigned to one of three diets: one with a low dose of isoflavones from kudzu, one with a high dose, and a control diet with no extra isoflavones. At the end of the study, each rat’s bone density was measured, in milligrams per square centimetre. The data as recorded are shown in http://ritsokiguess.site/isoflavones.txt.26 There are 15 observations for each treatment, and hence 45 altogether.\nHere are some code ideas you might need to use later, all part of the tidyverse. You may need to find out how they work.\n\ncol_names (in the read_ functions)\nconvert (in various tidyverse functions)\nfill\nna_if\nrename\nseparate_rows\nskip (in the read_ functions)\nvalues_drop_na (in the pivot_ functions)\n\nIf you use any of these, cite the webpage(s) or other source(s) where you learned about them.\n\nTake a look at the data file. Describe briefly what you see.\n\nSolution\nThe data values are (at least kind of) aligned in columns, suggesting read_table. There are up to six bone density values in each row, with a header that spans all of them (by the looks of it). The treatment column looks all right except that some of the rows are blank. The blank treatments are the same as the ones in the row(s) above them, you can infer, because there are 15 observations for each treatment, six, six, and then three. (This is how a spreadsheet is often laid out: blank means the same as the previous line.)27\nThis, you might observe, will need some tidying.\n\\(\\blacksquare\\)\n\nRead in the data, using read_table, and get it into a tidy form, suitable for making a graph. This means finishing with (at least) a column of treatments with a suitable name (the treatments will be text) and a column of bone density values (numbers), one for each rat. You can have other columns as well; there is no obligation to get rid of them. Describe your process clearly enough that someone new to this data set would be able to understand what you have done and reproduce it on another similar dataset. Before you begin, think about whether or not you want to keep the column headers that are in the data file or not. (It can be done either way, but one way is easier than the other.)\n\nSolution\nThe tidying part is a fair bit easier to see if you do not read the column headers. A clue to this is that bone_mineral_density is not aligned with the values (of bone mineral density) below it. The next question is how to do that. You might remember col_names=FALSE from when the data file has no column headers at all, but here it does have headers; we just want to skip over them. Keep reading in the documentation for read_table, and you’ll find an option skip that does exactly that, leading to:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/isoflavones.txt\"\nbmd0a &lt;- read_table(my_url, col_names = FALSE, skip = 1)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  X1 = col_character(),\n  X2 = col_double(),\n  X3 = col_double(),\n  X4 = col_double(),\n  X5 = col_double(),\n  X6 = col_double(),\n  X7 = col_double()\n)\n\n\nWarning: 6 parsing failures.\nrow col  expected    actual                                                 file\n  2  -- 7 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  3  -- 7 columns 3 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  5  -- 7 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  6  -- 7 columns 3 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  8  -- 7 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n... ... ......... ......... ....................................................\nSee problems(...) for more details.\n\nbmd0a\n\n\n\n  \n\n\n\nIf you miss the skip, the first row of “data” will be those column headers that were in the data file, and you really don’t want that. This link talks about both col_names and skip.\nThis, however, is looking very promising. A pivot_longer will get those columns of numbers into one column, which we can call something like bmd, and but, not so fast. What about those blank treatments in X1? The first two blank ones are control, the next two are low_dose and the last two are high_dose. How do we fill them in? The word “fill” might inspire you to read up on fill. Except that this doesn’t quite work, because it replaces missings with the non-missing value above them, and we have blanks, not missings.\nAll right, can we replace the blanks with missings, and then fill those? This might inspire you to go back to the list of ideas in the question, and find out what na_if does: namely, exactly this! Hence:\n\nbmd0a %&gt;% mutate(X1=na_if(X1, \"\")) %&gt;% \nfill(X1) \n\n\n\n  \n\n\n\nRun this one line at a time to see how it works. fill takes a column with missing values to replace, namely X1, and na_if takes two things: a column containing some values to make NA, and the values that should be made NA, namely the blank ones.\nSo that straightens out the treatment column. It needs renaming; you can do that now, or wait until later. I’m going to wait on that.\nYou need to organize the treatment column first, before you do the pivot_longer, or else that won’t work.28\nNow, we need to get one column of bone mass densities, instead of six. This you’ll recognize as a standard pivot_longer, with one tweak: those missing values in X5 through X7, which we want to get rid of. You might remember that this is what values_drop_na does:\n\nbmd0a %&gt;% mutate(X1=na_if(X1, \"\")) %&gt;% \nfill(X1) %&gt;% \npivot_longer(X2:X7, names_to=\"old\", values_to=\"bmd\", values_drop_na=TRUE)\n\n\n\n  \n\n\n\nIf you didn’t think of values_drop_na, do the pivot without, and then check that you have too many rows because the missings are still there (there are 45 rats but you have 54 rows), so add a drop_na() to the end of your pipe. The only missing values are in the column I called bmd.\nThis is almost there. We have a numeric column of bone mass densities, a column called old that we can ignore, and a treatment column with a stupid name that we can fix. I find rename backwards: the syntax is new name equals old name, so you start with the name that doesn’t exist yet and finish with the one you want to get rid of:\n\nbmd0a %&gt;% mutate(X1=na_if(X1, \"\")) %&gt;% \nfill(X1) %&gt;% \npivot_longer(X2:X7, names_to=\"old\", values_to=\"bmd\", values_drop_na=TRUE) %&gt;% \nrename(treatment=X1) -&gt; bmd1b\nbmd1b\n\n\n\n  \n\n\n\nDone!\nThe best way to describe this kind of work is to run your pipeline up to a point that needs explanation, describe what comes next, and then run the whole pipeline again up to the next point needing explanation, rinse and repeat. (This avoids creating unnecessary temporary dataframes, since the purpose of the pipe is to avoid those.)\nThe guideline for description is that if you don’t know what’s going to happen next, your reader won’t know either. For me, that was these steps:\n\nread the data file without row names and see how it looks\nfix up the treatment column (convincing myself and the reader that we were now ready to pivot-longer)\ndo the pivot_longer and make sure it worked\nrename the treatment column\n\nSo, I said there was another way. This happens to have a simple but clever solution. It starts from wondering “what happens if I read the data file with column headers, the normal way? Do it and find out:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/isoflavones.txt\"\nbmd0b &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  treatment = col_character(),\n  bone_mineral_density = col_double()\n)\n\n\nWarning: 9 parsing failures.\nrow col  expected    actual                                                 file\n  1  -- 2 columns 7 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  2  -- 2 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  3  -- 2 columns 3 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  4  -- 2 columns 7 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  5  -- 2 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n... ... ......... ......... ....................................................\nSee problems(...) for more details.\n\nbmd0b\n\n\n\n  \n\n\n\nThis looks … strange. There are two column headers, and so there are two columns. It so happened that this worked because the text bone_mineral_density is long enough to span all the columns of numbers. That second column is actually text: six or three numbers as text with spaces between them.\nThe first thing is, as before, to fill in the missing treatments, which is as above, but changing some names:\n\nbmd0b %&gt;% mutate(treatment=na_if(treatment, \"\")) %&gt;% \nfill(treatment) \n\n\n\n  \n\n\n\nThe way we learned in class for dealing with this kind of thing is separate. It is rather unwieldy here since we have to split bone_mineral_density into six (temporary) things:\n\nbmd0b %&gt;% mutate(treatment=na_if(treatment, \"\")) %&gt;% \nfill(treatment) %&gt;% \nseparate(bone_mineral_density, into = c(\"z1\", \"z2\", \"z3\", \"z4\", \"z5\", \"z6\"))\n\nWarning: Expected 6 pieces. Missing pieces filled with `NA` in 9 rows [1, 2, 3, 4, 5, 6,\n7, 8, 9].\n\n\n\n\n  \n\n\n\nThis works, though if you check, there’s a warning that some of the rows don’t have six values. However, these have been replaced by missings, which is just fine. From here, we do exactly what we did before: pivot-longer all the columns I called z-something, and get rid of the missings.\nHaving thought of separate, maybe you’re now wondering what separate_rows does. It turns out that it bypasses the business of creating extra columns and then pivoting them longer, thus:\n\nbmd0b %&gt;% mutate(treatment=na_if(treatment, \"\")) %&gt;% \nfill(treatment)  %&gt;% \nseparate_rows(bone_mineral_density, convert = TRUE) -&gt; bmd1a\nbmd1a\n\n\n\n  \n\n\n\nBoom! This takes all the things in that mess in bone_mineral_density, splits them up into individual data values, and puts them one per row back into the same column. The convert is needed because otherwise the values in the second column would be text and you wouldn’t be able to plot them. (If you don’t see that, use a mutate to convert the column into the numerical version of itself.)\n\\(\\blacksquare\\)\n\nThe statistician on this study is thinking about running an ordinary analysis of variance to compare the bone mineral density for the different treatments. Obtain a plot from your tidy dataframe that will help her decide whether that is a good idea.\n\nSolution\nThe key issues here are whether the values within each treatment group are close enough to normally distributed, and, if they are, whether the spreads are close enough to equal. The best plot is therefore a normal quantile plot of each of the three groups, in facets. You can do this without scales=\"free\":\n\nggplot(bmd1b, aes(sample=bmd)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~treatment)\n\n\n\n\nThe value of doing it this way is that you also get a sense of variability, from the slopes of the lines, or from how much of each box is filled vertically. (Here, the high-dose values are more spread-out than the other two groups, which are similar in spread.)\nYou could also do it with scales = \"free\":\n\nggplot(bmd1b, aes(sample=bmd)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~treatment, scales = \"free\")\n\n\n\n\nThe value of doing it this way is that you fill the facets (what I called “not wasting real estate” on an earlier assignment), and so you get a better assessment of normality, but the downside is that you will need another plot, for example a boxplot (see below) to assess equality of spreads if you are happy with the normality.\nI’m happy with either way of making the normal quantile plots, as long as you have a reason for your choice, coming from what you will be using the normal quantile plot for. You might not think of saying that here as you do it, but when you do the next part, you may realize that you need to assess equality of spreads, and in that case you should come back here and add a reason for using or not using scales = \"free\".\nThe next-best graph here is boxplots:\n\nggplot(bmd1b, aes(x=treatment, y=bmd)) + geom_boxplot()\n\n\n\n\nThis is not so good because it doesn’t address normality as directly (just giving you a general sense of shape). On the other hand, you can assess spread directly with a boxplot; see discussion above.\nThe grader is now probably thoroughly confused, so let me summarize possible answers in order of quality:\n\nA normal quantile plot of all three groups, using scales = \"free\" or not, with a good reason. (If with scales = \"free\", and there needs to be a comparison of spread, there needs to be a boxplot or similar below as well. That’s what I meant by “any additional graphs” in the next part.)\nA normal quantile plot of all three groups, using scales = \"free\" or not, without a good reason.\nA side-by-side boxplot. Saying in addition that normality doesn’t matter so much because we have moderate-sized samples of 15 and therefore that boxplots are good enough moves this answer up a place.\n\nNote that getting the graph is (relatively) easy once you have the tidy data, but is impossible if you don’t! This is the way the world of applied statistics works; without being able to get your data into the right form, you won’t be able to do anything else. This question is consistent with that fact; I’m not going to give you a tidy version of the data so that you can make some graphs. The point of this question is to see whether you can get the data tidy enough, and if you can, you get the bonus of being able to do something straightforward with it.\n\\(\\blacksquare\\)\n\nBased on your graph, and any additional graphs you wish to draw, what analysis would you recommend for this dataset? Explain briefly. (Don’t do the analysis.)\n\nSolution\nMake a decision about normality first. You need all three groups to be sufficiently normal. I don’t think there’s any doubt about the high-dose and low-dose groups; these are if anything short-tailed, which is not a problem for the ANOVA. You might find that the control group is OK too; make a call. Or you might find it skewed to the right, something suggested rather more by the boxplot. My take, from looking at the normal quantile plot, is that the highest value in the control group is a little too high, but with a sample size of 15, the Central Limit Theorem will take care of that. For yourself, you can find a bootstrapped sampling distribution of the sample mean for the control group and see how normal it looks.\nIf you are not happy with the normality, recommend Mood’s median test.\nIf you are OK with the normality, you need to assess equal spreads. You can do this from a boxplot, where the high-dose group clearly has bigger spread. Or, if you drew normal quantile plots without scales = \"free\", compare the slopes of the lines. This means that you need to recommend a Welch ANOVA.\nIf your normal quantile plots looked like this:\n\nggplot(bmd1b, aes(sample=bmd)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~treatment, scales = \"free\")\n\n\n\n\nthe only way to assess spread is to make another plot, and for this job, the boxplot is best.\nExtra 1: the bootstrapped sampling distribution of the sample mean for the control group goes this way:\n\nbmd1b %&gt;% \nfilter(treatment == \"control\") -&gt; d\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(d$bmd, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nNo problems there. The Welch ANOVA is fine.\nExtra 2: You might be curious how the analysis comes out. Here is Welch:\n\noneway.test(bmd~treatment, data=bmd1b)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  bmd and treatment\nF = 2.5385, num df = 8.000, denom df = 7.718, p-value = 0.1082\n\n\nNot all the same means, so use Games-Howell to explore:\n\ngamesHowellTest(bmd~factor(treatment), data = bmd1b)\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\n\n\n    Pairwise comparisons using Games-Howell test\n\n\ndata: bmd by factor(treatment)\n\n\n          198  209  218 226  232 245  control high_dose\n209       0.75 -    -   -    -   -    -       -        \n218       -    -    -   -    -   -    -       -        \n226       0.73 1.00 -   -    -   -    -       -        \n232       -    -    -   -    -   -    -       -        \n245       0.20 0.41 -   0.51 -   -    -       -        \ncontrol   0.13 0.80 -   0.97 -   0.77 -       -        \nhigh_dose 0.15 0.51 -   0.70 -   1.00 0.97    -        \nlow_dose  0.18 0.89 -   0.99 -   0.72 1.00    0.94     \n\n\n\nP value adjustment method: none\n\n\nalternative hypothesis: two.sided\n\n\nHigh dose is significantly different from both the other two, which are not significantly different from each other.\nMood’s median test, for comparison:\n\nmedian_test(bmd1b, bmd, treatment)\n\n$table\n           above\ngroup       above below\n  198           0     2\n  209           1     3\n  218           1     1\n  226           2     3\n  232           1     1\n  245           4     1\n  control       3     2\n  high_dose     4     2\n  low_dose      2     3\n\n$test\n       what     value\n1 statistic 6.0666667\n2        df 8.0000000\n3   P-value 0.6397643\n\n\nNot any significant differences, although it is a close thing.\nThe table of aboves and belows suggests the same thing as the Welch test: the high-dose values are mainly high, and the others are mostly low. But with these sample sizes it is not strong enough evidence. My guess is that the median test is lacking power compared to the Welch test; having seen that the Welch test is actually fine, it is better to use that here.29\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#jockos-garage-1",
    "href": "tidying-data.html#jockos-garage-1",
    "title": "15  Tidying data",
    "section": "15.24 Jocko’s Garage",
    "text": "15.24 Jocko’s Garage\nInsurance adjusters are concerned that Jocko’s Garage is giving estimates for repairing car damage that are too high. To see whether this is indeed the case, ten cars that had been in collisions were taken to both Jocko’s Garage and another garage, and the two estimates for repair were recorded. The data as recorded are here.\n\nTake a look at the data file (eg. by using your web browser). How are the data laid out? Do there appear to be column headers?\n\nSolution\nThe data are laid out in aligned columns, so that we will need to use read_table to read it in. There are no column headers, since there is no line at the top of the file saying what each column represents. (The fact that I was asking about column headers is kind of a clue that something non-standard is happening there.)\n\\(\\blacksquare\\)\n\nRead in and display the data file, bearing in mind what you just concluded about it. What names did the columns acquire?\n\nSolution\nAs mentioned above, you’ll need read_table, plus col_names=FALSE to not read the first row as column names:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/jocko.txt\"\ncars0 &lt;- read_table(my_url, col_names = FALSE)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  X1 = col_character(),\n  X2 = col_character(),\n  X3 = col_double(),\n  X4 = col_double(),\n  X5 = col_double(),\n  X6 = col_double(),\n  X7 = col_double()\n)\n\ncars0\n\n\n\n  \n\n\n\nThe column names have become X1 through X7. You’ll need to work with these in a minute, so it is good to be aware of that now.\nI used a “temporary” name for my dataframe since we are going to be tidying it before we do anything with it, and I’m saving the “good” name cars for the tidy one.\n\\(\\blacksquare\\)\n\nMake this data set tidy. That is, you need to end up with columns containing the repair cost estimates at each of the two garages and also identifying the cars, with each observation on one row. Describe your thought process. (It needs to be possible for the reader to follow your description and understand why it works.) Save your tidy dataframe.\n\nSolution\nThis looks very far from tidy right now. The things in X2 look like they will need to be variable names eventually, but there are two copies of them, and there are also five columns of data values that need eventually to become three. Having all the data values in one column might be a useful place to start:\n\ncars0 %&gt;% pivot_longer(X3:X7, names_to=\"old_cols\", values_to=\"values\") \n\n\n\n  \n\n\n\nThis is tidier, but it’s now too long: this has 30 rows but there are only 10 cars, or, depending on your point of view, there are 20 observations on 10 individual cars, so you could justify (in some way) having 20 rows, but not 30.\nNow, therefore, we need to pivot wider. But to get to this point, we had to try pivoting longer to see what it did, and then go from there. I don’t think it’s at all obvious that this is what will happen, so I think you need to do a pivot-longer first, talk about it, and then move on.\nFrom here, we want to make columns whose names are the things in X2, and whose values are the things in values. This is exactly what pivot_wider does, so add that to our pipe:\n\ncars0 %&gt;% pivot_longer(X3:X7, names_to=\"names\", values_to=\"values\") %&gt;% \npivot_wider(names_from = X2, values_from = values) -&gt; cars\ncars\n\n\n\n  \n\n\n\nThis is now tidy: one row for each of the 10 cars, one column containing the repair estimates for each car at each of the two garages, and a column identifying the cars. I think this is best because this is a matched-pairs study, and so you want the two measurements for each individual car in columns next to each other (for t.test with paired=TRUE).\nI think it is best to show the whole pipeline here, even though you are making R work a little harder, rather than having to make up a temporary variable name for the output from pivot_longer (that you are never going to look at again after this).\nIf you thought there were 20 observations, you have a bit more work to do (that you will have to undo later to get the right graph), namely:\n\ncars %&gt;% pivot_longer(c(Jocko, Other), names_to=\"garage\", values_to=\"estimate\") -&gt; cars1\ncars1\n\n\n\n  \n\n\n\nThis would be the right thing to do if you had independent observations (that is, 20 different cars, and you randomly choose a garage to send each one to). But you can have a car assessed for repair without actually repairing it, so it makes more sense to send each car to both garages, and compare like with like. Compare the kids learning to read; once a child has learned to read, you can’t teach them to read again, so that study had to be done with two independent samples.\nExtra: I thought about starting by making the dataframe even wider:\n\ncars0 %&gt;% pivot_wider(names_from = X2, values_from = X3:X7)\n\n\n\n  \n\n\n\nThis is sort of the right thing, but there are repeat columns now, depending on where the data values came from in cars0. What we want to do now is some kind of pivot_longer, creating three columns called Car, Jocko, and Other. If we only had one kind of thing to make longer, this would be a standard pivot_longer. But we have three. There are two “extras” to pivot_longer that will get you to the right place. The first one is to give multiple inputs to names_to, because the column names encode two things: where in the original data frame the value came from (which is now junk to us), and what the value actually represents, which we definitely do want to keep. I don’t have a good name for it, though, so I’ll call it z for now. Note that we need a names_sep that says what the two things in the column names are separated by, the underscore that the pivot_wider put in there:\n\ncars0 %&gt;% pivot_wider(names_from = X2, values_from = X3:X7) %&gt;% \npivot_longer(-X1, names_to = c(\"junk\", \"z\"), names_sep=\"_\")\n\n\n\n  \n\n\n\nThis is now exactly what I got by starting with pivot_longer, and so the same pivot_wider that I finished with before will tidy this up:\n\ncars0 %&gt;% pivot_wider(names_from = X2, values_from = X3:X7) %&gt;% \npivot_longer(-X1, names_to = c(\"junk\", \"z\"), names_sep=\"_\") %&gt;% \npivot_wider(names_from = z, values_from = value)\n\n\n\n  \n\n\n\nThis is now tidy, so you have achieved what you set out to do, but you have not done it the best way, so you should expect to lose a little something.\nThis kind of longer-then-wider happens often enough that there is an option in pivot_longer to do it in one step. Let’s remind ourselves of where we were:\n\ncars0 %&gt;% pivot_wider(names_from = X2, values_from = X3:X7) \n\n\n\n  \n\n\n\nThe second part of those funky column names needs to become the names of our new columns. To make that happen in one step, you put the special indicator .value in where we had z before:\n\ncars0 %&gt;% pivot_wider(names_from = X2, values_from = X3:X7) %&gt;% \npivot_longer(-X1, names_to = c(\"junk\", \".value\"), names_sep=\"_\")\n\n\n\n  \n\n\n\nand as if by magic, we have tidiness. It’s best to discover this and do it in two steps, though by starting with pivot_wider you have made it more difficult for yourself. By starting with pivot_longer, it is a very standard longer-then-wider, and there is nothing extra you have to learn. (The column X1 I added to the data so that pivot_wider would work smoothly. See what happens if you remove it with select(-X1) before you start pivoting.)\nThere is usually a relatively simple way to do these, and if your way is complicated, that is an invitation to try it again a different way. I don’t think there’s a way to do it in one step, though, because those things in X2 have to get to column names somehow, and they can only do so by being attached to which original column the values came from.\nAll of these ideas are here, which is a dense read, but worth working through to see what is possible. This problem is of the type in “Longer, then wider”.\n\\(\\blacksquare\\)\n\nMake a suitable graph to assess the comparison of interest, and say briefly what your graph tells you.\n\nSolution\nYou might be tempted to look at cars, see two quantitative variables, and think “scatterplot”:\n\nggplot(cars, aes(x=Jocko, y=Other)) + geom_point()\n\n\n\n\nThis says that a repair that is more expensive at one garage is more expensive at the other as well, which is true, but it’s an answer to the wrong question. We care about whether Jocko’s Garage is more expensive than the other one on the same car. To rescue the scatterplot, you can add the line \\(y=x\\) to the graph and see which side of the line the points are, which you might have to find out how to do:\n\nggplot(cars, aes(x=Jocko, y=Other)) + geom_point() + geom_abline(slope = 1, intercept = 0)\n\n\n\n\nMore of the points are below and to the right of the line, indicating that Jocko’s Garage is typically more expensive (in the cases where the other garage is more expensive, there is not much in it).\nThere is a more direct approach here, based on the idea that a matched pairs test looks at the differences between the two estimates for each car: work out the differences, and make a one-sample plot of them:\n\ncars %&gt;% mutate(diff=Jocko-Other) %&gt;% \nggplot(aes(x=diff)) + geom_histogram(bins = 4)\n\n\n\n\nMost of the differences, this way around, are positive, so the indication is that Jocko’s Garage is indeed more expensive. Don’t have too many bins.\nA one-sample boxplot of the differences would also work:\n\ncars %&gt;% mutate(diff=Jocko-Other) %&gt;% \nggplot(aes(x=1, y=diff)) + geom_boxplot()\n\n\n\n\nThis tells you that at least 75% of the differences are positive.\nIf you ended up with my cars1:\n\ncars1\n\n\n\n  \n\n\n\nthis is “obviously” a boxplot:\n\nggplot(cars1, aes(x=garage, y=estimate)) + geom_boxplot()\n\n\n\n\nexcept that you have not used the fact that each group is measurements on the same 10 cars. Here is a way to rescue that:\n\nggplot(cars1, aes(x=garage, y=estimate, group=Car)) + geom_point() + geom_line()\n\n\n\n\nThe majority of the lines go downhill, so Jocko’s Garage is more expensive most of the time. (The lines are really another way to look at the differences.) This last graph I would be good with, since it shows which pairs of measurements are related because of being on the same cars.\n\\(\\blacksquare\\)\n\nCarry out a test to make an appropriate comparison of the mean estimates. What do you conclude, in the context of the data?\n\nSolution\nComparing means requires the right flavour of \\(t\\)-test, in this case a matched-pairs one, with a one-sided alternative, since we were concerned that the Jocko estimates were bigger. In a matched pairs test, alternative says how the first column you name compares with the other one. If your columns are the opposite way to mine, your alternative needs to be “less”:\n\nwith(cars, t.test(Jocko, Other, paired = TRUE, alternative = \"greater\"))\n\n\n    Paired t-test\n\ndata:  Jocko and Other\nt = 2.8749, df = 9, p-value = 0.009164\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 40.76811      Inf\nsample estimates:\nmean difference \n          112.5 \n\n\nRemember that this flavour of \\(t\\)-test doesn’t take a data=, so you need to use with or dollar signs.\nThe P-value is actually just less than 0.01, so we can definitely conclude that the Jocko estimates are bigger on average.\nIf you calculated the differences earlier, feel free to use them here:\n\ncars %&gt;% mutate(diff=Jocko-Other) %&gt;% \nwith(., t.test(diff, mu=0, alternative = \"greater\"))\n\n\n    One Sample t-test\n\ndata:  diff\nt = 2.8749, df = 9, p-value = 0.009164\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 40.76811      Inf\nsample estimates:\nmean of x \n    112.5 \n\n\nSaving the data frame with the differences in it is probably smart.\nAgain, if you got to cars1, you might think to do this:\n\nt.test(estimate~garage, data=cars1, alternative=\"greater\")\n\n\n    Welch Two Sample t-test\n\ndata:  estimate by garage\nt = 0.32056, df = 17.798, p-value = 0.3761\nalternative hypothesis: true difference in means between group Jocko and group Other is greater than 0\n95 percent confidence interval:\n -496.4343       Inf\nsample estimates:\nmean in group Jocko mean in group Other \n             1827.5              1715.0 \n\n\nbut you would be wrong, because the two groups are not independent (they’re the same cars at each garage). You have also lost the significant result, because some of the repairs are more expensive than others (at both garages), and this introduces extra variability that this test does not account for.\nI said to compare the means, so I don’t want a sign test here. If you think we should be doing one, you’ll need to make the case for it properly: first, calculate and plot the differences and make the case that they’re not normal enough. I see left-skewness in the histogram of differences, but personally I don’t find this bad enough to worry about. If you do, make that case (but, a sample of size 10 even from a normal distribution might look this skewed) and then run the right test:\n\ncars %&gt;% mutate(diff=Jocko-Other) %&gt;% \nsign_test(diff, 0)\n\n$above_below\nbelow above \n    2     7 \n\n$p_values\n  alternative    p_value\n1       lower 0.98046875\n2       upper 0.08984375\n3   two-sided 0.17968750\n\n\nThe upper-tail P-value is the one you want (explain why), and this is not quite significant. This is different from the correct \\(t\\)-test for a couple of reasons: there is probably not much power with this small sample, and the two estimates that are higher at the Other garage are not much higher, which the \\(t\\)-test accounts for but the sign test does not.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#tidying-electricity-consumption-1",
    "href": "tidying-data.html#tidying-electricity-consumption-1",
    "title": "15  Tidying data",
    "section": "15.25 Tidying electricity consumption",
    "text": "15.25 Tidying electricity consumption\nHow does the consumption of electricity depend on temperature? To find out, a short-term study was carried out by a utility company based in a certain area. For a period of two years, the average monthly temperature was recorded (in degrees Fahrenheit), the mean daily demand for electricity per household (in kilowatt hours), and the cost per kilowatt hour of electricity for that year (8 cents for the first year and 10 cents for the second, which it will be easiest to treat as categorical).\nThe data were laid out in an odd way, as shown in http://ritsokiguess.site/datafiles/utils.txt, in aligned columns: the twelve months of temperature were laid out on two lines for the first year, then the twelve months of consumption for the first year on the next two lines, and then four more lines for the second year laid out the same way. Thus the temperature of 31 in the first line goes with the consumption of 55 in the third line, and the last measurements for that year are the 78 at the end of the second line (temperature) and 73 at the end of the fourth line (consumption). Lines 5 through 8 of the data file are the same thing for the second year (when electricity was more expensive).\nThe data seem to have been laid out in order of temperature, rather than in order of months, which I would have thought would make more sense. But this is what we have.\n\nRead in and display the data file, bearing in mind that it has no column names.\n\nSolution\nThat means col_names = FALSE when reading in. I gave this a “disposable” name, saving the good name utils for the tidy version:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/utils.txt\"\nutils0 &lt;- read_table(my_url, col_names = FALSE)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  X1 = col_character(),\n  X2 = col_character(),\n  X3 = col_double(),\n  X4 = col_double(),\n  X5 = col_double(),\n  X6 = col_double(),\n  X7 = col_double(),\n  X8 = col_double()\n)\n\nutils0\n\n\n\n  \n\n\n\nThe columns have acquired names X1 through X8. It doesn’t really matter what these names are, but as we will see shortly, it matters that they have names.\n\\(\\blacksquare\\)\n\nArrange these data tidily, so that there is a column of price (per kilowatt hour), a column of temperatures, and a column of consumptions. Describe your process, including why you got list-columns (if you did) and what you did about them (if necessary).\n\nSolution\nThis question is asking about your process as well as your answer, so I think it’s best to build a pipeline one step at a time (which corresponds in any case to how you would figure out what to do). The first step seems to be to make longer, for example getting all those numbers in one column. I’m not quite sure what to call the new columns, so I’ll make up some names and figure things out later:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") \n\n\n\n  \n\n\n\nIf you scroll down, X2 has consumptions as well as temperatures, so we need to get that straightened out.\nThis, so far, is actually a lot like the weather one in lecture (where we had a max and a min temperature), and the solution is the same: follow up with a pivot_wider to get the temperatures and consumptions in their own columns:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \npivot_wider(names_from = X2, values_from = value) \n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} %&gt;%\n  dplyr::group_by(X1, col, X2) %&gt;%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n  dplyr::filter(n &gt; 1L)\n\n\n\n\n  \n\n\n\nExcept that it didn’t appear to work. Although it actually did. These are list-columns. I actually recorded lecture 14a to help you with this. (See also the discussion in Extra 3 of the last part of the writers question, and the word “list” at the top of temperature and consumption). Each cell holds two numbers instead of the one you were probably expecting.\nWhy did that happen? The warning above the output is a clue. Something is going to be “not uniquely identified”. Think about how pivot_wider works. It has to decide which row and column of the wider dataframe to put each value in. The column comes from the names_from: temperature or consumption. So that’s not a problem. The row comes from the combination of the other columns not named in the pivot_wider: that means the ones called X1 and col. (Another way to see that is the columns in the result from the pivot_wider that do not have values in them: not temperature or consumption, the other two.)\nIf you look back at the things in col, they go from X3 to X8, so there are six of them. There are two values in X1, so there are \\(2 \\times 6 = 12\\) combinations of the two, and so 12 rows in the wider dataframe. This has two columns, and thus \\(12 \\times 2 = 24\\) cells altogether. But there were 48 values in the longer dataframe (go back and look: it has 48 rows), so there isn’t enough room for all of them here.\nIf you go back and look at the longer dataframe, you’ll see, for example, that there are two temperature values that go with an X1 of 8 cents and a col of X3, so that they will both have to be jammed into one cell of the wider dataframe.\nThe resolution of the list-columns here is the same as in the one about the writers: unnest them, and then you can ignore the warning:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \npivot_wider(names_from = X2, values_from = value) %&gt;% \nunnest(c(temperature, consumption)) -&gt; utils\n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} %&gt;%\n  dplyr::group_by(X1, col, X2) %&gt;%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n  dplyr::filter(n &gt; 1L)\n\nutils\n\n\n\n  \n\n\n\nThere were 24 months of data, and a temperature and consumption for each, so this is now tidy and I can give it a proper name.\nExtra: if you got to here and got scared:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \npivot_wider(names_from = X2, values_from = value) \n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} %&gt;%\n  dplyr::group_by(X1, col, X2) %&gt;%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n  dplyr::filter(n &gt; 1L)\n\n\n\n\n  \n\n\n\nwhich is an entirely reasonable reaction, you might have asked yourself how you could have prevented this from happening. The problem, as discussed earlier, is with the rows, and that the X1-col combinations repeat. Let’s go back to “longer”:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") \n\n\n\n  \n\n\n\nRows 1 and 7, 2 and 8, etc, are “replicates” in that they have the same X1 and col values but different temperatures. This is because they come from the same column in the original layout of the data (the 31 and the 62 are underneath each other). This means that the first six rows are “replicate 1” and the next six are “replicate 2”. Scrolling down, we then get to 8 cents and consumption, and we need to do the same again. So if we make a column that has 1s and 2s in the right places (six 1s, six 2s, repeat), we should then have unique rows for the pivot_wider.\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \nmutate(replicate = rep(1:2, each = 6, length.out = 48))\n\n\n\n  \n\n\n\nrep does repeats like this: something to repeat (the numbers 1 through 2), how many times to repeat each one (six times), and how long the final thing has to be (48, since there were 48 rows in the longer dataframe).\nThen, this time, if we do the pivot_wider, it should give us something tidy:\n\nutils0 %&gt;% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \nmutate(replicate = rep(1:2, each = 6, length.out = 48)) %&gt;% \npivot_wider(names_from = X2, values_from = value) \n\n\n\n  \n\n\n\nand so it does, with 24 rows for the 24 months.\nAnother, perhaps easier, way to think about this (you might find it easier, anyway) is to go back to the original dataframe and make the replicate there:\n\nutils0\n\n\n\n  \n\n\n\nThe first two rows are replicates (both 8 cents and temperature), then the third and fourth, and so on. So setting a replicate column as 1, 2, 1, 2 etc should do it, and this is short enough to type directly. Do this first, then the pivot_longer, then the pivot_wider as we did before, and we should end up with something tidy:\n\nutils0 %&gt;% mutate(replicate = c(1,2,1,2,1,2,1,2)) %&gt;% \npivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %&gt;% \npivot_wider(names_from = X2, values_from = value) %&gt;% \nunnest(c(temperature, consumption)) \n\n\n\n  \n\n\n\nIf you check this, you’ll see that replicate gets turned into the same thing in the longer dataframe that we had earlier, so you can do it either way.\nThe moral of the story is that when you are planning to do a pivot-wider, you ought to devote some attention to which rows things are going into. Sometimes you can get away with just doing it and it works, but thinking about rows is how to diagnose it when it doesn’t. (The ideas below also appear in Lecture 14a.) Here’s another mini-example where the observations are matched pairs but they come to us long, like two-sample data:\n\nd &lt;- tribble(\n~obs, ~y, ~time,\n1, 10, \"pre\",\n2, 13, \"post\",\n3, 12, \"pre\",\n4, 14, \"post\",\n5, 13, \"pre\",\n6, 15, \"post\"\n)\nd %&gt;% pivot_wider(names_from = time, values_from = y)\n\n\n\n  \n\n\n\nOh. The columns are all right, but the rows certainly are not.\nThe problem is that the only thing left after y and time have been used in the pivot_wider is the column obs, and there are six values there, so there are six rows. This is, in a way, the opposite of the problem we had before; now, there is not enough data to fill the twelve cells of the wider dataframe. For example, there is no pre measurement in the row where obs is 2, so this cell of the wider dataframe is empty: it has a missing value in it.\nThe problem is that the obs column numbered the six observations 1 through 6, but really they are three groups of two observations on three people, so instead of obs we need a column called person that shows which observations are the matched pairs, like this:\n\nd &lt;- tribble(\n~person, ~y, ~time,\n1, 10, \"pre\",\n1, 13, \"post\",\n2, 12, \"pre\",\n2, 14, \"post\",\n3, 13, \"pre\",\n3, 15, \"post\"\n)\n\nNow there are going to be three rows with a pre and a post in each:\n\nd %&gt;% pivot_wider(names_from = time, values_from = y)\n\n\n\n  \n\n\n\npivot_wider requires more thinking than pivot_longer, and when it does something mysterious, that’s when you need to have some understanding of how it works, so that you can fix things up.\n\\(\\blacksquare\\)\n\nMake a suitable graph of temperature, consumption and price in your tidy dataframe. Add smooth trends if appropriate. If you were unable to get the data tidy, use my tidy version here. (If you need the other file, right-click on “here” and Copy Link Address.)\n\nSolution\nI said earlier to treat price (rather badly labelled as X1) as categorical, so we have two quantitative variables and one categorical. This suggests a scatterplot with the two prices distinguished by colours. (We don’t have a mechanism for making three-dimensional plots, and in any case if you have a quantitative variable with not that many distinct different values, you can often treat that as categorical, such as price here.)\nBefore we make a graph, though, we should rename X1. The way you might think of is to create a new column with the same values as X1, but a new name.30 Like this. Consumption is the outcome, so it goes on the \\(y\\)-axis:\n\nutils %&gt;% \nmutate(price = X1) %&gt;% \nggplot(aes(x = temperature, y = consumption, colour = price)) + \ngeom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI said smooth trends rather than lines, because you don’t know until you draw the graph whether the trends are lines. If they’re not, there’s not much point in drawing lines through them. These ones are rather clearly curves, which we take up in the next part.\nIf you fail to rename X1, that’s what will appear on the legend, and the first thing your reader would ask is “what is X1?” When writing, you need to think of your reader, since they are (in the real world) paying you for your work.\nExtra: there is an official rename also. I haven’t used that in class, so if you discover this, make sure to say where you found out about it from:\n\nutils %&gt;% \nrename(price = X1)\n\n\n\n  \n\n\n\nThe syntax is “new name equals old one”. I used to think it was something like “take the column called X1 and rename it to price”, but as you see, that’s exactly backwards. The English-language version is “create a new column called price from the column previously called X1”.\n\\(\\blacksquare\\)\n\nWhat patterns or trends do you see on your graph? Do they make practical sense? There are two things I would like you to comment on.\n\nSolution\nThe two things are:\n\nthe relationships are both curves, going down and then up again.\nthe blue curve is above the red one.\n\nIf the temperature is low (30 degrees F is just below freezing), people will need to heat their houses, and the electricity consumption to do this is reflected in the curves being higher at the left. (Not all people have electric heating, but at least some people do, and electric heating uses a lot of electricity.) When the temperature is high, people will turn on the air-conditioning (which is usually electric), and that explains the sharp increase in consumption at high temperatures. In between is the zone where the house stays a good temperature without heating or cooling.\nSo why is the blue curve above the red one? This is saying that when electricity is cheaper, people will use more of it. (This seems to be particularly true when the temperature is high; people might “crank” the air-conditioning if it doesn’t cost too much to run.) Conversely, if electricity is more expensive, people might be more thoughtful about what temperature to turn on the heating or AC. (For example, in summer you might keep the drapes closed so that it stays cooler inside without needing to use the AC so much.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#tidy-blood-pressure-1",
    "href": "tidying-data.html#tidy-blood-pressure-1",
    "title": "15  Tidying data",
    "section": "15.26 Tidy blood pressure",
    "text": "15.26 Tidy blood pressure\nGoing to the dentist is scary for a lot of people. One way in which this might show up is that people might have higher blood pressure on average before their dentist’s appointment than an hour after the appointment is done. Ten randomly-chosen individuals have their (systolic31) blood pressure measured while they are in a dentist’s waiting room, and then again one hour after their appointment is finished.\nYou might have seen a tidy version of this data set before.\nThe data as I originally received it is in http://ritsokiguess.site/datafiles/blood_pressure2.csv.\n\nRead in and display the data as originally received.\n\nSolution\nYou ought to be suspicious that something is going to be up with the layout. With that in mind, I’m using a “disposable” name for this dataframe:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/blood_pressure2.csv\"\nbp0 &lt;- read_csv(my_url)\n\nRows: 2 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): time\ndbl (10): p1, p2, p3, p4, p5, p6, p7, p8, p9, p10\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbp0\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nDescribe briefly how the data you read in is not tidy, bearing in mind how the data were collected and how they would be analysed.\n\nSolution\nIn short, the things that are rows should be columns, and the things that are columns should be rows. Or, the individuals (people, here) should be in rows but they are in columns. Or, the variables (time points) should be in columns but they are in rows. One of those.\nAnother way to go at it is to note that the numbers are all blood pressure measurements, and so should be all in one column, labelled by which time and individual they belong to. This is, however, not quite right, for reasons of how the data were collected. They are pairs of measurements on the same individual, and so there should be (for something like a matched pairs \\(t\\)-test) a column of before measurements and a column of after measurements. This will mean some extra work in the next part to get it tidy.\n\\(\\blacksquare\\)\n\nProduce a tidy dataframe from the one you read in from the file. (How many rows should you have?)\n\nSolution\nThe usual starting point for these is to get all the measurements into one column and see what to do after that. This is pivot_longer:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") \n\n\n\n  \n\n\n\nThis would be tidy if we had 20 independent observations from 20 different people. But we don’t. We only have 10 people, with two measurements on each, so we should only have 10 rows. Having made things longer, they are now too long, and we have to make it wider again.\nWe want to have a column of before and a column of after, so the names of the new columns are coming from what I called time. The values are coming from what I called bp, so, gluing the pivot_wider on the end of the pipe:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %&gt;% \n  pivot_wider(names_from = time, values_from = bp) -&gt; blood_pressure\nblood_pressure\n\n\n\n  \n\n\n\nThis is now tidy, so I gave it a “permanent” name.\nI laid out the steps of my thinking, so you could follow my logic. I’m expecting your thinking to be about the same as mine, but the work you hand in can certainly be the finished pipe I had just above, as if you thought of it right away.\nExtra: pivot_wider is smarter than you think, but it can be helpful to know what it does, in order to help diagnose when things go wrong. Let’s go back and look at the too-long dataframe again:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") \n\n\n\n  \n\n\n\nEach one of those values in bp has to go somewhere in the wider dataframe. In particular, it needs to go in a particular row and column. The column is pretty obvious: the column whose name is in the time column. But the row is much less obvious. How does pivot_wider figure it out? Well, it looks for all combinations of values in the other columns, the ones not mentioned in the pivot_wider, and makes a row for each of those. In this case, the only other column is person, so it makes one row for each person. Since there is one before and one after measurement for each person, everything works smoothly.\nThis enables us to try a couple of what-ifs to see what can go wrong.\nFirst, what if there’s no person column at all, so there is nothing to say what row an observation should go in?\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %&gt;% \nselect(-person) %&gt;% \npivot_wider(names_from = time, values_from = bp)\n\nWarning: Values from `bp` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} %&gt;%\n  dplyr::group_by(time) %&gt;%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n  dplyr::filter(n &gt; 1L)\n\n\n\n\n  \n\n\n\nIt kinda works, but with a warning. The warning says “values are not uniquely identified”, which is a posh way to say that it doesn’t know where to put them (because there is no longer a way to say which row each observation should go in).\nHere’s another one, similar:\n\nd &lt;- tribble(\n~g, ~id, ~y,\n\"a\", \"p1\", 10,\n\"a\", \"p2\", 11,\n\"b\", \"p1\", 12,\n\"b\", \"p2\", 13,\n\"a\", \"p3\", 14,\n\"a\", \"p1\", 15\n)\nd\n\n\n\n  \n\n\n\nWhen we do this:\n\nd %&gt;% pivot_wider(names_from = g, values_from = y)\n\nWarning: Values from `y` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} %&gt;%\n  dplyr::group_by(id, g) %&gt;%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %&gt;%\n  dplyr::filter(n &gt; 1L)\n\n\n\n\n  \n\n\n\nwe get list-columns again (and the same warning). What this output is telling you is that mostly there is one number per id-group combination (the dbl[1]) but there are two observations labelled id p1 and group a, and no observations at all labelled id p3 and group b. It turns out^[I know because I made these data up. that the last row of the tribble contains errors. Fix them, and all is good:\n\nd &lt;- tribble(\n~g, ~id, ~y,\n\"a\", \"p1\", 10,\n\"a\", \"p2\", 11,\n\"b\", \"p1\", 12,\n\"b\", \"p2\", 13,\n\"a\", \"p3\", 14,\n\"b\", \"p3\", 15\n)\nd %&gt;% pivot_wider(names_from = g, values_from = y)\n\n\n\n  \n\n\n\nOne last one:\n\nd &lt;- tribble(\n~id, ~g, ~y,\n1, \"a\", 10,\n2, \"a\", 11,\n3, \"a\", 12,\n4, \"b\", 13,\n5, \"b\", 14,\n6, \"b\", 15\n)\nd\n\n\n\n  \n\n\n\nand then\n\nd %&gt;% pivot_wider(names_from = g, values_from = y)\n\n\n\n  \n\n\n\nWhere did those missing values come from? If you go back and look at this d, you’ll see that each person has only one measurement, either an a or a b, not both. There is, for example, nothing to go in the a column for person number 4, because their only measurement was in group b. This kind of thing happens with two independent samples, and is a warning that you don’t need to pivot wider; it’s already tidy:\n\nd\n\n\n\n  \n\n\n\nThink about the kind of layout you need for a two-sample \\(t\\)-test.\n\\(\\blacksquare\\)\n\nWhat kind of test might you run on these data? Explain briefly.\n\nSolution\nThis is a matched-pairs experiment, so it needs a matched-pairs analysis. This could be a matched-pairs \\(t\\)-test, or a sign test on the differences (testing that the population median difference is zero). You can suggest either, since we haven’t drawn any graphs yet, but “sign test” is not enough; you need to say something about what kind of sign test. (It’s actually quicker to answer “matched-pairs \\(t\\)-test” since you don’t need any more detail than that.)\n\\(\\blacksquare\\)\n\nDraw a suitable graph of these data.\n\nSolution\nGiven that we are going to do a matched-pairs analysis of some kind, the best graph looks at the differences between the two measurements. So calculate them first, and then make a one-sample plot of them, such as a histogram:\n\nblood_pressure %&gt;% mutate(diff = before - after) %&gt;% \nggplot(aes(x=diff)) + geom_histogram(bins=5)\n\n\n\n\nYou will need a suitably small number of bins, since we only have ten observations. You can take the differences the other way around if you prefer; they will then be mostly negative.\nYou might have looked at the two quantitative columns and thought “scatterplot”:\n\nggplot(blood_pressure, aes(x=before, y=after)) + geom_point()\n\n\n\n\nThis says that if the blood pressure before was large, the blood pressure afterwards is as well. This is fair enough, but it is the answer to a question we don’t care about. What we do care about is whether the after measurement is bigger than the before one for the same person, which this graph does not show. So this is not the best.\nTo rescue this graph, you can add the line \\(y=x\\) to it. The value of this is that a point above this line has the after measurement bigger than the corresponding before one, and a point below the line has the after measurement smaller. You will need to find out how to add a line with a given intercept and slope to the plot, since I haven’t shown you how to do it. It’s called geom_abline, thus:\n\nggplot(blood_pressure, aes(x=before, y=after)) + geom_point() +\ngeom_abline(intercept = 0, slope = 1)\n\n\n\n\nThis is insightful, because most of the points are below the line, so that most of the before measurements were bigger than the corresponding after ones.\nNote that if you put a regression line on your plot, you will need to offer a convincing explanation of why that offers insight, which I think you will find difficult.\nFinally, if you thought the long data frame was tidy, this one:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") \n\n\n\n  \n\n\n\nthen you can rescue some points here by making a suitable plot of that. A boxplot is not enough:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %&gt;% \nggplot(aes(x=time, y=bp)) + geom_boxplot()\n\n\n\n\nbecause you have lost the connection between the two measurements for each person. To keep that connection, start with the same plot but as points rather than boxes:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %&gt;% \nggplot(aes(x=time, y=bp)) + geom_point()\n\n\n\n\nand then join the two points that belong to the same person. This is done with geom_line as usual, only you have to say which points are going to be joined, namely the two for each person, and to do that, you specify person in group:\n\nbp0 %&gt;% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %&gt;% \nggplot(aes(x=time, y=bp, group=person)) + geom_point() + geom_line()\n\n\n\n\nMost of the lines are going uphill, so most of the after measurements are less than the corresponding before ones.32\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "tidying-data.html#footnotes",
    "href": "tidying-data.html#footnotes",
    "title": "15  Tidying data",
    "section": "",
    "text": "You can try it without. See below.↩︎\nYou could just as well make the point that the text 20.8 contains the number 20.8 and nothing else, so that parsing it as text in search of a number will pull out 20.8 as a number. If that logic works for you, go with it.↩︎\nOh, what a giveaway.↩︎\nUnlike thunderstorm watch and thunderstorm warning, which mean different things.↩︎\nEvidently the units were chosen for ease of recording; had the values been in grams instead, the person recording the data would have had to put a 0 and a decimal point on the front of each value. This is the old meaning of the word “coding”; making the data values be whole numbers and/or small deviations from something makes them easier to record, and in pre-computer days easier to calculate with. You will also see the same word used for classifying survey responses into categories, which is similar but not quite the same thing.↩︎\nA blood pressure is usually given as two numbers, like “120 over 80”. The first number, which is the one shown in our data, is called the systolic blood pressure. It is the pressure in the arteries when the heart is pumping. The second is called the diastolic blood pressure, and it is the pressure in the arteries when the heart is resting.↩︎\nYou can try it without. See below.↩︎\nSometimes the column playing the role of rep is interesting to us, but not here.↩︎\nTo allow for the fact that measurements on the same subject are not independent but correlated.↩︎\nAnd then thrown away.↩︎\nIt needs print around it to display it, as you need print to display something within a loop or a function.↩︎\nThis talks about means rather than individual observations; in individual cases, sometimes even drug A will come out best. But we’re interested in population means, since we want to do the greatest good for the greatest number. “Greatest good for the greatest number” is from Jeremy Bentham, 1748–1832, British philosopher and advocate of utilitarianism.↩︎\nAs str_c or paste do, actually, but the advantage of unite is that it gets rid of the other columns, which you probably no longer need.↩︎\nYou could just as well make the point that the text 20.8 contains the number 20.8 and nothing else, so that parsing it as text in search of a number will pull out 20.8 as a number. If that logic works for you, go with it.↩︎\nYou might think that missing is just missing, but R distinguishes between types of missing.↩︎\nWhich was the title of a song by Prince.↩︎\nAs, for example, when Prince died.↩︎\nOh, what a giveaway.↩︎\nIn some languages it is called switch. Python appears not to have it. What you do there instead is to use a Python dictionary to pick out the value you want.↩︎\nIf I was helping you, and you were struggling with ifelse but finally mastered it, it seemed easier to suggest that you used it again for the others.↩︎\nBut I didn’t want to complicate this question any farther.↩︎\nUnlike thunderstorm watch and thunderstorm warning, which mean different things.↩︎\nI did not know until just now that you could put two variables in a count and you get counts of all the combinations of them. Just goes to show the value of “try it and see”.↩︎\nI looked it up. It was 2003, my first summer in Ontario. I realize as I write this that you may not be old enough to remember these years. Sometimes I forget how old I am.↩︎\nThis is the same kind of thing as a “dictionary” in Python.↩︎\nEvidently the units were chosen for ease of recording; had the values been in grams instead, the person recording the data would have had to put a 0 and a decimal point on the front of each value. This is the old meaning of the word coding; making the data values be whole numbers and/or small deviations from something makes them easier to record, and in pre-computer days easier to calculate with. You will also see the same word used for classifying survey responses into categories, which is similar but not quite the same thing.↩︎\nIt shouldn’t be, but it often is.↩︎\nData tidying has a lot of this kind of thing: try something, see that it doesn’t work, figure out what went wrong, fix that, repeat. The work you hand in, or show to your boss, won’t necessarily look very much like your actual process.↩︎\nThis is the opposite way to the usual: when two tests disagree, it is usually the one with fewer assumptions that is preferred, but in this case, the Welch ANOVA is fine, and the median test fails to give significance because it is not using the data as efficiently.↩︎\nThis actually creates a copy of the original column, so if you look you now have two columns with the same thing in them, one with a bad name and one with a good one.↩︎\nA blood pressure is usually given as two numbers, like “120 over 80”. The first number, which is the one shown in our data, is called the systolic blood pressure. It is the pressure in the arteries when the heart is pumping. The second is called the diastolic blood pressure, and it is the pressure in the arteries when the heart is resting.↩︎\nThis is known in the trade as a spaghetti plot because the lines resemble strands of spaghetti.↩︎"
  },
  {
    "objectID": "simple-regression.html#rainfall-in-california",
    "href": "simple-regression.html#rainfall-in-california",
    "title": "16  Simple regression",
    "section": "16.1 Rainfall in California",
    "text": "16.1 Rainfall in California\nThe data in link are rainfall and other measurements for 30 weather stations in California. Our aim is to understand how well the annual rainfall at these stations (measured in inches) can be predicted from the other measurements, which are the altitude (in feet above sea level), the latitude (degrees north of the equator) and the distance from the coast (in miles).\n\nRead the data into R. You’ll have to be careful here, since the values are space-delimited, but sometimes by more than one space, to make the columns line up. read_table2, with filename or url, will read it in. One of the variables is called rainfall, so as long as you do not call the data frame that, you should be safe.\nMake a boxplot of the rainfall figures, and explain why the values are reasonable. (A rainfall cannot be negative, and it is unusual for a annual rainfall to exceed 60 inches.) A ggplot boxplot needs something on the \\(x\\)-axis: the number 1 will do.\nPlot rainfall against each of the other quantitative variables (that is, not station).\nLook at the relationship of each other variable with rainfall. Justify the assertion that latitude seems most strongly related with rainfall. Is that relationship positive or negative? linear? Explain briefly.\nFit a regression with rainfall as the response variable, and latitude as your explanatory variable. What are the intercept, slope and R-squared values? Is there a significant relationship between rainfall and your explanatory variable? What does that mean?\nFit a multiple regression predicting rainfall from all three of the other (quantitative) variables. Display the results. Comment is coming up later.\nWhat is the R-squared for the regression of the last part? How does that compare with the R-squared of your regression in part (e)?\nWhat do you conclude about the importance of the variables that you did not include in your model in (e)? Explain briefly.\nMake a suitable hypothesis test that the variables altitude and fromcoast significantly improve the prediction of rainfall over the use of latitude alone. What do you conclude?"
  },
  {
    "objectID": "simple-regression.html#carbon-monoxide-in-cigarettes",
    "href": "simple-regression.html#carbon-monoxide-in-cigarettes",
    "title": "16  Simple regression",
    "section": "16.2 Carbon monoxide in cigarettes",
    "text": "16.2 Carbon monoxide in cigarettes\nThe (US) Federal Trade Commission assesses cigarettes according to their tar, nicotine and carbon monoxide contents. In a particular year, 25 brands were assessed. For each brand, the tar, nicotine and carbon monoxide (all in milligrams) were measured, along with the weight in grams. Our aim is to predict carbon monoxide from any or all of the other variables. The data are in link. These are aligned by column (except for the variable names), with more than one space between each column of data.\n\nRead the data into R, and check that you have 25 observations and 4 variables.\nRun a regression to predict carbon monoxide from the other variables, and obtain a summary of the output.\nWhich one of your explanatory variables would you remove from this regression? Explain (very) briefly. Go ahead and fit the regression without it, and describe how the change in R-squared from the regression in (b) was entirely predictable.\nFit a regression predicting carbon monoxide from nicotine only, and display the summary.\nnicotine was far from being significant in the model of (c), and yet in the model of (d), it was strongly significant, and the R-squared value of (d) was almost as high as that of (c). What does this say about the importance of nicotine as an explanatory variable? Explain, as briefly as you can manage.\nMake a “pairs plot”: that is, scatter plots between all pairs of variables. This can be done by feeding the whole data frame into plot.1 Do you see any strong relationships that do not include co? Does that shed any light on the last part? Explain briefly (or “at length” if that’s how it comes out)."
  },
  {
    "objectID": "simple-regression.html#maximal-oxygen-uptake-in-young-boys",
    "href": "simple-regression.html#maximal-oxygen-uptake-in-young-boys",
    "title": "16  Simple regression",
    "section": "16.3 Maximal oxygen uptake in young boys",
    "text": "16.3 Maximal oxygen uptake in young boys\nA physiologist wanted to understand the relationship between physical characteristics of pre-adolescent boys and their maximal oxygen uptake (millilitres of oxygen per kilogram of body weight). The data are in link for a random sample of 10 pre-adolescent boys. The variables are (with units):\n\nuptake: Oxygen uptake (millitres of oxygen per kilogram of body weight)\nage: boy’s age (years)\nheight: boy’s height (cm)\nweight: boy’s weight (kg)\nchest: chest depth (cm).\n\n\nRead the data into R and confirm that you do indeed have 10 observations.\nFit a regression predicting oxygen uptake from all the other variables, and display the results.\n(A one-mark question.) Would you say, on the evidence so far, that the regression fits well or badly? Explain (very) briefly.\nIt seems reasonable that an older boy should have a greater oxygen uptake, all else being equal. Is this supported by your output? Explain briefly.\nIt seems reasonable that a boy with larger weight should have larger lungs and thus a statistically significantly larger oxygen uptake. Is that what happens here? Explain briefly.\nFit a model that contains only the significant explanatory variables from your first regression. How do the R-squared values from the two regressions compare? (The last sentence asks for more or less the same thing as the next part. Answer it either here or there. Either place is good.)\nHow has R-squared changed between your two regressions? Describe what you see in a few words.\nCarry out a test comparing the fit of your two regression models. What do you conclude, and therefore what recommendation would you make about the regression that would be preferred?\nObtain a table of correlations between all the variables in the data frame. Do this by feeding the whole data frame into cor. We found that a regression predicting oxygen uptake from just height was acceptably good. What does your table of correlations say about why that is? (Hint: look for all the correlations that are large.)"
  },
  {
    "objectID": "simple-regression.html#facebook-friends-and-grey-matter",
    "href": "simple-regression.html#facebook-friends-and-grey-matter",
    "title": "16  Simple regression",
    "section": "16.4 Facebook friends and grey matter",
    "text": "16.4 Facebook friends and grey matter\nIs there a relationship between the number of Facebook friends a person has, and the density of grey matter in the areas of the brain associated with social perception and associative memory? To find out, a 2012 study measured both of these variables for a sample of 40 students at City University in London (England). The data are at link. The grey matter density is on a \\(z\\)-score standardized scale. The values are separated by tabs.\nThe aim of this question is to produce an R Markdown report that contains your answers to the questions below.\nYou should aim to make your report flow smoothly, so that it would be pleasant for a grader to read, and can stand on its own as an analysis (rather than just being the answer to a question that I set you). Some suggestions: give your report a title and arrange it into sections with an Introduction; add a small amount of additional text here and there explaining what you are doing and why. I don’t expect you to spend a large amount of time on this, but I do hope you will make some effort. (My report came out to 4 Word pages.)\n\nRead in the data and make a scatterplot for predicting the number of Facebook friends from the grey matter density. On your scatterplot, add a smooth trend.\nDescribe what you see on your scatterplot: is there a trend, and if so, what kind of trend is it? (Don’t get too taken in by the exact shape of your smooth trend.) Think “form, direction, strength”.\nFit a regression predicting the number of Facebook friends from the grey matter density, and display the output.\nIs the slope of your regression line significantly different from zero? What does that mean, in the context of the data?\nAre you surprised by the results of parts (b) and (d)? Explain briefly.\nObtain a scatterplot with the regression line on it.\nObtain a plot of the residuals from the regression against the fitted values, and comment briefly on it."
  },
  {
    "objectID": "simple-regression.html#endogenous-nitrogen-excretion-in-carp",
    "href": "simple-regression.html#endogenous-nitrogen-excretion-in-carp",
    "title": "16  Simple regression",
    "section": "16.5 Endogenous nitrogen excretion in carp",
    "text": "16.5 Endogenous nitrogen excretion in carp\nA paper in Fisheries Science reported on variables that affect “endogenous nitrogen excretion” or ENE in carp raised in Japan. A number of carp were divided into groups based on body weight, and each group was placed in a different tank. The mean body weight of the carp placed in each tank was recorded. The carp were then fed a protein-free diet three times daily for a period of 20 days. At the end of the experiment, the amount of ENE in each tank was measured, in milligrams of total fish body weight per day. (Thus it should not matter that some of the tanks had more fish than others, because the scaling is done properly.)\nFor this question, write a report in R Markdown that answers the questions below and contains some narrative that describes your analysis. Create an HTML document from your R Markdown.\n\nRead the data in from link. There are 10 tanks.\nCreate a scatterplot of ENE (response) against bodyweight (explanatory). Add a smooth trend to your plot.\nIs there an upward or downward trend (or neither)? Is the relationship a line or a curve? Explain briefly.\nFit a straight line to the data, and obtain the R-squared for the regression.\nObtain a residual plot (residuals against fitted values) for this regression. Do you see any problems? If so, what does that tell you about the relationship in the data?\nFit a parabola to the data (that is, including an \\(x\\)-squared term). Compare the R-squared values for the models in this part and part (d). Does that suggest that the parabola model is an improvement here over the linear model?\nIs the test for the slope coefficient for the squared term significant? What does this mean?\nMake the scatterplot of part (b), but add the fitted curve. Describe any way in which the curve fails to fit well.\nObtain a residual plot for the parabola model. Do you see any problems with it? (If you do, I’m not asking you to do anything about them in this question, but I will.)"
  },
  {
    "objectID": "simple-regression.html#salaries-of-social-workers",
    "href": "simple-regression.html#salaries-of-social-workers",
    "title": "16  Simple regression",
    "section": "16.6 Salaries of social workers",
    "text": "16.6 Salaries of social workers\nAnother salary-prediction question: does the number of years of work experience that a social worker has help to predict their salary? Data for 50 social workers are in link.\n\nRead the data into R. Check that you have 50 observations on two variables. Also do something to check that the years of experience and annual salary figures look reasonable overall.\nMake a scatterplot showing how salary depends on experience. Does the nature of the trend make sense?\nFit a regression predicting salary from experience, and display the results. Is the slope positive or negative? Does that make sense?\nObtain and plot the residuals against the fitted values. What problem do you see?\nThe problem you unearthed in the previous part is often helped by a transformation. Run Box-Cox on your data to find a suitable transformation. What transformation is suggested?\nCalculate a new variable as suggested by your transformation. Use your transformed response in a regression, showing the summary.\nObtain and plot the residuals against the fitted values for this regression. Do you seem to have solved the problem with the previous residual plot?"
  },
  {
    "objectID": "simple-regression.html#predicting-volume-of-wood-in-pine-trees",
    "href": "simple-regression.html#predicting-volume-of-wood-in-pine-trees",
    "title": "16  Simple regression",
    "section": "16.7 Predicting volume of wood in pine trees",
    "text": "16.7 Predicting volume of wood in pine trees\nIn forestry, the financial value of a tree is the volume of wood that it contains. This is difficult to estimate while the tree is still standing, but the diameter is easy to measure with a tape measure (to measure the circumference) and a calculation involving \\(\\pi\\), assuming that the cross-section of the tree is at least approximately circular. The standard measurement is “diameter at breast height” (that is, at the height of a human breast or chest), defined as being 4.5 feet above the ground.\nSeveral pine trees had their diameter measured shortly before being cut down, and for each tree, the volume of wood was recorded. The data are in link. The diameter is in inches and the volume is in cubic inches. Is it possible to predict the volume of wood from the diameter?\n\nRead the data into R and display the values (there are not very many).\nMake a suitable plot.\nDescribe what you learn from your plot about the relationship between diameter and volume, if anything.\nFit a (linear) regression, predicting volume from diameter, and obtain the summary. How would you describe the R-squared?\nDraw a graph that will help you decide whether you trust the linearity of this regression. What do you conclude? Explain briefly.\nWhat would you guess would be the volume of a tree of diameter zero? Is that what the regression predicts? Explain briefly.\nA simple way of modelling a tree’s shape is to pretend it is a cone, like this, but probably taller and skinnier:\n\n\nwith its base on the ground. What is the relationship between the diameter (at the base) and volume of a cone? (If you don’t remember, look it up. You’ll probably get a formula in terms of the radius, which you’ll have to convert. Cite the website you used.)\n\nFit a regression model that predicts volume from diameter according to the formula you obtained in the previous part. You can assume that the trees in this data set are of similar heights, so that the height can be treated as a constant.\nDisplay the results."
  },
  {
    "objectID": "simple-regression.html#tortoise-shells-and-eggs",
    "href": "simple-regression.html#tortoise-shells-and-eggs",
    "title": "16  Simple regression",
    "section": "16.8 Tortoise shells and eggs",
    "text": "16.8 Tortoise shells and eggs\nA biologist measured the length of the carapace (shell) of female tortoises, and then x-rayed the tortoises to count how many eggs they were carrying. The length is measured in millimetres. The data are in link. The biologist is wondering what kind of relationship, if any, there is between the carapace length (as an explanatory variable) and the number of eggs (as a response variable).\n\nRead in the data, and check that your values look reasonable.\nObtain a scatterplot, with a smooth trend, of the data.\nThe biologist expected that a larger tortoise would be able to carry more eggs. Is that what the scatterplot is suggesting? Explain briefly why or why not.\nFit a straight-line relationship and display the summary.\nAdd a squared term to your regression, fit that and display the summary.\nIs a curve better than a line for these data? Justify your answer in two ways: by comparing a measure of fit, and by doing a suitable test of significance.\nMake a residual plot for the straight line model: that is, plot the residuals against the fitted values. Does this echo your conclusions of the previous part? In what way? Explain briefly."
  },
  {
    "objectID": "simple-regression.html#roller-coasters",
    "href": "simple-regression.html#roller-coasters",
    "title": "16  Simple regression",
    "section": "16.9 Roller coasters",
    "text": "16.9 Roller coasters\nA poll on the Discovery Channel asked people to nominate the best roller-coasters in the United States. We will examine the 10 roller-coasters that received the most votes. Two features of a roller-coaster that are of interest are the distance it drops from start to finish, measured here in feet2 and the duration of the ride, measured in seconds. Is it true that roller-coasters with a bigger drop also tend to have a longer ride? The data are at link.3\n\nRead the data into R and verify that you have a sensible number of rows and columns.\nMake a scatterplot of duration (response) against drop (explanatory), labelling each roller-coaster with its name in such a way that the labels do not overlap. Add a regression line to your plot.\nWould you say that roller-coasters with a larger drop tend to have a longer ride? Explain briefly.\nFind a roller-coaster that is unusual compared to the others. What about its combination of drop and duration is unusual?"
  },
  {
    "objectID": "simple-regression.html#running-and-blood-sugar",
    "href": "simple-regression.html#running-and-blood-sugar",
    "title": "16  Simple regression",
    "section": "16.10 Running and blood sugar",
    "text": "16.10 Running and blood sugar\nA diabetic wants to know how aerobic exercise affects his blood sugar. When his blood sugar reaches 170 (mg/dl), he goes out for a run at a pace of 10 minutes per mile. He runs different distances on different days. Each time he runs, he measures his blood sugar after the run. (The preferred blood sugar level is between 80 and 120 on this scale.) The data are in the file link. Our aim is to predict blood sugar from distance.\n\nRead in the data and display the data frame that you read in.\nMake a scatterplot and add a smooth trend to it.\nWould you say that the relationship between blood sugar and running distance is approximately linear, or not? It is therefore reasonable to use a regression of blood sugar on distance? Explain briefly.\nFit a suitable regression, and obtain the regression output.\nHow would you interpret the slope? That is, what is the slope, and what does that mean about blood sugar and running distance?\nIs there a (statistically) significant relationship between running distance and blood sugar? How do you know? Do you find this surprising, given what you have seen so far? Explain briefly.\nThis diabetic is planning to go for a 3-mile run tomorrow and a 5-mile run the day after. Obtain suitable 95% intervals that say what his blood sugar might be after each of these runs.\nWhich of your two intervals is longer? Does this make sense? Explain briefly."
  },
  {
    "objectID": "simple-regression.html#calories-and-fat-in-pizza",
    "href": "simple-regression.html#calories-and-fat-in-pizza",
    "title": "16  Simple regression",
    "section": "16.11 Calories and fat in pizza",
    "text": "16.11 Calories and fat in pizza\nThe file at link came from a spreadsheet of information about 24 brands of pizza: specifically, per 5-ounce serving, the number of calories, the grams of fat, and the cost (in US dollars). The names of the pizza brands are quite long. This file may open in a spreadsheet when you browse to the link, depending on your computer’s setup.\n\nRead in the data and display at least some of the data frame. Are the variables of the right types? (In particular, why is the number of calories labelled one way and the cost labelled a different way?)\nMake a scatterplot for predicting calories from the number of grams of fat. Add a smooth trend. What kind of relationship do you see, if any?\nFit a straight-line relationship, and display the intercept, slope, R-squared, etc. Is there a real relationship between the two variables, or is any apparent trend just chance?\nObtain a plot of the residuals against the fitted values for this regression. Does this indicate that there are any problems with this regression, or not? Explain briefly.\nThe research assistant in this study returns with two new brands of pizza (ones that were not in the original data). The fat content of a 5-ounce serving was 12 grams for the first brand and 20 grams for the second brand. For each of these brands of pizza, obtain a suitable 95% interval for the number of calories contained in a 5-ounce serving."
  },
  {
    "objectID": "simple-regression.html#where-should-the-fire-stations-be",
    "href": "simple-regression.html#where-should-the-fire-stations-be",
    "title": "16  Simple regression",
    "section": "16.12 Where should the fire stations be?",
    "text": "16.12 Where should the fire stations be?\nIn city planning, one major issue is where to locate fire stations. If a city has too many fire stations, it will spend too much on running them, but if it has too few, there may be unnecessary fire damage because the fire trucks take too long to get to the fire.\nThe first part of a study of this kind of issue is to understand the relationship between the distance from the fire station (measured in miles in our data set) and the amount of fire damage caused (measured in thousands of dollars). A city recorded the fire damage and distance from fire station for 15 residential fires (which you can take as a sample of “all possible residential fires in that city”). The data are in link.\n\nRead in and display the data, verifying that you have the right number of rows and the right columns.\n* Obtain a 95% confidence interval for the mean fire damage. (There is nothing here from STAD29, and your answer should have nothing to do with distance.)\nDraw a scatterplot for predicting the amount of fire damage from the distance from the fire station. Add a smooth trend to your plot.\n* Is there a relationship between distance from fire station and fire damage? Is it linear or definitely curved? How strong is it? Explain briefly.\nFit a regression predicting fire damage from distance. How is the R-squared consistent (or inconsistent) with your answer from part~(here)?\n Obtain a 95% confidence interval for the mean fire damage for a residence that is 4 miles from the nearest fire station*. (Note the contrast with part~(here).)\nCompare the confidence intervals of parts (here) and (here). Specifically, compare their centres and their lengths, and explain briefly why the results make sense."
  },
  {
    "objectID": "simple-regression.html#making-it-stop",
    "href": "simple-regression.html#making-it-stop",
    "title": "16  Simple regression",
    "section": "16.13 Making it stop",
    "text": "16.13 Making it stop\nIf you are driving, and you hit the brakes, how far do you travel before coming to a complete stop? Presumably this depends on how fast you are going. Knowing this relationship is important in setting speed limits on roads. For example, on a very bendy road, the speed limit needs to be low, because you cannot see very far ahead, and there could be something just out of sight that you need to stop for.\nData were collected for a typical car and driver, as shown in http://ritsokiguess.site/datafiles/stopping.csv. These are American data, so the speeds are miles per hour and the stopping distances are in feet.\n\nRead in and display (probably all of) the data.\nMake a suitable plot of the data.\nDescribe any trend you see in your graph.\nFit a linear regression predicting stopping distance from speed. (You might have some misgivings about doing this, but do it anyway.)\nPlot the residuals against the fitted values for this regression.\nWhat do you learn from the residual plot? Does that surprise you? Explain briefly.\nWhat is the actual relationship between stopping distance and speed, according to the physics? See if you can find out. Cite any books or websites that you use: that is, include a link to a website, or give enough information about a book that the grader could find it.\nFit the relationship that your research indicated (in the previous part) and display the results. Comment briefly on the R-squared value.\nSomebody says to you “if you have a regression with a high R-squared, like 95%, there is no need to look for a better model.” How do you respond to this? Explain briefly."
  },
  {
    "objectID": "simple-regression.html#predicting-height-from-foot-length",
    "href": "simple-regression.html#predicting-height-from-foot-length",
    "title": "16  Simple regression",
    "section": "16.14 Predicting height from foot length",
    "text": "16.14 Predicting height from foot length\nIs it possible to estimate the height of a person from the length of their foot? To find out, 33 (male) students had their height and foot length measured. The data are in http://ritsokiguess.site/datafiles/heightfoot.csv.\n\nRead in and display (some of) the data. (If you are having trouble, make sure you have exactly the right URL. The correct URL has no spaces or other strange characters in it.)\nMake a suitable plot of the two variables in the data frame.\nAre there any observations not on the trend of the other points? What is unusual about those observations?\nFit a regression predicting height from foot length, including any observations that you identified in the previous part. For that regression, plot the residuals against the fitted values and make a normal quantile plot of the residuals.\nEarlier, you identified one or more observations that were off the trend. How does this point or points show up on each of the plots you drew in the previous part?\nAny data points that concerned you earlier were actually errors. Create and save a new data frame that does not contain any of those data points.\nRun a regression predicting height from foot length for your data set without errors. Obtain a plot of the residuals against fitted values and a normal quantile plot of the residuals for this regression.\nDo you see any problems on the plots you drew in the previous part? Explain briefly.\nFind a way to plot the data and both regression lines on the same plot, in such a way that you can see which regression line is which. If you get help from anything outside the course materials, cite your source(s).\nDiscuss briefly how removing the observation(s) that were errors has changed where the regression line goes, and whether that is what you expected.\n\nMy solutions follow:"
  },
  {
    "objectID": "simple-regression.html#rainfall-in-california-1",
    "href": "simple-regression.html#rainfall-in-california-1",
    "title": "16  Simple regression",
    "section": "16.15 Rainfall in California",
    "text": "16.15 Rainfall in California\nThe data in link are rainfall and other measurements for 30 weather stations in California. Our aim is to understand how well the annual rainfall at these stations (measured in inches) can be predicted from the other measurements, which are the altitude (in feet above sea level), the latitude (degrees north of the equator) and the distance from the coast (in miles).\n\nRead the data into R. You’ll have to be careful here, since the values are space-delimited, but sometimes by more than one space, to make the columns line up. read_table, with filename or url, will read it in. One of the variables is called rainfall, so as long as you do not call the data frame that, you should be safe.\n\nSolution\nI used rains as the name of my data frame:\n\nmy_url=\"http://ritsokiguess.site/datafiles/calirain.txt\"\nrains=read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  station = col_character(),\n  rainfall = col_double(),\n  altitude = col_double(),\n  latitude = col_double(),\n  fromcoast = col_double()\n)\n\n\nI have the right number of rows and columns.\nI don’t need you to investigate the data yet (that happens in the next part), but this is interesting (to me):\n\nrains\n\n\n\n  \n\n\n\nSome of the station names are two words, but they have been smooshed into one word, so that read_table will recognize them as a single thing. Someone had already done that for us, so I didn’t even have to do it myself.\nIf the station names had been two genuine words, a .csv would probably have been the best choice (the actual data values being separated by commas then, and not spaces).\n\\(\\blacksquare\\)\n\nMake a boxplot of the rainfall figures, and explain why the values are reasonable. (A rainfall cannot be negative, and it is unusual for a annual rainfall to exceed 60 inches.) A ggplot boxplot needs something on the \\(x\\)-axis: the number 1 will do.\n\nSolution\n\nggplot(rains,aes(y=rainfall,x=1))+geom_boxplot()\n\n\n\n\nThere is only one rainfall over 60 inches, and the smallest one is close to zero but positive, so that is good.\nAnother possible plot here is a histogram, since there is only one quantitative variable:\n\nggplot(rains, aes(x=rainfall))+geom_histogram(bins=7)\n\n\n\n\nThis clearly shows the rainfall value above 60 inches, but some other things are less clear: are those two rainfall values around 50 inches above or below 50, and are those six rainfall values near zero actually above zero? Extra: What stations have those extreme values? Should you wish to find out:\n\nrains %&gt;% filter(rainfall&gt;60)\n\n\n\n  \n\n\n\nThis is a place right on the Pacific coast, almost up into Oregon (it’s almost the northernmost of all the stations). So it makes sense that it would have a high rainfall, if anywhere does. (If you know anything about rainy places, you’ll probably think of Vancouver and Seattle, in the Pacific Northwest.) Here it is: link. Which station has less than 2 inches of annual rainfall?\n\nrains %&gt;% filter(rainfall&lt;2)  \n\n\n\n  \n\n\n\nThe name of the station is a clue: this one is in the desert. So you’d expect very little rain. Its altitude is negative, so it’s actually below sea level. This is correct. Here is where it is: link.\n\\(\\blacksquare\\)\n\nPlot rainfall against each of the other quantitative variables (that is, not station).\n\nSolution\nThat is, altitude, latitude and fromcoast. The obvious way to do this (perfectly acceptable) is one plot at a time:\n\nggplot(rains,aes(y=rainfall,x=altitude))+geom_point()\n\n\n\n\n\nggplot(rains,aes(y=rainfall,x=latitude))+geom_point()\n\n\n\n\nand finally\n\nggplot(rains,aes(y=rainfall,x=fromcoast))+geom_point()\n\n\n\n\nYou can add a smooth trend to these if you want. Up to you. Just the points is fine with me.\nHere is a funky way to get all three plots in one shot:\n\nrains %&gt;% \n  pivot_longer(altitude:fromcoast, names_to=\"xname\",values_to=\"x\") %&gt;%\n  ggplot(aes(x=x,y=rainfall))+geom_point()+\n  facet_wrap(~xname,scales=\"free\")\n\n\n\n\nThis always seems extraordinarily strange if you haven’t run into it before. The strategy is to put all the \\(x\\)-variables you want to plot into one column and then plot your \\(y\\) against the \\(x\\)-column. Thus: make a column of all the \\(x\\)’s glued together, labelled by which \\(x\\) they are, then plot \\(y\\) against \\(x\\) but make a different sub-plot or “facet” for each different \\(x\\)-name. The last thing is that each \\(x\\) is measured on a different scale, and unless we take steps, all the sub-plots will have the same scale on each axis, which we don’t want.\nI’m not sure I like how it came out, with three very tall plots. facet_wrap can also take an nrow or an ncol, which tells it how many rows or columns to use for the display. Here, for example, two columns because I thought three was too many:\n\nrains %&gt;% \n  pivot_longer(altitude:fromcoast, names_to=\"xname\",values_to=\"x\") %&gt;%\n  ggplot(aes(x=x,y=rainfall))+geom_point()+\n  facet_wrap(~xname,scales=\"free\",ncol=2)\n\n\n\n\nNow, the three plots have come out about square, or at least “landscape”, which I like a lot better.\n\\(\\blacksquare\\)\n\nLook at the relationship of each other variable with rainfall. Justify the assertion that latitude seems most strongly related with rainfall. Is that relationship positive or negative? linear? Explain briefly.\n\nSolution\nLet’s look at the three variables in turn:\n\naltitude: not much of anything. The stations near sea level have rainfall all over the place, though the three highest-altitude stations have the three highest rainfalls apart from Crescent City.\nlatitude: there is a definite upward trend here, in that stations further north (higher latitude) are likely to have a higher rainfall. I’d call this trend linear (or, not obviously curved), though the two most northerly stations have one higher and one much lower rainfall than you’d expect.\nfromcoast: this is a weak downward trend, though the trend is spoiled by those three stations about 150 miles from the coast that have more than 40 inches of rainfall.\n\nOut of those, only latitude seems to have any meaningful relationship with rainfall.\n\\(\\blacksquare\\)\n\nFit a regression with rainfall as the response variable, and latitude as your explanatory variable. What are the intercept, slope and R-squared values? Is there a significant relationship between rainfall and your explanatory variable? What does that mean?\n\nSolution\nSave your lm into a variable, since it will get used again later:\n\nrainfall.1=lm(rainfall~latitude,data=rains)\nsummary(rainfall.1)\n\n\nCall:\nlm(formula = rainfall ~ latitude, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.297  -7.956  -2.103   6.082  38.262 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -113.3028    35.7210  -3.172  0.00366 ** \nlatitude       3.5950     0.9623   3.736  0.00085 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.82 on 28 degrees of freedom\nMultiple R-squared:  0.3326,    Adjusted R-squared:  0.3088 \nF-statistic: 13.96 on 1 and 28 DF,  p-value: 0.0008495\n\n\nMy intercept is \\(-113.3\\), slope is \\(3.6\\) and R-squared is \\(0.33\\) or 33%. (I want you to pull these numbers out of the output and round them off to something sensible.) The slope is significantly nonzero, its P-value being 0.00085: rainfall really does depend on latitude, although not strongly so.\nExtra: Of course, I can easily do the others as well, though you don’t have to:\n\nrainfall.2=lm(rainfall~fromcoast,data=rains)\nsummary(rainfall.2)\n\n\nCall:\nlm(formula = rainfall ~ fromcoast, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.240  -9.431  -6.603   2.871  51.147 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.77306    4.61296   5.154 1.82e-05 ***\nfromcoast   -0.05039    0.04431  -1.137    0.265    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.54 on 28 degrees of freedom\nMultiple R-squared:  0.04414,   Adjusted R-squared:   0.01 \nF-statistic: 1.293 on 1 and 28 DF,  p-value: 0.2651\n\n\nHere, the intercept is 23.8, the slope is \\(-0.05\\) and R-squared is a dismal 0.04 (4%). This is a way of seeing that this relationship is really weak, and it doesn’t even have a curve to the trend or anything that would compensate for this. I looked at the scatterplot again and saw that if it were not for the point bottom right which is furthest from the coast and has almost no rainfall, there would be almost no trend at all. The slope here is not significantly different from zero, with a P-value of 0.265.\nFinally:\n\nrainfall.3=lm(rainfall~altitude,data=rains)\nsummary(rainfall.3)\n\n\nCall:\nlm(formula = rainfall ~ altitude, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.620  -8.479  -2.729   4.555  58.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16.514799   3.539141   4.666  6.9e-05 ***\naltitude     0.002394   0.001428   1.676    0.105    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.13 on 28 degrees of freedom\nMultiple R-squared:  0.09121,   Adjusted R-squared:  0.05875 \nF-statistic:  2.81 on 1 and 28 DF,  p-value: 0.1048\n\n\nThe intercept is 16.5, the slope is 0.002 and the R-squared is 0.09 or 9%, also terrible. The P-value is 0.105, which is not small enough to be significant.\nSo it looks as if it’s only latitude that has any impact at all. This is the only explanatory variable with a significantly nonzero slope. On its own, at least.\n\\(\\blacksquare\\)\n\nFit a multiple regression predicting rainfall from all three of the other (quantitative) variables. Display the results. Comment is coming up later.\n\nSolution\nThis, then:\n\nrainfall.4=lm(rainfall~latitude+altitude+fromcoast,data=rains)\nsummary(rainfall.4)\n\n\nCall:\nlm(formula = rainfall ~ latitude + altitude + fromcoast, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.722  -5.603  -0.531   3.510  33.317 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.024e+02  2.921e+01  -3.505 0.001676 ** \nlatitude     3.451e+00  7.949e-01   4.342 0.000191 ***\naltitude     4.091e-03  1.218e-03   3.358 0.002431 ** \nfromcoast   -1.429e-01  3.634e-02  -3.931 0.000559 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.1 on 26 degrees of freedom\nMultiple R-squared:  0.6003,    Adjusted R-squared:  0.5542 \nF-statistic: 13.02 on 3 and 26 DF,  p-value: 2.205e-05\n\n\n\\(\\blacksquare\\)\n\nWhat is the R-squared for the regression of the last part? How does that compare with the R-squared of your regression in part (e)?\n\nSolution\nThe R-squared is 0.60 (60%), which is quite a bit bigger than the R-squared of 0.33 (33%) we got back in (e).\n\\(\\blacksquare\\)\n\nWhat do you conclude about the importance of the variables that you did not include in your model in (e)? Explain briefly.\n\nSolution\nBoth variables altitude and fromcoast are significant in this regression, so they have something to add over and above latitude when it comes to predicting rainfall, even though (and this seems odd) they have no apparent relationship with rainfall on their own. Another way to say this is that the three variables work together as a team to predict rainfall, and together they do much better than any one of them can do by themselves.\nThis also goes to show that the scatterplots we began with don’t get to the heart of multi-variable relationships, because they are only looking at the variables two at a time.\n\\(\\blacksquare\\)\n\nMake a suitable hypothesis test that the variables altitude and fromcoast significantly improve the prediction of rainfall over the use of latitude alone. What do you conclude?\n\nSolution\nThis calls for anova. Feed this two fitted models, smaller (fewer explanatory variables) first. The null hypothesis is that the two models are equally good (so we should go with the smaller); the alternative is that the larger model is better, so that the extra complication is worth it:\n\nanova(rainfall.1,rainfall.4)  \n\n\n\n  \n\n\n\nThe P-value is small, so we reject the null in favour of the alternative: the regression with all three explanatory variables fits better than the one with just latitude, so the bigger model is the one we should go with.\nIf you have studied these things: this one is a “multiple-partial \\(F\\)-test”, for testing the combined significance of more than one \\(x\\) but less than all the \\(x\\)’s.4\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#carbon-monoxide-in-cigarettes-1",
    "href": "simple-regression.html#carbon-monoxide-in-cigarettes-1",
    "title": "16  Simple regression",
    "section": "16.16 Carbon monoxide in cigarettes",
    "text": "16.16 Carbon monoxide in cigarettes\nThe (US) Federal Trade Commission assesses cigarettes according to their tar, nicotine and carbon monoxide contents. In a particular year, 25 brands were assessed. For each brand, the tar, nicotine and carbon monoxide (all in milligrams) were measured, along with the weight in grams. Our aim is to predict carbon monoxide from any or all of the other variables. The data are in link. These are aligned by column (except for the variable names), with more than one space between each column of data.\n\nRead the data into R, and check that you have 25 observations and 4 variables.\n\nSolution\nThis specification calls for read_table:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ftccigar.txt\"\ncigs &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  tar = col_double(),\n  nicotine = col_double(),\n  weight = col_double(),\n  co = col_double()\n)\n\ncigs\n\n\n\n  \n\n\n\nYes, I have 25 observations on 4 variables indeed.\nread_delim won’t work (try it and see what happens), because that would require the values to be separated by exactly one space.\n\\(\\blacksquare\\)\n\nRun a regression to predict carbon monoxide from the other variables, and obtain a summary of the output.\n\nSolution\nThe word “summary” is meant to be a big clue that summary is what you need:\n\ncigs.1 &lt;- lm(co ~ tar + nicotine + weight, data = cigs)\nsummary(cigs.1)\n\n\nCall:\nlm(formula = co ~ tar + nicotine + weight, data = cigs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89261 -0.78269  0.00428  0.92891  2.45082 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.2022     3.4618   0.925 0.365464    \ntar           0.9626     0.2422   3.974 0.000692 ***\nnicotine     -2.6317     3.9006  -0.675 0.507234    \nweight       -0.1305     3.8853  -0.034 0.973527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.446 on 21 degrees of freedom\nMultiple R-squared:  0.9186,    Adjusted R-squared:  0.907 \nF-statistic: 78.98 on 3 and 21 DF,  p-value: 1.329e-11\n\n\n\\(\\blacksquare\\)\n\nWhich one of your explanatory variables would you remove from this regression? Explain (very) briefly. Go ahead and fit the regression without it, and describe how the change in R-squared from the regression in (b) was entirely predictable.\n\nSolution\nFirst, the \\(x\\)-variable to remove. The obvious candidate is weight, since it has easily the highest, and clearly non-significant, P-value. So, out it comes:\n\ncigs.2 &lt;- lm(co ~ tar + nicotine, data = cigs)\nsummary(cigs.2)\n\n\nCall:\nlm(formula = co ~ tar + nicotine, data = cigs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89941 -0.78470 -0.00144  0.91585  2.43064 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.0896     0.8438   3.662 0.001371 ** \ntar           0.9625     0.2367   4.067 0.000512 ***\nnicotine     -2.6463     3.7872  -0.699 0.492035    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.413 on 22 degrees of freedom\nMultiple R-squared:  0.9186,    Adjusted R-squared:  0.9112 \nF-statistic: 124.1 on 2 and 22 DF,  p-value: 1.042e-12\n\n\nR-squared has dropped from 0.9186 to 0.9186! That is, taking out weight has not just had a minimal effect on R-squared; it’s not changed R-squared at all. This is because weight was so far from being significant: it literally had nothing to add.\nAnother way of achieving the same thing is via the function update, which takes a fitted model object and describes the change that you want to make:\n\ncigs.2a &lt;- update(cigs.1, . ~ . - weight)\nsummary(cigs.2a)\n\n\nCall:\nlm(formula = co ~ tar + nicotine, data = cigs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89941 -0.78470 -0.00144  0.91585  2.43064 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.0896     0.8438   3.662 0.001371 ** \ntar           0.9625     0.2367   4.067 0.000512 ***\nnicotine     -2.6463     3.7872  -0.699 0.492035    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.413 on 22 degrees of freedom\nMultiple R-squared:  0.9186,    Adjusted R-squared:  0.9112 \nF-statistic: 124.1 on 2 and 22 DF,  p-value: 1.042e-12\n\n\nThis can be shorter than describing the whole model again, as you do with the cigs.2 version of lm. The syntax is that you first specify a “base” fitted model object that you’re going to update. Because the model cigs.1 contains all the information about the kind of model it is, and which data frame the data come from, R already knows that this is a linear multiple regression and which \\(x\\)’s it contains. The second thing to describe is the change from the “base”. In this case, we want to use the same response variable and all the same explanatory variables that we had before, except for weight. This is specified by a special kind of model formula where . means “whatever was there before”: in English, “same response and same explanatories except take out weight”.\n\\(\\blacksquare\\)\n\nFit a regression predicting carbon monoxide from nicotine only, and display the summary.\n\nSolution\nAs you would guess:\n\ncigs.3 &lt;- lm(co ~ nicotine, data = cigs)\nsummary(cigs.3)\n\n\nCall:\nlm(formula = co ~ nicotine, data = cigs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3273 -1.2228  0.2304  1.2700  3.9357 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.6647     0.9936   1.675    0.107    \nnicotine     12.3954     1.0542  11.759 3.31e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.828 on 23 degrees of freedom\nMultiple R-squared:  0.8574,    Adjusted R-squared:  0.8512 \nF-statistic: 138.3 on 1 and 23 DF,  p-value: 3.312e-11\n\n\n\\(\\blacksquare\\)\n\nnicotine was far from being significant in the model of (c), and yet in the model of (d), it was strongly significant, and the R-squared value of (d) was almost as high as that of (c). What does this say about the importance of nicotine as an explanatory variable? Explain, as briefly as you can manage.\n\nSolution\nWhat this says is that you cannot say anything about the “importance” of nicotine without also describing the context that you’re talking about. By itself, nicotine is important, but when you have tar in the model, nicotine is not important: precisely, it now has nothing to add over and above the predictive value that tar has. You might guess that this is because tar and nicotine are “saying the same thing” in some fashion. We’ll explore that in a moment.\n\\(\\blacksquare\\)\n\nMake a “pairs plot”: that is, scatter plots between all pairs of variables. This can be done by feeding the whole data frame into plot.5 Do you see any strong relationships that do not include co? Does that shed any light on the last part? Explain briefly (or “at length” if that’s how it comes out).\n\nSolution\nPlot the entire data frame:\n\nplot(cigs)\n\n\n\n\nWe’re supposed to ignore co, but I comment that strong relationships between co and both of tar and nicotine show up here, along with weight being at most weakly related to anything else.\nThat leaves the relationship of tar and nicotine with each other. That also looks like a strong linear trend. When you have correlations between explanatory variables, it is called “multicollinearity”.\nHaving correlated \\(x\\)’s is trouble. Here is where we find out why. The problem is that when co is large, nicotine is large, and a large value of tar will come along with it. So we don’t know whether a large value of co is caused by a large value of tar or a large value of nicotine: there is no way to separate out their effects because in effect they are “glued together”.\nYou might know of this effect (in an experimental design context) as “confounding”: the effect of tar on co is confounded with the effect of nicotine on co, and you can’t tell which one deserves the credit for predicting co.\nIf you were able to design an experiment here, you could (in principle) manufacture a bunch of cigarettes with high tar; some of them would have high nicotine and some would have low. Likewise for low tar. Then the correlation between nicotine and tar would go away, their effects on co would no longer be confounded, and you could see unambiguously which one of the variables deserves credit for predicting co. Or maybe it depends on both, genuinely, but at least then you’d know.\nWe, however, have an observational study, so we have to make do with the data we have. Confounding is one of the risks we take when we work with observational data.\nThis was a “base graphics” plot. There is a way of doing a ggplot-style “pairs plot”, as this is called, thus:\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\ncigs %&gt;% ggpairs(progress = FALSE)\n\n\n\n\nAs ever, install.packages first, in the likely event that you don’t have this package installed yet. Once you do, though, I think this is a nicer way to get a pairs plot.\nThis plot is a bit more sophisticated: instead of just having the scatterplots of the pairs of variables in the row and column, it uses the diagonal to show a “kernel density” (a smoothed-out histogram), and upper-right it shows the correlation between each pair of variables. The three correlations between co, tar and nicotine are clearly the highest.\nIf you want only some of the columns to appear in your pairs plot, select them first, and then pass that data frame into ggpairs. Here, we found that weight was not correlated with anything much, so we can take it out and then make a pairs plot of the other variables:\n\ncigs %&gt;% select(-weight) %&gt;% ggpairs(progress = FALSE)\n\n\n\n\nThe three correlations that remain are all very high, which is entirely consistent with the strong linear relationships that you see bottom left.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#maximal-oxygen-uptake-in-young-boys-1",
    "href": "simple-regression.html#maximal-oxygen-uptake-in-young-boys-1",
    "title": "16  Simple regression",
    "section": "16.17 Maximal oxygen uptake in young boys",
    "text": "16.17 Maximal oxygen uptake in young boys\nA physiologist wanted to understand the relationship between physical characteristics of pre-adolescent boys and their maximal oxygen uptake (millilitres of oxygen per kilogram of body weight). The data are in link for a random sample of 10 pre-adolescent boys. The variables are (with units):\n\nuptake: Oxygen uptake (millitres of oxygen per kilogram of body weight)\nage: boy’s age (years)\nheight: boy’s height (cm)\nweight: boy’s weight (kg)\nchest: chest depth (cm).\n\n\nRead the data into R and confirm that you do indeed have 10 observations.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/youngboys.txt\"\nboys &lt;- read_delim(my_url, \" \")\n\nRows: 10 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (5): uptake, age, height, weight, chest\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nboys\n\n\n\n  \n\n\n\n10 boys (rows) indeed.\n\\(\\blacksquare\\)\n\nFit a regression predicting oxygen uptake from all the other variables, and display the results.\n\nSolution\nFitting four explanatory variables with only ten observations is likely to be pretty shaky, but we press ahead regardless:\n\nboys.1 &lt;- lm(uptake ~ age + height + weight + chest, data = boys)\nsummary(boys.1)\n\n\nCall:\nlm(formula = uptake ~ age + height + weight + chest, data = boys)\n\nResiduals:\n        1         2         3         4         5         6         7         8 \n-0.020697  0.019741 -0.003649  0.038470 -0.023639 -0.026026  0.050459 -0.014380 \n        9        10 \n 0.004294 -0.024573 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.774739   0.862818  -5.534 0.002643 ** \nage         -0.035214   0.015386  -2.289 0.070769 .  \nheight       0.051637   0.006215   8.308 0.000413 ***\nweight      -0.023417   0.013428  -1.744 0.141640    \nchest        0.034489   0.085239   0.405 0.702490    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03721 on 5 degrees of freedom\nMultiple R-squared:  0.9675,    Adjusted R-squared:  0.9415 \nF-statistic:  37.2 on 4 and 5 DF,  p-value: 0.0006513\n\n\n\\(\\blacksquare\\)\n\n(A one-mark question.) Would you say, on the evidence so far, that the regression fits well or badly? Explain (very) briefly.\n\nSolution\nR-squared of 0.97 (97%) is very high, so I’d say this regression fits very well. That’s all. I said “on the evidence so far” to dissuade you from overthinking this, or thinking that you needed to produce some more evidence. That, plus the fact that this was only one mark.\n\\(\\blacksquare\\)\n\nIt seems reasonable that an older boy should have a greater oxygen uptake, all else being equal. Is this supported by your output? Explain briefly.\n\nSolution\nIf an older boy has greater oxygen uptake (the “all else equal” was a hint), the slope of age should be positive. It is not: it is \\(-0.035\\), so it is suggesting (all else equal) that a greater age goes with a smaller oxygen uptake. The reason why this happens (which you didn’t need, but you can include it if you like) is that age has a non-small P-value of 0.07, so that the age slope is not significantly different from zero. With all the other variables, age has nothing to add over and above them, and we could therefore remove it.\n\\(\\blacksquare\\)\n\nIt seems reasonable that a boy with larger weight should have larger lungs and thus a statistically significantly larger oxygen uptake. Is that what happens here? Explain briefly.\n\nSolution\nLook at the P-value for weight. This is 0.14, not small, and so a boy with larger weight does not have a significantly larger oxygen uptake, all else equal. (The slope for weight is not significantly different from zero either.) I emphasized “statistically significant” to remind you that this means to do a test and get a P-value.\n\\(\\blacksquare\\)\n\nFit a model that contains only the significant explanatory variables from your first regression. How do the R-squared values from the two regressions compare? (The last sentence asks for more or less the same thing as the next part. Answer it either here or there. Either place is good.)\n\nSolution\nOnly height is significant, so that’s the only explanatory variable we need to keep. I would just do the regression straight rather than using update here:\n\nboys.2 &lt;- lm(uptake ~ height, data = boys)\nsummary(boys.2)\n\n\nCall:\nlm(formula = uptake ~ height, data = boys)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.069879 -0.033144  0.001407  0.009581  0.084012 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.843326   0.609198  -6.309 0.000231 ***\nheight       0.040718   0.004648   8.761 2.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05013 on 8 degrees of freedom\nMultiple R-squared:  0.9056,    Adjusted R-squared:  0.8938 \nF-statistic: 76.75 on 1 and 8 DF,  p-value: 2.258e-05\n\n\nIf you want, you can use update here, which looks like this:\n\nboys.2a &lt;- update(boys.1, . ~ . - age - weight - chest)\nsummary(boys.2a)\n\n\nCall:\nlm(formula = uptake ~ height, data = boys)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.069879 -0.033144  0.001407  0.009581  0.084012 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.843326   0.609198  -6.309 0.000231 ***\nheight       0.040718   0.004648   8.761 2.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05013 on 8 degrees of freedom\nMultiple R-squared:  0.9056,    Adjusted R-squared:  0.8938 \nF-statistic: 76.75 on 1 and 8 DF,  p-value: 2.258e-05\n\n\nThis doesn’t go quite so smoothly here because there are three variables being removed, and it’s a bit of work to type them all.\n\\(\\blacksquare\\)\n\nHow has R-squared changed between your two regressions? Describe what you see in a few words.\n\nSolution\nR-squared has dropped by a bit, from 97% to 91%. (Make your own call: pull out the two R-squared numbers, and say a word or two about how they compare. I don’t much mind what you say: “R-squared has decreased (noticeably)”, “R-squared has hardly changed”. But say something.)\n\\(\\blacksquare\\)\n\nCarry out a test comparing the fit of your two regression models. What do you conclude, and therefore what recommendation would you make about the regression that would be preferred?\n\nSolution\nThe word “test” again implies something that produces a P-value with a null hypothesis that you might reject. In this case, the test that compares two models differing by more than one \\(x\\) uses anova, testing the null hypothesis that the two regressions are equally good, against the alternative that the bigger (first) one is better. Feed anova two fitted model objects, smaller first:\n\nanova(boys.2, boys.1)\n\n\n\n  \n\n\n\nThis P-value of 0.123 is not small, so we do not reject the null hypothesis. There is not a significant difference in fit between the two models. Therefore, we should go with the smaller model boys.2 because it is simpler.\nThat drop in R-squared from 97% to 91% was, it turns out, not significant: the three extra variables could have produced a change in R-squared like that, even if they were worthless.6\nIf you have learned about “adjusted R-squared”, you might recall that this is supposed to go down only if the variables you took out should not have been taken out. But adjusted R-squared goes down here as well, from 94% to 89% (not quite as much, therefore). What happens is that adjusted R-squared is rather more relaxed about keeping variables than the anova \\(F\\)-test is; if we had used an \\(\\alpha\\) of something like 0.10, the decision between the two models would have been a lot closer, and this is reflected in the adjusted R-squared values.\n\\(\\blacksquare\\)\n\nObtain a table of correlations between all the variables in the data frame. Do this by feeding the whole data frame into cor. We found that a regression predicting oxygen uptake from just height was acceptably good. What does your table of correlations say about why that is? (Hint: look for all the correlations that are large.)\n\nSolution\nCorrelations first:\n\ncor(boys)\n\n          uptake       age    height    weight     chest\nuptake 1.0000000 0.1361907 0.9516347 0.6576883 0.7182659\nage    0.1361907 1.0000000 0.3274830 0.2307403 0.1657523\nheight 0.9516347 0.3274830 1.0000000 0.7898252 0.7909452\nweight 0.6576883 0.2307403 0.7898252 1.0000000 0.8809605\nchest  0.7182659 0.1657523 0.7909452 0.8809605 1.0000000\n\n\nThe correlations with age are all on the low side, but all the other correlations are high, not just between uptake and the other variables, but between the explanatory variables as well.\nWhy is this helpful in understanding what’s going on? Well, imagine a boy with large height (a tall one). The regression boys.2 says that this alone is enough to predict that such a boy’s oxygen uptake is likely to be large, since the slope is positive. But the correlations tell you more: a boy with large height is also (somewhat) likely to be older (have large age), heavier (large weight) and to have larger chest cavity. So oxygen uptake does depend on those other variables as well, but once you know height you can make a good guess at their values; you don’t need to know them.\nFurther remarks: age has a low correlation with uptake, so its non-significance earlier appears to be “real”: it really does have nothing extra to say, because the other variables have a stronger link with uptake than age. Height, however, seems to be the best way of relating oxygen uptake to any of the other variables. I think the suppositions from earlier about relating oxygen uptake to “bigness”7 in some sense are actually sound, but age and weight and chest capture “bigness” worse than height does. Later, when you learn about Principal Components, you will see that the first principal component, the one that best captures how the variables vary together, is often “bigness” in some sense.\nAnother way to think about these things is via pairwise scatterplots. The nicest way to produce these is via ggpairs from package GGally:\n\nboys %&gt;% ggpairs(progress = FALSE)\n\n\n\n\nA final remark: with five variables, we really ought to have more than ten observations (something like 50 would be better). But with more observations and the same correlation structure, the same issues would come up again, so the question would not be materially changed.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#facebook-friends-and-grey-matter-1",
    "href": "simple-regression.html#facebook-friends-and-grey-matter-1",
    "title": "16  Simple regression",
    "section": "16.18 Facebook friends and grey matter",
    "text": "16.18 Facebook friends and grey matter\nIs there a relationship between the number of Facebook friends a person has, and the density of grey matter in the areas of the brain associated with social perception and associative memory? To find out, a 2012 study measured both of these variables for a sample of 40 students at City University in London (England). The data are at link. The grey matter density is on a \\(z\\)-score standardized scale. The values are separated by tabs.\nThe aim of this question is to produce an R Markdown report that contains your answers to the questions below.\nYou should aim to make your report flow smoothly, so that it would be pleasant for a grader to read, and can stand on its own as an analysis (rather than just being the answer to a question that I set you). Some suggestions: give your report a title and arrange it into sections with an Introduction; add a small amount of additional text here and there explaining what you are doing and why. I don’t expect you to spend a large amount of time on this, but I do hope you will make some effort. (My report came out to 4 Word pages.)\n\nRead in the data and make a scatterplot for predicting the number of Facebook friends from the grey matter density. On your scatterplot, add a smooth trend.\n\nSolution\nBegin your document with a code chunk containing library(tidyverse). The data values are separated by tabs, which you will need to take into account:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/facebook.txt\"\nfb &lt;- read_tsv(my_url)\n\nRows: 40 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (2): GMdensity, FBfriends\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfb\n\n\n\n  \n\n\nggplot(fb, aes(x = GMdensity, y = FBfriends)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\nDescribe what you see on your scatterplot: is there a trend, and if so, what kind of trend is it? (Don’t get too taken in by the exact shape of your smooth trend.) Think “form, direction, strength”.\n\nSolution\nI’d say there seems to be a weak, upward, apparently linear trend. The points are not especially close to the trend, so I don’t think there’s any justification for calling this other than “weak”. (If you think the trend is, let’s say, “moderate”, you ought to say what makes you think that: for example, that the people with a lot of Facebook friends also tend to have a higher grey matter density. I can live with a reasonably-justified “moderate”.) The reason I said not to get taken in by the shape of the smooth trend is that this has a “wiggle” in it: it goes down again briefly in the middle. But this is likely a quirk of the data, and the trend, if there is any, seems to be an upward one.\n\\(\\blacksquare\\)\n\nFit a regression predicting the number of Facebook friends from the grey matter density, and display the output.\n\nSolution\nThat looks like this. You can call the “fitted model object” whatever you like, but you’ll need to get the capitalization of the variable names correct:\n\nfb.1 &lt;- lm(FBfriends ~ GMdensity, data = fb)\nsummary(fb.1)\n\n\nCall:\nlm(formula = FBfriends ~ GMdensity, data = fb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-339.89 -110.01   -5.12   99.80  303.64 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   366.64      26.35  13.916  &lt; 2e-16 ***\nGMdensity      82.45      27.58   2.989  0.00488 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 165.7 on 38 degrees of freedom\nMultiple R-squared:  0.1904,    Adjusted R-squared:  0.1691 \nF-statistic: 8.936 on 1 and 38 DF,  p-value: 0.004882\n\n\nI observe, though I didn’t ask you to, that the R-squared is pretty awful, going with a correlation of\n\nsqrt(0.1904)\n\n[1] 0.4363485\n\n\nwhich would look like as weak of a trend as we saw.8\n\\(\\blacksquare\\)\n\nIs the slope of your regression line significantly different from zero? What does that mean, in the context of the data?\n\nSolution\nThe P-value of the slope is 0.005, which is less than 0.05. Therefore the slope is significantly different from zero. That means that the number of Facebook friends really does depend on the grey matter density, for the whole population of interest and not just the 40 students observed here (that were a sample from that population). I don’t mind so much what you think the population is, but it needs to be clear that the relationship applies to a population. Another way to approach this is to say that you would expect this relationship to show up again in another similar experiment. That also works, because it gets at the idea of reproducibility.\n\\(\\blacksquare\\)\n\nAre you surprised by the results of parts (b) and (d)? Explain briefly.\n\nSolution\nI am surprised, because I thought the trend on the scatterplot was so weak that there would not be a significant slope. I guess there was enough of an upward trend to be significant, and with \\(n=40\\) observations we were able to get a significant slope out of that scatterplot. With this many observations, even a weak correlation can be significantly nonzero. You can be surprised or not, but you need to have some kind of consideration of the strength of the trend on the scatterplot as against the significance of the slope. For example, if you decided that the trend was “moderate” in strength, you would be justified in being less surprised than I was. Here, there is the usual issue that we have proved that the slope is not zero (that the relationship is not flat), but we may not have a very clear idea of what the slope actually is. There are a couple of ways to get a confidence interval. The obvious one is to use R as a calculator and go up and down twice its standard error (to get a rough idea):\n\n82.45 + 2 * 27.58 * c(-1, 1)\n\n[1]  27.29 137.61\n\n\nThe c() thing is to get both confidence limits at once. The smoother way is this:\n\nconfint(fb.1)\n\n                2.5 %   97.5 %\n(Intercept) 313.30872 419.9810\nGMdensity    26.61391 138.2836\n\n\nFeed confint a “fitted model object” and it’ll give you confidence intervals (by default 95%) for all the parameters in it.\nThe confidence interval for the slope goes from about 27 to about 138. That is to say, a one-unit increase in grey matter density goes with an increase in Facebook friends of this much. This is not especially insightful: it’s bigger than zero (the test was significant), but other than that, it could be almost anything. This is where the weakness of the trend comes back to bite us. With this much scatter in our data, we need a much larger sample size to estimate accurately how big an effect grey matter density has.\n\\(\\blacksquare\\)\n\nObtain a scatterplot with the regression line on it.\n\nSolution\nJust a modification of (a):\n\nggplot(fb, aes(x = GMdensity, y = FBfriends)) + geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\nObtain a plot of the residuals from the regression against the fitted values, and comment briefly on it.\n\nSolution\nThis is, to my mind, the easiest way:\n\nggplot(fb.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nThere is some “magic” here, since the fitted model object is not actually a data frame, but it works this way. That looks to me like a completely random scatter of points. Thus, I am completely happy with the straight-line regression that we fitted, and I see no need to improve it.\n(You should make two points here: one, describe what you see, and two, what it implies about whether or not your regression is satisfactory.)\nCompare that residual plot with this one:\n\nggplot(fb.1, aes(x = .fitted, y = .resid)) +\n  geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nNow, why did I try adding a smooth trend, and why is it not necessarily a good idea? The idea of a residual plot is that there should be no trend, and so the smooth trend curve ought to go straight across. The problem is that it will tend to wiggle, just by chance, as here: it looks as if it goes up and down before flattening out. But if you look at the points, they are all over the place, not close to the smooth trend at all. So the smooth trend is rather deceiving. Or, to put it another way, to indicate a real problem, the smooth trend would have to be a lot farther from flat than this one is. I’d call this one basically flat.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#endogenous-nitrogen-excretion-in-carp-1",
    "href": "simple-regression.html#endogenous-nitrogen-excretion-in-carp-1",
    "title": "16  Simple regression",
    "section": "16.19 Endogenous nitrogen excretion in carp",
    "text": "16.19 Endogenous nitrogen excretion in carp\nA paper in Fisheries Science reported on variables that affect “endogenous nitrogen excretion” or ENE in carp raised in Japan. A number of carp were divided into groups based on body weight, and each group was placed in a different tank. The mean body weight of the carp placed in each tank was recorded. The carp were then fed a protein-free diet three times daily for a period of 20 days. At the end of the experiment, the amount of ENE in each tank was measured, in milligrams of total fish body weight per day. (Thus it should not matter that some of the tanks had more fish than others, because the scaling is done properly.)\nFor this question, write a report in R Markdown that answers the questions below and contains some narrative that describes your analysis. Create an HTML document from your R Markdown.\n\nRead the data in from link. There are 10 tanks.\n\nSolution\nJust this. Listing the data is up to you, but doing so and commenting that the values appear to be correct will improve your report.\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/carp.txt\"\ncarp &lt;- read_delim(my_url, \" \")\n\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): tank, bodyweight, ENE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncarp\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nCreate a scatterplot of ENE (response) against bodyweight (explanatory). Add a smooth trend to your plot.\n\nSolution\n\nggplot(carp, aes(x = bodyweight, y = ENE)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis part is just about getting the plot. Comments are coming in a minute. Note that ENE is capital letters, so that ene will not work.\n\\(\\blacksquare\\)\n\nIs there an upward or downward trend (or neither)? Is the relationship a line or a curve? Explain briefly.\n\nSolution\nThe trend is downward: as bodyweight increases, ENE decreases. However, the decrease is rapid at first and then levels off, so the relationship is nonlinear. I want some kind of support for an assertion of non-linearity: anything that says that the slope or rate of decrease is not constant is good.\n\\(\\blacksquare\\)\n\nFit a straight line to the data, and obtain the R-squared for the regression.\n\nSolution\nlm. The first stage is to fit the straight line, saving the result in a variable, and the second stage is to look at the “fitted model object”, here via summary:\n\ncarp.1 &lt;- lm(ENE ~ bodyweight, data = carp)\nsummary(carp.1)\n\n\nCall:\nlm(formula = ENE ~ bodyweight, data = carp)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.800 -1.957 -1.173  1.847  4.572 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.40393    1.31464   8.675 2.43e-05 ***\nbodyweight  -0.02710    0.01027  -2.640   0.0297 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.928 on 8 degrees of freedom\nMultiple R-squared:  0.4656,    Adjusted R-squared:  0.3988 \nF-statistic: 6.971 on 1 and 8 DF,  p-value: 0.0297\n\n\nFinally, you need to give me a (suitably rounded) value for R-squared: 46.6% or 47% or the equivalents as a decimal. I just need the value at this point. This kind of R-squared is actually pretty good for natural data, but the issue is whether we can improve it by fitting a non-linear model.9\n\\(\\blacksquare\\)\n\nObtain a residual plot (residuals against fitted values) for this regression. Do you see any problems? If so, what does that tell you about the relationship in the data?\n\nSolution\nThis is the easiest way: feed the output of the regression straight into ggplot:\n\nggplot(carp.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\n\\(\\blacksquare\\)\n\nFit a parabola to the data (that is, including an \\(x\\)-squared term). Compare the R-squared values for the models in this part and part (d). Does that suggest that the parabola model is an improvement here over the linear model?\n\nSolution\nAdd bodyweight-squared to the regression. Don’t forget the I():\n\ncarp.2 &lt;- lm(ENE ~ bodyweight + I(bodyweight^2), data = carp)\nsummary(carp.2)\n\n\nCall:\nlm(formula = ENE ~ bodyweight + I(bodyweight^2), data = carp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0834 -1.7388 -0.5464  1.3841  2.9976 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     13.7127373  1.3062494  10.498 1.55e-05 ***\nbodyweight      -0.1018390  0.0288109  -3.535  0.00954 ** \nI(bodyweight^2)  0.0002735  0.0001016   2.692  0.03101 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.194 on 7 degrees of freedom\nMultiple R-squared:  0.7374,    Adjusted R-squared:  0.6624 \nF-statistic: 9.829 on 2 and 7 DF,  p-value: 0.009277\n\n\nR-squared has gone up from 47% to 74%, a substantial improvement. This suggests to me that the parabola model is a substantial improvement.10\nI try to avoid using the word “significant” in this context, since we haven’t actually done a test of significance.\nThe reason for the I() is that the up-arrow has a special meaning in lm, relating to interactions between factors (as in ANOVA), that we don’t want here. Putting I() around it means “use as is”, that is, raise bodyweight to power 2, rather than using the special meaning of the up-arrow in lm.\nBecause it’s the up-arrow that is the problem, this applies whenever you’re raising an explanatory variable to a power (or taking a reciprocal or a square root, say).\n\\(\\blacksquare\\)\n\nIs the test for the slope coefficient for the squared term significant? What does this mean?\n\nSolution\nLook along the bodyweight-squared line to get a P-value of 0.031. This is less than the default 0.05, so it is significant. This means, in short, that the quadratic model is a significant improvement over the linear one.11 Said longer: the null hypothesis being tested is that the slope coefficient of the squared term is zero (that is, that the squared term has nothing to add over the linear model). This is rejected, so the squared term has something to add in terms of quality of prediction.\n\\(\\blacksquare\\)\n\nMake the scatterplot of part (b), but add the fitted curve. Describe any way in which the curve fails to fit well.\n\nSolution\nThis is a bit slippery, because the points to plot and the fitted curve are from different data frames. What you do in this case is to put a data= in one of the geoms, which says “don’t use the data frame that was in the ggplot, but use this one instead”. I would think about starting with the regression object carp.2 as my base data frame, since we want (or I want) to do two things with that: plot the fitted values and join them with lines. Then I want to add the original data, just the points:\n\nggplot(carp.2, aes(x = carp$bodyweight, y = .fitted), colour = \"blue\") +\n  geom_line(colour = \"blue\") +\n  geom_point(data = carp, aes(x = bodyweight, y = ENE))\n\n\n\n\nThis works, but is not very aesthetic, because the bodyweight that is plotted against the fitted values is in the wrong data frame, and so we have to use the dollar-sign thing to get it from the right one.\nA better way around this is “augment” the data with output from the regression object. This is done using augment from package broom:\n\nlibrary(broom)\ncarp.2a &lt;- augment(carp.2, carp)\ncarp.2a\n\n\n\n  \n\n\n\nso now you see what carp.2a has in it, and then:\n\ng &lt;- ggplot(carp.2a, aes(x = bodyweight, y = .fitted)) +\n  geom_line(colour = \"blue\") +\n  geom_point(aes(y = ENE))\n\nThis is easier coding: there are only two non-standard things. The first is that the fitted-value lines should be a distinct colour like blue so that you can tell them from the data points. The second thing is that for the second geom_point, the one that plots the data, the \\(x\\) coordinate bodyweight is correct so that we don’t have to change that; we only have to change the \\(y\\)-coordinate, which is ENE. The plot is this:\n\ng\n\n\n\n\nConcerning interpretation, you have a number of possibilities here. The simplest is that the points in the middle are above the curve, and the points at the ends are below. (That is, negative residuals at the ends, and positive ones in the middle, which gives you a hint for the next part.) Another is that the parabola curve fails to capture the shape of the relationship; for example, I see nothing much in the data suggesting that the relationship should go back up, and even given that, the fitted curve doesn’t go especially near any of the points.\nI was thinking that the data should be fit better by something like the left half of an upward-opening parabola, but I guess the curvature on the left half of the plot suggests that it needs most of the left half of the parabola just to cover the left half of the plot.\nThe moral of the story, as we see in the next part, is that the parabola is the wrong curve for the job.\n\\(\\blacksquare\\)\n\nObtain a residual plot for the parabola model. Do you see any problems with it? (If you do, I’m not asking you to do anything about them in this question, but I will.)\n\n\\(\\blacksquare\\)\nThe same idea as before for the other residual plot. Use the fitted model object carp.2 as your data frame for the ggplot:\n\nggplot(carp.2, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nI think this is still a curve (or, it goes down and then sharply up at the end). Either way, there is still a pattern.\nThat was all I needed, but as to what this means: our parabola was a curve all right, but it appears not to be the right kind of curve. I think the original data looks more like a hyperbola (a curve like \\(y=1/x\\)) than a parabola, in that it seems to decrease fast and then gradually to a limit, and that suggests, as in the class example, that we should try an asymptote model. Note how I specify it, with the I() thing again, since / has a special meaning to lm in the same way that ^ does:\n\ncarp.3 &lt;- lm(ENE ~ I(1 / bodyweight), data = carp)\nsummary(carp.3)\n\n\nCall:\nlm(formula = ENE ~ I(1/bodyweight), data = carp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.29801 -0.12830  0.04029  0.26702  0.91707 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       5.1804     0.2823   18.35 8.01e-08 ***\nI(1/bodyweight) 107.6690     5.8860   18.29 8.21e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6121 on 8 degrees of freedom\nMultiple R-squared:  0.9766,    Adjusted R-squared:  0.9737 \nF-statistic: 334.6 on 1 and 8 DF,  p-value: 8.205e-08\n\n\nThat fits extraordinarily well, with an R-squared up near 98%. The intercept is the asymptote, which suggests a (lower) limit of about 5.2 for ENE (in the limit for large bodyweight). We would have to ask the fisheries scientist whether this kind of thing is a reasonable biological mechanism. It says that a carp always has some ENE, no matter how big it gets, but a smaller carp will have a lot more.\nDoes the fitted value plot look reasonable now? This is augment again since the fitted values and observed data come from different data frames:\n\nlibrary(broom)\naugment(carp.3, carp) %&gt;%\n  ggplot(aes(x = bodyweight, y = .fitted)) +\n  geom_line(colour = \"blue\") +\n  geom_point(aes(y = ENE))\n\n\n\n\nI’d say that does a really nice job of fitting the data. But it would be nice to have a few more tanks with large-bodyweight fish, to convince us that we have the shape of the trend right.\nAnd, as ever, the residual plot. That’s a lot easier than the plot we just did:\n\nggplot(carp.3, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nAll in all, that looks pretty good (and certainly a vast improvement over the ones you got before).\nWhen you write up your report, you can make it flow better by writing it in a way that suggests that each thing was the obvious thing to do next: that is, that you would have thought to do it next, rather than me telling you what to do.\nMy report (as an R Markdown file) is at link. Download it, knit it, play with it.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#salaries-of-social-workers-1",
    "href": "simple-regression.html#salaries-of-social-workers-1",
    "title": "16  Simple regression",
    "section": "16.20 Salaries of social workers",
    "text": "16.20 Salaries of social workers\nAnother salary-prediction question: does the number of years of work experience that a social worker has help to predict their salary? Data for 50 social workers are in link.\n\nRead the data into R. Check that you have 50 observations on two variables. Also do something to check that the years of experience and annual salary figures look reasonable overall.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/socwork.txt\"\nsoc &lt;- read_delim(my_url, \" \")\n\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): experience, salary\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsoc\n\n\n\n  \n\n\n\nThat checks that we have the right number of observations; to check that we have sensible values, something like summary is called for:\n\nsummary(soc)\n\n   experience        salary     \n Min.   : 1.00   Min.   :16105  \n 1st Qu.:13.50   1st Qu.:36990  \n Median :20.00   Median :50948  \n Mean   :18.12   Mean   :50171  \n 3rd Qu.:24.75   3rd Qu.:65204  \n Max.   :28.00   Max.   :99139  \n\n\nA person working in any field cannot have a negative number of years of experience, and cannot have more than about 40 years of experience (or else they would have retired). Our experience numbers fit that. Salaries had better be five or six figures, and salaries for social workers are not generally all that high, so these figures look reasonable.\nA rather more tidyverse way is this:\n\nsoc %&gt;% \n  summarize(across(everything(), \n                   list(min = \\(x) min(x),  max = \\(x) max(x))))\n\n\n\n  \n\n\n\nThis gets the minimum and maximum of all the variables. I would have liked them arranged in a nice rectangle (min and max as rows, the variables as columns), but that’s not how this came out. We fix that shortly.\nThe code so far uses across. This means to do something across multiple columns. In this case, we want to do the calculation on all the columns, so we use the select-helper everything. You can use any of the other select-helpers like starts_with, or you could do something like where(is.numeric) to do your summaries only on the quantitative columns (which would also work here). The thing after the everything() means “for each column selected, work out the min and max of it”; x is our name for “the variable we are looking at at the moment”.\nWhat, you want a nice rectangle? This is a pivot-longer, but a fancy version because the column names encode two kinds of things, a variable and a statistic. I took the view that I wanted variables in columns (as usual), and the different summary statistics in rows. This means that the first part of the column names we created above (eg. the salary part of salary_min) should stay in columns, and the rest of it should be pivoted longer. That means using the special name .value for the things that should stay as columns:\n\nsoc %&gt;% \n  summarize(across(everything(), \n                   list(min = \\(x) min(x),  max = \\(x) max(x)))) %&gt;% \n  pivot_longer(everything(), \n               names_to = c(\".value\", \"statistic\"), \n               names_sep = \"_\"\n               )\n\n\n\n  \n\n\n\nNote that we’re using two simpler tools here, rather than one complicated one: first we get the summary statistics, and once we have that, we can do some tidying to get it arranged the way we want.\nYour first guess is likely to be to make it too long:\n\nsoc %&gt;% \n  summarize(across(everything(), \n                   list(min = \\(x) min(x),  max = \\(x) max(x)))) %&gt;% \n  pivot_longer(everything(), \n               names_to = c(\"variable\", \"statistic\"), \n               names_sep = \"_\", \n               values_to = \"value\"\n               )\n\n\n\n  \n\n\n\nand then you’ll have to make it wider, or recall that you can do the thing with .value. We are working “columnwise”, doing something for each column, no matter how many there are. My go-to for this stuff is here.\nAnother way to work is with the five-number summary. This gives a more nuanced picture of the data values we have.12\nThe base-R five-number summary looks like this:\n\nqq &lt;- quantile(soc$experience)\nqq\n\n   0%   25%   50%   75%  100% \n 1.00 13.50 20.00 24.75 28.00 \n\n\nThis is what’s known as a “named vector”. The numbers on the bottom are the summaries themselves, and the names above say which percentile you are looking at. Unfortunately, the tidyverse doesn’t like names, so modelling after the above doesn’t quite work:\n\nsoc %&gt;% \n  summarize(across(everything(), list(q = \\(x) quantile(x))))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n\n\n  \n\n\n\nYou can guess which percentile is which (they have to be in order), but this is not completely satisfactory. It also gives a warning because the summary is five numbers long, rather than only one (like the mean, for example), and this is not the preferred way to handle this.\nThe warning mentions reframe, which is new (as in, less than a year old as I write this). Let’s see how it goes here:\n\nsoc %&gt;% \n  reframe(q_exp = quantile(experience), q_sal = quantile(salary))\n\n\n\n  \n\n\n\nThe idea is that reframe is like summarize, but it is designed for when your summary function returns more than one number, not just one number per group like mean or median do.\nThis is not quite the best (I don’t see the percentiles and I have to repeat myself), but at least I no longer get a warning. Here’s how you do it with across:\n\nsoc %&gt;% \n  reframe(across(everything(), \\(x) enframe(quantile(x)), .unpack = TRUE))\n\n\n\n  \n\n\n\nThe enframe turns a “named vector” (that is, a thing like my qq above) into a dataframe with two columns, one called name with the names (percentiles), and one called value with the values. By using across, you get those two columns for each variable, and you can see which of the five numbers is which percentile in each case.\n\\(\\blacksquare\\)\n\nMake a scatterplot showing how salary depends on experience. Does the nature of the trend make sense?\n\nSolution\nThe usual:\n\nggplot(soc, aes(x = experience, y = salary)) + geom_point()\n\n\n\n\nAs experience goes up, salary also goes up, as you would expect. Also, the trend seems more or less straight.\n\\(\\blacksquare\\)\n\nFit a regression predicting salary from experience, and display the results. Is the slope positive or negative? Does that make sense?\n\nSolution\n\nsoc.1 &lt;- lm(salary ~ experience, data = soc)\nsummary(soc.1)\n\n\nCall:\nlm(formula = salary ~ experience, data = soc)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17666.3  -5498.2   -726.7   4667.7  27811.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11368.7     3160.3   3.597 0.000758 ***\nexperience    2141.4      160.8  13.314  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8642 on 48 degrees of freedom\nMultiple R-squared:  0.7869,    Adjusted R-squared:  0.7825 \nF-statistic: 177.3 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\nThe slope is (significantly) positive, which squares with our guess (more experience goes with greater salary), and also the upward trend on the scatterplot. The value of the slope is about 2,000; this means that one more year of experience goes with about a $2,000 increase in salary.\n\\(\\blacksquare\\)\n\nObtain and plot the residuals against the fitted values. What problem do you see?\n\nSolution\nThe easiest way to do this with ggplot is to plot the regression object (even though it is not actually a data frame), and plot the .fitted and .resid columns in it, not forgetting the initial dots:\n\nggplot(soc.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nI see a “fanning-out”: the residuals are getting bigger in size (further away from zero) as the fitted values get bigger. That is, when the (estimated) salary gets larger, it also gets more variable.\nFanning-out is sometimes hard to see. What you can do if you suspect that it might have happened is to plot the absolute value of the residuals against the fitted values. The absolute value is the residual without its plus or minus sign, so if the residuals are getting bigger in size, their absolute values are getting bigger. That would look like this:\n\nggplot(soc.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI added a smooth trend to this to help us judge whether the absolute-value-residuals are getting bigger as the fitted values get bigger. It looks to me as if the overall trend is an increasing one, apart from those few small fitted values that have larger-sized residuals. Don’t get thrown off by the kinks in the smooth trend. Here is a smoother version:\n\nggplot(soc.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth(span = 2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe larger fitted values, according to this, have residuals larger in size.\nThe thing that controls the smoothness of the smooth trend is the value of span in geom_smooth. The default is 0.75. The larger the value you use, the smoother the trend; the smaller, the more wiggly. I’m inclined to think that the default value is a bit too small. Possibly this value is too big, but it shows you the idea.\n\\(\\blacksquare\\)\n\nThe problem you unearthed in the previous part is often helped by a transformation. Run Box-Cox on your data to find a suitable transformation. What transformation is suggested?\n\nSolution\nYou’ll need to load (and install if necessary) the package MASS that contains boxcox:\n\nlibrary(MASS)\n\nWhen you run this, you may see a warning containing the word “masked”. I talk about that below.\n\nboxcox(salary ~ experience, data = soc)\n\n\n\n\nThat one looks like \\(\\lambda=0\\) or log. You could probably also justify fourth root (power 0.25), but log is a very common transformation, which people won’t need much persuasion to accept.\nExtra: There’s one annoyance with MASS: it has a select (which I have never used), and if you load tidyverse first and MASS second, as I have done here, when you mean to run the column-selection select, it will actually run the select that comes from MASS, and give you an error that you will have a terrible time debugging. That’s what that “masked” message was when you loaded MASS. This is a great place to learn about the conflicted package. See here for how it works. (Scroll down to under the list of files.)\nIf you want to insist on something like “the select that lives in dplyr”, you can do that by saying dplyr::select. But this is kind of cumbersome if you don’t need to do it.\n\\(\\blacksquare\\)\n\nUse your transformed response in a regression, showing the summary.\n\nSolution\nYou can do the transformation right in the lm, as I do below, or if you prefer, you can create a new column that is the log-salary and then use that in the lm. Either way is good:\n\nsoc.3 &lt;- lm(log(salary) ~ experience, data = soc)\nsummary(soc.3)\n\n\nCall:\nlm(formula = log(salary) ~ experience, data = soc)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.35435 -0.09046 -0.01725  0.09739  0.26355 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9.841315   0.056356  174.63   &lt;2e-16 ***\nexperience  0.049979   0.002868   17.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1541 on 48 degrees of freedom\nMultiple R-squared:  0.8635,    Adjusted R-squared:  0.8607 \nF-statistic: 303.7 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\n\\(\\blacksquare\\)\n\nObtain and plot the residuals against the fitted values for this regression. Do you seem to have solved the problem with the previous residual plot?\n\nSolution\nAs we did before, treating the regression object as if it were a data frame:\n\nggplot(soc.3, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nThat, to my mind, is a horizontal band of points, so I would say yes, I have solved the fanning out.\nOne concern I have about the residuals is that there seem to be a couple of very negative values: that is, are the residuals normally distributed as they should be? Well, that’s easy enough to check:\n\nggplot(soc.3, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nThe issues here are that those bottom two values are a bit too low, and the top few values are a bit bunched up (that curve at the top). It is really not bad, though, so I am making the call that I don’t think I needed to worry. Note that the transformation we found here is the same as the log-salary used by the management consultants in the backward-elimination question, and with the same effect: an extra year of experience goes with a percent increase in salary.\nWhat increase? Well, the slope is about 0.05, so adding a year of experience is predicted to increase log-salary by 0.05, or to multiply actual salary by\n\nexp(0.05)\n\n[1] 1.051271\n\n\nor to increase salary by about 5%.13\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#predicting-volume-of-wood-in-pine-trees-1",
    "href": "simple-regression.html#predicting-volume-of-wood-in-pine-trees-1",
    "title": "16  Simple regression",
    "section": "16.21 Predicting volume of wood in pine trees",
    "text": "16.21 Predicting volume of wood in pine trees\nIn forestry, the financial value of a tree is the volume of wood that it contains. This is difficult to estimate while the tree is still standing, but the diameter is easy to measure with a tape measure (to measure the circumference) and a calculation involving \\(\\pi\\), assuming that the cross-section of the tree is at least approximately circular. The standard measurement is “diameter at breast height” (that is, at the height of a human breast or chest), defined as being 4.5 feet above the ground.\nSeveral pine trees had their diameter measured shortly before being cut down, and for each tree, the volume of wood was recorded. The data are in link. The diameter is in inches and the volume is in cubic inches. Is it possible to predict the volume of wood from the diameter?\n\nRead the data into R and display the values (there are not very many).\n\nSolution\nObserve that the data values are separated by spaces, and therefore that read_delim will do it:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/pinetrees.txt\"\ntrees &lt;- read_delim(my_url, \" \")\n\nRows: 10 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): diameter, volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntrees\n\n\n\n  \n\n\n\nThat looks like the data file.\n\\(\\blacksquare\\)\n\nMake a suitable plot.\n\nSolution\nNo clues this time. You need to recognize that you have two quantitative variables, so that a scatterplot is called for. Also, the volume is the response, so that should go on the \\(y\\)-axis:\n\nggplot(trees, aes(x = diameter, y = volume)) + geom_point()\n\n\n\n\nYou can put a smooth trend on it if you like, which would look like this:\n\nggplot(trees, aes(x = diameter, y = volume)) +\n  geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI’ll take either of those for this part, though I think the smooth trend actually obscures the issue here (because there is not so much data).\n\\(\\blacksquare\\)\n\nDescribe what you learn from your plot about the relationship between diameter and volume, if anything.\n\nSolution\nThe word “relationship” offers a clue that a scatterplot would have been a good idea, if you hadn’t realized by now. I am guided by “form, direction, strength” in looking at a scatterplot:\n\nForm: it is an apparently linear relationship.\nDirection: it is an upward trend: that is, a tree with a larger diameter also has a larger volume of wood. (This is not very surprising.)\nStrength: I’d call this a strong (or moderate-to-strong) relationship. (We’ll see in a minute what the R-squared is.)\n\nYou don’t need to be as formal as this, but you do need to get at the idea that it is an upward trend, apparently linear, and at least fairly strong.14\n\\(\\blacksquare\\)\n\nFit a (linear) regression, predicting volume from diameter, and obtain the summary. How would you describe the R-squared?\n\nSolution\nMy naming convention is (usually) to call the fitted model object by the name of the response variable and a number.15\n\nvolume.1 &lt;- lm(volume ~ diameter, data = trees)\nsummary(volume.1)\n\n\nCall:\nlm(formula = volume ~ diameter, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.497  -9.982   1.751   8.959  28.139 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -191.749     23.954  -8.005 4.35e-05 ***\ndiameter      10.894      0.801  13.600 8.22e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.38 on 8 degrees of freedom\nMultiple R-squared:  0.9585,    Adjusted R-squared:  0.9534 \nF-statistic:   185 on 1 and 8 DF,  p-value: 8.217e-07\n\n\nR-squared is nearly 96%, so the relationship is definitely a strong one.\nI also wanted to mention the broom package, which was installed with the tidyverse but which you need to load separately. It provides two handy ways to summarize a fitted model (regression, analysis of variance or whatever):\n\nlibrary(broom)\nglance(volume.1)\n\n\n\n  \n\n\n\nThis gives a one-line summary of a model, including things like R-squared. This is handy if you’re fitting more than one model, because you can collect the one-line summaries together into a data frame and eyeball them.\nThe other summary is this one:\n\ntidy(volume.1)\n\n\n\n  \n\n\n\nThis gives a table of intercepts, slopes and their P-values, but the value to this one is that it is a data frame, so if you want to pull anything out of it, you know how to do that:16\n\ntidy(volume.1) %&gt;% filter(term == \"diameter\")\n\n\n\n  \n\n\n\nThis gets the estimated slope and its P-value, without worrying about the corresponding things for the intercept, which are usually of less interest anyway.\n\\(\\blacksquare\\)\n\nDraw a graph that will help you decide whether you trust the linearity of this regression. What do you conclude? Explain briefly.\n\nSolution\nThe thing I’m fishing for is a residual plot (of the residuals against the fitted values), and on it you are looking for a random mess of nothingness:\n\nggplot(volume.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nMake a call. You could say that there’s no discernible pattern, especially with such a small data set, and therefore that the regression is fine. Or you could say that there is fanning-in: the two points on the right have residuals close to 0 while the points on the left have residuals larger in size. Say something.\nI don’t think you can justify a curve or a trend, because the residuals on the left are both positive and negative.\nMy feeling is that the residuals on the right are close to 0 because these points have noticeably larger diameter than the others, and they are influential points in the regression that will pull the line closer to themselves. This is why their residuals are close to zero. But I am happy with either of the points made in the paragraph under the plot.\n\\(\\blacksquare\\)\n\nWhat would you guess would be the volume of a tree of diameter zero? Is that what the regression predicts? Explain briefly.\n\nSolution\nLogically, a tree that has diameter zero is a non-existent tree, so its volume should be zero as well. In the regression, the quantity that says what volume is when diameter is zero is the intercept. Here the intercept is \\(-192\\), which is definitely not zero. In fact, if you look at the P-value, the intercept is significantly less than zero. Thus, the model makes no logical sense for trees of small diameter. The smallest tree in the data set has diameter 18, which is not really small, I suppose, but it is a little disconcerting to have a model that makes no logical sense.\n\\(\\blacksquare\\)\n\nA simple way of modelling a tree’s shape is to pretend it is a cone, like this, but probably taller and skinnier:\n\n\nwith its base on the ground. What is the relationship between the diameter (at the base) and volume of a cone? (If you don’t remember, look it up. You’ll probably get a formula in terms of the radius, which you’ll have to convert. Cite the website you used.)\nSolution\nAccording to link, the volume of a cone is \\(V=\\pi r^2h/3\\), where \\(V\\) is the volume, \\(r\\) is the radius (at the bottom of the cone) and \\(h\\) is the height. The diameter is twice the radius, so replace \\(r\\) by \\(d/2\\), \\(d\\) being the diameter. A little algebra gives \\[ V = \\pi d^2 h / 12.\\]\n\\(\\blacksquare\\)\n\nFit a regression model that predicts volume from diameter according to the formula you obtained in the previous part. You can assume that the trees in this data set are of similar heights, so that the height can be treated as a constant.\nDisplay the results.\n\nSolution\nAccording to my formula, the volume depends on the diameter squared, which I include in the model thus:\n\nvolume.2 &lt;- lm(volume ~ I(diameter^2), data = trees)\nsummary(volume.2)\n\n\nCall:\nlm(formula = volume ~ I(diameter^2), data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.708  -9.065  -5.722   3.032  40.816 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -30.82634   13.82243   -2.23   0.0563 .  \nI(diameter^2)   0.17091    0.01342   12.74 1.36e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.7 on 8 degrees of freedom\nMultiple R-squared:  0.953, Adjusted R-squared:  0.9471 \nF-statistic: 162.2 on 1 and 8 DF,  p-value: 1.359e-06\n\n\nThis adds an intercept as well, which is fine (there are technical difficulties around removing the intercept).\nThat’s as far as I wanted you to go, but (of course) I have a few comments.\nThe intercept here is still negative, but not significantly different from zero, which is a step forward. The R-squared for this regression is very similar to that from our linear model (the one for which the intercept made no sense). So, from that point of view, either model predicts the data well. I should look at the residuals from this one:\n\nggplot(volume.2, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nI really don’t think there are any problems there.\nNow, I said to assume that the trees are all of similar height. This seems entirely questionable, since the trees vary quite a bit in diameter, and you would guess that trees with bigger diameter would also be taller. It seems more plausible that the same kind of trees (pine trees in this case) would have the same “shape”, so that if you knew the diameter you could predict the height, with larger-diameter trees being taller. Except that we don’t have the heights here, so we can’t build a model for that.\nSo I went looking in the literature. I found this paper: link. This gives several models for relationships between volume, diameter and height. In the formulas below, there is an implied “plus error” on the right, and the \\(\\alpha_i\\) are parameters to be estimated.\nFor predicting height from diameter (equation 1 in paper):\n\\[  h = \\exp(\\alpha_1+\\alpha_2 d^{\\alpha_3}) \\]\nFor predicting volume from height and diameter (equation 6):\n\\[  V = \\alpha_1 d^{\\alpha_2} h^{\\alpha_3} \\]\nThis is a take-off on our assumption that the trees were cone-shaped, with cone-shaped trees having \\(\\alpha_1=\\pi/12\\), \\(\\alpha_2=2\\) and \\(\\alpha_3=1\\). The paper uses different units, so \\(\\alpha_1\\) is not comparable, but \\(\\alpha_2\\) and \\(\\alpha_3\\) are (as estimated from the data in the paper, which were for longleaf pine) quite close to 2 and 1.\nLast, the actual relationship that helps us: predicting volume from just diameter (equation 5):\n\\[  V = \\alpha_1 d^{\\alpha_2}\\]\nThis is a power law type of relationship. For example, if you were willing to pretend that a tree was a cone with height proportional to diameter (one way of getting at the idea of a bigger tree typically being taller, instead of assuming constant height as we did), that would imply \\(\\alpha_2=3\\) here.\nThis is non-linear as it stands, but we can bash it into shape by taking logs:\n\\[\n\\ln V = \\ln \\alpha_1 + \\alpha_2 \\ln d\n\\]\nso that log-volume has a linear relationship with log-diameter and we can go ahead and estimate it:\n\nvolume.3 &lt;- lm(log(volume) ~ log(diameter), data = trees)\nsummary(volume.3)\n\n\nCall:\nlm(formula = log(volume) ~ log(diameter), data = trees)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40989 -0.22341  0.01504  0.10459  0.53596 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -5.9243     1.1759  -5.038    0.001 ** \nlog(diameter)   3.1284     0.3527   8.870 2.06e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3027 on 8 degrees of freedom\nMultiple R-squared:  0.9077,    Adjusted R-squared:  0.8962 \nF-statistic: 78.68 on 1 and 8 DF,  p-value: 2.061e-05\n\n\nThe parameter that I called \\(\\alpha_2\\) above is the slope of this model, 3.13. This is a bit different from the figure in the paper, which was 2.19. I think these are comparable even though the other parameter is not (again, measurements in different units, plus, this time we need to take the log of it). I think the “slopes” are comparable because we haven’t estimated our slope all that accurately:\n\nconfint(volume.3)\n\n                  2.5 %    97.5 %\n(Intercept)   -8.635791 -3.212752\nlog(diameter)  2.315115  3.941665\n\n\nFrom 2.3 to 3.9. It is definitely not zero, but we are rather less sure about what it is, and 2.19 is not completely implausible.\nThe R-squared here, though it is less than the other ones we got, is still high. The residuals are these:\n\nggplot(volume.3, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nwhich again seem to show no problems. The residuals are smaller in size now because of the log transformation: the actual and predicted log-volumes are smaller numbers than the actual and predicted volumes, so the residuals are now closer to zero.\nDoes this model behave itself at zero? Well, roughly at least: if the diameter is very small, its log is very negative, and the predicted log-volume is also very negative (the slope is positive). So the predicted actual volume will be close to zero. If you want to make that mathematically rigorous, you can take limits, but that’s the intuition. We can also do some predictions: set up a data frame that has a column called diameter with some diameters to predict for:\n\nd &lt;- tibble(diameter = c(1, 2, seq(5, 50, 5)))\nd\n\n\n\n  \n\n\n\nand then feed that into predictionsfrom package marginaleffects:\n\np &lt;- cbind(predictions(volume.3, newdata = d)) \np %&gt;% select(diameter, estimate, conf.low, conf.high) -&gt; pp\npp\n\n\n\n  \n\n\n\nThese are predicted log-volumes, so we’d better anti-log them. log in R is natural logs, so this is inverted using exp. The ends of the confidence intervals can be exp-ed as well, which I do all at once:\n\npp %&gt;% mutate(across(-diameter, \\(x) exp(x)))\n\n\n\n  \n\n\n\nFor a diameter near zero, the predicted volume appears to be near zero as well. If you don’t like the scientific notation:\n\npp %&gt;% mutate(across(-diameter, \\(x) exp(x))) %&gt;% \n  mutate(across(-diameter, \\(x) format(x, scientific = FALSE)))\n\n\n\n  \n\n\n\nNote now that these, though they look like numbers, are actually text, so if you want to display numbers in non-scientific notation like this, do it at the very end, after you have finished any calculations with the numbers.\n\nI mentioned broom earlier. We can make a data frame out of the one-line summaries of our three models:\n\nbind_rows(glance(volume.1), glance(volume.2), glance(volume.3))\n\n\n\n  \n\n\n\n(I mistakenly put glimpse instead of glance there the first time. The former is for a quick look at a data frame, while the latter is for a quick look at a model.)\nThe three R-squareds are all high, with the one from the third model being a bit lower as we saw before.\nMy code is rather repetitious. There has to be a way to streamline it. I was determined to find out how. My solution involves putting the three models in a list-column, and then using rowwise to get the glance output for each one.\n\ntibble(i = 1:3, model = list(volume.1, volume.2, volume.3)) %&gt;% \n  rowwise() %&gt;% \n  mutate(glances = list(glance(model))) %&gt;% \n  unnest(glances)\n\n\n\n  \n\n\n\nI almost got caught by forgetting the list on the definition of glances. I certainly need it, because the output from glance is a (one-row) dataframe, not a single number.\nIt works. You see the three R-squared values in the first column of numbers. The third model is otherwise a lot different from the others because it has a different response variable.\nOther thoughts:\nHow might you measure or estimate the height of a tree (other than by climbing it and dropping a tape measure down)? One way, that works if the tree is fairly isolated, is to walk away from its base. Periodically, you point at the top of the tree, and when the angle between your arm and the ground reaches 45 degrees, you stop walking. (If it’s greater than 45 degrees, you walk further away, and if it’s less, you walk back towards the tree.) The distance between you and the base of the tree is then equal to the height of the tree, and if you have a long enough tape measure you can measure it.\nThe above works because the tangent of 45 degrees is 1. If you have a device that will measure the actual angle,17 you can be any distance away from the tree, point the device at the top, record the angle, and do some trigonometry to estimate the height of the tree (to which you add the height of your eyes).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#tortoise-shells-and-eggs-1",
    "href": "simple-regression.html#tortoise-shells-and-eggs-1",
    "title": "16  Simple regression",
    "section": "16.22 Tortoise shells and eggs",
    "text": "16.22 Tortoise shells and eggs\nA biologist measured the length of the carapace (shell) of female tortoises, and then x-rayed the tortoises to count how many eggs they were carrying. The length is measured in millimetres. The data are in link. The biologist is wondering what kind of relationship, if any, there is between the carapace length (as an explanatory variable) and the number of eggs (as a response variable).\n\nRead in the data, and check that your values look reasonable.\n\nSolution\nLook at the data first. The columns are aligned and separated by more than one space, so it’s read_table:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/tortoise-eggs.txt\"\ntortoises &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  length = col_double(),\n  eggs = col_double()\n)\n\ntortoises\n\n\n\n  \n\n\n\nThose look the same as the values in the data file. (Some comment is needed here. I don’t much mind what, but something that suggests that you have eyeballed the data and there are no obvious problems: that is what I am looking for.)\n\\(\\blacksquare\\)\n\nObtain a scatterplot, with a smooth trend, of the data.\n\nSolution\nSomething like this:\n\nggplot(tortoises, aes(x = length, y = eggs)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\nThe biologist expected that a larger tortoise would be able to carry more eggs. Is that what the scatterplot is suggesting? Explain briefly why or why not.\n\nSolution\nThe biologist’s expectation is of an upward trend. But it looks as if the trend on the scatterplot is up, then down, ie. a curve rather than a straight line. So this is not what the biologist was expecting.\n\\(\\blacksquare\\)\n\nFit a straight-line relationship and display the summary.\n\nSolution\n\ntortoises.1 &lt;- lm(eggs ~ length, data = tortoises)\nsummary(tortoises.1)\n\n\nCall:\nlm(formula = eggs ~ length, data = tortoises)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7790 -1.1772 -0.0065  2.0487  4.8556 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.43532   17.34992  -0.025    0.980\nlength       0.02759    0.05631   0.490    0.631\n\nResidual standard error: 3.411 on 16 degrees of freedom\nMultiple R-squared:  0.01478,   Adjusted R-squared:  -0.0468 \nF-statistic:  0.24 on 1 and 16 DF,  p-value: 0.6308\n\n\nI didn’t ask for a comment, but feel free to observe that this regression is truly awful, with an R-squared of less than 2% and a non-significant effect of length.\n\\(\\blacksquare\\)\n\nAdd a squared term to your regression, fit that and display the summary.\n\nSolution\nThe I() is needed because the raise-to-a-power symbol has a special meaning in a model formula, and we want to not use that special meaning:\n\ntortoises.2 &lt;- lm(eggs ~ length + I(length^2), data = tortoises)\nsummary(tortoises.2)\n\n\nCall:\nlm(formula = eggs ~ length + I(length^2), data = tortoises)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0091 -1.8480 -0.1896  2.0989  4.3605 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -8.999e+02  2.703e+02  -3.329  0.00457 **\nlength       5.857e+00  1.750e+00   3.347  0.00441 **\nI(length^2) -9.425e-03  2.829e-03  -3.332  0.00455 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.671 on 15 degrees of freedom\nMultiple R-squared:  0.4338,    Adjusted R-squared:  0.3583 \nF-statistic: 5.747 on 2 and 15 DF,  p-value: 0.01403\n\n\nAnother way is to use update:\n\ntortoises.2a &lt;- update(tortoises.1, . ~ . + I(length^2))\nsummary(tortoises.2a)\n\n\nCall:\nlm(formula = eggs ~ length + I(length^2), data = tortoises)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0091 -1.8480 -0.1896  2.0989  4.3605 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -8.999e+02  2.703e+02  -3.329  0.00457 **\nlength       5.857e+00  1.750e+00   3.347  0.00441 **\nI(length^2) -9.425e-03  2.829e-03  -3.332  0.00455 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.671 on 15 degrees of freedom\nMultiple R-squared:  0.4338,    Adjusted R-squared:  0.3583 \nF-statistic: 5.747 on 2 and 15 DF,  p-value: 0.01403\n\n\n\\(\\blacksquare\\)\n\nIs a curve better than a line for these data? Justify your answer in two ways: by comparing a measure of fit, and by doing a suitable test of significance.\n\nSolution\nAn appropriate measure of fit is R-squared. For the straight line, this is about 0.01, and for the regression with the squared term it is about 0.43. This tells us that a straight line fits appallingly badly, and that a curve fits a lot better. This doesn’t do a test, though. For that, look at the slope of the length-squared term in the second regression; in particular, look at its P-value. This is 0.0045, which is small: the squared term is necessary, and taking it out would be a mistake. The relationship really is curved, and trying to describe it with a straight line would be a big mistake.\n\\(\\blacksquare\\)\n\nMake a residual plot for the straight line model: that is, plot the residuals against the fitted values. Does this echo your conclusions of the previous part? In what way? Explain briefly.\n\nSolution\nPlot the things called .fitted and .resid from the regression object, which is not a data frame but you can treat it as if it is for this:\n\nggplot(tortoises.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nUp to you whether you put a smooth trend on it or not:\n\nggplot(tortoises.1, aes(x = .fitted, y = .resid)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLooking at the plot, you see a curve, up and down. The most negative residuals go with small or large fitted values; when the fitted value is in the middle, the residual is usually positive. A curve on the residual plot indicates a curve in the actual relationship. We just found above that a curve does fit a lot better, so this is all consistent.\nAside: the grey “envelope” is wide, so there is a lot of scatter on the residual plot. The grey envelope almost contains zero all the way across, so the evidence for a curve (or any other kind of trend) is not all that strong, based on this plot. This is in great contrast to the regression with length-squared, where the length-squared term is definitely necessary.\nThat was all I wanted, but you can certainly look at other plots. Normal quantile plot of the residuals:\n\nggplot(tortoises.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis is not the best: the low values are a bit too low, so that the whole picture is (a little) skewed to the left.18\nAnother plot you can make is to assess fan-out: you plot the absolute value19 of the residuals against the fitted values. The idea is that if there is fan-out, the absolute value of the residuals will get bigger:\n\nggplot(tortoises.1, aes(x = .fitted, y = abs(.resid))) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI put the smooth curve on as a kind of warning: it looks as if the size of the residuals goes down and then up again as the fitted values increase. But the width of the grey “envelope” and the general scatter of the points suggests that there is really not much happening here at all. On a plot of residuals, the grey envelope is really more informative than the blue smooth trend. On this one, there is no evidence of any fan-out (or fan-in).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#roller-coasters-1",
    "href": "simple-regression.html#roller-coasters-1",
    "title": "16  Simple regression",
    "section": "16.23 Roller coasters",
    "text": "16.23 Roller coasters\nA poll on the Discovery Channel asked people to nominate the best roller-coasters in the United States. We will examine the 10 roller-coasters that received the most votes. Two features of a roller-coaster that are of interest are the distance it drops from start to finish, measured here in feet20 and the duration of the ride, measured in seconds. Is it true that roller-coasters with a bigger drop also tend to have a longer ride? The data are at link.21\n\nRead the data into R and verify that you have a sensible number of rows and columns.\n\nSolution\nA .csv, so the usual for that:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/coasters.csv\"\ncoasters &lt;- read_csv(my_url)\n\nRows: 10 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): coaster_name, state\ndbl (2): drop, duration\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncoasters\n\n\n\n  \n\n\n\nThe number of marks for this kind of thing has been decreasing through the course, since by now you ought to have figured out how to do it without looking it up.\nThere are 10 rows for the promised 10 roller-coasters, and there are several columns: the drop for each roller-coaster and the duration of its ride, as promised, as well as the name of each roller-coaster and the state that it is in. (A lot of them seem to be in Ohio, for some reason that I don’t know.) So this all looks good.\n\\(\\blacksquare\\)\n\nMake a scatterplot of duration (response) against drop (explanatory), labelling each roller-coaster with its name in such a way that the labels do not overlap. Add a regression line to your plot.\n\nSolution\nThe last part, about the labels not overlapping, is an invitation to use ggrepel, which is the way I’d recommend doing this. (If not, you have to do potentially lots of work organizing where the labels sit relative to the points, which is time you probably don’t want to spend.) Thus:\n\nlibrary(ggrepel)\nggplot(coasters, aes(x = drop, y = duration, label = coaster_name)) +\n  geom_point() + geom_text_repel() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: label\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nThe se=FALSE at the end is optional; if you omit it, you get that “envelope” around the line, which is fine here.\nNote that with the labelling done this way, you can easily identify which roller-coaster is which.\nThe warning seems to be ggplot being over-zealous; the geom_point and the geom_smooth don’t need a label, but geom_text_repel certainly does. If it bothers you, move the label into the geom_text_repel:\n\nggplot(coasters, aes(x = drop, y = duration)) +\n  geom_point() + geom_text_repel(aes(label = coaster_name)) + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\nWould you say that roller-coasters with a larger drop tend to have a longer ride? Explain briefly.\n\nSolution\nI think there are two good answers here: “yes” and “kind of”. Supporting “yes” is the fact that the regression line does go uphill, so that overall, or on average, roller-coasters with a larger drop do tend to have a longer duration of ride as well. Supporting “kind of” is the fact that, though the regression line goes uphill, there are a lot of roller-coasters that are some way off the trend, far from the regression line. I am happy to go with either of those. I could also go with “not really” and the same discussion that I attached to “kind of”.\n\\(\\blacksquare\\)\n\nFind a roller-coaster that is unusual compared to the others. What about its combination of drop and duration is unusual?\n\nSolution\nThis is an invitation to find a point that is a long way off the line. I think the obvious choice is my first one below, but I would take either of the others as well:\n\n“Nitro” is a long way above the line. That means it has a long duration, relative to its drop. There are two other roller-coasters that have a larger drop but not as long a duration. In other words, this roller-coaster drops slowly, presumably by doing a lot of twisting, loop-the-loop and so on.\n“The Beast” is a long way below the line, so it has a short duration relative to its drop. It is actually the shortest ride of all, but is only a bit below average in terms of drop. This suggests that The Beast is one of those rides that drops a long way quickly.\n“Millennium Force” has the biggest drop of all, but a shorter-than-average duration. This looks like another ride with a big drop in it.\n\nA roller-coaster that is “unusual” will have a residual that is large in size (either positive, like Nitro, or negative, like the other two). I didn’t ask you to find the residuals, but if you want to, augment from broom is the smoothest way to go:\n\nlibrary(broom)\nduration.1 &lt;- lm(duration ~ drop, data = coasters)\naugment(duration.1, coasters) %&gt;%\n  select(coaster_name, duration, drop, .resid) %&gt;%\n  arrange(desc(abs(.resid)))\n\n\n\n  \n\n\n\naugment produces a data frame (of the original data frame with some new columns that come from the regression), so I can feed it into a pipe to do things with it, like only displaying the columns I want, and arranging them in order by absolute value of residual, so that the roller-coasters further from the line come out first. This identifies the three that we found above. The fourth one, “Ghost Rider”, is like Nitro in that it takes a (relatively) long time to fall not very far. You can also put augment in the middle of a pipe. What you may have to do then is supply the original data frame name to augment so that you have everything:\n\ncoasters %&gt;%\n  lm(duration ~ drop, data = .) %&gt;%\n  augment(coasters) %&gt;%\n  arrange(desc(abs(.resid)))\n\n\n\n  \n\n\n\nI wanted to hang on to the roller-coaster names, so I added the data frame name to augment. If you don’t (that is, you just put augment() in the middle of a pipe), then augment “attempts to reconstruct the data from the model”.22 That means you wouldn’t get everything from the original data frame; you would just get the things that were in the regression. In this case, that means you would lose the coaster names.\nA technicality (but one that you should probably care about): augment takes up to two inputs: a fitted model object like my duration.1, and an optional data frame to include other things from, like the coaster names. I had only one input to it in the pipe because the implied first input was the output from the lm, which doesn’t have a name; the input coasters in the pipe was what would normally be the second input to augment.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#running-and-blood-sugar-1",
    "href": "simple-regression.html#running-and-blood-sugar-1",
    "title": "16  Simple regression",
    "section": "16.24 Running and blood sugar",
    "text": "16.24 Running and blood sugar\nA diabetic wants to know how aerobic exercise affects his blood sugar. When his blood sugar reaches 170 (mg/dl), he goes out for a run at a pace of 10 minutes per mile. He runs different distances on different days. Each time he runs, he measures his blood sugar after the run. (The preferred blood sugar level is between 80 and 120 on this scale.) The data are in the file link. Our aim is to predict blood sugar from distance.\n\nRead in the data and display the data frame that you read in.\n\nSolution\nFrom the URL is easiest. These are delimited by one space, as you can tell by looking at the file:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/runner.txt\"\nruns &lt;- read_delim(my_url, \" \")\n\nRows: 12 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): distance, blood_sugar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nruns\n\n\n\n  \n\n\n\nThat looks like my data file.\n\\(\\blacksquare\\)\n\nMake a scatterplot and add a smooth trend to it.\n\nSolution\n\nggplot(runs, aes(x = distance, y = blood_sugar)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nblood_sugar should be on the vertical axis, since this is what we are trying to predict. Getting the x and the y right is easy on these, because they are the \\(x\\) and \\(y\\) for your plot.\n\\(\\blacksquare\\)\n\nWould you say that the relationship between blood sugar and running distance is approximately linear, or not? It is therefore reasonable to use a regression of blood sugar on distance? Explain briefly.\n\nSolution\nI’d say that this is about as linear as you could ever wish for. Neither the pattern of points nor the smooth trend have any kind of noticeable bend in them. (Observing a lack of curvature in either the points or the smooth trend is enough.) The trend is a linear one, so using a regression will be just fine. (If it weren’t, the rest of the question would be kind of dumb.)\n\\(\\blacksquare\\)\n\nFit a suitable regression, and obtain the regression output.\n\nSolution\nTwo steps: lm and then summary:\n\nruns.1 &lt;- lm(blood_sugar ~ distance, data = runs)\nsummary(runs.1)\n\n\nCall:\nlm(formula = blood_sugar ~ distance, data = runs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8238 -3.6167  0.8333  4.0190  5.5476 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  191.624      5.439   35.23 8.05e-12 ***\ndistance     -25.371      1.618  -15.68 2.29e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.788 on 10 degrees of freedom\nMultiple R-squared:  0.9609,    Adjusted R-squared:  0.957 \nF-statistic: 245.7 on 1 and 10 DF,  p-value: 2.287e-08\n\n\n\\(\\blacksquare\\)\n\nHow would you interpret the slope? That is, what is the slope, and what does that mean about blood sugar and running distance?\n\nSolution\nThe slope is \\(-25.37\\). This means that for each additional mile run, the runner’s blood sugar will decrease on average by about 25 units.\nYou can check this from the scatterplot. For example, from 2 to 3 miles, average blood sugar decreases from about 140 to about 115, a drop of 25.\n\\(\\blacksquare\\)\n\nIs there a (statistically) significant relationship between running distance and blood sugar? How do you know? Do you find this surprising, given what you have seen so far? Explain briefly.\n\nSolution\nLook at the P-value either on the distance line (for its \\(t\\)-test) or for the \\(F\\)-statistic on the bottom line. These are the same: 0.000000023. (They will be the same any time there is one \\(x\\)-variable.) This P-value is way smaller than 0.05, so there is a significant relationship between running distance and blood sugar. This does not surprise me in the slightest, because the trend on the scatterplot is so clear, there’s no way it could have happened by chance if in fact there were no relationship between running distance and blood sugar.\n\\(\\blacksquare\\)\n\nThis diabetic is planning to go for a 3-mile run tomorrow and a 5-mile run the day after. Obtain suitable 95% intervals that say what his blood sugar might be after each of these runs.\n\nSolution\nThis is a prediction interval, in each case, since we are talking about individual runs of 3 miles and 5 miles (not the mean blood sugar after all runs of 3 miles, which is what a confidence interval for the mean response would be). The procedure is to set up a data frame with the two distance values in it, and then feed that and the regression object into predict, coming up in a moment.\n\ndists &lt;- c(3, 5)\nnew &lt;- tibble(distance = dists)\nnew\n\n\n\n  \n\n\n\nThe important thing is that the name of the column of the new data frame must be exactly the same as the name of the explanatory variable in the regression. If they don’t match, predict won’t work. At least, it won’t work properly.23\nIf your first thought is datagrid, well, that will also work:\n\nnew2 &lt;- datagrid(model = runs.1, distance = c(5, 10))\nnew2\n\n\n\n  \n\n\n\nUse whichever of these methods comes to your mind.\nThen, predict, because you want prediction intervals rather than confidence intervals for the mean response (which is what marginaleffects gives you):\n\npp &lt;- predict(runs.1, new, interval = \"p\")\npp\n\n        fit       lwr       upr\n1 115.50952 104.37000 126.64905\n2  64.76667  51.99545  77.53788\n\n\nand display this with the distances by the side:\n\ncbind(new, pp)\n\n\n\n  \n\n\n\nor\n\ndata.frame(new, pp)\n\n\n\n  \n\n\n\nBlood sugar after a 3-mile run is predicted to be between 104 and 127; after a 5-mile run it is predicted to be between 52 and 77.5.\nExtra: both cbind and data.frame are “base R” ways of combining a data frame with something else to make a new data frame. They are not from the tidyverse. The tidyverse way is via tibble or bind_cols, but they are a bit more particular about what they will take: tibble takes vectors (single variables) and bind_cols takes vectors or data frames. The problem here is that pp is not either of those:\n\nclass(pp)\n\n[1] \"matrix\" \"array\" \n\n\nso that we have to use as_tibble first to turn it into a data frame, and thus:\n\npp %&gt;% as_tibble() %&gt;% bind_cols(new)\n\n\n\n  \n\n\n\nwhich puts things backwards, unless you do it like this:\n\nnew %&gt;% bind_cols(as_tibble(pp))\n\n\n\n  \n\n\n\nwhich is a pretty result from very ugly code.\nI also remembered that if you finish with a select, you get the columns in the order they were in the select:\n\npp %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(new) %&gt;%\n  select(c(distance, everything()))\n\n\n\n  \n\n\n\neverything is a so-called “select helper”. It means “everything except any columns you already named”, so this whole thing has the effect of listing the columns with distance first and all the other columns afterwards, in the order that they were in before.\n\\(\\blacksquare\\)\n\nWhich of your two intervals is longer? Does this make sense? Explain briefly.\n\nSolution\nThe intervals are about 22.25 and 25.5 units long. The one for a 5-mile run is a bit longer. I think this makes sense because 3 miles is close to the average run distance, so there is a lot of “nearby” data. 5 miles is actually longer than any of the runs that were actually done (and therefore we are actually extrapolating), but the important point for the prediction interval is that there is less nearby data: those 2-mile runs don’t help so much in predicting blood sugar after a 5-mile run. (They help some, because the trend is so linear. This is why the 5-mile interval is not so much longer. If the trend were less clear, the 5-mile interval would be more noticeably worse.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#calories-and-fat-in-pizza-1",
    "href": "simple-regression.html#calories-and-fat-in-pizza-1",
    "title": "16  Simple regression",
    "section": "16.25 Calories and fat in pizza",
    "text": "16.25 Calories and fat in pizza\nThe file at link came from a spreadsheet of information about 24 brands of pizza: specifically, per 5-ounce serving, the number of calories, the grams of fat, and the cost (in US dollars). The names of the pizza brands are quite long. This file may open in a spreadsheet when you browse to the link, depending on your computer’s setup.\n\nRead in the data and display at least some of the data frame. Are the variables of the right types? (In particular, why is the number of calories labelled one way and the cost labelled a different way?)\n\nSolution\nread_csv is the thing this time:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/Pizza.csv\"\npizza &lt;- read_csv(my_url)\n\nRows: 24 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Type\ndbl (3): Calories, Fat, Cost\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npizza\n\n\n\n  \n\n\n\nThe four variables are: the brand of pizza, which got read in as text, the number of calories (an integer), and the fat and cost, which are both decimal numbers so they get labelled dbl, which is short for “double-precision floating point number”.\nAnyway, these are apparently the right thing.\nExtra: I wanted to mention something else that I discovered yesterday.24 There is a package called rio that will read (and write) data in a whole bunch of different formats in a unified way.25 Anyway, the usual installation thing, done once:\n\ninstall.packages(\"rio\")\n\nwhich takes a moment since it probably has to install some other packages too, and then you read in a file like this:\n\nlibrary(rio)\npizza3 &lt;- import(my_url)\nhead(pizza3)\n\n\n\n  \n\n\n\nimport figures that you have a .csv file, so it calls up read_csv or similar.\nTechnical note: rio does not use the read_ functions, so what it gives you is actually a data.frame rather than a tibble, so that when you display it, you get the whole thing even if it is long. Hence the head here and below to display the first six lines.\nI originally had the data as an Excel spreadsheet, but import will gobble up that pizza too:\n\nmy_other_url &lt;- \"http://ritsokiguess.site/datafiles/Pizza_E29.xls\"\npizza4 &lt;- import(my_other_url)\nhead(pizza4)\n\n\n\n  \n\n\n\nThe corresponding function for writing a data frame to a file in the right format is, predictably enough, called export.\n\\(\\blacksquare\\)\n\nMake a scatterplot for predicting calories from the number of grams of fat. Add a smooth trend. What kind of relationship do you see, if any?\n\nSolution\nAll the variable names start with Capital Letters:\n\nggplot(pizza, aes(x = Fat, y = Calories)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThere is definitely an upward trend: the more fat, the more calories. The trend is more or less linear (or, a little bit curved: say what you like, as long as it’s not obviously crazy). I think, with this much scatter, there’s no real justification for fitting a curve.\n\\(\\blacksquare\\)\n\nFit a straight-line relationship, and display the intercept, slope, R-squared, etc. Is there a real relationship between the two variables, or is any apparent trend just chance?\n\nSolution\nlm, with summary:\n\npizza.1 &lt;- lm(Calories ~ Fat, data = pizza)\nsummary(pizza.1)\n\n\nCall:\nlm(formula = Calories ~ Fat, data = pizza)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-55.44 -11.67   6.18  17.87  41.61 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  194.747     21.605   9.014 7.71e-09 ***\nFat           10.050      1.558   6.449 1.73e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.79 on 22 degrees of freedom\nMultiple R-squared:  0.654, Adjusted R-squared:  0.6383 \nF-statistic: 41.59 on 1 and 22 DF,  p-value: 1.731e-06\n\n\nTo assess whether this trend is real or just chance, look at the P-value on the end of the Fat line, or on the bottom line where the \\(F\\)-statistic is (they are the same value of \\(1.73\\times 10^{-6}\\) or 0.0000017, so you can pick either). This P-value is really small, so the slope is definitely not zero, and therefore there really is a relationship between the two variables.\n\\(\\blacksquare\\)\n\nObtain a plot of the residuals against the fitted values for this regression. Does this indicate that there are any problems with this regression, or not? Explain briefly.\n\nSolution\nUse the regression object pizza.1:\n\nggplot(pizza.1, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nOn my residual plot, I see a slight curve in the smooth trend, but I am not worried about that because the residuals on the plot are all over the place in a seemingly random pattern (the grey envelope is wide and that is pretty close to going straight across). So I think a straight line model is satisfactory.\nThat’s all you needed, but it is also worth looking at a normal quantile plot of the residuals:\n\nggplot(pizza.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nA bit skewed to the left (the low ones are too low).\nAlso a plot of the absolute residuals, for assessing fan-out:\n\nggplot(pizza.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nA tiny bit of fan-in (residuals getting smaller in size as the fitted value gets bigger), but nothing much, I think.\nAnother way of assessing curvedness is to fit a squared term anyway, and see whether it is significant:\n\npizza.2 &lt;- update(pizza.1, . ~ . + I(Fat^2))\nsummary(pizza.2)\n\n\nCall:\nlm(formula = Calories ~ Fat + I(Fat^2), data = pizza)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.103 -14.280   5.513  15.423  35.474 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  90.2544    77.8156   1.160   0.2591  \nFat          25.9717    11.5121   2.256   0.0349 *\nI(Fat^2)     -0.5702     0.4086  -1.395   0.1775  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.25 on 21 degrees of freedom\nMultiple R-squared:  0.6834,    Adjusted R-squared:  0.6532 \nF-statistic: 22.66 on 2 and 21 DF,  p-value: 5.698e-06\n\n\nThe fat-squared term is not significant, so that curve on the smooth trend in the (first) residual plot was indeed nothing to get excited about.\n\\(\\blacksquare\\)\n\nThe research assistant in this study returns with two new brands of pizza (ones that were not in the original data). The fat content of a 5-ounce serving was 12 grams for the first brand and 20 grams for the second brand. For each of these brands of pizza, obtain a suitable 95% interval for the number of calories contained in a 5-ounce serving.\n\nSolution\nThe suitable interval here is a prediction interval, because we are interested in each case in the calorie content of the particular pizza brands that the research assistant returned with (and not, for example, in the mean calorie content for all brands of pizza that have 12 grams of fat per serving). Thus:\n\nnewfat &lt;- c(12, 20)\nnew &lt;- tibble(Fat = newfat)\nnew\n\n\n\n  \n\n\npreds &lt;- predict(pizza.1, new, interval = \"p\")\ncbind(new, preds)\n\n\n\n  \n\n\n\nUse datagrid to make new if you like, but it is a very simple dataframe, so there is no obligation to do it that way.\nOr, if you like:\n\nas_tibble(preds) %&gt;% bind_cols(new) %&gt;% select(Fat, everything())\n\n\n\n  \n\n\n\nFor the pizza with 12 grams of fat, the predicted calories are between 261 and 370 with 95% confidence, and for the pizza with 20 grams of fat, the calories are predicted to be between 337 and 454. (You should write down what these intervals are, and not leave the reader to find them in the output.)\n(Remember the steps: create a new data frame containing the values to predict for, and then feed that into predict along with the model that you want to use to predict with. The variable in the data frame has to be called precisely Fat with a capital F, otherwise it won’t work.)\nThese intervals are both pretty awful: you get a very weak picture of how many calories per serving the pizza brands in question might contain. This is for two reasons: (i) there was a fair bit of scatter in the original relationship, R-squared being around 65%, and (ii) even if we knew perfectly where the line went (which we don’t), there’s no guarantee that individual brands of pizza would be on it anyway. (Prediction intervals are always hit by this double whammy, in that individual observations suffer from variability in where the line goes and variability around whatever the line is.)\nI was expecting, when I put together this question, that the 20-grams-of-fat interval would be noticeably worse, because 20 is farther away from the mean fat content of all the brands. But there isn’t much to choose. For the confidence intervals for the mean calories of all brands with these fat contents, the picture is clearer:\n\nplot_cap(pizza.1, condition = \"Fat\")\n\n\n\n\nA fat value of 12 is close to the middle of the data, so the interval is shorter, but a value of 20 is out near the extreme and the interval is noticeably longer.\nThis part was a fair bit of work for 3 points, so I’m not insisting that you explain your choice of a prediction interval over a confidence interval, but I think it is still a smart thing to do, even purely from a marks point of view, because if you get it wrong for a semi-plausible reason, you might pick up some partial credit. Not pulling out your prediction intervals from your output is a sure way to lose a point, however.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#where-should-the-fire-stations-be-1",
    "href": "simple-regression.html#where-should-the-fire-stations-be-1",
    "title": "16  Simple regression",
    "section": "16.26 Where should the fire stations be?",
    "text": "16.26 Where should the fire stations be?\nIn city planning, one major issue is where to locate fire stations. If a city has too many fire stations, it will spend too much on running them, but if it has too few, there may be unnecessary fire damage because the fire trucks take too long to get to the fire.\nThe first part of a study of this kind of issue is to understand the relationship between the distance from the fire station (measured in miles in our data set) and the amount of fire damage caused (measured in thousands of dollars). A city recorded the fire damage and distance from fire station for 15 residential fires (which you can take as a sample of “all possible residential fires in that city”). The data are in link.\n\nRead in and display the data, verifying that you have the right number of rows and the right columns.\n\nSolution\nA quick check of the data reveals that the data values are separated by exactly one space, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/fire_damage.txt\"\nfire &lt;- read_delim(my_url, \" \")\n\nRows: 15 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): distance, damage\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfire\n\n\n\n  \n\n\n\n15 observations (rows), and promised, and a column each of distances and amounts of fire damage, also as promised.\n\\(\\blacksquare\\)\n\n* Obtain a 95% confidence interval for the mean fire damage. (There is nothing here from STAD29, and your answer should have nothing to do with distance.)\n\nSolution\nI wanted to dissuade you from thinking too hard here. It’s just an ordinary one-sample \\(t\\)-test, extracting the interval from it:\n\nt.test(fire$damage)\n\n\n    One Sample t-test\n\ndata:  fire$damage\nt = 12.678, df = 14, p-value = 4.605e-09\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.94488 30.88178\nsample estimates:\nmean of x \n 26.41333 \n\n\nOr\n\nwith(fire, t.test(damage))\n\n\n    One Sample t-test\n\ndata:  damage\nt = 12.678, df = 14, p-value = 4.605e-09\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.94488 30.88178\nsample estimates:\nmean of x \n 26.41333 \n\n\nIgnore the P-value (it’s testing that the mean is the default zero, which makes no sense). The confidence interval either way goes from 21.9 to 30.9 (thousand dollars).\n\\(\\blacksquare\\)\n\nDraw a scatterplot for predicting the amount of fire damage from the distance from the fire station. Add a smooth trend to your plot.\n\nSolution\nWe are predicting fire damage, so that goes on the \\(y\\)-axis:\n\nggplot(fire, aes(x = distance, y = damage)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\n* Is there a relationship between distance from fire station and fire damage? Is it linear or definitely curved? How strong is it? Explain briefly.\n\nSolution\nWhen the distance is larger, the fire damage is definitely larger, so there is clearly a relationship. I would call this one approximately linear: it wiggles a bit, but it is not to my mind obviously curved. I would also call it a strong relationship, since the points are close to the smooth trend.\n\\(\\blacksquare\\)\n\nFit a regression predicting fire damage from distance. How is the R-squared consistent (or inconsistent) with your answer from part~(here)?\n\nSolution\nThe regression is an ordinary lm:\n\ndamage.1 &lt;- lm(damage ~ distance, data = fire)\nsummary(damage.1)\n\n\nCall:\nlm(formula = damage ~ distance, data = fire)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4682 -1.4705 -0.1311  1.7915  3.3915 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  10.2779     1.4203   7.237 6.59e-06 ***\ndistance      4.9193     0.3927  12.525 1.25e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.316 on 13 degrees of freedom\nMultiple R-squared:  0.9235,    Adjusted R-squared:  0.9176 \nF-statistic: 156.9 on 1 and 13 DF,  p-value: 1.248e-08\n\n\nWe need to display the results, since we need to see the R-squared in order to say something about it.\nR-squared is about 92%, high, indicating a strong and linear relationship. Back in part~(here), I said that the relationship is linear and strong, which is entirely consistent with such an R-squared. (If you said something different previously, say how it does or doesn’t square with this kind of R-squared value.)\nPoints: one for fitting the regression, one for displaying it, and two (at the grader’s discretion) for saying what the R-squared is and how it’s consistent (or not) with part~(here).\nExtra: if you thought the trend was “definitely curved”, you would find that a parabola (or some other kind of curve) was definitely better than a straight line. Here’s the parabola:\n\ndamage.2 &lt;- lm(damage ~ distance + I(distance^2), data = fire)\nsummary(damage.2)\n\n\nCall:\nlm(formula = damage ~ distance + I(distance^2), data = fire)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8856 -1.6915 -0.0179  1.5490  3.6278 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    13.3395     2.5303   5.272 0.000197 ***\ndistance        2.6400     1.6302   1.619 0.131327    \nI(distance^2)   0.3376     0.2349   1.437 0.176215    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.227 on 12 degrees of freedom\nMultiple R-squared:  0.9347,    Adjusted R-squared:  0.9238 \nF-statistic: 85.91 on 2 and 12 DF,  p-value: 7.742e-08\n\n\nThere’s no evidence here that a quadratic is better.\nOr you might even have thought from the wiggles that it was more like cubic:\n\ndamage.3 &lt;- update(damage.2, . ~ . + I(distance^3))\nsummary(damage.3)\n\n\nCall:\nlm(formula = damage ~ distance + I(distance^2) + I(distance^3), \n    data = fire)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2325 -1.8377  0.0322  1.1512  3.1806 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    10.8466     4.3618   2.487   0.0302 *\ndistance        5.9555     4.9610   1.200   0.2552  \nI(distance^2)  -0.8141     1.6409  -0.496   0.6296  \nI(distance^3)   0.1141     0.1608   0.709   0.4928  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.274 on 11 degrees of freedom\nMultiple R-squared:  0.9376,    Adjusted R-squared:  0.9205 \nF-statistic: 55.07 on 3 and 11 DF,  p-value: 6.507e-07\n\n\nNo evidence that a cubic is better; that increase in R-squared up to about 94% is just chance (bearing in mind that adding any \\(x\\), even a useless one, will increase R-squared).\nHow bendy is the cubic?\n\nggplot(fire, aes(x = distance, y = damage)) + geom_point() +\n  geom_smooth(method = \"lm\") +\n  geom_line(data = damage.3, aes(y = .fitted), colour = \"red\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe cubic, in red, does bend a little, but it doesn’t do an obvious job of going through the points better than the straight line does. It seems to be mostly swayed by that one observation with damage over 40, and choosing a relationship by how well it fits one point is flimsy at the best of times. So, by Occam’s Razor, we go with the line rather than the cubic because it (i) fits equally well, (ii) is simpler.\n\\(\\blacksquare\\)\n\n Obtain a 95% confidence interval for the mean fire damage for a residence that is 4 miles from the nearest fire station*. (Note the contrast with part~(here).)\n\nSolution\nThis is a confidence interval for a mean response at a given value of the explanatory variable. This is as opposed to part~(here), which is averaged over all distances. So, follow the steps. Make a tiny data frame with this one value of distance:\n\nnew &lt;- datagrid(model = damage.1, distance = 4)\nnew\n\n\n\n  \n\n\n\nand then\n\npp &lt;- cbind(predictions(damage.1, newdata = new))\npp\n\n\n\n  \n\n\n\n28.5 to 31.4 (thousand dollars).\n(I saved this one because I want to refer to it again later.)\n\\(\\blacksquare\\)\n\nCompare the confidence intervals of parts (here) and (here). Specifically, compare their centres and their lengths, and explain briefly why the results make sense.\n\nSolution\nLet me just put them side by side for ease of comparison: part~(here) is:\n\nt.test(fire$damage)\n\n\n    One Sample t-test\n\ndata:  fire$damage\nt = 12.678, df = 14, p-value = 4.605e-09\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.94488 30.88178\nsample estimates:\nmean of x \n 26.41333 \n\n\nand part~(here)’s is\n\npp\n\n\n\n  \n\n\n\nThe centre of the interval is higher for the mean damage when the distance is 4. This is because the mean distance is a bit less than 4:\n\nfire %&gt;% summarize(m = mean(distance))\n\n\n\n  \n\n\n\nWe know it’s an upward trend, so our best guess at the mean damage is higher if the mean distance is higher (in (here), the distance is always 4: we’re looking at the mean fire damage for all residences that are 4 miles from a fire station.)\nWhat about the lengths of the intervals? The one in (here) is about \\(30.9-21.9=9\\) (thousand dollars) long, but the one in (here) is only \\(31.4-28.5=2.9\\) long, much shorter. This makes sense because the relationship is a strong one: knowing the distance from the fire station is very useful, because the bigger it is, the bigger the damage going to be, with near certainty. Said differently, if you know the distance, you can estimate the damage accurately. If you don’t know the distance (as is the case in (here)), you’re averaging over a lot of different distances and thus there is a lot of uncertainty in the amount of fire damage also.\nIf you have some reasonable discussion of the reason why the centres and lengths of the intervals differ, I’m happy. It doesn’t have to be the same as mine.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#making-it-stop-1",
    "href": "simple-regression.html#making-it-stop-1",
    "title": "16  Simple regression",
    "section": "16.27 Making it stop",
    "text": "16.27 Making it stop\nIf you are driving, and you hit the brakes, how far do you travel before coming to a complete stop? Presumably this depends on how fast you are going. Knowing this relationship is important in setting speed limits on roads. For example, on a very bendy road, the speed limit needs to be low, because you cannot see very far ahead, and there could be something just out of sight that you need to stop for.\nData were collected for a typical car and driver, as shown in http://ritsokiguess.site/datafiles/stopping.csv. These are American data, so the speeds are miles per hour and the stopping distances are in feet.\n\nRead in and display (probably all of) the data.\n\nSolution\nThe usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/stopping.csv\"\nstopping &lt;- read_csv(my_url)\n\nRows: 8 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): speed, distance\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstopping\n\n\n\n  \n\n\n\nThere are only eight observations.\n\\(\\blacksquare\\)\n\nMake a suitable plot of the data.\n\nSolution\nTwo quantitative variables means a scatterplot. Stopping distance is the outcome, so that goes on the \\(y\\)-axis:\n\nggplot(stopping, aes(x=speed, y=distance)) + geom_point()\n\n\n\n\n\\(\\blacksquare\\)\n\nDescribe any trend you see in your graph.\n\nSolution\nIt’s an upward trend, but not linear: the stopping distance seems to increase faster at higher speeds.\n\\(\\blacksquare\\)\n\nFit a linear regression predicting stopping distance from speed. (You might have some misgivings about doing this, but do it anyway.)\n\nSolution\nHaving observed a curved relationship, it seems odd to fit a straight line. But we are going to do it anyway and then critique what we have:\n\nstopping.1 &lt;- lm(distance~speed, data=stopping)\nsummary(stopping.1)\n\n\nCall:\nlm(formula = distance ~ speed, data = stopping)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.738 -22.351  -7.738  16.622  47.083 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -44.1667    22.0821   -2.00   0.0924 .  \nspeed         5.6726     0.5279   10.75 3.84e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 34.21 on 6 degrees of freedom\nMultiple R-squared:  0.9506,    Adjusted R-squared:  0.9424 \nF-statistic: 115.5 on 1 and 6 DF,  p-value: 3.837e-05\n\n\nExtra: note that R-squared is actually really high. We come back to that later.\n\\(\\blacksquare\\)\n\nPlot the residuals against the fitted values for this regression.\n\nSolution\n\nggplot(stopping.1, aes(x=.fitted, y=.resid)) + geom_point()\n\n\n\n\n\\(\\blacksquare\\)\n\nWhat do you learn from the residual plot? Does that surprise you? Explain briefly.\n\nSolution\nThe points on the residual plot form a (perfect) curve, so the original relationship was a curve. This is exactly what we saw on the scatterplot, so to me at least, this is no surprise.\n(Make sure you say how you know that the original relationship was a curve from looking at the residual plot. Joined-up thinking. There are two ways we know that the relationship is a curve. Get them both.)\n\\(\\blacksquare\\)\n\nWhat is the actual relationship between stopping distance and speed, according to the physics? See if you can find out. Cite any books or websites that you use: that is, include a link to a website, or give enough information about a book that the grader could find it.\n\nSolution\nI searched for “braking distance and speed” and found the first two things below, that seemed to be useful. Later, I was thinking about the fourth point (which came out of my head) and while searching for other things about that, I found the third thing:\n\na British road safety website, that says “The braking distance depends on how fast the vehicle was travelling before the brakes were applied, and is proportional to the square of the initial speed.”\nthe Wikipedia article on braking distance, which gives the actual formula. This is the velocity squared, divided by a constant that depends on the coefficient of friction. (That is why your driving instructor tells you to leave a bigger gap behind the vehicle in front if it is raining, and an even bigger gap if it is icy.)\nan Australian math booklet that talks specifically about braking distance and derives the formula (and the other equations of motion).\nalso, if you have done physics, you might remember the equation of motion \\(v^2 = u^2 + 2as\\), where \\(u\\) is the initial velocity, \\(v\\) is the final velocity, \\(a\\) is the acceleration and \\(s\\) is the distance covered. In this case, \\(v=0\\) (the car is stationary at the end), and so \\(-u^2/2a = s\\). The acceleration is negative (the car is slowing down), so the left side is, despite appearances, positive. There seems to be a standard assumption that deceleration due to braking is constant (the same for all speeds), at least if you are trying to stop a car in a hurry.\n\nThese are all saying that we should add a speed-squared term to our regression, and then we will have the relationship exactly right, according to the physics.\nExtra: Another way to measure how far you are behind the vehicle in front is time. Many of the British “motorways” (think 400-series highways) were built when I was young, and I remember a TV commercial that said “Only a Fool Breaks the Two Second Rule”.26 In those days (the linked one is from the 1970s),27 a lot of British drivers were not used to going that fast, or on roads that straight, so this was a way to know how big a gap to leave, so that you had time to take evasive action if needed. The value of the two-second rule is that it works for any speed, and you don’t have to remember a bunch of stopping distances. (When I did my (Canadian) driving theory test, I think I worked out and learned a formula for the stopping distances that I could calculate in my head. I didn’t have to get very close since the test was multiple-choice.)\n\\(\\blacksquare\\)\n\nFit the relationship that your research indicated (in the previous part) and display the results. Comment briefly on the R-squared value.\n\nSolution\nAdd a squared term in speed:\n\nstopping.2 &lt;- lm(distance~speed+I(speed^2), data=stopping)\nsummary(stopping.2)\n\n\nCall:\nlm(formula = distance ~ speed + I(speed^2), data = stopping)\n\nResiduals:\n       1        2        3        4        5        6        7        8 \n-1.04167  0.98214  0.08929  1.27976 -0.44643 -0.08929 -2.64881  1.87500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.041667   1.429997   0.728    0.499    \nspeed       1.151786   0.095433  12.069 6.89e-05 ***\nI(speed^2)  0.064583   0.001311  49.267 6.51e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.699 on 5 degrees of freedom\nMultiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 \nF-statistic: 2.462e+04 on 2 and 5 DF,  p-value: 1.039e-10\n\n\nThe R-squared now is basically 1, so that the model fits very close to perfectly.\nExtra: you probably found in your research that the distance should be just something times speed squared, with no constant or linear term. Here, though, we have a significant linear term as well. That is probably just chance, since the distances in the data look as if they have been rounded off. With more accurate values, I think the linear term would have been closer to zero.\nIf you want to go literally for the something-times-speed-squared, you can do that. This doesn’t quite work:\n\nstopping.3x &lt;- lm(distance~I(speed^2), data=stopping)\nsummary(stopping.3x)\n\n\nCall:\nlm(formula = distance ~ I(speed^2), data = stopping)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7327  -3.4670   0.6761   6.2323   8.4513 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.732704   4.362859   3.377   0.0149 *  \nI(speed^2)   0.079796   0.001805  44.218 8.96e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.514 on 6 degrees of freedom\nMultiple R-squared:  0.9969,    Adjusted R-squared:  0.9964 \nF-statistic:  1955 on 1 and 6 DF,  p-value: 8.958e-09\n\n\nbecause it still has an intercept in it. In R, the intercept is denoted by 1. It is always included, unless you explicitly remove it. Some odd things start to happen if you remove the intercept, so it is not a good thing to do unless you know what you are doing. The answers here have some good discussion. Having decided that you are going to remove the intercept, you can remove it the same way as anything else (see update in the multiple regression lecture) with “minus”. I haven’t shown you this, so if you do it, you will need to cite your source: that is, say where you learned what to do:\n\nstopping.3 &lt;- lm(distance~I(speed^2)-1, data=stopping)\nsummary(stopping.3)\n\n\nCall:\nlm(formula = distance ~ I(speed^2) - 1, data = stopping)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.6123  -0.7859  10.5314  15.5314  19.2141 \n\nCoefficients:\n           Estimate Std. Error t value Pr(&gt;|t|)    \nI(speed^2) 0.084207   0.001963   42.89 9.77e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.42 on 7 degrees of freedom\nMultiple R-squared:  0.9962,    Adjusted R-squared:  0.9957 \nF-statistic:  1840 on 1 and 7 DF,  p-value: 9.772e-10\n\n\nThe R-squared is still extremely high, much higher than for the straight line. The coefficient value, as I said earlier (citing Wikipedia), depends on the coefficient of friction; the stopping distances you see typically are based on a dry road, so you have to allow extra distance (or time: see above) if the road is not dry.\n\\(\\blacksquare\\)\n\nSomebody says to you “if you have a regression with a high R-squared, like 95%, there is no need to look for a better model.” How do you respond to this? Explain briefly.\n\nSolution\nAn example of a regression with an R-squared of 95% is the straight-line fit from earlier in this question. This is an example of a regression that fits well but is not appropriate because it doesn’t capture the form of the relationship.\nIn general, we are saying that no matter how high R-squared is, we might still be able to improve on the model we have. The flip side is that we might not be able to do any better (with another data set) than an R-squared of, say, 30%, because there is a lot of variability that is, as best as we can assess it, random and not explainable by anything.\nUsing R-squared as a measure of absolute model quality is, thus, a mistake. Or, to say it perhaps more clearly, asking “how high does R-squared have to be to indicate a good fit?” is asking the wrong question. The right thing to do is to concentrate on getting the form of the model right, and thereby get the R-squared as high as we can for that data set (which might be very high, as here, or not high at all).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#predicting-height-from-foot-length-1",
    "href": "simple-regression.html#predicting-height-from-foot-length-1",
    "title": "16  Simple regression",
    "section": "16.28 Predicting height from foot length",
    "text": "16.28 Predicting height from foot length\nIs it possible to estimate the height of a person from the length of their foot? To find out, 33 (male) students had their height and foot length measured. The data are in http://ritsokiguess.site/datafiles/heightfoot.csv.\n\nRead in and display (some of) the data. (If you are having trouble, make sure you have exactly the right URL. The correct URL has no spaces or other strange characters in it.)\n\nSolution\nThe usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/heightfoot.csv\"\nhf &lt;- read_csv(my_url)\n\nRows: 33 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): height, foot\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhf\n\n\n\n  \n\n\n\nCall the data frame whatever you like, but keeping away from the names height and foot is probably wise, since those are the names of the columns.\nThere are indeed 33 rows as promised.\nExtra: my comment in the question was to help you if you copy-pasted the file URL into R Studio. Depending on your setup, this might have gotten pasted with a space in it, at the point where it is split over two lines. The best way to proceed, one that won’t run into this problem, is to right-click on the URL and select Copy Link Address (or the equivalent on your system), and then it will put the whole URL on the clipboard in one piece, even if it is split over two lines in the original document, so that pasting it will work without problems.\n\\(\\blacksquare\\)\n\nMake a suitable plot of the two variables in the data frame.\n\nSolution\nThey are both quantitative, so a scatter plot is called for:\n\nggplot(hf, aes(y=height, x=foot)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI added a smooth trend, or you could just plot the points. (This is better than plotting a regression line at this stage, because we haven’t yet thought about whether the trend is straight.)\nNow that we’ve seen the scatterplot, the trend looks more or less straight (but you should take a look at the scatterplot first, with or without smooth trend, before you put a regression line on it). That point top left is a concern, though, which brings us to…\n\\(\\blacksquare\\)\n\nAre there any observations not on the trend of the other points? What is unusual about those observations?\n\nSolution\nThe observation with height greater than 80 at the top of the graph looks like an outlier and does not follow the trend of the rest of the points. Or, this individual is much taller than you would expect for someone with a foot length of 27 inches. Or, this person is over 7 feet tall, which makes little sense as a height. Say something about what makes this person be off the trend.\n\\(\\blacksquare\\)\n\nFit a regression predicting height from foot length, including any observations that you identified in the previous part. For that regression, plot the residuals against the fitted values and make a normal quantile plot of the residuals.\n\nSolution\nThese things. Displaying the summary of the regression is optional, but gives the grader an opportunity to check that your work is all right so far.\n\nhf.1 &lt;- lm(height~foot, data=hf)\nsummary(hf.1)\n\n\nCall:\nlm(formula = height ~ foot, data = hf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7491 -1.3901 -0.0310  0.8918 12.9690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  34.3363     9.9541   3.449 0.001640 ** \nfoot          1.3591     0.3581   3.795 0.000643 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.102 on 31 degrees of freedom\nMultiple R-squared:  0.3173,    Adjusted R-squared:  0.2952 \nF-statistic: 14.41 on 1 and 31 DF,  p-value: 0.0006428\n\nggplot(hf.1, aes(x=.fitted, y=.resid)) + geom_point()\n\n\n\nggplot(hf.1, aes(sample=.resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nNote that we did not exclude the off-trend point. Removing points because they are outliers is a bad idea. This is a good discussion of the issues.\n\\(\\blacksquare\\)\n\nEarlier, you identified one or more observations that were off the trend. How does this point or points show up on each of the plots you drew in the previous part?\n\nSolution\nOn its own at the top in both cases; the large positive residual on the first plot, and the unusually large value at the top right of the normal quantile plot. (You need to say one thing about each graph, or say as I did that the same kind of thing happens on both graphs.)\nExtra: in the residuals vs. fitted values, the other residuals show a slight upward trend. This is because the regression line for these data, with the outlier, is pulled (slightly) closer to the outlier and thus slightly further away from the other points, particularly the ones on the left, compared to the same data but with the outlier removed (which you will be seeing shortly). If the unusual point had happened to have an extreme \\(x\\) (foot length) as well, the effect would have been more pronounced.\nThis is the kind of thing I mean (made-up data):\n\ntibble(x = 1:10) %&gt;% \nmutate(y = rnorm(10, 10+2*x, 1)) %&gt;% \nmutate(y = ifelse(x == 9, 40, y)) -&gt; madeup\nmadeup\n\n\n\n  \n\n\n\n\nggplot(madeup, aes(x=x, y=y)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe second-last point is off a clearly linear trend otherwise (the smooth gets “distracted” by the outlying off-trend point). Fitting a regression anyway and looking at the residual plot gives this:\n\nmadeup.1 &lt;- lm(y~x, data = madeup)\nggplot(madeup.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nThis time you see a rather more obvious downward trend in the other residuals. The problem is not with them, but with the one very positive residual, corresponding to the outlier that is way off the trend on the scatterplot.\nThe residuals in a regression have to add up to zero. If one of them is very positive (as in the one you did and the example I just showed you), at least some of the other residuals have to become more negative to compensate – the ones on the right just above and the ones on the left in the one you did. If you have done STAC67, you will have some kind of sense of why that is: think about the two equations you have to solve to get the estimates of intercept and slope, and how they are related to the residuals. Slide 6 of this shows them; at the least squares estimates, these two partial derivatives both have to be zero, and the things inside the brackets are the residuals.\n\\(\\blacksquare\\)\n\nAny data points that concerned you earlier were actually errors. Create and save a new data frame that does not contain any of those data points.\n\nSolution\nFind a way to not pick that outlying point. For example, you can choose the observations with height less than 80:\n\nhf %&gt;% filter(height&lt;80) -&gt; hfx\nhfx\n\n\n\n  \n\n\n\nOnly 32 rows left.\nThere are many other possibilities. Find one.\n\\(\\blacksquare\\)\n\nRun a regression predicting height from foot length for your data set without errors. Obtain a plot of the residuals against fitted values and a normal quantile plot of the residuals for this regression.\n\nSolution\nCode-wise, the same as before, but with the new data set:\n\nhf.2 &lt;- lm(height~foot, data=hfx)\nsummary(hf.2)\n\n\nCall:\nlm(formula = height ~ foot, data = hfx)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5097 -1.0158  0.4757  1.1141  3.9951 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  30.1502     6.5411   4.609 7.00e-05 ***\nfoot          1.4952     0.2351   6.360 5.12e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.029 on 30 degrees of freedom\nMultiple R-squared:  0.5741,    Adjusted R-squared:  0.5599 \nF-statistic: 40.45 on 1 and 30 DF,  p-value: 5.124e-07\n\nggplot(hf.2, aes(x=.fitted, y=.resid)) + geom_point()\n\n\n\nggplot(hf.2, aes(sample=.resid)) + stat_qq() + stat_qq_line()\n\n\n\n\n\\(\\blacksquare\\)\n\nDo you see any problems on the plots you drew in the previous part? Explain briefly.\n\nSolution\nFor myself, I see a random scatter of points on the first plot, and points close to the line on the second one. Thus I don’t see any problems at all. I would declare myself happy with the second regression, after removing the outlier. (Remember that we removed the outlier because it was an error, not just because it was an outlier. Outliers can be perfectly correct data points, and if they are, they have to be included in the modelling.)\nYou might have a different point of view, in which case you need to make the case for it. You might see a (very mild) case of fanning out in the first plot, or the two most negative residuals might be a bit too low. These are not really outliers, though, not at least in comparison to what we had before.\nExtra: a standard diagnostic for fanning-out is to plot the absolute values of the residuals against the fitted values, with a smooth trend. If this looks like an increasing trend, there is fanning-out; a decreasing trend shows fanning-in. The idea is that we want to see whether the residuals are changing in size (for example, getting more positive and more negative both):\n\nggplot(hf.2, aes(x=.fitted, y=abs(.resid))) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nNo evidence of fanning-out at all. In fact, the residuals seem to be smallest in size in the middle.\nAnother thing you might think of is to try Box-Cox:\n\nboxcox(height~foot, data=hfx)\n\n\n\n\nIt looks as if the best \\(\\lambda\\) is \\(-1\\), and we should predict one over height from foot length. But this plot is deceiving, since it doesn’t even show the whole confidence interval for \\(\\lambda\\)! We should zoom out (a lot) more:\n\nboxcox(height~foot, data=hfx, lambda = seq(-8, 8, 0.1))\n\n\n\n\nThis shows that the confidence interval for \\(\\lambda\\) goes from about \\(-7\\) to almost 5: that is, any value of \\(\\lambda\\) in that interval is supported by the data! This very definitely includes the do-nothing \\(\\lambda=1\\), so there is really no support for any kind of transformation.\n\\(\\blacksquare\\)\n\nFind a way to plot the data and both regression lines on the same plot, in such a way that you can see which regression line is which. If you get help from anything outside the course materials, cite your source(s).\n\nSolution\nThis is the same idea as with the windmill data, page 22, though this one is a bit easier since everything is linear (no curves).\nThe easiest way is to use geom_smooth twice, once with the original data set, and then on the one with the outlier removed:\n\nggplot(hf, aes(y=height, x=foot)) + geom_point() + geom_smooth(method = \"lm\", se=F) +\ngeom_smooth(data=hfx, method=\"lm\", colour=\"red\", se=F)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nI would use the original data set as the “base”, since we want to plot its points (including the outlier) as well as its line. Then we want to plot just the line for the second data set. This entails using a data= in the second geom_smooth, to say that we want to get this regression line from a different data set, and also entails drawing this line in a different colour (or in some way distinguishing it from the first one). Putting the colour outside an aes is a way to make the whole line red. (Compare how you make points different colours according to their value on a third variable.)\nThis is, I think, the best way to do it. You can mimic the idea that I used for the windmill data:\n\nggplot(hf, aes(y=height, x=foot)) + geom_point() + geom_smooth(method = \"lm\", se=F) +\ngeom_line(data=hf.2, aes(y = .fitted))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nbut this is not as good, because you don’t need to use the trickery with geom_line: the second trend is another regression line not a curve, and we know how to draw regression lines with geom_smooth without having to actually fit them. (Doing it this way reveals that you are copying without thinking, instead of wondering whether there is a better way to do it.)\nThe idea of using a different data set in different “layers” of a plot is quite well-known. For example, the idea is the one in here, though used for a different purpose there (plotting different sets of points instead of different lines).\n\\(\\blacksquare\\)\n\nDiscuss briefly how removing the observation(s) that were errors has changed where the regression line goes, and whether that is what you expected.\n\nSolution\nThe regression line for the original data (my blue line) is pulled up compared to the one with outliers removed (red).\nThis is not very surprising, because we know that regression lines tend to get pulled towards outliers. What was surprising to me was that the difference wasn’t very big. Even at the low end, where the lines differ the most, the difference in predicted height is only about one inch. Since regression lines are based on means, I would have expected a big outlier to have moved the line a lot more.\nSay something about what you expected, and say something insightful about whether that was what you saw.\nExtra: the regression lines are very similar, but their R-squared values are not: 32% and 57% respectively. Having a point far from the line has a big (downward) impact on R-squared.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "simple-regression.html#footnotes",
    "href": "simple-regression.html#footnotes",
    "title": "16  Simple regression",
    "section": "",
    "text": "This is a base graphics graph rather than a ggplot one, but it will do for our purposes.↩︎\nRoller-coasters work by gravity, so there must be some drop.↩︎\nThese are not to be confused with what your mom insists that you place between your coffee mug and the table.↩︎\nIf you had just one \\(x\\), you’d use a \\(t\\)-test for its slope, and if you were testing all the \\(x\\)’s, you’d use the global \\(F\\)-test that appears in the regression output.↩︎\nThis is a base graphics graph rather than a ggplot one, but it will do for our purposes.↩︎\nRecall that adding \\(x\\)-variables to a regression will always make R-squared go up, even if they are just random noise.↩︎\nThis is not, I don’t think, a real word, but I mean size emphasizing how big a boy is generally, rather than how small.↩︎\nCorrelations have to go up beyond 0.50 before they start looking at all interesting.↩︎\nThe suspicion being that we can, since the scatterplot suggested serious non-linearity.↩︎\nAgain, not a surprise, given our initial scatterplot.↩︎\nNow we can use that word significant.↩︎\nThis might be overkill at this point, since we really only care about whether our data values are reasonable, and often just looking at the highest and lowest values will tell us that.↩︎\nMathematically, \\(e^x\\) is approximately \\(1+x\\) for small \\(x\\), which winds up meaning that the slope in a model like this, if it is small, indicates about the percent increase in the response associated with a 1-unit change in the explanatory variable. Note that this only works with \\(e^x\\) and natural logs, not base 10 logs or anything like that.↩︎\nWhen this was graded, it was 3 marks, to clue you in that there are three things to say.↩︎\nI have always used dots, but in the spirit of the tidyverse I suppose I should use underscores.↩︎\nThe summary output is more designed for looking at than for extracting things from.↩︎\nThese days, there are apps that will let you do this with your phone. I found one called Clinometer. See also link.↩︎\nThe very negative residuals are at the left and right of the residual plot; they are there because the relationship is a curve. If you were to look at the residuals from the model with length-squared, you probably wouldn’t see this.↩︎\nThe value, but throw away the minus sign if it has one.↩︎\nRoller-coasters work by gravity, so there must be some drop.↩︎\nThese are not to be confused with what your mom insists that you place between your coffee mug and the table.↩︎\nA quote from the package vignette.↩︎\nIt won’t give you an error, but it will go back to the original data frame to get distances to predict from, and you will get very confused. This is another example of (base) R trying to make life easier for you, but when it fails, it fails.↩︎\nR is like that: sometimes it seems as if it has infinite depth.↩︎\nIt does this by figuring what kind of thing you have, from the extension to its filename, and then calling an appropriate function to read in or write out the data. This is an excellent example of “standing on the shoulders of giants” to make our lives easier. The software does the hard work of figuring out what kind of thing you have and how to read it in; all we do is say import.↩︎\nThis is perhaps not a commercial so much as a public safety message.↩︎\nThere are some typical British cars of the era in the commercial.↩︎"
  },
  {
    "objectID": "multiple-regression.html#being-satisfied-with-hospital",
    "href": "multiple-regression.html#being-satisfied-with-hospital",
    "title": "17  Multiple regression",
    "section": "17.1 Being satisfied with hospital",
    "text": "17.1 Being satisfied with hospital\nA hospital administrator collects data to study the effect, if any, of a patient’s age, the severity of their illness, and their anxiety level, on the patient’s satisfaction with their hospital experience. The data, in the file link, are for 46 patients in a survey. The columns are: patient’s satisfaction score satis, on a scale of 0 to 100; the patient’s age (in years), the severity of the patient’s illness (also on a 0–100 scale), and the patient’s anxiety score on a standard anxiety test (scale of 0–5). Higher scores mean greater satisfaction, increased severity of illness and more anxiety.\n\nRead in the data and check that you have four columns in your data frame, one for each of your variables.\n* Obtain scatterplots of the response variable satis against each of the other variables.\nIn your scatterplots of (here), which relationship appears to be the strongest one?\n* Create a correlation matrix for all four variables. Does your strongest trend of the previous part have the strongest correlation?\nRun a regression predicting satisfaction from the other three variables, and display the output.\nDoes the regression fit well overall? How can you tell?\nTest the null hypothesis that none of your explanatory variables help, against the alternative that one or more of them do. (You’ll need an appropriate P-value. Which one is it?) What do you conclude?\nThe correlation between severity and satis is not small, but in my regression I found that severity was nowhere near significant. Why is this? Explain briefly. \nCarry out a backward elimination to determine which of age, severity and anxiety are needed to predict satisfaction. What do you get? Use \\(\\alpha = 0.10\\)."
  },
  {
    "objectID": "multiple-regression.html#salaries-of-mathematicians",
    "href": "multiple-regression.html#salaries-of-mathematicians",
    "title": "17  Multiple regression",
    "section": "17.2 Salaries of mathematicians",
    "text": "17.2 Salaries of mathematicians\nA researcher in a scientific foundation wanted to evaluate the relationship between annual salaries of mathematicians and three explanatory variables:\n\nan index of work quality\nnumber of years of experience\nan index of publication success.\n\nThe data can be found at link. Data from only a relatively small number of mathematicians were available.\n\nRead in the data and check that you have a sensible number of rows and the right number of columns. (What does “a sensible number of rows” mean here?)\nMake scatterplots of salary against each of the three explanatory variables. If you can, do this with one ggplot.\nComment briefly on the direction and strength of each relationship with salary.\n* Fit a regression predicting salary from the other three variables, and obtain a summary of the results.\nHow can we justify the statement “one or more of the explanatory variables helps to predict salary”? How is this consistent with the value of R-squared?\nWould you consider removing any of the variables from this regression? Why, or why not?\nDo you think it would be a mistake to take both of workqual and pubsucc out of the regression? Do a suitable test. Was your guess right?\nBack in part (here), you fitted a regression with all three explanatory variables. By making suitable plots, assess whether there is any evidence that (i) that the linear model should be a curve, (ii) that the residuals are not normally distributed, (iii) that there is “fan-out”, where the residuals are getting bigger in size as the fitted values get bigger? Explain briefly how you came to your conclusions in each case."
  },
  {
    "objectID": "multiple-regression.html#predicting-gpa-of-computer-science-students",
    "href": "multiple-regression.html#predicting-gpa-of-computer-science-students",
    "title": "17  Multiple regression",
    "section": "17.3 Predicting GPA of computer science students",
    "text": "17.3 Predicting GPA of computer science students\nThe file link contains some measurements of academic achievement for a number of university students studying computer science:\n\nHigh school grade point average\nMath SAT score\nVerbal SAT score\nComputer Science grade point average\nOverall university grade point average.\n\n\nRead in the data and display it (or at least the first ten lines).\n* Make a scatterplot of high school GPA against university GPA. Which variable should be the response and which explanatory? Explain briefly. Add a smooth trend to your plot.\nDescribe any relationship on your scatterplot: its direction, its strength and its shape. Justify your description briefly.\n* Fit a linear regression for predicting university GPA from high-school GPA and display the results.\nTwo students have been admitted to university. One has a high school GPA of 3.0 and the other a high school GPA of\n3.5. Obtain suitable intervals that summarize the GPAs that each of these two students might obtain in university.\n* Now obtain a regression predicting university GPA from high-school GPA as well as the two SAT scores. Display your results.\nTest whether adding the two SAT scores has improved the prediction of university GPA. What do you conclude?\nCarry out a backward elimination starting out from your model in part (here). Which model do you end up with? Is it the same model as you fit in (here)?\nThese students were studying computer science at university. Do you find your backward-elimination result sensible or surprising, given this? Explain briefly."
  },
  {
    "objectID": "multiple-regression.html#fish-and-mercury",
    "href": "multiple-regression.html#fish-and-mercury",
    "title": "17  Multiple regression",
    "section": "17.4 Fish and mercury",
    "text": "17.4 Fish and mercury\nEating fish is generally healthy, but sometimes the fish is contaminated with mercury. What might affect the amount of mercury in a fish? Large-mouth bass were studied in Florida lakes to examine factors that might affect the amount of mercury contamination. 38 lakes were used for the study. Water samples were taken from each lake and analyzed. Also, samples of fish were caught in each lake and the mercury concentration in their muscle tissue was measured. The resulting data are in http://ritsokiguess.site/datafiles/mercury.txt, separated by single spaces. The variables measured were as follows:\n\nstandardized mercury level (parts per million in 3-year-old fish)\nalkalinity of water (milligrams per litre)\ncalcium level of water (milligrams per litre)\npH of water (standard scale; see eg. this)\n\n\nRead in and display (some of) the data.\nPlot the mercury levels against each of the explanatory variables.\nDescribe any trends (or lack thereof) that you see on your graphs.\nConcerned by some of what you see on your plots, you consult with a fisheries scientist, who says that using the logs of alkalinity and calcium are often used in modelling this kind of data. Add columns containing the logs of these two variables to your dataframe, and run a regression predicting mercury from ph and the two new variables. Display the output from your regression.\nWhat might you learn from running Box-Cox here? Explain briefly, then run it (on the same variables as your regression above) and describe what the results tell you.\nUsing the results of the Box-Cox procedure and your previous work in the question, what seems to be the most appropriate regression now? Fit it, and display the results.\nObtain all the standard residual plots (the ones we have seen in this course) for this model. Describe any problems you see.\n\nMy solutions follow:"
  },
  {
    "objectID": "multiple-regression.html#being-satisfied-with-hospital-1",
    "href": "multiple-regression.html#being-satisfied-with-hospital-1",
    "title": "17  Multiple regression",
    "section": "17.5 Being satisfied with hospital",
    "text": "17.5 Being satisfied with hospital\nA hospital administrator collects data to study the effect, if any, of a patient’s age, the severity of their illness, and their anxiety level, on the patient’s satisfaction with their hospital experience. The data, in the file link, are for 46 patients in a survey. The columns are: patient’s satisfaction score satis, on a scale of 0 to 100; the patient’s age (in years), the severity of the patient’s illness (also on a 0–100 scale), and the patient’s anxiety score on a standard anxiety test (scale of 0–5). Higher scores mean greater satisfaction, increased severity of illness and more anxiety.\n\nRead in the data and check that you have four columns in your data frame, one for each of your variables.\n\nSolution\nThis one requires a little thought first. The data values are aligned in columns, and so are the column headers. Thus, read_table is what we need:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/satisfaction.txt\"\nsatisf &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  satis = col_double(),\n  age = col_double(),\n  severity = col_double(),\n  anxiety = col_double()\n)\n\nsatisf\n\n\n\n  \n\n\n\n46 rows and 4 columns: satisfaction score (response), age, severity and anxiety (explanatory).\nThere is a small question about what to call the data frame. Basically, anything other than satis will do, since there will be confusion if your data frame has the same name as one of its columns.\n\\(\\blacksquare\\)\n\n* Obtain scatterplots of the response variable satis against each of the other variables.\n\nSolution\nThe obvious way is to do these one after the other:\n\nggplot(satisf, aes(x = age, y = satis)) + geom_point()\n\n\n\nggplot(satisf, aes(x = severity, y = satis)) + geom_point()\n\n\n\nggplot(satisf, aes(x = anxiety, y = satis)) + geom_point()\n\n\n\n\nThis is fine, but there is also a way of getting all three plots with one ggplot. This uses the facet_wrap trick, but to set that up, we have to have all the \\(x\\)-variables in one column, with an extra column labelling which \\(x\\)-variable that value was. This uses pivot_longer. The right way to do this is in a pipeline:\n\nsatisf %&gt;%\n  pivot_longer(-satis, names_to=\"xname\", values_to=\"x\") %&gt;%\n  ggplot(aes(x = x, y = satis)) + geom_point() +\n  facet_wrap(~xname, scales = \"free\", ncol = 2)\n\n\n\n\nSteps: collect together the columns age through anxiety into one column whose values go in x, with names in xname, then plot this new x against satisfaction score, with a separate facet for each different \\(x\\) (in xname).\nWhat’s the difference between facet_grid and facet_wrap? The difference is that with facet_wrap, we are letting ggplot arrange the facets how it wants to. In this case, we didn’t care which explanatory variable went on which facet, just as long as we saw all of them somewhere. Inside facet_wrap there are no dots: a squiggle, followed by the name(s) of the variable(s) that distinguish(es) the facets.1 The only “design” decision I made here was that the facets should be arranged somehow in two columns, but I didn’t care which ones should be where.\nIn facet_grid, you have a variable that you want to be displayed in rows or in columns (not just in “different facets”). I’ll show you how that works here. Since I am going to draw two plots, I should save the long data frame first and re-use it, rather than calculating it twice (so that I ought now to go back and do the other one using the saved data frame, really):\n\nsatisf %&gt;% \n  pivot_longer(age:anxiety, names_to=\"xname\", \n               values_to=\"x\") -&gt; satisf.long\nsatisf.long\n\n\n\n  \n\n\n\nIf, at this or any stage, you get confused, the way to un-confuse yourself is to fire up R Studio and do this yourself. You have all the data and code you need. If you do it yourself, you can run pipes one line at a time, inspect things, and so on.\nFirst, making a row of plots, so that xname is the \\(x\\) of the facets:\n\nggplot(satisf.long, aes(x = x, y = satis)) + geom_point() +\n  facet_grid(. ~ xname, scales = \"free\")\n\n\n\n\nI find these too tall and skinny to see the trends, as on the first facet_wrap plot.\nAnd now, making a column of plots, with xname as \\(y\\):\n\nggplot(satisf.long, aes(x = x, y = satis)) + geom_point() +\n  facet_grid(xname ~ ., scales = \"free\")\n\n\n\n\nThis one looks weird because the three \\(x\\)-variables are on different scales. The effect of the scales=\"free\" is to allow the satis scale to vary, but the x scale cannot because the facets are all in a line. Compare this:\n\nggplot(satisf.long, aes(x = x, y = satis)) + geom_point() +\n  facet_wrap(~xname, ncol = 1, scales = \"free\")\n\n\n\n\nThis time, the \\(x\\) scales came out different (and suitable), but I still like squarer plots better for judging relationships.\n\\(\\blacksquare\\)\n\nIn your scatterplots of (here), which relationship appears to be the strongest one?\n\nSolution\nAll the trends appear to be downward ones, but I think satis and age is the strongest trend. The other ones look more scattered to me.\n\\(\\blacksquare\\)\n\n* Create a correlation matrix for all four variables. Does your strongest trend of the previous part have the strongest correlation?\n\nSolution\nThis is a matter of running the whole data frame through cor:\n\ncor(satisf)\n\n              satis        age   severity    anxiety\nsatis     1.0000000 -0.7736828 -0.5874444 -0.6023105\nage      -0.7736828  1.0000000  0.4666091  0.4976945\nseverity -0.5874444  0.4666091  1.0000000  0.7945275\nanxiety  -0.6023105  0.4976945  0.7945275  1.0000000\n\n\nIgnoring the correlations of variables with themselves, the correlation of satisf with age, the one I picked out, is the strongest (the most negative trend). If you picked one of the other trends as the strongest, you need to note how close it is to the maximum correlation: for example, if you picked satis and severity, that’s the second highest correlation (in size).\n\\(\\blacksquare\\)\n\nRun a regression predicting satisfaction from the other three variables, and display the output.\n\nSolution\n\nsatisf.1 &lt;- lm(satis ~ age + severity + anxiety, data = satisf)\nsummary(satisf.1)\n\n\nCall:\nlm(formula = satis ~ age + severity + anxiety, data = satisf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.954  -7.154   1.550   6.599  14.888 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 162.8759    25.7757   6.319 4.59e-06 ***\nage          -1.2103     0.3015  -4.015  0.00074 ***\nseverity     -0.6659     0.8210  -0.811  0.42736    \nanxiety      -8.6130    12.2413  -0.704  0.49021    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.29 on 19 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.621 \nF-statistic: 13.01 on 3 and 19 DF,  p-value: 7.482e-05\n\n\n\\(\\blacksquare\\)\n\nDoes the regression fit well overall? How can you tell?\n\nSolution\nFor this, look at R-squared, which is 0.682 (68.2%). This is one of those things to have an opinion about. I’d say this is good but not great. I would not call it “poor”, since there definitely is a relationship, even if it’s not a stupendously good one.\n\\(\\blacksquare\\)\n\nTest the null hypothesis that none of your explanatory variables help, against the alternative that one or more of them do. (You’ll need an appropriate P-value. Which one is it?) What do you conclude?\n\nSolution\nThis one is the (global) \\(F\\)-test, whose P-value is at the bottom. It translates to 0.000000000154, so this is definitely small, and we reject the null. Thus, one or more of age, severity and anxiety helps to predict satisfaction. (I would like to see this last sentence, rather than just “reject the null”.)\n\\(\\blacksquare\\)\n\nThe correlation between severity and satis is not small, but in my regression I found that severity was nowhere near significant. Why is this? Explain briefly. \n\nSolution\nThe key thing to observe is that the \\(t\\)-test in the regression says how important a variable is given the others that are already in the regression, or, if you prefer, how much that variable adds to the regression, on top of the ones that are already there. So here, we are saying that severity has nothing to add, given that the regression already includes the others. (That is, high correlation and strong significance don’t always go together.) For a little more insight, look at the correlation matrix of (here) again. The strongest trend with satis is with age, and indeed age is the one obviously significant variable in the regression. The trend of severity with satis is somewhat downward, and you might otherwise have guessed that this is strong enough to be significant. But see that severity also has a clear relationship with age. A patient with low severity of disease is probably also younger, and we know that younger patients are likely to be more satisfied. Thus severity has nothing (much) to add.\nExtra 1: The multiple regression is actually doing something clever here. Just looking at the correlations, it appears that all three variables are helpful, but the regression is saying that once you have looked at age (“controlled for age”), severity of illness does not have an impact: the correlation of severity with satis is as big as it is almost entirely because of age. This gets into the domain of “partial correlation”. If you like videos, you can see link for this. I prefer regression, myself, since I find it clearer. anxiety tells a different story: this is close to significant (or is significant at the \\(\\alpha=0.10\\) level), so the regression is saying that anxiety does appear to have something to say about satis over and above age. This is rather odd, to my mind, since anxiety has only a slightly stronger correlation with satis and about the same with age as severity does. But the regression is telling the story to believe, because it handles all the inter-correlations, not just the ones between pairs of variables.\nI thought it would be rather interesting to do some predictions here. Let’s predict satisfaction for all combinations of high and low age, severity and anxiety. I’ll use the quartiles for high and low.\nThe easiest way to get those is via summary:\n\nsummary(satisf)\n\n     satis            age           severity        anxiety     \n Min.   :26.00   Min.   :28.00   Min.   :43.00   Min.   :1.800  \n 1st Qu.:50.00   1st Qu.:33.00   1st Qu.:48.00   1st Qu.:2.150  \n Median :60.00   Median :40.00   Median :50.00   Median :2.300  \n Mean   :61.35   Mean   :39.61   Mean   :50.78   Mean   :2.296  \n 3rd Qu.:73.50   3rd Qu.:44.50   3rd Qu.:53.50   3rd Qu.:2.400  \n Max.   :89.00   Max.   :55.00   Max.   :62.00   Max.   :2.900  \n\n\nand then use datagrid to make combinations for prediction:\n\nnew &lt;- datagrid(model = satisf.1, age = c(33, 44.5), \n                severity = c(48, 53.5), anxiety = c(2.15, 2.4))\nnew\n\n\n\n  \n\n\n\nEight rows for the \\(2^3 = 8\\) combinations. Then get the predictions for these:\n\ncbind(predictions(satisf.1, newdata = new))\n\n\n\n  \n\n\n\nExtra 2: the standard errors vary quite a bit. The smallest ones are where age, severity, and anxiety are all high or all low (the last row and the first one). This is where most of the data is, because the three explanatory variables are positively correlated with each other (if you know that one of them is high, the others will probably be high too). The other standard errors are higher because there is not much data “nearby”, and so we don’t know as much about the quality of the predictions there.\nExtra 3: we had to copy the quartile values into the new dataframe we were making (to predict from), which ought to have caused you some concern, since there was no guarantee that we copied them correctly. It would be better to make a dataframe with just the quartiles, and feed that into datagrid. Here’s how we can do that.\n\nsatisf %&gt;% \n  summarize(across(-satis,\n                   \\(x) quantile(x, c(0.25, 0.75)))) -&gt; d\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\nd\n\n\n\n  \n\n\n\nTo summarize several columns at once, use across. This one reads “for each column that is not satis, work out the first and third quartiles (25th and 75th percentiles) of it. Recall that the first input to quantile is what to compute percentiles of, and the optional2 second input is which percentiles to compute. Also, when summarize is fed a summary that is more than one number long (two quartiles, here) it will automatically be unnested longer, which happens to be exactly what we want here.\nSo now, we need to take the columns from here and feed them into datagrid. The way to do that is to use with:\n\nnew &lt;- with(d, datagrid(model = satisf.1, age = age, severity = severity, anxiety = anxiety))\nnew\n\n\n\n  \n\n\n\nThe clunky repetition is needed because the first (eg.) age in age = age is the name that the column in new is going to get, and the second age is the thing that supplies the values to be combined (the column of d called age). This is exactly the same new that we had before, and so the predictions will be exactly the same as they were before.\n\\(\\blacksquare\\)\n\nCarry out a backward elimination to determine which of age, severity and anxiety are needed to predict satisfaction. What do you get? Use \\(\\alpha = 0.10\\).\n\nSolution\nThis means starting with the regression containing all the explanatory variables, which is the one I called satisf.1:\n\nsummary(satisf.1)\n\n\nCall:\nlm(formula = satis ~ age + severity + anxiety, data = satisf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.954  -7.154   1.550   6.599  14.888 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 162.8759    25.7757   6.319 4.59e-06 ***\nage          -1.2103     0.3015  -4.015  0.00074 ***\nseverity     -0.6659     0.8210  -0.811  0.42736    \nanxiety      -8.6130    12.2413  -0.704  0.49021    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.29 on 19 degrees of freedom\nMultiple R-squared:  0.6727,    Adjusted R-squared:  0.621 \nF-statistic: 13.01 on 3 and 19 DF,  p-value: 7.482e-05\n\n\nPull out the least-significant (highest P-value) variable, which here is severity. We already decided that this had nothing to add:\n\nsatisf.2 &lt;- update(satisf.1, . ~ . - severity)\nsummary(satisf.2)\n\n\nCall:\nlm(formula = satis ~ age + anxiety, data = satisf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.868  -6.649   2.506   6.445  16.120 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 147.0751    16.7334   8.789 2.64e-08 ***\nage          -1.2434     0.2961  -4.199 0.000442 ***\nanxiety     -15.8906     8.2556  -1.925 0.068593 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.2 on 20 degrees of freedom\nMultiple R-squared:  0.6613,    Adjusted R-squared:  0.6275 \nF-statistic: 19.53 on 2 and 20 DF,  p-value: 1.985e-05\n\n\nIf you like, copy and paste the first lm, edit it to get rid of severity, and run it again. But when I have a “small change” to make to a model, I like to use update.\nHaving taken severity out, anxiety has become significant (at \\(\\alpha = 0.10\\)). Since all of the explanatory variables are now significant, this is where we stop. If we’re predicting satisfaction, we need to know both a patient’s age and their anxiety score: being older or more anxious is associated with a decrease in satisfaction.\nThere is also a function step that will do this for you:\n\nstep(satisf.1, direction = \"backward\", test = \"F\")\n\nStart:  AIC=110.84\nsatis ~ age + severity + anxiety\n\n           Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    \n- anxiety   1     52.41 2064.0 109.43  0.4951 0.4902110    \n- severity  1     69.65 2081.2 109.62  0.6579 0.4273559    \n&lt;none&gt;                  2011.6 110.84                      \n- age       1   1706.67 3718.3 122.97 16.1200 0.0007404 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=109.43\nsatis ~ age + severity\n\n           Df Sum of Sq    RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;                  2064.0 109.43                      \n- severity  1    402.78 2466.8 111.53  3.9029 0.0621629 .  \n- age       1   1960.56 4024.6 122.79 18.9977 0.0003042 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCall:\nlm(formula = satis ~ age + severity, data = satisf)\n\nCoefficients:\n(Intercept)          age     severity  \n    166.591       -1.260       -1.089  \n\n\nwith the same result.3 This function doesn’t actually use P-values; instead it uses a thing called AIC. At each step, the variable with the lowest AIC comes out, and when &lt;none&gt; bubbles up to the top, that’s when you stop. The test=\"F\" means “include an \\(F\\)-test”, but the procedure still uses AIC (it just shows you an \\(F\\)-test each time as well). In this case, the other variables were in the same order throughout, but they don’t have to be (in the same way that removing one variable from a multiple regression can dramatically change the P-values of the ones that remain). Here, at the first step, &lt;none&gt; and anxiety were pretty close, but when severity came out, taking out nothing was a lot better than taking out anxiety.\nThe test=\"F\" on the end gets you the P-values. Using the \\(F\\)-test is right for regressions; for things like logistic regression that we see later, test=\"Chisq\" is the right one to use.4\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "multiple-regression.html#salaries-of-mathematicians-1",
    "href": "multiple-regression.html#salaries-of-mathematicians-1",
    "title": "17  Multiple regression",
    "section": "17.6 Salaries of mathematicians",
    "text": "17.6 Salaries of mathematicians\nA researcher in a scientific foundation wanted to evaluate the relationship between annual salaries of mathematicians and three explanatory variables:\n\nan index of work quality\nnumber of years of experience\nan index of publication success.\n\nThe data can be found at link. Data from only a relatively small number of mathematicians were available.\n\nRead in the data and check that you have a sensible number of rows and the right number of columns. (What does “a sensible number of rows” mean here?)\n\nSolution\nThis is a tricky one. There are aligned columns, but the column headers are not aligned with them. Thus read_table2 is what you need.\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/mathsal.txt\"\nsalaries &lt;- read_table2(my_url)\n\nWarning: `read_table2()` was deprecated in readr 2.0.0.\nℹ Please use `read_table()` instead.\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  salary = col_double(),\n  workqual = col_double(),\n  experience = col_double(),\n  pubsucc = col_double()\n)\n\nsalaries\n\n\n\n  \n\n\n\n24 observations (“only a relatively small number”) and 4 columns, one for the response and one each for the explanatory variables.\nOr, if you like,\n\ndim(salaries)\n\n[1] 24  4\n\n\nfor the second part: 24 rows and 4 columns again. I note, with only 24 observations, that we don’t really have enough data to investigate the effects of three explanatory variables, but we’ll do the best we can. If the pattern, whatever it is, is clear enough, we should be OK.\n\\(\\blacksquare\\)\n\nMake scatterplots of salary against each of the three explanatory variables. If you can, do this with one ggplot.\n\nSolution\nThe obvious way to do this is as three separate scatterplots, and that will definitely work. But you can do it in one go if you think about facets, and about having all the \\(x\\)-values in one column (and the names of the \\(x\\)-variables in another column):\n\nsalaries %&gt;%\n  pivot_longer(-salary, names_to=\"xname\", values_to=\"x\") %&gt;%\n  ggplot(aes(x = x, y = salary)) + geom_point() +\n  facet_wrap(~xname, ncol = 2, scales = \"free\")\n\n\n\n\nIf you don’t see how that works, run it yourself, one line at a time.\nI was thinking ahead a bit while I was coding that: I wanted the three plots to come out about square, and I wanted the plots to have their own scales. The last thing in the facet_wrap does the latter, and arranging the plots in two columns (thinking of the plots as a set of four with one missing) gets them more or less square.\nIf you don’t think of those, try it without, and then fix up what you don’t like.\n\\(\\blacksquare\\)\n\nComment briefly on the direction and strength of each relationship with salary.\n\nSolution\nTo my mind, all of the three relationships are going uphill (that’s the “direction” part). experience is the strongest, and pubsucc looks the weakest (that’s the “strength” part). If you want to say there is no relationship with pubsucc, that’s fine too. This is a judgement call. Note that all the relationships are more or less linear (no obvious curves here). We could also investigate the relationships among the explanatory variables:\n\ncor(salaries)\n\n              salary  workqual experience   pubsucc\nsalary     1.0000000 0.6670958  0.8585582 0.5581960\nworkqual   0.6670958 1.0000000  0.4669511 0.3227612\nexperience 0.8585582 0.4669511  1.0000000 0.2537530\npubsucc    0.5581960 0.3227612  0.2537530 1.0000000\n\n\nMentally cut off the first row and column (salary is the response). None of the remaining correlations are all that high, so we ought not to have any multicollinearity problems.\n\\(\\blacksquare\\)\n\n* Fit a regression predicting salary from the other three variables, and obtain a summary of the results.\n\nSolution\n\nsalaries.1 &lt;- lm(salary ~ workqual + experience + pubsucc, data = salaries)\nsummary(salaries.1)\n\n\nCall:\nlm(formula = salary ~ workqual + experience + pubsucc, data = salaries)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2463 -0.9593  0.0377  1.1995  3.3089 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.84693    2.00188   8.915 2.10e-08 ***\nworkqual     1.10313    0.32957   3.347 0.003209 ** \nexperience   0.32152    0.03711   8.664 3.33e-08 ***\npubsucc      1.28894    0.29848   4.318 0.000334 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.753 on 20 degrees of freedom\nMultiple R-squared:  0.9109,    Adjusted R-squared:  0.8975 \nF-statistic: 68.12 on 3 and 20 DF,  p-value: 1.124e-10\n\n\n\\(\\blacksquare\\)\n\nHow can we justify the statement “one or more of the explanatory variables helps to predict salary”? How is this consistent with the value of R-squared?\n\nSolution\n“One or more of the explanatory variables helps” is an invitation to consider the (global) \\(F\\)-test for the whole regression. This has the very small P-value of \\(1.124\\times 10^{-10}\\) (from the bottom line of the output): very small, so one or more of the explanatory variables does help, and the statement is correct. The idea that something helps to predict salary suggests (especially with such a small number of observations) that we should have a high R-squared. In this case, R-squared is 0.9109, which is indeed high.\n\\(\\blacksquare\\)\n\nWould you consider removing any of the variables from this regression? Why, or why not?\n\nSolution\nLook at the P-values attached to each variable. These are all very small: 0.003, 0.00000003 and 0.0003, way smaller than 0.05. So it would be a mistake to take any, even one, of the variables out: doing so would make the regression much worse. If you need convincing of that, see what happens when we take the variable with the highest P-value out — this is workqual:\n\nsalaries.2 &lt;- lm(salary ~ experience + pubsucc, data = salaries)\nsummary(salaries.2)\n\n\nCall:\nlm(formula = salary ~ experience + pubsucc, data = salaries)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2723 -0.7865 -0.3983  1.7277  3.2060 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 21.02546    2.14819   9.788 2.82e-09 ***\nexperience   0.37376    0.04104   9.107 9.70e-09 ***\npubsucc      1.52753    0.35331   4.324    3e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.137 on 21 degrees of freedom\nMultiple R-squared:  0.8609,    Adjusted R-squared:  0.8477 \nF-statistic:    65 on 2 and 21 DF,  p-value: 1.01e-09\n\n\nR-squared has gone down from 91% to 86%: maybe not so much in the grand scheme of things, but it is noticeably less. Perhaps better, since we are comparing models with different numbers of explanatory variables, is to compare the adjusted R-squared: this has gone down from 90% to 85%. The fact that this has gone down at all is enough to say that taking out workqual was a mistake.5\nAnother way of seeing whether a variable has anything to add in a regression containing the others is a partial regression plot. We take the residuals from salaries.2 above and plot them against the variable we removed, namely workqual.6 If workqual has nothing to add, there will be no pattern; if it does have something to add, there will be a trend. Like this. I use augment from broom:\n\nlibrary(broom)\nsalaries.2 %&gt;%\n  augment(salaries) %&gt;%\n  ggplot(aes(x = workqual, y = .resid)) + geom_point()\n\n\n\n\nThis is a mostly straight upward trend. So we need to add a linear term in workqual to the regression.7\n\\(\\blacksquare\\)\n\nDo you think it would be a mistake to take both of workqual and pubsucc out of the regression? Do a suitable test. Was your guess right?\n\nSolution\nI think it would be a big mistake. Taking even one of these variables out of the regression is a bad idea (from the \\(t\\)-tests), so taking out more than one would be a really bad idea. To perform a test, fit the model without these two explanatory variables:\n\nsalaries.3 &lt;- lm(salary ~ experience, data = salaries)\n\nand then use anova to compare the two regressions, smaller model first:\n\nanova(salaries.3, salaries.1)\n\n\n\n  \n\n\n\nThe P-value is extremely small, so the bigger model is definitely better than the smaller one: we really do need all three variables. Which is what we guessed.\n\\(\\blacksquare\\)\n\nBack in part (here), you fitted a regression with all three explanatory variables. By making suitable plots, assess whether there is any evidence that (i) that the linear model should be a curve, (ii) that the residuals are not normally distributed, (iii) that there is “fan-out”, where the residuals are getting bigger in size as the fitted values get bigger? Explain briefly how you came to your conclusions in each case.\n\nSolution\nI intended that (i) should just be a matter of looking at residuals vs. fitted values:\n\nggplot(salaries.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nThere is no appreciable pattern on here, so no evidence of a curve (or apparently of any other problems).\nExtra: you might read this that we should check residuals against the \\(x\\)-variables as well, which is a similar trick to the above one for plotting response against each of the explanatories. There is one step first, though: use augment from broom first to get a dataframe with the original \\(x\\)-variables and the residuals in it. The following thus looks rather complicated, and if it confuses you, run the code a piece at a time to see what it’s doing:\n\nsalaries.1 %&gt;%\n  augment(salaries) %&gt;%\n  pivot_longer(workqual:pubsucc, names_to=\"xname\", values_to=\"x\") %&gt;%\n  ggplot(aes(x = x, y = .resid)) + geom_point() +\n  facet_wrap(~xname, scales = \"free\", ncol = 2)\n\n\n\n\nThese three residual plots are also pretty much textbook random, so no problems here either.\nFor (ii), look at a normal quantile plot of the residuals, which is not as difficult as the plot I just did:\n\nggplot(salaries.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nThat is really pretty good. Maybe the second smallest point is a bit far off the line, but otherwise there’s nothing to worry about. A quick place to look for problems is the extreme observations, and the largest and smallest residuals are almost exactly the size we’d expect them to be.\nOur graph for assessing fan-in or fan-out is to plot the absolute values of the residuals against the fitted values. The plot from (i) suggests that we won’t have any problems here, but to investigate:\n\nggplot(salaries.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis is pretty nearly straight across. You might think it increases a bit at the beginning, but most of the evidence for that comes from the one observation with fitted value near 30 that happens to have a residual near zero. Conclusions based on one observation are not to be trusted! In summary, I’m happy with this linear multiple regression, and I don’t see any need to do anything more with it. I am, however, willing to have some sympathy with opinions that differ from mine, if they are supported by those graphs above.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "multiple-regression.html#predicting-gpa-of-computer-science-students-1",
    "href": "multiple-regression.html#predicting-gpa-of-computer-science-students-1",
    "title": "17  Multiple regression",
    "section": "17.7 Predicting GPA of computer science students",
    "text": "17.7 Predicting GPA of computer science students\nThe file link contains some measurements of academic achievement for a number of university students studying computer science:\n\nHigh school grade point average\nMath SAT score\nVerbal SAT score\nComputer Science grade point average\nOverall university grade point average.\n\n\nRead in the data and display it (or at least the first ten lines).\n\nSolution\nThe usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/gpa.txt\"\ngpa &lt;- read_delim(my_url, \" \")\n\nRows: 105 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (5): high_GPA, math_SAT, verb_SAT, comp_GPA, univ_GPA\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngpa\n\n\n\n  \n\n\n\nTwo SAT scores and three GPAs, as promised.\n\\(\\blacksquare\\)\n\n* Make a scatterplot of high school GPA against university GPA. Which variable should be the response and which explanatory? Explain briefly. Add a smooth trend to your plot.\n\nSolution\nHigh school comes before university, so high school should be explanatory and university should be the response. (High school grades are used as an admission criterion to university, so we would hope they would have some predictive value.)\n\nggplot(gpa, aes(x = high_GPA, y = univ_GPA)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\nDescribe any relationship on your scatterplot: its direction, its strength and its shape. Justify your description briefly.\n\nSolution\nTaking these points one at a time:\n\ndirection: upward (a higher high-school GPA generally goes with a higher university GPA as well. Or you can say that the lowest high-school GPAs go with the lowest university GPAs, and high with high, at least most of the time).\nstrength: something like moderately strong, since while the trend is upward, there is quite a lot of scatter. (This is a judgement call: something that indicates that you are basing your description on something reasonable is fine.)\nshape: I’d call this “approximately linear” since there is no clear curve here. The smooth trend wiggles a bit, but not enough to make me doubt a straight line.\n\nLooking ahead, I also notice that when high-school GPA is high, university GPA is also consistently high, but when high-school GPA is low, the university GPA is sometimes low and sometimes high, a lot more variable. (This suggests problems with fan-in later.) In a practical sense, what this seems to show is that people who do well at university usually did well in high-school as well, but sometimes their high-school grades were not that good. This is especially true for people with university GPAs around 3.25.\n\\(\\blacksquare\\)\n\n* Fit a linear regression for predicting university GPA from high-school GPA and display the results.\n\nSolution\nJust this, therefore:\n\ngpa.1 &lt;- lm(univ_GPA ~ high_GPA, data = gpa)\nsummary(gpa.1)\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.69040 -0.11922  0.03274  0.17397  0.91278 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.09682    0.16663   6.583 1.98e-09 ***\nhigh_GPA     0.67483    0.05342  12.632  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2814 on 103 degrees of freedom\nMultiple R-squared:  0.6077,    Adjusted R-squared:  0.6039 \nF-statistic: 159.6 on 1 and 103 DF,  p-value: &lt; 2.2e-16\n\n\nExtra: this question goes on too long, so I didn’t ask you to look at the residuals from this model, but my comments earlier suggested that we might have had some problems with fanning-in (the variability of predictions getting less as the high-school GPA increases). In case you are interested, I’ll look at this here. First, residuals against fitted values:\n\nggplot(gpa.1, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nIs that evidence of a trend in the residuals? Dunno. I’m inclined to call this an “inconsequential wiggle” and say it’s OK. Note that the grey envelope includes zero all the way across.\nNormal quantile plot of residuals:\n\nggplot(gpa.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nA somewhat long-tailed distribution: compared to a normal distribution, the residuals are a bit too big in size, both on the positive and negative end.\nThe problem I was really worried about was the potential of fanning-in, which we can assess by looking at the absolute residuals:\n\nggplot(gpa.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThat is definitely a downward trend in the size of the residuals, and I think I was right to be worried before. The residuals should be of similar size all the way across.\nThe usual problem of this kind is fanning-out, where the residuals get bigger in size as the fitted values increase. The bigger values equals more spread is the kind of thing that a transformation like taking logs will handle: the bigger values are all brought downwards, so they will be both smaller and less variable than they were. This one, though, goes the other way, so the only kind of transformation that might help is one at the other end of the scale (think of the Box-Cox lambda scale), something like maybe reciprocal, \\(\\lambda=-1\\) maybe.\nThe other thought I had was that there is this kind of break around a high-school GPA of 3 (go back to the scatterplot of (here)): when the high-school GPA is higher than 3, the university GPA is very consistent (and shows a clear upward trend), but when the high-school GPA is less than 3, the university GPA is very variable and there doesn’t seem to be any trend at all. So maybe two separate analyses would be the way to go.\nAll right, how does Box-Cox work out here, if at all?\n\nlibrary(MASS)\nboxcox(univ_GPA ~ high_GPA, data = gpa)\n\n\n\n\nIt doesn’t. All right, that answers that question.\n\\(\\blacksquare\\)\n\nTwo students have been admitted to university. One has a high school GPA of 3.0 and the other a high school GPA of\n3.5. Obtain suitable intervals that summarize the GPAs that each of these two students might obtain in university.\n\nSolution\nSince we are talking about individual students, rather than the mean of all students with these GPAs, prediction intervals are called for. The first step is to make a data frame to predict from. This has to contain columns for all the explanatory variables, just high_GPA in this case:\n\nnew &lt;- datagrid(model = gpa.1, high_GPA = c(3,3.5))\nnew\n\n\n\n  \n\n\n\nIn general, the advantage to doing it this way is that you also get representative values for any other explanatory variables (means for quantitative ones, the most common category for categorical ones). But in this case, the dataframe has just one column with two values in it, so any other way to create this dataframe is equally good, and, you might say, easier, like this:\n\ngpa.new &lt;- tibble(high_GPA = c(3, 3.5))\ngpa.new\n\n\n\n  \n\n\n\nThe next thing to consider is whether you want a confidence interval for the mean response (the kind of thing predictions gives you), or a prediction interval for an individual response. In this case, it’s the prediction interval, because we want to infer how these individual students will fare. To get this, you need the old-fashioned predict rather than predictions:\n\npreds &lt;- predict(gpa.1, gpa.new, interval = \"p\")\npreds\n\n       fit      lwr      upr\n1 3.121313 2.560424 3.682202\n2 3.458728 2.896105 4.021351\n\n\nand display that side by side with the values it was predicted from:\n\ncbind(gpa.new, preds)\n\n\n\n  \n\n\n\nor this way, if you like it better:\n\nas_tibble(preds) %&gt;% bind_cols(gpa.new) %&gt;% select(high_GPA, everything())\n\n\n\n  \n\n\n\nThus the predicted university GPA for the student with high school GPA 3.0 is between 2.6 and 3.7, and for the student with high school GPA 3.5 is between 2.9 and 4.0. (I think this is a good number of decimals to give, but in any case, you should actually say what the intervals are.)\nExtra: I observe that these intervals are almost exactly the same length. This surprises me a bit, since I would have said that 3.0 is close to the average high-school GPA and 3.5 is noticeably higher. If that’s the case, the prediction interval for 3.5 should be longer (since there is less “nearby data”). Was I right about that?\n\ngpa %&gt;% summarize(\n  mean = mean(high_GPA),\n  med = median(high_GPA),\n  q1 = quantile(high_GPA, 0.25),\n  q3 = quantile(high_GPA, 0.75)\n)\n\n\n\n  \n\n\n\nMore or less: the mean is close to 3, and 3.5 is close to the third quartile. So the thing about the length of the prediction interval is a bit of a mystery. Maybe it works better for the confidence interval for the mean:\n\ncbind(predictions(gpa.1, newdata = new))\n\n\n\n  \n\n\n\nThese intervals are a lot shorter, since we are talking about all students with the high-school GPAs in question, and we therefore no longer have to worry about variation from student to student (which is considerable). But my supposition about length is now correct: the interval for 3.5, which is further from the mean, is a little longer than the interval for 3.0. Thinking about it, it seems that the individual-to-individual variation, which is large, is dominating things for our prediction interval above.\n\\(\\blacksquare\\)\n\n* Now obtain a regression predicting university GPA from high-school GPA as well as the two SAT scores. Display your results.\n\nSolution\nCreate a new regression with all the explanatory variables you want in it:\n\ngpa.2 &lt;- lm(univ_GPA ~ high_GPA + math_SAT + verb_SAT, data = gpa)\nsummary(gpa.2)\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + math_SAT + verb_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68186 -0.13189  0.01289  0.16186  0.93994 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.5793478  0.3422627   1.693   0.0936 .  \nhigh_GPA    0.5454213  0.0850265   6.415  4.6e-09 ***\nmath_SAT    0.0004893  0.0010215   0.479   0.6330    \nverb_SAT    0.0010202  0.0008123   1.256   0.2120    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2784 on 101 degrees of freedom\nMultiple R-squared:  0.6236,    Adjusted R-squared:  0.6124 \nF-statistic: 55.77 on 3 and 101 DF,  p-value: &lt; 2.2e-16\n\n\n\\(\\blacksquare\\)\n\nTest whether adding the two SAT scores has improved the prediction of university GPA. What do you conclude?\n\nSolution\nSince we added two explanatory variables, the \\(t\\)-tests in gpa.2 don’t apply (they tell us whether we can take out one \\(x\\)-variable). We might have some suspicions, but that’s all they are. So we have to do anova:\n\nanova(gpa.1, gpa.2)\n\n\n\n  \n\n\n\nIf you put the models the other way around, you’ll get a negative \\(F\\)-statistic and degrees of freedom, which doesn’t make much sense (although the test will still work).\nThe null hypothesis here is that the two models fit equally well. Since the P-value is not small, we do not reject that null hypothesis, and therefore we conclude that the two models do fit equally well, and therefore we prefer the smaller one, the one that predicts university GPA from just high-school GPA. (Or, equivalently, we conclude that those two SAT scores don’t add anything to the prediction of how well a student will do at university, once you know their high-school GPA.)\nThis might surprise you, given what the SATs are supposed to be for. But that’s what the data say.\n\\(\\blacksquare\\)\n\nCarry out a backward elimination starting out from your model in part (here). Which model do you end up with? Is it the same model as you fit in (here)?\n\nSolution\nIn the model of (here), math_SAT was the least significant, so that comes out first. (I use update but I’m not insisting that you do:)\n\ngpa.3 &lt;- update(gpa.2, . ~ . - math_SAT)\nsummary(gpa.3)\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + verb_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68430 -0.11268  0.01802  0.14901  0.95239 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.6838723  0.2626724   2.604   0.0106 *  \nhigh_GPA    0.5628331  0.0765729   7.350 5.07e-11 ***\nverb_SAT    0.0012654  0.0006283   2.014   0.0466 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2774 on 102 degrees of freedom\nMultiple R-squared:  0.6227,    Adjusted R-squared:  0.6153 \nF-statistic: 84.18 on 2 and 102 DF,  p-value: &lt; 2.2e-16\n\n\nHere is where we have to stop, since both high-school GPA and verbal SAT score are significant, and so taking either of them out would be a bad idea. This is a different model than the one of (here). This is the case, even though the model with high-school GPA only was not significantly worse than the model containing everything. (This goes to show that model-building doesn’t always have nice clear answers.)\nIn the model I called gpa.2, neither of the SAT scores were anywhere near significant (considered singly), but as soon as we took out one of the SAT scores (my model gpa.3), the other one became significant. This goes to show that you shouldn’t take out more than one explanatory variable based on the results of the \\(t\\)-tests, and even if you test to see whether you should have taken out both of the SAT, you won’t necessarily get consistent results. Admittedly, it’s a close decision whether to keep or remove verb_SAT, since its P-value is close to 0.05.\nThe other way of tackling this one is via step, which does the backward elimination for you (not that it was much work here):\n\nstep(gpa.2, direction = \"backward\", test = \"F\")\n\nStart:  AIC=-264.6\nuniv_GPA ~ high_GPA + math_SAT + verb_SAT\n\n           Df Sum of Sq     RSS     AIC F value    Pr(&gt;F)    \n- math_SAT  1    0.0178  7.8466 -266.36  0.2294     0.633    \n- verb_SAT  1    0.1223  7.9511 -264.97  1.5777     0.212    \n&lt;none&gt;                   7.8288 -264.60                      \n- high_GPA  1    3.1896 11.0184 -230.71 41.1486 4.601e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=-266.36\nuniv_GPA ~ high_GPA + verb_SAT\n\n           Df Sum of Sq     RSS     AIC F value    Pr(&gt;F)    \n&lt;none&gt;                   7.8466 -266.36                      \n- verb_SAT  1    0.3121  8.1587 -264.26  4.0571   0.04662 *  \n- high_GPA  1    4.1562 12.0028 -223.73 54.0268 5.067e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + verb_SAT, data = gpa)\n\nCoefficients:\n(Intercept)     high_GPA     verb_SAT  \n   0.683872     0.562833     0.001265  \n\n\nThis gives the same result as we did from our backward elimination. The tables with AIC in them are each step of the elimination, and the variable at the top is the one that comes out next. (When &lt;none&gt; gets to the top, that’s when you stop.) What happened is that the two SAT scores started off highest, but once we removed math_SAT, verb_SAT jumped below &lt;none&gt; and so the verbal SAT score had to stay.\nBoth the P-value and the AIC say that the decision about keeping or removing verb_SAT is very close.\n\\(\\blacksquare\\)\n\nThese students were studying computer science at university. Do you find your backward-elimination result sensible or surprising, given this? Explain briefly.\n\nSolution\nI would expect computer science students to be strong students generally, good at math and possibly not so good with words. So it is not surprising that high-school GPA figures into the prediction, but I would expect math SAT score to have an impact also, and it does not. It is also rather surprising that verbal SAT score predicts success at university for these computer science students; you wouldn’t think that having better skills with words would be helpful. So I’m surprised. Here, I’m looking for some kind of discussion about what’s in the final backward-elimination model, and what you would expect to be true of computer science students.\nLet’s look at the final model from the backward elimination again:\n\nsummary(gpa.3)\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + verb_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68430 -0.11268  0.01802  0.14901  0.95239 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.6838723  0.2626724   2.604   0.0106 *  \nhigh_GPA    0.5628331  0.0765729   7.350 5.07e-11 ***\nverb_SAT    0.0012654  0.0006283   2.014   0.0466 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2774 on 102 degrees of freedom\nMultiple R-squared:  0.6227,    Adjusted R-squared:  0.6153 \nF-statistic: 84.18 on 2 and 102 DF,  p-value: &lt; 2.2e-16\n\n\nThe two slope estimates are both positive, meaning that, all else equal, a higher value for each explanatory variable goes with a higher university GPA. This indicates that a higher verbal SAT score goes with a higher university GPA: this is across all the university courses that a student takes, which you would expect to be math and computer science courses for a Comp Sci student, but might include a few electives or writing courses. Maybe what is happening is that the math SAT score is telling the same story as the high-school GPA for these students, and the verbal SAT score is saying something different. (For example, prospective computer science students are mostly likely to have high math SAT scores, so there’s not much information there.)\nI think I need to look at the correlations:\n\ncor(gpa)\n\n          high_GPA  math_SAT  verb_SAT  comp_GPA  univ_GPA\nhigh_GPA 1.0000000 0.7681423 0.7261478 0.7914721 0.7795631\nmath_SAT 0.7681423 1.0000000 0.8352272 0.6877209 0.6627837\nverb_SAT 0.7261478 0.8352272 1.0000000 0.6387512 0.6503012\ncomp_GPA 0.7914721 0.6877209 0.6387512 1.0000000 0.9390459\nuniv_GPA 0.7795631 0.6627837 0.6503012 0.9390459 1.0000000\n\n\nWe’ll ignore comp_GPA (i) because we haven’t been thinking about it and (ii) because it’s highly correlated with the university GPA anyway. (There isn’t a sense that one of the two university GPAs is explanatory and the other is a response, since students are taking the courses that contribute to them at the same time.)\nThe highest correlation with university GPA of what remains is high-school GPA, so it’s not at all surprising that this features in all our regressions. The correlations between university GPA and the two SAT scores are about equal, so there appears to be no reason to favour one of the SAT scores over the other. But, the two SAT scores are highly correlated with each other (0.835), which suggests that if you have one, you don’t need the other, because they are telling more or less the same story.\nThat makes me wonder how a regression with the SAT math score and not the SAT verbal score would look:\n\ngpa.4 &lt;- lm(univ_GPA ~ high_GPA + math_SAT, data = gpa)\nsummary(gpa.4)\n\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + math_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68079 -0.12264  0.00741  0.16579  0.90010 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.6072916  0.3425047   1.773   0.0792 .  \nhigh_GPA    0.5710745  0.0827705   6.899  4.5e-10 ***\nmath_SAT    0.0012980  0.0007954   1.632   0.1058    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2792 on 102 degrees of freedom\nMultiple R-squared:  0.6177,    Adjusted R-squared:  0.6102 \nF-statistic:  82.4 on 2 and 102 DF,  p-value: &lt; 2.2e-16\n\n\nMath SAT does not quite significantly add anything to the prediction, which confirms that we do better to use the verbal SAT score (surprising though it seems). Though, the two regressions with the single SAT scores, gpa.3 and gpa.4, have almost the same R-squared values. It’s not clear-cut at all. In the end, you have to make a call and stand by it.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "multiple-regression.html#fish-and-mercury-1",
    "href": "multiple-regression.html#fish-and-mercury-1",
    "title": "17  Multiple regression",
    "section": "17.8 Fish and mercury",
    "text": "17.8 Fish and mercury\nEating fish is generally healthy, but sometimes the fish is contaminated with mercury. What might affect the amount of mercury in a fish? Large-mouth bass were studied in Florida lakes to examine factors that might affect the amount of mercury contamination. 38 lakes were used for the study. Water samples were taken from each lake and analyzed. Also, samples of fish were caught in each lake and the mercury concentration in their muscle tissue was measured. The resulting data are in http://ritsokiguess.site/datafiles/mercury.txt, separated by single spaces. The variables measured were as follows:\n\nstandardized mercury level (parts per million in 3-year-old fish)\nalkalinity of water (milligrams per litre)\ncalcium level of water (milligrams per litre)\npH of water (standard scale; see eg. this)\n\n\nRead in and display (some of) the data.\n\nSolution\nThe data values were separated by single spaces, so this one is read_delim:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/mercury.txt\"\nmercury &lt;- read_delim(my_url, \" \")\n\nRows: 38 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (4): mercury, alkalinity, calcium, ph\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmercury\n\n\n\n  \n\n\n\nExtra: I found these data in a textbook, and I couldn’t find them online anywhere, so I typed them in myself. This is how I did it.\nFirst, I entered the values as a piece of text:\n\nmercury_txt &lt;- \"\nmercury alkalinity calcium ph\n1330 2.5 2.9 4.6\n250 19.6 4.5 7.3\n450 5.2 2.8 5.4\n160 71.4 55.2 8.1\n720 26.4 9.2 5.8\n810 4.8 4.6 6.4\n710 6.6 2.7 5.4\n510 16.5 13.8 7.2\n1000 7.1 5.2 5.8\n150 83.7 66.5 8.2\n190 108.5 35.6 8.7\n1020 6.4 4.0 5.8\n450 7.5 2.0 4.4\n590 17.3 10.7 6.7\n810 7.0 6.3 6.9\n420 10.5 6.3 5.5\nO530 30.0 13.9 6.9\n310 55.4 15.9 7.3\n470 6.3 3.3 5.8\n250 67.0 58.6 7.8\n410 28.8 10.2 7.4\n160 119.1 38.4 7.9\n160 25.4 8.8 7.1\n230 106.5 90.7 6.8\n560 8.5 2.5 7.0\n890 87.6 85.5 7.5\n180 114.0 72.6 7.0\n190 97.5 45.5 6.8\n440 11.8 24.2 5.9\n160 66.5 26.0 8.3\n670 16.0 41.2 6.7\n550 5.0 23.6 6.2\n580 25.6 12.6 6.2\n980 1.2 2.1 4.3\n310 34.0 13.1 7.0\n430 15.5 5.2 6.9\n280 17.3 3.0 5.2\n250 71.8 20.5 7.9\n\"\n\nThen, I wanted to get these into a file laid out just like that, which is what writeLines does:\n\nwriteLines(mercury_txt, \"mercury.txt\")\n\nand then I uploaded the file to the course website.\n\\(\\blacksquare\\)\n\nPlot the mercury levels against each of the explanatory variables.\n\nSolution\nThe best way to do this is in one shot, using facets. This means organizing the dataframe so that there is one column of \\(y\\)-values, and also one column of \\(x\\)-values, with an additional column labelling which \\(x\\) it is. This, as you’ll recall, is exactly what pivot_longer does. To show you:\n\nmercury %&gt;% \npivot_longer(-mercury, names_to = \"xname\", values_to = \"xval\")\n\n\n\n  \n\n\n\nand now we plot mercury against xval in facets according to xname. The other thing to remember is that the explanatory variables are on different scales, so we should use scales=\"free\" to allow each facet to have its own scales:\n\nmercury %&gt;% \npivot_longer(-mercury, names_to = \"xname\", values_to = \"xval\") %&gt;% \nggplot(aes(x = xval, y = mercury)) + geom_point() + geom_smooth() + \nfacet_wrap(~xname, scales = \"free\", ncol = 2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI did one more thing, which is to note that I am going to be assessing these relationships in a moment, so I would rather have squarer plots than the tall skinny ones that come out by default (that is to say these):\n\nmercury %&gt;% \npivot_longer(-mercury, names_to = \"xname\", values_to = \"xval\") %&gt;% \nggplot(aes(x = xval, y = mercury)) + geom_point() + geom_smooth() + \nfacet_wrap(~xname, scales = \"free\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nTo make this happen, I added ncol=2, which says to arrange the facets in two columns (that is, as three cells of a \\(2\\times 2\\) grid), and that makes them come out more landscape than portrait. nrow=2 would have had the same effect.\nIf you are stuck, get three separate graphs, but note that you are making more work for yourself (that you will have to do again later).\n\\(\\blacksquare\\)\n\nDescribe any trends (or lack thereof) that you see on your graphs.\n\nSolution\nThink about “form, direction, strength” to guide you in interpreting what you see: is it a line or a curve, does it go up or down, and are the points mostly close to the trend or not? I think it makes most sense to talk about those things for the three trends one after the other:\n\nalkalinity: this is a curved trend, but downward (the rate of decrease is fast at the beginning and then levels off). There is one clear outlier, but otherwise most of the points are close to the trend.\ncalcium: this is a down-and-up curved trend, though I think most of the evidence for the “up” part is the outlier on the middle right of the graph; without that, it would probably be a levelling-off decreasing trend as for alkalinity. There seem to be more outliers on this plot, and, on top of that, the points are not that close to the trend.\nph: this is a downward trend, more or less linear, but of only moderate strength. The points can be some way from the trend, but (in contrast to the other plots) there don’t seem to be any points a long way away.\n\nIf you are going to talk about outliers, you need to be specific about where they are: describe where they are on the plot, or give approximate coordinates (you only need to be accurate enough to make it clear which point you are talking about). For example, I described one of the outliers on the calcium plot as “middle right”, or you could describe the same point as having calcium above 80 and mercury near 1000, which narrows it down enough. There is a grey area between outliers and not being close to the trend overall; if it is a lot of points, I’d call it a weaker trend (as for ph), but if it’s a few points that are a long way off, as for calcium, I’d call that outliers.\nExtra: most of the outliers are above the trends, which suggests that something to bring the high values down a bit would be helpful: that is, a transformation like log or square root. That’s coming up later.\n\\(\\blacksquare\\)\n\nConcerned by some of what you see on your plots, you consult with a fisheries scientist, who says that using the logs of alkalinity and calcium are often used in modelling this kind of data. Add columns containing the logs of these two variables to your dataframe, and run a regression predicting mercury from ph and the two new variables. Display the output from your regression.\n\nSolution\nI would create the two new variables and save them back into the original dataframe, myself:\n\nmercury %&gt;% \nmutate(log_alkalinity = log(alkalinity), log_calcium = log(calcium)) -&gt; mercury\n\nthough you could equally well create a new dataframe to hold them, as long as you remember to use the new dataframe from here on.\nThe regression has no great surprises:\n\nmercury.1 &lt;- lm(mercury ~ log_alkalinity + log_calcium + ph, data = mercury)\nsummary(mercury.1)\n\n\nCall:\nlm(formula = mercury ~ log_alkalinity + log_calcium + ph, data = mercury)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-326.02 -129.92  -27.67   71.17  581.76 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     1181.20     244.21   4.837 2.79e-05 ***\nlog_alkalinity  -221.18      58.87  -3.757 0.000646 ***\nlog_calcium       87.90      51.43   1.709 0.096520 .  \nph               -36.63      51.09  -0.717 0.478362    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 204.4 on 34 degrees of freedom\nMultiple R-squared:  0.5578,    Adjusted R-squared:  0.5188 \nF-statistic:  14.3 on 3 and 34 DF,  p-value: 3.424e-06\n\n\n\\(\\blacksquare\\)\n\nWhat might you learn from running Box-Cox here? Explain briefly, then run it (on the same variables as your regression above) and describe what the results tell you.\n\nSolution\nBox-Cox is to find out whether you need to transform the response variable, here mercury. The hint is that we have already transformed two of the explanatory variables, the ones that had some unusually large values, and so those are presumably now all right now.\nboxcox comes from the MASS package, so install and load that first.\n\nboxcox(mercury ~ log_alkalinity + log_calcium + ph, data = mercury)\n\n\n\n\nThe confidence interval for \\(\\lambda\\) goes from about \\(-0.4\\) to \\(0.5\\). The only round-number powers in there are 0 (log) and 0.5 (square root, right on the end). The \\(\\lambda\\) for the log transformation is right in the middle of the interval, so that’s what I would choose. That means that we should use log of mercury instead of mercury itself in a regression.\nExtra: I looked at the residual plots of the regression mercury.1, in the hope that they would point you towards some kind of transformation of mercury, but they really didn’t – the biggest feature was an upper-end outlier, more extreme than the one you see below, that appeared on all of them. So I didn’t have you produce those graphs.\n\\(\\blacksquare\\)\n\nUsing the results of the Box-Cox procedure and your previous work in the question, what seems to be the most appropriate regression now? Fit it, and display the results.\n\nSolution\nThis says to predict log-mercury (Box-Cox) from log-alkalinity, log-calcium (the fisheries scientist) and pH:\n\nmercury.2 &lt;- lm(log(mercury) ~ log_alkalinity + log_calcium + ph, data = mercury)\nsummary(mercury.2)\n\n\nCall:\nlm(formula = log(mercury) ~ log_alkalinity + log_calcium + ph, \n    data = mercury)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75244 -0.30191 -0.00783  0.23852  1.22932 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     7.55983    0.48981  15.434  &lt; 2e-16 ***\nlog_alkalinity -0.45880    0.11807  -3.886 0.000449 ***\nlog_calcium     0.14702    0.10315   1.425 0.163185    \nph             -0.07998    0.10248  -0.780 0.440527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4099 on 34 degrees of freedom\nMultiple R-squared:  0.6069,    Adjusted R-squared:  0.5723 \nF-statistic:  17.5 on 3 and 34 DF,  p-value: 4.808e-07\n\n\nThere’s no need to define a new column containing log-mercury in the dataframe, since you can define it in the lm. (Note for me: do I need to define new columns anywhere?)\n\\(\\blacksquare\\)\n\nObtain all the standard residual plots (the ones we have seen in this course) for this model. Describe any problems you see.\n\nSolution\nresiduals against fitted\n\nggplot(mercury.2, aes(x=.fitted, y=.resid)) + geom_point()\n\n\n\n\nnormal quantile plot of residuals\n\nggplot(mercury.2, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nagainst the explanatory variables. This uses the ideas of augment (from broom) and pivoting longer:\n\nmercury.2 %&gt;% augment(mercury) %&gt;% \npivot_longer(ph:log_calcium, names_to = \"xname\", values_to = \"xval\") %&gt;% \nggplot(aes(x = xval, y = .resid)) + geom_point() +\nfacet_wrap(~xname, scales = \"free\", ncol = 2)\n\n\n\n\nI think the only troublesome feature on there is the large positive residual that appears at the top of all the plots. Other than that, I see nothing troubling.\nYou might, if you look a bit longer (but remember, apophenia!) see a tiny up and down on the plot with log-alkalinity, and maybe a small downward trend on the plots with the other two explanatory variables, but I would need a lot of convincing to say that these are more than chance. You are looking for obvious trouble, and I really don’t think there’s any sign of that here.\nExtra: you sometimes see a downward trend on residual plots that have an outlier on them. This is because if you have an outlier, it can change the slope disproportionately from what it ought to be, and then most of the observations at one end will be underestimated and most of the observations at the other end will be overestimated.\nExtra 2: which one was that outlier anyway?\n\nmercury.2 %&gt;% augment(mercury) %&gt;% \nfilter(.resid &gt; 1)\n\n\n\n  \n\n\n\nHow do these compare to the other data values?\n\nsummary(mercury)\n\n    mercury         alkalinity        calcium             ph       \n Min.   : 150.0   Min.   :  1.20   Min.   : 2.000   Min.   :4.300  \n 1st Qu.: 250.0   1st Qu.:  7.20   1st Qu.: 4.525   1st Qu.:5.800  \n Median : 445.0   Median : 18.45   Median :11.650   Median :6.850  \n Mean   : 488.4   Mean   : 37.15   Mean   :22.361   Mean   :6.634  \n 3rd Qu.: 650.0   3rd Qu.: 66.88   3rd Qu.:33.200   3rd Qu.:7.300  \n Max.   :1330.0   Max.   :119.10   Max.   :90.700   Max.   :8.700  \n log_alkalinity    log_calcium    \n Min.   :0.1823   Min.   :0.6931  \n 1st Qu.:1.9738   1st Qu.:1.5096  \n Median :2.9131   Median :2.4520  \n Mean   :3.0193   Mean   :2.4801  \n 3rd Qu.:4.2028   3rd Qu.:3.4938  \n Max.   :4.7800   Max.   :4.5076  \n\n\nThe variable values are all high (even the pH, a modest-looking 7.5, is above Q3).\nRemember that the fitted value is of log-mercury, so we have to anti-log it to understand it (anti-log is “exp” since these are “natural” or base-\\(e\\) logs):\n\nexp(5.561902)\n\n[1] 260.3175\n\n\nThis was a much higher mercury value than expected, given the other variables.\n\\(\\blacksquare\\)\nnote to author: the datafile for _chemical.Rmd seems no longer to be readable."
  },
  {
    "objectID": "multiple-regression.html#footnotes",
    "href": "multiple-regression.html#footnotes",
    "title": "17  Multiple regression",
    "section": "",
    "text": "If there are more than one, they should be separated by plus signs as in lm. Each facet then has as many labels as variables. I haven’t actually done this myself, but from looking at examples, I think this is the way it works.↩︎\nIf you don’t give it, you get the five-number summary.↩︎\nThis is because we used \\(\\alpha = 0.10\\). step tends to keep explanatory variables that you might consider marginal because it uses AIC (see below) rather than P-values directly.↩︎\nThis is “F” in quotes, meaning \\(F\\)-test, not “F” without quotes, which means FALSE.↩︎\nAdjusted R-squareds are easier to compare in this context, since you don’t have to make a judgement about whether it has changed substantially, whatever you think substantially means.↩︎\nThe residuals have to be the ones from a regression not including the \\(x\\)-variable you’re testing.↩︎\nOr not take it out in the first place.↩︎"
  },
  {
    "objectID": "regression-categorical.html#crickets-revisited",
    "href": "regression-categorical.html#crickets-revisited",
    "title": "18  Regression with categorical variables",
    "section": "18.1 Crickets revisited",
    "text": "18.1 Crickets revisited\nThis is a continuation of the crickets problem that you may have seen before (minus the data tidying).\nMale tree crickets produce “mating songs” by rubbing their wings together to produce a chirping sound. It is hypothesized that female tree crickets identify males of the correct species by how fast (in chirps per second) the male’s mating song is. This is called the “pulse rate”. Some data for two species of crickets are in link as a CSV file. The columns are species (text), temperature, and pulse rate (numbers). This is the tidied version of the data set that the previous version of this question had you create. The research question is whether males of the different species have different average pulse rates. It is also of interest to see whether temperature has an effect, and if so, what.\n\nRead the data into R and display what you have.\nDo a two-sample \\(t\\)-test to see whether the mean pulse rates differ between species. What do you conclude?\nCan you do that two-sample \\(t\\)-test as a regression?\nThe analysis in the last part did not use temperature, however. Is it possible that temperature also has an effect? To assess this, draw a scatterplot of pulse rate against temperature, with the points distinguished, somehow, by the species they are from.1\nWhat does the plot tell you that the \\(t\\)-test doesn’t? How would you describe differences in pulse rates between species now?\nFit a regression predicting pulse rate from species and temperature. Compare the P-value for species in this regression to the one from the \\(t\\)-test. What does that tell you?\nMake suitable residual plots for the regression pulse.1."
  },
  {
    "objectID": "regression-categorical.html#pulse-rates-and-marching",
    "href": "regression-categorical.html#pulse-rates-and-marching",
    "title": "18  Regression with categorical variables",
    "section": "18.2 Pulse rates and marching",
    "text": "18.2 Pulse rates and marching\nForty students, some male and some female, measured their resting pulse rates. Then they marched in place for one minute and measured their pulse rate again. Our aim is to use regression to predict the pulse rate after the marching from the pulse rate before, and to see whether that is different for males and females. The data set is in http://ritsokiguess.site/datafiles/pulsemarch.csv.\n\nRead in and display (some of) the data.\nMake a suitable graph using all three variables, adding appropriate regression line(s) to the plot.\nExplain briefly but carefully how any effects of pulse rate before on pulse rate after, and also of sex on pulse rate after, show up on your plot. (If either explanatory variable has no effect, explain how you know.)\nRun a regression predicting pulse rate after from the other two variables. Display the output.\nLooking at your graph, does the significance (or lack of) of each of your two explanatory variables surprise you? Explain briefly.\nWhat does the numerical value of the Estimate for Sex in your regression output mean, in the context of this data set? Explain briefly.\n\nMy solutions follow:"
  },
  {
    "objectID": "regression-categorical.html#crickets-revisited-1",
    "href": "regression-categorical.html#crickets-revisited-1",
    "title": "18  Regression with categorical variables",
    "section": "18.3 Crickets revisited",
    "text": "18.3 Crickets revisited\nThis is a continuation of the crickets problem that you may have seen before (minus the data tidying).\nMale tree crickets produce “mating songs” by rubbing their wings together to produce a chirping sound. It is hypothesized that female tree crickets identify males of the correct species by how fast (in chirps per second) the male’s mating song is. This is called the “pulse rate”. Some data for two species of crickets are in link as a CSV file. The columns are species (text), temperature, and pulse rate (numbers). This is the tidied version of the data set that the previous version of this question had you create. The research question is whether males of the different species have different average pulse rates. It is also of interest to see whether temperature has an effect, and if so, what.\n\nRead the data into R and display what you have.\n\nSolution\nNothing terribly surprising here:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/crickets2.csv\"\ncrickets &lt;- read_csv(my_url)\n\nRows: 31 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): species\ndbl (2): temperature, pulse_rate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncrickets\n\n\n\n  \n\n\n\n31 crickets, which is what I remember. What species are there?\n\ncrickets %&gt;% count(species)\n\n\n\n  \n\n\n\nThat looks good. We proceed.\n\\(\\blacksquare\\)\n\nDo a two-sample \\(t\\)-test to see whether the mean pulse rates differ between species. What do you conclude?\n\nSolution\nDrag your mind way back to this:\n\nt.test(pulse_rate ~ species, data = crickets)\n\n\n    Welch Two Sample t-test\n\ndata:  pulse_rate by species\nt = 5.2236, df = 28.719, p-value = 1.401e-05\nalternative hypothesis: true difference in means between group exclamationis and group niveus is not equal to 0\n95 percent confidence interval:\n 14.08583 32.22677\nsample estimates:\nmean in group exclamationis        mean in group niveus \n                   85.58571                    62.42941 \n\n\nThere is strong evidence of a difference in means (a P-value around 0.00001), and the confidence interval says that the mean chirp rate is higher for exclamationis. That is, not just for the crickets that were observed here, but for all crickets of these two species.\n\\(\\blacksquare\\)\n\nCan you do that two-sample \\(t\\)-test as a regression?\n\nSolution\nHang onto the “pulse rate depends on species” idea and try that in lm:\n\npulse.0 &lt;- lm(pulse_rate ~ species, data = crickets)\nsummary(pulse.0)\n\n\nCall:\nlm(formula = pulse_rate ~ species, data = crickets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.486  -9.458  -1.729  13.342  22.271 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     85.586      3.316  25.807  &lt; 2e-16 ***\nspeciesniveus  -23.156      4.478  -5.171 1.58e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.41 on 29 degrees of freedom\nMultiple R-squared:  0.4797,    Adjusted R-squared:  0.4617 \nF-statistic: 26.74 on 1 and 29 DF,  p-value: 1.579e-05\n\n\nI had to use “model 0” for this since I already have a pulse.1 below and I didn’t want to go down and renumber everything.\nLook along the speciesniveus line. Ignoring the fact that it is negative, the \\(t\\)-statistic is almost the same as before (5.17 vs.\n5.22) and so is the P-value (\\(1.4 \\times 10^{-5}\\) vs. \\(1.6 \\times 10^{-5}\\)).\nWhy aren’t they exactly the same? Regression is assuming equal variances everywhere (that is, within the two species), and before, we did the Welch-Satterthwaite test that does not assume equal variances. What if we do the pooled \\(t\\)-test instead?\n\nt.test(pulse_rate ~ species, data = crickets, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  pulse_rate by species\nt = 5.1706, df = 29, p-value = 1.579e-05\nalternative hypothesis: true difference in means between group exclamationis and group niveus is not equal to 0\n95 percent confidence interval:\n 13.99690 32.31571\nsample estimates:\nmean in group exclamationis        mean in group niveus \n                   85.58571                    62.42941 \n\n\nNow the regression and the \\(t\\)-test do give exactly the same answers. We’ll think about that equal-spreads assumption again later.\n\\(\\blacksquare\\)\n\nThe analysis in the last part did not use temperature, however. Is it possible that temperature also has an effect? To assess this, draw a scatterplot of pulse rate against temperature, with the points distinguished, somehow, by the species they are from.2\n\nSolution\nOne of the wonderful things about ggplot is that doing the obvious thing works:\n\nggplot(crickets, aes(x = temperature, y = pulse_rate, colour = species)) +\n  geom_point()\n\n\n\n\n\\(\\blacksquare\\)\n\nWhat does the plot tell you that the \\(t\\)-test doesn’t? How would you describe differences in pulse rates between species now?\n\nSolution\nThe plot tells you that (for both species) as temperature goes up, pulse rate goes up as well. Allowing for that, the difference in pulse rates between the two species is even clearer than it was before. To see an example, pick a temperature, and note that the mean pulse rate at that temperature seems to be at least 10 higher for exclamationis, with a high degree of consistency. The \\(t\\)-test mixed up all the pulse rates at all the different temperatures. Even though the conclusion was clear enough, it could be clearer if we incorporated temperature into the analysis. There was also a potential source of unfairness in that the exclamationis crickets tended to be observed at higher temperatures than niveus crickets; since pulse rates increase with temperature, the apparent difference in pulse rates between the species might have been explainable by one species being observed mainly in higher temperatures. This was utterly invisible to us when we did the \\(t\\)-test, but it shows the importance of accounting for all the relevant variables when you do your analysis.3 If the species had been observed at opposite temperatures, we might have concluded4 that niveus have the higher pulse rates on average. I come back to this later when I discuss the confidence interval for species difference that comes out of the regression model with temperature.\n\\(\\blacksquare\\)\n\nFit a regression predicting pulse rate from species and temperature. Compare the P-value for species in this regression to the one from the \\(t\\)-test. What does that tell you?\n\nSolution\nThis is actually a so-called “analysis of covariance model”, which properly belongs in D29, but it’s really just a regression:\n\npulse.1 &lt;- lm(pulse_rate ~ species + temperature, data = crickets)\nsummary(pulse.1)\n\n\nCall:\nlm(formula = pulse_rate ~ species + temperature, data = crickets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0128 -1.1296 -0.3912  0.9650  3.7800 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -7.21091    2.55094  -2.827  0.00858 ** \nspeciesniveus -10.06529    0.73526 -13.689 6.27e-14 ***\ntemperature     3.60275    0.09729  37.032  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.786 on 28 degrees of freedom\nMultiple R-squared:  0.9896,    Adjusted R-squared:  0.9888 \nF-statistic:  1331 on 2 and 28 DF,  p-value: &lt; 2.2e-16\n\n\nThe P-value for species is now \\(6.27\\times 10^{-14}\\) or 0.00000000000006, which is even less than the P-value of 0.00001 that came out of the \\(t\\)-test. That is to say, when you know temperature, you can be even more sure of your conclusion that there is a difference between the species.\nThe R-squared for this regression is almost 99%, which says that if you know both temperature and species, you can predict the pulse rate almost exactly.\nIn the regression output, the slope for species is about \\(-10\\). It is labelled speciesniveus. Since species is categorical, lm uses the first category, exclamationis, as the baseline and expresses each other species relative to that. Since the slope is about \\(-10\\), it says that at any given temperature, the mean pulse rate for niveus is about 10 less than for exclamationis. This is pretty much what the scatterplot told us.\nWe can go a little further here:\n\nconfint(pulse.1)\n\n                   2.5 %    97.5 %\n(Intercept)   -12.436265 -1.985547\nspeciesniveus -11.571408 -8.559175\ntemperature     3.403467  3.802038\n\n\nThe second line says that the pulse rate for niveus is between about 8.5 and 11.5 less than for exclamationis, at any given temperature (comparing the two species at the same temperature as each other, but that temperature could be anything). This is a lot shorter than the CI that came out of the \\(t\\)-test, that went from 14 to 32. This is because we are now accounting for temperature, which also makes a difference. (In the \\(t\\)-test, the temperatures were all mixed up). What we also see is that the \\(t\\)-interval is shifted up compared to the one from the regression. This is because the \\(t\\)-interval conflates5 two things: the exclamationis crickets do have a higher pulse rate, but they were also observed at higher temperatures, which makes it look as if their pulse rates are more higher6 than they really are, when you account for temperature.\nThis particular model constrains the slope with temperature to be the same for both species (just the intercepts differ). If you want to allow the slopes to differ between species, you add an interaction between temperature and species:\n\npulse.2 &lt;- lm(pulse_rate ~ species * temperature, data = crickets)\nsummary(pulse.2)\n\n\nCall:\nlm(formula = pulse_rate ~ species * temperature, data = crickets)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7031 -1.3417 -0.1235  0.8100  3.6330 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -11.0408     4.1515  -2.659    0.013 *  \nspeciesniveus              -4.3484     4.9617  -0.876    0.389    \ntemperature                 3.7514     0.1601  23.429   &lt;2e-16 ***\nspeciesniveus:temperature  -0.2340     0.2009  -1.165    0.254    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.775 on 27 degrees of freedom\nMultiple R-squared:  0.9901,    Adjusted R-squared:  0.989 \nF-statistic: 898.9 on 3 and 27 DF,  p-value: &lt; 2.2e-16\n\n\nTo see whether adding the interaction term added anything to the prediction,7 compare the model with and without using anova:\n\nanova(pulse.1, pulse.2)\n\n\n\n  \n\n\n\nThere’s no significant improvement by adding the interaction, so there’s no evidence that having different slopes for each species is necessary. This is the same interpretation as any anova for comparing two regressions: the two models are not significantly different in fit, so go with the simpler one, that is, the one without the interaction.\nNote that anova gave the same P-value as did the \\(t\\)-test for the slope coefficient for the interaction in summary, 0.254 in both cases. This is because there were only two species and therefore only one slope coefficient was required to distinguish them. If there had been three species, we would have had to look at the anova output to hunt for a difference among species, since there would have been two slope coefficients, each with its own P-value.8\nIf you haven’t seen interactions before, don’t worry about this. The idea behind it is that we are testing whether we needed lines with different slopes and we concluded that we don’t. Don’t worry so much about the mechanism behind pulse.2; just worry about how it somehow provides a way of modelling two different slopes, one for each species, which we can then test to see whether it helps.\nThe upshot is that we do not need different slopes; the model pulse.1 with the same slope for each species describes what is going on.\nggplot makes it almost laughably easy to add regression lines for each species to our plot, thus:\n\nggplot(crickets, aes(x = temperature, y = pulse_rate, colour = species)) +\n  geom_point() + geom_smooth(method = \"lm\", se = F)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe lines are almost exactly parallel, so having the same slope for each species makes perfect sense.\n\\(\\blacksquare\\)\n\nMake suitable residual plots for the regression pulse.1.\n\nSolution\nFirst, the plot of residuals against fitted values (after all, it is a regression):\n\nggplot(pulse.1, aes(x = .fitted, y = .resid)) + geom_point()\n\n\n\n\nThis looks nice and random.\nNow, we plot the residuals against the explanatory variables. There are two, temperature and species, but the latter is categorical. We’ll have some extra issues around species, but before we get to that, we have to remember that the data and the output from the regression are in different places when we plot them. There are different ways to get around that. Perhaps the simplest is to use pulse.1 as our “default” data frame and then get temperature from the right place:\n\nggplot(pulse.1, aes(x = crickets$temperature, y = .resid)) + geom_point()\n\n\n\n\nI don’t see anything untoward there.\nSpecies. We want to compare the residuals for the two species, which is categorical. Since the residuals are quantitative, this suggests a boxplot. Remembering to get species from the right place again, that goes like this:\n\nggplot(pulse.1, aes(x = crickets$species, y = .resid)) + geom_boxplot()\n\n\n\n\nFor the residuals, the median should be zero within each group, and the two groups should be approximately normal with mean 0 and about the same spread. Same spread looks OK, since the boxes are almost exactly the same height, but the normality is not quite there, since both distributions are a little bit skewed to the right. That would also explain why the median residual in each group is a little bit less than zero, because the mathematics requires the overall mean residual to be zero, and the right-skewness would make the mean higher than the median.\nIs that non-normality really problematic? Well, I could look at the normal quantile plot of all the residuals together:\n\nggplot(pulse.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n\n\n\n\nThere’s a little weirdness at the top, and a tiny indication of a curve (that would suggest a little right-skewedness), but not really much to worry about. If that third-highest residual were a bit lower (say, 3 rather than 3.5) and maybe if the lowest residual was a bit lower, I don’t think we’d have anything to complain about at all.\nSo, I’m not worried.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "regression-categorical.html#pulse-rates-and-marching-1",
    "href": "regression-categorical.html#pulse-rates-and-marching-1",
    "title": "18  Regression with categorical variables",
    "section": "18.4 Pulse rates and marching",
    "text": "18.4 Pulse rates and marching\nForty students, some male and some female, measured their resting pulse rates. Then they marched in place for one minute and measured their pulse rate again. Our aim is to use regression to predict the pulse rate after the marching from the pulse rate before, and to see whether that is different for males and females. The data set is in http://ritsokiguess.site/datafiles/pulsemarch.csv.\n\nRead in and display (some of) the data.\n\nSolution\nAs usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/pulsemarch.csv\"\nmarch &lt;- read_csv(my_url)\n\nRows: 40 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Sex\ndbl (2): Before, After\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmarch\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake a suitable graph using all three variables, adding appropriate regression line(s) to the plot.\n\nSolution\nTwo quantitative and one categorical says scatterplot, with colour distinguishing the categories (two here). geom_smooth adds a regression line to the plot for each Sex, which is what we want. I used se=F to remove the grey envelopes from the plot (because I thought they confused the issue):\n\nggplot(march, aes(x=Before, y=After, colour=Sex)) + geom_point() + \ngeom_smooth(method = \"lm\", se=F)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHaving only one regression line is not so good because that only shows that pulse rate after goes up with pulse rate before, but not if and how the sexes differ.\nExtra: I took a shortcut of the process here, to make the question shorter. In practice, what you’d do is to put smooth trends on the plot first:\n\nggplot(march, aes(x=Before, y=After, colour=Sex)) + geom_point() + \ngeom_smooth(se=F)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe red trend looks curved, but if you look carefully, pretty much all of9 the evidence for the curve comes from that point on the right with pulse rate before over 90 and pulse rate after around 100. If it weren’t for that, the red trend would be pretty close to linear. As you’ll recall, a decision about the kind of trend based on one observation is a pretty flimsy decision.\nThen, having seen that the trends are not obviously curved, you would draw the plot with the straight lines. (Fitting separate curves is a whole different story that I didn’t want to get into.)\n\\(\\blacksquare\\)\n\nExplain briefly but carefully how any effects of pulse rate before on pulse rate after, and also of sex on pulse rate after, show up on your plot. (If either explanatory variable has no effect, explain how you know.)\n\nSolution\nThere is an upward trend, so if the pulse rate before is higher, so is the pulse rate after. This is true for both males and females. (Or, holding Sex fixed, that is, comparing two people of the same sex.)\nThe red line is always above the blue line, so at any given Before pulse rate, the After pulse rate for a female is predicted to be higher than that for a male.\nNote that you have to be careful: when talking about the effect of each explanatory variable, you have to hold the other one constant (in general, hold all the other ones constant). If you can word that in a way that makes sense in the context of the data you are looking at, so much the better.\n\\(\\blacksquare\\)\n\nRun a regression predicting pulse rate after from the other two variables. Display the output.\n\nSolution\nThus:\n\nmarch.1 &lt;- lm(After~Before+Sex, data=march)\nsummary(march.1)\n\n\nCall:\nlm(formula = After ~ Before + Sex, data = march)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.8653  -4.6319  -0.4271   3.3856  16.0047 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.8003     7.9217   2.499   0.0170 *  \nBefore        0.9064     0.1127   8.046  1.2e-09 ***\nSexMale      -4.8191     2.2358  -2.155   0.0377 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.918 on 37 degrees of freedom\nMultiple R-squared:  0.6468,    Adjusted R-squared:  0.6277 \nF-statistic: 33.87 on 2 and 37 DF,  p-value: 4.355e-09\n\n\nExtra: if you want “all the other variables except the response” as explanatory, there is also this shortcut:\n\nmarch.1a &lt;- lm(After~., data=march)\nsummary(march.1a)\n\n\nCall:\nlm(formula = After ~ ., data = march)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.8653  -4.6319  -0.4271   3.3856  16.0047 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.8003     7.9217   2.499   0.0170 *  \nSexMale      -4.8191     2.2358  -2.155   0.0377 *  \nBefore        0.9064     0.1127   8.046  1.2e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.918 on 37 degrees of freedom\nMultiple R-squared:  0.6468,    Adjusted R-squared:  0.6277 \nF-statistic: 33.87 on 2 and 37 DF,  p-value: 4.355e-09\n\n\n\\(\\blacksquare\\)\n\nLooking at your graph, does the significance (or lack of) of each of your two explanatory variables surprise you? Explain briefly.\n\nSolution\nWe noted a clear upward trend before, for both sexes, so there is no surprise that the Before pulse rate is significant.\nThe red dots (females) on the graph seemed to be on average above the blue ones (males), at least for similar before pulse rates. (This is not completely convincing, so you are entitled to be surprised also; note that the P-value, while significant, is not that small).\nExtra: comparing the lines is less convincing, because how do we get a feel for whether these lines are more different than chance? One deceiving way to (fail to) get a feel for this is to re-draw our plot but with the grey envelopes:\n\nggplot(march, aes(x=Before, y=After, colour=Sex)) + geom_point() + \ngeom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe grey envelopes overlap substantially, which would make you think the lines are not significantly different. But, this is not the right way to compare the lines. It is a similar problem to that of comparing two means (that we would normally do with a two-sample test of some kind) by working out the two one-sample confidence intervals, and seeing whether they overlap. If they do not, then you can be sure that the means differ, but if they do overlap, then you cannot say anything about whether the means differ: maybe they do, maybe they don’t. This one is analogous; the grey envelopes overlap, so maybe the lines differ, maybe they don’t. Looking at the grey envelopes in this case gives you no insight about whether males and females differ.\nHere is a short discussion of this issue (in the context of comparing two means).\n\\(\\blacksquare\\)\n\nWhat does the numerical value of the Estimate for Sex in your regression output mean, in the context of this data set? Explain briefly.\n\nSolution\nThe estimate is labelled SexMale, and its value is \\(-4.8\\).\nSex is a categorical variable, so it has a baseline category, which is the first one, Female. The Estimate SexMale shows how males compare to the baseline (females), at a fixed Before pulse rate. This value is \\(-4.8\\), so, at any Before pulse rate, the male After pulse rate is predicted to be 4.8 less than the female one.\nI think you have to mention the value \\(-4.8\\), so that you can talk intelligently about what it means for these data.\nExtra: the implication of our model is that the predicted difference is the same all the way along. You might have your doubts about that; you might think the lines are closer together on the left and further apart on the right. Another way to think about this is whether the lines are parallel: that is, whether they have the same slope. I’m inclined to think they do; the data points are fairly scattered, and I think the slopes would have to be a lot more different to be significantly different. But you don’t have to take my word for it: we can test this by adding an interaction term to the model. You might have seen this in ANOVA, where you are assessing the effect of one factor at different levels of the other. This is more or less the same idea. Note the * rather than the + in the model formula:10\n\nmarch.2 &lt;- lm(After~Before*Sex, data=march)\nsummary(march.2)\n\n\nCall:\nlm(formula = After ~ Before * Sex, data = march)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2831  -4.3638  -0.3965   3.4077  16.6188 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     13.4390    11.1416   1.206    0.236    \nBefore           0.9991     0.1604   6.230 3.43e-07 ***\nSexMale          7.9470    15.8095   0.503    0.618    \nBefore:SexMale  -0.1846     0.2263  -0.816    0.420    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.949 on 36 degrees of freedom\nMultiple R-squared:  0.6532,    Adjusted R-squared:  0.6243 \nF-statistic:  22.6 on 3 and 36 DF,  p-value: 2.11e-08\n\n\nThe Before:SexMale term tests the interaction, and you see it is nowhere near significant. There is no justification for having lines with different slopes for males and females.\nWe were lucky here in that Sex has only two levels, so looking at the summary gave us what we wanted. If we had had an Other category for Sex, for people who don’t identify with either male or female, there would be two Estimates in the summary table, one comparing Male with Female, and one comparing Other with Female.11 But maybe the significant difference is Male vs. Other, and we would never see it.\nTo look for any effect of a categorical variable, the right way is to use drop1, to see which variables, including categorical ones, can be removed as a whole, thus:12\n\ndrop1(march.2, test=\"F\")\n\n\n\n  \n\n\n\nThis only lists things that can be removed, in this case the interaction. It is not significant, so out it comes (resulting in our model march.1):\n\ndrop1(march.1, test=\"F\")\n\n\n\n  \n\n\n\nBoth remaining explanatory variables are significant, so we need to keep them both.\nOur categorical explanatory variable has only two levels, so drop1 and summary give the exact same P-values.\nExtra 2:\nLet’s go back and look at our data set again:\n\nmarch\n\n\n\n  \n\n\n\nYou might have been thinking, when we started, that these are before and after measurements on the same people, and so what we have here is matched pairs. So we do, but it’s not the kind of matched pairs we are accustomed to. Let’s begin by taking differences, getting rid of the Before and After columns, and see what we have left:\n\nmarch %&gt;% \nmutate(difference=After-Before) %&gt;% \nselect(-After, -Before) -&gt; march_paired\nmarch_paired\n\n\n\n  \n\n\n\nIn matched pairs, we are used to having one column of differences, and we test that for a mean or median of zero, to express no difference between before and after (or whatever it was). But now, we have an extra column Sex. We are not interested here in whether the differences average out to zero;13 we care more about whether the differences differ (!) between males and females. That is to say, we have a two-sample matched pairs test!\nAt this point, your head ought to be hurting!\nHowever, at this point what we are saying is that if you believe that the difference is a good way to summarize the effect of the exercise, then we have one measurement for each person, independent because different people’s measurements will be independent. It doesn’t matter where they came from. We have measurements on two groups, so some kind of two-sample test will be good. Which kind? Let’s look at a graph, a good one now being a boxplot:\n\nggplot(march_paired, aes(x=Sex, y=difference)) + geom_boxplot()\n\n\n\n\nOr, if you like, a facetted normal quantile plot:\n\nggplot(march_paired, aes(sample=difference)) +\nstat_qq() + stat_qq_line() +\nfacet_wrap(~Sex)\n\n\n\n\nIt seems to me that these are normal enough for a \\(t\\)-test, given the sample sizes (feel free to disagree):\n\nmarch_paired %&gt;% count(Sex)\n\n\n\n  \n\n\n\nThe spreads look a bit different, so I think I would prefer the Welch test here:\n\nt.test(difference~Sex, data=march_paired)\n\n\n    Welch Two Sample t-test\n\ndata:  difference by Sex\nt = 2.0307, df = 23.296, p-value = 0.05386\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -0.08848652  9.92181986\nsample estimates:\nmean in group Female   mean in group Male \n           13.375000             8.458333 \n\n\nThis time, there is not quite a significant difference between males and females. (The P-value is just the other side of 0.05.) Though the conclusion is different, the P-values are fairly similar.\nWhich test is better? I think treating it as matched pairs is assuming that the differences after minus before are the things to be looking at. This assumes that the after measurements are the before measurement plus a something that depends on treatment, but not on the before measurement. This would fail, for example, if all the after measurements are two times the before ones (so that the difference would be bigger if the before score was bigger). The regression approach is more flexible, because any linear relationship is taken care of. A matched-pairs model of this kind is a special case of the regression model but with the slope set to be 1. In our regression, the slope is less than 1, but not by much.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "regression-categorical.html#footnotes",
    "href": "regression-categorical.html#footnotes",
    "title": "18  Regression with categorical variables",
    "section": "",
    "text": "This was the actual reason I thought of this question originally: I wanted you to do this.↩︎\nThis was the actual reason I thought of this question originally: I wanted you to do this.↩︎\nAnd it shows the value of looking at relevant plots.↩︎\nMistakenly.↩︎\nMixes up.↩︎\nThis is actually grammatically correct.↩︎\nThough it’s hard to imagine being able to improve on an R-squared of 99%.↩︎\nThis wouldn’t have told us about the overall effect of species.↩︎\nMy mind just jumped to a former German soccer player by the name of Klaus Allofs.↩︎\nTo be precise, the * means “the main effects and the interaction together”; if you want to talk about just the interaction term, you denote it by :; note the Before:SexMale term in the summary table.↩︎\nFemale is the baseline, so everything gets compared with that, whether you like it or not.↩︎\nThe test piece says to do an \\(F\\)-test, which is different from without the quotes, which would mean not to do any tests, F without quotes meaning FALSE.↩︎\nI think it’s a given that pulse rates will be higher after exercise than before.↩︎"
  },
  {
    "objectID": "dates-and-times.html#growth-of-mizuna-lettuce-seeds",
    "href": "dates-and-times.html#growth-of-mizuna-lettuce-seeds",
    "title": "19  Dates and times",
    "section": "19.1 Growth of Mizuna lettuce seeds",
    "text": "19.1 Growth of Mizuna lettuce seeds\nIn 2010, a group of students planted some Mizuna lettuce seeds, and recorded how they grew. The data were saved in an Excel spreadsheet, which is at link. The columns are: the date, the height (in cm) of (I presume) the tallest plant, the amount of water added since the previous date (ml), the temperature in the container where the seedlings were growing, and any additional notes that the students made (edited for length by me). The top line of the data file is variable names.\n\nRead the spreadsheet data.\nMake a plot of height against your dates, with the points joined by lines.\nLabel each point on the plot with the amount of water added up to that point."
  },
  {
    "objectID": "dates-and-times.html#types-of-childbirth",
    "href": "dates-and-times.html#types-of-childbirth",
    "title": "19  Dates and times",
    "section": "19.2 Types of childbirth",
    "text": "19.2 Types of childbirth\nChildbirths can be of two types: a “vaginal” birth in which the child is born through the mother’s vagina in the normal fashion, and a “cesarean section” where a surgeon cuts through the wall of the mother’s abdomen, and the baby is delivered through the incision. Cesarean births are used when there are difficulties in pregnancy or during childbirth that would make a vaginal birth too risky. A hospital kept track of the number of vaginal and Cesarean births for the twelve months of 2012. Of interest is whether the Cesarean rate (the ratio of Cesarean births to all births) was increasing, decreasing or remaining stable over that time. The data may be found at link. The columns are the names of the months (in 2012), the number of cesarean births and the number of vaginal births. (The data are not real, but are typical of the kind of thing you would observe.)\n\nRead the data into R and display your data frame.\nCreate a column of actual dates and also a column of cesarean rates, as defined above. Store your new data frame in a variable and display it. For the dates, assume that each date is of the 1st of the month that it belongs to.\nPlot the cesarean rate against time, with a smooth trend. Do you see an upward trend, a downward trend, no trend, or something else?\nTry to summarize the trend you just found with a correlation. What goes wrong? How can you fix it?"
  },
  {
    "objectID": "dates-and-times.html#wolves-and-caribou",
    "href": "dates-and-times.html#wolves-and-caribou",
    "title": "19  Dates and times",
    "section": "19.3 Wolves and caribou",
    "text": "19.3 Wolves and caribou\nIn Denali National Park, Alaska, the size of the wolf population depends on the size of the caribou population (since wolves hunt and kill caribou). This is a large national park, so caribou are found in very large herds, so big, in fact, that the well-being of the entire herd is not threatened by wolf attacks.1 Can the size of the caribou population be used to predict the size of the wolf population? The data can be found at link. The columns are: the date of the survey,2 the name of the park employee in charge of the survey, the caribou population (in hundreds) and the wolf population (actual count).3\n\nTake a look at the data file. How would you describe its format? Read it into R, and check that you got something sensible.\nCreate a new data frame where the column labelled date is now a genuine R Date, using something from lubridate.\nCreate new columns containing the days of the week and the month names for the dates.\nEnough playing around with dates. Make a scatterplot of caribou population (explanatory) against wolf population (response). Do you see any relationship?\nOn your plot from the previous part, label each point with the year it belongs to. You can do this in two steps: first make a new column containing just the years, and then use it as labels for the points on the plot.\nMake a plot of caribou population against time (this is done the obvious way). What seems to be happening to the caribou population over time?\nThe caribou and wolf populations over time are really “time series”. See if you can make a plot of both the caribou and wolf populations against time. You can make two \\(y\\)-axes, one for caribou and one for wolf; this will probably require some research on your part to figure out."
  },
  {
    "objectID": "dates-and-times.html#dealing-with-dates-in-the-worcester-heart-attack-study",
    "href": "dates-and-times.html#dealing-with-dates-in-the-worcester-heart-attack-study",
    "title": "19  Dates and times",
    "section": "19.4 Dealing with dates in the Worcester Heart Attack study",
    "text": "19.4 Dealing with dates in the Worcester Heart Attack study\nThe Worcester Heart Attack Study is an ongoing study of heart attacks in the Worcester, MA area. The main purpose of the study is to investigate changes over time in incidence and death rates, and also the use of different treatment approaches. We will be mainly using this data set to investigate data handling and dealing with dates. The data can be found at link.\n\nRead the data into R. The reading-in part is straightforward, but check what type of thing each column is. Is that what it should be?\nThe date columns should be R dates. They are not year-month-day, so converting them via as.Date (which is what read_delim tries to do) will not work. Load the lubridate package, and create new columns in your data frame that are properly dates. Save your data frame, and list it to demonstrate that it worked.\nCreate three new variables diff1, diff2, diff3 that are the numbers of days between each of your dates, and save the data frame in which they have been created. Verify that at least some of them are the same as los and lenfol.\nConstruct side-by-side boxplots of the length of followup by each followup status. You’ll need to make sure that the followup status, as it gets fed into ggplot, is a factor, or, at least, not the number that it is now."
  },
  {
    "objectID": "dates-and-times.html#going-to-sleep",
    "href": "dates-and-times.html#going-to-sleep",
    "title": "19  Dates and times",
    "section": "19.5 Going to sleep",
    "text": "19.5 Going to sleep\nA student keeps track of what time they go to bed and what time they get up in the morning. They also have an app on their phone that measures the number of hours they were asleep during that time. The data for one week are in http://ritsokiguess.site/datafiles/sleeping.csv, in the 24-hour clock.\n\nRead in and display the data. What type of things are each of your columns?\nWork out the fractional number of hours that the student was in bed each of these nights. (They may not have been asleep this whole time.) Your result needs to be a number since we will be doing some calculations with it shortly.\nThe student is concerned with something they call “sleep efficiency”. This is the percentage of time in bed spent sleeping. Work out the student’s sleep efficiency for the seven nights in this dataframe. Which night was the student’s sleep efficiency greatest?\nDisplay the time spent in bed each night as a number of hours, minutes and seconds.\nMake a graph of what time the student went to bed each night. Bear in mind that you only need the times, not the dates, and that you want a graph that is informative, showing appropriately the distribution of times the student went to bed.\n\nMy solutions follow:"
  },
  {
    "objectID": "dates-and-times.html#growth-of-mizuna-lettuce-seeds-1",
    "href": "dates-and-times.html#growth-of-mizuna-lettuce-seeds-1",
    "title": "19  Dates and times",
    "section": "19.6 Growth of Mizuna lettuce seeds",
    "text": "19.6 Growth of Mizuna lettuce seeds\nIn 2010, a group of students planted some Mizuna lettuce seeds, and recorded how they grew. The data were saved in an Excel spreadsheet, which is at link. The columns are: the date, the height (in cm) of (I presume) the tallest plant, the amount of water added since the previous date (ml), the temperature in the container where the seedlings were growing, and any additional notes that the students made (edited for length by me). The top line of the data file is variable names.\n\nRead the spreadsheet data.\n\nSolution\nThis is read_excel from package readxl. I’m not sure what will happen to the dates yet. Note that this needs a “local” copy of the spreadsheet (that is, you have to download it and save it on your computer, then upload it to rstudio.cloud), possibly using file.choose to help R find it. I put my copy in the same project folder as I was working in, so I just need the file name:\n\nlibrary(readxl)\nmizuna &lt;- read_excel(\"mizuna.xlsx\")\nmizuna\n\n\n\n  \n\n\n\nThe dates did get read properly. dttm is “date-time”, so I guess it’s allowing for the possibility that my dates had times attached as well. The years do actually come out right.\n\\(\\blacksquare\\)\n\nMake a plot of height against your dates, with the points joined by lines.\n\nSolution\n\nggplot(mizuna, aes(x = date, y = height)) + geom_point() + geom_line()\n\n\n\n\n\\(\\blacksquare\\)\n\nLabel each point on the plot with the amount of water added up to that point.\n\nSolution\nThis is water again. The way to do this is to load ggrepel, then add geom_text_repel to the plot, by adding label=water to the original aes:\n\nlibrary(ggrepel)\nggplot(mizuna, aes(x = date, y = height, label = water)) +\n  geom_point() + geom_line() + geom_text_repel(colour = \"red\")\n\n\n\n\nI made the text red, so that you can see it more easily. It “repels” away from the points, but not from the lines joining them. Which makes me wonder whether this would work better (I explain alpha afterwards):\n\nlibrary(ggrepel)\nggplot(mizuna, aes(x = date, y = height, label = water)) +\n  geom_point() + geom_line() + geom_label_repel(colour = \"red\", \n                                                alpha = 0.7)\n\n\n\n\nThe difference between text and label is that text just uses the text of the variable to mark the point, while label puts that text in a box.\nI think it works better. You can see where the line goes (under the boxes with the labels in them), but you can see the labels clearly.\nWhat that alpha does is to make the thing it’s attached to (the labels) partly transparent. If you leave it out (try it), the black line disappears completely under the label boxes and you can’t see where it goes at all. The value you give for alpha says how transparent the thing is, from 1 (not transparent at all) down to 0 (invisible). I first tried 0.3, and you could hardly see the boxes; then I tried 0.7 so that the boxes were a bit more prominent but the lines underneath were still slightly visible, and I decided that this is what I liked. I think making the labels a different colour was a good idea, since that helps to distinguish the number on the label from the line underneath.\nYou can apply alpha to pretty much any ggplot thing that might be on top of something else, to make it possible to see what’s underneath it. The commonest use for it is if you have a scatterplot with a lot of points; normally, you only see some of the points, because the plot is then a sea of black. But if you make the points partly transparent, you can see more of what’s nearby that would otherwise have been hidden.\nAt some point, I also have to show you folks jitter, which plots in slightly different places points that would otherwise overprint each other exactly, and you wouldn’t know how many of them there were, like the outliers on the boxplots of German children near the new airport.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dates-and-times.html#types-of-childbirth-1",
    "href": "dates-and-times.html#types-of-childbirth-1",
    "title": "19  Dates and times",
    "section": "19.7 Types of childbirth",
    "text": "19.7 Types of childbirth\nChildbirths can be of two types: a “vaginal” birth in which the child is born through the mother’s vagina in the normal fashion, and a “cesarean section” where a surgeon cuts through the wall of the mother’s abdomen, and the baby is delivered through the incision. Cesarean births are used when there are difficulties in pregnancy or during childbirth that would make a vaginal birth too risky. A hospital kept track of the number of vaginal and Cesarean births for the twelve months of 2012. Of interest is whether the Cesarean rate (the ratio of Cesarean births to all births) was increasing, decreasing or remaining stable over that time. The data may be found at link. The columns are the names of the months (in 2012), the number of cesarean births and the number of vaginal births. (The data are not real, but are typical of the kind of thing you would observe.)\n\nRead the data into R and display your data frame.\n\nSolution\nThis is a space-delimited text file, which means:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/birthtypes.txt\"\nbirths &lt;- read_delim(my_url, \" \")\n\nRows: 12 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): month\ndbl (2): cesarean, vaginal\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbirths\n\n\n\n  \n\n\n\nSome text and two numbers for each month. Check.\n\\(\\blacksquare\\)\n\nCreate a column of actual dates and also a column of cesarean rates, as defined above. Store your new data frame in a variable and display it. For the dates, assume that each date is of the 1st of the month that it belongs to.\n\nSolution\nThe easiest way is to use str_c or paste to create a text date with year, month and day in some order, and then to use the appropriate function from lubridate to turn that into an actual date. If you use str_c, you (probably) need the sep thing to make sure the values get a space between them; paste does this automatically. (The next question is whether ymd or whatever can cope without spaces, but I’m not exploring that.) The cesarean rate is cesarean divided by cesarean plus vaginal:\n\nlibrary(lubridate)\nb2 &lt;- births %&gt;%\n  mutate(datestr = str_c(\"2012\", month, \"1\", sep = \" \")) %&gt;%\n  mutate(thedate = ymd(datestr)) %&gt;%\n  mutate(cesarean_rate = cesarean / (cesarean + vaginal))\nb2\n\n\n\n  \n\n\n\nIf you don’t like that, create columns that contain 2012 and 1 all the way down. If you set a column name equal to a single value, that single value gets repeated the right number of times:4\n\nbirths %&gt;% mutate(year = 2012, day = 1)\n\n\n\n  \n\n\n\nand then use unite as in class. The distinction is that unite only works on columns. It also “swallows up” the columns that it is made out of; in this case, the original year, month and day disappear:\n\nbirths %&gt;%\n  mutate(year = 2012, day = 1) %&gt;%\n  unite(datestr, year, month, day) %&gt;%\n  mutate(thedate = ymd(datestr)) %&gt;%\n  mutate(cesarean_rate = cesarean / (cesarean + vaginal)) -&gt; b3\nb3 %&gt;% mutate(the_month = month(thedate))\n\n\n\n  \n\n\n\nI don’t mind which order you glue your year, month and day together, as long as you construct the dates with the consistent lubridate function.\n\\(\\blacksquare\\)\n\nPlot the cesarean rate against time, with a smooth trend. Do you see an upward trend, a downward trend, no trend, or something else?\n\nSolution\nThis is a scatterplot with time on the \\(x\\) axis:\n\nggplot(b3, aes(x = thedate, y = cesarean_rate)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI like this better than joining the points by lines, since we already have a trend on the plot, but you can do that in some contrasting way:\n\nggplot(b3, aes(x = thedate, y = cesarean_rate)) + geom_point() +\n  geom_line(linetype = \"dashed\") + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI see a downward trend. (“A downward trend with a wiggle” if you like.) There is a certain unevenness in the trend of the actual data, but the overall picture appears to be downhill.\n\\(\\blacksquare\\)\n\nTry to summarize the trend you just found with a correlation. What goes wrong? How can you fix it?\n\nSolution\nSomething like this is the obvious guess:\n\nwith(b3, cor(thedate, cesarean_rate))\n\nError in cor(thedate, cesarean_rate): 'x' must be numeric\n\n\nThis fails because thedate is not of itself a number. But lurking in the background is how the date is actually represented: as a number of days since Jan 1, 1970. Thus, passing it through as.numeric might turn it into that:\n\nb3 %&gt;% mutate(numeric_date = as.numeric(thedate)) -&gt; b5\nb5\n\n\n\n  \n\n\n\nA little mental calculation suggests that these dates in 2012 are a bit over 40 years, that is \\(40 \\times 365 \\simeq 14000\\) days, since the “zero” date of Jan 1, 1970, and so it turns out. This suggests that we can calculate a correlation with the numeric dates:\n\nwith(b5, cor(numeric_date, cesarean_rate))\n\n[1] -0.7091219\n\n\nand we can make a test of the null hypothesis that the correlation is zero (against a two-sided alternative) thus:\n\nwith(b5, cor.test(numeric_date, cesarean_rate))\n\n\n    Pearson's product-moment correlation\n\ndata:  numeric_date and cesarean_rate\nt = -3.1804, df = 10, p-value = 0.009813\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9119078 -0.2280145\nsample estimates:\n       cor \n-0.7091219 \n\n\nThat downward trend is more than just chance, with a P-value just under 0.01. Having said that, though, if you look at the confidence interval for the correlation, it includes almost all the negative values it could be, so that with only 12 observations we really know very little about the correlation other than that it appears to be negative.\nExtra: In practice, you would typically have a much longer time series of measurements than this, such as monthly measurements for several years. In looking at only one year, like we did here, we could get trapped by seasonal effects: for example, cesarean rates might always go down through the year and then jump up again in January. Looking at several years would enable us to disentangle seasonal effects that happen every year from long-term trends. (As an example of this, think of Toronto snowfall: there is almost always snow in the winter and there is never snow in the summer, a seasonal effect, but in assessing climate change, you want to think about long-term trends in snowfall, after allowing for which month you’re looking at.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dates-and-times.html#wolves-and-caribou-1",
    "href": "dates-and-times.html#wolves-and-caribou-1",
    "title": "19  Dates and times",
    "section": "19.8 Wolves and caribou",
    "text": "19.8 Wolves and caribou\nIn Denali National Park, Alaska, the size of the wolf population depends on the size of the caribou population (since wolves hunt and kill caribou). This is a large national park, so caribou are found in very large herds, so big, in fact, that the well-being of the entire herd is not threatened by wolf attacks.5 Can the size of the caribou population be used to predict the size of the wolf population? The data can be found at link. The columns are: the date of the survey,6 the name of the park employee in charge of the survey, the caribou population (in hundreds) and the wolf population (actual count).7\n\nTake a look at the data file. How would you describe its format? Read it into R, and check that you got something sensible.\n\nSolution\nThis looks at first sight as if it’s separated by spaces, but most of the data values are separated by more than one space. If you look further, you’ll see that the values are lined up in columns, with the variable names aligned at the top.\nThis used to be exactly the kind of thing that read_table would read, but no longer. We start with the usual library(tidyverse):\n\nlibrary(tidyverse)\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/caribou.txt\"\ndenali &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  date = col_character(),\n  name = col_character(),\n  caribou = col_character(),\n  wolf = col_character()\n)\n\n\nWarning: 7 parsing failures.\nrow col  expected    actual                                             file\n  1  -- 4 columns 5 columns 'http://ritsokiguess.site/datafiles/caribou.txt'\n  2  -- 4 columns 5 columns 'http://ritsokiguess.site/datafiles/caribou.txt'\n  3  -- 4 columns 5 columns 'http://ritsokiguess.site/datafiles/caribou.txt'\n  4  -- 4 columns 6 columns 'http://ritsokiguess.site/datafiles/caribou.txt'\n  5  -- 4 columns 5 columns 'http://ritsokiguess.site/datafiles/caribou.txt'\n... ... ......... ......... ................................................\nSee problems(...) for more details.\n\ndenali\n\n\n\n  \n\n\n\nThe spaces within the names have spilled into the next column, so that the dates and first names are (mostly) correct, but the initials should stay with the names.\nThe right thing now is read_fwf, where fwf stands for “fixed-width format”\n\ndl_file &lt;- \"caribou.txt\"\ndownload.file(my_url, dl_file)\nread_fwf(dl_file)\n\nRows: 8 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\n\nchr (4): X1, X2, X3, X4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n  \n\n\n\nThat’s much closer, but it thought the column names were part of the data. We seem to need to enter them specifically:\n\nmy_cols &lt;- c(\"date\", \"name\", \"caribou\", \"wolf\")\ndenali &lt;- read_fwf(dl_file, fwf_empty(dl_file, col_names = my_cols), skip = 1)\n\nRows: 7 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\n\nchr (2): date, name\ndbl (2): caribou, wolf\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndenali\n\n\n\n  \n\n\n\nand that’s worked. The fwf_empty says to guess where the columns are based on where there are spaces all the way down (as read_table used to do), and to use the specified column names. The top line of the datafile is those column names, though, so we need to skip that row. A bit fiddly.\nAnyway, that (finally) worked: four columns with the right names, and the counts of caribou and wolf are numbers. There are only seven years of surveys; in real-life data you would have more. But the point here is working with dates.\nThe only (small) weirdness is that the dates are text rather than having been converted into dates. This is because they are not year-month-day, which is the only format that gets automatically converted into dates when read in. (You could use mdy from lubridate to make them dates.)\nExtra: you might have wondered how the names survived, even though they have spaces in them, sometimes more than one. Here’s how the file looks:\n\ndate       name             caribou wolf\n09/01/1995 David S.         30       66\n09/24/1996 Youngjin K.      34       79\n10/03/1997 Srinivasan M.    27       70\n09/15/1998 Lee Anne J.      25       60\n09/08/1999 Stephanie T.     17       48\n09/03/2000 Angus Mc D.      23       55\n10/06/2001 David S.         20       60\n\nWhat read_table looks for is columns that contain spaces all the way down, and separates the values there. For example, between the year ofdate and the first name in name there is a space all the way down. After the names and before the caribou counts there are several spaces, and there is one space between the words caribou and wolf in the header line that goes all the way down. Thus four columns, date, name, caribou and wolf. This means that the spaces within the names don’t cause any problems at all, since the spaces aren’t in the same place in every line.8\n\\(\\blacksquare\\)\n\nCreate a new data frame where the column labelled date is now a genuine R Date, using something from lubridate.\n\nSolution\nWhat you do is to look at the format of the dates as they are now. They appear to be month-day-year, American style.9\nThus the function needed is mdy. It doesn’t matter whether the months are names or numbers:\n\ndenali %&gt;% mutate(date = mdy(date)) -&gt; denali\ndenali\n\n\n\n  \n\n\n\nI lived on the edge and overwrote both my column and the whole data frame.10\nThe dates are displayed in ISO format, year-month-day. You see at the top of the column that they now really are dates, not just pieces of text that look like dates.\n\\(\\blacksquare\\)\n\nCreate new columns containing the days of the week and the month names for the dates.\n\nSolution\nThis involves digging in the lubridate help to find out how to extract things from a date. It turns out that wday extracts the day of the week from a date, by default as a number, and month gets the month, also by default as a number:\n\ndenali %&gt;% mutate(mon = month(date), wd = wday(date))\n\n\n\n  \n\n\n\nThis is not what we wanted, though; we wanted the names of the months and of the days. To fix that, add label=T to both functions:\n\ndenali %&gt;% mutate(mon = month(date, label = T), wd = wday(date, label = T))\n\n\n\n  \n\n\n\nand that cracks it.\nNo need to save this data frame anywhere, since we’re not using any of this later.\nExtra: the ord means “ordered factor”, which makes sense since these are categorical variables with a natural order. This means that you could do something like counting the number of surveys in each month like this:\n\ndenali %&gt;%\n  mutate(mon = month(date, label = T), wd = wday(date, label = T)) %&gt;%\n  count(mon)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nEnough playing around with dates. Make a scatterplot of caribou population (explanatory) against wolf population (response). Do you see any relationship?\n\nSolution\nNothing terribly surprising here:\n\nggplot(denali, aes(x = caribou, y = wolf)) + geom_point()\n\n\n\n\nIf you like, add a smooth trend to it:11\n\nggplot(denali, aes(x = caribou, y = wolf)) + geom_point() + geom_smooth(se = F)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThis is an upward trend: when one population is large, the other one is large too. This is typical for predator-prey relationships: when there is more to eat (more caribou) the wolf population goes up, and when less, it goes down.\n\\(\\blacksquare\\)\n\nOn your plot from the previous part, label each point with the year it belongs to. You can do this in two steps: first make a new column containing just the years, and then use it as labels for the points on the plot.\n\nSolution\nI’m going to use geom_text_repel for the labels from package ggrepel. The year values are gotten using the lubridate function year:\n\ndenali %&gt;%\n  mutate(year = year(date)) %&gt;%\n  ggplot(aes(x = caribou, y = wolf, label = year)) + geom_point() + geom_text_repel()\n\n\n\n\nI thought about joining up the points in year order. This is actually not geom_line as you would have guessed, since what that does is to join points in the order of the variable on the \\(x\\)-axis.12 To join points in the order that they are in the data (what we want here, because the points are in time order in the data), use instead geom_path:\n\ndenali %&gt;%\n  mutate(year = year(date)) %&gt;%\n  ggplot(aes(x = caribou, y = wolf, label = year)) + geom_point() +\n  geom_text_repel() + geom_path()\n\n\n\n\nIn 1996, both populations were large, and both showed a steady decline until 1999. In 2000 and 2001, both populations seemed to be on the way up again, and you can imagine that in a couple of years, things would go back to about where they were in 1995.\n\\(\\blacksquare\\)\n\nMake a plot of caribou population against time (this is done the obvious way). What seems to be happening to the caribou population over time?\n\nSolution\nMake a scatterplot, with the survey date as explanatory variable, and caribou population as response (since time always goes on the \\(x\\)-axis):\n\nggplot(denali, aes(x = date, y = caribou)) + geom_point() + geom_line()\n\n\n\n\nI used an ordinary geom_line this time, to connect neighbouring years, as is often done with a time series. The overall trend is downward, though the 1999 value might be a low from which the population is recovering.\n\\(\\blacksquare\\)\n\nThe caribou and wolf populations over time are really “time series”. See if you can make a plot of both the caribou and wolf populations against time. You can make two \\(y\\)-axes, one for caribou and one for wolf; this will probably require some research on your part to figure out.\n\nSolution\nThe obvious starting point is to note that both the caribou and wolf columns are animal populations, just of different animals. One way of plotting both populations is to pivot_longer them up into one longer column, and then plot them against time, with the two animals distinguished by colour:\n\ndenali %&gt;%\n  pivot_longer(caribou:wolf, names_to=\"animal\", values_to=\"population\") %&gt;%\n  ggplot(aes(x = date, y = population, colour = animal)) +\n  geom_point() + geom_line()\n\n\n\n\nThis is not quite the story, though, because the caribou and wolf populations are on different scales. The caribou population is numbered in hundreds, while the wolf population is an actual count.\nThe surveys are late in the year, so the one that is nearly in 1996 is actually the 1995 survey.\nWhat would be nice would be to have a secondary \\(y\\)-axis, so that there were two \\(y\\)-scales, one for each animal. This is very easy to manipulate, though (you can change either scale and get a very different-looking graph), so we ought to be careful.\nAll right, so let’s put the caribou on the left:\n\nggplot(denali, aes(x = date, y = caribou)) + geom_line()\n\n\n\n\nOr we can add a colour aesthetic to distinguish the caribou from the wolf populations, that we’re going to add in a moment. This looks rather odd at first:\n\nggplot(denali, aes(x = date, y = caribou, colour = \"caribou\")) + geom_line()\n\n\n\n\nNow we think about adding the wolf numbers. This is done by adding a second geom_line, overriding the y and the colour to specify that this is wolf now:\n\nggplot(denali, aes(x = date, y = caribou, colour = \"caribou\")) +\n  geom_line() +\n  geom_line(aes(y = wolf, colour = \"wolf\"))\n\n\n\n\nWhat has happened is that we get lines of different colour for each animal, with a legend. So far so good. The problem is that the wolf numbers are about 2.5 times bigger than the caribou numbers,13 so that we don’t get a good sense of how they go up and down together. If we divided the wolf numbers by 2.5, we would see this better:\n\nggplot(denali, aes(x = date, y = caribou, colour = \"caribou\")) +\n  geom_line() +\n  geom_line(aes(y = wolf / 2.5, colour = \"wolf\"))\n\n\n\n\nNow we get to the secondary \\(y\\)-axis. We want to label this wolf and have it reflect that we actually made the graph by dividing the wolf values by 2.5:\n\nggplot(denali, aes(x = date, y = caribou, colour = \"caribou\")) +\n  geom_line() +\n  geom_line(aes(y = wolf / 2.5, colour = \"wolf\")) +\n  scale_y_continuous(sec.axis = sec_axis(~ . * 2.5, name = \"wolf\"))\n\n\n\n\nWoo, and, very possibly, hoo. I got most of these ideas from link.\nNow we see how the populations vary over time, and also that they vary together.\nThis is about the only double-\\(y\\)-axis setup that I like, with scales chosen so that both the series vary about the same amount. By “discreetly” changing the wolf scale, you could make it look as if one population was much bigger than the other, or varied much more than the other. Lies and statistics.\nIn my opinion, too many people just plot series against time, possibly with a second \\(y\\)-axis.14 Variables that vary together, like the wolf and caribou populations here, ought to be plotted against each other on a scatterplot, possibly with the time points labelled.\nThe ambitious among you may like to compare the graphs here with other predator-prey relationships. If you are of a mathematical bent, you might look into the Lotka-Volterra equations, which is a system of two differential equations describing how changes in one population cause changes in the other population.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dates-and-times.html#dealing-with-dates-in-the-worcester-heart-attack-study-1",
    "href": "dates-and-times.html#dealing-with-dates-in-the-worcester-heart-attack-study-1",
    "title": "19  Dates and times",
    "section": "19.9 Dealing with dates in the Worcester Heart Attack study",
    "text": "19.9 Dealing with dates in the Worcester Heart Attack study\nThe Worcester Heart Attack Study is an ongoing study of heart attacks in the Worcester, MA area. The main purpose of the study is to investigate changes over time in incidence and death rates, and also the use of different treatment approaches. We will be mainly using this data set to investigate data handling and dealing with dates. The data can be found at link.\n\nRead the data into R. The reading-in part is straightforward, but check what type of thing each column is. Is that what it should be?\n\nSolution\nThis is read_delim:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/whas500.txt\"\nwhas &lt;- read_delim(my_url, \" \")\n\nRows: 500 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr  (3): admitdate, disdate, fdate\ndbl (19): id, age, gender, hr, sysbp, diasbp, bmi, cvd, afb, sho, chf, av3, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwhas\n\n\n\n  \n\n\n\nTo see what type everything is, note that when you display a tibble, the type of all the columns on the screen is displayed at the top. Click the little right-arrow to see more columns and to check their type.\nAll the numbers are properly integer (int) or decimal (dbl) numbers, but the date columns are chr or text. This means that they haven’t been read as Dates (because they were not in year-month-day order). This is (as we will see) unlike SAS, which determined that they were dates, and even used the first 20 rows of the file to determine what format of dates they were.\n\\(\\blacksquare\\)\n\nThe date columns should be R dates. They are not year-month-day, so converting them via as.Date (which is what read_delim tries to do) will not work. Load the lubridate package, and create new columns in your data frame that are properly dates. Save your data frame, and list it to demonstrate that it worked.\n\nSolution\nYou can load lubridate first, but there is no need since it is now part of the tidyverse.\nThese dates are day-month-year, so we need dmy from lubridate:\n\nwhas %&gt;% mutate(\n  admit = dmy(admitdate),\n  dis = dmy(disdate),\n  f = dmy(fdate)\n) -&gt; whas2\nglimpse(whas2)\n\nRows: 500\nColumns: 25\n$ id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ age       &lt;dbl&gt; 83, 49, 70, 70, 70, 70, 57, 55, 88, 54, 48, 75, 48, 54, 67, …\n$ gender    &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, …\n$ hr        &lt;dbl&gt; 89, 84, 83, 65, 63, 76, 73, 91, 63, 104, 95, 154, 85, 95, 93…\n$ sysbp     &lt;dbl&gt; 152, 120, 147, 123, 135, 83, 191, 147, 209, 166, 160, 193, 1…\n$ diasbp    &lt;dbl&gt; 78, 60, 88, 76, 85, 54, 116, 95, 100, 106, 110, 123, 80, 65,…\n$ bmi       &lt;dbl&gt; 25.54051, 24.02398, 22.14290, 26.63187, 24.41255, 23.24236, …\n$ cvd       &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, …\n$ afb       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ sho       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ chf       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ av3       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ miord     &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ mitype    &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ year      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ admitdate &lt;chr&gt; \"13-01-1997\", \"19-01-1997\", \"01-01-1997\", \"17-02-1997\", \"01-…\n$ disdate   &lt;chr&gt; \"18-01-1997\", \"24-01-1997\", \"06-01-1997\", \"27-02-1997\", \"07-…\n$ fdate     &lt;chr&gt; \"31-12-2002\", \"31-12-2002\", \"31-12-2002\", \"11-12-1997\", \"31-…\n$ los       &lt;dbl&gt; 5, 5, 5, 10, 6, 1, 5, 4, 4, 5, 5, 10, 7, 21, 4, 1, 13, 14, 6…\n$ dstat     &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lenfol    &lt;dbl&gt; 2178, 2172, 2190, 297, 2131, 1, 2122, 1496, 920, 2175, 2173,…\n$ fstat     &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ admit     &lt;date&gt; 1997-01-13, 1997-01-19, 1997-01-01, 1997-02-17, 1997-03-01,…\n$ dis       &lt;date&gt; 1997-01-18, 1997-01-24, 1997-01-06, 1997-02-27, 1997-03-07,…\n$ f         &lt;date&gt; 2002-12-31, 2002-12-31, 2002-12-31, 1997-12-11, 2002-12-31,…\n\n\nThere are a lot of columns, so I used glimpse. The three new variables we created are at the end of the list. They are correctly Dates, and they have the right values, the ones we can see at least.\nThe indentation is up to you. I think it’s nice to make the creations of the three new variables line up. You can also make the opening and closing brackets on the long mutate aligned, or you can do as I have done here and put two closing brackets on the end. The rationale for this is that each of the variable definition lines in the mutate ends either with a comma or an extra closing bracket, the latter being on the last line. Your choice here is a matter of taste or (in your working life) the coding norms of the team you’re working with.\nExtra: you may have been offended by the repetition above. It so happens that these columns’ names all end in date and they are the only ones that do, so we can use a “select helper” to select only them, and then submit all of them to a mutate via across, which goes like this:\n\nwhas %&gt;% mutate(across(ends_with(\"date\"), \\(date) dmy(date))) %&gt;% glimpse()\n\nRows: 500\nColumns: 22\n$ id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ age       &lt;dbl&gt; 83, 49, 70, 70, 70, 70, 57, 55, 88, 54, 48, 75, 48, 54, 67, …\n$ gender    &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, …\n$ hr        &lt;dbl&gt; 89, 84, 83, 65, 63, 76, 73, 91, 63, 104, 95, 154, 85, 95, 93…\n$ sysbp     &lt;dbl&gt; 152, 120, 147, 123, 135, 83, 191, 147, 209, 166, 160, 193, 1…\n$ diasbp    &lt;dbl&gt; 78, 60, 88, 76, 85, 54, 116, 95, 100, 106, 110, 123, 80, 65,…\n$ bmi       &lt;dbl&gt; 25.54051, 24.02398, 22.14290, 26.63187, 24.41255, 23.24236, …\n$ cvd       &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, …\n$ afb       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ sho       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ chf       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ av3       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ miord     &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ mitype    &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ year      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ admitdate &lt;date&gt; 1997-01-13, 1997-01-19, 1997-01-01, 1997-02-17, 1997-03-01,…\n$ disdate   &lt;date&gt; 1997-01-18, 1997-01-24, 1997-01-06, 1997-02-27, 1997-03-07,…\n$ fdate     &lt;date&gt; 2002-12-31, 2002-12-31, 2002-12-31, 1997-12-11, 2002-12-31,…\n$ los       &lt;dbl&gt; 5, 5, 5, 10, 6, 1, 5, 4, 4, 5, 5, 10, 7, 21, 4, 1, 13, 14, 6…\n$ dstat     &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lenfol    &lt;dbl&gt; 2178, 2172, 2190, 297, 2131, 1, 2122, 1496, 920, 2175, 2173,…\n$ fstat     &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n\n\nOne line, as you see, not three. The English-language version of this reads “for each of the columns whose name ends with date, work out dmy of it”, that is to say, convert it into a date. We can use any of the select-helpers in this, including listing the column numbers or names; in this case our date variables all ended with date.\nThis overwrites the original date columns (you can see that they are now dates), but you can give them new names thus. This is inside the across inside the mutate, so it needs two close-brackets after (the probable reason for the error if you get one):\n\nwhas %&gt;% mutate(across(ends_with(\"date\"), \\(date) dmy(date), \n                       .names = \"{.col}_d\")) %&gt;% glimpse()\n\nRows: 500\nColumns: 25\n$ id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ age         &lt;dbl&gt; 83, 49, 70, 70, 70, 70, 57, 55, 88, 54, 48, 75, 48, 54, 67…\n$ gender      &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0…\n$ hr          &lt;dbl&gt; 89, 84, 83, 65, 63, 76, 73, 91, 63, 104, 95, 154, 85, 95, …\n$ sysbp       &lt;dbl&gt; 152, 120, 147, 123, 135, 83, 191, 147, 209, 166, 160, 193,…\n$ diasbp      &lt;dbl&gt; 78, 60, 88, 76, 85, 54, 116, 95, 100, 106, 110, 123, 80, 6…\n$ bmi         &lt;dbl&gt; 25.54051, 24.02398, 22.14290, 26.63187, 24.41255, 23.24236…\n$ cvd         &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1…\n$ afb         &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0…\n$ sho         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ chf         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1…\n$ av3         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ miord       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0…\n$ mitype      &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1…\n$ year        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ admitdate   &lt;chr&gt; \"13-01-1997\", \"19-01-1997\", \"01-01-1997\", \"17-02-1997\", \"0…\n$ disdate     &lt;chr&gt; \"18-01-1997\", \"24-01-1997\", \"06-01-1997\", \"27-02-1997\", \"0…\n$ fdate       &lt;chr&gt; \"31-12-2002\", \"31-12-2002\", \"31-12-2002\", \"11-12-1997\", \"3…\n$ los         &lt;dbl&gt; 5, 5, 5, 10, 6, 1, 5, 4, 4, 5, 5, 10, 7, 21, 4, 1, 13, 14,…\n$ dstat       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ lenfol      &lt;dbl&gt; 2178, 2172, 2190, 297, 2131, 1, 2122, 1496, 920, 2175, 217…\n$ fstat       &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1…\n$ admitdate_d &lt;date&gt; 1997-01-13, 1997-01-19, 1997-01-01, 1997-02-17, 1997-03-0…\n$ disdate_d   &lt;date&gt; 1997-01-18, 1997-01-24, 1997-01-06, 1997-02-27, 1997-03-0…\n$ fdate_d     &lt;date&gt; 2002-12-31, 2002-12-31, 2002-12-31, 1997-12-11, 2002-12-3…\n\n\nThe three columns on the end are the new actual-dates we created. To give them new names, use .names inside across, and in that is a recipe that says how to make the new names. {.col} means the name the column had before, and the _d after that means to add that to the old name to make the new one.\n\\(\\blacksquare\\)\n\nCreate three new variables diff1, diff2, diff3 that are the numbers of days between each of your dates, and save the data frame in which they have been created. Verify that at least some of them are the same as los and lenfol.\n\nSolution\nI don’t know what R’s internal storage is for dates (it might be seconds or milliseconds or anything, not necessarily days), so subtracting them requires care; you have to divide by the length of a day (in whatever units), thus:\n\nwhas3 &lt;- whas2 %&gt;% mutate(\n  diff1 = (dis - admit) / ddays(1),\n  diff2 = (f - admit) / ddays(1),\n  diff3 = (f - dis) / ddays(1)\n)\nglimpse(whas3)\n\nRows: 500\nColumns: 28\n$ id        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ age       &lt;dbl&gt; 83, 49, 70, 70, 70, 70, 57, 55, 88, 54, 48, 75, 48, 54, 67, …\n$ gender    &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, …\n$ hr        &lt;dbl&gt; 89, 84, 83, 65, 63, 76, 73, 91, 63, 104, 95, 154, 85, 95, 93…\n$ sysbp     &lt;dbl&gt; 152, 120, 147, 123, 135, 83, 191, 147, 209, 166, 160, 193, 1…\n$ diasbp    &lt;dbl&gt; 78, 60, 88, 76, 85, 54, 116, 95, 100, 106, 110, 123, 80, 65,…\n$ bmi       &lt;dbl&gt; 25.54051, 24.02398, 22.14290, 26.63187, 24.41255, 23.24236, …\n$ cvd       &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, …\n$ afb       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ sho       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ chf       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ av3       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ miord     &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ mitype    &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ year      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ admitdate &lt;chr&gt; \"13-01-1997\", \"19-01-1997\", \"01-01-1997\", \"17-02-1997\", \"01-…\n$ disdate   &lt;chr&gt; \"18-01-1997\", \"24-01-1997\", \"06-01-1997\", \"27-02-1997\", \"07-…\n$ fdate     &lt;chr&gt; \"31-12-2002\", \"31-12-2002\", \"31-12-2002\", \"11-12-1997\", \"31-…\n$ los       &lt;dbl&gt; 5, 5, 5, 10, 6, 1, 5, 4, 4, 5, 5, 10, 7, 21, 4, 1, 13, 14, 6…\n$ dstat     &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lenfol    &lt;dbl&gt; 2178, 2172, 2190, 297, 2131, 1, 2122, 1496, 920, 2175, 2173,…\n$ fstat     &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ admit     &lt;date&gt; 1997-01-13, 1997-01-19, 1997-01-01, 1997-02-17, 1997-03-01,…\n$ dis       &lt;date&gt; 1997-01-18, 1997-01-24, 1997-01-06, 1997-02-27, 1997-03-07,…\n$ f         &lt;date&gt; 2002-12-31, 2002-12-31, 2002-12-31, 1997-12-11, 2002-12-31,…\n$ diff1     &lt;dbl&gt; 5, 5, 5, 10, 6, 1, 5, 4, 4, 5, 5, 10, 7, 21, 4, 1, 13, 14, 6…\n$ diff2     &lt;dbl&gt; 2178, 2172, 2190, 297, 2131, 1, 2122, 1496, 920, 2175, 2173,…\n$ diff3     &lt;dbl&gt; 2173, 2167, 2185, 287, 2125, 0, 2117, 1492, 916, 2170, 2168,…\n\n\nThe extra d on the front of ddays indicates that these are what is known to lubridate as “durations”: a period of time 1 day long that could be any day (as opposed to “June 1, 1970” which is 1 day long, but tied to a particular day).\nlos should be the number of days in hospital, what I calculated as diff1, and lenfol should be the time from being admitted to last followup, which is my diff2. My output from glimpse confirms that.\nExtra: of course, checking that the first few values match is a nice confirmation, but is not actually a proof. For that, we should compare all 500 values, and it would be best to do it in such a way that R is comparing all 500 values for us, since it would be a lot more reliable than the human eye. R has a function all.equal which does exactly that. By way of warmup:\n\nx &lt;- 1:4\ny &lt;- 1:4\nz &lt;- c(1, 2, 3, 5)\nall.equal(x, y)\n\n[1] TRUE\n\nall.equal(x, z)\n\n[1] \"Mean relative difference: 0.25\"\n\n\nI thought the second one was just going to say FALSE, but it gave us a message instead, saying how close x and z were on average, so that we could decide whether they were close enough to call equal, or, as in this case, not.\nAnyway:\n\nwith(whas3, all.equal(lenfol, diff2))\n\n[1] TRUE\n\nwith(whas3, all.equal(los, diff1))\n\n[1] TRUE\n\n\nso they really are all equal, all 500 of them.15\n\\(\\blacksquare\\)\n\nConstruct side-by-side boxplots of the length of followup by each followup status. You’ll need to make sure that the followup status, as it gets fed into ggplot, is a factor, or, at least, not the number that it is now.\n\nSolution\nThe easiest way to make a factor is to wrap fstat, which is a numeric 0 or 1, in factor():\n\nggplot(whas3, aes(x = factor(fstat), y = lenfol)) + geom_boxplot()\n\n\n\n\nOr create a factor version of fstat first:\n\nwhas3 %&gt;%\n  mutate(ffstat = factor(fstat)) %&gt;%\n  ggplot(aes(x = ffstat, y = lenfol)) + geom_boxplot()\n\n\n\n\nI think the second way looks better, because you get a cleaner \\(x\\)-axis on your plot. But if you’re doing this for exploration, rather than as something that’s going to appear in a report for your boss, the first way is fine.\nggplot also treats text stuff as categorical where needed, so this also works:\n\nwhas3 %&gt;%\n  mutate(cfstat = as.character(fstat)) %&gt;%\n  ggplot(aes(x = cfstat, y = lenfol)) + geom_boxplot()\n\n\n\n\nExtra: this is an example of what’s called “survival data”: the purpose of the study was to see what affected how long a person survived after a heart attack. Each patient was followed up for the number of days in lenfol, but followup could have stopped for two (or more) reasons: the patient died (indicated by fstat being 1), or something else happened to them (fstat is 0), such as moving away from where the study was conducted, getting another disease, the funding for this study running out, or simply losing touch with the people doing the study. Such a patient is called “lost to followup” or “censored”, and all we know about their survival is that they were still alive when last seen, but we don’t know how long they lived after that.\nFor example:\n\nwhas %&gt;% select(id, lenfol, fstat)\n\n\n\n  \n\n\n\nThe patient with id 4 died after 297 days, but patients 1 through 3 lived for over 2000 days and were still alive when last seen. My guess for patients 1 through 3 is that the study ended and they were still alive:\n\nwhas %&gt;% summarize(maxfol = max(lenfol)/365.25)\n\n\n\n  \n\n\n\nThe longest time anyone was followed up was six and a half years. Studies like this are funded for some number of years (say 10), and people can join after the beginning. (If they happen to join near the end, they won’t get followed up for very long.)\nWe’re not going to analyze these data, but if we were, we would want to take advantage of the information in the patient who lived for “at least 2178 days”. Looking only at the patients who we knew to have died would be wasteful and might introduce a bias; for example, if we were comparing several treatments, and one of the treatments was so good that almost everybody on it was still alive at the end, we would want to have a strong inference that this treatment was the best.\nWith that in mind, let’s redraw our boxplot with better labels for the followup status:\n\nwhas3 %&gt;% \n  mutate(followup_status = ifelse(fstat == 1, \"died\", \"censored\")) %&gt;% \n  ggplot(aes(x = followup_status, y = lenfol)) + geom_boxplot()\n\n\n\n\nNow we have a clearer sense of what is going on. Out of the patients who died, some of them survived a long time, but most of them died fairly quickly. Out of the patients who were censored, the times they were observed were all over the place, which suggests that (at least for the ones still in the study at the end) they joined the study at all kinds of different times.\nAnother graph that is possible here is a facetted histogram:\n\nwhas3 %&gt;% \n  mutate(followup_status = ifelse(fstat == 1, \"died\", \"censored\")) %&gt;% \n  ggplot(aes(x = lenfol)) + geom_histogram(bins = 10) +\n  facet_wrap(~followup_status)\n\n\n\n\nThe right-skewed distribution of times to death is what we saw from the boxplot, but what is that periodic thing on the left? Let’s convert the days to years and draw again:\n\nwhas3 %&gt;% \n  mutate(followup_status = ifelse(fstat == 1, \"died\", \"censored\")) %&gt;% \n  mutate(followup_years = lenfol/365.25) %&gt;% \n  ggplot(aes(x = followup_years)) + geom_histogram(bins = 20) +\n  facet_wrap(~followup_status)\n\n\n\n\nThat’s odd. On the left, it looks as if there were bursts of patients admitted to the study at around 1.5, 3.5, and 5.5 years from the end. (These are, remember, all people who survived and mostly people who survived to the end.) Not what I would have expected – I would have expected a steady stream of patients, the heart attack victims as they happened to come in.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dates-and-times.html#going-to-sleep-1",
    "href": "dates-and-times.html#going-to-sleep-1",
    "title": "19  Dates and times",
    "section": "19.10 Going to sleep",
    "text": "19.10 Going to sleep\nA student keeps track of what time they go to bed and what time they get up in the morning. They also have an app on their phone that measures the number of hours they were asleep during that time. The data for one week are in http://ritsokiguess.site/datafiles/sleeping.csv, in the 24-hour clock.\n\nRead in and display the data. What type of things are each of your columns?\n\nSolution\nThe usual, to start:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/sleeping.csv\"\nsleep &lt;- read_csv(my_url)\n\nRows: 7 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (1): sleep.time\ndttm (2): bed.time, rise.time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsleep\n\n\n\n  \n\n\n\nOn mine, the sleep time is an ordinary decimal number, but the two times are something called dttm, which I can guess means date-time. In your notebook, you might see S3:POSIXct, and you probably don’t know what that is (although you can guess).16\nIf you search for this, you’ll find some links to the help files, but a bit further down is this, which says it in a few words: “These objects store the number of seconds (for POSIXct) … since January 1st 1970 at midnight.”17\nMake the claim that the first two columns are genuine date-times, and if they are labelled S3:POSIXct for you, say how you know. That is to say, they may look like pieces of text laid out as date-times, but they are actual date-times stored internally as seconds since Jan 1 1970 and displayed nicely. Thus we do not need to use ymd_hms or anything similar to deal with them.\n\\(\\blacksquare\\)\n\nWork out the fractional number of hours that the student was in bed each of these nights. (They may not have been asleep this whole time.) Your result needs to be a number since we will be doing some calculations with it shortly.\n\nSolution\nSince these are genuine date-times, you can take the difference, but the unit is not predictable. Internally, these are stored as a number of seconds, but it displays a “nice” unit:\n\nsleep %&gt;% mutate(in_bed = rise.time - bed.time)\n\n\n\n  \n\n\n\nIn this case, we did get a number of hours, but in the next part, we are going to do a calculation like this:\n\nsleep %&gt;% mutate(in_bed = rise.time - bed.time) %&gt;% \nmutate(ratio = sleep.time / in_bed)\n\nError in `mutate()`:\nℹ In argument: `ratio = sleep.time/in_bed`.\nCaused by error in `/.difftime`:\n! second argument of / cannot be a \"difftime\" object\n\n\nand this doesn’t work because you can’t divide a number by a time. (What would its units be?) So we have to turn in_bed into a number, and to do that we can divide by the number of seconds in an hour:\n\nsleep %&gt;% mutate(in_bed = (rise.time - bed.time) / dhours(1))\n\n\n\n  \n\n\n\nThis is now correctly a (decimal) number.\n\\(\\blacksquare\\)\n\nThe student is concerned with something they call “sleep efficiency”. This is the percentage of time in bed spent sleeping. Work out the student’s sleep efficiency for the seven nights in this dataframe. Which night was the student’s sleep efficiency greatest?\n\nSolution\nDivide the sleep time by the in-bed time and multiply by 100. To answer the last part of the question, you might think of sorting these in descending order as well:\n\nsleep %&gt;% mutate(in_bed = (rise.time - bed.time) / dhours(1)) %&gt;% \nmutate(efficiency = sleep.time / in_bed * 100) %&gt;% \narrange(desc(efficiency))\n\n\n\n  \n\n\n\nThe night of September 8. This was the night the student went to bed the latest, but they were asleep almost all the time they were in bed.\n\\(\\blacksquare\\)\n\nDisplay the time spent in bed each night as a number of hours, minutes and seconds.\n\nSolution\nThe idea here is to display the time between going to bed and getting up as an interval, using %--%, and then turn that into a period:\n\nsleep %&gt;% mutate(in_bed_hms = as.period(bed.time %--% rise.time))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake a graph of what time the student went to bed each night. Bear in mind that you only need the times, not the dates, and that you want a graph that is informative, showing appropriately the distribution of times the student went to bed.\n\nSolution\nIf you just pull out the times, some of them will be at the end of the day and some will be at the beginning. Extracting the hours, minutes and seconds is one way:18\n\nsleep %&gt;% mutate(h = hour(bed.time), m = minute(bed.time), s = second(bed.time))\n\n\n\n  \n\n\n\nYou could convert these into fractional hours to make a histogram of:\n\nsleep %&gt;% mutate(h = hour(bed.time), m = minute(bed.time), s = second(bed.time)) %&gt;% \nmutate(bed_time_hours = h + m / 60 + s / (60*60))\n\n\n\n  \n\n\n\nbut if you make a histogram of these, this is what you get:\n\nsleep %&gt;% mutate(h = hour(bed.time), m = minute(bed.time), s = second(bed.time)) %&gt;% \nmutate(bed_time_hours = h + m / 60 + s / (60*60)) %&gt;% \nggplot(aes(x = bed_time_hours)) + geom_histogram(bins = 5)\n\n\n\n\nbut this makes no sense because the bedtimes after midnight are on the end of the previous day, not the beginning of the next one!\nWith that in mind, let’s move the bedtimes that are, say, before 3:00am to the end of the previous day by adding 24 to them before we make the graph:\n\nsleep %&gt;% mutate(h = hour(bed.time), m = minute(bed.time), s = second(bed.time)) %&gt;% \nmutate(bed_time_hours = h + m / 60 + s / (60*60)) %&gt;% \nmutate(bed_time_hours = ifelse(bed_time_hours &lt; 3, bed_time_hours + 24, bed_time_hours)) %&gt;% \nggplot(aes(x = bed_time_hours)) + geom_histogram(bins = 5)\n\n\n\n\nThis gives a sense of where the bedtimes are. If you’re used to reading the 24-hour clock, you’ll know that 23 is 11:00pm, and you’ll have a sense that some of the bedtimes were 11 or a bit earlier and some were around midnight. (I like the 24-hour clock.) There are only 7 observations, so the graph you get won’t look very nice as a histogram, but at least this one says something about when the student went to bed, in a way that puts times just after midnight next to times just before. You should give some thought about the number of bins; with only 7 observations, even 5 bins is pushing it, but this looked nicer to me than 4 bins.\nIf you’re more used to the 12-hour clock, you’ll want to convert the times to something between 10 and 12. You can do this with an ifelse as above, subtracting 12 from the ones before midnight and adding 12 to the ones after. Or you can recognize this as modulo arithmetic (the clock is a classic case: what is 10:00pm plus 3 hours?) A little thought will reveal that subtracting (or adding) 12 hours and taking the result modulo 24 would do it: the pre-midnight bedtimes will get turned into a number like 10 or 11, and the post-midnight ones to 12 and a bit. R has a modulo operator, which is %% (cite your source: mine was this):\n\nsleep %&gt;% mutate(h = hour(bed.time), m = minute(bed.time), s = second(bed.time)) %&gt;% \nmutate(bed_time_hours = h + m / 60 + s / (60*60)) %&gt;% \nmutate(bed_time_hours = (bed_time_hours - 12) %% 24) %&gt;% \nggplot(aes(x = bed_time_hours)) + geom_histogram(bins = 5)\n\n\n\n\nand you might find the \\(x\\)-scale of that easier to cope with. (The bins have come out differently, for some reason.)\nI think the best graph uses the fact that date-times plot nicely, so if we keep them as date-times, the \\(x\\)-scale will look nice. The problem is that they are times on different days. What if we faked it up so that they were all on the same day (or, at least, consecutive days, to account for the ones after midnight)?\nLet’s look at our dataframe again:\n\nsleep\n\n\n\n  \n\n\n\nThe rise.time values are all a.m., and on consecutive days, so if we subtract consecutive numbers of days from the bed.times, we’ll put them all on appropriate days too:\n\nsleep %&gt;% mutate(time_of_bed = bed.time - ddays(0:6))\n\n\n\n  \n\n\n\nThese are all around the midnight at the end of September 1, so some of them are in the early hours of September 2. Now, if we make a histogram of those:\n\nsleep %&gt;% mutate(time_of_bed = bed.time - ddays(0:6)) %&gt;% \nggplot(aes(x = time_of_bed)) + geom_histogram(bins = 5)\n\n\n\n\nNow the \\(x\\)-axis formatting looks like a time, and spills seamlessly into the next day. (There was no real range of dates, so the formatting is of the times only.)\nOne more embellishment, idea from here:\n\nsleep %&gt;% mutate(time_of_bed = bed.time - ddays(0:6)) %&gt;% \nggplot(aes(x = time_of_bed)) + geom_histogram(bins = 5) +\nscale_x_datetime(date_labels = \"%l:%M %p\")\n\n\n\n\nThe scale_x and scale_y functions customize the \\(x\\) and \\(y\\) axes respectively. Inside date_labels go some codes that say what time units you want to display: in this case, the 12-hour hours, the minutes, and whether the time is AM or PM. The codes come from a function called strftime, and a full list is here. Alternatively, you can look up the help for R’s function of the same name with ?strftime.19\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "dates-and-times.html#footnotes",
    "href": "dates-and-times.html#footnotes",
    "title": "19  Dates and times",
    "section": "",
    "text": "In fact, it is believed that wolves help keep caribou herds strong by preventing over-population: that is, the weakest caribou are the ones taken by wolves.↩︎\nThe survey is always taken in the fall, but the date varies.↩︎\nCounting animals in a region, especially rare, hard-to-find animals, is a whole science in itself. Our data are probably themselves estimates (with some uncertainty).↩︎\nThis is an example of R’s so-called “recycling rules”.↩︎\nIn fact, it is believed that wolves help keep caribou herds strong by preventing over-population: that is, the weakest caribou are the ones taken by wolves.↩︎\nThe survey is always taken in the fall, but the date varies.↩︎\nCounting animals in a region, especially rare, hard-to-find animals, is a whole science in itself. Our data are probably themselves estimates (with some uncertainty).↩︎\nThe only way this would fail is if every first name had the same number of letters in it; then the space between first name and initial of last name would be in the same place in every line.↩︎\nNot a surprise since Denali National Park is in Alaska.↩︎\nIt’s actually not really living on the edge, because if it doesn’t work, you go back and read the data in from the file again.↩︎\nThis wiggles more than I would like, with such a small number of observations. Try putting something like span=2 in the smooth to make it less wiggly.↩︎\nI have to say that I didn’t know that until just now.↩︎\nWhich means, if you stop to think about it, that there are actually about 40 times more caribou than wolves.↩︎\nAnd all too often with Excel (spit).↩︎\nThe computer scientists among you will note that I shouldn’t have done this, because diff1 through diff3 are double-precision decimal numbers, so I should have tested their equality with lenfol and los by working out the absolute differences and testing whether they were all small. On consulting the help for all.equal, though, I find that it does work properly, because it actually tests whether the things being compared differ by less than a quantity tolerance which defaults to 0.000000015, and if they do it calls them equal. This is all tied in with the difference between integers and decimal numbers as they are represented on a computer: exactly and approximately, respectively. A double-precision number has about 16 significant digits of accuracy; equal things won’t have all 16 digits equal, most likely, but they would be expected to have at least 8 of those digits the same. CSCA08 stuff, I imagine. This is where you can casually toss around terms like “machine epsilon”. Oh! I just realized something. You know how very very small P-values are shown in R as &lt;2.2e-16? That’s the machine epsilon. Anything smaller than that is indistinguishable from zero, and you can’t have a P-value be exactly zero. The default tolerance I mentioned above is the square root of this, which is normally used for such things.↩︎\nWhich one you see will depend on which type of output you are looking at. In your notebook you will probably see the POSIXCT thing, but in your HTML or PDF output it will probably say ``dttm’’.↩︎\nThe piece in quotes comes word-for-word from the source: it is exactly what the author said. Except that the author also talks about dates, which don’t concern us here, so I removed that bit and replaced it with the three dots, called an “ellipsis”, to show that the author said some extra stuff that I didn’t quote. I checked that what remained does actually still capture what the author said. Extra-in-note: an ellipsis is not to be confused with the conic section called an ellipse, and these three dots are not to be confused with the three dots in an R function, where they mean “anything else that was passed in to the function”. Both uses of the three dots capture the idea of “something was missed out”.↩︎\nMake sure you use “hour” and not “hours” as I did the first time. That computes the total number of hours between the zero date of Jan 1, 1970 and the time given, and so is way too large to be an answer here!↩︎\nConfusingly, uppercase I and lowercase l not only look the same, but they also both display the 12-hour hour. The former adds a zero to the front if the hour is a single digit, and the latter does not. All the hours here have two digits, though, so it comes out the same whichever you use.↩︎"
  },
  {
    "objectID": "functions.html#making-some-r-functions",
    "href": "functions.html#making-some-r-functions",
    "title": "20  Functions",
    "section": "20.1 Making some R functions",
    "text": "20.1 Making some R functions\nLet’s write some simple R functions to convert temperatures, and later to play with text.\n\nA temperature in Celsius is converted to one in Kelvin by adding 273.15. (A temperature of \\(-273.15\\) Celsius, 0 Kelvin, is the “absolute zero” temperature that nothing can be colder than.) Write a function called c_to_k that converts an input Celsius temperature to one in Kelvin, and test that it works.\nWrite a function to convert a Fahrenheit temperature to Celsius. The way you do that is to subtract 32 and then multiply by \\(5/9\\).\nUsing the functions you already wrote, write a function to convert an input Fahrenheit temperature to Kelvin.\nRewrite your Fahrenheit-to-Celsius convertor to take a suitable default value and check that it works as a default.\nWhat happens if you feed your Fahrenheit-to-Celsius convertor a vector of Fahrenheit temperatures? What if you use it in a mutate?\nWrite another function called wrap that takes two arguments: a piece of text called text, which defaults to hello, and another piece of text called outside, which defaults to *. The function returns text with the text outside placed before and after, so that calling the function with the defaults should return *hello*. To do this, you can use str_c from stringr (loaded with the tidyverse) which places its text arguments side by side and glues them together into one piece of text. Test your function briefly.\nWhat happens if you want to change the default outside but use the default for text? How do you make sure that happens? Explore.\nWhat happens if you feed your function wrap a vector for either of its arguments? What about if you use it in a mutate?"
  },
  {
    "objectID": "functions.html#the-collatz-sequence",
    "href": "functions.html#the-collatz-sequence",
    "title": "20  Functions",
    "section": "20.2 The Collatz sequence",
    "text": "20.2 The Collatz sequence\nThe Collatz sequence is a sequence of integers \\(x_1, x_2, \\ldots\\) defined in a deceptively simple way: if \\(x_n\\) is the current term of the sequence, then \\(x_{n+1}\\) is defined as \\(x_n/2\\) if \\(x_n\\) is even, and \\(3x_n+1\\) if \\(x_n\\) is odd. We are interested in understanding how this sequence behaves; for example, what happens to it as \\(n\\) gets large, for different choices of the first term \\(x_1\\)? We will explore this numerically with R; the ambitious among you might like to look into the mathematics of it.\n\nWhat happens to the sequence when it reaches 4? What would be a sensible way of defining where it ends? Explain briefly.\nWrite an R function called is_odd that returns TRUE if its input is an odd number and FALSE if it is even (you can assume that the input is an integer and not a decimal number). To do that, you can use the function %% where a %% b is the remainder when a is divided by b. To think about oddness or evenness, consider the remainder when you divide by 2.\nWrite an R function called hotpo11 that takes an integer as input and returns the next number in the Collatz sequence. To do this, use the function you just wrote that determines whether a number is even or odd.\nNow write a function hotpo that will return the whole Collatz sequence for an input \\(x_1\\). For this, assume that you will eventually get to 1.\nWrite two (very small) functions that take an entire sequence as input and return (i) the length of the sequence and (ii) the maximum value it attains.\nMake a data frame consisting of the values 11 through 20, and, using tidyverse ideas, obtain a data frame containing the Collatz sequences starting at each of those values, along with their lengths and their maximum values. Which sequence is longest? Which one goes up highest?"
  },
  {
    "objectID": "functions.html#coefficient-of-variation",
    "href": "functions.html#coefficient-of-variation",
    "title": "20  Functions",
    "section": "20.3 Coefficient of Variation",
    "text": "20.3 Coefficient of Variation\nThe coefficient of variation of a vector x is defined as the standard deviation of x divided by the mean of x.\n\nWrite a function called cv that calculates the coefficient of variation of its input and returns the result. You should use base R’s functions that reliably compute the pieces that you need.\nUse your function to find the coefficient of variation of the set of integers 1 through 5.\nDefine a vector as follows:\n\n\nv &lt;- c(-2.8, -1.8, -0.8, 1.2, 4.2)\n\nWhat is its coefficient of variation, according to your function? Does this make sense? Why did this happen? Explain briefly.\n\nMost people only calculate a coefficient of variation if there are no negative numbers. Rewrite your function so that it gives an error if there are any negative numbers in the input, and test it with the vector v above. Hint: you might need to add error=TRUE to your chunk header to allow your document to preview/knit (inside the curly brackets at the top of the chunk, after a comma)."
  },
  {
    "objectID": "functions.html#rescaling",
    "href": "functions.html#rescaling",
    "title": "20  Functions",
    "section": "20.4 Rescaling",
    "text": "20.4 Rescaling\nSuppose we have this vector of values:\n\nz &lt;- c(10, 14, 11)\nz\n\n[1] 10 14 11\n\n\nWe want to scale these so that the smallest value is 0 and the largest is 1. We are going to be doing this a lot, so we are going to write a function that will work for any input.\n\nUsing a copy of my z, work out min(z) and max(z). What do they do? Explain (very) briefly.\nWhat do these lines of code do, using the same z that I had? Run them and see, and describe briefly what s contains.\n\n\nlo &lt;- min(z)\nhi &lt;- max(z)\ns &lt;- (z - lo) / (hi - lo)\ns\n\n\nWrite a function called rescale that implements the calculation above, for any input vector called x. (Note that I changed the name.)\nTest your function on my z, and on another vector of your choosing. Explain briefly why the answer you get from your vector makes sense.\nWhat happens if your input to rescale is a vector of numbers all the same? Give an example. Rewrite your function to intercept this case and give a helpful error message.\nMake a dataframe (containing any numeric values), and in it create a new column containing the rescaled version of one of its columns, using your function. Show your result.\nWe might want to rescale the input not to be between 0 and 1, but between two values a and b that we specify as input. If a and/or b are not given, we want to use the values 0 for a and 1 for b. Rewrite your function to rescale the input to be between a and b instead of 0 and 1. Hint: allow your function to produce values between 0 and 1 as before, and then note that if all the values in a vector s are between 0 and 1, then all the values in a+(b-a)*s are between \\(a\\) and \\(b\\).\nTest your new function two or more times, on input where you know or can guess what the output is going to be. In each case, explain briefly why your output makes sense.\n\nMy solutions follow:"
  },
  {
    "objectID": "functions.html#making-some-r-functions-1",
    "href": "functions.html#making-some-r-functions-1",
    "title": "20  Functions",
    "section": "20.5 Making some R functions",
    "text": "20.5 Making some R functions\nLet’s write some simple R functions to convert temperatures, and later to play with text.\n\nA temperature in Celsius is converted to one in Kelvin by adding 273.15. (A temperature of \\(-273.15\\) Celsius, 0 Kelvin, is the “absolute zero” temperature that nothing can be colder than.) Write a function called c_to_k that converts an input Celsius temperature to one in Kelvin, and test that it works.\n\nSolution\nThis is mostly an exercise in structuring your function correctly. Let’s call the input C (uppercase C, since lowercase c has a special meaning to R):\n\nc_to_k &lt;- function(C) {\n  C + 273.15\n}\nc_to_k(0)\n\n[1] 273.15\n\nc_to_k(20)\n\n[1] 293.15\n\n\nThis is the simplest way to do it: the last line of the function, if calculated but not saved, is the value that gets returned to the outside world. The checks suggest that it worked.\nIf you’re used to Python or similar, you might prefer to calculate the value to be returned and then return it. You can do that in R too:\n\nc_to_k &lt;- function(C) {\n  K &lt;- C + 273.15\n  return(K)\n}\nc_to_k(0)\n\n[1] 273.15\n\nc_to_k(20)\n\n[1] 293.15\n\n\nThat works just as well, and for the rest of this question, you can go either way.2\n\\(\\blacksquare\\)\n\nWrite a function to convert a Fahrenheit temperature to Celsius. The way you do that is to subtract 32 and then multiply by \\(5/9\\).\n\nSolution\nOn the model of the previous one, we should call this f_to_c. I’m going to return the last line, but you can save the calculated value and return that instead:\n\nf_to_c &lt;- function(F) {\n  (F - 32) * 5 / 9\n}\nf_to_c(32)\n\n[1] 0\n\nf_to_c(50)\n\n[1] 10\n\nf_to_c(68)\n\n[1] 20\n\n\nAmericans are very good at saying things like “temperatures in the 50s”, which don’t mean much to me, so I like to have benchmarks to work with: these are the Fahrenheit versions of 0, 10, and 20 Celsius.\nThus “in the 50s” means “between about 10 and 15 Celsius”.\n\\(\\blacksquare\\)\n\nUsing the functions you already wrote, write a function to convert an input Fahrenheit temperature to Kelvin.\n\nSolution\nThis implies that you can piggy-back on the functions you just wrote, which goes as below. First you convert the Fahrenheit to Celsius, and then you convert that to Kelvin. (This is less error-prone than trying to use algebra to get a formula for this conversion and then implementing that):\n\nf_to_k &lt;- function(F) {\n  C &lt;- f_to_c(F)\n  K &lt;- c_to_k(C)\n  return(K)\n}\nf_to_k(32)\n\n[1] 273.15\n\nf_to_k(68)\n\n[1] 293.15\n\n\nThese check because in Celsius they are 0 and 20 and we found the Kelvin equivalents of those to be these values earlier.\nI wrote this one with a return because I thought it made the structure clearer: run one function, save the result, run another function, save the result, then return what you’ve got.\n\\(\\blacksquare\\)\n\nRewrite your Fahrenheit-to-Celsius convertor to take a suitable default value and check that it works as a default.\n\nSolution\nYou can choose any default you like. I’ll take a default of 68 (what I would call “a nice day”):\n\nf_to_c &lt;- function(F = 68) {\n  (F - 32) * 5 / 9\n}\nf_to_c(68)\n\n[1] 20\n\nf_to_c()\n\n[1] 20\n\n\nThe change is in the top line of the function. You see the result: if we run it without an input, we get the same answer as if the input had been 68.\n\\(\\blacksquare\\)\n\nWhat happens if you feed your Fahrenheit-to-Celsius convertor a vector of Fahrenheit temperatures? What if you use it in a mutate?\n\nSolution\nTry it and see:\n\ntemps &lt;- seq(30, 80, 10)\ntemps\n\n[1] 30 40 50 60 70 80\n\nf_to_c(temps)\n\n[1] -1.111111  4.444444 10.000000 15.555556 21.111111 26.666667\n\n\nEach of the Fahrenheit temperatures gets converted into a Celsius one. This is perhaps more useful in a data frame, thus:\n\ntibble(temps = seq(30, 80, 10)) %&gt;%\n  mutate(celsius = f_to_c(temps))\n\n\n\n  \n\n\n\nAll the temperatures are side-by-side with their equivalents.\nHere’s another way to do the above:\n\ntemps &lt;- seq(30, 80, 10)\ntemps %&gt;%\n  enframe(value = \"fahrenheit\") %&gt;%\n  mutate(celsius = f_to_c(temps))\n\n\n\n  \n\n\n\nenframe creates a two-column data frame out of a vector (like temps). A vector can have “names”, in which case they’ll be used as the name column; the values will go in a column called value unless you rename it, as I did.\n\\(\\blacksquare\\)\n\nWrite another function called wrap that takes two arguments: a piece of text called text, which defaults to hello, and another piece of text called outside, which defaults to *. The function returns text with the text outside placed before and after, so that calling the function with the defaults should return *hello*. To do this, you can use str_c from stringr (loaded with the tidyverse) which places its text arguments side by side and glues them together into one piece of text. Test your function briefly.\n\nSolution\nThis:\n\nwrap &lt;- function(text = \"hello\", outside = \"*\") {\n  str_c(outside, text, outside)\n}\n\nI can run this with the defaults:\n\nwrap()\n\n[1] \"*hello*\"\n\n\nor with text of my choosing:\n\nwrap(\"goodbye\", \"_\")\n\n[1] \"_goodbye_\"\n\n\nI think that’s what I meant by “test briefly”.\n\\(\\blacksquare\\)\n\nWhat happens if you want to change the default outside but use the default for text? How do you make sure that happens? Explore.\n\nSolution\nThe obvious thing is this, which doesn’t work:\n\nwrap(\"!\")\n\n[1] \"*!*\"\n\n\nThis takes text to be !, and outside to be the default. How do we get outside to be ! instead? The key is to specify the input by name:\n\nwrap(outside = \"!\")\n\n[1] \"!hello!\"\n\n\nThis correctly uses the default for text.\nIf you specify inputs without names, they are taken to be in the order that they appear in the function definition. As soon as they get out of order, which typically happens by using the default for something early in the list, as we did here for text, you have to specify names for anything that comes after that. These are the names you put on the function’s top line.\nYou can always use names:\n\nwrap(text = \"thing\", outside = \"**\")\n\n[1] \"**thing**\"\n\n\nand if you use names, they don’t even have to be in order:\n\nwrap(outside = \"!?\", text = \"fred\")\n\n[1] \"!?fred!?\"\n\n\n\\(\\blacksquare\\)\n\nWhat happens if you feed your function wrap a vector for either of its arguments? What about if you use it in a mutate?\n\nSolution\nLet’s try:\n\nmytext &lt;- c(\"a\", \"b\", \"c\")\nwrap(text = mytext)\n\n[1] \"*a*\" \"*b*\" \"*c*\"\n\n\n\nmyout &lt;- c(\"*\", \"!\")\nwrap(outside = myout)\n\n[1] \"*hello*\" \"!hello!\"\n\n\nIf one of the inputs is a vector, the other one gets “recycled” as many times as the vector is long. What if they’re both vectors?\n\nmytext2 &lt;- c(\"a\", \"b\", \"c\", \"d\")\nwrap(mytext2, myout)\n\nError in `str_c()`:\n! Can't recycle `..1` (size 2) to match `..2` (size 4).\n\n\nThis gives an error because str_c won’t let you recycle things that are both vectors.\nThe mutate thing is easier, because all the columns in a data frame have to be the same length. LETTERS is a vector with the uppercase letters in it:\n\ntibble(mytext = LETTERS[1:6], myout = c(\"*\", \"**\", \"!\", \"!!\", \"_\", \"__\")) %&gt;%\n  mutate(newthing = wrap(mytext, myout))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "functions.html#the-collatz-sequence-1",
    "href": "functions.html#the-collatz-sequence-1",
    "title": "20  Functions",
    "section": "20.6 The Collatz sequence",
    "text": "20.6 The Collatz sequence\nThe Collatz sequence is a sequence of integers \\(x_1, x_2, \\ldots\\) defined in a deceptively simple way: if \\(x_n\\) is the current term of the sequence, then \\(x_{n+1}\\) is defined as \\(x_n/2\\) if \\(x_n\\) is even, and \\(3x_n+1\\) if \\(x_n\\) is odd. We are interested in understanding how this sequence behaves; for example, what happens to it as \\(n\\) gets large, for different choices of the first term \\(x_1\\)? We will explore this numerically with R; the ambitious among you might like to look into the mathematics of it.\n\nWhat happens to the sequence when it reaches 4? What would be a sensible way of defining where it ends? Explain briefly.\n\nSolution\nWhen the sequence reaches 4 (that is, when its current term is 4), the next term is 2 and the one after that is 1. Then the following term is 4 again (\\((3 \\times 1)+1\\)) and then it repeats indefinitely, \\(4, 2, 1, 4, 2, 1, \\ldots\\). I think a sensible way to define where the sequence ends is to say “when it reaches 1”, since if you start at 2 you’ll never reach 4 (so “when it reaches 4” won’t work), and it seems at least plausible that it will hit the cycle 4, 2, 1 sometime.\n\\(\\blacksquare\\)\n\nWrite an R function called is_odd that returns TRUE if its input is an odd number and FALSE if it is even (you can assume that the input is an integer and not a decimal number). To do that, you can use the function %% where a %% b is the remainder when a is divided by b. To think about oddness or evenness, consider the remainder when you divide by 2.\n\nSolution\nLet’s try this out. For example, 5 is odd and 6 is even, so\n\n5 %% 2\n\n[1] 1\n\n6 %% 2\n\n[1] 0\n\n\nWhen a number is odd, its remainder on dividing by 2 is 1, and when even, the remainder is 0. There is an additional shortcut here in that 1 is the numeric value of TRUE and 0 of FALSE, so all we have to do is calculate the remainder on dividing by 2, turn it into a logical, and return it:\n\nis_odd &lt;- function(x) {\n  r &lt;- x %% 2\n  as.logical(r)\n}\n\nYou probably haven’t seen as.logical before, but it’s the same idea as as.numeric: turn something that looks like a TRUE or FALSE into something that actually is.\nWe should test it:\n\nis_odd(19)\n\n[1] TRUE\n\nis_odd(12)\n\n[1] FALSE\n\nis_odd(0)\n\n[1] FALSE\n\n\n0 is usually considered an even number, so this is good.\n\\(\\blacksquare\\)\n\nWrite an R function called hotpo13 that takes an integer as input and returns the next number in the Collatz sequence. To do this, use the function you just wrote that determines whether a number is even or odd.\n\nSolution\nThe logic is “if the input is odd, return 3 times it plus 1, otherwise return half of it”. The R structure is an if-then-else:\n\nhotpo1 &lt;- function(x) {\n  if (is_odd(x)) 3 * x + 1 else x / 2\n}\n\nIn R, the condition that is tested goes in brackets, and then if the value-if-true and the value-if-false are single statements, you just type them. (If they are more complicated than that, you put them in curly brackets.) Now you see the value of writing is_odd earlier; this code almost looks like the English-language description of the sequence. If we had not written is_odd before, the condition would have looked something like\n\nif (x %% 2 == 1) 3 * x + 1 else x / 2\n\nwhich would have been a lot harder to read.\nAll right, let’s try that out:\n\nhotpo1(4)\n\n[1] 2\n\nhotpo1(7)\n\n[1] 22\n\nhotpo1(24)\n\n[1] 12\n\n\nThat looks all right so far.\n\\(\\blacksquare\\)\n\nNow write a function hotpo that will return the whole Collatz sequence for an input \\(x_1\\). For this, assume that you will eventually get to 1.\n\nSolution\nThis is a loop, but not a for loop (or something that we could do rowwise), because we don’t know how many times we have to go around. This is the kind of thing that we should use a while loop for: “keep going while a condition is true”. In this case, we should keep going if we haven’t reached 1 yet. If we haven’t reached 1, we should generate the next value of the sequence and glue it onto what we have so far. To initialize the sequence, we start with the input value. There is an R trick to glue a value onto the sequence, which is to use c with a vector and a value, and save it back into the vector:\n\nhotpo &lt;- function(x) {\n  sequence &lt;- x\n  term &lt;- x\n  while (term &gt; 1) {\n    term &lt;- hotpo1(term)\n    sequence &lt;- c(sequence, term)\n  }\n  sequence\n}\n\nI use term to hold the current term of the sequence, and overwrite it with the next one (since I don’t need the old one any more).\nDoes it work?\n\nhotpo(4)\n\n[1] 4 2 1\n\nhotpo(12)\n\n [1] 12  6  3 10  5 16  8  4  2  1\n\nhotpo(97)\n\n  [1]   97  292  146   73  220  110   55  166   83  250  125  376  188   94   47\n [16]  142   71  214  107  322  161  484  242  121  364  182   91  274  137  412\n [31]  206  103  310  155  466  233  700  350  175  526  263  790  395 1186  593\n [46] 1780  890  445 1336  668  334  167  502  251  754  377 1132  566  283  850\n [61]  425 1276  638  319  958  479 1438  719 2158 1079 3238 1619 4858 2429 7288\n [76] 3644 1822  911 2734 1367 4102 2051 6154 3077 9232 4616 2308 1154  577 1732\n [91]  866  433 1300  650  325  976  488  244  122   61  184   92   46   23   70\n[106]   35  106   53  160   80   40   20   10    5   16    8    4    2    1\n\n\n97 is a wild ride, but it does eventually get to 1.\nExtra: where I originally saw this, which was “Metamagical Themas” by Douglas Hofstadter, he was illustrating the programming language Lisp and the process of recursion, whereby you define a function in terms of itself. This one is a natural for that, because the Collatz sequence starting at \\(x\\) is \\(x\\) along with the Collatz sequence starting at the next term. For example, if you start at 12, the next term is 6, so that the Collatz sequence starting at 12 is 12 followed by the Collatz sequence starting at 6. There is no dependence any further back. You can do recursion in R also; there is no problem with a function calling itself:\n\nhotpo_rec &lt;- function(x) {\n  if (x == 1) 1 else c(x, hotpo_rec(hotpo1(x)))\n}\n\nRecursive functions have two parts: a “base case” that says how you know you are done (the 1 here), and a “recursion” that says how you move to a simpler case, here working out the next term, getting the whole sequence for that, and gluing the input onto the front. It seems paradoxical that you define a function in terms of itself, but what you are doing is calling a simpler sequence, in this case one that is length one shorter than the sequence for the original input. Thus, we hope,4 we will eventually reach 1.\nDoes it work?\n\nhotpo_rec(12)\n\n [1] 12  6  3 10  5 16  8  4  2  1\n\nhotpo_rec(97)\n\n  [1]   97  292  146   73  220  110   55  166   83  250  125  376  188   94   47\n [16]  142   71  214  107  322  161  484  242  121  364  182   91  274  137  412\n [31]  206  103  310  155  466  233  700  350  175  526  263  790  395 1186  593\n [46] 1780  890  445 1336  668  334  167  502  251  754  377 1132  566  283  850\n [61]  425 1276  638  319  958  479 1438  719 2158 1079 3238 1619 4858 2429 7288\n [76] 3644 1822  911 2734 1367 4102 2051 6154 3077 9232 4616 2308 1154  577 1732\n [91]  866  433 1300  650  325  976  488  244  122   61  184   92   46   23   70\n[106]   35  106   53  160   80   40   20   10    5   16    8    4    2    1\n\n\nIt does.\nRecursive functions are often simple to understand, but they are not always very efficient. They can take a lot of memory, because they have to handle the intermediate calls to the function, which they have to save to use later (in the case of hotpo_rec(97) there are a lot of those). Recursive functions are often paired with a technique called “memoization”, where each time you calculate the function’s value, you save it in another array. The first thing you do in the recursive function is to check whether you already have the answer, in which case you just look it up and return it. It was a lot of work here to calculate the sequence from 97, but if we had saved the results, we would already have the answers for 292, 146, 73, 220 and so on, and getting those later would be a table lookup rather than another recursive calculation.\n\\(\\blacksquare\\)\n\nWrite two (very small) functions that take an entire sequence as input and return (i) the length of the sequence and (ii) the maximum value it attains.\n\nSolution\nThese are both one-liners. Call the input whatever you like:\n\nhotpo_len &lt;- function(sequence) length(sequence)\nhotpo_max &lt;- function(sequence) max(sequence)\n\nBecause they are one-liners, you don’t even need the curly brackets, although there’s no problem if they are there.\nTesting:\n\nhotpo_len(hotpo(12))\n\n[1] 10\n\nhotpo_max(hotpo(97))\n\n[1] 9232\n\n\nThis checks with what we had before.\n\\(\\blacksquare\\)\n\nMake a data frame consisting of the values 11 through 20, and, using tidyverse ideas, obtain a data frame containing the Collatz sequences starting at each of those values, along with their lengths and their maximum values. Which sequence is longest? Which one goes up highest?\n\nSolution\nThis one uses rowwise ideas:5\n\ntibble(x = 11:20) %&gt;%\n  rowwise %&gt;% \n  mutate(sequence = list(hotpo(x))) %&gt;%\n  mutate(length = hotpo_len(sequence)) %&gt;%\n  mutate(high = hotpo_max(sequence))\n\n\n\n  \n\n\n\nFirst, we obtain a list-column containing the sequences (which is why its calculation needs a list around it), then two ordinary columns of their lengths and their maximum values.\nThe sequences for 18 and 19 are the longest, but the sequence for 15 goes up the highest.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "functions.html#coefficient-of-variation-1",
    "href": "functions.html#coefficient-of-variation-1",
    "title": "20  Functions",
    "section": "20.7 Coefficient of Variation",
    "text": "20.7 Coefficient of Variation\nThe coefficient of variation of a vector x is defined as the standard deviation of x divided by the mean of x.\n\nWrite a function called cv that calculates the coefficient of variation of its input and returns the result. You should use base R’s functions that reliably compute the pieces that you need.\n\nSolution\nI like to make a function skeleton first to be sure I remember all the pieces. You can do this by making a code chunk, typing fun, waiting a moment, and selecting the “snippet” that is offered to you. That gives you this, with your cursor on name:\n\nname &lt;- function(variables) {\n\n}\n\nGive your function a name (I asked you to use cv), hit tab, enter a name like x for the input, hit tab again, and then fill in the body of the function, to end up with something like this:\n\ncv &lt;- function(x) {\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\n\nI think this is best, for reasons discussed below.\nSome observations:\n\nin the last line of the function, you calculate something, and that something is the thing that gets returned. You do not need to save it in a variable, because then you have to return that variable by having its name alone on the last line.\nR has a return() function, but it is bad style to use it to return a value at the end of a function. The time to use it is when you test something early in the function and want to return early with a value like zero or missing without doing any actual calculation. Looking ahead a bit, you might want to return “missing” if the mean is zero before dividing by it, which you could do like this, this being good style:\n\n\ncv2 &lt;- function(x) {\nmean_x &lt;- mean(x)\nif (mean_x == 0) return(NA)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\n\n\nuse the built-in mean and sd rather than trying to roll your own, because they have been tested by many users over a long time and they work. That’s what I meant by “pieces” in the question.\nyou might be tempted to do something like this:\n\n\ncv3 &lt;- function(x) {\nsd(x)/mean(x)\n}\n\nThis will work, but it is not a good habit to get into, and thus not best as an answer to this question. This one line of code says three things: “work out the SD of x”, “work out the mean of x”, and “divide them by each other”. It is much clearer, for anyone else reading the code (including yourself in six months when you have forgotten what you were thinking) to have the three things one per line, so that anyone reading the code sees that each line does one thing and what that one thing is. There is a reason why production code and code golf are two very different things. Code that is easy to read is also easy to maintain, either by you or others.\n\nusing something other than mean and sd as the names of your intermediate results is a good idea because these names are already used by R (as the names of the functions that compute the mean and SD). Redefining names that R uses can make other code behave unpredictably and cause hard-to-find bugs.6\n\n\\(\\blacksquare\\)\n\nUse your function to find the coefficient of variation of the set of integers 1 through 5.\n\nSolution\nThis can be as simple as\n\ncv(1:5)\n\n[1] 0.5270463\n\n\nor, equally good, define this into a vector first:\n\ny &lt;- 1:5\ncv(y)\n\n[1] 0.5270463\n\n\nor, displaying a little less understanding, type the five numbers into the vector first (or directly as input to the function):\n\ny &lt;- c(1, 2, 3, 4, 5)\ncv(y)\n\n[1] 0.5270463\n\ncv(c(1, 2, 3, 4, 5))\n\n[1] 0.5270463\n\n\n\\(\\blacksquare\\)\n\nDefine a vector as follows:\n\n\nv &lt;- c(-2.8, -1.8, -0.8, 1.2, 4.2)\n\nWhat is its coefficient of variation, according to your function? Does this make sense? Why did this happen? Explain briefly.\nSolution\nTry it and see:\n\ncv(v)\n\n[1] 6.248491e+16\n\n\nA very large number, much bigger than any of the data values; with these human-sized numbers, we’d expect a human-sized coefficient of variation as well.\nWhat actually happened was that the mean of v is this:\n\nmean(v)\n\n[1] 4.440892e-17\n\n\nZero, or close enough, so that in calculating the coefficient of variation, we divided by something that was (almost) zero and got a result that was (almost) infinite.7\nThe possibility of getting a zero mean is why most people only calculate a coefficient of variation if all of the numbers are positive, which brings us to the next part:\n\\(\\blacksquare\\)\n\nMost people only calculate a coefficient of variation if there are no negative numbers. Rewrite your function so that it gives an error if there are any negative numbers in the input, and test it with the vector v above. Hint: you might need to add error=TRUE to your chunk header to allow your document to preview/knit (inside the curly brackets at the top of the chunk, after a comma).\n\nSolution\nThis is a case for stopifnot, or of if coupled with stop. Check this up front, as the first thing you do before you calculate anything else. As to what to check, there are several possibilities:\n\nstop if any of the numbers are negative\ncontinue if all of the numbers are positive\nstop if the smallest number is negative (the smallest number is negative if and only if not all the numbers are positive)\n\nR has functions any and all that do what you’d expect:\n\nw &lt;- 1:5\nw\n\n[1] 1 2 3 4 5\n\nany(w&gt;3.5)\n\n[1] TRUE\n\nall(w&lt;4.5)\n\n[1] FALSE\n\n\nAre there any numbers greater than 3.5 (yes, 4 and 5); are all the numbers less than 4.5 (no, 5 isn’t).\nCite your sources for these if you use either of them, since this is the first place in the course that I’m mentioning either of them.\nRemember that if you use stopifnot, the condition that goes in there is what has to be true if the function is to run; if you use if and stop, the condition is what will stop the function running. With that in mind, I would code my three possibilities above this way. First off, here’s the original:\n\ncv &lt;- function(x) {\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\n\nthen, stop if any of the numbers are negative:\n\ncv &lt;- function(x) {\nif (any(x&lt;0)) stop(\"A value is negative\")\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): A value is negative\n\n\ncontinue if all the numbers are positive\n\ncv &lt;- function(x) {\nstopifnot(all(x&gt;0))\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): all(x &gt; 0) is not TRUE\n\n\nstop if the smallest value is negative\n\ncv &lt;- function(x) {\nif (min(x)&lt;0) stop(\"Smallest value is negative\")\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): Smallest value is negative\n\n\nThere are (at least) three other possibilities: you can negate the logical condition and interchange if/stop and stopifnot, thus (at the expense of some clarity of reading):\ncontinue if it is not true that any of the numbers are negative\n\ncv &lt;- function(x) {\nstopifnot(!any(x&lt;0))\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): !any(x &lt; 0) is not TRUE\n\n\n(you might be thinking of De Morgan’s laws here)\nstop if it is not true that all the numbers are positive\n\ncv &lt;- function(x) {\nif (!all(x&gt;0)) stop(\"Not all values are positive\")\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): Not all values are positive\n\n\ncontinue if the smallest value is not negative\n\ncv &lt;- function(x) {\nstopifnot(min(x)&gt;=0)\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): min(x) &gt;= 0 is not TRUE\n\n\nor another way to do the last one, a more direct negation of the condition, which at my guess needs some extra brackets:\n\ncv &lt;- function(x) {\nstopifnot(!(min(x)&lt;0))\nmean_x &lt;- mean(x)\nsd_x &lt;- sd(x)\nsd_x/mean_x\n}\ncv(v)\n\nError in cv(v): !(min(x) &lt; 0) is not TRUE\n\n\nThis one is hard to parse: what does that last message mean? I would take a negative off each side and read it as “min of x is negative is TRUE”, but that takes extra effort.\nI said that last one needed some extra brackets. This is, I thought, to get the order of operations right (operator precedence); it turns out not to matter because “not” has lower precedence than most other things, so that these do actually work (the “not” is evaluated after the less-than and the other things, so last of all here, even though it appears to be “glued” to the min):\n\n!min(v)&lt;0\n\n[1] FALSE\n\n!min(1:5)&lt;0\n\n[1] TRUE\n\n\nSee this for details. See especially the second set of examples, the ones beginning with “Special operators”, and see especially-especially the comment at the bottom of these examples! That is to say, you should put in the extra brackets unless you also make the case that they are not needed, because anyone reading your code is guaranteed to be confused by it when they read it (including you in six months, because you will not remember the operator priority of “not”).\nMy take is that one of the first three of the seven possibilities for coding stopifnot or if with stop is the best, since these more obviously encode the condition for continuing or stopping as appropriate. There are two things here: one is that you have to get the code right, but the second is that you have to get the code clear, so that it is obvious to anyone reading it that it does the right thing (once again, this includes you in six months). On that score, the first three alternatives are a direct expression of what you want to achieve, and the last four make it look as if you found a way of coding it that worked and stopped there, without thinking about whether there were any other, clearer or more expressive, possibilities.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "functions.html#rescaling-1",
    "href": "functions.html#rescaling-1",
    "title": "20  Functions",
    "section": "20.8 Rescaling",
    "text": "20.8 Rescaling\nSuppose we have this vector of values:\n\nz &lt;- c(10, 14, 11)\nz\n\n[1] 10 14 11\n\n\nWe want to scale these so that the smallest value is 0 and the largest is 1. We are going to be doing this a lot, so we are going to write a function that will work for any input.\n\nUsing a copy of my z, work out min(z) and max(z). What do they do? Explain (very) briefly.\n\nSolution\nSimply this – define z first:\n\nz &lt;- c(10, 14, 11)\nmin(z)\n\n[1] 10\n\nmax(z)\n\n[1] 14\n\n\nThey are respectively (and as you would guess) the smallest and largest of the values in z. (A nice gentle warmup, but I wanted to get you on the right track for what is coming up.)\n\\(\\blacksquare\\)\n\nWhat do these lines of code do, using the same z that I had? Run them and see, and describe briefly what s contains.\n\n\nlo &lt;- min(z)\nhi &lt;- max(z)\ns &lt;- (z - lo) / (hi - lo)\ns\n\nSolution\nHere we go:\n\nlo &lt;- min(z)\nhi &lt;- max(z)\ns &lt;- (z - lo) / (hi - lo)\ns\n\n[1] 0.00 1.00 0.25\n\n\nThe lowest value became 0, the highest 1, and the other one to something in between. Saying this shows the greatest insight.\nExtra: the reason for doing it in three steps rather than one (see below) is (i) it makes it clearer what is going on (and thus makes it less likely that you make a mistake), and (ii) it is more efficient, since my way only finds the minimum once instead of twice. Compare this approach with mine above:\n\n(z - min(z)) / (max(z) - min(z))\n\n[1] 0.00 1.00 0.25\n\n\nMore complicated with all the brackets, and two min(z). Admittedly the difference here will be thousandths of a second, but why call a function twice when you don’t have to?\n\\(\\blacksquare\\)\n\nWrite a function called rescale that implements the calculation above, for any input vector called x. (Note that I changed the name.)\n\nSolution\nWrite a function skeleton:\n\nrescale &lt;- function(x) {\n\n}\n\nand inside the curly brackets put the code from above, replacing z with x everywhere:\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\n(x - lo) / (hi - lo)\n}\n\nYou’ll need to make sure your function returns something to the outside world. Either don’t save the last line in s (as I did here), or save it in s and then return s:\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\ns &lt;- (x - lo) / (hi - lo)\ns\n}\n\nor use return() if you must, but be aware that this is bad style in R (unlike Python, where you need it). The approved way of using return in R is when you are returning something earlier than the last line of a function, for example, you are testing a simple case first and returning the value that goes with that, before getting down to the serious computation.\nExtra: in the spirit of what’s coming up below, you might check first whether the maximum and minimum are the same and return something else if that’s the case:\n\nrescale0 &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nif (lo == hi) return(0)\ns &lt;- (x - lo) / (hi - lo)\ns\n}\n\nThis is good style; in this case, if lo and hi are the same, we want to return something else (zero) to the outside world, and then we do the calculation, knowing that lo and hi are different, so that we are sure we are not dividing by zero (but I get ahead of myself).\nDoing it this way, there is something to be careful of: a function ought to return a predictable type of thing: numbers, in this case. If you have your function return text on error, like this:\n\nrescale0 &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nif (lo == hi) return(\"High and low need to be different\")\ns &lt;- (x - lo) / (hi - lo)\ns\n}\n\nthen you can get into trouble:\n\nrescale0(z)\n\n[1] 0.00 1.00 0.25\n\nrescale0(c(3,3,3))\n\n[1] \"High and low need to be different\"\n\n\nThe first one is numbers and the second one is text.\n\\(\\blacksquare\\)\n\nTest your function on my z, and on another vector of your choosing. Explain briefly why the answer you get from your vector makes sense.\n\nSolution\nOn my z:\n\nrescale(z)\n\n[1] 0.00 1.00 0.25\n\n\nThe same values as I got before, so that works.\nFor your vector, use whatever you like. I think it makes sense to have the values already in order, to make it easier to check. Here’s one possibility:\n\nw &lt;- 2:6\nw\n\n[1] 2 3 4 5 6\n\n\nand then\n\nrescale(w)\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\n\nThe smallest value is 2, which goes to zero; the largest is 6, which goes to 1, and the others are equally spaced between in both the input and the output.\nAnother possibility is to use a vector with values whose largest and smallest you can clearly see:\n\nw &lt;- c(10, 11, 100, 0, 20)\nrescale(w)\n\n[1] 0.10 0.11 1.00 0.00 0.20\n\n\nClearly the smallest value is 0 and the largest is 100. These become 0 and 1, and these particular values make it easy to see what happened: each of the other values got divided by 100.\nSome discussion is needed here, in that you need to say something convincing about why your answer is right.\nExtra: This is why I had you use a name other than z for the input to your function. The function can be used on any input, not just the z that we tested it on. There’s another R-specific reason, which is that you need to be careful about using the named inputs only. Consider this function:\n\nff &lt;- function(x) {\nx + z\n}\n\nff(10)\n\n[1] 20 24 21\n\n\nWhere did z come from? R used the z we had before, which is rather dangerous: what if we had a z lying around from some completely different work? Much better to have a function work with only inputs in the top line:\n\nff &lt;- function(x, z) {\nx + z\n}\nff(10, 3)\n\n[1] 13\n\nff(10)\n\nError in ff(10): argument \"z\" is missing, with no default\n\n\nThe first time, the two inputs are added together, but the second time it tells you it was expecting a value to use for z and didn’t see one. Much safer.\n\\(\\blacksquare\\)\n\nWhat happens if your input to rescale is a vector of numbers all the same? Give an example. Rewrite your function to intercept this case and give a helpful error message.\n\nSolution\nFirst, try it and see. Any collection of values all the same will do:\n\nrescale(c(3,3,3))\n\n[1] NaN NaN NaN\n\n\nNaN stands for “not a number”. The way we got it is that the minimum and maximum were the same, so our function ended up dividing by zero (in fact, working out zero divided by zero). This is, in R terms, not even an error, but the answer is certainly not helpful.\nThe easiest way to check inputs is to use stopifnot to express what should be true if the function is to proceed. Here, we want the maximum and minimum to be different, so:\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(hi != lo)\n(x - lo) / (hi - lo)\n}\nrescale(c(3,3,3))\n\nError in rescale(c(3, 3, 3)): hi != lo is not TRUE\n\n\nThis is much clearer: I only have to recall what my hi and lo are to see what the problem is.\nExtra 1: by calculating and saving the min and max up front, I still only need to calculate them once. If you do it this way:\n\nrescale &lt;- function(x) {\nstopifnot(max(x) != min(x))\n(x - min(x)) / (max(x) - min(x))\n}\nrescale(c(3,3,3))\n\nError in rescale(c(3, 3, 3)): max(x) != min(x) is not TRUE\n\n\nyou get a slightly more informative error message, but you have calculated the max twice and the min three times for no reason.\nExtra 2: stopifnot is shorthand for this:\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nif (hi == lo) stop(\"min and max are the same!\")\n(x - lo) / (hi - lo)\n}\nrescale(c(3,3,3))\n\nError in rescale(c(3, 3, 3)): min and max are the same!\n\n\nI didn’t show you this, so if you use stop, you must tell me where you found out about it. This is better than returning some text (see rescale0 above) or printing a message: it’s an error, so you want to make it look like an error. I am very sympathetic to being persuaded that this is better than stopifnot, because you can customize the message (and, also, you don’t have to go through the double-negative contortions of stopifnot). Another way to use stopifnot and get a customized message is this one (that I only learned about right when you were writing this Assignment):\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(\"high and low must be different\" = (hi != lo))\n(x - lo) / (hi - lo)\n}\nrescale(c(3,3,3))\n\nError in rescale(c(3, 3, 3)): high and low must be different\n\n\nThis is called a “named argument”, and the name, if given, is used as an error message.\nExtra 3: returning to my rescale0 from above:\n\nrescale0\n\nfunction(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nif (lo == hi) return(\"High and low need to be different\")\ns &lt;- (x - lo) / (hi - lo)\ns\n}\n&lt;bytecode: 0x560128ae4ba0&gt;\n\n\nthis can get you into trouble if you use it in a dataframe. This is a bit complicated, since it has to use list-columns. Here we go:\n\ntibble(x = list(z, c(3,3,3)))\n\n\n\n  \n\n\n\nJust to check that this does contain what you think it does:\n\ntibble(x = list(z, c(3,3,3))) %&gt;% unnest(x)\n\n\n\n  \n\n\n\nSo now, for each of those two input vectors, what happens when we run rescale0 on them? This is rowwise:\n\ntibble(x = list(z, c(3,3,3))) %&gt;% \n  rowwise() %&gt;% \n  mutate(ans = list(rescale0(x)))\n\n\n\n  \n\n\n\nThe first ans is a vector of 3 numbers, and the second one is one piece of text (the “error message”). I was actually surprised it got this far. So what happens when we unnest the second column?\n\ntibble(x = list(z, c(3,3,3))) %&gt;% \n  rowwise() %&gt;% \n  mutate(ans = list(rescale0(x))) %&gt;% \n  unnest(ans)\n\nError in `list_unchop()`:\n! Can't combine `x[[1]]` &lt;double&gt; and `x[[2]]` &lt;character&gt;.\n\n\nNow we get a confusing error: it’s here that combining some numbers and some text in one column of a dataframe doesn’t work. To forestall this, we need to go back and rewrite rescale0 to not mix things up. Having it return an error, as the latest version of rescale does, gives an error here too, but at least we know what it means:\n\ntibble(x = list(z, c(3,3,3))) %&gt;% \n  rowwise() %&gt;% \n  mutate(ans = list(rescale(x))) %&gt;% \n  unnest(ans)\n\nError in `mutate()`:\nℹ In argument: `ans = list(rescale(x))`.\nℹ In row 2.\nCaused by error in `rescale()`:\n! high and low must be different\n\n\nbecause this is the error we anticipated: it says “somewhere within the list-column x, specifically in its second row, is a vector where everything is the same”.\n\\(\\blacksquare\\)\n\nMake a dataframe (containing any numeric values), and in it create a new column containing the rescaled version of one of its columns, using your function. Show your result.\n\nSolution\nThis is less difficult than you might be expecting: make a dataframe with at least one numeric column, and use mutate:\n\nd &lt;- tibble(y=2:6)\nd\n\n\n\n  \n\n\n\nand then\n\nd %&gt;% mutate(s=rescale(y))\n\n\n\n  \n\n\n\nYou can supply the values for what I called y, or use random numbers. It’s easier for you to check that it has worked if your column playing the role of my y has not too many values in it.\nExtra: this is actually already in the tidyverse under the name percent_rank (“percentile ranks”):\n\nd %&gt;% mutate(s = percent_rank(y))\n\n\n\n  \n\n\n\nThe value 5, for example, is at the 75th percentile.\n\\(\\blacksquare\\)\n\nWe might want to rescale the input not to be between 0 and 1, but between two values a and b that we specify as input. If a and/or b are not given, we want to use the values 0 for a and 1 for b. Rewrite your function to rescale the input to be between a and b instead of 0 and 1. Hint: allow your function to produce values between 0 and 1 as before, and then note that if all the values in a vector s are between 0 and 1, then all the values in a+(b-a)*s are between \\(a\\) and \\(b\\).\n\nSolution\nI’m showing you my thought process in this one. The answer I want from you is the one at the end.\nSo, start by copying and pasting what you had before:\n\nrescale &lt;- function(x) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(hi != lo)\n(x - lo) / (hi - lo)\n}\n\nOn the top line, add the extra inputs and their default values. I also changed the name of my function, for reasons you’ll see later:\n\nrescale2 &lt;- function(x, a=0, b=1) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(hi != lo)\n(x - lo) / (hi - lo)\n}\n\nSave the last line, since we have to do something else with it:\n\nrescale2 &lt;- function(x, a=0, b=1) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(hi != lo)\ns &lt;- (x - lo) / (hi - lo)\n}\n\nand finally add the calculation in the hint, which we don’t need to save because we are returning it:\n\nrescale2 &lt;- function(x, a=0, b=1) {\nlo &lt;- min(x)\nhi &lt;- max(x)\nstopifnot(hi != lo)\ns &lt;- (x - lo) / (hi - lo)\na + (b-a) * s\n}\n\nThis complete function is what I want to see from you. (You should keep the stopifnot, because this function will have the exact same problem as the previous one if all the values in x are the same.)\nA better way is to observe that you can call functions inside functions. The function above is now a bit messy since it has several steps. Something that corresponds better to my hint is to call the original rescale first, and then modify its result:\n\nrescale3 &lt;- function(x, a=0, b=1) {\ns &lt;- rescale(x)\na + (b-a) * s\n}\n\nThe logic to this is rather clearly “rescale the input to be between 0 and 1, then rescale that to be between \\(a\\) and \\(b\\).” My rescale2 does exactly the same thing, but it’s much less clear that it does so, unless you happen to have in your head how rescale works. (I think you are more likely to remember, sometime in the future, what rescale does, compared to precisely how it works.)\nThat is why rescale3 is better than rescale2. Remember that you can, and generally should, use functions that have already been written (by you or someone else) as part of functions that do more complex things. See also my second point below.\nExtra: there are two important principles about why functions are important:\n\nthey allow you to re-do a calculation on many different inputs (the point I’ve been making up to now)\nby abstracting a calculation into a thing with a name, it makes it easier to understand that calculation’s role in something bigger. The only thing we had to remember in rescale3 is what the last line did, because the name of the function called on the first line tells us what happens there. This is much easier than remembering what the first four lines of rescale2 do.\n\nThe second principle here is what psychologists call “chunking”: you view a thing like my function rescale as a single item, rather than as four separate lines of code, and then that single item can be part of something larger (like my rescale3), and you have a smaller number of things to keep track of.\n\\(\\blacksquare\\)\n\nTest your new function two or more times, on input where you know or can guess what the output is going to be. In each case, explain briefly why your output makes sense.\n\nSolution\nI’ll start by using the default values for a and b (so I don’t have to specify them):\n\nrescale2(2:6)\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\nrescale3(2:6)\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\n\nI did both of the variants of my function; of course, you’ll only have one variant.\nWe got the same answer as before for the same input, so the default values \\(a=0, b=1\\) look as if they have been used.\nLet’s try a different one:\n\nv &lt;- c(7, 11, 12)\nrescale2(v, 10, 30)\n\n[1] 10 26 30\n\nrescale3(v, 10, 30)\n\n[1] 10 26 30\n\n\nThe lowest value in v has become 10, and the highest has become 30. (Also, the in-between value 11 was closer to 12 than to 7, and it has become something closer to 30 than to 10.)\nExtra: you can also name any of your inputs:\n\nrescale3(x=v, a=10, b=30)\n\n[1] 10 26 30\n\n\nand if you name them, you can shuffle the order too:\n\nrescale3(a=10, b=30, x=v)\n\n[1] 10 26 30\n\n\nThe point of doing more than one test is to check that different aspects of your function all work. Therefore, the best testing here checks that the defaults work, and that the answer is sensible for some different a and b (to check that this works as well).\nWhen you write your version of rescale with the optional inputs, it’s best if you do it so that the things you have to supply (the vector of numbers) is first. If you put a and b first, when you want to omit them, you’ll have to call the input vector by name, like this:\n\nrescale3(x=v)\n\n[1] 0.0 0.8 1.0\n\n\nbecause otherwise the input vector will be taken to be a, not what you want.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "functions.html#footnotes",
    "href": "functions.html#footnotes",
    "title": "20  Functions",
    "section": "",
    "text": "Hotpo is short for half or triple-plus-one.↩︎\nR style is to use the last line of the function for the return value, unless you are jumping out of the function before the end, in which case use return.↩︎\nHotpo is short for half or triple-plus-one.↩︎\nNobody knows whether you always get to 1, but also nobody has ever found a case where you don’t. Collatz’s conjecture, that you will get to 1 eventually, is known to be true for all starting \\(x_1\\) up to some absurdly large number, but not for all starting points.↩︎\nI should have been more careful in my functions to make sure everything was integers, and, in particular, to do integer division by 2 because I knew that this division was going to come out even.↩︎\nThis is something I’ve done in the past and been bitten by, so I am trying to get myself not to do it any more.↩︎\nIn computing with decimal numbers, things are almost never exactly zero or exactly infinite; they are very small or very big. The mean here, which you would calculate to be zero, is less than the so-called machine epsilon, which is about 10 to the minus 16 in R (R works in double precision). A mean that small is, to the computer, indistinguishable from zero. It came out that way because the last value is added to a total that is by that point negative, and so you have a loss of accuracy because of subtracting nearly equal quantities. I learned all about this stuff in my first real computer science course, which I think was a numerical math course, some absurd number of years ago. It looks as if this gets taught in CSCC37 these days.↩︎"
  },
  {
    "objectID": "vector-matrix.html#heights-and-foot-lengths-again",
    "href": "vector-matrix.html#heights-and-foot-lengths-again",
    "title": "21  Vector and matrix algebra",
    "section": "21.1 Heights and foot lengths again",
    "text": "21.1 Heights and foot lengths again\nEarlier, we investigated some data on predicting the height of a person from the length of their foot. The data were in http://ritsokiguess.site/datafiles/heightfoot.csv.\n\nRead in and display (some of) the data.\nIn your regression course, you learned (or will learn) the matrix formulation of the least squares estimates of intercept and slope. This produces a vector \\(\\hat\\beta\\) containing estimates of both the intercept and the slope, from the formula\n\n\\[ \\hat\\beta = (X^T X)^{-1} X^T y, \\]\nwhere:\n\n\\(X\\) is a matrix containing a column of 1s followed by all the columns of explanatory variables\n\\(X^T\\) denotes the (matrix) transpose of \\(X\\)\n\\(M^{-1}\\) denotes the inverse of the matrix \\(M\\)\n\\(y\\) denotes the column of response variable values.\n\nUse the formula above to obtain the least squares estimates of intercepts and slope for this regression, using R’s vector-matrix algebra. Hint: you are advised to do the calculation in steps, or else it will be very hard to read, and hard for the grader to check that it is correct.\n\nVerify that your calculation is correct by running the regression.\n\nMy solutions follow:"
  },
  {
    "objectID": "vector-matrix.html#heights-and-foot-lengths-again-1",
    "href": "vector-matrix.html#heights-and-foot-lengths-again-1",
    "title": "21  Vector and matrix algebra",
    "section": "21.2 Heights and foot lengths again",
    "text": "21.2 Heights and foot lengths again\nEarlier, we investigated some data on predicting the height of a person from the length of their foot. The data were in http://ritsokiguess.site/datafiles/heightfoot.csv.\n\nRead in and display (some of) the data.\n\nSolution\nCopy what you did before:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/heightfoot.csv\"\nhf &lt;- read_csv(my_url)\n\nRows: 33 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): height, foot\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhf\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nIn your regression course, you learned (or will learn) the matrix formulation of the least squares estimates of intercept and slope. This produces a vector \\(\\hat\\beta\\) containing estimates of both the intercept and the slope, from the formula\n\n\\[ \\hat\\beta = (X^T X)^{-1} X^T y, \\]\nwhere:\n\n\\(X\\) is a matrix containing a column of 1s followed by all the columns of explanatory variables\n\\(X^T\\) denotes the (matrix) transpose of \\(X\\)\n\\(M^{-1}\\) denotes the inverse of the matrix \\(M\\)\n\\(y\\) denotes the column of response variable values.\n\nUse the formula above to obtain the least squares estimates of intercepts and slope for this regression, using R’s vector-matrix algebra. Hint: you are advised to do the calculation in steps, or else it will be very hard to read, and hard for the grader to check that it is correct.\nSolution\nThere is some setup first: we have to get hold of \\(X\\) and \\(y\\) from the data as a matrix and a vector respectively. I would use tidyverse ideas to do this, and then turn them into a matrix at the end, which I think is best. Don’t forget to create a column of 1s to make the first column of \\(X\\)\n\nhf %&gt;% mutate(one=1) %&gt;% \nselect(one, foot) %&gt;% \nas.matrix() -&gt; X\nhead(X)\n\n     one foot\n[1,]   1 27.0\n[2,]   1 29.0\n[3,]   1 25.5\n[4,]   1 27.9\n[5,]   1 27.0\n[6,]   1 26.0\n\n\n(head displays the first six rows, or else you’ll be displaying all 33, which is too many.)\nAnother approach is this:\n\nX &lt;- cbind(1, hf$foot)\nhead(X)\n\n     [,1] [,2]\n[1,]    1 27.0\n[2,]    1 29.0\n[3,]    1 25.5\n[4,]    1 27.9\n[5,]    1 27.0\n[6,]    1 26.0\n\n\nNote that the recycling rules mean that a column with only one value in it will be repeated to the length of the other one, and so this is better than working out how many observations there are and repeating 1 that many times.\nThe choice here is whether to use tidyverse stuff and turn into a matrix at the end, or make a matrix at the start (which is what cbind from base R is doing). I don’t believe you’ve seen that in this course, so you ought to cite your source if you go that way.\nThe simplest choice for making \\(y\\) is this:\n\ny &lt;- hf$height\ny\n\n [1] 66.5 73.5 70.0 71.0 73.0 71.0 71.0 69.5 73.0 71.0 69.0 69.0 73.0 75.0 73.0\n[16] 72.0 69.0 68.0 72.5 78.0 79.0 71.0 74.0 66.0 71.0 71.0 71.0 84.0 77.0 72.0\n[31] 70.0 76.0 68.0\n\n\nThis also works:\n\nhf %&gt;% select(height) %&gt;% pull(height)\n\n [1] 66.5 73.5 70.0 71.0 73.0 71.0 71.0 69.5 73.0 71.0 69.0 69.0 73.0 75.0 73.0\n[16] 72.0 69.0 68.0 72.5 78.0 79.0 71.0 74.0 66.0 71.0 71.0 71.0 84.0 77.0 72.0\n[31] 70.0 76.0 68.0\n\n\n(remembering that you don’t want to have anything that’s a dataframe), or this:\n\nhf %&gt;% select(height) %&gt;% as.matrix() -&gt; yy\nhead(yy)\n\n     height\n[1,]   66.5\n[2,]   73.5\n[3,]   70.0\n[4,]   71.0\n[5,]   73.0\n[6,]   71.0\n\n\nremembering that a (column) vector and a 1-column matrix are the same thing as R is concerned.\nNow we want to construct some things. I would go at it this way, rather than trying to do everything at once (if you do, you will either get lost now, or in six months when you try to figure out what you did):\n\nXt &lt;- t(X) # X-transpose\nXtX &lt;- Xt %*% X \nXtXi &lt;- solve(XtX)\nXty &lt;- Xt %*% y\nXtXi %*% Xty\n\n          [,1]\n[1,] 34.336335\n[2,]  1.359062\n\n\nThe intercept is 34.33 and the slope is 1.36.\nThese compute, respectively, \\(X^T\\), \\(X^T X\\), the inverse of that, \\(X^T y\\) and \\(\\hat\\beta\\). Expect credit for laying out your calculation clearly.\nExtra: the value of this formula is that it applies no matter whether you have one \\(x\\)-variable, as here (or in the windmill data), or whether you have a lot (as in the asphalt data). In either case, \\(\\hat\\beta\\) contains the estimate of the intercept followed by all the slope estimates, however many there are. There are also matrix formulas that tell you how the slopes or the residuals will change if you remove one observation or one explanatory variable, so that something like step will work very efficiently, and calculations for leverages likewise.\n\\(\\blacksquare\\)\n\nVerify that your calculation is correct by running the regression.\n\nSolution\nThe usual lm:\n\nhf.1 &lt;- lm(height ~ foot, data = hf)\nsummary(hf.1)\n\n\nCall:\nlm(formula = height ~ foot, data = hf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7491 -1.3901 -0.0310  0.8918 12.9690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  34.3363     9.9541   3.449 0.001640 ** \nfoot          1.3591     0.3581   3.795 0.000643 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.102 on 31 degrees of freedom\nMultiple R-squared:  0.3173,    Adjusted R-squared:  0.2952 \nF-statistic: 14.41 on 1 and 31 DF,  p-value: 0.0006428\n\n\nThe same.\nExtra: in this “well-conditioned” case,1 it makes no difference, but if \\(X^T X\\) is almost singular, so that it almost doesn’t have an inverse (for example, some of your explanatory variables are highly correlated with each other), you can get into trouble. Regression calculations in practice use something more sophisticated like the singular value decomposition of \\(X^TX\\) to diagnose whether \\(X^TX\\) is actually singular or almost so, which from a numerical point of view is almost as bad, and to produce a sensible answer in that case.\nI guess I should try to make up one where it struggles. Let me do one with two \\(x\\)’s that are strongly correlated:\n\nd &lt;- tribble(\n~x1, ~x2, ~y,\n10, 20, 55,\n11, 19.0001, 60,\n12, 17.9999, 61,\n13, 17.0001, 64,\n14, 15.9998, 66,\n15, 15.0001, 67\n)\nd\n\n\n\n  \n\n\n\nx2 is almost exactly equal to 30 minus x1. What’s the right answer?\n\nd.1 &lt;- lm(y ~ x1 + x2, data = d)\nsummary(d.1)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = d)\n\nResiduals:\n       1        2        3        4        5        6 \n-1.37530  1.26859  0.03118  0.63549  0.43765 -0.99760 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -11837.3   138685.6  -0.085    0.937\nx1             398.0     4622.9   0.086    0.937\nx2             395.7     4622.8   0.086    0.937\n\nResidual standard error: 1.303 on 3 degrees of freedom\nMultiple R-squared:  0.9485,    Adjusted R-squared:  0.9141 \nF-statistic: 27.61 on 2 and 3 DF,  p-value: 0.0117\n\ncoef(d.1)\n\n(Intercept)          x1          x2 \n-11837.2938    398.0000    395.6835 \n\n\nYou should be right away suspicious here: the R-squared is high, but neither of the explanatory variables are significant! (This actually means that you can remove one of them, either one.) The standard errors are also suspiciously large, never a good sign. If you’ve done C67, you might be thinking about variance inflation factors here:\n\nlibrary(car)\nvif(d.1)\n\n       x1        x2 \n220326271 220326271 \n\n\nThese are both huge (greater than 5 or 10 or whatever guideline you use), indicating that they are highly correlated with each other (as we know they are).\nAll right, how does the matrix algebra work? This is just the same as before:\n\nd %&gt;% mutate(one=1) %&gt;% \nselect(one, starts_with(\"x\")) %&gt;% \nas.matrix() -&gt; X\nhead(X)\n\n     one x1      x2\n[1,]   1 10 20.0000\n[2,]   1 11 19.0001\n[3,]   1 12 17.9999\n[4,]   1 13 17.0001\n[5,]   1 14 15.9998\n[6,]   1 15 15.0001\n\n\nand then\n\ny &lt;- d$y\nXt &lt;- t(X) # X-transpose\nXtX &lt;- Xt %*% X \nXtXi &lt;- solve(XtX)\nXty &lt;- Xt %*% y\nXtXi %*% Xty\n\n           [,1]\none -11837.1777\nx1     397.9962\nx2     395.6796\n\n\nThese answers are actually noticeably different from the right answers (with a few more decimals here):\n\ncoef(d.1)\n\n(Intercept)          x1          x2 \n-11837.2938    398.0000    395.6835 \n\n\nOne way of finding out how nearly singular \\(X^TX\\) is is to look at its eigenvalues. You’ll recall that a singular matrix has one or more zero eigenvalues:\n\neigen(XtX)\n\neigen() decomposition\n$values\n[1] 2.781956e+03 3.404456e+01 8.805965e-11\n\n$vectors\n            [,1]        [,2]        [,3]\n[1,] -0.04643281 -0.00782885  0.99889074\n[2,] -0.57893109 -0.81469635 -0.03329647\n[3,] -0.81405331  0.57983495 -0.03329628\n\n\nThe third eigenvalue is \\(8.8 \\times 10^{-11}\\), which is very close to zero, especially compared to the other two, which are 34 and over two thousand. This is a very nearly singular matrix, and hence \\((X^TX)^{-1}\\) is very close to not existing at all, and that would mean that you couldn’t even compute the intercept and slope estimates, never mind hope to get close to the right answer.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "vector-matrix.html#footnotes",
    "href": "vector-matrix.html#footnotes",
    "title": "21  Vector and matrix algebra",
    "section": "",
    "text": "Which in this case means that the column of 1s and the actual x values are not strongly correlated, which means that the x-values vary enough.↩︎"
  },
  {
    "objectID": "bootstrap.html#air-conditioning-failures",
    "href": "bootstrap.html#air-conditioning-failures",
    "title": "22  The Bootstrap",
    "section": "22.1 Air conditioning failures",
    "text": "22.1 Air conditioning failures\nBack in 1963, there was a report on failures in air-conditioning equipment in aircraft. For one aircraft, the air-conditioning equipment failed 12 times, and the number of hours it ran before failing each time was recorded. The data are in link. Boeing was interested in the mean failure time, because the company wanted to plan for engineers to fix the failures (and thus needed to estimate a failure rate).\nThere is randomization here. Your answers will differ slightly from mine, unless you throw in this before you start (or at least before you generate your first random numbers).\n\nset.seed(457299)\n\n\nRead in the data, and observe that you have the correct number of rows. (Note that the failure times are in ascending order).\nWhat do you notice about the shape of the distribution of failure times? Explain briefly.\nObtain the means of 1000 bootstrap samples (that is, samples from the data with replacement). Save them.\nMake a normal quantile plot of your bootstrap distribution. What do you see? Explain briefly.\nObtain the 95% bootstrap percentile confidence interval for the mean.\nObtain the 95% bootstrap-\\(t\\) confidence interval for the mean, and compare your two intervals.\nObtain the BCa 95% confidence interval for the mean.\nCompare the BCa confidence interval with the other ones. Which one would you recommend? Explain briefly."
  },
  {
    "objectID": "bootstrap.html#air-conditioning-failures-bootstrapping-the-median",
    "href": "bootstrap.html#air-conditioning-failures-bootstrapping-the-median",
    "title": "22  The Bootstrap",
    "section": "22.2 Air conditioning failures: bootstrapping the median",
    "text": "22.2 Air conditioning failures: bootstrapping the median\nWith a skewed data distribution such as the air-conditioning failure times, we might be interested in inference for the median. One way to get a confidence interval for the median is to invert the sign test, as in smmr, but another way is to obtain a bootstrap sampling distribution for the median. How do these approaches compare for the air-conditioning data? We explore this here.\n\nRead in the air-conditioning data again (if you don’t already have it lying around). The link is in the previous question.\nUse smmr to get a confidence interval for the median (based on the sign test).\nObtain the bootstrap distribution of the sample median. Make a normal quantile plot of it. What do you notice? Explain briefly.\nObtain a 95% bootstrap percentile confidence interval for the median. How does it compare with the one you obtained earlier?\nObtain a 95% BCa interval. Compare it with the two other intervals you found."
  },
  {
    "objectID": "bootstrap.html#comparing-eyesight",
    "href": "bootstrap.html#comparing-eyesight",
    "title": "22  The Bootstrap",
    "section": "22.3 Comparing eyesight",
    "text": "22.3 Comparing eyesight\nDo people see on average better with their left eye or their right eye, or is there no difference? To find out, 15 subjects were shown a sequence of images, some to their left eye and some to their right (with a blindfold on the other eye). The subjects were asked to identify some objects in each image, and were given an overall score for each eye, based on their ability to identify objects with each eye. (A higher score is better.) Data in http://ritsokiguess.site/datafiles/eyesight.csv.\n\nRead in and display (some of) the data.\nExplain briefly why looking at differences (say right minus left) makes sense for these data, and calculate and save a dataframe with the differences added to it.\nMake a suitable normal quantile plot, and describe what it tells you.\nObtain a bootstrap distribution of the sample median.\nMake a histogram of your bootstrap distribution of the median. Use a lot of bins, such as the default 30, for this. What do you notice about the distribution? Why did it come out this way?\nFind a 95% percentile interval for the population median.1\nFind the BCA 95% confidence interval for the population median difference.\nWhat do your intervals tell us about any possible difference between left eye and right eye in terms of ability to identify objects in images? Do the intervals agree or disagree about this?"
  },
  {
    "objectID": "bootstrap.html#bootstrapping-the-irs-data",
    "href": "bootstrap.html#bootstrapping-the-irs-data",
    "title": "22  The Bootstrap",
    "section": "22.4 Bootstrapping the IRS data",
    "text": "22.4 Bootstrapping the IRS data\nYou might recall the IRS data from when we were learning about the sign test. The idea was that we wanted to see how long “on average” it took people to fill out a tax form. The data are in http://ritsokiguess.site/datafiles/irs.txt.\n\nRead in and display (some of) the data. There is only one column of data, so you can pretend the values are separated by anything.\nObtain a bootstrap distribution of the sample median.\nMake a suitable graph of the bootstrap distribution of the median. What seems odd about it? Why did that happen? (Hint: use more bins on your plot than usual, like 50.)\nFind 95% percentile and bootstrap-\\(t\\) intervals for the population median. (Hint: your dataframe of bootstrapped medians may still be rowwise, so you might need to run ungroup first.)\n\nMy solutions follow:"
  },
  {
    "objectID": "bootstrap.html#air-conditioning-failures-1",
    "href": "bootstrap.html#air-conditioning-failures-1",
    "title": "22  The Bootstrap",
    "section": "22.5 Air conditioning failures",
    "text": "22.5 Air conditioning failures\nBack in 1963, there was a report on failures in air-conditioning equipment in aircraft. For one aircraft, the air-conditioning equipment failed 12 times, and the number of hours it ran before failing each time was recorded. The data are in link. Boeing was interested in the mean failure time, because the company wanted to plan for engineers to fix the failures (and thus needed to estimate a failure rate).\nThere is randomization here. Your answers will differ slightly from mine, unless you throw in this before you start (or at least before you generate your first random numbers).\n\nset.seed(457299)\n\n\nRead in the data, and observe that you have the correct number of rows. (Note that the failure times are in ascending order).\n\nSolution\nThis is a .csv so read_csv is the thing:\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/pasias/master/air_conditioning.csv\"\naircon &lt;- read_csv(my_url)\n\nRows: 12 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): failure, hours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\naircon\n\n\n\n  \n\n\n\nTwelve rows (12 failure times).\n\\(\\blacksquare\\)\n\nWhat do you notice about the shape of the distribution of failure times? Explain briefly.\n\nSolution\nMake a suitable graph. The obvious one is a histogram:\n\nggplot(aircon, aes(x = hours)) + geom_histogram(bins = 7)\n\n\n\n\nYou’ll have to play with the number of bins (there are only 12 observations). I got 7 from the Freedman-Diaconis rule:\n\nnclass.FD(aircon$hours)\n\n[1] 7\n\n\nI was a little suspicious that the data would not be much like normal (I have run into failure times before), so I kept away from the Sturges rule.\nAnother possibility is a one-group boxplot:\n\nggplot(aircon, aes(y = hours, x = 1)) + geom_boxplot()\n\n\n\n\nIf you like, you can do a normal quantile plot. I rank that third here, because there is nothing immediately implying a comparison with the normal distribution, but I would accept it:\n\nggplot(aircon, aes(sample = hours)) + stat_qq() + stat_qq_line()\n\n\n\n\nPick a visual and defend it.\nAll three of these graphs are showing a strong skewness to the right.\nExtra: this is probably not a surprise, because a time until failure cannot be less than zero, and distributions with a limit tend to be skewed away from that limit. (If you look back at the data, there are some very small failure times, but there are also some very big ones. The very small ones are saying that the lower limit matters.) If you were modelling these times until failure, you might use a distribution like the exponential or gamma or Weibull.\n\\(\\blacksquare\\)\n\nObtain the means of 1000 bootstrap samples (that is, samples from the data with replacement). Save them.\n\nSolution\nSomething like this, therefore:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(sample = list(sample(aircon$hours, replace = TRUE))) %&gt;% \n  mutate(sample_mean = mean(sample)) -&gt; means\nmeans\n\n\n\n  \n\n\n\nForgetting the rowwise will cause all sorts of trouble.\n\\(\\blacksquare\\)\n\nMake a normal quantile plot of your bootstrap distribution. What do you see? Explain briefly.\n\nSolution\nThis:\n\nggplot(means, aes(sample = sample_mean)) + stat_qq() + stat_qq_line()\n\n\n\n\nThis is still skewed to the right (it has a curved shape, or, the low values and the high values are both too high compared to the normal).\nExtra: this is less skewed than the original data was, because, with a sample size of 12, we have a little help from the Central Limit Theorem, but not much. This picture is the one that has to be normal enough for \\(t\\) procedures to work, and it is not. This comes back into the picture when we compare our confidence intervals later.\nAlso, it makes sense to see how normal a sampling distribution of a mean is, so a normal quantile plot would be my first choice for this.\n\\(\\blacksquare\\)\n\nObtain the 95% bootstrap percentile confidence interval for the mean.\n\nSolution\nThis is the 2.5 and 97.5 percentiles of the bootstrapped sampling distribution of the mean:\n\nquantile(means$sample_mean, c(0.025, 0.975))\n\n     2.5%     97.5% \n 47.05625 187.93333 \n\n\n\\(\\blacksquare\\)\n\nObtain the 95% bootstrap-\\(t\\) confidence interval for the mean, and compare your two intervals.\n\nSolution\nThe key is to remember that the original sample (and thus each bootstrap sample) had \\(n=12\\), so there are \\(12-1=11\\) df. (The fact that there were 1000 bootstrap samples is neither here nor there). This is how I like to do it:\n\nt_star &lt;- qt(0.975, 11)\nt_star\n\n[1] 2.200985\n\nmean(means$sample_mean) + c(-1, 1) * t_star * sd(means$sample_mean)\n\n[1]  25.33401 186.81249\n\n\nThe c(-1, 1) thing is the calculation version of the \\(\\pm\\), and gets both limits at once. Pull the above apart to see how it works. If you don’t like that, you might prefer something like this:\n\nthe_mean &lt;- mean(means$sample_mean)\nthe_sd &lt;- sd(means$sample_mean)\nmargin &lt;- t_star * the_sd\nthe_mean - margin\n\n[1] 25.33401\n\nthe_mean + margin\n\n[1] 186.8125\n\n\nI apologize for the crazy first line of that! As for comparison: the bootstrap-\\(t\\) interval goes down a lot further, though the upper limits are quite similar (on this scale). Both intervals are very long and don’t tell us much about the population mean time to failure, which is not very surprising given the small sample size (\\(n=12\\)) and the large variability in the data.\nExtra: the non-normality of the bootstrap (sampling) distribution says that we should definitely not trust the bootstrap-\\(t\\), and probably not the bootstrap percentile interval either. Which brings us to the next part.\n\\(\\blacksquare\\)\n\nObtain the BCa 95% confidence interval for the mean.\n\nSolution\nThis means (possibly) installing and (certainly) loading the bootstrap package, and then:\n\ntheta &lt;- function(x) {\n  mean(x)\n}\nbca_all &lt;- with(aircon, bcanon(hours, 1000, theta))\nbca &lt;- bca_all$confpoints\nbca\n\n     alpha bca point\n[1,] 0.025  55.58333\n[2,] 0.050  61.25000\n[3,] 0.100  70.66667\n[4,] 0.160  78.50000\n[5,] 0.840 160.66667\n[6,] 0.900 178.50000\n[7,] 0.950 204.25000\n[8,] 0.975 228.75000\n\n\nPull out the ones from this that you need: the top one and the bottom one, to get an interval of 55.6 to 228.8.\nI seem to need to define the function theta first and pass it into bcanon as the third input. You may have more luck with bcanon(hours, 1000, mean) than I did. Try it.\nOr, if you feel like some extra coding: turn this matrix into a data frame, grab the rows you want, and then the column you want:\n\nbca %&gt;%\n  as_tibble() %&gt;%\n  filter(alpha %in% c(0.025, 0.975)) %&gt;%\n  pull(`bca point`)\n\n[1]  55.58333 228.75000\n\n\n\\(\\blacksquare\\)\n\nCompare the BCa confidence interval with the other ones. Which one would you recommend? Explain briefly.\n\nSolution\nIn this example, the bootstrap-\\(t\\) and percentile intervals are very different, so we should use neither of them, and prefer the BCa interval.\nExtra: as usual in this kind of case, the BCa contains values for the mean pulled out into the long tail, but that’s a proper adjustment for the sampling distribution being skewed.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "bootstrap.html#air-conditioning-failures-bootstrapping-the-median-1",
    "href": "bootstrap.html#air-conditioning-failures-bootstrapping-the-median-1",
    "title": "22  The Bootstrap",
    "section": "22.6 Air conditioning failures: bootstrapping the median",
    "text": "22.6 Air conditioning failures: bootstrapping the median\nWith a skewed data distribution such as the air-conditioning failure times, we might be interested in inference for the median. One way to get a confidence interval for the median is to invert the sign test, as in smmr, but another way is to obtain a bootstrap sampling distribution for the median. How do these approaches compare for the air-conditioning data? We explore this here.\n\nRead in the air-conditioning data again (if you don’t already have it lying around). The link is in the previous question.\n\nSolution\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/pasias/master/air_conditioning.csv\"\naircon &lt;- read_csv(my_url)\n\nRows: 12 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): failure, hours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\naircon\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nUse smmr to get a confidence interval for the median (based on the sign test).\n\nSolution\nInput to ci_median is data frame and column:\n\nci_median(aircon, hours)\n\n[1]   7.002319 129.998291\n\n\n\\(\\blacksquare\\)\n\nObtain the bootstrap distribution of the sample median. Make a normal quantile plot of it. What do you notice? Explain briefly.\n\nSolution\nThe usual do-it-yourself bootstrap:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(samples = list(sample(aircon$hours, replace = TRUE))) %&gt;% \n  mutate(medians = median(samples)) -&gt; meds\n\nI actually copied and pasted my code from the previous problem, changing mean to median.\nAs for a plot, well, this:\n\nggplot(meds, aes(sample = medians)) + stat_qq() + stat_qq_line()\n\n\n\n\nNot only does this not look very normal, but also there are those curious horizontal patches of points (that, you might recall, are characteristic of a discrete distribution). This has happened because there are only a few possible medians: the median has to be either a data value or halfway between two data values, so there are only something like \\(2(12)-1=23\\) different possible medians, with the ones in the middle being more likely.\nThis also shows up on a histogram, but only if you have enough bins. (If you don’t have enough bins, some of the neighbouring possible values end up in the same bin; here, the aim is to have enough bins to show the discreteness, rather than the usual thing of having few enough bins to show the shape.)\n\nggplot(meds, aes(x = medians)) + geom_histogram(bins = 30)\n\n\n\n\n\\(\\blacksquare\\)\n\nObtain a 95% bootstrap percentile confidence interval for the median. How does it compare with the one you obtained earlier?\n\nSolution\nAlso, the usual:\n\nquantile(meds$medians, c(0.025, 0.975))\n\n 2.5% 97.5% \n 12.5 115.0 \n\n\nThis goes down and up not quite so far as the interval from smmr. That might be because the smmr interval is too wide (based on a not-very-powerful test), or because the bootstrap quantile interval is too narrow (as it usually is). It’s hard to tell which it is.\n\\(\\blacksquare\\)\n\nObtain a 95% BCa interval. Compare it with the two other intervals you found.\n\nSolution\nYet more copying and pasting (from the previous question):\n\ntheta &lt;- function(x) {\n  median(x)\n}\nbca_all &lt;- with(aircon, bcanon(hours, 1000, theta))\nbca &lt;- bca_all$confpoints\nbca\n\n     alpha bca point\n[1,] 0.025      12.5\n[2,] 0.050      12.5\n[3,] 0.100      18.0\n[4,] 0.160      30.5\n[5,] 0.840      94.5\n[6,] 0.900      98.0\n[7,] 0.950     100.0\n[8,] 0.975     115.0\n\n\nMy 95% BCa interval is from 12.5 to 115.\nAgain, I seem to need to define the tiny function, while you can probably call bcanon(hours, 1000, median). Try it and see.\nMy BCa interval is the same as the bootstrap percentile interval and a little shorter than the one that came from the sign test. I would guess that the BCa interval is the most trustworthy of the three, though there is here not that much difference between them. All the intervals are again very long, a reflection of the small sample size and large variability.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "bootstrap.html#comparing-eyesight-1",
    "href": "bootstrap.html#comparing-eyesight-1",
    "title": "22  The Bootstrap",
    "section": "22.7 Comparing eyesight",
    "text": "22.7 Comparing eyesight\nDo people see on average better with their left eye or their right eye, or is there no difference? To find out, 15 subjects were shown a sequence of images, some to their left eye and some to their right (with a blindfold on the other eye). The subjects were asked to identify some objects in each image, and were given an overall score for each eye, based on their ability to identify objects with each eye. (A higher score is better.) Data in http://ritsokiguess.site/datafiles/eyesight.csv.\n\nRead in and display (some of) the data.\n\nSolution\nThis is a csv, so no surprises:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/eyesight.csv\"\nsight &lt;- read_csv(my_url)\n\nRows: 15 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): person, right, left\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsight\n\n\n\n  \n\n\n\n15 observations, with the subjects labelled by number, and a score for each subject and each eye.\n\\(\\blacksquare\\)\n\nExplain briefly why looking at differences (say right minus left) makes sense for these data, and calculate and save a dataframe with the differences added to it.\n\nSolution\nThis is matched pairs data, with two observations for each subject. A matched pairs analysis, whether by a sign test or a matched-pairs \\(t\\)-test, would be based on one difference for each subject, and so those would make sense to calculate. (You’ll recall that a matched pairs analysis uses the differences and not the original data.)\nThus, saving back into our original dataframe:\n\nsight %&gt;% \nmutate(difference = right - left) -&gt; sight\nsight\n\n\n\n  \n\n\n\nExtra: this is one of those cases where having long data would make it very much more difficult to work out the differences for each person. Try it and see. How will you match up the two measurements for each person?\n\\(\\blacksquare\\)\n\nMake a suitable normal quantile plot, and describe what it tells you.\n\nSolution\nA normal quantile plot of the differences, therefore, since normality of the two individual scores is immaterial:\n\nggplot(sight, aes(sample = difference)) + stat_qq() +\nstat_qq_line()\n\n\n\n\nWe have what I think is best described as “long tails”, with the high values being too high and the low ones being a bit too low for a normal distribution. I think this is a better description than “outliers” because outliers are isolated unusual values, not five observations out of fifteen!\nThe plot is telling us that a matched-pairs \\(t\\)-test is questionable, and that we might do a sign test instead. Or, as we explore in this question, find a bootstrap distribution (in this case, for the median).\nExtra: the one kind of sensible plot that uses the original data in this situation would be a scatterplot, since the right and left scores are matched up:\n\nggplot(sight, aes(x = right, y = left)) + \ngeom_point() + geom_abline(slope = 1, intercept = 0)\n\n\n\n\nI added the line \\(y = x\\) to the plot. The value of doing that is that a point to the right and below the line has the right-eye score bigger than the left-eye one, and vice versa for a point to the left and above. This plot tells us that a small majority of the subjects had a higher score with the right eye, and for the ones that had a higher score with the left eye, the difference wasn’t usually very big.\nThis plot tells us nothing about normality of differences, though (not without some careful looking), which is one of the things we usually care about.\n\\(\\blacksquare\\)\n\nObtain a bootstrap distribution of the sample median.\n\nSolution\nBorrow the idea from lecture, replacing mean with median:\n\ntibble(sim = 1:1000) %&gt;% \nrowwise() %&gt;% \nmutate(sample = list(sample(sight$difference, replace = TRUE))) %&gt;% \nmutate(my_median = median(sample)) -&gt; meds\nmeds\n\n\n\n  \n\n\n\nThe steps are:\n\ncreate a dataframe with a column called sim to label the simulations\nfrom here on out, work “rowwise”, that is, with one row at a time\ngenerate a bootstrap sample for each row. A bootstrap sample is fifteen observations rather than just one, so we are making a list-column and thus the list has to go on the front\nwork out the median of each bootstrap sample. Remember, the rowwise applies until you cancel it,2 and so this will be the median of the bootstrap sample on each row, one at a time.\n\nAs ever, if you want to see what’s going on, run this one line at a time.\n\\(\\blacksquare\\)\n\nMake a histogram of your bootstrap distribution of the median. Use a lot of bins, such as the default 30, for this. What do you notice about the distribution? Why did it come out this way?\n\nSolution\nFor this histogram, there is no need to specify a number of bins (unless you want to):\n\nggplot(meds, aes(x = my_median)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe distribution is very discrete (this shows up more clearly with more bins).\nThe data values are all integers (and therefore so are the differences). The median of an odd number of data values must be one of the data values, and the bootstrap samples only contain (varying numbers of copies of) the differences in the original dataset, so each bootstrap sample must have a median that is an integer too.\nExtra: in case you are thinking that this happened because the data values were integers, no, it would happen even if the data were decimal numbers. Let’s make some fake data of 15 random normals and then do the same thing again:\n\nfake_data &lt;- tibble(x = rnorm(15))\nfake_data\n\n\n\n  \n\n\n\nand once again bootstrap the median:\n\ntibble(sim = 1:1000) %&gt;% \nrowwise() %&gt;% \nmutate(sample = list(sample(fake_data$x, replace = TRUE))) %&gt;% \nmutate(my_median = median(sample)) -&gt; meds2\nmeds2\n\n\n\n  \n\n\n\nYou can see even from these few that the bootstrap distribution of the median has repeats, so there should also be some discreteness here:\n\nggplot(meds2, aes(x = my_median)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe discreteness is a property of the fact that we were bootstrapping the median, and the median has to be one of the data values.\nTo confirm that, recall that our original data were integers:\n\nsight\n\n\n\n  \n\n\n\nbut even for these, if you bootstrap the mean, you don’t get the same discreteness:\n\ntibble(sim = 1:1000) %&gt;% \nrowwise() %&gt;% \nmutate(sample = list(sample(sight$difference, replace = TRUE))) %&gt;% \nmutate(my_mean = mean(sample)) -&gt; means\nmeans %&gt;% \nggplot(aes(x = my_mean)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis is too many bins for 1000 bootstrap samples, so the shape is kind of irregular, but there are not the big gaps that the bootstrap distribution of the sample median has. Indeed, this ought to be somewhere near normal and is:\n\nggplot(means, aes(sample = my_mean)) + stat_qq() +\nstat_qq_line()\n\n\n\n\n(This is saying that the Central Limit Theorem is really helping, even for a sample size of only 15 from clearly non-normal data, so the paired \\(t\\) may not be as bad as we would have thought.)\n\\(\\blacksquare\\)\n\nFind a 95% percentile interval for the population median.3\n\nSolution\nThe percentile interval comes from the middle 95% of the bootstrap distribution of medians:\n\nquantile(meds$my_median, c(0.025, 0.975))\n\n 2.5% 97.5% \n   -2     4 \n\n\nThe bootstrap percentile interval goes from \\(-2\\) to 4. Like the CI for the median based on the sign test, the ends of this interval must be data values.\nExtra: for comparison, the interval from the sign test is this:\n\nci_median(sight, difference)\n\n[1] -1.997070  4.994629\n\n\nwhich is, when rounded off, from \\(-2\\) to 5, very like the percentile interval.\n\\(\\blacksquare\\)\n\nFind the BCA 95% confidence interval for the population median difference.\n\nSolution\nLoad (and if necessary install) the bootstrap package, and then:\n\nbca &lt;- bcanon(sight$difference, 1000, median)\nbca$confpoints\n\n     alpha bca point\n[1,] 0.025        -2\n[2,] 0.050        -2\n[3,] 0.100         0\n[4,] 0.160         0\n[5,] 0.840         3\n[6,] 0.900         3\n[7,] 0.950         4\n[8,] 0.975         4\n\n\n\\(-2\\) to 4, in this case like the percentile interval.4 Note how this one is data values also.\n\\(\\blacksquare\\)\n\nWhat do your intervals tell us about any possible difference between left eye and right eye in terms of ability to identify objects in images? Do the intervals agree or disagree about this?\n\nSolution\nThe intervals are not quite all the same, but one thing they have in common is that they all have a negative lower limit and a positive upper one (more positive than the negative one is negative). This says that 0 is a plausible difference in each case, and thus it is reasonable to conclude that there is no evidence of any difference between the two eyes, based on this sample of 15 subjects.\nThe intervals do all go more positive than negative, which says that if anything the scores are better with the right eye than the left on average (from the way around that we took the differences). However, there is no evidence here that this is any more than chance.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "bootstrap.html#bootstrapping-the-irs-data-1",
    "href": "bootstrap.html#bootstrapping-the-irs-data-1",
    "title": "22  The Bootstrap",
    "section": "22.8 Bootstrapping the IRS data",
    "text": "22.8 Bootstrapping the IRS data\nYou might recall the IRS data from when we were learning about the sign test. The idea was that we wanted to see how long “on average” it took people to fill out a tax form. The data are in http://ritsokiguess.site/datafiles/irs.txt.\n\nRead in and display (some of) the data. There is only one column of data, so you can pretend the values are separated by anything.\n\nSolution\nPretty much any of the read_ functions will work, even this one:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/irs.txt\"\nirs &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Time = col_double()\n)\n\nirs\n\n\n\n  \n\n\n\nOne column called Time.\n\\(\\blacksquare\\)\n\nObtain a bootstrap distribution of the sample median.\n\nSolution\nThe lecture notes use the exact same dataset, so you can borrow ideas from there:\nSet up a dataframe with one row for each bootstrap sample you’re going to draw, 1000 in this case:\n\ntibble(sim = 1:1000)\n\n\n\n  \n\n\n\nCreate a column with a new bootstrap sample for each sim. This means doing rowwise first and then wrapping the sampling in list because you are creating a list-column of samples:\n\ntibble(sim = 1:1000) %&gt;% \nrowwise() %&gt;% \nmutate(sample = list(sample(irs$Time, replace = TRUE)))\n\n\n\n  \n\n\n\nEach thing in sample has 30 observations in it (one bootstrap sample). If you want, you can unnest to take a look at the values; they should be the ones in the dataset, possibly with extra repeats.\nNext, work out the median of each bootstrapped sample, which is simple because we are still working rowwise:\n\ntibble(sim = 1:1000) %&gt;% \nrowwise() %&gt;% \nmutate(sample = list(sample(irs$Time, replace = TRUE))) %&gt;% \nmutate(my_median = median(sample)) -&gt; bs\nbs\n\n\n\n  \n\n\n\nAs you realize, bs stands for “bootstrap”. Of course.\n\\(\\blacksquare\\)\n\nMake a suitable graph of the bootstrap distribution of the median. What seems odd about it? Why did that happen? (Hint: use more bins on your plot than usual, like 50.)\n\nSolution\nThe medians are already in a dataframe, so go straight ahead:\n\nggplot(bs, aes(x = my_median)) + geom_histogram(bins = 50)\n\n\n\n\nWhat we are seeing at this resolution is that the distribution is very irregular, with funny holes in it, more than you would expect even with this many bins. By way of comparison, the bootstrap distribution of the mean looks a lot smoother:\n\ntibble(sim = 1:1000) %&gt;% \n  rowwise() %&gt;% \n  mutate(my_sample = list(sample(irs$Time, replace = TRUE))) %&gt;% \n  mutate(my_mean = mean(my_sample)) %&gt;% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 50)\n\n\n\n\nThis is somewhat irregular, because we really have too many bins, but there are not nearly so many holes and irregular heights as on the plot for the median. I had you use a lot of bins in this special case because I wanted you to see just how irregular the bootstrapped distribution for the median really was.\nSo why did that happen? Think about what the sample median is for 30 observations: it is the mean of the 15th and 16th smallest values when you arrange them in order. A bootstrap sample must contain the same values as the original dataset (just probably not the same frequencies of them). So the median of a bootstrap sample must be the average of two of the values in the original dataset, and probably two that were close together. What that means is that there are not very many possible medians of the bootstrap samples, and they form a clearly discrete rather than a continuous distribution. (The sample mean, on the other hand, uses all the values in the bootstrap sample, and so there are a lot more possible bootstrap means than bootstrap medians; the distribution of those is as good as continuous.)\nWhat this means is that bootstrapping for medians is odd (it always looks like this), but that’s what the bootstrap distribution looks like.\n\\(\\blacksquare\\)\n\nFind 95% percentile and bootstrap-\\(t\\) intervals for the population median. (Hint: your dataframe of bootstrapped medians may still be rowwise, so you might need to run ungroup first.)\n\nSolution\nThe percentile interval comes from the middle 95% of the bootstrap distribution of medians. The dataframe bs is still rowwise, so we have to undo that first to do it the obvious way:\n\nbs %&gt;% \n  ungroup() %&gt;% \n  summarize(ci1 = quantile(my_median, 0.025),\n            ci2 = quantile(my_median, 0.975))\n\n\n\n  \n\n\n\nOr, pull out just that column and find the two quantiles of that, for which there are two ways, the base R way:\n\nquantile(bs$my_median, c(0.025, 0.975))\n\n 2.5% 97.5% \n  121   215 \n\n\nand the slightly odd-looking:\n\nbs %&gt;% pull(my_median) %&gt;% \n  quantile(c(0.025, 0.975))\n\n 2.5% 97.5% \n  121   215 \n\n\nAll of these get you to the same place. There is even one more:\n\nbs %&gt;% \n  ungroup() %&gt;% \n  reframe(ci = quantile(my_median, c(0.025, 0.975)))\n\n\n\n  \n\n\n\nThis is reframe rather than summarize because quantile in this case returns two numbers, the two percentiles we want, and summarize expects only one. (This is newish behaviour.) Another way is to use summarize, but wrap the call to quantile in list so that it returns only one thing (the list, containing two numbers, but bundled up in one list). Then you need to unnest it to see the values:\n\nbs %&gt;% \n  ungroup() %&gt;% \n  summarize(ci = list(quantile(my_median, c(0.025, 0.975)))) %&gt;% \n  unnest(ci)  \n\n\n\n  \n\n\n\nIf you had rowwise in the back of your mind, you might have tried this. Try it up to but not including the unnest to see how it works.\nFor the bootstrap \\(t\\), estimate the population median as the sample median:\n\nmed &lt;- median(irs$Time)\nmed\n\n[1] 172.5\n\n\nget its standard error from the SD of the bootstrap distribution of medians:\n\nse &lt;- sd(bs$my_median)\nse\n\n[1] 23.31839\n\n\nthen go up and down twice this (or 1.96 if you believe in \\(z\\)):\n\nmed + c(-2, 2)*se\n\n[1] 125.8632 219.1368\n\n\nExtra: in this case, we also have the CI for the median that came out of the sign test:\n\nlibrary(smmr)\nci_median(irs, Time)\n\n[1] 119.0065 214.9955\n\n\nThis one is actually very close to the bootstrap percentile interval, while the bootstrap \\(t\\) interval is higher at both ends.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "bootstrap.html#footnotes",
    "href": "bootstrap.html#footnotes",
    "title": "22  The Bootstrap",
    "section": "",
    "text": "I was also going to have you do a bootstrap-t interval, but I’m not completely convinced I got that right when I was explaining it to you before.↩︎\nThat is done using ungroup, should you ever need to stop working rowwise. This seems like an odd choice of function, since the usual use of ungroup is to undo a group-by, but what ungroup actually does is to remove any special properties a dataframe has, including both groups and any rowwise behaviour.↩︎\nI was also going to have you do a bootstrap-t interval, but I’m not completely convinced I got that right when I was explaining it to you before.↩︎\nThey don’t often agree this well, but all of these intervals in this situation but have data values at their endpoints, and all of our data values are integers.↩︎"
  },
  {
    "objectID": "stan.html#estimating-proportion-in-favour-from-a-survey",
    "href": "stan.html#estimating-proportion-in-favour-from-a-survey",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.1 Estimating proportion in favour from a survey",
    "text": "23.1 Estimating proportion in favour from a survey\nYou are probably familiar with the kind of surveys where you are given a statement, like “I am the kind of person that finishes a task they start”, and you have to express your agreement or disagreement with it. Usually, you are given a five-point or seven-point scale on which you express your level of agreement (from “strongly agree” through “neither agree nor disagree” to “strongly disagree”, for example). Here, we will simplify things a little and only allow respondents to agree or disagree. So the kind of data you would have is a number of people that took part, and the number of these that said “agree”.\nCommon assumptions that are made in this kind of analysis are: (i) the responses are independent of each other, and (ii) each respondent has the same unknown probability of agreeing. You might quibble about (ii), but the assumption we are making here is that we know nothing about the respondents apart from whether they agreed or disagreed. (In practice, we’d collect all kinds of demographic information about each respondent, and this might give us a clue about how they’ll respond, but here we’re keeping it simple.) Under our assumptions, the number of respondents that agree has a binomial distribution with \\(n\\) being our sample size, and \\(p\\) being the probability we are trying to estimate. Let’s estimate \\(p\\) using Stan: that is to say, let’s obtain the posterior distribution of \\(p\\).\n\nIn R Studio, open a new Stan file (with File, New File, Stan File). You’ll see a template file of Stan code. Edit the model section to reflect that you have observed a number of successes x that we are modelling to have a binomial distribution with number of trials n and success probability p.\nIn the line of Stan code you wrote, there should be three variables. Which of these are parameters and which are data? Explain briefly.\nI hope you found that there is only one parameter, p, in this problem. We know that \\(0 \\le p \\le 1\\), and we need a prior distribution for it. A common choice is a beta distribution. Look at the Stan manual, link. The density function is given in 19.1.1. It has two parameters \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\). \\(B(\\alpha, \\beta)\\) given there is a constant. Add to your model section to express that p has a prior distribution with parameters alpha and beta. (alpha and beta will be input data when we run this code.)\nAbove your model section, complete a parameters section that says what kind of variable p is. If p has upper or lower limits, put these in as well. You can edit the parameters section that is in the template.\nEverything else is data. Complete a data section (edit the one in the template) to say what type of thing everything else is, including limits if it has any. Don’t forget the parameters in the prior distribution!\nSave your code, if you haven’t already. I used the filename binomial.stan. In your Stan code window, at the top right, you’ll see a button marked Check. This checks whether your code is syntactically correct. Click it.\nCompile your model. (This may take a minute or so, depending on how fast your R Studio is.) When the spinny thing stops spinning, it’s done.\nIn most surveys, the probability to be estimated is fairly close to 0.5. A beta prior with \\(\\alpha=\\beta=2\\) expresses the idea that any value of p is possible, but values near 0.5 are more likely.\n\nA survey of 277 randomly selected adult female shoppers was taken. 69 of them agreed that when an advertised item is not available at the local supermarket, they request a raincheck.\nUsing the above information, set up a data list suitable for input to a run of stan.\n\nSample from the posterior distribution of p with these data, and display your results.\n\n\nObtain a 90% posterior interval for the probability that a randomly chosen adult female shopper will request a raincheck.\nObtain a 95% (frequentist) confidence interval for p, and compare the results. (Hint: prop.test.) Comment briefly.\n(optional) This is one of those problems where you can obtain the answer analytically. What is the posterior distribution of \\(p\\), using a prior \\(beta(\\alpha, \\beta)\\) distribution for \\(p\\) and observing \\(x\\) successes out of \\(n\\) trials?"
  },
  {
    "objectID": "stan.html#bayesian-regression",
    "href": "stan.html#bayesian-regression",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.2 Bayesian regression",
    "text": "23.2 Bayesian regression\nIn this question, we will develop Stan code to run a simple linear regression, and later apply it to some data (and do a bit of elicitation of prior distributions along the way).\n\nCreate a .stan file that will run a simple linear regression predicting a variable y from a variable x, estimating an intercept a and a slope b. Use normal prior distributions for a and b, and allow the means and SDs of the prior distributions for a and b to be specified (as data, later). The regression model says that the response y has a normal distribution with mean a+bx and SD sigma which is also estimated. Give this a prior chi-squared distribution with a prior mean that is also input.\nCheck your Stan code for syntactic correctness, and when it is correct, compile it.\nWe are going to be analyzing some data on vocabulary size (the number of words known) by children of different ages. It is suspected that the relationship between age and vocabulary size is approximately linear. You go consult with an early childhood expert, and they tell you this:\n\n\nIn children of age up to about six, vocabulary almost always increases by between 300 and 700 words per year.\nI can’t talk about vocabulary of children of age 0, because children don’t start learning to talk until age about 18 months (1.5 years).\nChildren of age 1.5 years almost always have a vocabulary between 0 and 500 words (depending on exactly what age they started talking.)\nEven if we know a child’s age, our prediction of their vocabulary size might be off by as much as 200 words.\n\nUse this information to obtain parameters for your prior distributions.\n\nSome data were collected on age and vocabulary size of 10 randomly selected children, shown here: link. Read in and display the data; the values are separated by single spaces.\nUse this dataset, along with your prior distribution from above, to obtain posterior distributions for intercept, slope and error SD. What is the 95% posterior interval for the slope?\nPlot a histogram of the posterior distribution of the slope. Does its shape surprise you? Explain briefly.\nWhat can we say about the vocabulary size of a randomly selected child of age 5 (a new one, not the one in the original data set)? Use an appropriate predictive distribution."
  },
  {
    "objectID": "stan.html#estimating-p-the-bayesian-way",
    "href": "stan.html#estimating-p-the-bayesian-way",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.3 Estimating \\(p\\) the Bayesian way",
    "text": "23.3 Estimating \\(p\\) the Bayesian way\nA binomial experiment with 8 trials produces the following results: success, failure, success, success, failure, success, success, success. (Each result is therefore a Bernoulli trial.) The person who gave you the data says that the success probability is most likely somewhere near 0.5, but might be near 0 or 1. The aim of this question is to estimate the success probability using Bayesian methods.\nIn this question, use cmdstanr (see this site for instructions). Documentation for Stan is here. You will probably want to be running R on your own computer.\n\nWrite a Stan program that will estimate the success probability \\(p\\). To do this, start with the likelihood (Stan has a function bernoulli that takes one parameter, the success probability). The data, as 1s and 0s, will be in a vector x. Use a beta distribution with unknown parameters as a prior for p. (We will worry later what those parameters should be.)\nCompile your code, correcting any errors until it compiles properly.\nThe person who brought you the data told you that the success probability p should be somewhere near 0.5 (and is less likely to be close to 0 or 1). Use this information to pick a prior distribution for p. (The exact answer you get doesn’t really matter, but try to interpret the statement in some kind of sensible way.)\nCreate an R list that contains all your data for your Stan model. Remember that Stan expects the data in x to be 0s and 1s.\nRun your Stan model to obtain a simulated posterior distribution, using all the other defaults.\nMake a plot of the posterior distribution of the probability of success. (Use the posterior and bayesplot packages if convenient.)\nThe posterior predictive distribution is rather odd here: the only possible values that can be observed are 0 and 1. Nonetheless, obtain the posterior predictive distribution for these data, and explain briefly why it is not surprising that it came out as it did.\n\nMy solutions follow:"
  },
  {
    "objectID": "stan.html#estimating-proportion-in-favour-from-a-survey-1",
    "href": "stan.html#estimating-proportion-in-favour-from-a-survey-1",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.4 Estimating proportion in favour from a survey",
    "text": "23.4 Estimating proportion in favour from a survey\nYou are probably familiar with the kind of surveys where you are given a statement, like “I am the kind of person that finishes a task they start”, and you have to express your agreement or disagreement with it. Usually, you are given a five-point or seven-point scale on which you express your level of agreement (from “strongly agree” through “neither agree nor disagree” to “strongly disagree”, for example). Here, we will simplify things a little and only allow respondents to agree or disagree. So the kind of data you would have is a number of people that took part, and the number of these that said “agree”.\nCommon assumptions that are made in this kind of analysis are: (i) the responses are independent of each other, and (ii) each respondent has the same unknown probability of agreeing. You might quibble about (ii), but the assumption we are making here is that we know nothing about the respondents apart from whether they agreed or disagreed. (In practice, we’d collect all kinds of demographic information about each respondent, and this might give us a clue about how they’ll respond, but here we’re keeping it simple.) Under our assumptions, the number of respondents that agree has a binomial distribution with \\(n\\) being our sample size, and \\(p\\) being the probability we are trying to estimate. Let’s estimate \\(p\\) using Stan: that is to say, let’s obtain the posterior distribution of \\(p\\).\n\nIn R Studio, open a new Stan file (with File, New File, Stan File). You’ll see a template file of Stan code. Edit the model section to reflect that you have observed a number of successes x that we are modelling to have a binomial distribution with number of trials n and success probability p.\n\nSolution\nThis is quicker to do than to ask for. Make a guess at this:\n\nmodel {\n  // likelihood\n  x ~ binomial(n, p);\n}\nand then check the manual link, looking for Sampling Statement, to make sure that this is what is expected. It is. (I got to this page by googling “Stan binomial distribution”.)\nThe “likelihood” line with the two slashes is a comment, C++ style. It is optional, but I like to have it to keep things straight.\n\\(\\blacksquare\\)\n\nIn the line of Stan code you wrote, there should be three variables. Which of these are parameters and which are data? Explain briefly.\n\nSolution\nThe way to think about this is to ask yourself which of x, n, and p are being given to the Stan code as data, and which you are trying to estimate. The only thing we are estimating here is p, so that is a parameter. The number of trials n and the number of successes x are data that you will observe (treated as “given” or “fixed” in the Bayesian framework).\n\\(\\blacksquare\\)\n\nI hope you found that there is only one parameter, p, in this problem. We know that \\(0 \\le p \\le 1\\), and we need a prior distribution for it. A common choice is a beta distribution. Look at the Stan manual, link. The density function is given in 19.1.1. It has two parameters \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\). \\(B(\\alpha, \\beta)\\) given there is a constant. Add to your model section to express that p has a prior distribution with parameters alpha and beta. (alpha and beta will be input data when we run this code.)\n\nSolution\nYour model section should now look like this:\n\nmodel {\n  // prior\n  p ~ beta(alpha, beta);\n  // likelihood\n  x ~ binomial(n, p);\n}\n\n\\(\\blacksquare\\)\n\nAbove your model section, complete a parameters section that says what kind of variable p is. If p has upper or lower limits, put these in as well. You can edit the parameters section that is in the template.\n\nSolution\np is a real variable taking values between 0 and 1, so this:\n\nparameters {\n  real&lt;lower=0, upper=1&gt; p;\n}\n\n\\(\\blacksquare\\)\n\nEverything else is data. Complete a data section (edit the one in the template) to say what type of thing everything else is, including limits if it has any. Don’t forget the parameters in the prior distribution!\n\nSolution\nWe said before that n and x were (genuine) data. These are positive integers; also x cannot be bigger than n (why not?). In the data section also go the parameters alpha and beta of the prior distribution. These are real numbers bigger than zero. These two together give us this:\n\ndata {\n  int&lt;lower=0&gt; n;\n  int&lt;lower=0, upper=n&gt; x;\n  real&lt;lower=0&gt; alpha;\n  real&lt;lower=0&gt; beta;\n}\n\nPutting in lower and upper limits, if you have them, will help because if you happen to enter data that does not respect the limits, you’ll get an error right there, and you won’t waste time sampling.\nIt is more important to put in limits in the parameters section, because that is telling the sampler not to go there (eg. a value of \\(p\\) outside \\([0,1]\\)).\n\\(\\blacksquare\\)\n\nSave your code, if you haven’t already. I used the filename binomial.stan. In your Stan code window, at the top right, you’ll see a button marked Check. This checks whether your code is syntactically correct. Click it.\n\nSolution\nThis appeared in my console:\n\n&gt; rstan:::rstudio_stanc(\"binomial.stan\")\nbinomial.stan is syntactically correct.\n\nIf you don’t see this, there is some kind of code error. You’ll then see some output that points you to a line of your code. The error is either there or at the end of the previous line (eg. you forgot a semicolon). Here is a typical one:\n\n&gt; rstan:::rstudio_stanc(\"binomial.stan\")\nSYNTAX ERROR, MESSAGE(S) FROM PARSER:\nerror in 'model377242ac03ef_binomial' at line 24, column 0\n-------------------------------------------------\n22: parameters {\n23:   real&lt;lower=0, upper=1&gt; p\n24: }\n^\n25: \n-------------------------------------------------\n\nPARSER EXPECTED: \";\"\nError in stanc(filename, allow_undefined = TRUE) : \nfailed to parse Stan model 'binomial' due to the above error.\n\nThe compiler (or at least the code checker) was expecting a semicolon, and when it got to the close-curly-bracket on line 24, that was where it knew that the semicolon was missing (and thus it objected there and not earlier). The above was on my own computer. When I tried it on rstudio.cloud, I thought I had everything correct but I got an error message like this:\n\nError in sink(type = \"output\") : invalid connection\n\nthat I couldn’t get rid of. This might happen to you also. If you get an error, fix it and check again. Repeat until your code is “syntactically correct”. (That means that it will compile, but not that it will necessarily do what you want.) This process is an integral part of coding, so get used to it. It doesn’t matter how many errors you make; what matters is that you find and correct them all.\n\\(\\blacksquare\\)\n\nCompile your model. (This may take a minute or so, depending on how fast your R Studio is.) When the spinny thing stops spinning, it’s done.\n\nSolution\nGo down to the console and type something like\n\nbinomial &lt;- cmdstan_model(\"binomial.stan\")\n\nIf it doesn’t work, make sure you installed and loaded cmdstanr first, with install.packages and library respectively.\nIf it sits there and does nothing for a while, this is actually a good sign. If it finds an error, it will tell you. If you get your command prompt &gt; back without it saying anything, that means it worked. (This is a Unix thing: no comment means no error.)\nIf you happen to compile it a second time, without changing anything in the Stan code, it won’t make you wait while it compiles again: it will say “Model executable is up to date!”.\n\\(\\blacksquare\\)\n\nIn most surveys, the probability to be estimated is fairly close to 0.5. A beta prior with \\(\\alpha=\\beta=2\\) expresses the idea that any value of p is possible, but values near 0.5 are more likely.\n\nA survey of 277 randomly selected adult female shoppers was taken. 69 of them agreed that when an advertised item is not available at the local supermarket, they request a raincheck.\nUsing the above information, set up a data list suitable for input to a run of stan.\nSolution\nLook in your data section, and see what you need to provide values for. The order doesn’t matter; make a list with the named pieces and their values, in some order. You need values for these four things:\n\nbinomial_data &lt;- list(n = 277, x = 69, alpha = 2, beta = 2)\n\nExtra: in case you are wondering where the parameters for the prior came from: in this case, I looked on the Wikipedia page for the beta distribution and saw that \\(\\alpha=\\beta=2\\) is a good shape, so I used that. In practice, getting a reasonable prior is a more difficult problem, called “elicitation”. What you have to do is ask a subject matter expert what they think p might be, giving you a range of values such as a guessed-at 95% confidence interval, like “I think p is almost certainly between 0.1 and 0.6”. Then you as a statistician have to choose values for alpha and beta that match this, probably by trial and error. The beta distribution is part of R, so this is doable, for example like this:\n\ncrossing(alpha = 1:10, beta = 1:10) %&gt;%\n  mutate(\n    lower = qbeta(0.025, alpha, beta),\n    upper = qbeta(0.975, alpha, beta)\n  ) %&gt;%\n  mutate(sse = (lower - 0.1)^2 + (upper - 0.6)^2) %&gt;%\n  arrange(sse)\n\n\n\n  \n\n\n\nThis says that \\(\\alpha=4, \\beta=8\\) is a pretty good choice.1\nMy process:\n\nPick some values of alpha and beta to try, and make all possible combinations of them.\nFind the 2.5 and 97.5 percentiles of the beta distribution for each of those values. The “inverse CDF” (the value \\(x\\) that has this much of the probability below it) is what we want here; this is obtained in R by putting q in front of the name of the distribution. For example, does this make sense to you?\n\n\nqnorm(0.025)\n\n[1] -1.959964\n\n\n\nWe want the lower limit to be close to 0.1 and the upper limit to be close to 0.6. Working out the sum of squared errors for each alpha-beta combo is a way to do this; if sse is small, that combination of alpha and beta gave lower and upper limits close to 0.1 and 0.6.\nArrange the sse values smallest to largest. The top rows are the best choices of alpha and beta.\n\n\\(\\blacksquare\\)\n\nSample from the posterior distribution of p with these data, and display your results.\n\nSolution\nThis is what I got:\n\nbinomial_fit &lt;- binomial$sample(binomial_data)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.7 seconds.\n\nbinomial_fit\n\n variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n     lp__ -159.33 -159.06 0.72 0.31 -160.74 -158.84 1.00     1541     2042\n     p       0.25    0.25 0.03 0.03    0.21    0.29 1.00     1396     1747\n\n\nYour results should be similar, though probably not identical, to mine. (There is a lot of randomness involved here.)\n\\(\\blacksquare\\)\n\nObtain a 90% posterior interval for the probability that a randomly chosen adult female shopper will request a raincheck.\n\nSolution\nRead off the q5 and q95 values for p. Mine are 0.21 and 0.29.\n\\(\\blacksquare\\)\n\nObtain a 95% (frequentist) confidence interval for p, and compare the results. (Hint: prop.test.) Comment briefly.\n\nSolution\nIf you remember this well enough, you can do it by hand, but there’s no need:\n\nprop.test(69, 277)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  69 out of 277, null probability 0.5\nX-squared = 68.751, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.2001721 0.3051278\nsample estimates:\n        p \n0.2490975 \n\n\nMy 95% intervals are very close.\nNumerically, this is because the only (material) difference between them is the presence of the prior in the Bayesian approach. We have quite a lot of data, though, so the choice of prior is actually not that important (“the data overwhelm the prior”). I could have used alpha=8, beta=4 that I obtained in the Extra above, and it wouldn’t have made any noticeable difference.\nConceptually, though, the interpretations of these intervals are very different: the Bayesian posterior interval really does say “the probability of \\(p\\) being between 0.20 and 0.31 is 0.95”, while for the confidence interval you have to talk about repeated sampling: “the procedure producing the 95% confidence interval will contain the true value of \\(p\\) in 95% of all possible samples”. This might seem clunky in comparison; a Bayesian would tell you that the interpretation of the posterior interval is what you want the interpretation of the confidence interval to be!\n\\(\\blacksquare\\)\n\n(optional) This is one of those problems where you can obtain the answer analytically. What is the posterior distribution of \\(p\\), using a prior \\(beta(\\alpha, \\beta)\\) distribution for \\(p\\) and observing \\(x\\) successes out of \\(n\\) trials?\n\nSolution\nWith this stuff, you can throw away any constants.\nThe likelihood is (proportional to) \\[ p^x (1-p)^{n-x}.\\] There is a binomial coefficient that I threw away.\nLook up the form of the beta density if you don’t know it (or look above): the prior for \\(p\\) is proportional to\n\\[ p^{\\alpha-1} (1-p)^{\\beta-1}.\\]\nPosterior is proportional to likelihood times prior:\n\\[ p^{x + \\alpha - 1} (1-p)^{n-x +\\beta - 1}\\]\nwhich is recognized as a beta distribution with parameters \\(x+\\alpha\\), \\(n-x+\\beta\\). Typically (unless you are very sure about \\(p\\) a priori (that is, before collecting any data)), \\(x\\) and \\(n-x\\) will be much larger than \\(\\alpha\\) and \\(\\beta\\), so this will look a lot like a binomial likelihood, which is why the confidence interval and posterior interval in our example came out very similar. I leave it to you to decide which you prefer: algebra and intelligence (and luck, often), or writing code to sample from the posterior. I know what I prefer!\nExtra: one of the people behind Stan is on Twitter with handle @betanalpha.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "stan.html#bayesian-regression-1",
    "href": "stan.html#bayesian-regression-1",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.5 Bayesian regression",
    "text": "23.5 Bayesian regression\nIn this question, we will develop Stan code to run a simple linear regression, and later apply it to some data (and do a bit of elicitation of prior distributions along the way).\n\nCreate a .stan file that will run a simple linear regression predicting a variable y from a variable x, estimating an intercept a and a slope b. Use normal prior distributions for a and b, and allow the means and SDs of the prior distributions for a and b to be specified (as data, later). The regression model says that the response y has a normal distribution with mean a+bx and SD sigma which is also estimated. Give this a prior chi-squared distribution with a prior mean that is also input.\n\nSolution\nThis is a lot. Breathe. Pause. Then, in R Studio, File, New File and Stan File. Leave the template there, and change what you need as you go. I would start with the model part. The likelihood part says that y has a normal distribution with mean a+bx and SD sigma, thus:\n\n// likelihood\ny ~ normal(a+b*x, sigma);\n\nThere is a subtlety here that I’ll get to later, but this is the easiest way to begin. Next, take a look at what’s here. x and y are data, and the other things, a, b, sigma are parameters. These last three need prior distributions. I said to use normal distributions for the first two, and a chi-squared distribution for the last one. (In practice, of course, you get to choose these, in consultation with the subject matter expert, but these are likely to be pretty reasonable.) I’ve given the parameters of these prior distributions longish names, so I hope I’m trading more typing for less confusion:\n\nmodel {\n  // prior\n  a ~ normal(prior_int_mean, prior_int_sd);\n  b ~ normal(prior_slope_mean, prior_slope_sd);\n  sigma ~ chi_square(prior_sigma_mean);\n  // likelihood\n  y ~ normal(a+b*x, sigma);\n}\n\nThe chi-squared distribution is written that way in Stan, and has only one parameter, a degrees of freedom that is also its mean.\nOur three parameters then need to be declared, in the parameters section. a and b can be any real number, while sigma has to be positive:\n\nparameters {\n  real a;\n  real b;\n  real&lt;lower=0&gt; sigma;\n}\n\nEverything else is data, and we have a lot of data this time:\n\ndata {\n  int&lt;lower=0&gt; n;\n  vector[n] x;\n  vector[n] y;\n  real prior_int_mean;\n  real&lt;lower=0&gt; prior_int_sd;\n  real prior_slope_mean;\n  real&lt;lower=0&gt; prior_slope_sd;\n  real&lt;lower=0&gt; prior_sigma_mean;\n}\n\nThe five things at the bottom are the prior distribution parameters, which we are going to be eliciting later. The means for intercept and slope can be anything; the prior SDs have to be positive, and so does the prior mean for sigma, since it’s actually a degrees of freedom that has to be positive.\nNow we come to two pieces of subtlety. The first is that the x and y are going to have some (unknown) number of values in them, but we need to declare them with some length. The solution to that is to have the number of observations n also be part of the data. Once we have that, we can declare x and y to be of length n with no problems.\nThe second piece of subtlety is that you were probably expecting this:\n\nreal x[n];\nreal y[n];\n\nThis is usually what you need, but the problem is that when you work out a+b*x later on, it doesn’t work because you are trying to multiply an array of values x by a single value b. (Try it.) There are two ways around this: (i), if you instead declare x and y to be (real) vectors of length n, Stan borrows from R’s multiplication of a vector by a scalar and it works, by multiplying each element of the vector by the scalar. Or, (ii), you can go back to declaring x and y as real things of length n, and use a loop to get each y from its corresponding x, like this:\n\nfor (i in 1:n) {\n  y[i] ~ normal(a + b * x[i], sigma)\n}\n\n\nand this works because a, b, and x[i] are all scalar. I have to say that I don’t really understand the distinction between real x[n] and vector[n] x, except that sometimes one works and the other doesn’t.\nThe manual tells you that the vector way is “much faster”, though in a simple problem like this one I doubt that it makes any noticeable difference.\nMy code looks like this, in total:\n\ndata {\n  int&lt;lower=0&gt; n;\n  vector[n] x;\n  vector[n] y;\n  real prior_int_mean;\n  real&lt;lower=0&gt; prior_int_sd;\n  real prior_slope_mean;\n  real&lt;lower=0&gt; prior_slope_sd;\n  real&lt;lower=0&gt; prior_sigma_mean;\n}\n\nparameters {\n  real a;\n  real b;\n  real&lt;lower=0&gt; sigma;\n}\n\nmodel {\n  // prior\n  a ~ normal(prior_int_mean, prior_int_sd);\n  b ~ normal(prior_slope_mean, prior_slope_sd);\n  sigma ~ chi_square(prior_sigma_mean);\n  // likelihood\n  y ~ normal(a+b*x, sigma);\n}\n\n\n\\(\\blacksquare\\)\n\nCheck your Stan code for syntactic correctness, and when it is correct, compile it.\n\nSolution\nClick the Check button top right of the window where your Stan code is. If it finds any errors, correct them and try again.\nTo compile, the usual thing:\n\nreg &lt;- cmdstan_model(\"reg.stan\")\n\nand wait for it to do its thing. With luck, Check will have found all the errors and this will quietly (eventually) do its job.\n\\(\\blacksquare\\)\n\nWe are going to be analyzing some data on vocabulary size (the number of words known) by children of different ages. It is suspected that the relationship between age and vocabulary size is approximately linear. You go consult with an early childhood expert, and they tell you this:\n\n\nIn children of age up to about six, vocabulary almost always increases by between 300 and 700 words per year.\nI can’t talk about vocabulary of children of age 0, because children don’t start learning to talk until age about 18 months (1.5 years).\nChildren of age 1.5 years almost always have a vocabulary between 0 and 500 words (depending on exactly what age they started talking.)\nEven if we know a child’s age, our prediction of their vocabulary size might be off by as much as 200 words.\n\nUse this information to obtain parameters for your prior distributions.\nSolution\nThis is the typical kind of way in which you would elicit a prior distribution; you try to turn what the expert tells you into something you can use.\nLet’s assume that the “almost always” above corresponds to a 95% confidence interval, and since our intercept and slope have prior normal distributions, this is, to the accuracy that we are working, mean plus/minus 2 SD. (You can make different assumptions and you’ll get a somewhat different collection of prior distributions.)\nThe first statement talks about the change in vocabulary size per year. This is talking about the slope. The supposed 95% confidence interval given translates to \\(500 \\pm 2(100)\\), so the prior mean for the slope is 500 and the prior SD is 100.\nNot so hard. The problems start with the second one.\nWe want a prior mean and SD for the intercept, that is, for the mean and SD of vocabulary size at age 0, but the expert (in their second statement) is telling us this makes no sense. The third statement says that at age 1.5, a 95% CI for vocabulary size is \\(250 \\pm 2(125)\\). You can go a number of different ways from here, but a simple one is use our best guess for the slope, 500, to project back 1.5 years from here by decreasing the mean by \\((500)(1.5)=750\\), that is, to \\(-500 \\pm 2(125)\\).\nThe last one we need is the prior mean for sigma. This is what the last statement is getting at. Up to you whether you think this is an estimate of sigma or twice sigma. Let’s take 200 as a prior estimate of sigma, to be safe.\nYou see that getting a useful prior depends on asking the right questions and making good use of the answers you get.\nSome people like to use “ignorance” priors, where you assign equal probability to all possible values of the parameter. I don’t, because these are saying that a slope of 10 million is just as likely as a slope of 1, regardless of the actual circumstances; you will almost always have some idea of what you are expecting. It might be vague, but it won’t be infinitely vague.\n\\(\\blacksquare\\)\n\nSome data were collected on age and vocabulary size of 10 randomly selected children, shown here: link. Read in and display the data; the values are separated by single spaces.\n\nSolution\nThus:\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/pasias/master/vocab.txt\"\nvocabulary &lt;- read_delim(my_url, \" \")\n\nRows: 10 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): age, vocab\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvocabulary\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nUse this dataset, along with your prior distribution from above, to obtain posterior distributions for intercept, slope and error SD. What is the 95% posterior interval for the slope?\n\nSolution\nTwo parts: set up the data, and then sample it:\n\nreg_data &lt;- list(\n  n = 10, x = vocabulary$age, y = vocabulary$vocab,\n  prior_int_mean = -500,\n  prior_int_sd = 125,\n  prior_slope_mean = 500,\n  prior_slope_sd = 100,\n  prior_sigma_mean = 200\n)\nreg.1 &lt;- reg$sample(reg_data)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.5 seconds.\n\nreg.1\n\n variable    mean  median    sd   mad      q5     q95 rhat ess_bulk ess_tail\n    lp__   373.71  374.04  1.23  1.01  371.27  375.06 1.00     1776     2161\n    a     -614.66 -613.42 97.42 96.47 -774.81 -450.23 1.00     1842     2104\n    b      521.54  521.47 26.16 26.22  478.21  564.96 1.00     1669     2008\n    sigma  189.37  188.52 18.99 19.15  159.25  221.56 1.00     2374     2239\n\n\nOne line per parameter (plus the log-posterior distribution, not very useful to us). To get a 95% posterior interval for the slope, use the 2.5 and 97.5 percentiles of the posterior for b, which are 467 and 572. (This is about \\(520 \\pm 52\\), rounding crudely, while the prior distribution said \\(500 \\pm 200\\), so the data have allowed us to estimate the slope a fair bit more accurately.)\n\\(\\blacksquare\\)\n\nPlot a histogram of the posterior distribution of the slope. Does its shape surprise you? Explain briefly.\n\nSolution\nThis is most easily mcmc_hist from bayesplot:\n\nmcmc_hist(reg.1$draws(\"b\"), binwidth = 20)\n\n\n\n\nI’m guessing you have a better intuition for bins as opposed to binwidth (the latter being what you need here), so you can try it without giving a binwidth at all (and getting way too many bins), and then see if you can figure out what binwidth should be to get you a sensible number of bins. This one looks pretty good to me.\nThe shape is very normal. This is because everything is normal: the prior and the data-generating process both, so it is not surprising at all that the posterior came out normal. (You may remember from your regression course that if you have a normal regression model, the slope also has a normal distribution.)\n\\(\\blacksquare\\)\n\nWhat can we say about the vocabulary size of a randomly selected child of age 5 (a new one, not the one in the original data set)? Use an appropriate predictive distribution.\n\nSolution\nIf you have done a regression course, you might recognize this as being the Bayesian version of a prediction interval. How might we make a predictive distribution for this? Well, first we need to extract the sampled values from the posteriors:\n\nas_draws_df(reg.1$draws()) %&gt;%\n  as_tibble() -&gt; sims\nsims\n\n\n\n  \n\n\n\nand now we need to simulate some response values for our notional child of age 5. That means simulating for an x of 5, using each of those values of a, b and sigma:\n\nsims %&gt;%\n  rowwise() %&gt;% \n  mutate(sim_vocab = rnorm(1, a + b * 5, sigma)) -&gt; sims2\nsims2\n\n\n\n  \n\n\nggplot(sims2, aes(x = sim_vocab)) + geom_histogram(bins = 20)\n\n\n\n\nThat’s the distribution of the vocabulary size of children aged 5. We can get a 95% interval from this the usual way: find the 2.5 and 97.5 percentiles:\n\nwith(sims2, quantile(sim_vocab, c(0.025, 0.975)))\n\n    2.5%    97.5% \n1589.785 2401.273 \n\n\nThe actual child of age 5 that we observed had a vocabulary of 2060 words, squarely in the middle of this interval.\nIs the posterior predictive interval like the prediction interval?\n\nvocabulary.1 &lt;- lm(vocab ~ age, data = vocabulary)\nnew &lt;- tibble(age = 5)\npredict(vocabulary.1, new, interval = \"p\")\n\n       fit      lwr      upr\n1 2027.939 1818.223 2237.656\n\n\nIt seems a bit wider.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "stan.html#estimating-p-the-bayesian-way-1",
    "href": "stan.html#estimating-p-the-bayesian-way-1",
    "title": "23  Bayesian Statistics with Stan",
    "section": "23.6 Estimating \\(p\\) the Bayesian way",
    "text": "23.6 Estimating \\(p\\) the Bayesian way\nA binomial experiment with 8 trials produces the following results: success, failure, success, success, failure, success, success, success. (Each result is therefore a Bernoulli trial.) The person who gave you the data says that the success probability is most likely somewhere near 0.5, but might be near 0 or 1. The aim of this question is to estimate the success probability using Bayesian methods.\nIn this question, use cmdstanr (see this site for instructions). Documentation for Stan is here. You will probably want to be running R on your own computer.\n\nWrite a Stan program that will estimate the success probability \\(p\\). To do this, start with the likelihood (Stan has a function bernoulli that takes one parameter, the success probability). The data, as 1s and 0s, will be in a vector x. Use a beta distribution with unknown parameters as a prior for p. (We will worry later what those parameters should be.)\n\nSolution\nFile, New and Stan. Leave the template program there if you like, as a reminder of what to do. In the model section is where the likelihood goes, like this:2\nmodel {\n// likelihood\nx ~ bernoulli(p);\n}\nThe right one here is bernoulli since your data are Bernoulli trials (successes and failures, coded as 1s and 0s). If you had a summarized total number of successes and a number of trials, then that would be binomial. It actually doesn’t make any difference which way you do it, but it’s probably easier to think about it this way because it’s more like the Poisson one in lecture.\nThinking ahead, x is going to be data, and p is a parameter, so p will need a prior distribution. The standard one for a Bernoulli success probability is a beta distribution. This is actually the conjugate prior, if you have learned about those: if p has a beta prior and the likelihood is Bernoulli, then the posterior is also beta. Back in the days when algebra was your only hope for this kind of thing, conjugate priors were very helpful, but now that we can sample from any posterior, the fact that a prior is conjugate is neither here nor there. Having said that, the beta distribution is a nice choice for a prior for this, because it is restricted to \\([0, 1]\\) the same way that a Bernoulli p is.\nI’m going leave the prior parameters for p unknown for now; we’ll just call them a and b.3 Here’s our completed model section:\nmodel {\n// prior\np ~ beta(a, b);\n// likelihood\nx ~ bernoulli(p);\n}\na and b are not parameters; they are some numbers that we will supply, so they will be part of the data section. Leaving them unspecified like this, rather than hard-coding them, is good coding practice, since the code we finish with can be used for any Bernoulli estimation problem, not just the one we happen to have.\nThere is only one parameter, p, so the parameters section is short:\nparameters {\nreal&lt;lower=0,upper=1&gt; p;\n}\nWe know that p must be between 0 and 1, so we specify that here so that the sampler doesn’t stray into impossible values for p.\nThat goes before the model section. Everything else is data. We also want to avoid hard-coding the number of observations, so we will also have an n as data, which we declare first, so we can declare the array of values x to be of length n:\ndata {\nint&lt;lower=0&gt; n;\nreal a;\nreal b;\nint&lt;lower=0, upper=1&gt; x[n];\n}\nx is an integer array of length n. This is how you declare one of those: the type is first, along with any limits, and then the length of the array is appended in square brackets to the name of the array.\nArrange your code in a file with extension .stan, with data first, parameters second, and model third. I called mine bernoulli.stan.\nExtra: there are two ways to declare a real-valued array y: as real y[n], or as vector[n] y. Sometimes it matters which way you do it (and I don’t have a clear sense of when it matters). The two ways differ in what you can do with them.\n\\(\\blacksquare\\)\n\nCompile your code, correcting any errors until it compiles properly.\n\nSolution\ncmdstanr goes like this:\n\nm2 &lt;- cmdstan_model(\"bernoulli.stan\")\n\n\nm2\n\ndata {\n  int&lt;lower=0&gt; n;\n  real a;\n  real b;\n  array[n] int&lt;lower=0, upper=1&gt; x;\n}\n\nparameters {\n  real&lt;lower=0,upper=1&gt; p;\n}\n\nmodel {\n  // prior\n  p ~ beta(a, b);\n  // likelihood\n  x ~ bernoulli(p);\n}\n\n\nIf it doesn’t compile, you have some fixing up to do. The likely first problem is that you have missed a semicolon somewhere. The error message will at least give you a hint about where the problem is. Fix any errors you see and try again. If you end up with a different error message, that at least is progress.\n\\(\\blacksquare\\)\n\nThe person who brought you the data told you that the success probability p should be somewhere near 0.5 (and is less likely to be close to 0 or 1). Use this information to pick a prior distribution for p. (The exact answer you get doesn’t really matter, but try to interpret the statement in some kind of sensible way.)\n\nSolution\nI don’t know how much intuition you have for what beta distributions look like, so let’s play around a bit. Let’s imagine we have a random variable \\(Y\\) that has a beta distribution. This distribution has two parameters, usually called \\(a\\) and \\(b\\). Let’s draw some pictures and see if we can find something that would serve as a prior. R has dbeta that is the beta distribution density function.\nStart by choosing some values for \\(Y\\):\n\ny &lt;- seq(0, 1, 0.01)\ny\n\n  [1] 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14\n [16] 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29\n [31] 0.30 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44\n [46] 0.45 0.46 0.47 0.48 0.49 0.50 0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58 0.59\n [61] 0.60 0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.70 0.71 0.72 0.73 0.74\n [76] 0.75 0.76 0.77 0.78 0.79 0.80 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89\n [91] 0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1.00\n\n\nthen work out dbeta of these for your choice of parameters, then plot it. I’m going straight to a function for this, since I anticipate doing it several times. This y and the two parameters should be input to the function:\n\nplot_beta &lt;- function(y, a, b) {\n  tibble(y=y) %&gt;% \n  mutate(density = dbeta(y, a, b)) %&gt;% \n  ggplot(aes(x = y, y = density)) + geom_line()\n}\nplot_beta(y, 1, 1)\n\n\n\n\nThe beta with parameters 1 and 1 is a uniform distribution. (If you look up the beta density function, you’ll see why that is.)\nLet’s try another:\n\nplot_beta(y, 3, 2)\n\n\n\n\nThis one is skewed to the left. You might guess that having the second parameter bigger would make it skewed to the right:\n\nplot_beta(y, 3, 7)\n\n\n\n\nwhich indeed is the case. If you try some other values, you’ll see that this pattern with the skewness continues to hold. Furthermore, the right-skewed distributions have their peak to the left of 0.5, and the left-skewed ones have their peak to the right of 0.5.\nTherefore, you would think, having the two parameters the same would give a symmetric distribution:\n\nplot_beta(y, 2, 2)\n\n\n\n\nNote that the peak is now at 0.5, which is what we wanted.\nThe question called for a prior distribution of values “somewhere near 0.5”, and you could reasonably say that this does the job. What does 3 and 3 look like?\n\nplot_beta(y, 3, 3)\n\n\n\n\nThis is more concentrated around 0.5, and as you increase the two parameter values while keeping them equal, it gets more concentrated still:\n\nplot_beta(y, 20, 20)\n\n\n\n\nFor our purposes, this is undoubtedly too concentrated around 0.5; there is no chance of \\(y\\) being outside \\([0.25, 0.75]\\). I would go with parameters 2 and 2 or maybe 3 and 3. As I said, pretty much any choice of parameter values that are both the same is at least somewhat justifiable.\nIf you don’t want to go through all of this, find some pictures of beta distributions with different parameters, and pick one you like. The Wikipedia page is one place (from which you would probably pick 2 and 2). Here is another, from which you might pick 5 and 5.\nIn practice, you would have some back-and-forth with the person who brought you the data, and try to match what they are willing to say about p, without looking at the data, to what you know or can find out about the beta distribution. This process is called “prior elicitation”.\nExtra: if you have ever obtained the posterior distribution in this case by algebra, you might recall that the effect of the prior distribution is to add some “fake” Bernoulli trials to the data. With \\(a=b=2\\), for example, you imagine \\(2+2-2 = 2\\) fake trials, with \\(2-1=1\\) success and \\(2-1=1\\) failure, to add to the data. This brings the estimate of p a little closer to 0.5 than it would be with just plain maximum likelihood.\n\\(\\blacksquare\\)\n\nCreate an R list that contains all your data for your Stan model. Remember that Stan expects the data in x to be 0s and 1s.\n\nSolution\nTurn those successes and failures in the question into a vector of 0 and 1 values, with 1 being success (you wanted to estimate the probability of success): they were success, failure, success, success, failure, success, success, success.\n\nx &lt;- c(1, 0, 1, 1, 0, 1, 1, 1)\nx\n\n[1] 1 0 1 1 0 1 1 1\n\n\nThen make a “named list” of inputs to your Stan program, including the parameter values for the prior distribution (I went with 2 and 2):\n\nstan_data &lt;- list(\nn = 8,\na = 2,\nb = 2,\nx = x\n)\nstan_data\n\n$n\n[1] 8\n\n$a\n[1] 2\n\n$b\n[1] 2\n\n$x\n[1] 1 0 1 1 0 1 1 1\n\n\n\\(\\blacksquare\\)\n\nRun your Stan model to obtain a simulated posterior distribution, using all the other defaults.\n\nSolution\nThe cmdstanr way:\n\nfit2 &lt;- m2$sample(data = stan_data)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.\n\nfit2\n\n variable  mean median   sd  mad    q5   q95 rhat ess_bulk ess_tail\n     lp__ -8.18  -7.88 0.75 0.33 -9.66 -7.64 1.00     1926     2095\n     p     0.67   0.68 0.13 0.14  0.44  0.87 1.00     1371     1602\n\n\nThis one gives you a 90% posterior interval instead of a 95% one, but the posterior mean is 0.66, as before, and the interval says that p is likely bigger than about 0.4; the data did not narrow it down much apart from that.\n\\(\\blacksquare\\)\n\nMake a plot of the posterior distribution of the probability of success. (Use the posterior and bayesplot packages if convenient.)\n\nSolution\nThis means extracting the sampled values of \\(p\\) first. The cmdstanr way is not very convenient, at least at first:\n\nbern.2a &lt;- fit2$draws()\nstr(bern.2a)\n\n 'draws_array' num [1:1000, 1:4, 1:2] -8.38 -9.05 -7.74 -8.44 -9.31 ...\n - attr(*, \"dimnames\")=List of 3\n  ..$ iteration: chr [1:1000] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ chain    : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ variable : chr [1:2] \"lp__\" \"p\"\n\n\nThis is a 3-dimensional array (sample by chain by variable). For plotting and so on, we really want this as a dataframe. At this point, I would use the posterior and bayesplot packages, which you should install following the instructions for cmdstanr at the top of this page. Put the names of the extra two packages in place of the cmdstanr that you see there.\n\nlibrary(posterior)\nlibrary(bayesplot)\n\nTo get the samples as a dataframe:\n\nbern.2 &lt;- as_draws_df(fit2$draws())\nbern.2\n\n\n\n  \n\n\n\nYou don’t even need to go this far to make a plot of the posterior distribution, because bayesplot does it automatically:\n\nmcmc_hist(fit2$draws(\"p\"), binwidth =  0.05)\n\n\n\n\nRather annoyingly, this plot function passes binwidth on to geom_histogram, but not bins!\nThis is skewed to the left. The reason for the skewness here is that the upper limit for \\(p\\) is 1, and there is a reasonable chance of \\(p\\) being close to 1, so the distribution is skewed in the opposite direction. There is basically no chance of \\(p\\) being close to zero. If we had had more data, it is more likely that the values of \\(p\\) near 0 and 1 would be ruled out, and then we might have ended up with something more symmetric.\nExtra: If you remember the algebra for this, the posterior distribution for p actually has a beta distribution, with parameters \\(2+6=8\\) and \\(2+2=4\\).4 Our simulated posterior looks to have the right kind of shape to be this, being skewed to the left.\n\\(\\blacksquare\\)\n\nThe posterior predictive distribution is rather odd here: the only possible values that can be observed are 0 and 1. Nonetheless, obtain the posterior predictive distribution for these data, and explain briefly why it is not surprising that it came out as it did.\n\nSolution\nWith cmdstanr, start from what I called bern.2.5\nThe way to obtain the (sampled) posterior predictive distribution is to get the posterior distribution of values of \\(p\\) in a dataframe, and make a new column as random values from the data-generating mechanism (here Bernoulli). This is easier to do and then talk about:\n\nbern.2 %&gt;% \n  as_tibble() %&gt;% \n  mutate(x = rbernoulli(4000, p)) -&gt; ppd\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `x = rbernoulli(4000, p)`.\nCaused by warning:\n! `rbernoulli()` was deprecated in purrr 1.0.0.\n\nppd\n\n\n\n  \n\n\n\nThe values of x in the last column are TRUE for success and FALSE for failure (they could have been 1 and 0). Thus, the first x is a Bernoulli trial with success probability the first value of p, the second one uses the second value of p, and so on. Most of the success probabilities are bigger than 0.5, so most of the posterior predictive distribution is successes.\nIt seems to go better if you turn bern.2 into a tibble before generating x.\nA bar chart would be an appropriate plot (you can either think of x as categorical, or as a discrete 0 or 1):\n\nggplot(ppd, aes(x=x)) + geom_bar()\n\n\n\n\nwhich shows the majority of successes in the posterior predictive distribution. The idea is that the data and the posterior predictive distribution ought to be similar, and we did indeed have a majority of successes in our data as well.\nYou might have been perplexed by the 4000 in the code above. bernoulli is vectorized, meaning that if you give it a vector of values for p, it will generate Bernoulli trials for each one in turn, and the whole result should be 4000 values long altogether. We’ll see a way around that in a moment, but you could also do this using rbinom (random binomials) if you do it right:\n\nbern.2 %&gt;% \n  as_tibble() %&gt;% \n  mutate(x = rbinom(4000, 1, p)) \n\n\n\n  \n\n\n\nThere are 4000 random binomials altogether, and each one has one trial. This is confusing, and a less confusing way around this is to work one row at time with rowwise:\n\nbern.2 %&gt;% \n  as_tibble() %&gt;% \n  rowwise() %&gt;% \n  mutate(x = rbernoulli(1, p))\n\n\n\n  \n\n\n\nor\n\nbern.2 %&gt;% \n  as_tibble() %&gt;% \n  rowwise() %&gt;%\n  mutate(x = rbinom(1, 1, p)) \n\n\n\n  \n\n\n\nExtra: I’d also like to put in a plug for the tidybayes package. This works best with rstan, though it will work with cmdstanr also. The first thing it will help you with is setting up the data:\n\nlibrary(tidybayes)\ntibble(x) %&gt;% compose_data()\n\n$x\n[1] 1 0 1 1 0 1 1 1\n\n$n\n[1] 8\n\n\nStarting from a dataframe of data (our x), this returns you a list that you can submit as data = to sampling. Note that it counts how many observations you have, on the basis that you’ll be sending this to Stan as well (we did).\nAnother thing that this will do is to handle categorical variables. Say you had something like this, with g being a group label:\n\nd &lt;- tribble(\n~g, ~y,\n\"a\", 10,\n\"a\", 11,\n\"a\", 12,\n\"b\", 13,\n\"b\", 14,\n\"b\", 15\n)\ncompose_data(d)\n\n$g\n[1] 1 1 1 2 2 2\n\n$n_g\n[1] 2\n\n$y\n[1] 10 11 12 13 14 15\n\n$n\n[1] 6\n\n\nKnowing that Stan only has real and int, it labels the groups with numbers, and keeps track of how many groups there are as well as how many observations. These are all things that Stan needs to know. See slides 32 and 34 of my lecture notes, where I prepare to fit an ANOVA model. The tidybayes way is, I have to say, much cleaner than the way I did it in the lecture notes. After you have fitted the model, tidybayes lets you go back and re-associate the real group names with the ones Stan used, so that you could get a posterior mean and interval for each of the two groups.\nAfter obtaining the posterior distribution, tidybayes also helps in understanding it. This is how you get hold of the sampled values. Install laRs using\n\ndevtools::install_github(\"Agasax/laRs\")\n\n\nlibrary(laRs) \nbern.2 %&gt;% \n  spread_draws(p)\n\n\n\n  \n\n\n\nwhich you can then summarize:\n\nbern.2 %&gt;% spread_draws(p) %&gt;% \n  median_hdi()\n\n\n\n  \n\n\n\nThe median of the posterior distribution, along with a 95% Bayesian posterior interval based on the highest posterior density. There are other possibilities.\nOr you can plot it:\n\nbern.2 %&gt;%\n  spread_draws(p) %&gt;% \n  ggplot(aes(x = p)) + stat_slab()\n\n\n\n\n(a density plot)\nor, posterior predictive distribution:\n\nbern.2 %&gt;% \n  spread_draws(p) %&gt;% \n  rowwise() %&gt;% \n  mutate(x = rbernoulli(1, p)) %&gt;% \n  ggplot(aes(x = x)) +\n  geom_bar()\n\n\n\n\nThis is a nice introduction to tidybayes, with a running example.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "stan.html#footnotes",
    "href": "stan.html#footnotes",
    "title": "23  Bayesian Statistics with Stan",
    "section": "",
    "text": "Alpha and beta don’t have to be integers; you could use seq to create sequences of values for alpha and beta that include decimal numbers.↩︎\nThe comment line, with two slashes on the front, is optional but will help you keep track of what’s what.↩︎\nWe’ll come back later to the question of what a and b should be for our situation.↩︎\nThe first 2 in each case is the parameter of the prior distribution and the second number is the number of successes or failures observed in the data.↩︎\nI am writing this a couple of days after the Ever Given was freed from blocking the Suez Canal. One of the memes I saw about this was actually a meme-upon-a-meme: on the picture of the tiny tractor and the huge ship, someone had superimposed that picture of Bernie Sanders sitting on his chair. Feel the bern.2.↩︎"
  },
  {
    "objectID": "logistic-regression.html#finding-wolf-spiders-on-the-beach",
    "href": "logistic-regression.html#finding-wolf-spiders-on-the-beach",
    "title": "24  Logistic regression",
    "section": "24.1 Finding wolf spiders on the beach",
    "text": "24.1 Finding wolf spiders on the beach\n A team of Japanese researchers were investigating what would cause the burrowing wolf spider Lycosa ishikariana* to be found on a beach. They hypothesized that it had to do with the size of the grains of sand on the beach. They went to 28 beaches in Japan, measured the average sand grain size (in millimetres), and observed the presence or absence of this particular spider on each beach. The data are in link.\n\nWhy would logistic regression be better than regular regression in this case?\nRead in the data and check that you have something sensible. (Look at the data file first: the columns are aligned but the headers are not aligned with them.)\nMake a boxplot of sand grain size according to whether the spider is present or absent. Does this suggest that sand grain size has something to do with presence or absence of the spider?\nFit a logistic regression predicting the presence or absence of spiders from the grain size, and obtain its summary. Note that you will need to do something with the response variable first.\nIs there a significant relationship between grain size and presence or absence of spiders at the \\(\\alpha=0.10\\) level? Explain briefly.\nObtain predicted probabilities of spider presence for a representative collection of grain sizes. I only want predicted probabilities, not any kind of intervals.\nGiven your previous work in this question, does the trend you see in your predicted probabilities surprise you? Explain briefly."
  },
  {
    "objectID": "logistic-regression.html#killing-aphids",
    "href": "logistic-regression.html#killing-aphids",
    "title": "24  Logistic regression",
    "section": "24.2 Killing aphids",
    "text": "24.2 Killing aphids\nAn experiment was designed to examine how well the insecticide rotenone kills aphids that feed on the chrysanthemum plant called Macrosiphoniella sanborni. The explanatory variable is the log concentration (in milligrams per litre) of the insecticide. At each of the five different concentrations, approximately 50 insects were exposed. The number of insects exposed at each concentration, and the number killed, are shown below.\n\nLog-Concentration   Insects exposed    Number killed   \n0.96                       50              6               \n1.33                       48              16              \n1.63                       46              24              \n2.04                       49              42              \n2.32                       50              44              \n\n\nGet these data into R. You can do this by copying the data into a file and reading that into R (creating a data frame), or you can enter the data manually into R using c, since there are not many values. In the latter case, you can create a data frame or not, as you wish. Demonstrate that you have the right data in R.\n* Looking at the data, would you expect there to be a significant effect of log-concentration? Explain briefly.\nWe are going to do a logistic regression to predict how likely an insect is to be killed, as it depends on the log-concentration. Create a suitable response variable, bearing in mind (i) that we have lots of insects exposed to each different concentration, and (ii) what needs to go into each column of the response.\nRun a suitable logistic regression, and obtain a summary of the results.\nDoes your analysis support your answer to (here)? Explain briefly.\nObtain predicted probabilities of an insect’s being killed at each of the log-concentrations in the data set.\nPeople in this kind of work are often interested in the “median lethal dose”. In this case, that would be the log-concentration of the insecticide that kills half the insects. Based on your predictions, roughly what do you think the median lethal dose is?"
  },
  {
    "objectID": "logistic-regression.html#the-effects-of-substance-a",
    "href": "logistic-regression.html#the-effects-of-substance-a",
    "title": "24  Logistic regression",
    "section": "24.3 The effects of Substance A",
    "text": "24.3 The effects of Substance A\nIn a dose-response experiment, animals (or cell cultures or human subjects) are exposed to some toxic substance, and we observe how many of them show some sort of response. In this experiment, a mysterious Substance A is exposed at various doses to 100 cells at each dose, and the number of cells at each dose that suffer damage is recorded. The doses were 10, 20, … 70 (mg), and the number of damaged cells out of 100 were respectively 10, 28, 53, 77, 91, 98, 99.\n\nFind a way to get these data into R, and show that you have managed to do so successfully.\nWould you expect to see a significant effect of dose for these data? Explain briefly.\nFit a logistic regression modelling the probability of a cell being damaged as it depends on dose. Display the results. (Comment on them is coming later.)\nDoes your output indicate that the probability of damage really does increase with dose? (There are two things here: is there really an effect, and which way does it go?)\nObtain predicted damage probabilities for some representative doses.\nDraw a graph of the predicted probabilities, and to that add the observed proportions of damage at each dose. Hints: you will have to calculate the observed proportions first. See here, near the bottom, to find out how to add data to one of these graphs. The geom_point line is the one you need.\n\nLooking at the predicted probabilities, would you say that the model fits well? Explain briefly."
  },
  {
    "objectID": "logistic-regression.html#what-makes-an-animal-get-infected",
    "href": "logistic-regression.html#what-makes-an-animal-get-infected",
    "title": "24  Logistic regression",
    "section": "24.4 What makes an animal get infected?",
    "text": "24.4 What makes an animal get infected?\nSome animals got infected with a parasite. We are interested in whether the likelihood of infection depends on any of the age, weight and sex of the animals. The data are at link. The values are separated by tabs.\n\nRead in the data and take a look at the first few lines. Is this one animal per line, or are several animals with the same age, weight and sex (and infection status) combined onto one line? How can you tell?\n* Make suitable plots or summaries of infected against each of the other variables. (You’ll have to think about sex, um, you’ll have to think about the sex variable, because it too is categorical.) Anything sensible is OK here. You might like to think back to what we did in Question here for inspiration. (You can also investigate table, which does cross-tabulations.)\nWhich, if any, of your explanatory variables appear to be related to infected? Explain briefly.\nFit a logistic regression predicting infected from the other three variables. Display the summary.\n* Which variables, if any, would you consider removing from the model? Explain briefly.\nAre the conclusions you drew in (here) and (here) consistent, or not? Explain briefly.\n* The first and third quartiles of age are 26 and 130; the first and third quartiles of weight are 9 and 16. Obtain predicted probabilities for all combinations of these and sex. (You’ll need to start by making a new data frame, using datagrid to get all the combinations.)"
  },
  {
    "objectID": "logistic-regression.html#the-brain-of-a-cat",
    "href": "logistic-regression.html#the-brain-of-a-cat",
    "title": "24  Logistic regression",
    "section": "24.5 The brain of a cat",
    "text": "24.5 The brain of a cat\nA large number (315) of psychology students were asked to imagine that they were serving on a university ethics committee hearing a complaint against animal research being done by a member of the faculty. The students were told that the surgery consisted of implanting a device called a cannula in each cat’s brain, through which chemicals were introduced into the brain and the cats were then given psychological tests. At the end of the study, the cats’ brains were subjected to histological analysis. The complaint asked that the researcher’s authorization to carry out the study should be withdrawn, and the cats should be handed over to the animal rights group that filed the complaint. It was suggested that the research could just as well be done with computer simulations.\nAll of the psychology students in the survey were told all of this. In addition, they read a statement by the researcher that no animal felt much pain at any time, and that computer simulation was not an adequate substitute for animal research. Each student was also given one of the following scenarios that explained the benefit of the research:\n\n“cosmetic”: testing the toxicity of chemicals to be used in new lines of hair care products.\n“theory”: evaluating two competing theories about the function of a particular nucleus in the brain.\n“meat”: testing a synthetic growth hormone said to potentially increase meat production.\n“veterinary”: attempting to find a cure for a brain disease that is killing domesticated cats and endangered species of wild cats.\n“medical”: evaluating a potential cure for a debilitating disease that afflicts many young adult humans.\n\nFinally, each student completed two questionnaires: one that would assess their “relativism”: whether or not they believe in universal moral principles (low score) or whether they believed that the appropriate action depends on the person and situation (high score). The second questionnaire assessed “idealism”: a high score reflects a belief that ethical behaviour will always lead to good consequences (and thus that if a behaviour leads to any bad consequences at all, it is unethical).1\nAfter being exposed to all of that, each student stated their decision about whether the research should continue or stop.\nI should perhaps stress at this point that no actual cats were harmed in the collection of these data (which can be found as a .csv file at link). The variables in the data set are these:\n\ndecision: whether the research should continue or stop (response)\nidealism: score on idealism questionnaire\nrelativism: score on relativism questionnaire\ngender of student\nscenario of research benefits that the student read.\n\nA more detailed discussion^(“[If you can believe it.] of this study is at link.\n\nRead in the data and check by looking at the structure of your data frame that you have something sensible. Do not call your data frame decision, since that’s the name of one of the variables in it.\nFit a logistic regression predicting decision from gender. Is there an effect of gender?\nTo investigate the effect (or non-effect) of gender, create a contingency table by feeding decision and gender into table. What does this tell you?\n* Is your slope for gender in the previous logistic regression positive or negative? Is it applying to males or to females? Looking at the conclusions from your contingency table, what probability does that mean your logistic regression is actually modelling?\nAdd the two variables idealism and relativism to your logistic regression. Do either or both of them add significantly to your model? Explain briefly.\nAdd the variable scenario to your model. That is, fit a new model with that variable plus all the others.\nUse anova to compare the models with and without scenario. You’ll have to add a test=\"Chisq\" to your anova, to make sure that the test gets done. Does scenario make a difference or not, at \\(\\alpha=0.10\\)? Explain briefly. (The reason we have to do it this way is that scenario is a factor with five levels, so it has four slope coefficients. To test them all at once, which is what we need to make an overall test for scenario, this is the way it has to be done.)\nLook at the summary of your model that contained scenario. Bearing in mind that the slope coefficient for scenariocosmetic is zero (since this is the first scenario alphabetically), which scenarios have the most positive and most negative slope coefficients? What does that tell you about those scenarios’ effects?\nDescribe the effects that having (i) a higher idealism score and (ii) a higher relativity score have on a person’s probability of saying that the research should stop. Do each of these increase or decrease that probability? Explain briefly."
  },
  {
    "objectID": "logistic-regression.html#how-not-to-get-heart-disease",
    "href": "logistic-regression.html#how-not-to-get-heart-disease",
    "title": "24  Logistic regression",
    "section": "24.6 How not to get heart disease",
    "text": "24.6 How not to get heart disease\nWhat is associated with heart disease? In a study, a large number of variables were measured, as follows:\n\nage (years)\nsex male or female\npain.type Chest pain type (4 values: typical angina, atypical angina, non-anginal pain, asymptomatic)\nresting.bp Resting blood pressure, on admission to hospital\nserum.chol Serum cholesterol\nhigh.blood.sugar: greater than 120, yes or no\nelectro resting electrocardiographic results (normal, having ST-T, hypertrophy)\nmax.hr Maximum heart rate\nangina Exercise induced angina (yes or no)\noldpeak ST depression induced by exercise relative to rest. See link.\nslope Slope of peak exercise ST segment. Sloping up, flat or sloping down\ncolored number of major vessels (0–3) coloured by fluoroscopy\nthal normal, fixed defect, reversible defect\nheart.disease yes, no (response)\n\nI don’t know what most of those are, but we will not let that stand in our way. Our aim is to find out what variables are associated with heart disease, and what values of those variables give high probabilities of heart disease being present. The data are in link.\n\nRead in the data. Display the first few lines and convince yourself that those values are reasonable.\nIn a logistic regression, what probability will be predicted here? Explain briefly but convincingly. (Is each line of the data file one observation or a summary of several?)\n* Fit a logistic regression predicting heart disease from everything else (if you have a column called X or X1, ignore that), and display the results.\nQuite a lot of our explanatory variables are factors. To assess whether the factor as a whole should stay or can be removed, looking at the slopes won’t help us very much (since they tell us whether the other levels of the factor differ from the baseline, which may not be a sensible comparison to make). To assess which variables are candidates to be removed, factors included (properly), we can use drop1. Feed drop1 a fitted model and the words test=\"Chisq\" (take care of the capitalization!) and you’ll get a list of P-values. Which variable is the one that you would remove first? Explain briefly.\nI’m not going to make you do the whole backward elimination (I’m going to have you use step for that later), but do one step: that is, fit a model removing the variable you think should be removed, using update, and then run drop1 again to see which variable will be removed next.\nUse step to do a backward elimination to find which variables have an effect on heart disease. Display your final model (which you can do by saving the output from step in a variable, and asking for the summary of that. In step, you’ll need to specify a starting model (the one from part (here)), the direction of elimination, and the test to display the P-value for (the same one as you used in drop1). (Note: the actual decision to keep or drop explanatory variables is based on AIC rather than the P-value, with the result that step will sometimes keep variables you would have dropped, with P-values around 0.10.)\nDisplay the summary of the model that came out of step.\nWe are going to make a large number of predictions. Create and save a data frame that contains predictions for all combinations of representative values for all the variables in the model that came out of step. By “representative” I mean all the values for a categorical variable, and the five-number summary for a numeric variable. (Note that you will get a lot of predictions.)\nFind the largest predicted probability (which is the predicted probability of heart disease) and display all the variables that it was a prediction for.\nCompare the summary of the final model from step with your highest predicted heart disease probability and the values of the other variables that make it up. Are they consistent?"
  },
  {
    "objectID": "logistic-regression.html#successful-breastfeeding",
    "href": "logistic-regression.html#successful-breastfeeding",
    "title": "24  Logistic regression",
    "section": "24.7 Successful breastfeeding",
    "text": "24.7 Successful breastfeeding\nA regular pregnancy lasts 40 weeks, and a baby that is born at or before 33 weeks is called “premature”. The number of weeks at which a baby is born is called its “gestational age”. Premature babies are usually smaller than normal and may require special care. It is a good sign if, when the mother and baby leave the hospital to go home, the baby is successfully breastfeeding.\nThe data in link are from a study of 64 premature infants. There are three columns: the gestational age (a whole number of weeks), the number of babies of that gestational age that were successfully breastfeeding when they left the hospital, and the number that were not. (There were multiple babies of the same gestational age, so the 64 babies are summarized in 6 rows of data.)\n\nRead the data into R and display the data frame.\nVerify that there were indeed 64 infants, by having R do a suitable calculation on your data frame that gives the right answer for the right reason.\nDo you think, looking at the data, that there is a relationship between gestational age and whether or not the baby was successfully breastfeeding when it left the hospital? Explain briefly.\nWhy is logistic regression a sensible technique to use here? Explain briefly.\nFit a logistic regression to predict the probability that an infant will be breastfeeding from its gestational age. Show the output from your logistic regression.\nDoes the significance or non-significance of the slope of gest.age surprise you? Explain briefly.\nIs your slope (in the Estimate column) positive or negative? What does that mean, in terms of gestational ages and breastfeeding? Explain briefly.\nObtain the predicted probabilities that an infant will successfully breastfeed for a representative collection of gestational ages."
  },
  {
    "objectID": "logistic-regression.html#making-it-over-the-mountains",
    "href": "logistic-regression.html#making-it-over-the-mountains",
    "title": "24  Logistic regression",
    "section": "24.8 Making it over the mountains",
    "text": "24.8 Making it over the mountains\nIn 1846, the Donner party (Donner and Reed families) left Springfield, Illinois for California in covered wagons. After reaching Fort Bridger, Wyoming, the leaders decided to find a new route to Sacramento. They became stranded in the eastern Sierra Nevada mountains at a place now called Donner Pass, when the region was hit by heavy snows in late October. By the time the survivors were rescued on April 21, 1847, 40 out of 87 had died.\nAfter the rescue, the age and gender of each person in the party was recorded, along with whether they survived or not. The data are in link.\n\nRead in the data and display its structure. Does that agree with the description in the previous paragraph?\nMake graphical or numerical summaries for each pair of variables. That is, you should make a graph or numerical summary for each of age vs. gender, age vs.\nsurvived and gender vs. survived. In choosing the kind of graph or summary that you will use, bear in mind that survived and gender are factors with two levels.\nFor each of the three graphs or summaries in the previous question, what do they tell you about the relationship between the pair of variables concerned? Explain briefly.\nFit a logistic regression predicting survival from age and gender. Display the summary.\nDo both explanatory variables have an impact on survival? Does that seem to be consistent with your numerical or graphical summaries? Explain briefly.\nAre the men typically older, younger or about the same age as the women? Considering this, explain carefully what the negative gendermale slope in your logistic regression means.\nObtain predicted probabilities of survival for each combination of some representative ages and of the two genders in this dataset.\nDo your predictions support your conclusions from earlier about the effects of age and gender? Explain briefly."
  },
  {
    "objectID": "logistic-regression.html#who-needs-the-most-intensive-care",
    "href": "logistic-regression.html#who-needs-the-most-intensive-care",
    "title": "24  Logistic regression",
    "section": "24.9 Who needs the most intensive care?",
    "text": "24.9 Who needs the most intensive care?\nThe “APACHE II” is a scale for assessing patients who arrive in the intensive care unit (ICU) of a hospital. These are seriously ill patients who may die despite the ICU’s best attempts. APACHE stands for “Acute Physiology And Chronic Health Evaluation”.2 The scale score is calculated from several physiological measurements such as body temperature, heart rate and the Glasgow coma scale, as well as the patient’s age. The final result is a score between 0 and 71, with a higher score indicating more severe health issues. Is it true that a patient with a higher APACHE II score has a higher probability of dying?\nData from one hospital are in link. The columns are: the APACHE II score, the total number of patients who had that score, and the number of patients with that score who died.\n\nRead in and display the data (however much of it displays). Why are you convinced that have the right thing?\nDoes each row of the data frame relate to one patient or sometimes to more than one? Explain briefly.\nExplain why this is the kind of situation where you need a two-column response, and create this response variable, bearing in mind that I will (later) want you to estimate the probability of dying, given the apache score.\nFit a logistic regression to estimate the probability of death from the apache score, and display the results.\nIs there a significant effect of apache score on the probability of survival? Explain briefly.\nIs the effect of a larger apache score to increase or to decrease the probability of death? Explain briefly.\nObtain the predicted probability of death for a representative collection of the apache scores that were in the data set. Do your predictions behave as you would expect (from your earlier work)?\nMake a plot of predicted death probability against apache score (joined by lines) with, also on the plot, the observed proportion of deaths within each apache score, plotted against apache score. Does there seem to be good agreement between observation and prediction? Hint: calculate what you need to from the original dataframe first, save it, then make a plot of the predictions, and then to the plot add the appropriate thing from the dataframe you just saved."
  },
  {
    "objectID": "logistic-regression.html#go-away-and-dont-come-back",
    "href": "logistic-regression.html#go-away-and-dont-come-back",
    "title": "24  Logistic regression",
    "section": "24.10 Go away and don’t come back!",
    "text": "24.10 Go away and don’t come back!\nWhen a person has a heart attack and survives it, the major concern of health professionals is to prevent the person having a second heart attack. Two factors that are believed to be important are anger and anxiety; if a heart attack survivor tends to be angry or anxious, they are believed to put themselves at increased risk of a second heart attack.\nTwenty heart attack survivors took part in a study. Each one was given a test designed to assess their anxiety (a higher score on the test indicates higher anxiety), and some of the survivors took an anger management course. The data are in link; y and n denote “yes” and “no” respectively. The columns denote (i) whether or not the person had a second heart attack, (ii) whether or not the person took the anger management class, (iii) the anxiety score.\n\nRead in and display the data.\n* Fit a logistic regression predicting whether or not a heart attack survivor has a second heart attack, as it depends on anxiety score and whether or not the person took the anger management class. Display the results.\nIn the previous part, how can you tell that you were predicting the probability of having a second heart attack (as opposed to the probability of not having one)?\n* For the two possible values y and n of anger and the anxiety scores 40, 50 and 60, make a data frame containing all six combinations, and use it to obtain predicted probabilities of a second heart attack. Display your predicted probabilities side by side with what they are predictions for.\nUse your predictions from the previous part to describe the effect of changes in anxiety and anger on the probability of a second heart attack.\nAre the effects you described in the previous part consistent with the summary output from glm that you obtained in (here)? Explain briefly how they are, or are not. (You need an explanation for each of anxiety and anger, and you will probably get confused if you look at the P-values, so don’t.)\n\nMy solutions follow:"
  },
  {
    "objectID": "logistic-regression.html#finding-wolf-spiders-on-the-beach-1",
    "href": "logistic-regression.html#finding-wolf-spiders-on-the-beach-1",
    "title": "24  Logistic regression",
    "section": "24.11 Finding wolf spiders on the beach",
    "text": "24.11 Finding wolf spiders on the beach\n A team of Japanese researchers were investigating what would cause the burrowing wolf spider Lycosa ishikariana* to be found on a beach. They hypothesized that it had to do with the size of the grains of sand on the beach. They went to 28 beaches in Japan, measured the average sand grain size (in millimetres), and observed the presence or absence of this particular spider on each beach. The data are in link.\n\nWhy would logistic regression be better than regular regression in this case?\n\nSolution\nBecause the response variable, whether or not the spider is present, is a categorical yes/no success/failure kind of variable rather than a quantitative numerical one, and when you have this kind of response variable, this is when you want to use logistic regression.\n\\(\\blacksquare\\)\n\nRead in the data and check that you have something sensible. (Look at the data file first: the columns are aligned but the headers are not aligned with them.)\n\nSolution\nThe nature of the file means that you need read_table:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/spiders.txt\"\nspider &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Grain.size = col_double(),\n  Spiders = col_character()\n)\n\nspider\n\n\n\n  \n\n\n\nWe have a numerical sand grain size and a categorical variable Spiders that indicates whether the spider was present or absent. As we were expecting. (The categorical variable is actually text or chr, which will matter in a minute.)\n\\(\\blacksquare\\)\n\nMake a boxplot of sand grain size according to whether the spider is present or absent. Does this suggest that sand grain size has something to do with presence or absence of the spider?\n\nSolution\n\nggplot(spider, aes(x = Spiders, y = Grain.size)) + geom_boxplot()\n\n\n\n\nThe story seems to be that when spiders are present, the sand grain size tends to be larger. So we would expect to find some kind of useful relationship in the logistic regression.\nNote that we have reversed the cause and effect here: in the boxplot we are asking “given that the spider is present or absent, how big are the grains of sand?”, whereas the logistic regression is asking “given the size of the grains of sand, how likely is the spider to be present?”. But if one variable has to do with the other, we would hope to see the link either way around.\n\\(\\blacksquare\\)\n\nFit a logistic regression predicting the presence or absence of spiders from the grain size, and obtain its summary. Note that you will need to do something with the response variable first.\n\nSolution\nThe presence or absence of spiders is our response. This is actually text at the moment:\n\nspider\n\n\n\n  \n\n\n\nso we need to make a factor version of it first. I’m going to live on the edge and overwrite everything:\n\nspider &lt;- spider %&gt;% mutate(Spiders = factor(Spiders))\nspider\n\n\n\n  \n\n\n\nSpiders is now a factor, correctly. (Sometimes a text variable gets treated as a factor, sometimes it needs to be an explicit factor. This is one of those times.) Now we go ahead and fit the model. I’m naming this as response-with-a-number, so I still have the Capital Letter. Any choice of name is good, though.\n\nSpiders.1 &lt;- glm(Spiders ~ Grain.size, family = \"binomial\", data = spider)\nsummary(Spiders.1)\n\n\nCall:\nglm(formula = Spiders ~ Grain.size, family = \"binomial\", data = spider)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)   -1.648      1.354  -1.217   0.2237  \nGrain.size     5.122      3.006   1.704   0.0884 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.165  on 27  degrees of freedom\nResidual deviance: 30.632  on 26  degrees of freedom\nAIC: 34.632\n\nNumber of Fisher Scoring iterations: 5\n\n\n\\(\\blacksquare\\)\n\nIs there a significant relationship between grain size and presence or absence of spiders at the \\(\\alpha=0.10\\) level? Explain briefly.\n\nSolution\nThe P-value on the Grain.size line is just less than 0.10 (it is 0.088), so there is just a significant relationship. It isn’t a very strong significance, though. This might be because we don’t have that much data: even though we have 28 observations, which, on the face of it, is not a very small sample size, each observation doesn’t tell us much: only whether the spider was found on that beach or not. Typical sample sizes in logistic regression are in the hundreds — the same as for opinion polls, because you’re dealing with the same kind of data. The weak significance here lends some kind of weak support to the researchers’ hypothesis, but I’m sure they were hoping for something better.\n\\(\\blacksquare\\)\n\nObtain predicted probabilities of spider presence for a representative collection of grain sizes. I only want predicted probabilities, not any kind of intervals.\n\nSolution\nMake a data frame of values to predict from directly, using tribble or for that matter datagrid. For some reason, I chose these four values:\n\nnew &lt;- tribble(\n  ~Grain.size,\n  0.2,\n  0.5,\n  0.8,\n  1.1\n)\nnew\n\n\n\n  \n\n\n\nand then\n\ncbind(predictions(Spiders.1, newdata = new))\n\n\n\n  \n\n\n\nOne of the above is all I need. I prefer the first one, since that way we don’t even have to decide what th e representative values are.\nExtra:\nNote that the probabilities don’t go up linearly. (If they did, they would soon get bigger than 1!). It’s actually the log-odds that go up linearly.\nTo verify this, you can add a type to the predictions:\n\ncbind(predictions(Spiders.1, newdata = new, type = \"link\"))\n\n\n\n  \n\n\n\nThis one, as you see shortly, makes more sense with equally-spaced grain sizes.\nThe meaning of that slope coefficient in the summary, which is about 5, is that if you increase grain size by 1, you increase the log-odds by 5. In the table above, we increased the grain size by 0.3 each time, so we would expect to increase the log-odds by \\((0.3)(5)=1.5\\), which is (to this accuracy) what happened.\nLog-odds are hard to interpret. Odds are a bit easier, and to get them, we have to exp the log-odds:\n\ncbind(predictions(Spiders.1, newdata = new, type = \"link\")) %&gt;% \n  mutate(exp_pred = exp(estimate))\n\n\n\n  \n\n\n\nThus, with each step of 0.3 in grain size, we multiply the odds of finding a spider by about\n\nexp(1.5)\n\n[1] 4.481689\n\n\nor about 4.5 times. If you’re a gambler, this might give you a feel for how large the effect of grain size is. Or, of course, you can just look at the probabilities.\n\\(\\blacksquare\\)\n\nGiven your previous work in this question, does the trend you see in your predicted probabilities surprise you? Explain briefly.\n\nSolution\nMy predicted probabilities go up as grain size goes up. This fails to surprise me for a couple of reasons: first, in my boxplots, I saw that grain size tended to be larger when spiders were present, and second, in my logistic regression summary, the slope was positive, so likewise spiders should be more likely as grain size goes up. Observing just one of these things is enough, though of course it’s nice if you can spot both.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#killing-aphids-1",
    "href": "logistic-regression.html#killing-aphids-1",
    "title": "24  Logistic regression",
    "section": "24.12 Killing aphids",
    "text": "24.12 Killing aphids\nAn experiment was designed to examine how well the insecticide rotenone kills aphids that feed on the chrysanthemum plant called Macrosiphoniella sanborni. The explanatory variable is the log concentration (in milligrams per litre) of the insecticide. At each of the five different concentrations, approximately 50 insects were exposed. The number of insects exposed at each concentration, and the number killed, are shown below.\n\nLog-Concentration   Insects exposed    Number killed   \n0.96                       50              6               \n1.33                       48              16              \n1.63                       46              24              \n2.04                       49              42              \n2.32                       50              44              \n\n\nGet these data into R. You can do this by copying the data into a file and reading that into R (creating a data frame), or you can enter the data manually into R using c, since there are not many values. In the latter case, you can create a data frame or not, as you wish. Demonstrate that you have the right data in R.\n\nSolution\nThere are a couple of ways. My current favourite is the tidyverse-approved tribble method. A tribble is a “transposed tibble”, in which you copy and paste the data, inserting column headings and commas in the right places. The columns don’t have to line up, since it’s the commas that determine where one value ends and the next one begins:\n\ndead_bugs &lt;- tribble(\n  ~log_conc, ~exposed, ~killed,\n  0.96, 50, 6,\n  1.33, 48, 16,\n  1.63, 46, 24,\n  2.04, 49, 42,\n  2.32, 50, 44\n)\ndead_bugs\n\n\n\n  \n\n\n\nNote that the last data value has no comma after it, but instead has the closing bracket of tribble.\nYou can have extra spaces if you wish. They will just be ignored. If you are clever in R Studio, you can insert a column of commas all at once (using “multiple cursors”). I used to do it like this. I make vectors of each column using c and then glue the columns together into a data frame:\n\nlog_conc &lt;- c(0.96, 1.33, 1.63, 2.04, 2.32)\nexposed &lt;- c(50, 48, 46, 49, 50)\nkilled &lt;- c(6, 16, 24, 42, 44)\ndead_bugs2 &lt;- tibble(log_conc, exposed, killed)\ndead_bugs2\n\n\n\n  \n\n\n\nThe values are correct — I checked them.\nNow you see why tribble stands for “transposed tibble”: if you want to construct a data frame by hand, you have to work with columns and then glue them together, but tribble allows you to work “row-wise” with the data as you would lay it out on the page.\nThe other obvious way to read the data values without typing them is to copy them into a file and read that. The values as laid out are aligned in columns. They might be separated by tabs, but they are not. (It’s hard to tell without investigating, though a tab is by default eight spaces and these values look farther apart than that.) I copied them into a file exposed.txt in my current folder (or use file.choose):\n\nbugs2 &lt;- read_table(\"exposed.txt\")\n\nWarning: Missing column names filled in: 'X6' [6]\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  `Log-Concentration` = col_double(),\n  Insects = col_double(),\n  exposed = col_double(),\n  Number = col_logical(),\n  killed = col_character(),\n  X6 = col_character()\n)\n\n\nWarning: 5 parsing failures.\nrow col  expected    actual          file\n  1  -- 6 columns 4 columns 'exposed.txt'\n  2  -- 6 columns 4 columns 'exposed.txt'\n  3  -- 6 columns 4 columns 'exposed.txt'\n  4  -- 6 columns 4 columns 'exposed.txt'\n  5  -- 6 columns 4 columns 'exposed.txt'\n\nbugs2\n\n\n\n  \n\n\n\nThis didn’t quite work: the last column Number killed got split into two, with the actual number killed landing up in Number and the column killed being empty. If you look at the data file, the data values for Number killed are actually aligned with the word Number, which is why it came out this way. Also, you’ll note, the column names have those “backticks” around them, because they contain illegal characters like a minus sign and spaces. Perhaps a good way to pre-empt3 all these problems is to make a copy of the data file with the illegal characters replaced by underscores, which is my file exposed2.txt:\n\nbugs2 &lt;- read_table(\"exposed2.txt\")\n\nWarning: Missing column names filled in: 'X4' [4]\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Log_Concentration = col_double(),\n  Insects_exposed = col_double(),\n  Number_killed = col_double(),\n  X4 = col_logical()\n)\n\nbugs2\n\n\n\n  \n\n\n\nThis is definitely good. We’d have to be careful with Capital Letters this way, but it’s definitely good.\nYou may have thought that this was a lot of fuss to make about reading in data, but the point is that data can come your way in lots of different forms, and you need to be able to handle whatever you receive so that you can do some analysis with it.\n\\(\\blacksquare\\)\n\n* Looking at the data, would you expect there to be a significant effect of log-concentration? Explain briefly.\n\nSolution\nThe numbers of insects killed goes up sharply as the concentration increases, while the numbers of insects exposed don’t change much. So I would expect to see a strong, positive effect of concentration, and I would expect it to be strongly significant, especially since we have almost 250 insects altogether.\n\\(\\blacksquare\\)\n\nWe are going to do a logistic regression to predict how likely an insect is to be killed, as it depends on the log-concentration. Create a suitable response variable, bearing in mind (i) that we have lots of insects exposed to each different concentration, and (ii) what needs to go into each column of the response.\n\nSolution\nThere needs to be a two-column response variable. The first column needs to be the number of “successes” (insects killed, here) and the second needs to be the number of “failures” (insects that survived). We don’t actually have the latter, but we know how many insects were exposed in total to each dose, so we can work it out. Like this:\n\ndead_bugs %&gt;%\n  mutate(survived = exposed - killed) %&gt;%\n  select(killed, survived) %&gt;%\n  as.matrix() -&gt; response\nresponse\n\n     killed survived\n[1,]      6       44\n[2,]     16       32\n[3,]     24       22\n[4,]     42        7\n[5,]     44        6\n\n\nglm requires an R matrix rather than a data frame, so the last stage of our pipeline is to create one (using the same numbers that are in the data frame: all the as. functions do is to change what type of thing it is, without changing its contents).\nIt’s also equally good to create the response outside of the data frame and use cbind to glue its columns together:\n\nresp2 &lt;- with(\n  dead_bugs,\n  cbind(killed, survived = exposed - killed)\n)\nresp2\n\n     killed survived\n[1,]      6       44\n[2,]     16       32\n[3,]     24       22\n[4,]     42        7\n[5,]     44        6\n\n\n\\(\\blacksquare\\)\n\nRun a suitable logistic regression, and obtain a summary of the results.\n\nSolution\nI think you know how to do this by now:\n\nbugs.1 &lt;- glm(response ~ log_conc, family = \"binomial\", data = dead_bugs)\nsummary(bugs.1)\n\n\nCall:\nglm(formula = response ~ log_conc, family = \"binomial\", data = dead_bugs)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -4.8923     0.6426  -7.613 2.67e-14 ***\nlog_conc      3.1088     0.3879   8.015 1.11e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 96.6881  on 4  degrees of freedom\nResidual deviance:  1.4542  on 3  degrees of freedom\nAIC: 24.675\n\nNumber of Fisher Scoring iterations: 4\n\n\n\\(\\blacksquare\\)\n\nDoes your analysis support your answer to (here)? Explain briefly.\n\nSolution\nThat’s a very small P-value, \\(1.1\\times 10^{-15}\\), on log_conc, so there is no doubt that concentration has an effect on an insect’s chances of being killed. This is exactly what I guessed in (here), which I did before looking at the results, honest!\n\\(\\blacksquare\\)\n\nObtain predicted probabilities of an insect’s being killed at each of the log-concentrations in the data set.\n\nSolution\nUse a newdata that is the original dataframe:\n\ncbind(predictions(bugs.1, newdata = dead_bugs))\n\n\n\n  \n\n\n\nThe advantage of this is that you can compare the observed with the predicted. For example, 44 out of 50 insects were killed at log-dose 2.32, which is a proportion of 0.88, pretty close to the prediction of 0.91.\nExtra: you could also make a plot of these. The normal thing is to use plot_cap, but this time, the response variable is that matrix thing outside the dataframe, which confuses the issue. So let’s make a more detailed set of predictions and plot them ourselves (effectively, doing the same thing plot_cap does but building it ourselves):\n\nnew &lt;- tibble(log_conc = seq(0.5, 2.5, 0.01))\ncbind(predictions(bugs.1, newdata = new)) %&gt;% \n  select(log_conc, estimate, conf.low, conf.high) %&gt;% \n  ggplot(aes(x = log_conc, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3)\n\n\n\n\nAs the log-concentration goes up, you can see how clearly the probability of the aphid being killed goes up. The confidence band is narrow because there is lots of data (almost 250 aphids altogether).\nThe final alpha = 0.3 makes the ribbon see-through, so that you can see the actual predictions behind it. A smaller value of alpha makes it more transparent, but I think this strikes a decent compromise between being able to clearly see both the predictions and the confidence limits.\n\\(\\blacksquare\\)\n\nPeople in this kind of work are often interested in the “median lethal dose”. In this case, that would be the log-concentration of the insecticide that kills half the insects. Based on your predictions, roughly what do you think the median lethal dose is?\n\nSolution\nThe log-concentration of 1.63 is predicted to kill just over half the insects, so the median lethal dose should be a bit less than 1.63. It should not be as small as 1.33, though, since that log-concentration only kills less than a third of the insects. So I would guess somewhere a bit bigger than 1.5. Any guess somewhere in this ballpark is fine: you really cannot be very accurate.\nIf you read through the Extra to the previous part (or at least looked at the graph), the median lethal dose is where the curve of predictions passes through 0.5 on the \\(y\\)-axis. This is at a log-concentration of just less than 1.6; if you judge from the scale where the crossing-point is, it’s something like 1.57.\nExtra: this is kind of a strange prediction problem, because we know what the response variable should be, and we want to know what the explanatory variable’s value is. Normally we do predictions the other way around.4 So the only way to get a more accurate figure is to try some different log-concentrations, and see which one gets closest to a probability 0.5 of killing the insect.\nSomething like this would work:\n\nnew &lt;- datagrid(model = bugs.1, log_conc = seq(1.5, 1.63, 0.01))\ncbind(predictions(bugs.1, newdata = new))\n\n\n\n  \n\n\n\nThe closest one of these to a probability of 0.5 is 0.4971, which goes with a log-concentration of 1.57: indeed, a bit bigger than 1.5 and a bit less than 1.63, and close to what I read off from my graph. The seq in the construction of the new data frame is “fill sequence”: go from 1.5 to 1.63 in steps of 0.01. We are predicting for values of log_conc that we chose, so the way to go is to make a new dataframe with datagrid and then feed that into predictions with newdata.\nNow, of course this is only our “best guess”, like a single-number prediction in regression. There is uncertainty attached to it (because the actual logistic regression might be different from the one we estimated), so we ought to provide a confidence interval for it. But I’m riding the bus as I type this, so I can’t look it up right now.\nLater: there’s a function called dose.p in MASS that appears to do this:\n\nlethal &lt;- dose.p(bugs.1)\nlethal\n\n             Dose         SE\np = 0.5: 1.573717 0.05159576\n\n\nWe have a sensible point estimate (the same 1.57 that we got by hand), and we have a standard error, so we can make a confidence interval by going up and down twice it (or 1.96 times it) from the estimate. The structure of the result is a bit arcane, though:\n\nstr(lethal)\n\n 'glm.dose' Named num 1.57\n - attr(*, \"names\")= chr \"p = 0.5:\"\n - attr(*, \"SE\")= num [1, 1] 0.0516\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr \"p = 0.5:\"\n  .. ..$ : NULL\n - attr(*, \"p\")= num 0.5\n\n\nIt is what R calls a “vector with attributes”. To get at the pieces and calculate the interval, we have to do something like this:\n\n(lethal_est &lt;- as.numeric(lethal))\n\n[1] 1.573717\n\n(lethal_SE &lt;- as.vector(attr(lethal, \"SE\")))\n\n[1] 0.05159576\n\n\nand then make the interval:\n\nlethal_est + c(-2, 2) * lethal_SE\n\n[1] 1.470526 1.676909\n\n\n1.47 to 1.68.\nI got this idea from page 4.14 of link. I think I got a little further than he did. An idea that works more generally is to get several intervals all at once, say for the “quartile lethal doses” as well:\n\nlethal &lt;- dose.p(bugs.1, p = c(0.25, 0.5, 0.75))\nlethal\n\n              Dose         SE\np = 0.25: 1.220327 0.07032465\np = 0.50: 1.573717 0.05159576\np = 0.75: 1.927108 0.06532356\n\n\nThis looks like a data frame or matrix, but is actually a “named vector”, so enframe will get at least some of this and turn it into a genuine data frame:\n\nenframe(lethal)\n\n\n\n  \n\n\n\nThat doesn’t get the SEs, so we’ll make a new column by grabbing the “attribute” as above:\n\nenframe(lethal) %&gt;% mutate(SE = attr(lethal, \"SE\"))\n\n\n\n  \n\n\n\nand now we make the intervals by making new columns containing the lower and upper limits:\n\nenframe(lethal) %&gt;%\n  mutate(SE = attr(lethal, \"SE\")) %&gt;%\n  mutate(LCL = value - 2 * SE, UCL = value + 2 * SE)\n\n\n\n  \n\n\n\nNow we have intervals for the median lethal dose, as well as for the doses that kill a quarter and three quarters of the aphids.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#the-effects-of-substance-a-1",
    "href": "logistic-regression.html#the-effects-of-substance-a-1",
    "title": "24  Logistic regression",
    "section": "24.13 The effects of Substance A",
    "text": "24.13 The effects of Substance A\nIn a dose-response experiment, animals (or cell cultures or human subjects) are exposed to some toxic substance, and we observe how many of them show some sort of response. In this experiment, a mysterious Substance A is exposed at various doses to 100 cells at each dose, and the number of cells at each dose that suffer damage is recorded. The doses were 10, 20, … 70 (mg), and the number of damaged cells out of 100 were respectively 10, 28, 53, 77, 91, 98, 99.\n\nFind a way to get these data into R, and show that you have managed to do so successfully.\n\nSolution\nThere’s not much data here, so we don’t need to create a file, although you can do so if you like (in the obvious way: type the doses and damaged cell numbers into a .txt file or spreadsheet and read in with the appropriate read_ function). Or, use a tribble:\n\ndr &lt;- tribble(\n  ~dose, ~damaged,\n  10, 10,\n  20, 28,\n  30, 53,\n  40, 77,\n  50, 91,\n  60, 98,\n  70, 99\n)\ndr\n\n\n\n  \n\n\n\nOr, make a data frame with the values typed in:\n\ndr2 &lt;- tibble(\n  dose = seq(10, 70, 10),\n  damaged = c(10, 28, 53, 77, 91, 98, 99)\n)\ndr2\n\n\n\n  \n\n\n\nseq fills a sequence “10 to 70 in steps of 10”, or you can just list the doses.\nI like this better than making two columns not attached to a data frame, but that will work as well.\nFor these, find a way you like, and stick with it.\n\\(\\blacksquare\\)\n\nWould you expect to see a significant effect of dose for these data? Explain briefly.\n\nSolution\nThe number of damaged cells goes up sharply as the dose goes up (from a very small number to almost all of them). So I’d expect to see a strongly significant effect of dose. This is far from something that would happen by chance.\n\\(\\blacksquare\\)\n\nFit a logistic regression modelling the probability of a cell being damaged as it depends on dose. Display the results. (Comment on them is coming later.)\n\nSolution\nThis has a bunch of observations at each dose (100 cells, in fact), so we need to do the two-column response thing: the successes in the first column and the failures in the second. This means that we first need to calculate the number of cells at each dose that were not damaged, by subtracting the number that were damaged from 100. R makes this easier than you’d think, as you see. A mutate is the way to go: create a new column in dr and save back in dr (because I like living on the edge):\n\ndr &lt;- dr %&gt;% mutate(undamaged = 100 - damaged)\ndr\n\n\n\n  \n\n\n\nThe programmer in you is probably complaining “but, 100 is a number and damaged is a vector of 7 numbers. How does R know to subtract each one from 100?” Well, R has what’s known as “recycling rules”: if you try to add or subtract (or elementwise multiply or divide) two vectors of different lengths, it recycles the smaller one by repeating it until it’s as long as the longer one. So rather than 100-damaged giving an error, it does what you want.5\nI took the risk of saving the new data frame over the old one. If it had failed for some reason, I could have started again.\nNow we have to make our response “matrix” with two columns, using cbind:\n\nresponse &lt;- with(dr, cbind(damaged, undamaged))\nresponse\n\n     damaged undamaged\n[1,]      10        90\n[2,]      28        72\n[3,]      53        47\n[4,]      77        23\n[5,]      91         9\n[6,]      98         2\n[7,]      99         1\n\n\nNote that each row adds up to 100, since there were 100 cells at each dose. This is actually trickier than it looks: the two things in cbind are columns (vectors), and cbind glues them together to make an R matrix:\n\nclass(response)\n\n[1] \"matrix\" \"array\" \n\n\nwhich is what glm needs here, even though it looks a lot like a data frame (which wouldn’t work here). This would be a data frame:\n\ndr %&gt;% select(damaged, undamaged) %&gt;% class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nand would therefore be the wrong thing to give glm. So I had to do it with cbind, or use some other trickery, like this:\n\ndr %&gt;%\n  select(damaged, undamaged) %&gt;%\n  as.matrix() -&gt; resp\nclass(resp)\n\n[1] \"matrix\" \"array\" \n\n\nNow we fit our model:\n\ncells.1 &lt;- glm(response ~ dose, family = \"binomial\", data = dr)\nsummary(cells.1)\n\n\nCall:\nglm(formula = response ~ dose, family = \"binomial\", data = dr)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.275364   0.278479  -11.76   &lt;2e-16 ***\ndose         0.113323   0.008315   13.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 384.13349  on 6  degrees of freedom\nResidual deviance:   0.50428  on 5  degrees of freedom\nAIC: 31.725\n\nNumber of Fisher Scoring iterations: 4\n\n\n\\(\\blacksquare\\)\n\nDoes your output indicate that the probability of damage really does increase with dose? (There are two things here: is there really an effect, and which way does it go?)\n\nSolution\nThe slope of dose is significantly nonzero (P-value less than \\(2.2 \\times 10^{-16}\\), which is as small as it can be). The slope itself is positive, which means that as dose goes up, the probability of damage goes up. That’s all I needed, but if you want to press on: the slope is 0.113, so an increase of 1 in dose goes with an increase of 0.113 in the log-odds of damage. Or it multiplies the odds of damage by \\(\\exp(0.113)\\). Since 0.113 is small, this is about \\(1.113\\) (mathematically, \\(e^x\\simeq 1+x\\) if \\(x\\) is small), so that the increase is about 11%. If you like, you can get a rough CI for the slope by going up and down twice its standard error (this is the usual approximately-normal thing). Here that would be\n\n0.113323 + c(-2, 2) * 0.008315\n\n[1] 0.096693 0.129953\n\n\nI thought about doing that in my head, but thought better of it, since I have R just sitting here. The interval is short (we have lots of data) and definitely does not contain zero.\n\\(\\blacksquare\\)\n\nObtain predicted damage probabilities for some representative doses.\n\nSolution\nPick some representative doses first, and then use them as newdata. The ones in the original data are fine (there are only seven of them):\n\np &lt;- cbind(predictions(cells.1, newdata = dr))\np\n\n\n\n  \n\n\n\nI saved mine to use again later, but you don’t have to unless you want to use your predictions again later.\n\\(\\blacksquare\\)\n\nDraw a graph of the predicted probabilities, and to that add the observed proportions of damage at each dose. Hints: you will have to calculate the observed proportions first. See here, near the bottom, to find out how to add data to one of these graphs. The geom_point line is the one you need.\n\nLooking at the predicted probabilities, would you say that the model fits well? Explain briefly.\nSolution\nThis ought to be based on plot_cap, but that doesn’t work here because of the response that’s not part of the dataframe. So we will be making this ourselves. Let’s start with a plot of the predictions, using the prediction we did just now:\n\nggplot(p, aes(x = dose, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3)\n\n\n\n\nThis is not a smooth curve like the ones plot_cap makes, but that’s all right, because we want to compare the predictions with the data.\nLet’s take a look at our original dataframe:\n\ndr\n\n\n\n  \n\n\n\nTo that we need to add a column of proportion damaged, which is damaged divided by damaged plus undamaged. This last ought to be 100, but data can go missing for any number of reasons, so it pays not to assume that they add up to 100 every time:\n\ndr %&gt;% mutate(prop = damaged / (damaged + undamaged)) -&gt; dr2\ndr2\n\n\n\n  \n\n\n\nCheck. I saved this to add to the graph later.\nNow you can add a geom_point with a data = and an aes, making the points red, except that the obvious doesn’t quite work:\n\nggplot(p, aes(x = dose, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) +\n  geom_point(data = dr2, aes(x =  dose, y = prop), colour = \"red\")\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 3rd layer.\nCaused by error:\n! object 'conf.low' not found\n\n\nThe message is not very helpful, but I can tell you where it comes from. When you add something to a plot like this, all the things in the original ggplot are “inherited” by anything else that you add to the plot, so that you either have to overwrite them with something new (as I did with x and y) or you get the previous values, one of which was evidently conf.low. To override this behaviour, which we don’t want because we have nothing called conf.low in our data, add inherit.aes = FALSE to the geom_point:6\n\nggplot(p, aes(x = dose, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) +\n  geom_point(data = dr2, aes(x =  dose, y = prop), colour = \"red\", inherit.aes = FALSE)\n\n\n\n\nI also made the data points red (you don’t need to, but if you want to, put the colour outside the aes, to make it clear that the colour is constant, not determined by any of the variables in your dataframe).\nThe predicted probabilities ought to be close to the observed proportions. They are in fact very close to them, so the model fits very well indeed.\nYour actual words are a judgement call, so precisely what you say doesn’t matter so much, but I think this model fit is actually closer than you could even hope to expect, it’s that good. But, your call. I think your answer ought to contain “close” or “fits well” at the very least.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#what-makes-an-animal-get-infected-1",
    "href": "logistic-regression.html#what-makes-an-animal-get-infected-1",
    "title": "24  Logistic regression",
    "section": "24.14 What makes an animal get infected?",
    "text": "24.14 What makes an animal get infected?\nSome animals got infected with a parasite. We are interested in whether the likelihood of infection depends on any of the age, weight and sex of the animals. The data are at link. The values are separated by tabs.\n\nRead in the data and take a look at the first few lines. Is this one animal per line, or are several animals with the same age, weight and sex (and infection status) combined onto one line? How can you tell?\n\nSolution\nThe usual beginnings, bearing in mind the data layout:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/infection.txt\"\ninfect &lt;- read_tsv(my_url)\n\nRows: 81 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): infected, sex\ndbl (2): age, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ninfect\n\n\n\n  \n\n\n\nSuccess. This appears to be one animal per line, since there is no indication of frequency (of “how many”). If you were working as a consultant with somebody’s data, this would be a good thing to confirm with them before you went any further.\nYou can check a few more lines to convince yourself and the story is much the same. The other hint is that you have actual categories of response, which usually indicates one individual per row, but not always. If it doesn’t, you have some extra work to do to bash it into the right format.\nExtra: let’s see whether we can come up with an example of that. I’ll make a smaller example, and perhaps the place to start is “all possible combinations” of a few things. crossing is the same idea as datagrid, except that it doesn’t need a model, and so is better for building things from scratch:\n\nd &lt;- crossing(age = c(10, 12), gender = c(\"f\", \"m\"), infected = c(\"y\", \"n\"))\nd\n\n\n\n  \n\n\n\nThese might be one individual per row, or they might be more than one, as they would be if we have a column of frequencies:\n\nd &lt;- d %&gt;% mutate(freq = c(12, 19, 17, 11, 18, 26, 16, 8))\nd\n\n\n\n  \n\n\n\nNow, these are multiple observations per row (the presence of frequencies means there’s no doubt about that), but the format is wrong: infected is my response variable, and we want the frequencies of infected being y or n in separate columns — that is, we have to untidy the data a bit to make it suitable for modelling. This is pivot_wider, the opposite of pivot_longer:\n\nd %&gt;% pivot_wider(names_from=infected, values_from=freq)\n\n\n\n  \n\n\n\nNow you can pull out the columns y and n and make them into your response, and predict that from age and gender.\nThe moral of this story is that if you are going to have multiple observations per row, you probably want the combinations of explanatory variables one per row, but you want the categories of the response variable in separate columns.\nBack to where we were the rest of the way.\n\\(\\blacksquare\\)\n\n* Make suitable plots or summaries of infected against each of the other variables. (You’ll have to think about sex, um, you’ll have to think about the sex variable, because it too is categorical.) Anything sensible is OK here. You might like to think back to what we did in Question here for inspiration. (You can also investigate table, which does cross-tabulations.)\n\nSolution\nWhat comes to my mind for the numerical variables age and weight is boxplots:\n\nggplot(infect, aes(x = infected, y = age)) + geom_boxplot()\n\n\n\nggplot(infect, aes(x = infected, y = weight)) + geom_boxplot()\n\n\n\n\nThe variables sex and infected are both categorical. I guess a good plot for those would be some kind of grouped bar plot, which I have to think about. So let’s first try a numerical summary, a cross-tabulation, which is gotten via table:\n\nwith(infect, table(sex, infected))\n\n        infected\nsex      absent present\n  female     17      11\n  male       47       6\n\n\nOr, if you like the tidyverse:\n\ninfect %&gt;% count(sex, infected)\n\n\n\n  \n\n\n\nNow, bar plots. Let’s start with one variable. The basic bar plot has categories of a categorical variable along the \\(x\\)-axis and each bar is a count of how many observations were in that category. What is nice about geom_bar is that it will do the counting for you, so that the plot is just this:\n\nggplot(infect, aes(x = sex)) + geom_bar()\n\n\n\n\nThere are about twice as many males as females.\nYou may think that this looks like a histogram, which it almost does, but there is an important difference: the kind of variable on the \\(x\\)-axis. Here, it is a categorical variable, and you count how many observations fall in each category (at least, ggplot does). On a histogram, the \\(x\\)-axis variable is a continuous numerical one, like height or weight, and you have to chop it up into intervals (and then you count how many observations are in each chopped-up interval).\nTechnically, on a bar plot, the bars have a little gap between them (as here), whereas the histogram bars are right next to each other, because the right side of one histogram bar is the left side of the next.\nAll right, two categorical variables. The idea is that you have each bar divided into sub-bars based on the frequencies of a second variable, which is specified by fill. Here’s the basic idea:\n\nggplot(infect, aes(x = sex, fill = infected)) + geom_bar()\n\n\n\n\nThis is known in the business as a “stacked bar chart”. The issue is how much of each bar is blue, which is unnecessarily hard to judge because the male bar is taller. (Here, it is not so bad, because the amount of blue in the male bar is smaller and the bar is also taller. But we got lucky here.)\nThere are two ways to improve this. One is known as a “grouped bar chart”, which goes like this:\n\nggplot(infect, aes(x = sex, fill = infected)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\nThe absent and present frequencies for females are next to each other, and the same for males, and you can read off how big they are from the \\(y\\)-scale. This is my preferred graph for two (or more than two) categorical variables.\nYou could switch the roles of sex and infected and get a different chart, but one that conveys the same information. Try it. (The reason for doing it the way around I did is that being infected or not is the response and sex is explanatory, so that on my plot you can ask “out of the males, how many were infected?”, which is the way around that makes sense.)\nThe second way is to go back to stacked bars, but make them the same height, so you can compare the fractions of the bars that are each colour. This is position=\"fill\":\n\nggplot(infect, aes(x = sex, fill = infected)) +\n  geom_bar(position = \"fill\")\n\n\n\n\nThis also shows that more of the females were infected than the males, but without getting sidetracked into the issue that there were more males to begin with.\nI wrote this question in early 2017. At that time, I wrote:\n\nI learned about this one approximately two hours ago. I just ordered Hadley Wickham’s new book “R for Data Science” from Amazon, and it arrived today. It’s in there. (A good read, by the way. I’m thinking of using it as a recommended text next year.) As is so often the way with ggplot, the final answer looks very simple, but there is a lot of thinking required to get there, and you end up having even more respect for Hadley Wickham for the clarity of thinking that enabled this to be specified in a simple way.\n\n(end quote)\n\\(\\blacksquare\\)\n\nWhich, if any, of your explanatory variables appear to be related to infected? Explain briefly.\n\nSolution\nLet’s go through our output from (here). In terms of age, when infection is present, animals are (slightly) older. So there might be a small age effect. Next, when infection is present, weight is typically a lot less. So there ought to be a big weight effect. Finally, from the table, females are somewhere around 50-50 infected or not, but very few males are infected. So there ought to be a big sex effect as well. This also appears in the grouped bar plot, where the red (“absent”) bar for males is much taller than the blue (“present”) bar, but for females the two bars are almost the same height. So the story is that we would expect a significant effect of sex and weight, and maybe of age as well.\n\\(\\blacksquare\\)\n\nFit a logistic regression predicting infected from the other three variables. Display the summary.\n\nSolution\nThus:\n\ninfect.1 &lt;- glm(infected ~ age + weight + sex, family = \"binomial\", data = infect)\n\nError in eval(family$initialize): y values must be 0 &lt;= y &lt;= 1\n\n\nOh, I forgot to turn infected into a factor. This is the shortcut way to do that:\n\ninfect.1 &lt;- glm(factor(infected) ~ age + weight + sex, family = \"binomial\", data = infect)\nsummary(infect.1)\n\n\nCall:\nglm(formula = factor(infected) ~ age + weight + sex, family = \"binomial\", \n    data = infect)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.609369   0.803288   0.759 0.448096    \nage          0.012653   0.006772   1.868 0.061701 .  \nweight      -0.227912   0.068599  -3.322 0.000893 ***\nsexmale     -1.543444   0.685681  -2.251 0.024388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 83.234  on 80  degrees of freedom\nResidual deviance: 59.859  on 77  degrees of freedom\nAIC: 67.859\n\nNumber of Fisher Scoring iterations: 5\n\n\nOr you could create a new or redefined column in the data frame containing the factor version of infected, for example in this way:\n\ninfect %&gt;%\n  mutate(infected = factor(infected)) %&gt;%\n  glm(infected ~ age + weight + sex, family = \"binomial\", data = .) -&gt; infect.1a\nsummary(infect.1a)\n\n\nCall:\nglm(formula = infected ~ age + weight + sex, family = \"binomial\", \n    data = .)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.609369   0.803288   0.759 0.448096    \nage          0.012653   0.006772   1.868 0.061701 .  \nweight      -0.227912   0.068599  -3.322 0.000893 ***\nsexmale     -1.543444   0.685681  -2.251 0.024388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 83.234  on 80  degrees of freedom\nResidual deviance: 59.859  on 77  degrees of freedom\nAIC: 67.859\n\nNumber of Fisher Scoring iterations: 5\n\n\nEither way is good, and gives the same answer. The second way uses the data=. trick to ensure that the input data frame to glm is the output from the previous step, the one with the factor version of infected in it. The data=. is needed because glm requires a model formula first rather than a data frame (if the data were first, you could just omit it).\n\\(\\blacksquare\\)\n\n* Which variables, if any, would you consider removing from the model? Explain briefly.\n\nSolution\nThis is the same idea as in multiple regression: look at the end of the line for each variable to get its individual P-value, and if that’s not small, you can take that variable out. age has a P-value of 0.062, which is (just) larger than 0.05, so we can consider removing this variable. The other two P-values, 0.00089 and 0.024, are definitely less than 0.05, so those variables should stay.\nAlternatively, you can say that the P-value for age is small enough to be interesting, and therefore that age should stay. That’s fine, but then you need to be consistent in the next part.\nYou probably noted that sex is categorical. However, it has only the obvious two levels, and such a categorical variable can be assessed for significance this way. If you were worried about this, the right way to go is drop1:\n\ndrop1(infect.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe P-values are similar, but not identical.7\nI have to stop and think about this. There is a lot of theory that says there are several ways to do stuff in regression, but they are all identical. The theory doesn’t quite apply the same in generalized linear models (of which logistic regression is one): if you had an infinite sample size, the ways would all be identical, but in practice you’ll have a very finite amount of data, so they won’t agree.\nI’m thinking about my aims here: I want to decide whether each \\(x\\)-variable should stay in the model, and for that I want a test that expresses whether the model fits significantly worse if I take it out. The result I get ought to be the same as physically removing it and comparing the models with anova, eg. for age:\n\ninfect.1b &lt;- update(infect.1, . ~ . - age)\nanova(infect.1b, infect.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThis is the same thing as drop1 gives.\nSo, I think: use drop1 to assess whether anything should come out of a model like this, and use summary to obtain the slopes to interpret (in this kind of model, whether they’re positive or negative, and thus what kind of effect each explanatory variable has on the probability of whatever-it-is.)\n\\(\\blacksquare\\)\n\nAre the conclusions you drew in (here) and (here) consistent, or not? Explain briefly.\n\nSolution\nI think they are extremely consistent. When we looked at the plots, we said that weight and sex had large effects, and they came out definitely significant. There was a small difference in age between the infected and non-infected groups, and age came out borderline significant (with a P-value definitely larger than for the other variables, so that the evidence of its usefulness was weaker).\n\\(\\blacksquare\\)\n\n* The first and third quartiles of age are 26 and 130; the first and third quartiles of weight are 9 and 16. Obtain predicted probabilities for all combinations of these and sex. (You’ll need to start by making a new data frame, using datagrid to get all the combinations.)\n\nSolution\nHere’s how datagrid goes. Note my use of plural names to denote the things I want all combinations of:\n\nages &lt;- c(26, 130)\nweights &lt;- c(9, 16)\nsexes &lt;- c(\"female\", \"male\")\nnew &lt;- datagrid(model = infect.1, age = ages, weight = weights, sex = sexes)\nnew\n\n\n\n  \n\n\n\nThis is about on the upper end of what you would want to do just using the one line with datagrid in it and putting the actual values in instead of ages, weights etc. To my mind, once it gets longer than about this long, doing on one line starts to get unwieldy. But your taste might be different than mine.\nAside:\nI could have asked you to include some more values of age and weight, for example the median as well, to get a clearer picture. But that would have made infect.new bigger, so I stopped here. (If we had been happy with five representative values of age and weight, we could have done predictions (below) with variables and not had to make new at all.)\ndatagrid makes a data frame from input vectors, so it doesn’t matter if those are different lengths. In fact, it’s also possible to make this data frame from things like quartiles stored in a data frame. To do that, you wrap the whole datagrid in a with:\n\nd &lt;- tibble(age = ages, weight = weights, sex = sexes)\nd\n\n\n\n  \n\n\n\n\nnew &lt;- with(d, datagrid(model = infect.1, age=age, weight=weight, sex=sex))\nnew\n\n\n\n  \n\n\n\nThis one is a little confusing because in eg. age = age, the first age refers to the column that will be in new, and the second one refers to the values that are going in there, namely the column called age in the dataframe d.8\nEnd of aside.\nNext, the predictions:\n\ncbind(predictions(infect.1, new))\n\n\n\n  \n\n\n\nExtra: I didn’t ask you to comment on these, since the question is long enough already. But that’s not going to stop me!\nThese, in predicted, are predicted probabilities of infection.9\nThe way I remember the one-column-response thing is that the first level is the baseline (as it is in a regression with a categorical explanatory variable), and the second level is the one whose probability is modelled (in the same way that the second, third etc. levels of a categorical explanatory variable are the ones that appear in the summary table).\nLet’s start with sex. The probabilities of a female being infected are all much higher than of a corresponding male (with the same age and weight) being infected. Compare, for example, lines 1 and 2. Or 3 and 4. Etc. So sex has a big effect.\nWhat about weight? As weight goes from 9 to 16, with everything else the same, the predicted probability of infection goes sharply down. This is what we saw before: precisely, the boxplot showed us that infected animals were likely to be less heavy.\nLast, age. As age goes up, the probabilities go (somewhat) up as well. Compare, for example, lines 1 and 5 or lines 4 and 8. I think this is a less dramatic change than for the other variables, but that’s a judgement call.\nI got this example from (horrible URL warning) here: link It starts on page 275 in my edition. He goes at the analysis a different way, but he finishes with another issue that I want to show you.\nLet’s work out the residuals and plot them against our quantitative explanatory variables. I think the best way to do this is augment from broom, to create a data frame containing the residuals alongside the original data:\n\nlibrary(broom)\ninfect.1a &lt;- infect.1 %&gt;% augment(infect)\ninfect.1a %&gt;% as_tibble()\n\n\n\n  \n\n\nggplot(infect.1a, aes(x = weight, y = .resid)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\ninfect.1a is, I think, a genuine data.frame rather than a tibble.\nI don’t quite know what to make of that plot. It doesn’t look quite random, and yet there are just some groups of points rather than any real kind of trend.\nThe corresponding plot with age goes the same way:\n\nggplot(infect.1a, aes(x = age, y = .resid)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nCrawley found the slightest suggestion of an up-and-down curve in there. I’m not sure I agree, but that’s what he saw. As with a regular regression, the residuals against anything should look random, with no trends. (Though the residuals from a logistic regression can be kind of odd, because the response variable can only be 1 or 0.) Crawley tries adding squared terms to the logistic regression, which goes like this. The glm statement is long, as they usually are, so it’s much easier to use update:\n\ninfect.2 &lt;- update(infect.1, . ~ . + I(age^2) + I(weight^2))\n\nAs we saw before, when thinking about what to keep, we want to look at drop1:\n\ndrop1(infect.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe squared terms are both significant. The linear terms, age and weight, have to stay, regardless of their significance.10 What do the squared terms do to the predictions? Before, there was a clear one-directional trend in the relationships with age and weight. Has that changed? Let’s see. We’ll need a few more ages and weights to investigate with. I think the set of five representative values that comes out of predictions with variables would be ideal (and saves us having to make another new).\nLet’s start by assessing the effect of age:\n\nsummary(infect)\n\n   infected              age             weight          sex           \n Length:81          Min.   :  1.00   Min.   : 1.00   Length:81         \n Class :character   1st Qu.: 26.00   1st Qu.: 9.00   Class :character  \n Mode  :character   Median : 87.00   Median :13.00   Mode  :character  \n                    Mean   : 83.65   Mean   :11.49                     \n                    3rd Qu.:130.00   3rd Qu.:16.00                     \n                    Max.   :206.00   Max.   :18.00                     \n\nnew &lt;- datagrid(model = infect.2, age = c(1, 26, 84, 130, 206))\nnew\n\n\n\n  \n\n\ncbind(predictions(infect.2, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nThe actual values of age we chose are as shown. The other columns are constant; the values are the mean weight and the more common sex. We really can see the effect of age with all else constant.\nHere, the predicted infection probabilities go up with age and then come down again, as a squared term in age will allow them to do, compared to what we had before:\n\ncbind(predictions(infect.1, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nwhere the probabilities keep on going up.\nAll right, what about the effect of weight? Here’s the first model:\n\nnew &lt;- datagrid(model = infect.2, weight = c(1, 9, 13, 16, 18))\ncbind(predictions(infect.1, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nand here’s the second one with squared terms:\n\ncbind(predictions(infect.2, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nThis one is not so dissimilar: in the linear model, the predicted probabilities of infection start high and go down, but in the model with squared terms they go slightly up before going down.\nWe couldn’t have a squared term in sex, since there are only two sexes (in this data set), so the predictions might be pretty similar for the two models:\n\nnew &lt;- datagrid(model = infect.2, sex = c(\"female\", \"male\"))\ncbind(predictions(infect.1, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nand\n\ncbind(predictions(infect.2, newdata = new)) %&gt;% \n  select(estimate, age, weight, sex)\n\n\n\n  \n\n\n\nThey are actually quite different. For the model with squared terms in age and weight, the predicted probabilities of infection are a lot higher for both males and females, at least at these (mean) ages and weights.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#the-brain-of-a-cat-1",
    "href": "logistic-regression.html#the-brain-of-a-cat-1",
    "title": "24  Logistic regression",
    "section": "24.15 The brain of a cat",
    "text": "24.15 The brain of a cat\nA large number (315) of psychology students were asked to imagine that they were serving on a university ethics committee hearing a complaint against animal research being done by a member of the faculty. The students were told that the surgery consisted of implanting a device called a cannula in each cat’s brain, through which chemicals were introduced into the brain and the cats were then given psychological tests. At the end of the study, the cats’ brains were subjected to histological analysis. The complaint asked that the researcher’s authorization to carry out the study should be withdrawn, and the cats should be handed over to the animal rights group that filed the complaint. It was suggested that the research could just as well be done with computer simulations.\nAll of the psychology students in the survey were told all of this. In addition, they read a statement by the researcher that no animal felt much pain at any time, and that computer simulation was not an adequate substitute for animal research. Each student was also given one of the following scenarios that explained the benefit of the research:\n\n“cosmetic”: testing the toxicity of chemicals to be used in new lines of hair care products.\n“theory”: evaluating two competing theories about the function of a particular nucleus in the brain.\n“meat”: testing a synthetic growth hormone said to potentially increase meat production.\n“veterinary”: attempting to find a cure for a brain disease that is killing domesticated cats and endangered species of wild cats.\n“medical”: evaluating a potential cure for a debilitating disease that afflicts many young adult humans.\n\nFinally, each student completed two questionnaires: one that would assess their “relativism”: whether or not they believe in universal moral principles (low score) or whether they believed that the appropriate action depends on the person and situation (high score). The second questionnaire assessed “idealism”: a high score reflects a belief that ethical behaviour will always lead to good consequences (and thus that if a behaviour leads to any bad consequences at all, it is unethical).11\nAfter being exposed to all of that, each student stated their decision about whether the research should continue or stop.\nI should perhaps stress at this point that no actual cats were harmed in the collection of these data (which can be found as a .csv file at link). The variables in the data set are these:\n\ndecision: whether the research should continue or stop (response)\nidealism: score on idealism questionnaire\nrelativism: score on relativism questionnaire\ngender of student\nscenario of research benefits that the student read.\n\nA more detailed discussion12 of this study is at link.\n\nRead in the data and check by looking at the structure of your data frame that you have something sensible. Do not call your data frame decision, since that’s the name of one of the variables in it.\n\nSolution\nSo, like this, using the name decide in my case:\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/decision.csv\"\ndecide &lt;- read_csv(my_url)\n\nRows: 315 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): decision, gender, scenario\ndbl (2): idealism, relativism\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndecide\n\n\n\n  \n\n\n\nThe variables are all the right things and of the right types: the decision, gender and the scenario are all text (representing categorical variables), and idealism and relativism, which were scores on a test, are quantitative (numerical). There are, as promised, 315 observations.\n\\(\\blacksquare\\)\n\nFit a logistic regression predicting decision from gender. Is there an effect of gender?\n\nSolution\nTurn the response into a factor somehow, either by creating a new variable in the data frame or like this:\n\ndecide.1 &lt;- glm(factor(decision) ~ gender, data = decide, family = \"binomial\")\nsummary(decide.1)\n\n\nCall:\nglm(formula = factor(decision) ~ gender, family = \"binomial\", \n    data = decide)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.8473     0.1543   5.491 3.99e-08 ***\ngenderMale   -1.2167     0.2445  -4.976 6.50e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 425.57  on 314  degrees of freedom\nResidual deviance: 399.91  on 313  degrees of freedom\nAIC: 403.91\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe P-value for gender is \\(6.5 \\times 10^{-7}\\), which is very small, so there is definitely an effect of gender. It’s not immediately clear what kind of effect it is: that’s the reason for the next part, and we’ll revisit this slope coefficient in a moment. Categorical explanatory variables are perfectly all right as text. Should I have used drop1 to assess the significance? Maybe:\n\ndrop1(decide.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe thing is, this gives us a P-value but not a slope, which we might have wanted to try to interpret. Also, the P-value in summary is so small that it is likely to be still significant in drop1 as well.\n\\(\\blacksquare\\)\n\nTo investigate the effect (or non-effect) of gender, create a contingency table by feeding decision and gender into table. What does this tell you?\n\nSolution\n\nwith(decide, table(decision, gender))\n\n          gender\ndecision   Female Male\n  continue     60   68\n  stop        140   47\n\n\nFemales are more likely to say that the study should stop (a clear majority), while males are more evenly split, with a small majority in favour of the study continuing.\nIf you want the column percents as well, you can use prop.table. Two steps: save the table from above into a variable, then feed that into prop.table, calling for column percentages rather than row percentages:\n\ntab &lt;- with(decide, table(decision, gender))\nprop.table(tab, 2)\n\n          gender\ndecision      Female      Male\n  continue 0.3000000 0.5913043\n  stop     0.7000000 0.4086957\n\n\nWhy column percentages? Well, thinking back to STAB22 or some such place, when one of your variables is acting like a response or outcome (decision here), make the percentages out of the other one. Given that a student is a female, how likely are they to call for the research to stop? The other way around makes less sense: given that a person wanted the research to stop, how likely are they to be female?\nAbout 70% of females and 40% of males want the research to stop. That’s a giant-sized difference. No wonder it was significant.\nThe other way of making the table is to use xtabs, with the same result:\n\nxtabs(~ decision + gender, data = decide)\n\n          gender\ndecision   Female Male\n  continue     60   68\n  stop        140   47\n\n\nIn this one, the frequency variable goes on the left side of the squiggle. We don’t have one here (each row of the data frame represents one student), so we leave the left side blank. I tried putting a . there, but that doesn’t work since there is no “whatever was there before” as there is, for example, in update.\n\\(\\blacksquare\\)\n\n* Is your slope for gender in the previous logistic regression positive or negative? Is it applying to males or to females? Looking at the conclusions from your contingency table, what probability does that mean your logistic regression is actually modelling?\n\nSolution\nMy slope is \\(-1.2167\\), negative, and it is attached to males (note that the slope is called gendermale: because “female” is before “male” alphabetically, females are used as the baseline and this slope says how males compare to them). This negative male coefficient means that the probability of whatever is being modelled is less for males than it is for females. Looking at the contingency table for the last part, the probability of “stop” should be less for males, so the logistic regression is actually modelling the probability of “stop”. Another way to reason that this must be the right answer is that the two values of decision are continue and stop; continue is first alphabetically, so it’s the baseline, and the other one, stop, is the one whose probability is being modelled. That’s why I made you do that contingency table. Another way to think about this is to do a prediction, which would go like this:\n\nnew &lt;- datagrid(model = decide.1, gender = levels(factor(decide$gender)))\nnew\n\n\n\n  \n\n\ncbind(predictions(decide.1, newdata = new))\n\n\n\n  \n\n\n\nTechnical note: we could simply supply the two genders in the definition of new, remembering to type the Capital Letters. The other way is to find out which genders there are in the data, and one way is to temporarily turn gender into a factor and then find out which different values it has, which are called “levels”.\nThe probability of whatever-it-is is exactly 70% for females and about 40% for males. A quick look at the contingency table shows that exactly 70% (\\(140/200\\)) of the females think the research should stop, and a bit less than 50% of the males think the same thing. So the model is predicting the probability of “stop”.\nThere’s a logic to this: it’s not just this way “because it is”. It’s the same idea of the first category, now of the response factor, being a “baseline”, and what actually gets modelled is the second category, relative to the baseline.\n\\(\\blacksquare\\)\n\nAdd the two variables idealism and relativism to your logistic regression. Do either or both of them add significantly to your model? Explain briefly.\n\nSolution\nThe obvious way of doing this is to type out the entire model, with the two new variables on the end. You have to remember to turn decision into a factor again:\n\ndecide.2 &lt;- glm(factor(decision) ~ gender + idealism + relativism,\n  data = decide, family = \"binomial\"\n)\nsummary(decide.2)\n\n\nCall:\nglm(formula = factor(decision) ~ gender + idealism + relativism, \n    family = \"binomial\", data = decide)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.4876     0.9787  -1.520  0.12849    \ngenderMale   -1.1710     0.2679  -4.372 1.23e-05 ***\nidealism      0.6893     0.1115   6.180 6.41e-10 ***\nrelativism   -0.3432     0.1245  -2.757  0.00584 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 425.57  on 314  degrees of freedom\nResidual deviance: 346.50  on 311  degrees of freedom\nAIC: 354.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nThis is not so bad, copying and pasting. But the way I like better, when you’re making a smallish change to a longish model, is to use update:\n\ndecide.2 &lt;- update(decide.1, . ~ . + idealism + relativism)\nsummary(decide.2)\n\n\nCall:\nglm(formula = factor(decision) ~ gender + idealism + relativism, \n    family = \"binomial\", data = decide)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.4876     0.9787  -1.520  0.12849    \ngenderMale   -1.1710     0.2679  -4.372 1.23e-05 ***\nidealism      0.6893     0.1115   6.180 6.41e-10 ***\nrelativism   -0.3432     0.1245  -2.757  0.00584 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 425.57  on 314  degrees of freedom\nResidual deviance: 346.50  on 311  degrees of freedom\nAIC: 354.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nEither way is good. The conclusion you need to draw is that they both have something to add, because their P-values are both less than 0.05.\nOr (and perhaps better) you can look at drop1 of either of these:\n\ndrop1(decide.2, test = \"Chisq\")\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nAdd the variable scenario to your model. That is, fit a new model with that variable plus all the others.\n\nSolution\nTo my mind, update wins hands down here:\n\ndecide.3 &lt;- update(decide.2, . ~ . + scenario)\n\nYou can display the summary here if you like, but we’re not going to look at it yet.\n\\(\\blacksquare\\)\n\nUse anova to compare the models with and without scenario. You’ll have to add a test=\"Chisq\" to your anova, to make sure that the test gets done. Does scenario make a difference or not, at \\(\\alpha=0.10\\)? Explain briefly. (The reason we have to do it this way is that scenario is a factor with five levels, so it has four slope coefficients. To test them all at once, which is what we need to make an overall test for scenario, this is the way it has to be done.)\n\nSolution\nThese are the models that you fit in the last two parts:\n\nanova(decide.2, decide.3, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe P-value is not less than 0.05, but it is less than 0.10, which is what I implied to assess it with, so the scenario does make some kind of difference.\nExtra: another way to do this, which I like better (but the anova way was what I asked in the original question), is to look at decide.3 and ask “what can I get rid of”, in such a way that categorical variables stay or go as a whole. This is done using drop1. It’s a little different from the corresponding thing in regression because the right way to do the test is not an F test, but now a chi-squared test (this is true for all generalized linear models of which logistic regression is one):\n\ndrop1(decide.3, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe test for scenario has four degrees of freedom (since there are five scenarios), and is in fact exactly the same test as in anova, significant at \\(\\alpha=0.10\\).\n\\(\\blacksquare\\)\n\nLook at the summary of your model that contained scenario. Bearing in mind that the slope coefficient for scenariocosmetic is zero (since this is the first scenario alphabetically), which scenarios have the most positive and most negative slope coefficients? What does that tell you about those scenarios’ effects?\n\nSolution\nAll right. This is the model I called decide.3:\n\nsummary(decide.3)\n\n\nCall:\nglm(formula = factor(decision) ~ gender + idealism + relativism + \n    scenario, family = \"binomial\", data = decide)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -1.5694     1.0426  -1.505   0.1322    \ngenderMale          -1.2551     0.2766  -4.537 5.70e-06 ***\nidealism             0.7012     0.1139   6.156 7.48e-10 ***\nrelativism          -0.3264     0.1267  -2.576   0.0100 *  \nscenariomeat         0.1565     0.4283   0.365   0.7149    \nscenariomedical     -0.7095     0.4202  -1.688   0.0914 .  \nscenariotheory       0.4501     0.4271   1.054   0.2919    \nscenarioveterinary  -0.1672     0.4159  -0.402   0.6878    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 425.57  on 314  degrees of freedom\nResidual deviance: 338.06  on 307  degrees of freedom\nAIC: 354.06\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe most positive coefficient is for theory and the most negative one is for medical. (The zero coefficient is in the middle.) Since we are modelling the probability of saying that the research should stop (part (here)), this means that:\n\nthe “theory” scenario (evaluating theories about brain function) is most likely to lead to someone saying that the research should stop (other things being equal)\nthe “medical” scenario (finding a cure for a human disease) is most likely to lead to someone saying that the research should continue (or least likely to say that it should stop), again, other things being equal.\n\nThese make some kind of sense because being exposed to a scenario where there are tangible benefits later ought to be most favourable to the research continuing, and people are not going to be impressed by something that is “only theoretical” without any clear benefits.\nWe can also tackle this by doing some predictions. We want all the categories for scenario, and we might as well use average values for everything else:\n\nnew &lt;- datagrid(model = decide.3, scenario = levels(factor(decide$scenario)))\nnew\n\n\n\n  \n\n\ncbind(predictions(decide.3, newdata = new)) %&gt;% select(estimate, gender, idealism, relativism, scenario)\n\n\n\n  \n\n\n\nThe scenarios are over on the right, and the values of the other variables are the same all the way down (means for the quantitative ones, the most common category for the categorical ones). Having checked that this is indeed the case, we really only need the predictions and the scenarios.\nThis echoes what we found before: the probability of saying that the research should stop is highest for “theory” and the lowest for “medical”.\nI assumed in my model that the effect of the scenarios was the same for males and females. If I wanted to test that, I’d have to add an interaction and test that. This works most nicely using update and then anova, to fit the model with interaction and compare it with the model without:\n\ndecide.4 &lt;- update(decide.3, . ~ . + gender * scenario)\nanova(decide.3, decide.4, test = \"Chisq\")\n\n\n\n  \n\n\n\nNo evidence at all that the scenarios have different effects for the different genders. The appropriate predictions should show that too:\n\nnew &lt;- datagrid(model = decide.4, \n                gender = levels(factor(decide$gender)),\n                scenario = levels(factor(decide$scenario)))\ncbind(predictions(decide.4, newdata = new)) %&gt;% select(estimate, gender, idealism, relativism, scenario)\n\n\n\n  \n\n\n\nThe predicted probabilities that the experiment should be stopped are a lot lower for males than females across the board (this is the strongly significant gender effect). But, given that, the probability for medical is the lowest for both males and females, and the probability for theory is the highest for females and almost the highest for males. (The pattern for males and females is similar enough to suggest that there should be no interaction.)\nSo fitting an interaction was a waste of time, but it was worth checking whether it was.\n\\(\\blacksquare\\)\n\nDescribe the effects that having (i) a higher idealism score and (ii) a higher relativity score have on a person’s probability of saying that the research should stop. Do each of these increase or decrease that probability? Explain briefly.\n\nSolution\nLook back at the summary for the model that I called decide.3. (Or decide.2: the slope coefficients are very similar.) The one for idealism is positive, so that a higher idealism score goes with a greater likelihood of saying that the research should stop. The slope coefficient for relativity is negative, so it’s the other way around: a higher relativity score goes with a lower chance of saying that the research should stop.\nThat’s all I needed, but as an extra, we can look back at the description of these scales in the question.\nThe relativism one was that a person believed that the most moral action depends on the situation (as opposed to a person having something like religious faith that asserts universal moral principles that are always true. That would be a low score on the relativism scale). Somebody with a low score on this scale might believe something like “it is always wrong to experiment on animals”, whereas somebody with a high relativism score might say that it was sometimes justified. Thus, other things being equal, a low relativism score would go with “stop” and a high relativism score would (or might) go with “continue”. This would mean a negative slope coefficient for relativism, which is what we observed. (Are you still with me? There was some careful thinking there.)\nWhat about idealism? This is a belief that ethical behaviour will always lead to good consequences, and thus, if the consequences are bad, the behaviour must not have been ethical. A person who scores high on idealism is likely to look at the consequences (experimentation on animals), see that as a bad thing, and thus conclude that the research should be stopped. The idealism slope coefficient, by that argument, should be positive, and is.\nThis can also be done by a prediction. In the light of what we have done before, the suggestion is this. Idealism and relativism are quantitative, so let’s grab their quartiles, giving us \\(4 = 2 \\times 2\\) combinations:\n\nnew &lt;- datagrid(model = decide.3, \n                idealism = quantile(decide$idealism, c(0.25, 0.75)),\n                relativism = quantile(decide$relativism, c(0.25, 0.75)))\nnew\n\n\n\n  \n\n\ncbind(predictions(decide.3, newdata = new)) %&gt;% select(estimate, gender, idealism, relativism, scenario)\n\n\n\n  \n\n\n\nFor both of the idealism scores, the higher relativism score went with a lower probability of “stop” (the “negative” effect), and for both of the relativism scores, the higher idealism score went with a higher probability of “stop” (the positive effect).\nYet another way to assess this would be to make a graph:\n\nplot_cap(decide.3, condition = c(\"relativism\", \"idealism\"))\n\n\n\n\nThe story from here is the same: as relativism increases (move from left to right), the probability of stop decreases, but as idealism increases (from the red line up to the purple one), the probability of stop increases.\nThat’s quite enough discussion of the question, except that the data didn’t come to me in the form that you see them, so I figured I would like to share the story of the data processing as well. I think this is important because in your future work you are likely to spend a lot of your time getting data from how you receive it to something suitable for analysis.\nThese data came from a psychology study (with, probably, the students in a class serving as experimental subjects). Social scientists like to use SPSS software, so the data came to me as an SPSS .sav file.13 The least-fuss way of handling this that I could think of was to use import from the rio package, which I think I mentioned before:\n\nlibrary(rio)\nx &lt;- import(\"/home/ken/Downloads/Logistic.sav\")\nstr(x)\n\n'data.frame':   315 obs. of  11 variables:\n $ decision   : num  0 1 1 0 1 1 0 0 0 0 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n  ..- attr(*, \"labels\")= Named num [1:2] 0 1\n  .. ..- attr(*, \"names\")= chr [1:2] \"stop\" \"continue\"\n $ idealism   : num  8.2 6.8 8.2 7.4 1.7 5.6 7.2 7.8 7.8 8 ...\n  ..- attr(*, \"format.spss\")= chr \"F12.4\"\n $ relatvsm   : num  5.1 5.3 6 6.2 3.1 7.7 6.7 4 4.7 7.6 ...\n  ..- attr(*, \"format.spss\")= chr \"F12.4\"\n $ gender     : num  0 1 0 0 0 1 0 1 0 0 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n  ..- attr(*, \"labels\")= Named num [1:2] 0 1\n  .. ..- attr(*, \"names\")= chr [1:2] \"Female\" \"Male\"\n $ cosmetic   : num  1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n $ theory     : num  0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n $ meat       : num  0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n $ veterin    : num  0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.spss\")= chr \"F1.0\"\n $ idealism_LN: num  2.104 1.917 2.104 2.001 0.531 ...\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n  ..- attr(*, \"display_width\")= int 13\n $ relatvsm_LN: num  1.63 1.67 1.79 1.82 1.13 ...\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n  ..- attr(*, \"display_width\")= int 13\n $ scenario   : num  1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n  ..- attr(*, \"display_width\")= int 10\n\n\nThe last line str displays the “structure” of the data frame that was obtained. Normally a data frame read into R has a much simpler structure than this, but this is R trying to interpret how SPSS does things. Here, each column (listed on the lines beginning with a dollar sign) has some values, listed after num; they are all numeric, even the categorical ones. What happened to the categorical variables is that they got turned into numbers, and they have a names “attribute” further down that says what those numbers actually represent. Thus, on the gender line, the subjects are a female (0), then a male (1), then three females, then a male, and so on. Variables like gender are thus so far neither really factors nor text variables, and so we’ll have to do a bit of processing before we can use them: we want to replace the numerical values by the appropriate “level”.\nTo turn a numeric variable into text depending on the value, we can use ifelse, but this gets unwieldy if there are more than two values to translate. For that kind of job, I think case_when is a lot easier to read. It also lets us have a catch-all for catching errors — “impossible” values occur distressingly often in real data:\n\nxx &lt;- x %&gt;%\n  mutate(\n    decision = case_when(\n      decision == 0 ~ \"stop\",\n      decision == 1 ~ \"continue\",\n      TRUE ~ \"error\"\n    ),\n    gender = case_when(\n      gender == 0 ~ \"Female\",\n      gender == 1 ~ \"Male\",\n      TRUE ~ \"error\"\n    ),\n    scenario = case_when(\n      scenario == 1 ~ \"cosmetic\",\n      scenario == 2 ~ \"theory\",\n      scenario == 3 ~ \"meat\",\n      scenario == 4 ~ \"veterinary\",\n      scenario == 5 ~ \"medical\",\n      TRUE ~ \"error\"\n    )\n  )\nxx %&gt;% as_tibble() %&gt;% select(-(cosmetic:veterin))\n\n\n\n  \n\n\n\nxx is a “real” data.frame (that’s what rio reads in), and has some extra columns that we don’t want to see right now.\nI have three new variables being created in one mutate. Each is being created using a case_when. The thing on the left of each squiggle is a logical condition being tested; the first of these logical conditions to come out TRUE provides the value for the new variable on the right of the squiggle. Thus, if the (old) scenario is 2, the new scenario will be theory. The TRUE lines in each case provide something that is guaranteed to be true, even if all the other lines are false (eg. if scenario is actually recorded as 7, which would be an error).\nI overwrote the old variable values with the new ones, which is a bit risky, but then I’d have more things to get rid of later.\nMy next step is to check that I don’t actually have any errors:\n\nxx %&gt;% count(scenario, gender, decision)\n\n\n\n  \n\n\n\nDon’t see any errors there.\nSo now let’s write what we have to a file. I think a .csv would be smart:\n\nxx %&gt;%\n  select(decision, idealism, relatvsm, gender, scenario) %&gt;%\n  write_csv(\"decision.csv\")\n\nThere is one more tiny detail: in SPSS, variable names can have a maximum of eight letters. “Relativism” has 10. So the original data file had the name “relativism” minus the two “i”s. I changed that so you would be dealing with a proper English word. (That change is not shown here.)\nThere is actually a town called Catbrain. It’s in England, near Bristol, and seems to be home to a street of car dealerships. One of the questions in the chapter on making maps asks you to find out where it is exactly.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#how-not-to-get-heart-disease-1",
    "href": "logistic-regression.html#how-not-to-get-heart-disease-1",
    "title": "24  Logistic regression",
    "section": "24.16 How not to get heart disease",
    "text": "24.16 How not to get heart disease\nWhat is associated with heart disease? In a study, a large number of variables were measured, as follows:\n\nage (years)\nsex male or female\npain.type Chest pain type (4 values: typical angina, atypical angina, non-anginal pain, asymptomatic)\nresting.bp Resting blood pressure, on admission to hospital\nserum.chol Serum cholesterol\nhigh.blood.sugar: greater than 120, yes or no\nelectro resting electrocardiographic results (normal, having ST-T, hypertrophy)\nmax.hr Maximum heart rate\nangina Exercise induced angina (yes or no)\noldpeak ST depression induced by exercise relative to rest. See link.\nslope Slope of peak exercise ST segment. Sloping up, flat or sloping down\ncolored number of major vessels (0–3) coloured by fluoroscopy\nthal normal, fixed defect, reversible defect\nheart.disease yes, no (response)\n\nI don’t know what most of those are, but we will not let that stand in our way. Our aim is to find out what variables are associated with heart disease, and what values of those variables give high probabilities of heart disease being present. The data are in link.\n\nRead in the data. Display the first few lines and convince yourself that those values are reasonable.\n\nSolution\nA .csv file, so:\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/heartf.csv\"\nheart &lt;- read_csv(my_url)\n\nNew names:\nRows: 270 Columns: 15\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(8): sex, pain.type, high.blood.sugar, electro, angina, slope, thal, hea... dbl\n(7): ...1, age, resting.bp, serum.chol, max.hr, oldpeak, colored\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nheart\n\n\n\n  \n\n\n\nYou should check that the variables that should be numbers actually are, that the variables that should be categorical have (as far as is shown) the right values as per my description above, and you should make some comment in that direction.\nMy variables appear to be correct, apart possibly for that variable X1 which is actually just the row number.\n\\(\\blacksquare\\)\n\nIn a logistic regression, what probability will be predicted here? Explain briefly but convincingly. (Is each line of the data file one observation or a summary of several?)\n\nSolution\nEach line of the data file is a single observation, not frequencies of yes and no (like the premature babies question, later, is). The response variable is a factor, so the first level is the baseline and the second level is the one predicted. R puts factor levels alphabetically, so no is first and yes is second. That is, a logistic regression will predict the probability that a person does have heart disease. I want to see that logic (which is why I said “convincingly”): one observation per line, and therefore that the second level of the factor is predicted, which is yes.\n\\(\\blacksquare\\)\n\n* Fit a logistic regression predicting heart disease from everything else (if you have a column called X or X1, ignore that), and display the results.\n\nSolution\nA lot of typing, since there are so many variables. Don’t forget that the response variable must be a factor:\n\nheart.1 &lt;- glm(factor(heart.disease) ~ age + sex + pain.type + resting.bp + serum.chol +\n  high.blood.sugar + electro + max.hr + angina + oldpeak + slope + colored + thal,\nfamily = \"binomial\", data = heart\n)\n\nYou can split this over several lines (and probably should), but make sure to end each line in such a way that there is unambiguously more to come, for example with a plus or a comma (though probably the fact that you have an unclosed bracket will be enough).\nThe output is rather lengthy:\n\nsummary(heart.1)\n\n\nCall:\nglm(formula = factor(heart.disease) ~ age + sex + pain.type + \n    resting.bp + serum.chol + high.blood.sugar + electro + max.hr + \n    angina + oldpeak + slope + colored + thal, family = \"binomial\", \n    data = heart)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -3.973837   3.133311  -1.268 0.204707    \nage                 -0.016007   0.026394  -0.606 0.544208    \nsexmale              1.763012   0.580761   3.036 0.002400 ** \npain.typeatypical   -0.997298   0.626233  -1.593 0.111264    \npain.typenonanginal -1.833394   0.520808  -3.520 0.000431 ***\npain.typetypical    -2.386128   0.756538  -3.154 0.001610 ** \nresting.bp           0.026004   0.012080   2.153 0.031346 *  \nserum.chol           0.006621   0.004228   1.566 0.117322    \nhigh.blood.sugaryes -0.370040   0.626396  -0.591 0.554692    \nelectronormal       -0.633593   0.412073  -1.538 0.124153    \nelectroSTT           0.013986   3.184512   0.004 0.996496    \nmax.hr              -0.019337   0.011486  -1.683 0.092278 .  \nanginayes            0.596869   0.460540   1.296 0.194968    \noldpeak              0.449245   0.244631   1.836 0.066295 .  \nslopeflat            0.827054   0.966139   0.856 0.391975    \nslopeupsloping      -0.122787   1.041666  -0.118 0.906166    \ncolored              1.199839   0.280947   4.271 1.95e-05 ***\nthalnormal           0.146197   0.845517   0.173 0.862723    \nthalreversible       1.577988   0.838550   1.882 0.059863 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 370.96  on 269  degrees of freedom\nResidual deviance: 168.90  on 251  degrees of freedom\nAIC: 206.9\n\nNumber of Fisher Scoring iterations: 6\n\n\nI didn’t ask you for further comment, but note that quite a lot of these variables are factors, so you get slopes for things like pain.typeatypical. When you have a factor in a model, there is a slope for each level except for the first, which is a baseline (and its slope is taken to be zero). That would be asymptomatic for pain.type. The \\(t\\)-tests for the other levels of pain.type say whether that level of pain type differs significantly (in terms of probability of heart disease) from the baseline level. Here, pain type atypical is not significantly different from the baseline, but the other two pain types, nonanginal and typical, are significantly different. If you think about this from an ANOVA-like point of view, the question about pain.type’s significance is really “is there at least one of the pain types that is different from the others”, and if we’re thinking about whether we should keep pain.type in the logistic regression, this is the kind of question we should be thinking about.\n\\(\\blacksquare\\)\n\nQuite a lot of our explanatory variables are factors. To assess whether the factor as a whole should stay or can be removed, looking at the slopes won’t help us very much (since they tell us whether the other levels of the factor differ from the baseline, which may not be a sensible comparison to make). To assess which variables are candidates to be removed, factors included (properly), we can use drop1. Feed drop1 a fitted model and the words test=\"Chisq\" (take care of the capitalization!) and you’ll get a list of P-values. Which variable is the one that you would remove first? Explain briefly.\n\nSolution\nFollowing the instructions:\n\ndrop1(heart.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe highest P-value, 0.5525, goes with high.blood.sugar, so this one comes out first. (The P-value for age is almost as high, 0.5427, so you might guess that this will be next.)\nYou might be curious about how these compare with the P-values on summary. These two P-values are almost the same as the ones on summary, because they are a two-level factor and a numeric variable respectively, and so the tests are equivalent in the two cases. (The P-values are not identical because the tests on summary and drop1 are the kind of thing that would be identical on a regular regression but are only “asymptotically the same” in logistic regression, so you’d expect them to be close without being the same, as here. “Asymptotically the same” means that if you had an infinitely large sample size, they’d be identical, but our sample size of 200-odd individuals is not infinitely large! Anyway, the largest P-value on the summary is 0.9965, which goes with electroSTT. electro, though, is a factor with three levels; this P-value says that STT is almost identical (in its effects on heart disease) with the baseline hypertrophy. But there is a third level, normal, which is a bit different from hypertrophy. So the factor electro overall has some effect on heart disease, which is reflected in the drop1 P-value of 0.12: this might go later, but it has to stay for now because at least one of its levels is different from the others in its effect on heart disease. (In backward elimination, multi-level factors are removed in their entirety if none of their levels have a different effect from any of the others.)\nThe power just went out here, so I am using my laptop on battery on its own screen, rather than on the big screen I have in my office, which is much better.\n\\(\\blacksquare\\)\n\nI’m not going to make you do the whole backward elimination (I’m going to have you use step for that later), but do one step: that is, fit a model removing the variable you think should be removed, using update, and then run drop1 again to see which variable will be removed next.\n\nSolution\nupdate is the obvious choice here, since we’re making a small change to a very big model:\n\nheart.2 &lt;- update(heart.1, . ~ . - high.blood.sugar)\ndrop1(heart.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe power is back.\nThe next variable to go is indeed age, with a P-value that has hardly changed: it is now 0.5218.\n\\(\\blacksquare\\)\n\nUse step to do a backward elimination to find which variables have an effect on heart disease. Display your final model (which you can do by saving the output from step in a variable, and asking for the summary of that. In step, you’ll need to specify a starting model (the one from part (here)), the direction of elimination, and the test to display the P-value for (the same one as you used in drop1). (Note: the actual decision to keep or drop explanatory variables is based on AIC rather than the P-value, with the result that step will sometimes keep variables you would have dropped, with P-values around 0.10.)\n\nSolution\nThe hints ought to lead you to this:\n\nheart.3 &lt;- step(heart.1, direction = \"backward\", test = \"Chisq\")\n\nStart:  AIC=206.9\nfactor(heart.disease) ~ age + sex + pain.type + resting.bp + \n    serum.chol + high.blood.sugar + electro + max.hr + angina + \n    oldpeak + slope + colored + thal\n\n                   Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n- high.blood.sugar  1   169.25 205.25  0.3528 0.5525052    \n- age               1   169.27 205.27  0.3705 0.5427474    \n- electro           2   171.31 205.31  2.4119 0.2994126    \n- angina            1   170.55 206.55  1.6562 0.1981121    \n&lt;none&gt;                  168.90 206.90                      \n- slope             2   172.98 206.98  4.0844 0.1297422    \n- serum.chol        1   171.34 207.34  2.4484 0.1176468    \n- max.hr            1   171.84 207.84  2.9391 0.0864608 .  \n- oldpeak           1   172.44 208.44  3.5449 0.0597303 .  \n- resting.bp        1   173.78 209.78  4.8793 0.0271810 *  \n- thal              2   180.78 214.78 11.8809 0.0026308 ** \n- sex               1   179.16 215.16 10.2684 0.0013533 ** \n- pain.type         3   187.85 219.85 18.9557 0.0002792 ***\n- colored           1   191.78 227.78 22.8878 1.717e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=205.25\nfactor(heart.disease) ~ age + sex + pain.type + resting.bp + \n    serum.chol + electro + max.hr + angina + oldpeak + slope + \n    colored + thal\n\n             Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n- electro     2   171.65 203.65  2.3963  0.301750    \n- age         1   169.66 203.66  0.4104  0.521756    \n- angina      1   170.78 204.78  1.5323  0.215764    \n- slope       2   173.18 205.18  3.9288  0.140240    \n&lt;none&gt;            169.25 205.25                      \n- serum.chol  1   171.69 205.69  2.4458  0.117841    \n- max.hr      1   172.35 206.35  3.0969  0.078440 .  \n- oldpeak     1   173.26 207.26  4.0094  0.045248 *  \n- resting.bp  1   173.84 207.84  4.5942  0.032080 *  \n- sex         1   179.27 213.27 10.0229  0.001546 ** \n- thal        2   181.61 213.61 12.3588  0.002072 ** \n- pain.type   3   190.54 220.54 21.2943 9.145e-05 ***\n- colored     1   191.87 225.87 22.6232 1.971e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=203.65\nfactor(heart.disease) ~ age + sex + pain.type + resting.bp + \n    serum.chol + max.hr + angina + oldpeak + slope + colored + \n    thal\n\n             Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n- age         1   172.03 202.03  0.3894 0.5326108    \n- angina      1   173.13 203.13  1.4843 0.2231042    \n&lt;none&gt;            171.65 203.65                      \n- slope       2   175.99 203.99  4.3442 0.1139366    \n- max.hr      1   175.00 205.00  3.3560 0.0669599 .  \n- serum.chol  1   175.11 205.11  3.4610 0.0628319 .  \n- oldpeak     1   175.42 205.42  3.7710 0.0521485 .  \n- resting.bp  1   176.61 206.61  4.9639 0.0258824 *  \n- thal        2   182.91 210.91 11.2633 0.0035826 ** \n- sex         1   182.77 212.77 11.1221 0.0008531 ***\n- pain.type   3   192.83 218.83 21.1859 9.632e-05 ***\n- colored     1   194.90 224.90 23.2530 1.420e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=202.03\nfactor(heart.disease) ~ sex + pain.type + resting.bp + serum.chol + \n    max.hr + angina + oldpeak + slope + colored + thal\n\n             Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n- angina      1   173.57 201.57  1.5385 0.2148451    \n&lt;none&gt;            172.03 202.03                      \n- slope       2   176.33 202.33  4.2934 0.1168678    \n- max.hr      1   175.00 203.00  2.9696 0.0848415 .  \n- serum.chol  1   175.22 203.22  3.1865 0.0742492 .  \n- oldpeak     1   175.92 203.92  3.8856 0.0487018 *  \n- resting.bp  1   176.63 204.63  4.5911 0.0321391 *  \n- thal        2   183.38 209.38 11.3500 0.0034306 ** \n- sex         1   183.97 211.97 11.9388 0.0005498 ***\n- pain.type   3   193.71 217.71 21.6786 7.609e-05 ***\n- colored     1   195.73 223.73 23.6997 1.126e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=201.57\nfactor(heart.disease) ~ sex + pain.type + resting.bp + serum.chol + \n    max.hr + oldpeak + slope + colored + thal\n\n             Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;            173.57 201.57                      \n- slope       2   178.44 202.44  4.8672 0.0877201 .  \n- serum.chol  1   176.83 202.83  3.2557 0.0711768 .  \n- max.hr      1   177.52 203.52  3.9442 0.0470322 *  \n- oldpeak     1   177.79 203.79  4.2135 0.0401045 *  \n- resting.bp  1   178.56 204.56  4.9828 0.0256006 *  \n- thal        2   186.22 210.22 12.6423 0.0017978 ** \n- sex         1   185.88 211.88 12.3088 0.0004508 ***\n- pain.type   3   200.68 222.68 27.1025 5.603e-06 ***\n- colored     1   196.98 222.98 23.4109 1.308e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output is very long. In terms of AIC, which is what step uses, age hangs on for a bit, but eventually gets eliminated.\nThere are a lot of variables left.\n\\(\\blacksquare\\)\n\nDisplay the summary of the model that came out of step.\n\nSolution\nThis:\n\nsummary(heart.3)\n\n\nCall:\nglm(formula = factor(heart.disease) ~ sex + pain.type + resting.bp + \n    serum.chol + max.hr + oldpeak + slope + colored + thal, family = \"binomial\", \n    data = heart)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -4.818418   2.550437  -1.889 0.058858 .  \nsexmale              1.850559   0.561583   3.295 0.000983 ***\npain.typeatypical   -1.268233   0.604488  -2.098 0.035903 *  \npain.typenonanginal -2.086204   0.486591  -4.287 1.81e-05 ***\npain.typetypical    -2.532340   0.748941  -3.381 0.000722 ***\nresting.bp           0.024125   0.011077   2.178 0.029410 *  \nserum.chol           0.007142   0.003941   1.812 0.069966 .  \nmax.hr              -0.020373   0.010585  -1.925 0.054262 .  \noldpeak              0.467028   0.233280   2.002 0.045284 *  \nslopeflat            0.859564   0.922749   0.932 0.351582    \nslopeupsloping      -0.165832   0.991474  -0.167 0.867167    \ncolored              1.134561   0.261547   4.338 1.44e-05 ***\nthalnormal           0.323543   0.813442   0.398 0.690818    \nthalreversible       1.700314   0.805127   2.112 0.034699 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 370.96  on 269  degrees of freedom\nResidual deviance: 173.57  on 256  degrees of freedom\nAIC: 201.57\n\nNumber of Fisher Scoring iterations: 6\n\n\nNot all of the P-values in the step output wound up being less than 0.05, but they are all at least reasonably small. As discussed above, some of the P-values in the summary are definitely not small, but they go with factors where there are significant effects somewhere. For example, thalnormal is not significant (that is, normal is not significantly different from the baseline fixed), but the other level reversible is different from fixed. You might be wondering about slope: on the summary there is nothing close to significance, but on the step output, slope has at least a reasonably small P-value of 0.088. This is because the significant difference does not involve the baseline: it’s actually between flat with a positive slope and upsloping with a negative one.\n\\(\\blacksquare\\)\n\nWe are going to make a large number of predictions. Create and save a data frame that contains predictions for all combinations of representative values for all the variables in the model that came out of step. By “representative” I mean all the values for a categorical variable, and the five-number summary for a numeric variable. (Note that you will get a lot of predictions.)\n\nSolution\nThe hard work is in listing all the variables. The easiest way to make sure you have them all is to look at the summary of your best model (mine was called heart.3) first, and copy them from the Call at the top. This is easier than looking at the table of Coefficients (or tidy output) because for categorical variables like pain.type you will have to distinguish the name of the variable from its levels. For example, the table of Coefficients has pain.typeatypical and pain.typenonanginal. Is it obvious to you where the variable name ends and its level begins?14\nAll right, let’s set up our dataframe to predict from. This needs the five-number summary of quantitative variables (via quantile), and the levels of the categorical ones (via levels(factor())). Take a deep breath and begin:\n\nnew &lt;- datagrid(model = heart.3, \n                sex = levels(factor(heart$sex)),\n                pain.type = levels(factor(heart$pain.type)),\n                resting.bp = quantile(heart$resting.bp),\n                serum.chol = quantile(heart$serum.chol),\n                max.hr = quantile(heart$max.hr),\n                oldpeak = quantile(heart$oldpeak),\n                slope = levels(factor(heart$slope)),\n                colored = quantile(heart$colored),\n                thal = levels(factor(heart$thal)))\nnew\n\n\n\n  \n\n\n\n\np &lt;- cbind(predictions(heart.3, newdata = new))\np\n\n\n\n  \n\n\n\nThere are a mere 108,000 rows here (and a fair few columns also). That is fine — as long as you don’t display them all for a grader to have to page through!\n\\(\\blacksquare\\)\n\nFind the largest predicted probability (which is the predicted probability of heart disease) and display all the variables that it was a prediction for.\n\nSolution\nThe (at current writing) approved way to do this is to use slice_max. This finds the rows with maximum value(s) on a variable, which is exactly what we want. It goes like this:\n\np %&gt;% slice_max(estimate, n = 1)\n\n\n\n  \n\n\n\nThe inputs to slice_max are the column whose maximum value you want, and the number of rows you want (so n = 3 would display the three rows with the highest predicted probabilities).\nVariations:\n\nby using prop instead of n, you can display the proportion of rows with the highest values on your variable, such as the 10% of rows with the highest predicted probabilities with prop = 0.10\nthere is also slice_min that displays the rows with the lowest values on a variable, or the input proportion of rows with the lowest values\nthere are alse slice_head and slice_tail that display the first and last (respectively) rows in a dataframe. The default display of a dataframe in an R Notebook is thus\n\n\np %&gt;% slice_head(n = 10)\n\n\n\n  \n\n\n\nexcept that the default display also tells you how many rows there are altogether.\n\nyou may have run into slice_sample, which displays a randomly-chosen number or proportion of rows from a dataframe. This is useful after you read in a dataframe from a file, if you want to get a sense of what kind of values you have in your dataframe (for example, if they are ordered by something and looking at the first ten rows won’t tell you the whole story, such as having males listed first and you want to check that there are some females as well):\n\n\np %&gt;% slice_sample(n = 10)\n\n\n\n  \n\n\n\nIf you didn’t think of slice_max, there are lots of other ways. Find one. Here are some examples:\n\np %&gt;% filter(estimate == max(estimate))\n\n\n\n  \n\n\n\nor if you didn’t think of that, you can find the maximum first, and then display the rows with predictions close to it:\n\np %&gt;% summarize(m = max(estimate))\n\n\n\n  \n\n\np %&gt;% filter(estimate &gt; 0.999998)\n\n\n\n  \n\n\n\nor even find which row has the maximum, and then display that row:\n\np %&gt;% summarize(row = which.max(estimate))\n\n\n\n  \n\n\np %&gt;% slice(67059)\n\n\n\n  \n\n\n\nor sort the rows by estimate, descending, and display the top few:\n\np %&gt;% arrange(desc(estimate)) %&gt;% slice(1:8)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nCompare the summary of the final model from step with your highest predicted heart disease probability and the values of the other variables that make it up. Are they consistent?\n\nSolution\nSince we were predicting the probability of heart disease, a more positive slope in the model from step will be associated with a higher probability of heart disease. So, there, we are looking for a couple of things: if the variable is a factor, we’re looking for the level with the most positive slope (bearing in mind that this might be the baseline), and for a numeric variable, if the slope is positive, a high value is associated with heart disease, and if negative, a low value. Bearing that in mind, we go back to my summary(heart.3):\n\nsummary(heart.3)\n\n\nCall:\nglm(formula = factor(heart.disease) ~ sex + pain.type + resting.bp + \n    serum.chol + max.hr + oldpeak + slope + colored + thal, family = \"binomial\", \n    data = heart)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -4.818418   2.550437  -1.889 0.058858 .  \nsexmale              1.850559   0.561583   3.295 0.000983 ***\npain.typeatypical   -1.268233   0.604488  -2.098 0.035903 *  \npain.typenonanginal -2.086204   0.486591  -4.287 1.81e-05 ***\npain.typetypical    -2.532340   0.748941  -3.381 0.000722 ***\nresting.bp           0.024125   0.011077   2.178 0.029410 *  \nserum.chol           0.007142   0.003941   1.812 0.069966 .  \nmax.hr              -0.020373   0.010585  -1.925 0.054262 .  \noldpeak              0.467028   0.233280   2.002 0.045284 *  \nslopeflat            0.859564   0.922749   0.932 0.351582    \nslopeupsloping      -0.165832   0.991474  -0.167 0.867167    \ncolored              1.134561   0.261547   4.338 1.44e-05 ***\nthalnormal           0.323543   0.813442   0.398 0.690818    \nthalreversible       1.700314   0.805127   2.112 0.034699 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 370.96  on 269  degrees of freedom\nResidual deviance: 173.57  on 256  degrees of freedom\nAIC: 201.57\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nsex: being male has the higher risk, by a lot\npain: all the slopes shown are negative, so the highest risk goes with the baseline one asymptomatic.\nresting.bp: positive slope, so higher risk with higher value.\nserum.chol: same.\nmax.hr: negative slope, so greatest risk with smaller value.\noldpeak: positive slope, greater risk with higher value again.\nslope: flat has greatest risk.\ncolored: positive slope, so beware of higher value.\nthal: reversible has greatest risk.\n\nThen we can do the same thing for the prediction.\nand the highest prediction:\n\np %&gt;% slice_max(estimate, n = 1)\n\n\n\n  \n\n\n\nFor the numerical variables, we may need to check back to the previous part to see whether the value shown was high or low. Once you have done that, you can see that the variable values for the highest predicted probability do indeed match the ones we thought should be the highest risk.\nExtra: the interesting thing about this is that even after adjusting for all of the other variables, there is a greater risk of heart disease if you are male (and the model shows that the risk is much greater). That is to say, it’s being male that makes the difference, not the fact that any of the other variables are different for males.\nIt’s rather difficult to scan 108,000 predictions to see the effect of being male, but we can do this:\n\nnew &lt;- datagrid(model = heart.3, sex = levels(factor(heart$sex)))\ncbind(predictions(heart.3, newdata = new))\n\n\n\n  \n\n\n\nWhat this does is to choose a single representative value for all the other variables: the mean for a quantitative variable like resting blood pressure, and the most common category for a categorical variable like pain.type. If you scan all the way along the two rows, you find that the values for all the variables are the same except for sex at the end. The predicted probabilities of heart disease are very different for males and females (much higher for males), especially given that all else really is equal.\nTo see this graphically, we can use plot_cap, and we can include another variable such as resting blood pressure. It’s better to list the quantitative one first:\n\nplot_cap(heart.3, condition = c(\"resting.bp\", \"sex\"))\n\n\n\n\nAs the resting blood pressure increases, the probability of heart disease increases, but the blue line for males is well above the red one for females all the way across. For example, for a 50% chance of heart disease, this will happen for males with a resting blood pressure of about 120, but for females not until the resting blood pressure reaches 190!\nPerhaps, therefore, the easiest way to avoid a heart attack is to not be male!\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#successful-breastfeeding-1",
    "href": "logistic-regression.html#successful-breastfeeding-1",
    "title": "24  Logistic regression",
    "section": "24.17 Successful breastfeeding",
    "text": "24.17 Successful breastfeeding\nA regular pregnancy lasts 40 weeks, and a baby that is born at or before 33 weeks is called “premature”. The number of weeks at which a baby is born is called its “gestational age”. Premature babies are usually smaller than normal and may require special care. It is a good sign if, when the mother and baby leave the hospital to go home, the baby is successfully breastfeeding.\nThe data in link are from a study of 64 premature infants. There are three columns: the gestational age (a whole number of weeks), the number of babies of that gestational age that were successfully breastfeeding when they left the hospital, and the number that were not. (There were multiple babies of the same gestational age, so the 64 babies are summarized in 6 rows of data.)\n\nRead the data into R and display the data frame.\n\nSolution\nNo great challenge here, I hope:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/breastfeed.csv\"\nbreastfeed &lt;- read_csv(my_url)\n\nRows: 6 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): gest.age, bf.yes, bf.no\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbreastfeed\n\n\n\n  \n\n\n\nThat looks reasonable.\n\\(\\blacksquare\\)\n\nVerify that there were indeed 64 infants, by having R do a suitable calculation on your data frame that gives the right answer for the right reason.\n\nSolution\nThe second and third columns are all frequencies, so it’s a question of adding them up. For example:\n\nbreastfeed %&gt;% summarize(total = sum(bf.yes) + sum(bf.no))\n\n\n\n  \n\n\n\nor if you want to go nuts (this one pivot-longers all the frequencies together into one column and then adds them up):\n\nbreastfeed %&gt;%\n  pivot_longer(bf.yes:bf.no, names_to=\"yesno\", values_to=\"freq\") %&gt;%\n  summarize(total = sum(freq))\n\n\n\n  \n\n\n\nFind a way to get it done. If it works and it does the right thing, it’s good.\nDo not copy the numbers out of your data frame, type them in again and use R to add them up. Do something with your data frame as you read it in.\n\\(\\blacksquare\\)\n\nDo you think, looking at the data, that there is a relationship between gestational age and whether or not the baby was successfully breastfeeding when it left the hospital? Explain briefly.\n\nSolution\nThe babies with the youngest gestational age (the most premature) were mostly not breastfeeding when they left the hospital. Most of the 30- and 31-week babies were breastfeeding, and almost all of the 32- and 33-week babies were breastfeeding. So I think there will be a relationship: as gestational age increases, the probability that the baby will be breastfeeding will also increase. (This, looking ahead, suggests a positive slope in a logistic regression.)\n\\(\\blacksquare\\)\n\nWhy is logistic regression a sensible technique to use here? Explain briefly.\n\nSolution\nThe response variable is a yes/no: whether or not an infant is breastfeeding. We want to predict the probability of the response being in one or the other category. This is what logistic regression does. (The explanatory variable(s) are usually numerical, as here, but they could be factors as well, or a mixture. The key is the kind of response. The number of babies that are successfully breastfeeding at a certain gestational age is modelled as binomial with \\(n\\) being the total number of babies of that gestational age, and \\(p\\) being something that might depend, and here does depend, on gestational age.)\n\\(\\blacksquare\\)\n\nFit a logistic regression to predict the probability that an infant will be breastfeeding from its gestational age. Show the output from your logistic regression.\n\nSolution\nThese are summarized data, rather than one infant per line, so what we have to do is to make a two-column response “matrix”, successes in the first column and failures in the second, and then predict that from gestational age. (That’s why this was three marks rather than two.) So, let’s make the response first:\n\nresponse &lt;- with(breastfeed, cbind(bf.yes, bf.no))\nresponse\n\n     bf.yes bf.no\n[1,]      2     4\n[2,]      2     3\n[3,]      7     2\n[4,]      7     2\n[5,]     16     4\n[6,]     14     1\n\n\nor, more Tidyverse-like, but we have to remember to turn it into a matrix:\n\nresponse &lt;- breastfeed %&gt;%\n  select(starts_with(\"bf\")) %&gt;%\n  as.matrix()\nresponse\n\n     bf.yes bf.no\n[1,]      2     4\n[2,]      2     3\n[3,]      7     2\n[4,]      7     2\n[5,]     16     4\n[6,]     14     1\n\n\nI used a select-helper, because what immediately came to me was that the names of the columns I wanted started with bf, but whatever way you have that works is good. Now we fit the logistic regression:\n\nbreastfeed.1 &lt;- glm(response ~ gest.age, data = breastfeed, family = \"binomial\")\nsummary(breastfeed.1)\n\n\nCall:\nglm(formula = response ~ gest.age, family = \"binomial\", data = breastfeed)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -16.7198     6.0630  -2.758  0.00582 **\ngest.age      0.5769     0.1977   2.918  0.00352 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11.1772  on 5  degrees of freedom\nResidual deviance:  1.4968  on 4  degrees of freedom\nAIC: 19.556\n\nNumber of Fisher Scoring iterations: 4\n\n\n\\(\\blacksquare\\)\n\nDoes the significance or non-significance of the slope of gest.age surprise you? Explain briefly.\n\nSolution\nThe slope is significant (P-value 0.0035 is much less than 0.05). We said above that we expected there to be a relationship between gestational age and whether or not the baby was breastfeeding, and this significant slope is confirming that there is a relationship. So this is exactly what we expected to see, and not a surprise at all. If you concluded above that you did not see a relationship, you should colour yourself surprised here. Consistency.\n\\(\\blacksquare\\)\n\nIs your slope (in the Estimate column) positive or negative? What does that mean, in terms of gestational ages and breastfeeding? Explain briefly.\n\nSolution\nMy slope is 0.5769, positive. That means that as the explanatory variable, gestational age, increases, the probability of the event (that the baby is breastfeeding) also increases. This is also what I observed above: almost all of the near-term (large gestational age) babies were breastfeeding, whereas a fair few of the small-gestational-age (very premature) ones were not.\nWe know that the event is “is breastfeeding” rather than “is not”, because the first column of our response matrix is “yes” rather than “no”:\n\nresponse\n\n     bf.yes bf.no\n[1,]      2     4\n[2,]      2     3\n[3,]      7     2\n[4,]      7     2\n[5,]     16     4\n[6,]     14     1\n\n\n(If you had happened to make your response matrix the other way around, the event would have been “is not breastfeeding”, and then your slope would have been the same size but negative.)\n\\(\\blacksquare\\)\n\nObtain the predicted probabilities that an infant will successfully breastfeed for a representative collection of gestational ages.\n\nSolution\nPick some gestational ages, like the median and quartiles, or just pick some values like 25, 30, 35:15\n\nnew &lt;- datagrid(model = breastfeed.1, gest.age = c(25, 30, 35))\ncbind(predictions(breastfeed.1, newdata = new))\n\n\n\n  \n\n\n\nOr, if you wanted to make a graph of the observed and predicted proportions/probabilities, you would have to build it yourself since the response variable is not in the dataframe, like this:\n\nnew &lt;- datagrid(model = breastfeed.1, gest.age = 28:33)\np &lt;- cbind(predictions(breastfeed.1, new))\nggplot(p, aes(x = gest.age, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3)\n\n\n\n\nor, if you wanted to add the data to this:\n\nbreastfeed %&gt;% \n  mutate(total = bf.yes + bf.no, \n         proportion = bf.yes / total) -&gt; d\nggplot(p, aes(x = gest.age, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) +\n  geom_point(data = d, aes(x = gest.age, y = proportion, size = total), inherit.aes = FALSE)\n\n\n\n\nWhat did I do there? I first created some new variables: total is the total number of babies of each gestational age, and proportion is the observed proportion of breastfeeding babies at each gestational age (number of yes divided by total). pred which are the predictions we did above. Then I repeat the plot_cap, and on that plot I add the observed proportions against gestational age (as points). To do that, I need to change which dataframe I am using (the temporary one called d with the proportions and totals in it), and the variables I am plotting (which are now the ones in d). The way I do that is to put an aes inside the geom_line to say “use this x and y instead”. I also wanted to draw attention to the gestational ages where more babies were observed; I did this by making the size of the plotted points proportional to how many babies there were at that gestational age (which was the quantity total I calculated above).\nThe legend for total tells you what size point corresponds to how many total babies. The final thing is that we used some other things (like the min and max of the confidence interval) that we would also have to supply values for were it not for the inherit.aes at the end; this means “don’t use anything from the original ggplot but supply everything yourself”.\nThe idea is that the observed and predicted should be reasonably close, or at least not grossly different, especially when there is a lot of data (the circles are large), and I think they are close, which indicates that our model is doing a good job. You can see that there is not much data on the left, with small gestational ages, so the confidence interval around the predictions is wider there. On the right, where there is lots of data, the interval is narrower. When the gestational age is large (the baby is closer to being full term rather than premature), there is a good chance that the baby will be able to breastfeed, and we are fairly sure about that.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#making-it-over-the-mountains-1",
    "href": "logistic-regression.html#making-it-over-the-mountains-1",
    "title": "24  Logistic regression",
    "section": "24.18 Making it over the mountains",
    "text": "24.18 Making it over the mountains\nIn 1846, the Donner party (Donner and Reed families) left Springfield, Illinois for California in covered wagons. After reaching Fort Bridger, Wyoming, the leaders decided to find a new route to Sacramento. They became stranded in the eastern Sierra Nevada mountains at a place now called Donner Pass, when the region was hit by heavy snows in late October. By the time the survivors were rescued on April 21, 1847, 40 out of 87 had died.\nAfter the rescue, the age and gender of each person in the party was recorded, along with whether they survived or not. The data are in link.\n\nRead in the data and display its structure. Does that agree with the description in the previous paragraph?\n\nSolution\nNothing very new here:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/donner.txt\"\ndonner &lt;- read_delim(my_url, \" \")\n\nRows: 45 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): gender, survived\ndbl (1): age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndonner\n\n\n\n  \n\n\n\nThe ages look like ages, and the other variables are categorical as promised, with the right levels. So this looks sensible.\nI usually write (or rewrite) the data description myself, but the one I found for these data at link was so nice that I used it as is. You can see that the data format used on that website is not as nice as ours. (I did some work to make it look nicer for you, with proper named categories rather than 0 and 1.)\n\\(\\blacksquare\\)\n\nMake graphical or numerical summaries for each pair of variables. That is, you should make a graph or numerical summary for each of age vs. gender, age vs.\nsurvived and gender vs. survived. In choosing the kind of graph or summary that you will use, bear in mind that survived and gender are factors with two levels.\n\nSolution\nThinking about graphs first: we have two numeric-vs-factor graphs (the ones involving age), which I think should be boxplots, though they could be something like side-by-side histograms (if you are willing to grapple with facet_grid). The other two variables are both factors, so a good graph for them would be something like a grouped bar plot (as in the question about parasites earlier). If you prefer numerical summaries: you could do mean age (or some other summary of age) for each group defined by gender or survivorship, and you could do a cross-tabulation of frequencies for gender and survival. I’ll take any mixture of graphs and numerical summaries that addresses each pair of variables somehow and summarizes them in a sensible way. Starting with age vs. gender:\n\nggplot(donner, aes(x = gender, y = age)) + geom_boxplot()\n\n\n\n\nor:\n\nggplot(donner, aes(x = age)) + geom_histogram(bins = 10) + facet_grid(gender ~ .)\n\n\n\n\nor:\n\ndonner %&gt;% group_by(gender) %&gt;% summarize(m = mean(age))\n\n\n\n  \n\n\n\nAge vs. survived is the same idea:\n\nggplot(donner, aes(x = survived, y = age)) + geom_boxplot()\n\n\n\n\nor:\n\nggplot(donner, aes(x = age)) + geom_histogram(bins = 10) + facet_grid(survived ~ .)\n\n\n\n\nor:\n\ndonner %&gt;% group_by(survived) %&gt;% summarize(m = mean(age))\n\n\n\n  \n\n\n\nFor survived against gender, the obvious thing is a cross-tabulation, gotten like this:\n\nwith(donner, table(gender, survived))\n\n        survived\ngender   no yes\n  female  5  10\n  male   20  10\n\n\nor like this:\n\ndonner %&gt;% group_by(gender, survived) %&gt;% summarize(n = n())\n\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nFor a graph, borrow the grouped bar-plot idea:\n\nggplot(donner, aes(x = gender, fill = survived)) + geom_bar(position = \"dodge\")\n\n\n\n\nI think this way around for x and fill is better, since we want to ask something like “out of the females, how many survived” (that is, gender is explanatory and survival response).\n\\(\\blacksquare\\)\n\nFor each of the three graphs or summaries in the previous question, what do they tell you about the relationship between the pair of variables concerned? Explain briefly.\n\nSolution\nI think the graphs are the easiest thing to interpret, but use whatever you got:\n\nThe females on average were younger than the males. (This was still true with the medians, even though those very old males might have pulled the mean up.)\nThe people who survived were on average younger than those who didn’t (or, the older people tended not to survive).\nA greater proportion of females survived than males.\n\nA relevant point about each relationship is good.\n\\(\\blacksquare\\)\n\nFit a logistic regression predicting survival from age and gender. Display the summary.\n\nSolution\nEach row of the data frame is one person, so we can use the survived column without going through that two-column response business. However, the response variable survived is a categorical variable expressed as text, so we need to make it into a factor first. Either create a new variable that is the factor version of survived, or do it right in the glm:\n\ndonner.1 &lt;- glm(factor(survived) ~ age + gender, family = \"binomial\", data = donner)\nsummary(donner.1)\n\n\nCall:\nglm(formula = factor(survived) ~ age + gender, family = \"binomial\", \n    data = donner)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  3.23041    1.38686   2.329   0.0198 *\nage         -0.07820    0.03728  -2.097   0.0359 *\ngendermale  -1.59729    0.75547  -2.114   0.0345 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.827  on 44  degrees of freedom\nResidual deviance: 51.256  on 42  degrees of freedom\nAIC: 57.256\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe ought to take a moment to think about what is being predicted here:\n\nlevels(factor(donner$survived))\n\n[1] \"no\"  \"yes\"\n\n\nThe baseline is the first of these, no, and the thing that is predicted is the probability of the second one, yes (that is, the probability of surviving).\n\\(\\blacksquare\\)\n\nDo both explanatory variables have an impact on survival? Does that seem to be consistent with your numerical or graphical summaries? Explain briefly.\n\nSolution\nBoth explanatory variables have a P-value (just) less than 0.05, so they both have an impact on survival: taking either one of them out of the logistic regression would be a mistake. To see if this makes sense, go back to your plots or summaries, the ones involving survival. For age, the mean or median age of the survivors was less than for the people who died, by five year (median) or eight years (mean), so it makes sense that there would be an age effect. For gender, two-thirds of the women survived and two-thirds of the men died, so there ought to be a gender effect and is.\n\\(\\blacksquare\\)\n\nAre the men typically older, younger or about the same age as the women? Considering this, explain carefully what the negative gendermale slope in your logistic regression means.\n\nSolution\nThe men are typically older than the women. The negative (and significant) gendermale slope means that the probability of a male surviving is less than that of a woman of the same age. Or, though the males are typically older, even after you allow for that, the males have worse survival. (genderfemale is the baseline.) You need to get at the idea of “all else equal” when you’re assessing regression slopes of any kind: regular regression, logistic regression or survival analysis (coming up later). That’s why I said “carefully” in the question. If I say “carefully” or “precisely”, a complete answer is looking for a specific thing to show that you understand the issue completely.\n\\(\\blacksquare\\)\n\nObtain predicted probabilities of survival for each combination of some representative ages and of the two genders in this dataset.\n\nSolution\nMake a dataframe containing some ages (you pick them) and the two genders, in combination, using datagrid:\n\nsummary(donner)\n\n      age          gender            survived        \n Min.   :15.0   Length:45          Length:45         \n 1st Qu.:24.0   Class :character   Class :character  \n Median :28.0   Mode  :character   Mode  :character  \n Mean   :31.8                                        \n 3rd Qu.:40.0                                        \n Max.   :65.0                                        \n\n\n\nnew &lt;- datagrid(model = donner.1, age = c(15, 25, 30, 40, 65), gender = c(\"female\", \"male\"))\nnew\n\n\n\n  \n\n\n\n\ncbind(predictions(donner.1, newdata = new))\n\n\n\n  \n\n\n\n(There are five ages chosen, something like a five-number summary, and two genders here, so ten predictions.)\nThese, remember, are predicted probabilities of surviving.\n\\(\\blacksquare\\)\n\nDo your predictions support your conclusions from earlier about the effects of age and gender? Explain briefly.\n\nSolution\nWe said before that the probability of survival was lower if the age was higher. This is confirmed here: for example, look at the odd-numbered rows 1, 3, 5, 7, 9, which are all females of increasing ages; the probability of survival decreases. (Or look at males, in the even-numbered rows; the effect is the same.)\nTo see the effect of gender, look at two predictions of different genders but the same age (eg. rows 1 and 2). The female is always predicted to have the higher survival probability. This is also what we saw before. The effect of gender is substantial, but not strongly significant, because we only have 45 observations, not so many when all we know about each person is whether they survived or not. I wanted you to think about these different ways to understand the model, and to understand that they all say the same thing, in different ways (and thus you can look at whichever of them is most convenient or comprehensible). For the logistic and survival models, I find looking at predictions to be the easiest way to understand what the model is saying.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#who-needs-the-most-intensive-care-1",
    "href": "logistic-regression.html#who-needs-the-most-intensive-care-1",
    "title": "24  Logistic regression",
    "section": "24.19 Who needs the most intensive care?",
    "text": "24.19 Who needs the most intensive care?\nThe “APACHE II” is a scale for assessing patients who arrive in the intensive care unit (ICU) of a hospital. These are seriously ill patients who may die despite the ICU’s best attempts. APACHE stands for “Acute Physiology And Chronic Health Evaluation”.16 The scale score is calculated from several physiological measurements such as body temperature, heart rate and the Glasgow coma scale, as well as the patient’s age. The final result is a score between 0 and 71, with a higher score indicating more severe health issues. Is it true that a patient with a higher APACHE II score has a higher probability of dying?\nData from one hospital are in link. The columns are: the APACHE II score, the total number of patients who had that score, and the number of patients with that score who died.\n\nRead in and display the data (however much of it displays). Why are you convinced that have the right thing?\n\nSolution\nData values separated by one space, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/apache.txt\"\nicu &lt;- read_delim(my_url, \" \")\n\nRows: 38 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): apache, patients, deaths\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nicu\n\n\n\n  \n\n\n\nI had to stop and think about what to call the data frame, since one of the columns is called apache.\nAnyway, I appear to have an apache score between 0 and something, a number of patients and a number of deaths (that is no bigger than the number of patients). If you check the original data, the apache scores go up to 41 and are all the values except for a few near the end, so it makes perfect sense that there would be 38 rows.\nBasically, any comment here is good, as long as you make one and it has something to do with the data.\napache scores could be as high as 71, but I imagine a patient would have to be very ill to get a score anywhere near that high.\n\\(\\blacksquare\\)\n\nDoes each row of the data frame relate to one patient or sometimes to more than one? Explain briefly.\n\nSolution\nSometimes to more than one. The number in the patients column says how many patients that line refers to: that is to say (for example) the row where apache equals 6 represents all the patients whose apache score was 6, however many of them there were (14 in this case). I had to be careful with the wording because the first two rows of the data frame actually do refer to only one patient each (who survived in both cases), but the later rows do refer to more than one patient.\n\\(\\blacksquare\\)\n\nExplain why this is the kind of situation where you need a two-column response, and create this response variable, bearing in mind that I will (later) want you to estimate the probability of dying, given the apache score.\n\nSolution\nThis needs a two-column response precisely because each row represents (or could represent) more than one observation. The two columns are the number of observations referring to the event of interest (dying), and the number of observations where that didn’t happen (survived). We don’t actually have the numbers of survivals, but we can calculate these by subtracting from the numbers of patients (since a patient must have either lived or died):\n\nresponse &lt;- icu %&gt;%\n  mutate(survivals = patients - deaths) %&gt;%\n  select(deaths, survivals) %&gt;%\n  as.matrix()\nresponse\n\n      deaths survivals\n [1,]      0         1\n [2,]      0         1\n [3,]      1         3\n [4,]      0        11\n [5,]      3         6\n [6,]      3        11\n [7,]      4         8\n [8,]      5        17\n [9,]      3        30\n[10,]      6        13\n[11,]      5        26\n[12,]      5        12\n[13,]     13        19\n[14,]      7        18\n[15,]      7        11\n[16,]      8        16\n[17,]      8        19\n[18,]     13         6\n[19,]      7         8\n[20,]      6         7\n[21,]      9         8\n[22,]     12         2\n[23,]      7         6\n[24,]      8         3\n[25,]      8         4\n[26,]      2         4\n[27,]      5         2\n[28,]      1         2\n[29,]      4         3\n[30,]      4         1\n[31,]      3         0\n[32,]      3         0\n[33,]      1         0\n[34,]      1         0\n[35,]      1         0\n[36,]      1         0\n[37,]      1         0\n[38,]      0         1\n\n\nnoting that the deaths column has to come first since that’s what we want the probability of. It has to be a matrix, so as.matrix is the final step. You can quickly check that the two numbers in each row add up to the number of patients for that row.\nOr do everything outside of the data frame:\n\nsurvivals &lt;- with(icu, patients - deaths)\nresp &lt;- with(icu, cbind(deaths, survivals))\nresp\n\n      deaths survivals\n [1,]      0         1\n [2,]      0         1\n [3,]      1         3\n [4,]      0        11\n [5,]      3         6\n [6,]      3        11\n [7,]      4         8\n [8,]      5        17\n [9,]      3        30\n[10,]      6        13\n[11,]      5        26\n[12,]      5        12\n[13,]     13        19\n[14,]      7        18\n[15,]      7        11\n[16,]      8        16\n[17,]      8        19\n[18,]     13         6\n[19,]      7         8\n[20,]      6         7\n[21,]      9         8\n[22,]     12         2\n[23,]      7         6\n[24,]      8         3\n[25,]      8         4\n[26,]      2         4\n[27,]      5         2\n[28,]      1         2\n[29,]      4         3\n[30,]      4         1\n[31,]      3         0\n[32,]      3         0\n[33,]      1         0\n[34,]      1         0\n[35,]      1         0\n[36,]      1         0\n[37,]      1         0\n[38,]      0         1\n\nclass(resp)\n\n[1] \"matrix\" \"array\" \n\n\nOr use the dollar sign instead of the withs. Any of those is good.\nI have no objection to your displaying the response matrix.\n\\(\\blacksquare\\)\n\nFit a logistic regression to estimate the probability of death from the apache score, and display the results.\n\nSolution\n\napache.1 &lt;- glm(response ~ apache, family = \"binomial\", data = icu)\nsummary(apache.1)\n\n\nCall:\nglm(formula = response ~ apache, family = \"binomial\", data = icu)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.2903     0.2765  -8.282  &lt; 2e-16 ***\napache        0.1156     0.0160   7.227 4.94e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 106.009  on 37  degrees of freedom\nResidual deviance:  43.999  on 36  degrees of freedom\nAIC: 125.87\n\nNumber of Fisher Scoring iterations: 4\n\n\nMy naming convention has gotten messed up again. This should really be called deaths.1 or something like that, but that would be a really depressing name.\n\\(\\blacksquare\\)\n\nIs there a significant effect of apache score on the probability of survival? Explain briefly.\n\nSolution\nA gimme two points. The P-value for apache is \\(4.94 \\times 10^{-13}\\), very small, so apache score definitely has an effect on the probability of survival.\n\\(\\blacksquare\\)\n\nIs the effect of a larger apache score to increase or to decrease the probability of death? Explain briefly.\n\nSolution\nThe slope coefficient for apache is 0.1156, positive, and since we are modelling the probability of death (the first column of the response matrix), this says that as apache goes up, the probability of death goes up as well. If you made your response matrix with the columns the other way around, the slope coefficient for apache should be \\(-0.1156\\), but the explanation should come to the same place, because this says that the probability of survival goes down as apache goes up.\n\\(\\blacksquare\\)\n\nObtain the predicted probability of death for a representative collection of the apache scores that were in the data set. Do your predictions behave as you would expect (from your earlier work)?\n\nSolution\n“Representative” is a clue that you can choose more or less what values you like. The median and quartiles, or the five-number summary, or something near those, are reasonable choices:\n\nsummary(icu)\n\n     apache         patients         deaths      \n Min.   : 0.00   Min.   : 1.00   Min.   : 0.000  \n 1st Qu.:10.25   1st Qu.: 3.00   1st Qu.: 1.000  \n Median :19.50   Median :11.50   Median : 4.000  \n Mean   :19.55   Mean   :11.95   Mean   : 4.605  \n 3rd Qu.:28.75   3rd Qu.:17.75   3rd Qu.: 7.000  \n Max.   :41.00   Max.   :33.00   Max.   :13.000  \n\nnew &lt;- datagrid(model = apache.1, apache = c(10, 20, 30))\nnew\n\n\n\n  \n\n\n\n\ncbind(predictions(apache.1, newdata = new))\n\n\n\n  \n\n\n\nAs the apache score goes up, the predicted probability of death also goes up. This is what we would expect to see given the positive slope on apache in the model apache.1. (Again, if your model had the columns of the response the other way around, you were predicting the probability of survival, and your predictions should then show it going down rather than up to go with the negative slope you would then have had.)\n\\(\\blacksquare\\)\n\nMake a plot of predicted death probability against apache score (joined by lines) with, also on the plot, the observed proportion of deaths within each apache score, plotted against apache score. Does there seem to be good agreement between observation and prediction? Hint: calculate what you need to from the original dataframe first, save it, then make a plot of the predictions, and then to the plot add the appropriate thing from the dataframe you just saved.\n\nSolution\nAll right, following the hint, let’s start with the original dataframe, called icu. The plot of the predictions is going to show predicted probabilities of death, so from the data we need to find the observed proportions of death at each apache score. Our dataframe has one row for each apache score, so this is not too hard. We have the total number of patients at each score, and the number of deaths out of those patients, so the proportion that died is the second of those divided by the first one:\n\nicu %&gt;% mutate(proportion = deaths / patients) -&gt; d\nd\n\n\n\n  \n\n\n\nThat looks all right.\nTo plot predicted values, your first port of call would normally be plot_cap, since the job of this function is to make a lot of predictions and construct a nice plot of them, but this time we cannot since the response variable is a matrix and thus not part of the dataframe. So we have to make this ourselves. Let’s do the predictions again with more Apache scores, so that we get a better plot:\n\nnew &lt;- datagrid(model = apache.1, apache = 0:40)\ncbind(predictions(apache.1, newdata = new))\n\n\n\n  \n\n\n\nThen, make a graph of these:\n\ncbind(predictions(apache.1, newdata = new)) %&gt;% \n  ggplot(aes(x = apache, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3)\n\n\n\n\nThis again shows that the predicted probability of death goes up sharply with apache score. Let’s add the data to this graph. The trick is to remember how to add points to a graph you already have, when the points come from a different data set:\n\ncbind(predictions(apache.1, newdata = new)) %&gt;% \n  ggplot(aes(x = apache, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) + \n  geom_point(data = d, aes(x = apache, y = proportion), inherit.aes = FALSE)\n\n\n\n\nThat does a reasonably good job, but we can do better. We observe now that most of the points are reasonably close to the curve, except for that one in the bottom right, a patient who had the highest apache score of all, but who happened to survive. The observed proportions that are 1 or 0 over on the right might have been based on only one patient, and the others might have been based on more. So it would be better to encode how many patients each point is based on, for example by the size of the point (you may be able to think of other ways like colour, and you can experiment to see what gets the message across best in your opinion).\nA reminder that the inherit.aes is to tell ggplot not to take anything for the geom_point from the plot_cap graph (which has additional things that it would be a pain to override).\nAll right, we can make the points different sizes according to the number of patients they are based on, thus:\n\ncbind(predictions(apache.1, newdata = new)) %&gt;% \n  ggplot(aes(x = apache, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) + \n  geom_point(data = d, aes(x = apache, y = proportion, size = patients), \n             inherit.aes = FALSE)\n\n\n\n\nThis shows clearly that the points on the left and right of the graph are based on very few patients, maybe only one each time. But most of the scores between about 10 and 20 were represented by more patients, maybe up to 30. The bigger circles seem to follow the trend of the predictions pretty well, which is what we were hoping for; the proportions based on small numbers of patients might be further away from the predictions, and that’s all right.\nExtra: you can see that we want the proportions to be based on reasonable numbers of patients, but the other end of the issue is that we don’t want to combine patients with very different apache scores, because then you wouldn’t get much of a picture of how well the data follow the trend. This is very much the same idea as choosing the number of bins on a histogram; if you have too many bins, each one will contain very few observations and you won’t see the pattern very clearly, but if you have too few bins, you’ll have lots of observations in each bin but you still won’t see the shape.\nWith that in mind, perhaps we can try binning the observations in our data here. Let’s see what that does. The starting point is to redefine what I called d before which had the proportion in it:\n\nbreak_points &lt;- seq(-1, 45, 4)\nbreak_points\n\n [1] -1  3  7 11 15 19 23 27 31 35 39 43\n\nicu %&gt;% mutate(bins = cut(apache, breaks = break_points))\n\n\n\n  \n\n\n\nMy bins are 4 apache points wide. I have no idea yet whether that’s any good, but I want to get the procedure down so that I can come back and change that later if I want to.\nWhat I am using to make the bins is a base R function called cut. This makes categorical bins out of a quantitative variable (which is normally a bad idea, but we have our reasons here). It takes two inputs: a vector of numbers to bin, and the points that divide one bin from the next, which I defined into a vector called break_points first. The function cut defines bins as “half-open”, meaning that the top end is included but the bottom end is excluded. (Note for example which bin an apache score of 3 goes into.)\nNow we want to total up the patients and deaths within each bin, and, for reasons you’ll see later, we want to know where the middle of each bin is (for which I will use the median of the apache scores within that bin):\n\nicu %&gt;% mutate(bins = cut(apache, breaks = break_points)) %&gt;% \n  group_by(bins) %&gt;% \n  summarize(patients = sum(patients), deaths = sum(deaths), apache = median(apache))\n\n\n\n  \n\n\n\nWe have redefined the names: patients and deaths are now the totals of patients and deaths within each bin, and apache is now something like the middle apache score in each bin.\nNow we can work out the proportion of patients that died within each bin, and save that:\n\nicu %&gt;% mutate(bins = cut(apache, breaks = break_points)) %&gt;% \n  group_by(bins) %&gt;% \n  summarize(patients = sum(patients), deaths = sum(deaths), apache = median(apache)) %&gt;% \n  mutate(proportion = deaths / patients) -&gt; d\nd\n\n\n\n  \n\n\n\nNow we redo our graph, but using the proportions in here as the observed proportions of deaths in the data. You might now realize the reason for those values in the apache column: on the graph, the proportion values will be our \\(y\\) coordinates, but we needed something to be the \\(x\\) coordinates.\nActually redrawing our graph is in fact exactly the same code as before; the thing that has changed is our definition of d in it:\n\ncbind(predictions(apache.1, newdata = new)) %&gt;% \n  ggplot(aes(x = apache, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() + geom_ribbon(alpha = 0.3) + \n  geom_point(data = d, aes(x = apache, y = proportion, size = patients), \n             inherit.aes = FALSE)\n\n\n\n\nAnd now we see, with more patients per bin, that the agreement between data and prediction is very good apart from the very small bins on the right.\nOne modification that you might like to pursue is to have variable-width bins: wider bins at the extremes and narrower bins in the middle, so that each bin has about the same number of patients. You could do this by modifying what I called break_points so that the numbers in it are more spread out at the extremes and closer together in the middle.\nExtra extra: if you’re a football fan, you could imagine doing a very similar analysis by modelling the probability of successfully kicking a field goal as it depends on the distance it is attempted from. This would probably work pretty well for the NFL and (no doubt) for the CFL as well. In both these codes of football, field goals are always attempted from between two side-to-side marks on the field called “hash marks”. In the NFL, the hash marks are close together, so field goals are attempted from more or less directly in front of the goalposts. In the CFL, the hash marks are further apart, so a field goal might be attempted from off to the side of the goalposts, and then the probability of success might also depend on how far off to the side you are.\nIf you happen also to be a rugby fan, you’ll know that kicks at goal might need to be attempted from anywhere on the field, even out near the sidelines, and in that case, the chance of kicking a goal definitely depends on where on the field you are kicking it from as well as how far out it is.\nExtra extra extra: football (NFL and CFL) have an effective formality called the “point after”: after a team scores a touchdown, the kicker kicks at goal from right in front of the posts, which is almost never missed. In rugby, the equivalent to a touchdown is called a “try”, and to score a try the player must literally touch the ball down. (Thus, to a rugby fan, the NFL and CFL “touchdowns” are absurd misnomers.) After a try is scored, there is a kick at goal (called a “conversion”) which is not taken from right in front of the posts like in the NFL, but from the side-to-side position on the field where the try was scored. Thus, if a try is scored near the sidelines, as it often is, the conversion has to be kicked from near the sideline as well. The kicker can go as far back down the field as they like, but as you might imagine, a sideline conversion is still very difficult to kick.\nExtra-to-the-fourth: the reason for those names in rugby. In the old days, if a team scored a try and kicked the conversion as well, it was called a “goal”, and as a rugby player, you wanted your team to score a goal. Scoring a try allowed a team to “try” to “convert” it into a “goal” by kicking the conversion, hence the names. Because the NFL and CFL have their roots in rugby, they have the “point after” a touchdown as well (with the option now of allowing teams to try for two points after a touchdown as well).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#go-away-and-dont-come-back-1",
    "href": "logistic-regression.html#go-away-and-dont-come-back-1",
    "title": "24  Logistic regression",
    "section": "24.20 Go away and don’t come back!",
    "text": "24.20 Go away and don’t come back!\nWhen a person has a heart attack and survives it, the major concern of health professionals is to prevent the person having a second heart attack. Two factors that are believed to be important are anger and anxiety; if a heart attack survivor tends to be angry or anxious, they are believed to put themselves at increased risk of a second heart attack.\nTwenty heart attack survivors took part in a study. Each one was given a test designed to assess their anxiety (a higher score on the test indicates higher anxiety), and some of the survivors took an anger management course. The data are in link; y and n denote “yes” and “no” respectively. The columns denote (i) whether or not the person had a second heart attack, (ii) whether or not the person took the anger management class, (iii) the anxiety score.\n\nRead in and display the data.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ha2.txt\"\nha &lt;- read_delim(my_url, \" \")\n\nRows: 20 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): second, anger\ndbl (1): anxiety\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nha\n\n\n\n  \n\n\n\nThe anxiety scores are numbers; the other two variables are “yes” and “no”, which makes perfect sense.\n\\(\\blacksquare\\)\n\n* Fit a logistic regression predicting whether or not a heart attack survivor has a second heart attack, as it depends on anxiety score and whether or not the person took the anger management class. Display the results.\n\nSolution\nThis:\n\nha.1 &lt;- glm(factor(second) ~ anger + anxiety, family = \"binomial\", data = ha)\nsummary(ha.1)\n\n\nCall:\nglm(formula = factor(second) ~ anger + anxiety, family = \"binomial\", \n    data = ha)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -6.36347    3.21362  -1.980   0.0477 *\nangery      -1.02411    1.17101  -0.875   0.3818  \nanxiety      0.11904    0.05497   2.165   0.0304 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 27.726  on 19  degrees of freedom\nResidual deviance: 18.820  on 17  degrees of freedom\nAIC: 24.82\n\nNumber of Fisher Scoring iterations: 4\n\n\nThere’s no need to worry about two-column responses here, because each row of the data frame refers to only one person, and the response variable second is already a categorical variable with two categories (that needs to be turned into a factor for glm).\n\\(\\blacksquare\\)\n\nIn the previous part, how can you tell that you were predicting the probability of having a second heart attack (as opposed to the probability of not having one)?\n\nSolution\nThe levels of a factor are taken in alphabetical order, with n as the baseline, so we are predicting the probability of the second one y.\n\\(\\blacksquare\\)\n\n* For the two possible values y and n of anger and the anxiety scores 40, 50 and 60, make a data frame containing all six combinations, and use it to obtain predicted probabilities of a second heart attack. Display your predicted probabilities side by side with what they are predictions for.\n\nSolution\nThis time, I give you the values I want the predictions for (as opposed to calculating something like quartiles from the data), so we might as well just type them in.\nFirst step, datagrid:\n\nnew &lt;- datagrid(model = ha.1, anger = c(\"y\", \"n\"), anxiety = c(40, 50, 60))\nnew\n\n\n\n  \n\n\n\nThen, the predictions:\n\ncbind(predictions(ha.1, newdata = new))\n\n\n\n  \n\n\n\nExtra: In the help file for predictions, you will see it done in one step, with code like\npredictions(ha.1, newdata = datagrid(anger = c(\"y\", \"n\"), ...))\nThis saves constructing new first, but the code for datagrid is to my mind usually rather long to fit on one line inside the predictions(...).\n\\(\\blacksquare\\)\n\nUse your predictions from the previous part to describe the effect of changes in anxiety and anger on the probability of a second heart attack.\n\nSolution\nThe idea is to change one variable while leaving the other fixed. (This is the common refrain of “all else equal”.)\nPick a level of anger, say n (it doesn’t matter which) and look at the effect of anxiety. The probability of a second heart attack increases sharply from 0.17 to 0.40 to 0.69. So an increased anxiety score is associated with an increased probability of second heart attack (all else equal).\nTo assess the effect of taking the anger management course, pick an anxiety value, say 40, and compare the probabilities for anger n and y. For someone who has not taken the anger management course, the probability is 0.17, but for someone who has, it drops all the way to 0.07. (The pattern is the same at the other anxiety scores.)\nExtra: the reason it doesn’t matter what value of the other variable you look at (as long as you keep it fixed) is that the model is “additive” with no interaction, so that the effect of one variable does not depend on the value of the other one. If we wanted to see whether the effect of anxiety was different according to whether or not the person had taken the anger management course, we would add an interaction between anxiety and anger. But we won’t be doing this kind of thing until we get to analysis of variance, so you don’t need to worry about it yet.\n\\(\\blacksquare\\)\n\nAre the effects you described in the previous part consistent with the summary output from glm that you obtained in (here)? Explain briefly how they are, or are not. (You need an explanation for each of anxiety and anger, and you will probably get confused if you look at the P-values, so don’t.)\n\nSolution\nIn the previous part, we found that increased anxiety went with an increased probability of second heart attack. Back in (here), we found a positive slope of 0.11904 for anxiety, which also means that a higher anxiety score goes with a higher probability of second heart attack. That was not too hard. The other one is a little more work. anger is categorical with two categories n and y. The first one, n, is the baseline, so angery shows how y compares to n. The slope \\(-1.04211\\) is negative, which means that someone who has taken the anger management course has a lower probability of a second heart attack than someone who hasn’t (all else equal). This is the same story that we got from the predictions. That’s it for that, but I suppose I should talk about those P-values. anxiety is significant, so it definitely has an effect on the probability of a second heart attack. The pattern is clear enough even with this small data set. Here’s a visual:\n\nggplot(ha, aes(x = second, y = anxiety)) + geom_boxplot()\n\n\n\n\nThis is the wrong way around in terms of cause and effect, but it shows pretty clearly that people who suffer a second heart attack have a higher level of anxiety than those who don’t.\nThat’s clear enough, but what about anger? That is not significant, but there seems to be a visible effect of anger on the predicted probabilities of (here): there we saw that if you had done the anger management course, your probability of a second heart attack was lower. But that’s only the predicted probability, and there is also uncertainty about that, probably quite a lot because we don’t have much data. So if we were to think about confidence intervals for the predicted probabilities, they would be wide, and for the two levels of anger at a fixed anxiety they would almost certainly overlap.\nAnother way of seeing that is a visual, which would be a side-by-side bar chart:\n\nggplot(ha, aes(x = anger, fill = second)) + geom_bar(position = \"dodge\")\n\n\n\n\nA small majority of people who took the anger management did not have a second heart attack, while a small minority of those who did not, did.17 But with these small numbers, the difference, even though it points the way we would have guessed, is not large enough to be significant:\n\nwith(ha, table(anger, second))\n\n     second\nanger n y\n    n 4 7\n    y 6 3\n\n\nThis is not nearly far enough from an equal split to be significant. (It’s the same kind of idea as for Mood’s median test in C32/C33.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "logistic-regression.html#footnotes",
    "href": "logistic-regression.html#footnotes",
    "title": "24  Logistic regression",
    "section": "",
    "text": "I get confused about the difference between morals and ethics. [This is a very short description of that difference:] (http://smallbusiness.chron.com/differences-between-ethical-issues-moral-issues-business-48134.html). The basic idea is that morals are part of who you are, derived from religion, philosophy etc. Ethics are how you act in a particular situation: that is, your morals, what you believe, inform your ethics, what you do. That’s why the students had to play the role of an ethics committee, rather than a morals committee; presumably the researcher had good morals, but an ethics committee had to evaluate what he was planning to do, rather than his character as a person.↩︎\nAs with many of these acronyms, you get the idea that the acronym came first and they devised some words to fit it.↩︎\nMy daughter learned the word pre-empt because we like to play a bridge app on my phone; in the game of bridge, you make a pre-emptive bid when you have no great strength but a lot of cards of one suit, say seven, and it won’t be too bad if that suit is trumps, no matter what your partner has. If you have a weakish hand with a lot of cards in one suit, your opponents are probably going to be able to bid and make something, so you pre-emptively bid first to try and make it difficult for them.↩︎\nThis kind of thing is sometimes called an inverse prediction.↩︎\nThe usual application of this is to combine a number with a vector. If you try to subtract a length-2 vector from a length-6 vector, R will repeat the shorter one three times and do the subtraction, but if you try to subtract a length-2 vector from a length-7 vector, where you’d have to repeat the shorter one a fractional number of times, R will do it, but you’ll get a warning, because this is probably not what you wanted. Try it and see.↩︎\nThis is done in the geom_point on the website, which is where I learned about it.↩︎\nThe test is this way because it’s a generalized linear model rather than a regular regression.↩︎\nSee, I told you it was a little confusing.↩︎\nWhen you have one observation per line, the predictions are of the second of the two levels of the response variable. When you make that two-column response, the predictions are of the probability of being in the first column. That’s what it is. As the young people say, don’t @ me.↩︎\nWhen you have higher-order terms, you have to keep the lower-order ones as well: higher powers, or interactions (as we see in ANOVA later).↩︎\nI get confused about the difference between morals and ethics. [This is a very short description of that difference:] (http://smallbusiness.chron.com/differences-between-ethical-issues-moral-issues-business-48134.html). The basic idea is that morals are part of who you are, derived from religion, philosophy etc. Ethics are how you act in a particular situation: that is, your morals, what you believe, inform your ethics, what you do. That’s why the students had to play the role of an ethics committee, rather than a morals committee; presumably the researcher had good morals, but an ethics committee had to evaluate what he was planning to do, rather than his character as a person.↩︎\nIf you can believe it.↩︎\nIf you took STAB23, you’ll have used PSPP, which is a free version of SPSS.↩︎\nIf it is for you, go right ahead, but for me it wasn’t.↩︎\nI realize I am extrapolating with these values.↩︎\nAs with many of these acronyms, you get the idea that the acronym came first and they devised some words to fit it.↩︎\nRead that carefully.↩︎"
  },
  {
    "objectID": "ordinal-response.html#do-you-like-your-mobile-phone",
    "href": "ordinal-response.html#do-you-like-your-mobile-phone",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.1 Do you like your mobile phone?",
    "text": "25.1 Do you like your mobile phone?\nA phone company commissioned a survey of their customers’ satisfaction with their mobile devices. The responses to the survey were on a so-called Likert scale of “very unsatisfied”, “unsatisfied”, “satisfied”, “very satisfied”. Also recorded were each customer’s gender and age group (under 18, 18–24, 25–30, 31 or older). (A survey of this kind does not ask its respondents for their exact age, only which age group they fall in.) The data, as frequencies of people falling into each category combination, are in link.\n\n* Read in the data and take a look at the format. Use a tool that you know about to arrange the frequencies in just one column, with other columns labelling the response categories that the frequencies belong to. Save the new data frame. (Take a look at it if you like.)\nWe are going to fit ordered logistic models below. To do that, we need our response variable to be a factor with its levels in the right order. By looking at the data frame you just created, determine what kind of thing your intended response variable currently is.\nIf your intended response variable is not a factor, create a factor in your data frame with levels in the right order. Hint: look at the order your levels are in the data.\n* Fit ordered logistic models to predict satisfaction from (i) gender and age group, (ii) gender only, (iii) age group only. (You don’t need to examine the models.) Don’t forget a suitable weights!\nUse drop1 on your model containing both explanatory variables to determine whether you can remove either of them. Use test=\"Chisq\" to obtain P-values.\nUse anova to decide whether we are justified in removing gender from a model containing both gender and age.group. Compare your P-value with the one from drop1.\nUse anova to see whether we are justified in removing age.group from a model containing both gender and age.group. Compare your P-value with the one from drop1 above.\nWhich of the models you have fit so far is the most appropriate one? Explain briefly.\nObtain predicted probabilities of a customer falling in the various satisfaction categories, as it depends on gender and age group. To do that, you need to feed predict three things: the fitted model that contains both age group and gender, the data frame that you read in from the file back in part (here) (which contains all the combinations of age group and gender), and an appropriate type.\n* Describe any patterns you see in the predictions, bearing in mind the significance or not of the explanatory variables."
  },
  {
    "objectID": "ordinal-response.html#finding-non-missing-values",
    "href": "ordinal-response.html#finding-non-missing-values",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.2 Finding non-missing values",
    "text": "25.2 Finding non-missing values\n* This is to prepare you for something in the next question. It’s meant to be easy.\nIn R, the code NA stands for “missing value” or “value not known”. In R, NA should not have quotes around it. (It is a special code, not a piece of text.)\n\nCreate a vector v that contains some numbers and some missing values, using c(). Put those values into a one-column data frame.\nObtain a new column containing is.na(v). When is this true and when is this false?\nThe symbol ! means “not” in R (and other programming languages). What does !is.na(v) do? Create a new column containing that.\nUse filter to display just the rows of your data frame that have a non-missing value of v."
  },
  {
    "objectID": "ordinal-response.html#high-school-and-beyond",
    "href": "ordinal-response.html#high-school-and-beyond",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.3 High School and Beyond",
    "text": "25.3 High School and Beyond\nA survey called High School and Beyond was given to a large number of American high school seniors (grade 12) by the National Center of Education Statistics. The data set at link is a random sample of 200 of those students.\nThe variables collected are:\n\ngender: student’s gender, female or male.\nrace: the student’s race (African-American, Asian,1 Hispanic, White).\nses: Socio-economic status of student’s family (low, middle, or high)\nschtyp: School type, public or private.\nprog: Student’s program, general, academic, or vocational.\nread: Score on standardized reading test.\nwrite: Score on standardized writing test.\nmath: Score on standardized math test.\nscience: Score on standardized science test.\nsocst: Score on standardized social studies test.\n\nOur aim is to see how socio-economic status is related to the other variables.\n\nRead in and display (some of) the data.\nExplain briefly why an ordinal logistic regression is appropriate for our aims.\nFit an ordinal logistic regression predicting socio-economic status from the scores on the five standardized tests. (You don’t need to display the results.) You will probably go wrong the first time. What kind of thing does your response variable have to be?\nRemove any non-significant explanatory variables one at a time. Use drop1 to decide which one to remove next.\nThe quartiles of the science test score are 44 and 58. The quartiles of the socst test score are 46 and 61. Make a data frame that has all combinations of those quartiles. If your best regression had any other explanatory variables in it, also put the means of those variables into this data frame.\nUse the data frame you created in the previous part, together with your best model, to obtain predicted probabilities of being in each ses category. Display these predicted probabilities so that they are easy to read.\nWhat is the effect of an increased science score on the likelihood of a student being in the different socioeconomic groups, all else equal? Explain briefly. In your explanation, state clearly how you are using your answer to the previous part."
  },
  {
    "objectID": "ordinal-response.html#how-do-you-like-your-steak",
    "href": "ordinal-response.html#how-do-you-like-your-steak",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.4 How do you like your steak?",
    "text": "25.4 How do you like your steak?\nWhen you order a steak in a restaurant, the server will ask you how you would like it cooked, or to be precise, how much you would like it cooked: rare (hardly cooked at all), through medium rare, medium, medium well to well (which means “well done”, so that the meat has only a little red to it). Could you guess how a person likes their steak cooked, from some other information about them? The website link commissioned a survey where they asked a number of people how they preferred their steak, along with as many other things as they could think of to ask. (Many of the variables below are related to risk-taking, which was something the people designing the survey thought might have something to do with liking steak rare.) The variables of interest are all factors or true/false:\n\nrespondent_ID: a ten-digit number identifying each person who responded to the survey.\nlottery_a: true if the respondent preferred lottery A with a small chance to win a lot of money, to lottery B, with a larger chance to win less money.\nsmoke: true if the respondent is currently a smoker\nalcohol: true if the respondent at least occasionally drinks alcohol.\ngamble: true if the respondent likes to gamble (eg. betting on horse racing or playing the lottery)\nskydiving: true if the respondent has ever been skydiving.\nspeed: true if the respondent likes to drive fast\ncheated: true if the respondent has ever cheated on a spouse or girlfriend/boyfriend\nsteak: true if the respondent likes to eat steak\nsteak_prep (response): how the respondent likes their steak cooked (factor, as described above, with 5 levels).\nfemale: true if the respondent is female\nage: age group, from 18–29 to 60+.\nhhold_income: household income group, from $0–24,999 to $150,000+.\neduc: highest level of education attained, from “less than high school” up to “graduate degree”\nregion: region (of the US) that the respondent lives in (five values).\n\nThe data are in link. This is the cleaned-up data from a previous question, with the missing values removed.\n\nRead in the data and display the first few lines.\nWe are going to predict steak_prep from some of the other variables. Why is the model-fitting function polr from package MASS the best choice for these data (alternatives being glm and multinom from package nnet)?\nWhat are the levels of steak_prep, in the order that R thinks they are in? If they are not in a sensible order, create an ordered factor where the levels are in a sensible order.\nFit a model predicting preferred steak preparation in an ordinal logistic regression from educ, female and lottery_a. This ought to be easy from your previous work, but you have to be careful about one thing. No need to print out the results.\nRun drop1 on your fitted model, with test=\"Chisq\". Which explanatory variable should be removed first, if any? Bear in mind that the variable with the smallest AIC should come out first, in case your table doesn’t get printed in order.\nRemove the variable that should come out first, using update. (If all the variables should stay, you can skip this part.)\nUsing the best model that you have so far, predict the probabilities of preferring each different steak preparation (method of cooking) for each combination of the variables that remain. (Some of the variables are TRUE and FALSE rather than factors. Bear this in mind.) Describe the effects of each variable on the predicted probabilities, if any. Note that there is exactly one person in the study whose educational level is “less than high school”.\nIs it reasonable to remove all the remaining explanatory variables from your best model so far? Fit a model with no explanatory variables, and do a test. (In R, if the right side of the squiggle is a 1, that means “just an intercept”. Or you can remove whatever remains using update.) What do you conclude? Explain briefly.\nIn the article for which these data were collected, link, does the author obtain consistent conclusions with yours? Explain briefly. (It’s not a very long article, so it won’t take you long to skim through, and the author’s point is pretty clear.)"
  },
  {
    "objectID": "ordinal-response.html#how-do-you-like-your-steak-the-data",
    "href": "ordinal-response.html#how-do-you-like-your-steak-the-data",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.5 How do you like your steak – the data",
    "text": "25.5 How do you like your steak – the data\n This question takes you through the data preparation for one of the other questions. You don’t have to do this* question, but you may find it interesting or useful.\nWhen you order a steak in a restaurant, the server will ask you how you would like it cooked, or to be precise, how much you would like it cooked: rare (hardly cooked at all), through medium rare, medium, medium well to well (which means “well done”, so that the meat has only a little red to it). Could you guess how a person likes their steak cooked, from some other information about them? The website link commissioned a survey where they asked a number of people how they preferred their steak, along with as many other things as they could think of to ask. (Many of the variables below are related to risk-taking, which was something the people designing the survey thought might have something to do with liking steak rare.) The variables of interest are all factors or true/false:\n\nrespondent_ID: a ten-digit number identifying each person who responded to the survey.\nlottery_a: true if the respondent preferred lottery A with a small chance to win a lot of money, to lottery B, with a larger chance to win less money.\nsmoke: true if the respondent is currently a smoker\nalcohol: true if the respondent at least occasionally drinks alcohol.\ngamble: true if the respondent likes to gamble (eg. betting on horse racing or playing the lottery)\nskydiving: true if the respondent has ever been skydiving.\nspeed: true if the respondent likes to drive fast\ncheated: true if the respondent has ever cheated on a spouse or girlfriend/boyfriend\nsteak: true if the respondent likes to eat steak\nsteak_prep (response): how the respondent likes their steak cooked (factor, as described above, with 5 levels).\nfemale: true if the respondent is female\nage: age group, from 18–29 to 60+.\nhhold_income: household income group, from $0–24,999 to $150,000+.\neduc: highest level of education attained, from “less than high school” up to “graduate degree”\nregion: region (of the US) that the respondent lives in (five values).\n\nThe data are in link.\n\nRead in the data and display the first few lines.\nWhat do you immediately notice about your data frame? Run summary on the entire data frame. Would you say you have a lot of missing values, or only a few?\nWhat does the function drop_na do when applied to a data frame with missing values? To find out, pass the data frame into drop_na(), then into summary again. What has happened?\nWrite the data into a .csv file, with a name like steak1.csv. Open this file in a spreadsheet and (quickly) verify that you have the right columns and no missing values.\n\nMy solutions follow:"
  },
  {
    "objectID": "ordinal-response.html#do-you-like-your-mobile-phone-1",
    "href": "ordinal-response.html#do-you-like-your-mobile-phone-1",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.6 Do you like your mobile phone?",
    "text": "25.6 Do you like your mobile phone?\nA phone company commissioned a survey of their customers’ satisfaction with their mobile devices. The responses to the survey were on a so-called Likert scale of “very unsatisfied”, “unsatisfied”, “satisfied”, “very satisfied”. Also recorded were each customer’s gender and age group (under 18, 18–24, 25–30, 31 or older). (A survey of this kind does not ask its respondents for their exact age, only which age group they fall in.) The data, as frequencies of people falling into each category combination, are in link.\n\n* Read in the data and take a look at the format. Use a tool that you know about to arrange the frequencies in just one column, with other columns labelling the response categories that the frequencies belong to. Save the new data frame. (Take a look at it if you like.)\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/mobile.txt\"\nmobile &lt;- read_delim(my_url, \" \")\n\nRows: 8 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): gender, age.group\ndbl (4): very.unsat, unsat, sat, very.sat\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmobile\n\n\n\n  \n\n\n\nWith multiple columns that are all frequencies, this is a job for pivot_longer:\n\nmobile %&gt;% \n  pivot_longer(very.unsat:very.sat, \n               names_to=\"satisfied\", \n               values_to=\"frequency\") -&gt; mobile.long\nmobile.long\n\n\n\n  \n\n\n\nYep, all good. See how mobile.long contains what it should? (For those keeping track, the original data frame had 8 rows and 4 columns to collect up, and the new one has \\(8\\times 4=32\\) rows.)\n\\(\\blacksquare\\)\n\nWe are going to fit ordered logistic models below. To do that, we need our response variable to be a factor with its levels in the right order. By looking at the data frame you just created, determine what kind of thing your intended response variable currently is.\n\nSolution\nI looked at mobile.long in the previous part, but if you didn’t, look at it here:\n\nmobile.long\n\n\n\n  \n\n\n\nMy intended response variable is what I called satisfied. This is chr or “text”, not the factor that I want.\n\\(\\blacksquare\\)\n\nIf your intended response variable is not a factor, create a factor in your data frame with levels in the right order. Hint: look at the order your levels are in the data.\n\nSolution\nMy intended response satisfied is text, not a factor, so I need to do this part. The hint is to look at the column satisfied in mobile.long and note that the satisfaction categories appear in the data in the order that we want. This is good news, because we can use fct_inorder like this:\n\nmobile.long %&gt;%\n  mutate(satis = fct_inorder(satisfied)) -&gt; mobile.long\n\nIf you check, by looking at the data frame, satis is a factor, and you can also do this to verify that its levels are in the right order:\n\nwith(mobile.long, levels(satis))\n\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n\n\nSuccess.\nExtra: so now you are asking, what if the levels are in the wrong order in the data? Well, below is what you used to have to do, and it will work for this as well. I’ll first find what levels of satisfaction I have. This can be done by counting them, or by finding the distinct ones:\n\nmobile.long %&gt;% count(satisfied)\n\n\n\n  \n\n\n\nor\n\nmobile.long %&gt;% distinct(satisfied)\n\n\n\n  \n\n\n\nIf you count them, they come out in alphabetical order. If you ask for the distinct ones, they come out in the order they were in mobile.long, which is the order the columns of those names were in mobile, which is the order we want.\nTo actually grab those satisfaction levels as a vector (that we will need in a minute), use pluck to pull the column out of the data frame as a vector:\n\nv1 &lt;- mobile.long %&gt;%\n  distinct(satisfied) %&gt;%\n  pluck(\"satisfied\")\nv1\n\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n\n\nwhich is in the correct order, or\n\nv2 &lt;- mobile.long %&gt;%\n  count(satisfied) %&gt;%\n  pluck(\"satisfied\")\nv2\n\n[1] \"sat\"        \"unsat\"      \"very.sat\"   \"very.unsat\"\n\n\nwhich is in alphabetical order. The problem with the second one is that we know the correct order, but there isn’t a good way to code that, so we have to rearrange it ourselves. The correct order from v2 is 4, 2, 1, 3, so:\n\nv3 &lt;- c(v2[4], v2[2], v2[1], v2[3])\nv3\n\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n\nv4 &lt;- v2[c(4, 2, 1, 3)]\nv4\n\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n\n\nEither of these will work. The first one is more typing, but is perhaps more obvious. There is a third way, which is to keep things as a data frame until the end, and use slice to pick out the rows in the right order:\n\nv5 &lt;- mobile.long %&gt;%\n  count(satisfied) %&gt;%\n  slice(c(4, 2, 1, 3)) %&gt;%\n  pluck(\"satisfied\")\nv5\n\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n\n\nIf you don’t see how that works, run it yourself, one line at a time.\nThe other way of doing this is to physically type them into a vector, but this carries the usual warnings of requiring you to be very careful and that it won’t be reproducible (eg. if you do another survey with different response categories).\nSo now create the proper response variable thus, using your vector of categories:\n\nmobile.long %&gt;%\n  mutate(satis = ordered(satisfied, v1)) -&gt; mobile.long2\nmobile.long2\n\n\n\n  \n\n\n\nsatis has the same values as satisfied, but its label ord means that it is an ordered factor, as we want.\n\\(\\blacksquare\\)\n\n* Fit ordered logistic models to predict satisfaction from (i) gender and age group, (ii) gender only, (iii) age group only. (You don’t need to examine the models.) Don’t forget a suitable weights!\n\nSolution\n(i):\n\nlibrary(MASS)\nmobile.1 &lt;- polr(satis ~ gender + age.group, weights = frequency, data = mobile.long)\n\nFor (ii) and (iii), update is the thing (it works for any kind of model):\n\nmobile.2 &lt;- update(mobile.1, . ~ . - age.group)\nmobile.3 &lt;- update(mobile.1, . ~ . - gender)\n\nWe’re not going to look at these, because the output from summary is not very illuminating. What we do next is to try to figure out which (if either) of the explanatory variables age.group and gender we need.\n\\(\\blacksquare\\)\n\nUse drop1 on your model containing both explanatory variables to determine whether you can remove either of them. Use test=\"Chisq\" to obtain P-values.\n\nSolution\ndrop1 takes a fitted model, and tests each term in it in turn, and says which (if any) should be removed. Here’s how it goes:\n\ndrop1(mobile.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe possibilities are to remove gender, to remove age.group or to remove nothing. The best one is “remove nothing”, because it’s the one on the output with the smallest AIC. Both P-values are small, so it would be a mistake to remove either of the explanatory variables.\n\\(\\blacksquare\\)\n\nUse anova to decide whether we are justified in removing gender from a model containing both gender and age.group. Compare your P-value with the one from drop1.\n\nSolution\nThis is a comparison of the model with both variables (mobile.1) and the model with gender removed (mobile.3). Use anova for this, smaller (fewer-\\(x\\)) model first:\n\nanova(mobile.3, mobile.1)\n\n\n\n  \n\n\n\nThe P-value is (just) less than 0.05, so the models are significantly different. That means that the model with both variables in fits significantly better than the model with only age.group, and therefore that taking gender out is a mistake.\nThe P-value is identical to the one from drop1 (because they are both doing the same test).\n\\(\\blacksquare\\)\n\nUse anova to see whether we are justified in removing age.group from a model containing both gender and age.group. Compare your P-value with the one from drop1 above.\n\nSolution\nExactly the same idea as the last part. In my case, I’m comparing models mobile.2 and mobile.1:\n\nanova(mobile.2, mobile.1)\n\n\n\n  \n\n\n\nThis one is definitely significant, so I need to keep age.group for sure. Again, the P-value is the same as the one in drop1.\n\\(\\blacksquare\\)\n\nWhich of the models you have fit so far is the most appropriate one? Explain briefly.\n\nSolution\nI can’t drop either of my variables, so I have to keep them both: mobile.1, with both age.group and gender.\n\\(\\blacksquare\\)\n\nObtain predicted probabilities of a customer falling in the various satisfaction categories, as it depends on gender and age group. To do that, you need to feed predict three things: the fitted model that contains both age group and gender, the data frame that you read in from the file back in part (here) (which contains all the combinations of age group and gender), and an appropriate type.\n\nSolution\nxxx use predictions\nMy model containing both \\(x\\)s was mobile.1, the data frame read in from the file was called mobile, and I need type=\"p\" to get probabilities:\n\nprobs &lt;- predict(mobile.1, mobile, type = \"p\")\nmobile %&gt;%\n  select(gender, age.group) %&gt;%\n  cbind(probs)\n\n\n\n  \n\n\n\nThis worked for me, but this might happen to you, with the same commands as above:\n\n\nError in MASS::select(., gender, age.group): unused arguments (gender, age.group)\n\n\nOh, this didn’t work. Why not? There don’t seem to be any errors.\nThis is the kind of thing that can bother you for days. The resolution (that it took me a long time to discover) is that you might have the tidyverse and also MASS loaded, in the wrong order, and MASS also has a select (that takes different inputs and does something different). If you look back at part (here), you might have seen a message there when you loaded MASS that select was “masked”. When you have two packages that both contain a function with the same name, the one that you can see (and that will get used) is the one that was loaded last, which is the MASS select (not the one we actually wanted, which is the tidyverse select). There are a couple of ways around this. One is to un-load the package we no longer need (when we no longer need it). The mechanism for this is shown at the end of part (here). The other is to say explicitly which package you want your function to come from, so that there is no doubt. The tidyverse is actually a collection of packages. The best way to find out which one our select comes from is to go to the Console window in R Studio and ask for the help for select. With both tidyverse and MASS loaded, the help window offers you a choice of both selects; the one we want is “select/rename variables by name”, and the actual package it comes from is dplyr.\nThere is a third choice, which is the one I prefer now: install and load the package conflicted. When you run your code and it calls for something like select that is in two packages that you have loaded, it gives an error, like this:\nError: [conflicted] `select` found in 2 packages.\nEither pick the one you want with `::` \n* MASS::select\n* dplyr::select\nOr declare a preference with `conflict_prefer()`\n* conflict_prefer(\"select\", \"MASS\")\n* conflict_prefer(\"select\", \"dplyr\")\nFixing this costs you a bit of time upfront, but once you have fixed it, you know that the right thing is being run. What I do is to copy-paste one of those conflict_prefer lines, in this case the second one, and put it before the select that now causes the error. Right after the library(conflicted) is a good place. When you use conflicted, you will probably have to run several times to fix up all the conflicts, which will be a bit frustrating, and you will end up with several conflict_prefer lines, but once you have them there, you won’t have to worry about the right function being called because you have explicitly said which one you want.\nThis is a non-standard use of cbind because I wanted to grab only the gender and age group columns from mobile first, and then cbind that to the predicted probabilities. The missing first input to cbind is “whatever came out of the previous step”, that is, the first two columns of mobile.\nI only included the first two columns of mobile in the cbind, because the rest of the columns of mobile were frequencies, which I don’t need to see. (Having said that, it would be interesting to make a plot using the observed proportions and predicted probabilities, but I didn’t ask you for that.)\nThis is an unusual predict, because we didn’t have to make a data frame (with my usual name new) containing all the combinations of things to predict for. We were lucky enough to have those already in the original data frame mobile.\nThe usual way to do this is something like the trick that we did for getting the different satisfaction levels:\n\ngenders &lt;- mobile.long %&gt;% distinct(gender) %&gt;% pluck(\"gender\")\nage.groups &lt;- mobile.long %&gt;%\n  distinct(age.group) %&gt;%\n  pluck(\"age.group\")\n\nThis is getting perilously close to deserving a function written to do it (strictly speaking, we should, since this is three times we’ve used this idea now).\nThen crossing to get the combinations, and then predict:\n\nnew &lt;- crossing(gender = genders, age.group = age.groups)\nnew\n\n\n\n  \n\n\npp &lt;- predict(mobile.1, new, type = \"p\")\ncbind(new, pp)\n\n\n\n  \n\n\n\nThis method is fine if you want to do this question this way; the way I suggested first ought to be easier, but the nice thing about this is its mindlessness: you always do it about the same way.\n\\(\\blacksquare\\)\n\n* Describe any patterns you see in the predictions, bearing in mind the significance or not of the explanatory variables.\n\nSolution\nI had both explanatory variables being significant, so I would expect to see both an age-group effect and a gender effect. For both males and females, there seems to be a decrease in satisfaction as the customers get older, at least until age 30 or so. I can see this because the predicted prob. of “very satisfied” decreases, and the predicted prob. of “very unsatisfied” increases. The 31+ age group are very similar to the 25–30 group for both males and females. So that’s the age group effect. What about a gender effect? Well, for all the age groups, the males are more likely to be very satisfied than the females of the corresponding age group, and also less likely to to be very unsatisfied. So the gender effect is that males are more satisfied than females overall. (Or, the males are less discerning. Take your pick.) When we did the tests above, age group was very definitely significant, and gender less so (P-value around 0.03). This suggests that the effect of age group ought to be large, and the effect of gender not so large. This is about what we observed: the age group effect was pretty clear, and the gender effect was noticeable but small: the females were less satisfied than the males, but there wasn’t all that much difference.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ordinal-response.html#finding-non-missing-values-1",
    "href": "ordinal-response.html#finding-non-missing-values-1",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.7 Finding non-missing values",
    "text": "25.7 Finding non-missing values\n* This is to prepare you for something in the next question. It’s meant to be easy.\nIn R, the code NA stands for “missing value” or “value not known”. In R, NA should not have quotes around it. (It is a special code, not a piece of text.)\n\nCreate a vector v that contains some numbers and some missing values, using c(). Put those values into a one-column data frame.\n\nSolution\nLike this. The arrangement of numbers and missing values doesn’t matter, as long as you have some of each:\n\nv &lt;- c(1, 2, NA, 4, 5, 6, 9, NA, 11)\nmydata &lt;- tibble(v)\nmydata\n\n\n\n  \n\n\n\nThis has one column called v.\n\\(\\blacksquare\\)\n\nObtain a new column containing is.na(v). When is this true and when is this false?\n\nSolution\n\nmydata &lt;- mydata %&gt;% mutate(isna = is.na(v))\nmydata\n\n\n\n  \n\n\n\nThis is TRUE if the corresponding element of v is missing (in my case, the third value and the second-last one), and FALSE otherwise (when there is an actual value there).\n\\(\\blacksquare\\)\n\nThe symbol ! means “not” in R (and other programming languages). What does !is.na(v) do? Create a new column containing that.\n\nSolution\nTry it and see. Give it whatever name you like. My name reflects that I know what it’s going to do:\n\nmydata &lt;- mydata %&gt;% mutate(notisna = !is.na(v))\nmydata\n\n\n\n  \n\n\n\nThis is the logical opposite of is.na: it’s true if there is a value, and false if it’s missing.\n\\(\\blacksquare\\)\n\nUse filter to display just the rows of your data frame that have a non-missing value of v.\n\nSolution\nfilter takes a column to say which rows to pick, in which case the column should contain something that either is TRUE or FALSE, or something that can be interpreted that way:\n\nmydata %&gt;% filter(notisna)\n\n\n\n  \n\n\n\nor you can provide filter something that can be calculated from what’s in the data frame, and also returns something that is either true or false:\n\nmydata %&gt;% filter(!is.na(v))\n\n\n\n  \n\n\n\nIn either case, I only have non-missing values of v.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ordinal-response.html#high-school-and-beyond-1",
    "href": "ordinal-response.html#high-school-and-beyond-1",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.8 High School and Beyond",
    "text": "25.8 High School and Beyond\nA survey called High School and Beyond was given to a large number of American high school seniors (grade 12) by the National Center of Education Statistics. The data set at link is a random sample of 200 of those students.\nThe variables collected are:\n\ngender: student’s gender, female or male.\nrace: the student’s race (African-American, Asian,2 Hispanic, White).\nses: Socio-economic status of student’s family (low, middle, or high)\nschtyp: School type, public or private.\nprog: Student’s program, general, academic, or vocational.\nread: Score on standardized reading test.\nwrite: Score on standardized writing test.\nmath: Score on standardized math test.\nscience: Score on standardized science test.\nsocst: Score on standardized social studies test.\n\nOur aim is to see how socio-economic status is related to the other variables.\n\nRead in and display (some of) the data.\n\nSolution\nThis is a .csv file (I tried to make it easy for you):\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/hsb.csv\"\nhsb &lt;- read_csv(my_url)\n\nRows: 200 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): race, ses, schtyp, prog, gender\ndbl (6): id, read, write, math, science, socst\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhsb\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nExplain briefly why an ordinal logistic regression is appropriate for our aims.\n\nSolution\nThe response variable ses is categorical, with categories that come in order (low less than middle less than high).\n\\(\\blacksquare\\)\n\nFit an ordinal logistic regression predicting socio-economic status from the scores on the five standardized tests. (You don’t need to display the results.) You will probably go wrong the first time. What kind of thing does your response variable have to be?\n\nSolution\nIt has to be an ordered factor, which you can create in the data frame (or outside, if you prefer):\n\nhsb &lt;- hsb %&gt;% mutate(ses = ordered(ses, c(\"low\", \"middle\", \"high\")))\nhsb\n\n\n\n  \n\n\n\nses is now ord. Good. Now fit the model:\n\nses.1 &lt;- polr(ses ~ read + write + math + science + socst, data = hsb)\n\nNo errors is good.\n\\(\\blacksquare\\)\n\nRemove any non-significant explanatory variables one at a time. Use drop1 to decide which one to remove next.\n\nSolution\n\ndrop1(ses.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nI would have expected the AIC column to come out in order, but it doesn’t. Never mind. Scan for the largest P-value, which belongs to read. (This also has the lowest AIC.) So, remove read:\n\nses.2 &lt;- update(ses.1, . ~ . - read)\ndrop1(ses.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nNote how the P-value for science has come down a long way.\nA close call, but math goes next. The update doesn’t take long to type:\n\nses.3 &lt;- update(ses.2, . ~ . - math)\ndrop1(ses.3, test = \"Chisq\")\n\n\n\n  \n\n\n\nscience has become significant now (probably because it was strongly correlated with at least one of the variables we removed (at my guess, math). That is, we didn’t need both science and math, but we do need one of them.\nI think we can guess what will happen now: write comes out, and the other two variables will stay, so that’ll be where we stop:\n\nses.4 &lt;- update(ses.3, . ~ . - write)\ndrop1(ses.4, test = \"Chisq\")\n\n\n\n  \n\n\n\nIndeed so. We need just the science and social studies test scores to predict socio-economic status.\nUsing AIC to decide on which variable to remove next will give the same answer here, but I would like to see the test= part in your drop1 to give P-values (expect to lose something, but not too much, if that’s not there).\nExtras: I talked about correlation among the explanatory variables earlier, which I can explore:\n\nhsb %&gt;% select(read:socst) %&gt;% cor()\n\n             read     write      math   science     socst\nread    1.0000000 0.5967765 0.6622801 0.6301579 0.6214843\nwrite   0.5967765 1.0000000 0.6174493 0.5704416 0.6047932\nmath    0.6622801 0.6174493 1.0000000 0.6307332 0.5444803\nscience 0.6301579 0.5704416 0.6307332 1.0000000 0.4651060\nsocst   0.6214843 0.6047932 0.5444803 0.4651060 1.0000000\n\n\nThe first time I did this, I forgot that I had MASS loaded (for the polr), and so, to get the right select, I needed to say which one I wanted.\nAnyway, the correlations are all moderately high. There’s nothing that stands out as being much higher than the others. The lowest two are between social studies and math, and social studies and science. That would be part of the reason that social studies needs to stay. The highest correlation is between math and reading, which surprises me (they seem to be different skills).\nSo there was not as much insight there as I expected.\nThe other thing is that you can use step for the variable-elimination task as well:\n\nses.5 &lt;- step(ses.1, direction = \"backward\", test = \"Chisq\")\n\nStart:  AIC=404.63\nses ~ read + write + math + science + socst\n\n          Df    AIC    LRT Pr(&gt;Chi)   \n- read     1 403.09 0.4620 0.496684   \n- math     1 403.19 0.5618 0.453517   \n- write    1 403.81 1.1859 0.276167   \n&lt;none&gt;       404.63                   \n- science  1 404.89 2.2630 0.132499   \n- socst    1 410.08 7.4484 0.006349 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=403.09\nses ~ write + math + science + socst\n\n          Df    AIC    LRT Pr(&gt;Chi)   \n- math     1 402.04 0.9541 0.328689   \n- write    1 402.10 1.0124 0.314325   \n&lt;none&gt;       403.09                   \n- science  1 404.29 3.1968 0.073782 . \n- socst    1 410.58 9.4856 0.002071 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=402.04\nses ~ write + science + socst\n\n          Df    AIC     LRT  Pr(&gt;Chi)    \n- write    1 400.60  0.5587 0.4547813    \n&lt;none&gt;       402.04                      \n- science  1 405.41  5.3680 0.0205095 *  \n- socst    1 411.07 11.0235 0.0008997 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=400.6\nses ~ science + socst\n\n          Df    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;       400.60                      \n- science  1 403.45  4.8511 0.0276291 *  \n- socst    1 409.74 11.1412 0.0008443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI would accept you doing it this way, again as long as you have the test= there as well.\n\\(\\blacksquare\\)\n\nThe quartiles of the science test score are 44 and 58. The quartiles of the socst test score are 46 and 61. Make a data frame that has all combinations of those quartiles. If your best regression had any other explanatory variables in it, also put the means of those variables into this data frame.\n\nSolution\nThis is what datagrid does by default (from package marginaleffects):\n\nnew &lt;- datagrid(model = ses.5, science = c(44, 58), socst = c(46, 61))\nnew\n\n\n\n  \n\n\n\nThis explicitly fills in mean values or most frequent categories for all the other variables in the dataset, even though those other variables are not in the model. The two variables you actually care about are over on the right.\nSince there are only two variables left, this new data frame has only \\(2^2=4\\) rows.\nThere is a veiled hint here that these are the two variables that should have remained in your regression. If that was not what you got, the means of the other variables in the model will go automatically into your new:\n\ndatagrid(model = ses.1, science = c(44, 58), socst = c(46, 61))\n\n\n\n  \n\n\n\nso you don’t have to do anything extra.\n\\(\\blacksquare\\)\n\nUse the data frame you created in the previous part, together with your best model, to obtain predicted probabilities of being in each ses category. Display these predicted probabilities so that they are easy to read.\n\nSolution\nThis is predictions, and we’ve done the setup. My best model was called ses.4.\n\ncbind(predictions(ses.4, newdata = new)) %&gt;% \n  select(group, estimate, science, socst) \n\n\nRe-fitting to get Hessian\n\n\n\n\n  \n\n\n\npredictions always works by having one column of predictions. That isn’t the best layout here, though; we want to see the three predicted probabilities for a particular value of science and socst all in one row, which means pivoting-wider:\n\ncbind(predictions(ses.4, newdata = new)) %&gt;% \n  select(group, estimate, science, socst) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\nRe-fitting to get Hessian\n\n\n\n\n  \n\n\n\nThe easiest strategy seems to be to run predictions first, see that it comes out long, and then wonder how to fix it. Then pick the columns you care about: the predicted group, the predictions, and the columns for science and social science, and then pivot wider.\n\\(\\blacksquare\\)\n\nWhat is the effect of an increased science score on the likelihood of a student being in the different socioeconomic groups, all else equal? Explain briefly. In your explanation, state clearly how you are using your answer to the previous part.\n\nSolution\nUse your predictions; hold the socst score constant (that’s the all else equal part). So compare the first and third rows (or, if you like, the second and fourth rows) of your predictions and see what happens as the science score goes from 44 to 58. What I see is that the probability of being low goes noticeably down as the science score increases, the probability of middle stays about the same, and the probability of high goes up (by about the same amount as the probability of low went down). In other words, an increased science score goes with an increased chance of high (and a decreased chance of low).\nIf your best model doesn’t have science in it, then you need to say something like “science has no effect on socio-economic status”, consistent with what you concluded before: if you took it out, it’s because you thought it had no effect.\nExtra: the effect of an increased social studies score is almost exactly the same as an increased science score (so I didn’t ask you about that). From a social-science point of view, this makes perfect sense: the higher the social-economic stratum a student comes from, the better they are likely to do in school. I’ve been phrasing this as “association”, because really the cause and effect is the other way around: a student’s family socioeconomic status is explanatory, and school performance is response. But this was the nicest example I could find of an ordinal response data set.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ordinal-response.html#how-do-you-like-your-steak-1",
    "href": "ordinal-response.html#how-do-you-like-your-steak-1",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.9 How do you like your steak?",
    "text": "25.9 How do you like your steak?\nWhen you order a steak in a restaurant, the server will ask you how you would like it cooked, or to be precise, how much you would like it cooked: rare (hardly cooked at all), through medium rare, medium, medium well to well (which means “well done”, so that the meat has only a little red to it). Could you guess how a person likes their steak cooked, from some other information about them? The website link commissioned a survey where they asked a number of people how they preferred their steak, along with as many other things as they could think of to ask. (Many of the variables below are related to risk-taking, which was something the people designing the survey thought might have something to do with liking steak rare.) The variables of interest are all factors or true/false:\n\nrespondent_ID: a ten-digit number identifying each person who responded to the survey.\nlottery_a: true if the respondent preferred lottery A with a small chance to win a lot of money, to lottery B, with a larger chance to win less money.\nsmoke: true if the respondent is currently a smoker\nalcohol: true if the respondent at least occasionally drinks alcohol.\ngamble: true if the respondent likes to gamble (eg. betting on horse racing or playing the lottery)\nskydiving: true if the respondent has ever been skydiving.\nspeed: true if the respondent likes to drive fast\ncheated: true if the respondent has ever cheated on a spouse or girlfriend/boyfriend\nsteak: true if the respondent likes to eat steak\nsteak_prep (response): how the respondent likes their steak cooked (factor, as described above, with 5 levels).\nfemale: true if the respondent is female\nage: age group, from 18–29 to 60+.\nhhold_income: household income group, from $0–24,999 to $150,000+.\neduc: highest level of education attained, from “less than high school” up to “graduate degree”\nregion: region (of the US) that the respondent lives in (five values).\n\nThe data are in link. This is the cleaned-up data from a previous question, with the missing values removed.\n\nRead in the data and display the first few lines.\n\nSolution\nThe usual:\n\nsteak &lt;- read_csv(\"steak1.csv\")\n\nRows: 331 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): steak_prep, age, hhold_income, educ, region\ndbl (1): respondent_id\nlgl (9): lottery_a, smoke, alcohol, gamble, skydiving, speed, cheated, steak...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsteak\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nWe are going to predict steak_prep from some of the other variables. Why is the model-fitting function polr from package MASS the best choice for these data (alternatives being glm and multinom from package nnet)?\n\nSolution\nIt all depends on the kind of response variable. We have a response variable with five ordered levels from Rare to Well. There are more than two levels (it is more than a “success” and “failure”), which rules out glm, and the levels are ordered, which rules out multinom. As we know, polr handles an ordered response, so it is the right choice.\n\\(\\blacksquare\\)\n\nWhat are the levels of steak_prep, in the order that R thinks they are in? If they are not in a sensible order, create an ordered factor where the levels are in a sensible order.\n\nSolution\nThis is the most direct way to find out:\n\nsteak %&gt;% distinct(steak_prep) %&gt;% pull(steak_prep) -&gt; preps\npreps\n\n[1] \"Medium rare\" \"Rare\"        \"Medium\"      \"Medium Well\" \"Well\"       \n\n\nThis is almost the right order (distinct uses the order in the data frame). We just need to switch the first two around, and then we’ll be done:\n\npreps1 &lt;- preps[c(2, 1, 3, 4, 5)]\npreps1\n\n[1] \"Rare\"        \"Medium rare\" \"Medium\"      \"Medium Well\" \"Well\"       \n\n\nIf you used count, there’s a bit more work to do:\n\npreps2 &lt;- steak %&gt;% count(steak_prep) %&gt;% pull(steak_prep)\npreps2\n\n[1] \"Medium\"      \"Medium Well\" \"Medium rare\" \"Rare\"        \"Well\"       \n\n\nbecause count puts them in alphabetical order, so:\n\npreps3 &lt;- preps2[c(4, 2, 1, 3, 5)]\npreps3\n\n[1] \"Rare\"        \"Medium Well\" \"Medium\"      \"Medium rare\" \"Well\"       \n\n\nThese use the idea in the attitudes-to-abortion question: create a vector of the levels in the right order, then create an ordered factor with ordered(). If you like, you can type the levels in the right order (I won’t penalize you for that here), but it’s really better to get the levels without typing or copying-pasting, so that you don’t make any silly errors copying them (which will mess everything up later).\nSo now I create my ordered response:\n\nsteak &lt;- steak %&gt;% mutate(steak_prep_ord = ordered(steak_prep, preps1))\n\nor using one of the other preps vectors containing the levels in the correct order. As far as polr is concerned, it doesn’t matter whether I start at Rare and go “up”, or start at Well and go “down”. So if you do it the other way around, that’s fine. As long as you get the levels in a sensible order, you’re good.\n\\(\\blacksquare\\)\n\nFit a model predicting preferred steak preparation in an ordinal logistic regression from educ, female and lottery_a. This ought to be easy from your previous work, but you have to be careful about one thing. No need to print out the results.\n\nSolution\nThe thing you have to be careful about is that you use the ordered factor that you just created as the response:\n\nsteak.1 &lt;- polr(steak_prep_ord ~ educ + female + lottery_a, data = steak)\n\n\\(\\blacksquare\\)\n\nRun drop1 on your fitted model, with test=\"Chisq\". Which explanatory variable should be removed first, if any? Bear in mind that the variable with the smallest AIC should come out first, in case your table doesn’t get printed in order.\n\nSolution\nThis:\n\ndrop1(steak.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nMy table is indeed out of order (which is why I warned you about it, in case that happens to you as well). The smallest AIC goes with female, which also has a very non-significant P-value, so this one should come out first.\n\\(\\blacksquare\\)\n\nRemove the variable that should come out first, using update. (If all the variables should stay, you can skip this part.)\n\nSolution\nYou could type or copy-paste the whole model again, but update is quicker:\n\nsteak.2 &lt;- update(steak.1, . ~ . - female)\n\nThat’s all.\nI wanted to get some support for my drop1 above (since I was a bit worried about those out-of-order rows). Now that we have fitted a model with female and one without, we can compare them using anova:\n\nanova(steak.2, steak.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nDon’t get taken in by that “LR stat” that may be on the end of the first row of the output table; the P-value might have wrapped onto the second line, and is in fact exactly the same as in the drop1 output (it is doing exactly the same test). As non-significant as you could wish for.\nExtra: I was curious about whether either of the other \\(x\\)’s could come out now:\n\ndrop1(steak.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nlottery_a should come out, but educ is edging towards significance. We are about to do predictions; in those, the above suggests that there may be some visible effect of education, but there may not be much effect of lottery_a.\nAll right, so what happens when we remove lottery_a? That we find out later.\n\\(\\blacksquare\\)\n\nUsing the best model that you have so far, predict the probabilities of preferring each different steak preparation (method of cooking) for each combination of the variables that remain. (Some of the variables are TRUE and FALSE rather than factors. Bear this in mind.) Describe the effects of each variable on the predicted probabilities, if any. Note that there is exactly one person in the study whose educational level is “less than high school”.\n\nSolution\nAgain, I’m leaving it to you to follow all the steps. My variables remaining are educ and lottery_a, which are respectively categorical and logical.\nThe first step is to get all combinations of their values, along with “typical” values for the others:\n\nnew &lt;- datagrid(model = steak.2, \n                educ = levels(factor(steak$educ)),\n                lottery_a = c(FALSE, TRUE))\nnew\n\n\n\n  \n\n\n\nI wasn’t sure how to handle the logical lottery_a, so I just typed the TRUE and FALSE.\nOn to the predictions, remembering to make them wider:\n\ncbind(predictions(steak.2, newdata = new)) %&gt;% \n  select(rowid, group, estimate, educ, lottery_a) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\nRe-fitting to get Hessian\n\n\n\n\n  \n\n\n\nThere are 5 levels of education, 2 levels of lottery_a, and 5 ways in which you might ask for your steak to be cooked, so the original output from predictions has \\(5 \\times 2 \\times 5 = 50\\) rows, and the output you see above has \\(5 \\times 2 = 10\\) rows.\nI find this hard to read, so I’m going to round off those predictions. Three or four decimals seems to be sensible. The time to do this is while they are all in one column, that is, before the pivot_wider. On my screen, the education levels also came out rather long, so I’m going to shorten them as well:\n\ncbind(predictions(steak.2, newdata = new)) %&gt;% \n  select(rowid, group, estimate, educ, lottery_a) %&gt;% \n  mutate(estimate = round(estimate, 3),\n         educ = abbreviate(educ, 15)) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\nRe-fitting to get Hessian\n\n\n\n\n  \n\n\n\nThat’s about as much as I can shorten the education levels while still having them readable.\nThen, say something about the effect of changing educational level on the predictions, and say something about the effect of favouring Lottery A vs. not. I don’t much mind what: you can say that there is not much effect (of either variable), or you can say something like “people with a graduate degree are slightly more likely to like their steak rare and less likely to like it well done” (for education level) and “people who preferred Lottery A are slightly less likely to like their steak rare and slightly more likely to like it well done” (for effect of Lottery A). You can see these by comparing the odd-numbered rows rows with each other to assess the effect of education while holding attitudes towards lottery_a constant (or the even-numbered rows, if you prefer), and you can compare eg. rows 1 and 2 to assess the effect of Lottery A (compare two lines with the same educational level but different preferences re Lottery A).\nI would keep away from saying anything about education level “less than high school”, since this entire level is represented by exactly one person.\n\\(\\blacksquare\\)\n\nIs it reasonable to remove all the remaining explanatory variables from your best model so far? Fit a model with no explanatory variables, and do a test. (In R, if the right side of the squiggle is a 1, that means “just an intercept”. Or you can remove whatever remains using update.) What do you conclude? Explain briefly.\n\nSolution\nThe fitting part is the challenge, since the testing part is anova again. The direct fit is this:\n\nsteak.3 &lt;- polr(steak_prep_ord ~ 1, data = steak)\n\nand the update version is this, about equally long, starting from steak.2 since that is the best model so far:\n\nsteak.3a &lt;- update(steak.2, . ~ . - educ - lottery_a)\n\nYou can use whichever you like. Either way, the second part is anova, and the two possible answers should be the same:\n\nanova(steak.3, steak.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nor\n\nanova(steak.3a, steak.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nAt the 0.05 level, removing both of the remaining variables is fine: that is, nothing (out of these variables) has any impact on the probability that a diner will prefer their steak cooked a particular way. However, it is a very close call; the P-value is only just bigger than 0.05.\nHowever, with data like this and a rather exploratory analysis, I might think about using a larger \\(\\alpha\\) like 0.10, and at this level, taking out both these two variables is a bad idea. You could say that one or both of them is “potentially useful” or “provocative” or something like that.\nIf you think that removing these two variables is questionable, you might like to go back to that drop1 output I had above:\n\ndrop1(steak.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe smallest AIC goes with lottery_a, so that comes out (it is nowhere near significant):\n\nsteak.4 &lt;- update(steak.2, . ~ . - lottery_a)\ndrop1(steak.4, test = \"Chisq\")\n\n\n\n  \n\n\n\nand what you see is that educational level is right on the edge of significance, so that may or may not have any impact. Make a call. But if anything, it’s educational level that makes a difference.\n\\(\\blacksquare\\)\n\nIn the article for which these data were collected, link, does the author obtain consistent conclusions with yours? Explain briefly. (It’s not a very long article, so it won’t take you long to skim through, and the author’s point is pretty clear.)\n\nSolution\nThe article says that nothing has anything to do with steak preference. Whether you agree or not depends on what you thought above about dropping those last two variables. So say something consistent with what you said in the previous part. Two points for saying that the author said “nothing has any effect”, and one point for how your findings square with that.\nExtra: now that you have worked through this great long question, this is where I tell you that I simplified things a fair bit for you! There were lots of other variables that might have had an impact on how people like their steaks, and we didn’t even consider those. Why did I choose what I did here? Well, I wanted to fit a regression predicting steak preference from everything else, do a big backward elimination, but:\n\nsteak.5 &lt;- polr(steak_prep_ord ~ ., data = steak)\n\nWarning: glm.fit: algorithm did not converge\n\n\nError in polr(steak_prep_ord ~ ., data = steak): attempt to find suitable starting values failed\n\n\nThe . in place of explanatory variables means “all the other variables”, including the nonsensical personal ID. That saved me having to type them all out.\nUnfortunately, however, it didn’t work. The problem is a numerical one. Regular regression has a well-defined procedure, where the computer follows through the steps and gets to the answer, every time. Once you go beyond regression, however, the answer is obtained by a step-by-step method: the computer makes an initial guess, tries to improve it, then tries to improve it again, until it can’t improve things any more, at which point it calls it good. The problem here is that polr cannot even get the initial guess! (It apparently is known to suck at this, in problems as big and complicated as this one.)\nI don’t normally recommend forward selection, but I wonder whether it works here:\n\nsteak.5 &lt;- polr(steak_prep_ord ~ 1, data = steak)\nsteak.6 &lt;- step(steak.5,\n  scope = . ~ lottery_a + smoke + alcohol + gamble + skydiving +\n    speed + cheated + female + age + hhold_income + educ + region,\n  direction = \"forward\", test = \"Chisq\", trace = 0\n)\ndrop1(steak.6, test = \"Chisq\")\n\n\n\n  \n\n\n\nIt does, and it says the only thing to add out of all the variables is education level. So, for you, I picked this along with a couple of other plausible-sounding variables and had you start from there.\nForward selection starts from a model containing nothing and asks “what can we add?”. This is a bit more complicated than backward elimination, because now you have to say what the candidate things to add are. That’s the purpose of that scope piece, and there I had no alternative but to type the names of all the variables. Backward elimination is easier, because the candidate variables to remove are the ones in the model, and you don’t need a scope. The trace=0 says “don’t give me any output” (you can change it to a different value if you want to see what that does), and last, the drop1 looks at what is actually in the final model (with a view to asking what can be removed, but we don’t care about that here).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ordinal-response.html#how-do-you-like-your-steak-the-data-1",
    "href": "ordinal-response.html#how-do-you-like-your-steak-the-data-1",
    "title": "25  Logistic regression with ordinal response",
    "section": "25.10 How do you like your steak – the data",
    "text": "25.10 How do you like your steak – the data\nThis question takes you through the data preparation for one of the other questions. You don’t have to do this question, but you may find it interesting or useful.\nWhen you order a steak in a restaurant, the server will ask you how you would like it cooked, or to be precise, how much you would like it cooked: rare (hardly cooked at all), through medium rare, medium, medium well to well (which means “well done”, so that the meat has only a little red to it). Could you guess how a person likes their steak cooked, from some other information about them? The website link commissioned a survey where they asked a number of people how they preferred their steak, along with as many other things as they could think of to ask. (Many of the variables below are related to risk-taking, which was something the people designing the survey thought might have something to do with liking steak rare.) The variables of interest are all factors or true/false:\n\nrespondent_ID: a ten-digit number identifying each person who responded to the survey.\nlottery_a: true if the respondent preferred lottery A with a small chance to win a lot of money, to lottery B, with a larger chance to win less money.\nsmoke: true if the respondent is currently a smoker\nalcohol: true if the respondent at least occasionally drinks alcohol.\ngamble: true if the respondent likes to gamble (eg. betting on horse racing or playing the lottery)\nskydiving: true if the respondent has ever been skydiving.\nspeed: true if the respondent likes to drive fast\ncheated: true if the respondent has ever cheated on a spouse or girlfriend/boyfriend\nsteak: true if the respondent likes to eat steak\nsteak_prep (response): how the respondent likes their steak cooked (factor, as described above, with 5 levels).\nfemale: true if the respondent is female\nage: age group, from 18–29 to 60+.\nhhold_income: household income group, from $0–24,999 to $150,000+.\neduc: highest level of education attained, from “less than high school” up to “graduate degree”\nregion: region (of the US) that the respondent lives in (five values).\n\nThe data are in link.\n\nRead in the data and display the first few lines.\n\nSolution\nThe usual:\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/steak.csv\"\nsteak0 &lt;- read_csv(my_url)\n\nRows: 550 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): steak_prep, age, hhold_income, educ, region\ndbl (1): respondent_id\nlgl (9): lottery_a, smoke, alcohol, gamble, skydiving, speed, cheated, steak...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsteak0\n\n\n\n  \n\n\n\nI’m using a temporary name for reasons that will become clear shortly.\n\\(\\blacksquare\\)\n\nWhat do you immediately notice about your data frame? Run summary on the entire data frame. Would you say you have a lot of missing values, or only a few?\n\nSolution\nI see missing values, starting in the very first row. Running the data frame through summary gives this, either as summary(steak0) or this way:\n\nsteak0 %&gt;% summary()\n\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:279       FALSE:453       FALSE:125      \n Median :3.235e+09   TRUE :267       TRUE :84        TRUE :416      \n Mean   :3.235e+09   NA's :4         NA's :13        NA's :9        \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.238e+09                                                  \n   gamble        skydiving         speed          cheated       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:280       FALSE:502       FALSE:59        FALSE:447      \n TRUE :257       TRUE :36        TRUE :480       TRUE :92       \n NA's :13        NA's :12        NA's :11        NA's :11       \n                                                                \n                                                                \n   steak          steak_prep          female            age           \n Mode :logical   Length:550         Mode :logical   Length:550        \n FALSE:109       Class :character   FALSE:246       Class :character  \n TRUE :430       Mode  :character   TRUE :268       Mode  :character  \n NA's :11                           NA's :36                          \n                                                                      \n                                                                      \n hhold_income           educ              region         \n Length:550         Length:550         Length:550        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n\n\nMake a call about whether you think that’s a lot of missing values or only a few. This might not be all of them, because missing text doesn’t show here (we see later how to make it show up).\n\\(\\blacksquare\\)\n\nWhat does the function drop_na do when applied to a data frame with missing values? To find out, pass the data frame into drop_na(), then into summary again. What has happened?\n\nSolution\nLet’s try it and see.\n\nsteak0 %&gt;% drop_na() %&gt;% summary()\n\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:171       FALSE:274       FALSE:65       \n Median :3.235e+09   TRUE :160       TRUE :57        TRUE :266      \n Mean   :3.235e+09                                                  \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.235e+09                                                  \n   gamble        skydiving         speed          cheated         steak        \n Mode :logical   Mode :logical   Mode :logical   Mode :logical   Mode:logical  \n FALSE:158       FALSE:308       FALSE:28        FALSE:274       TRUE:331      \n TRUE :173       TRUE :23        TRUE :303       TRUE :57                      \n                                                                               \n                                                                               \n                                                                               \n  steak_prep          female            age            hhold_income      \n Length:331         Mode :logical   Length:331         Length:331        \n Class :character   FALSE:174       Class :character   Class :character  \n Mode  :character   TRUE :157       Mode  :character   Mode  :character  \n                                                                         \n                                                                         \n                                                                         \n     educ              region         \n Length:331         Length:331        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nThe missing values, the ones we can see anyway, have all gone. Precisely, drop_na, as its name suggests, drops all the rows that have missing values in them anywhere. This is potentially wasteful, since a row might be missing only one value, and we drop the entire rest of the row, throwing away the good data as well. If you check, we started with 550 rows, and we now have only 311 left. Ouch.\nSo now we’ll save this into our “good” data frame, which means doing it again (now that we know it works):\n\nsteak0 %&gt;% drop_na() -&gt; steak\n\nExtra: another way to handle missing data is called “imputation”: what you do is to estimate a value for any missing data, and then use that later on as if it were the truth. One way of estimating missing values is to do a regression (of appropriate kind: regular or logistic) to predict a column with missing values from all the other columns.\nExtra extra: below we see how we used to have to do this, for your information.\nFirst, we run complete.cases on the data frame:\n\ncomplete.cases(steak0)\n\n  [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [37]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n [61] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n [73] FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [85] FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n [97]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n[109] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n[133] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[145]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[157]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n[169]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n[181] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n[205]  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[217] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[253] FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n[265] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n[277]  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[289]  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n[301] FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[313]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[325] FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n[337]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[349] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n[361] FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n[373] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n[385]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[397]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[409]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE\n[421]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[433]  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n[445]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n[457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[469]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n[481] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n[493] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[505]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n[517]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n[529] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n[541] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\nYou might be able to guess what this does, in the light of what we just did, but if not, you can investigate. Let’s pick three rows where complete.cases is TRUE and three where it’s FALSE, and see what happens.\nI’ll pick rows 496, 497, and 498 for the TRUE rows, and 540, 541 and 542 for the FALSE ones. Let’s assemble these rows into a vector and use slice to display the rows with these numbers:\n\nrows &lt;- c(496, 497, 498, 540, 541, 542)\nrows\n\n[1] 496 497 498 540 541 542\n\n\nLike this:\n\nsteak0 %&gt;% slice(rows)\n\n\n\n  \n\n\n\nWhat’s the difference? The rows where complete.cases is FALSE have one (or more) missing values in them; where complete.cases is TRUE the rows have no missing values. (Depending on the rows you choose, you may not see the missing value(s), as I didn’t.) Extra (within “extra extra”: I hope you are keeping track): this is a bit tricky to investigate more thoroughly, because the text variables might have missing values in them, and they won’t show up unless we turn them into a factor first:\n\nsteak0 %&gt;%\n  mutate(across(where(is.character), \\(x) factor(x))) %&gt;%\n  summary()\n\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:279       FALSE:453       FALSE:125      \n Median :3.235e+09   TRUE :267       TRUE :84        TRUE :416      \n Mean   :3.235e+09   NA's :4         NA's :13        NA's :9        \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.238e+09                                                  \n                                                                    \n   gamble        skydiving         speed          cheated       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:280       FALSE:502       FALSE:59        FALSE:447      \n TRUE :257       TRUE :36        TRUE :480       TRUE :92       \n NA's :13        NA's :12        NA's :11        NA's :11       \n                                                                \n                                                                \n                                                                \n   steak               steak_prep    female           age     \n Mode :logical   Medium     :132   Mode :logical   &gt;60  :131  \n FALSE:109       Medium rare:166   FALSE:246       18-29:110  \n TRUE :430       Medium Well: 75   TRUE :268       30-44:133  \n NA's :11        Rare       : 23   NA's :36        45-60:140  \n                 Well       : 36                   NA's : 36  \n                 NA's       :118                              \n                                                              \n              hhold_income                               educ    \n $0 - $24,999       : 51   Bachelor degree                 :174  \n $100,000 - $149,999: 76   Graduate degree                 :133  \n $150,000+          : 54   High school degree              : 39  \n $25,000 - $49,999  : 77   Less than high school degree    :  2  \n $50,000 - $99,999  :172   Some college or Associate degree:164  \n NA's               :120   NA's                            : 38  \n                                                                 \n                region   \n Pacific           : 91  \n South Atlantic    : 88  \n East North Central: 86  \n Middle Atlantic   : 72  \n West North Central: 42  \n (Other)           :133  \n NA's              : 38  \n\n\nThere are missing values everywhere. What the where does is to do something for each column where the first thing is true: here, if the column is text, then replace it by the factor version of itself. This makes for a better summary, one that shows how many observations are in each category, and, more important for us, how many are missing (a lot).\nAll right, so there are 15 columns, so let’s investigate missingness in our rows by looking at the columns 1 through 8 and then 9 through 15, so they all fit on the screen. Recall that you can select columns by number:\n\nsteak0 %&gt;% select(1:8) %&gt;% slice(rows)\n\n\n\n  \n\n\n\nand\n\nsteak0 %&gt;% select(9:15) %&gt;% slice(rows)\n\n\n\n  \n\n\n\nIn this case, the first three rows have no missing values anywhere, and the last three rows have exactly one missing value. This corresponds to what we would expect, with complete.cases identifying rows that have any missing values.\nWhat we now need to do is to obtain a data frame that contains only the rows with non-missing values. This can be done by saving the result of complete.cases in a variable first; filter can take anything that produces a true or a false for each row, and will return the rows for which the thing it was fed was true.\n\ncc &lt;- complete.cases(steak0)\nsteak0 %&gt;% filter(cc) -&gt; steak.complete\n\nA quick check that we got rid of the missing values:\n\nsteak.complete\n\n\n\n  \n\n\n\nThere are no missing values there. Of course, this is not a proof, and there might be some missing values further down, but at least it suggests that we might be good.\nFor proof, this is the easiest way I know:\n\nsteak.complete %&gt;%\n  mutate(across(where(is.character), \\(x) factor(x))) %&gt;%\n  summary()\n\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:171       FALSE:274       FALSE:65       \n Median :3.235e+09   TRUE :160       TRUE :57        TRUE :266      \n Mean   :3.235e+09                                                  \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.235e+09                                                  \n                                                                    \n   gamble        skydiving         speed          cheated         steak        \n Mode :logical   Mode :logical   Mode :logical   Mode :logical   Mode:logical  \n FALSE:158       FALSE:308       FALSE:28        FALSE:274       TRUE:331      \n TRUE :173       TRUE :23        TRUE :303       TRUE :57                      \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n       steak_prep    female           age                  hhold_income\n Medium     :109   Mode :logical   &gt;60  :82   $0 - $24,999       : 37  \n Medium rare:128   FALSE:174       18-29:70   $100,000 - $149,999: 66  \n Medium Well: 56   TRUE :157       30-44:93   $150,000+          : 39  \n Rare       : 18                   45-60:86   $25,000 - $49,999  : 55  \n Well       : 20                              $50,000 - $99,999  :134  \n                                                                       \n                                                                       \n                               educ                    region  \n Bachelor degree                 :120   South Atlantic    :68  \n Graduate degree                 : 86   Pacific           :57  \n High school degree              : 20   East North Central:48  \n Less than high school degree    :  1   Middle Atlantic   :46  \n Some college or Associate degree:104   West North Central:29  \n                                        Mountain          :24  \n                                        (Other)           :59  \n\n\nIf there were any missing values, they would be listed on the end of the counts of observations for each level, or on the bottom of the five-number sumamries. But there aren’t. So here’s your proof.\n\\(\\blacksquare\\)\n\nWrite the data into a .csv file, with a name like steak1.csv. Open this file in a spreadsheet and (quickly) verify that you have the right columns and no missing values.\n\nSolution\nThis is write_csv, using my output from drop_na:\n\nwrite_csv(steak, \"steak1.csv\")\n\nOpen up Excel, or whatever you have, and take a look. You should have all the right columns, and, scrolling down, no visible missing values.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ordinal-response.html#footnotes",
    "href": "ordinal-response.html#footnotes",
    "title": "25  Logistic regression with ordinal response",
    "section": "",
    "text": "I’m always amused at how Americans put all Asians into one group.↩︎\nI’m always amused at how Americans put all Asians into one group.↩︎"
  },
  {
    "objectID": "nominal-response.html#finding-non-missing-values",
    "href": "nominal-response.html#finding-non-missing-values",
    "title": "26  Logistic regression with nominal response",
    "section": "26.1 Finding non-missing values",
    "text": "26.1 Finding non-missing values\n* This is to prepare you for something in the next question. It’s meant to be easy.\nIn R, the code NA stands for “missing value” or “value not known”. In R, NA should not have quotes around it. (It is a special code, not a piece of text.)\n\nCreate a vector v that contains some numbers and some missing values, using c(). Put those values into a one-column data frame.\nObtain a new column containing is.na(v). When is this true and when is this false?\nThe symbol ! means “not” in R (and other programming languages). What does !is.na(v) do? Create a new column containing that.\nUse filter to display just the rows of your data frame that have a non-missing value of v."
  },
  {
    "objectID": "nominal-response.html#european-social-survey-and-voting",
    "href": "nominal-response.html#european-social-survey-and-voting",
    "title": "26  Logistic regression with nominal response",
    "section": "26.2 European Social Survey and voting",
    "text": "26.2 European Social Survey and voting\nThe European Social Survey is a giant survey carried out across Europe covering demographic information, attitudes to and amount of education, politics and so on. In this question, we will investigate what might make British people vote for a certain political party.\nThe information for this question is in a (large) spreadsheet at link. There is also a “codebook” at link that tells you what all the variables are. The ones we will use are the last five columns of the spreadsheet, described on pages 11 onwards of the codebook. (I could have given you more, but I didn’t want to make this any more complicated than it already was.)\n\nRead in the .csv file, and verify that you have lots of rows and columns.\n* Use the codebook to find out what the columns prtvtgb, gndr, agea, eduyrs and inwtm are. What do the values 1 and 2 for gndr mean? (You don’t, at this point, have to worry about the values for the other variables.)\nThe three major political parties in Britain are the Conservative, Labour and Liberal Democrat. (These, for your information, correspond roughly to the Canadian Progressive Conservative, NDP and Liberal parties.) For the variable that corresponds to “political party voted for at the last election”, which values correspond to these three parties?\nNormally, I would give you a tidied-up data set. But I figure you could use some practice tidying this one up. As the codebook shows, there are some numerical codes for missing values, and we want to omit those. We want just the columns prtvtgb through inwtm from the right side of the spreadsheet. Use dplyr or tidyr tools to (i) select only these columns, (ii) include the rows that correspond to people who voted for one of the three major parties, (iii) include the rows that have an age at interview less than 999, (iv) include the rows that have less than 40 years of education, (v) include the rows that are not missing on inwtm (use the idea from Question~here for (v)). The last four of those (the inclusion of rows) can be done in one go.\nWhy is my response variable nominal rather than ordinal? How can I tell? Which R function should I use, therefore, to fit my model?\n* Take the political party voted for, and turn it into a factor, by feeding it into factor. Fit an appropriate model to predict political party voted for at the last election (as a factor) from all the other variables. Gender is really a categorical variable too, but since there are only two possible values it can be treated as a number.\nWe have a lot of explanatory variables. The standard way to test whether we need all of them is to take one of them out at a time, and test which ones we can remove. This is a lot of work. We won’t do that. Instead, the R function step does what you want. You feed step two things: a fitted model object, and the option trace=0 (otherwise you get a lot of output). The final part of the output from step tells you which explanatory variables you need to keep. Run step on your fitted model. Which explanatory variables need to stay in the model here?\nFit the model indicated by step (in the last part).\nI didn’t think that interview length could possibly be relevant to which party a person voted for. Test whether interview length can be removed from your model of the last part. What do you conclude? (Note that step and this test may disagree.)\nUse your best model to obtain predictions from some suitably chosen combinations of values of the explanatory variables that remain. (If you have quantitative explanatory variables left, you could use their first and third quartiles as values to predict from. Running summary on the data frame will get summaries of all the variables.)\nWhat is the effect of increasing age? What is the effect of an increase in years of education?"
  },
  {
    "objectID": "nominal-response.html#alligator-food",
    "href": "nominal-response.html#alligator-food",
    "title": "26  Logistic regression with nominal response",
    "section": "26.3 Alligator food",
    "text": "26.3 Alligator food\nWhat do alligators most like to eat? 219 alligators were captured in four Florida lakes. Each alligator’s stomach contents were observed, and the food that the alligator had eaten was classified into one of five categories: fish, invertebrates (such as snails or insects), reptiles (such as turtles), birds, and “other” (such as amphibians, plants or rocks). The researcher noted for each alligator what that alligator had most of in its stomach, as well as the gender of each alligator and whether it was “large” or “small” (greater or less than 2.3 metres in length). The data can be found in link. The numbers in the data set (apart from the first column) are all frequencies. (You can ignore that first column “profile”.)\nOur aim is to predict food type from the other variables.\n\nRead in the data and display the first few lines. Describe how the data are not “tidy”.\nUse pivot_longer to arrange the data suitably for analysis (which will be using multinom). Demonstrate (by looking at the first few rows of your new data frame) that you now have something tidy.\nWhat is different about this problem, compared to Question here, that would make multinom the right tool to use?\nFit a suitable multinomial model predicting food type from gender, size and lake. Does each row represent one alligator or more than one? If more than one, account for this in your modelling.\nDo a test to see whether Gender should stay in the model. (This will entail fitting another model.) What do you conclude?\nPredict the probability that an alligator prefers each food type, given its size, gender (if necessary) and the lake it was found in, using the more appropriate of the two models that you have fitted so far. This means (i) making a data frame for prediction, and (ii) obtaining and displaying the predicted probabilities in a way that is easy to read.\nWhat do you think is the most important way in which the lakes differ? (Hint: look at where the biggest predicted probabilities are.)\nHow would you describe the major difference between the diets of the small and large alligators?"
  },
  {
    "objectID": "nominal-response.html#crimes-in-san-francisco",
    "href": "nominal-response.html#crimes-in-san-francisco",
    "title": "26  Logistic regression with nominal response",
    "section": "26.4 Crimes in San Francisco",
    "text": "26.4 Crimes in San Francisco\nThe data in link is a subset of a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are:\n\nDates: the date and time of the crime\nCategory: the category of crime, eg. “larceny” or “vandalism” (response).\nDescript: detailed description of crime.\nDayOfWeek: the day of the week of the crime.\nPdDistrict: the name of the San Francisco Police Department district in which the crime took place.\nResolution: how the crime was resolved\nAddress: approximate street address of crime\nX: longitude\nY: latitude\n\nOur aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which are the ones included in this data file.\nSome of the model-fitting takes a while (you’ll see why below). If you’re using R Markdown, you can wait for the models to fit each time you re-run your document, or insert cache=T in the top line of your code chunk (the one with r in curly brackets in it, above the actual code). Put a comma and the cache=T inside the curly brackets. What that does is to re-run that code chunk only if it changes; if it hasn’t changed it will use the saved results from last time it was run. That can save you a lot of waiting around.\n\nRead in the data and display the dataset (or, at least, part of it).\nFit a multinomial logistic regression that predicts crime category from day of week and district. (You don’t need to look at it.) The model-fitting produces some output. (If you’re using R Markdown, that will come with it.)\nFit a model that predicts Category from only the district. Hand in the output from the fitting process as well.\nUse anova to compare the two models you just obtained. What does the anova tell you?\nUsing your preferred model, obtain predicted probabilities that a crime will be of each of these four categories for each day of the week in the TENDERLOIN district (the name is ALL CAPS). This will mean constructing a data frame to predict from, obtaining the predictions and then displaying them suitably.\nDescribe briefly how the weekend days Saturday and Sunday differ from the rest."
  },
  {
    "objectID": "nominal-response.html#crimes-in-san-francisco-the-data",
    "href": "nominal-response.html#crimes-in-san-francisco-the-data",
    "title": "26  Logistic regression with nominal response",
    "section": "26.5 Crimes in San Francisco – the data",
    "text": "26.5 Crimes in San Francisco – the data\nThe data in link is a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are:\n\nDates: the date and time of the crime\nCategory: the category of crime, eg. “larceny” or “vandalism” (response).\nDescript: detailed description of crime.\nDayOfWeek: the day of the week of the crime.\nPdDistrict: the name of the San Francisco Police Department district in which the crime took place.\nResolution: how the crime was resolved\nAddress: approximate street address of crime\nX: longitude\nY: latitude\n\nOur aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which we will have to discover.\n\nRead in the data and verify that you have these columns and a lot of rows. (The data may take a moment to read in. You will see why.)\nHow is the response variable here different to the one in the question about steak preferences (and therefore why would multinom from package nnet be the method of choice)?\nFind out which crime categories there are, and arrange them in order of how many crimes there were in each category.\nWhich are the four most frequent “interesting” crime categories, that is to say, not including “other offenses” and “non-criminal”? Get them into a vector called my.crimes. See if you can find a way of doing this that doesn’t involve typing them in (for full marks).\n(Digression, but needed for the next part.) The R vector letters contains the lowercase letters from a to z. Consider the vector ('a','m',3,'Q'). Some of these are found amongst the lowercase letters, and some not. Type these into a vector v and explain briefly why v %in% letters produces what it does.\nWe are going to filter only the rows of our data frame that have one of the crimes in my.crimes as their Category. Also, select only the columns Category, DayOfWeek and PdDistrict. Save the resulting data frame and display its structure. (You should have a lot fewer rows than you did before.)\nSave these data in a file sfcrime1.csv."
  },
  {
    "objectID": "nominal-response.html#what-sports-do-these-athletes-play",
    "href": "nominal-response.html#what-sports-do-these-athletes-play",
    "title": "26  Logistic regression with nominal response",
    "section": "26.6 What sports do these athletes play?",
    "text": "26.6 What sports do these athletes play?\nThe data at link are physical and physiological measurements of 202 male and female Australian elite athletes. The data values are separated by tabs. We are going to see whether we can predict the sport an athlete plays from their height and weight.\nThe sports, if you care, are respectively basketball, “field athletics” (eg. shot put, javelin throw, long jump etc.), gymnastics, netball, rowing, swimming, 400m running, tennis, sprinting (100m or 200m running), water polo.\n\nRead in the data and display the first few rows.\nMake a scatterplot of height vs. weight, with the points coloured by what sport the athlete plays. Put height on the \\(x\\)-axis and weight on the \\(y\\)-axis.\nExplain briefly why a multinomial model (multinom from nnet) would be the best thing to use to predict sport played from the other variables.\nFit a suitable model for predicting sport played from height and weight. (You don’t need to look at the results.) 100 steps isn’t quite enough, so set maxit equal to a larger number to allow the estimation to finish.\nDemonstrate using anova that Wt should not be removed from this model.\nMake a data frame consisting of all combinations of Ht 160, 180 and 200 (cm), and Wt 50, 75, and 100 (kg), and use it to obtain predicted probabilities of athletes of those heights and weights playing each of the sports. Display the results. You might have to display them smaller, or reduce the number of decimal places1 to fit them on the page.\nFor an athlete who is 180 cm tall and weighs 100 kg, what sport would you guess they play? How sure are you that you are right? Explain briefly.\n\nMy solutions follow:"
  },
  {
    "objectID": "nominal-response.html#finding-non-missing-values-1",
    "href": "nominal-response.html#finding-non-missing-values-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.7 Finding non-missing values",
    "text": "26.7 Finding non-missing values\n* This is to prepare you for something in the next question. It’s meant to be easy.\nIn R, the code NA stands for “missing value” or “value not known”. In R, NA should not have quotes around it. (It is a special code, not a piece of text.)\n\nCreate a vector v that contains some numbers and some missing values, using c(). Put those values into a one-column data frame.\n\nSolution\nLike this. The arrangement of numbers and missing values doesn’t matter, as long as you have some of each:\n\nv &lt;- c(1, 2, NA, 4, 5, 6, 9, NA, 11)\nmydata &lt;- tibble(v)\nmydata\n\n\n\n  \n\n\n\nThis has one column called v.\n\\(\\blacksquare\\)\n\nObtain a new column containing is.na(v). When is this true and when is this false?\n\nSolution\n\nmydata &lt;- mydata %&gt;% mutate(isna = is.na(v))\nmydata\n\n\n\n  \n\n\n\nThis is TRUE if the corresponding element of v is missing (in my case, the third value and the second-last one), and FALSE otherwise (when there is an actual value there).\n\\(\\blacksquare\\)\n\nThe symbol ! means “not” in R (and other programming languages). What does !is.na(v) do? Create a new column containing that.\n\nSolution\nTry it and see. Give it whatever name you like. My name reflects that I know what it’s going to do:\n\nmydata &lt;- mydata %&gt;% mutate(notisna = !is.na(v))\nmydata\n\n\n\n  \n\n\n\nThis is the logical opposite of is.na: it’s true if there is a value, and false if it’s missing.\n\\(\\blacksquare\\)\n\nUse filter to display just the rows of your data frame that have a non-missing value of v.\n\nSolution\nfilter takes a column to say which rows to pick, in which case the column should contain something that either is TRUE or FALSE, or something that can be interpreted that way:\n\nmydata %&gt;% filter(notisna)\n\n\n\n  \n\n\n\nor you can provide filter something that can be calculated from what’s in the data frame, and also returns something that is either true or false:\n\nmydata %&gt;% filter(!is.na(v))\n\n\n\n  \n\n\n\nIn either case, I only have non-missing values of v.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#european-social-survey-and-voting-1",
    "href": "nominal-response.html#european-social-survey-and-voting-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.8 European Social Survey and voting",
    "text": "26.8 European Social Survey and voting\nThe European Social Survey is a giant survey carried out across Europe covering demographic information, attitudes to and amount of education, politics and so on. In this question, we will investigate what might make British people vote for a certain political party.\nThe information for this question is in a (large) spreadsheet at link. There is also a “codebook” at link that tells you what all the variables are. The ones we will use are the last five columns of the spreadsheet, described on pages 11 onwards of the codebook. (I could have given you more, but I didn’t want to make this any more complicated than it already was.)\n\nRead in the .csv file, and verify that you have lots of rows and columns.\n\nSolution\nThe obvious way. Printing it out will display some of the data and tell you how many rows and columns you have:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ess.csv\"\ness &lt;- read_csv(my_url)\n\nRows: 2286 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): cntry, cname, cproddat, name\ndbl (13): cedition, cseqno, essround, edition, idno, dweight, pspwght, pweig...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ness\n\n\n\n  \n\n\n\n2286 rows and 17 columns.\n\\(\\blacksquare\\)\n\n* Use the codebook to find out what the columns prtvtgb, gndr, agea, eduyrs and inwtm are. What do the values 1 and 2 for gndr mean? (You don’t, at this point, have to worry about the values for the other variables.)\n\nSolution\nRespectively, political party voted for at last election, gender (of respondent), age at interview, years of full-time education, length of interview (in minutes). For gndr, male is 1 and female is 2.\n\\(\\blacksquare\\)\n\nThe three major political parties in Britain are the Conservative, Labour and Liberal Democrat. (These, for your information, correspond roughly to the Canadian Progressive Conservative, NDP and Liberal parties.) For the variable that corresponds to “political party voted for at the last election”, which values correspond to these three parties?\n\nSolution\n1, 2 and 3 respectively. (That was easy!)\n\\(\\blacksquare\\)\n\nNormally, I would give you a tidied-up data set. But I figure you could use some practice tidying this one up. As the codebook shows, there are some numerical codes for missing values, and we want to omit those. We want just the columns prtvtgb through inwtm from the right side of the spreadsheet. Use dplyr or tidyr tools to (i) select only these columns, (ii) include the rows that correspond to people who voted for one of the three major parties, (iii) include the rows that have an age at interview less than 999, (iv) include the rows that have less than 40 years of education, (v) include the rows that are not missing on inwtm (use the idea from Question~here for (v)). The last four of those (the inclusion of rows) can be done in one go.\n\nSolution\nThe major parties are numbered 1, 2 and 3, so we can select the ones less than 4 (or &lt;=3). The reference back to the last question is a hint to use !is.na(). It also works to use drop_na, if you are familiar with that.\n\ness %&gt;%\n  select(prtvtgb:inwtm) %&gt;%\n  filter(prtvtgb &lt; 4, agea &lt; 999, eduyrs &lt; 40, !is.na(inwtm)) -&gt; ess.major\n\nYou might get a weird error in select, something about “unused argument”. If this happens to you, it’s not because you used select wrong, it’s because you used the wrong select! There is one in MASS, and you need to make sure that this package is “detached” so that you use the select you want, namely the one in dplyr, loaded with the tidyverse. Use the instructions at the end of the mobile phones question or the abortion question to do this.\nThe other way around this is to say, instead of select, dplyr::select with two colons. This means “the select that lives in dplyr, no other”, and is what Wikipedia calls “disambiguation”: out of several things with the same name, you say which one you mean.\nIf you do the pipeline, you will probably not get it right the first time. (I didn’t.) For debugging, try out one step at a time, and summarize what you have so far, so that you can check it for correctness. A handy trick for that is to make the last piece of your pipeline summary(), which produces a summary of the columns of the resulting data frame. For example, I first did this (note that my filter is a lot simpler than the one above):\n\ness %&gt;%\n  select(prtvtgb:inwtm) %&gt;%\n  filter(prtvtgb &lt; 4, !is.na(inwtm)) %&gt;%\n  summary()\n\n    prtvtgb           gndr            agea            eduyrs     \n Min.   :1.000   Min.   :1.000   Min.   : 18.00   Min.   : 0.00  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.: 44.00   1st Qu.:11.00  \n Median :2.000   Median :2.000   Median : 58.00   Median :13.00  \n Mean   :1.803   Mean   :1.572   Mean   : 61.74   Mean   :14.23  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 71.00   3rd Qu.:16.00  \n Max.   :3.000   Max.   :2.000   Max.   :999.00   Max.   :88.00  \n     inwtm       \n Min.   :  7.00  \n 1st Qu.: 35.00  \n Median : 41.00  \n Mean   : 43.54  \n 3rd Qu.: 50.00  \n Max.   :160.00  \n\n\nThe mean of a categorical variable like party voted for or gender doesn’t make much sense, but it looks as if all the values are sensible ones (1 to 3 and 1, 2 respectively). However, the maximum values of age and years of education look like missing value codes, hence the other requirements I put in the question.2\nDisplaying as the last step of your pipeline also works, but the advantage of summary is that you get to see whether there are any unusual values, in this case unusually large values that are missing value codes.\n\\(\\blacksquare\\)\n\nWhy is my response variable nominal rather than ordinal? How can I tell? Which R function should I use, therefore, to fit my model?\n\nSolution\nThe response variable is political party voted for. There is no (obvious) ordering to this (unless you want to try to place the parties on a left-right spectrum), so this is nominal, and you’ll need multinom from package nnet.\nIf I had included the minor parties and you were working on a left-right spectrum, you would have had to decide where to put the somewhat libertarian Greens3 or the parties that exist only in Northern Ireland.4\n\\(\\blacksquare\\)\n\n* Take the political party voted for, and turn it into a factor, by feeding it into factor. Fit an appropriate model to predict political party voted for at the last election (as a factor) from all the other variables. Gender is really a categorical variable too, but since there are only two possible values it can be treated as a number.\n\nSolution\nThis, or something like it. multinom lives in package nnet, which you’ll have to install first if you haven’t already:\n\nlibrary(nnet)\ness.1 &lt;- multinom(factor(prtvtgb) ~ gndr + agea + eduyrs + inwtm, data = ess.major)\n\n# weights:  18 (10 variable)\ninitial  value 1343.602829 \niter  10 value 1256.123798\nfinal  value 1247.110080 \nconverged\n\n\nOr create a factor version of your response in the data frame first:\n\ness.major &lt;- ess.major %&gt;% mutate(party = factor(prtvtgb))\n\nand then:\n\ness.1a &lt;- multinom(party ~ gndr + agea + eduyrs + inwtm, data = ess.major)\n\n# weights:  18 (10 variable)\ninitial  value 1343.602829 \niter  10 value 1256.123798\nfinal  value 1247.110080 \nconverged\n\n\n\\(\\blacksquare\\)\n\nWe have a lot of explanatory variables. The standard way to test whether we need all of them is to take one of them out at a time, and test which ones we can remove. This is a lot of work. We won’t do that. Instead, the R function step does what you want. You feed step two things: a fitted model object, and the option trace=0 (otherwise you get a lot of output). The final part of the output from step tells you which explanatory variables you need to keep. Run step on your fitted model. Which explanatory variables need to stay in the model here?\n\nSolution\nI tried to give you lots of hints here:\n\ness.2a &lt;- step(ess.1, trace = 0)\n\ntrying - gndr \ntrying - agea \ntrying - eduyrs \ntrying - inwtm \n# weights:  15 (8 variable)\ninitial  value 1343.602829 \niter  10 value 1248.343563\nfinal  value 1248.253638 \nconverged\ntrying - agea \ntrying - eduyrs \ntrying - inwtm \n\ness.2a\n\nCall:\nmultinom(formula = factor(prtvtgb) ~ agea + eduyrs + inwtm, data = ess.major)\n\nCoefficients:\n  (Intercept)        agea     eduyrs       inwtm\n2    1.632266 -0.02153694 -0.0593757 0.009615167\n3   -1.281031 -0.01869263  0.0886487 0.009337084\n\nResidual Deviance: 2496.507 \nAIC: 2512.507 \n\n\nIf you didn’t save your output in a variable, you’ll get my last bit automatically.\nThe end of the output gives us coefficients for (and thus tells us we need to keep) age, years of education and interview length.\nThe actual numbers don’t mean much; it’s the indication that the variable has stayed in the model that makes a difference.5\nIf you’re wondering about the process: first step tries to take out each explanatory variable, one at a time, from the starting model (the one that contains all the variables). Then it finds the best model out of those and fits it. (It doesn’t tell us which model this is, but evidently it’s the one without gender.) Then it takes that model and tries to remove its explanatory variables one at a time (there are only three of them left). Having decided it cannot remove any of them, it stops, and shows us what’s left.\nLeaving out the trace=0 shows more output and more detail on the process, but I figured this was enough (and this way, you don’t have to wade through all of that output). Try values like 1 or 2 for trace and see what you get.\n\\(\\blacksquare\\)\n\nFit the model indicated by step (in the last part).\n\nSolution\nCopy and paste, and take out the variables you don’t need. Or, better, save the output from step in a variable. This then becomes a fitted model object and you can look at it any of the ways you can look at a model fit. I found that gender needed to be removed, but if yours is different, follow through with whatever your step said to do.\n\ness.2 &lt;- multinom(party ~ agea + eduyrs + inwtm, data = ess.major)\n\n# weights:  15 (8 variable)\ninitial  value 1343.602829 \niter  10 value 1248.343563\nfinal  value 1248.253638 \nconverged\n\n\nIf you saved the output from step, you’ll already have this and you don’t need to do it again:\n\nanova(ess.2, ess.2a)\n\n\n\n  \n\n\n\nSame model.\n\\(\\blacksquare\\)\n\nI didn’t think that interview length could possibly be relevant to which party a person voted for. Test whether interview length can be removed from your model of the last part. What do you conclude? (Note that step and this test may disagree.)\n\nSolution\nFit the model without inwtm:\n\ness.3 &lt;- multinom(party ~ agea + eduyrs, data = ess.major)\n\n# weights:  12 (6 variable)\ninitial  value 1343.602829 \niter  10 value 1250.418281\nfinal  value 1250.417597 \nconverged\n\n\nand then use anova to compare them:\n\nanova(ess.3, ess.2)\n\n\n\n  \n\n\n\nThe P-value, 0.1149, is not small, which says that the smaller model is good, ie. the one without interview length.\nI thought drop1 would also work here, but it appears not to:\n\ndrop1(ess.1, test = \"Chisq\")\n\ntrying - gndr \n\n\nError in if (trace) {: argument is not interpretable as logical\n\n\nI think that’s a bug in multinom, since normally if step works, then drop1 will work too (normally step uses drop1).\nThe reason for the disagreement between step and anova is that step will tend to keep marginal explanatory variables, that is, ones that are “potentially interesting” but whose P-values might not be less than 0.05. There is still no substitute for your judgement in figuring out what to do! step uses a thing called AIC to decide what to do, rather than actually doing a test. If you know about “adjusted R-squared” in choosing explanatory variables for a regression, it’s the same idea: a variable can be not quite significant but still make the adjusted R-squared go up (typically only a little).\n\\(\\blacksquare\\)\n\nUse your best model to obtain predictions from some suitably chosen combinations of values of the explanatory variables that remain. (If you have quantitative explanatory variables left, you could use their first and third quartiles as values to predict from. Running summary on the data frame will get summaries of all the variables.)\n\nSolution\nFirst make our new data frame of values to predict from. You can use quantile or summary to find the quartiles. I only had agea and eduyrs left, having decided that interview time really ought to come out:\n\nsummary(ess.major)\n\n    prtvtgb           gndr            agea           eduyrs     \n Min.   :1.000   Min.   :1.000   Min.   :18.00   Min.   : 0.00  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:44.00   1st Qu.:11.00  \n Median :2.000   Median :2.000   Median :58.00   Median :13.00  \n Mean   :1.803   Mean   :1.574   Mean   :57.19   Mean   :13.45  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:71.00   3rd Qu.:16.00  \n Max.   :3.000   Max.   :2.000   Max.   :94.00   Max.   :33.00  \n     inwtm       party  \n Min.   :  7.0   1:484  \n 1st Qu.: 35.0   2:496  \n Median : 41.0   3:243  \n Mean   : 43.7          \n 3rd Qu.: 50.0          \n Max.   :160.0          \n\n\nQuartiles for age are 44 and 71, and for years of education are 11 and 16.\nThis time, instead of predicting for variable values that predictions chooses for us (like a five-number summary), we are predicting for “custom” values, ones that we chose. To set that up, the marginaleffects way is to use datagrid like this:\n\nnew &lt;- datagrid(model = ess.3, agea = c(44, 71), eduyrs = c(11, 16))\nnew\n\n\n\n  \n\n\n\nWhat datagrid does is to make all combinations of your variable values, and along with that, to use “typical” values for the others: the mean, in the case of quantitative variables like inwtm, and the most common category for categorical ones like party. If you feed datagrid a model first, it only includes variables in that model, which is easier to make sense of:\n\ndatagrid(newdata = ess.major, agea = c(44, 71), eduyrs = c(11, 16))\n\n\n\n  \n\n\n\nThe other variables don’t make much sense, since they are really categorical but expressed as numbers, but they are not in the best model, so that doesn’t do any harm. (In other cases, you might need to be more careful.)\nNext, we feed this into predictions, using the above dataframe as newdata, and with our best model, ess.3 (the one without interview length). The results might confuse you at first, since you will probably get an error:\n\ncbind(predictions(ess.3, newdata = new))\n\n\n\n  \n\n\n\nThe error message gives you a hint about what to do: add a type = \"probs\" to predictions (which is a consequence of how multinom works):\n\ncbind(predictions(ess.3, newdata = new, type = \"probs\"))\n\n\n\n  \n\n\n\nThere are twelve rows for our four predictions, because there are three predictions for each of our four “people”: the probabilities of each one voting for each of the three parties. The party predicted for is in the column group, and the probability of each person (labelled by rowid) voting for that party is in predicted. Let’s simplify things by keeping only those columns and the ones we are predicting for:\n\ncbind(predictions(ess.3, newdata = new, type = \"probs\")) %&gt;% \n  select(rowid, group, estimate, agea, eduyrs)\n\n\n\n  \n\n\n\nand then pivot wider to get all three predictions for each person on one line:\n\ncbind(predictions(ess.3, newdata = new, type = \"probs\")) %&gt;% \n  select(rowid, group, estimate, agea, eduyrs) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nWhat is the effect of increasing age? What is the effect of an increase in years of education?\n\nSolution\nTo assess the effect of age, hold years of education constant. Thus, compare lines 1 and 3 (or 2 and 4): increasing age tends to increase the chance that a person will vote Conservative (party 1), and decrease the chance that a person will vote Labour (party 2). There doesn’t seem to be much effect of age on the chance that a person will vote Liberal Democrat.\nTo assess education, hold age constant, and thus compare rows 1 and 2 (or rows 3 and 4). This time, there isn’t much effect on the chances of voting Conservative, but as education increases, the chance of voting Labour goes down, and the chance of voting Liberal Democrat goes up.\nA little history: back 150 or so years ago, Britain had two political parties, the Tories and the Whigs. The Tories became the Conservative party (and hence, in Britain and in Canada, the Conservatives are nicknamed Tories6). The Whigs became Liberals. At about the same time as working people got to vote (not women, yet, but working men) the Labour Party came into existence. The Labour Party has always been affiliated with working people and trades unions, like the NDP here. But power has typically alternated between Conservative and Labour goverments, with the Liberals as a third party. In the 1980s a new party called the Social Democrats came onto the scene, but on realizing that they couldn’t make much of a dent by themselves, they merged with the Liberals to form the Liberal Democrats, which became a slightly stronger third party.\nI was curious about what the effect of interview length would be. Presumably, the effect is small, but I have no idea which way it would be. To assess this, this is predictions again, but this time we can let predictions pick some values for inwtm for us, and leave everything else at their mean. We have to remember to use the model ess.2 that contained interview length, this time:\n\ncbind(predictions(ess.2, variables = \"inwtm\", type = \"probs\")) %&gt;% \n  select(rowid, group, estimate, agea, eduyrs, inwtm) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\nAs interview length goes up (for a respondent with average age and years of education, though the pattern would be the same for people of different ages and different amounts of education), the respondent is less likely to vote Conservative (party 1), and more likely to vote for one of the other two parties.\nBut, as we suspected, the effect is small (except for that very long interview length) and not really worth worrying about.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#alligator-food-1",
    "href": "nominal-response.html#alligator-food-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.9 Alligator food",
    "text": "26.9 Alligator food\nWhat do alligators most like to eat? 219 alligators were captured in four Florida lakes. Each alligator’s stomach contents were observed, and the food that the alligator had eaten was classified into one of five categories: fish, invertebrates (such as snails or insects), reptiles (such as turtles), birds, and “other” (such as amphibians, plants or rocks). The researcher noted for each alligator what that alligator had most of in its stomach, as well as the gender of each alligator and whether it was “large” or “small” (greater or less than 2.3 metres in length). The data can be found in link. The numbers in the data set (apart from the first column) are all frequencies. (You can ignore that first column “profile”.)\nOur aim is to predict food type from the other variables.\n\nRead in the data and display the first few lines. Describe how the data are not “tidy”.\n\nSolution\nSeparated by exactly one space:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/alligator.txt\"\ngators.orig &lt;- read_delim(my_url, \" \")\n\nRows: 16 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (3): Gender, Size, Lake\ndbl (6): profile, Fish, Invertebrate, Reptile, Bird, Other\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngators.orig\n\n\n\n  \n\n\n\nThe last five columns are all frequencies. Or, one of the variables (food type) is spread over five columns instead of being contained in one. Either is good.\nMy choice of “temporary” name reflects that I’m going to obtain a “tidy” data frame called gators in a moment.\n\\(\\blacksquare\\)\n\nUse pivot_longer to arrange the data suitably for analysis (which will be using multinom). Demonstrate (by looking at the first few rows of your new data frame) that you now have something tidy.\n\nSolution\nI’m creating my “official” data frame here:\n\ngators.orig %&gt;% \n  pivot_longer(Fish:Other, names_to = \"Food.type\", values_to = \"Frequency\") -&gt; gators\ngators\n\n\n\n  \n\n\n\nI gave my column names Capital Letters to make them consistent with the others (and in an attempt to stop infesting my brain with annoying variable-name errors when I fit models later).\nLooking at the first few lines reveals that I now have a column of food types and one column of frequencies, both of which are what I wanted. I can check that I have all the different food types by finding the distinct ones:\n\ngators %&gt;% distinct(Food.type)\n\n\n\n  \n\n\n\n(Think about why count would be confusing here.)\nNote that Food.type is text (chr) rather than being a factor. I’ll hold my breath and see what happens when I fit a model where it is supposed to be a factor.\n\\(\\blacksquare\\)\n\nWhat is different about this problem, compared to Question here, that would make multinom the right tool to use?\n\nSolution\nLook at the response variable Food.type (or whatever you called it): this has multiple categories, but they are not ordered in any logical way. Thus, in short, a nominal response.\n\\(\\blacksquare\\)\n\nFit a suitable multinomial model predicting food type from gender, size and lake. Does each row represent one alligator or more than one? If more than one, account for this in your modelling.\n\nSolution\nEach row of the tidy gators represents as many alligators as are in the Frequency column. That is, if you look at female small alligators in Lake George that ate mainly fish, there are three of those.7 This to remind you to include the weights piece, otherwise multinom will assume that you have one observation per line and not as many as the number in Frequency.\nThat is the reason that count earlier would have been confusing: it would have told you how many rows contained each food type, rather than how many alligators, and these would have been different:\n\ngators %&gt;% count(Food.type)\n\n\n\n  \n\n\ngators %&gt;% count(Food.type, wt = Frequency)\n\n\n\n  \n\n\n\nEach food type appears on 16 rows, but is the favoured diet of very different numbers of alligators. Note the use of wt= to specify a frequency variable.8\nYou ought to understand why those are different.\nAll right, back to modelling:\n\nlibrary(nnet)\ngators.1 &lt;- multinom(Food.type ~ Gender + Size + Lake,\n  weights = Frequency, data = gators\n)\n\n# weights:  35 (24 variable)\ninitial  value 352.466903 \niter  10 value 270.228588\niter  20 value 268.944257\nfinal  value 268.932741 \nconverged\n\n\nThis worked, even though Food.type was actually text. I guess it got converted to a factor. The ordering of the levels doesn’t matter here anyway, since this is not an ordinal model.\nNo need to look at it, since the output is kind of confusing anyway:\n\nsummary(gators.1)\n\nCall:\nmultinom(formula = Food.type ~ Gender + Size + Lake, data = gators, \n    weights = Frequency)\n\nCoefficients:\n             (Intercept)     Genderm   Size&gt;2.3 Lakehancock Lakeoklawaha\nFish           2.4322304  0.60674971 -0.7308535  -0.5751295    0.5513785\nInvertebrate   2.6012531  0.14378459 -2.0671545  -2.3557377    1.4645820\nOther          1.0014505  0.35423803 -1.0214847   0.1914537    0.5775317\nReptile       -0.9829064 -0.02053375 -0.1741207   0.5534169    3.0807416\n             Laketrafford\nFish          -1.23681053\nInvertebrate  -0.08096493\nOther          0.32097943\nReptile        1.82333205\n\nStd. Errors:\n             (Intercept)   Genderm  Size&gt;2.3 Lakehancock Lakeoklawaha\nFish           0.7706940 0.6888904 0.6523273   0.7952147     1.210229\nInvertebrate   0.7917210 0.7292510 0.7084028   0.9463640     1.232835\nOther          0.8747773 0.7623738 0.7250455   0.9072182     1.374545\nReptile        1.2827234 0.9088217 0.8555051   1.3797755     1.591542\n             Laketrafford\nFish            0.8661187\nInvertebrate    0.8814625\nOther           0.9589807\nReptile         1.3388017\n\nResidual Deviance: 537.8655 \nAIC: 585.8655 \n\n\nYou get one coefficient for each variable (along the top) and for each response group (down the side), using the first group as a baseline everywhere. These numbers are hard to interpret; doing predictions is much easier.\n\\(\\blacksquare\\)\n\nDo a test to see whether Gender should stay in the model. (This will entail fitting another model.) What do you conclude?\n\nSolution\nThe other model to fit is the one without the variable you’re testing:\n\ngators.2 &lt;- update(gators.1, . ~ . - Gender)\n\n# weights:  30 (20 variable)\ninitial  value 352.466903 \niter  10 value 272.246275\niter  20 value 270.046891\nfinal  value 270.040139 \nconverged\n\n\nI did update here to show you that it works, but of course there’s no problem in just writing out the whole model again and taking out Gender, preferably by copying and pasting:\n\ngators.2x &lt;- multinom(Food.type ~ Size + Lake,\n  weights = Frequency, data = gators\n)\n\n# weights:  30 (20 variable)\ninitial  value 352.466903 \niter  10 value 272.246275\niter  20 value 270.046891\nfinal  value 270.040139 \nconverged\n\n\nand then you compare the models with and without Gender using anova:\n\nanova(gators.2, gators.1)\n\n\n\n  \n\n\n\nThe P-value is not small, so the two models fit equally well, and therefore we should go with the smaller, simpler one: that is, the one without Gender.\nSometimes drop1 works here too (and sometimes it doesn’t, for reasons I haven’t figured out):\n\ndrop1(gators.1, test = \"Chisq\")\n\ntrying - Gender \n\n\nError in if (trace) {: argument is not interpretable as logical\n\n\nI don’t even know what this error message means, never mind what to do about it.\n\\(\\blacksquare\\)\n\nPredict the probability that an alligator prefers each food type, given its size, gender (if necessary) and the lake it was found in, using the more appropriate of the two models that you have fitted so far. This means (i) making a data frame for prediction, and (ii) obtaining and displaying the predicted probabilities in a way that is easy to read.\n\nSolution\nOur best model gators.2 contains size and lake, so we need to predict for all combinations of those.\nFirst, get hold of those combinations, which you can do this way:\n\nnew &lt;- datagrid(model = gators.2, \n                Size = levels(factor(gators$Size)),\n                Lake = levels(factor(gators$Lake)))\nnew\n\n\n\n  \n\n\n\nThere are four lakes and two sizes, so we should (and do) have eight rows.\nNext, use this to make predictions, not forgetting the type = \"probs\" that you need for this kind of model. The last step puts the “long” predictions “wider” so that you can eyeball them:\n\ncbind(predictions(gators.2, newdata = new)) %&gt;% \n  select(group, estimate, Size, Lake) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate) -&gt; preds1\npreds1\n\n\n\n  \n\n\n\nI saved these to look at again later (you don’t need to).\nIf you thought that the better model was the one with Gender in it, or you otherwise forgot that you didn’t need Gender then you needed to include Gender in new also.\n\\(\\blacksquare\\)\n\nWhat do you think is the most important way in which the lakes differ? (Hint: look at where the biggest predicted probabilities are.)\n\nSolution\nHere are the predictions again:\n\npreds1\n\n\n\n  \n\n\n\nFollowing my own hint: the preferred diet in George and Hancock lakes is fish, but the preferred diet in Oklawaha and Trafford lakes is (at least sometimes) invertebrates. That is to say, the preferred diet in those last two lakes is less likely to be invertebrates than it is in the first two (comparing for alligators of the same size). This is true for both large and small alligators, as it should be, since there is no interaction in the model.\nThat will do, though you can also note that reptiles are more commonly found in the last two lakes, and birds sometimes appear in the diet in Hancock and Trafford but rarely in the other two lakes.\nAnother way to think about this is to hold size constant and compare lakes (and then check that it applies to the other size too). In this case, you’d find the biggest predictions among the first four rows, and then check that the pattern persists in the second four rows. (It does.)\nI think looking at predicted probabilities like this is the easiest way to see what the model is telling you.\n\\(\\blacksquare\\)\n\nHow would you describe the major difference between the diets of the small and large alligators?\n\nSolution\nSame idea: hold lake constant, and compare small and large, then check that your conclusion holds for the other lakes as it should. For example, in George Lake, the large alligators are more likely to eat fish, and less likely to eat invertebrates, compared to the small ones. The other food types are not that much different, though you might also note that birds appear more in the diets of large alligators than small ones. Does that hold in the other lakes? I think so, though there is less difference for fish in Hancock lake than the others (where invertebrates are rare for both sizes). Birds don’t commonly appear in any alligator’s diets, but where they do, they are commoner for large alligators than small ones.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#crimes-in-san-francisco-1",
    "href": "nominal-response.html#crimes-in-san-francisco-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.10 Crimes in San Francisco",
    "text": "26.10 Crimes in San Francisco\nThe data in link is a subset of a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are:\n\nDates: the date and time of the crime\nCategory: the category of crime, eg. “larceny” or “vandalism” (response).\nDescript: detailed description of crime.\nDayOfWeek: the day of the week of the crime.\nPdDistrict: the name of the San Francisco Police Department district in which the crime took place.\nResolution: how the crime was resolved\nAddress: approximate street address of crime\nX: longitude\nY: latitude\n\nOur aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which are the ones included in this data file.\nSome of the model-fitting takes a while (you’ll see why below). If you’re using R Markdown, you can wait for the models to fit each time you re-run your document, or insert #| cache: true below the top line of your code chunk (above the first line of actual code). What that does is to re-run that code chunk only if it changes; if it hasn’t changed it will use the saved results from last time it was run. That can save you a lot of waiting around.\n\nRead in the data and display the dataset (or, at least, part of it).\n\nSolution\nThe usual:\n\nmy_url &lt;- \"http://utsc.utoronto.ca/~butler/d29/sfcrime1.csv\"\nsfcrime &lt;- read_csv(my_url)\n\nRows: 359528 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Category, DayOfWeek, PdDistrict\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsfcrime\n\n\n\n  \n\n\n\nThis is a tidied-up version of the data, with only the variables we’ll look at, and only the observations from one of the “big four” crimes, a mere 300,000 of them. This is the data set we created earlier.\n\\(\\blacksquare\\)\n\nFit a multinomial logistic regression that predicts crime category from day of week and district. (You don’t need to look at it.) The model-fitting produces some output, which will at least convince you that it is working, since it takes some time.\n\nSolution\nThe modelling part is easy enough, as long as you can get the uppercase letters in the right places:\n\nsfcrime.1 &lt;- multinom(Category ~ DayOfWeek + PdDistrict, data=sfcrime)\n\n# weights:  68 (48 variable)\ninitial  value 498411.639069 \niter  10 value 430758.073422\niter  20 value 430314.270403\niter  30 value 423303.587698\niter  40 value 420883.528523\niter  50 value 418355.242764\nfinal  value 418149.979622 \nconverged\n\n\n\\(\\blacksquare\\)\n\nFit a model that predicts Category from only the district.\n\nSolution\nSame idea. Write it out, or use update:\n\nsfcrime.2 &lt;- update(sfcrime.1, . ~ . - DayOfWeek)\n\n# weights:  44 (30 variable)\ninitial  value 498411.639069 \niter  10 value 426003.543845\niter  20 value 425542.806828\niter  30 value 421715.787609\nfinal  value 418858.235297 \nconverged\n\n\n\\(\\blacksquare\\)\n\nUse anova to compare the two models you just obtained. What does the anova tell you?\n\nSolution\nThis:\n\nanova(sfcrime.2, sfcrime.1)\n\n\n\n  \n\n\n\nThis is a very small P-value. The null hypothesis is that the two models are equally good, and this is clearly rejected. We need the bigger model: that is, we need to keep DayOfWeek in there, because the pattern of crimes (in each district) differs over day of week.\nOne reason the P-value came out so small is that we have a ton of data, so that even a very small difference between days of the week could come out very strongly significant. The Machine Learning people (this is a machine learning dataset) don’t worry so much about tests for that reason: they are more concerned with predicting things well, so they just throw everything into the model and see what comes out.\n\\(\\blacksquare\\)\n\nUsing your preferred model, obtain predicted probabilities that a crime will be of each of these four categories for each day of the week in the TENDERLOIN district (the name is ALL CAPS). This will mean constructing a data frame to predict from, obtaining the predictions and then displaying them suitably.\n\nSolution\nUse datagrid first to get the combinations you want (and only those), namely all the days of the week, but only the district called TENDERLOIN.\nSo, let’s get the days of the week. The easiest way is to count them and ignore the counts:9\n\nsfcrime %&gt;% count(DayOfWeek) %&gt;% \n  pull(DayOfWeek) -&gt; daysofweek\ndaysofweek\n\n[1] \"Friday\"    \"Monday\"    \"Saturday\"  \"Sunday\"    \"Thursday\"  \"Tuesday\"  \n[7] \"Wednesday\"\n\n\nAnother way is the levels(factor()) thing you may have seen before:\n\nlevels(factor(sfcrime$DayOfWeek))\n\n[1] \"Friday\"    \"Monday\"    \"Saturday\"  \"Sunday\"    \"Thursday\"  \"Tuesday\"  \n[7] \"Wednesday\"\n\n\nNow we can use these in datagrid:10\n\nnew &lt;- datagrid(model = sfcrime.1, DayOfWeek = daysofweek, PdDistrict = \"TENDERLOIN\")\nnew\n\n\n\n  \n\n\n\nGood. And then predict for just these. This is slow, but not as slow as predicting for all districts. I’m saving the result of this slow part, so that it doesn’t matter if I change my mind later about what to do with it. I want to make sure that I don’t have to do it again, is all:11\n\np &lt;- cbind(predictions(sfcrime.1, newdata = new, type = \"probs\"))\np\n\n\n\n  \n\n\n\nThis, as you remember, is long format, so grab the columns you need from it and pivot wider. The columns you want to make sure you have are estimate, group (the type of crime), and the day of week:\n\np %&gt;% \n  select(group, estimate, DayOfWeek) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\nSuccess. (If you don’t get rid of enough, you still have 28 rows and a bunch of missing values; in that case, pivot_wider will infer that everything should be in its own row.)\n\\(\\blacksquare\\)\n\nDescribe briefly how the weekend days Saturday and Sunday differ from the rest.\n\nSolution\nThe days ended up in some quasi-random order, but Saturday and Sunday are still together, so we can still easily compare them with the rest. My take is that the last two columns don’t differ much between weekday and weekend, but the first two columns do: the probability of a crime being an assault is a bit higher on the weekend, and the probability of a crime being drug-related is a bit lower. I will accept anything reasonable supported by the predictions you got. We said there was a strongly significant day-of-week effect, but the changes from weekday to weekend are actually pretty small (but the changes from one weekday to another are even smaller). This supports what I guessed before, that with this much data even a small effect (the one shown here) is statistically significant.12\nExtra: I want to compare another district. What districts do we have?\n\nsfcrime %&gt;% count(PdDistrict)\n\n\n\n  \n\n\n\nThis is the number of our “big four” crimes committed in each district. Let’s look at the lowest-crime district RICHMOND. I copy and paste my code. Since I want to compare two districts, I include both:\n\nnew &lt;- datagrid(model = sfcrime.1, PdDistrict = c(\"TENDERLOIN\", \"RICHMOND\"), DayOfWeek = daysofweek)\nnew\n\n\n\n  \n\n\n\nand then as we just did. I’m going to be a bit more selective about the columns I keep this time, since the display will be a bit wider and I don’t want it to be too big for the page:\n\np &lt;- cbind(predictions(sfcrime.1, newdata = new, type = \"probs\"))\np\n\n\n\n  \n\n\n\n\np %&gt;% \n  select(group, estimate, DayOfWeek, PdDistrict) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\nRichmond is obviously not a drug-dealing kind of place; most of its crimes are theft of one kind or another. But the predicted effect of weekday vs. weekend is the same: Richmond doesn’t have many assaults or drug crimes, but it also has more assaults and fewer drug crimes on the weekend than during the week. There is not much effect of day of the week on the other two crime types in either place.\nThe consistency of pattern, even though the prevalence of the different crime types differs by location, is a feature of the model: we fitted an additive model, that says there is an effect of weekday, and independently there is an effect of location. The pattern over weekday is the same for each location, implied by the model. This may or may not be supported by the actual data.\nThe way to assess this is to fit a model with interaction (we will see more of this when we revisit ANOVA later), and compare the fit. This one takes longer to fit:\n\nsfcrime.3 &lt;- update(sfcrime.1, . ~ . + DayOfWeek * PdDistrict)\n\n# weights:  284 (210 variable)\ninitial  value 498411.639069 \niter  10 value 429631.807781\niter  20 value 429261.427210\niter  30 value 428111.625547\niter  40 value 423807.031450\niter  50 value 421129.496196\niter  60 value 420475.833895\niter  70 value 419523.235916\niter  80 value 418621.612920\niter  90 value 418147.629782\niter 100 value 418036.670485\nfinal  value 418036.670485 \nstopped after 100 iterations\n\n\nThis one didn’t actually complete the fitting process: it got to 100 times around and stopped (since that’s the default limit). We can make it go a bit further thus:\n\nsfcrime.3 &lt;- update(sfcrime.1, .~.+DayOfWeek*PdDistrict, maxit=300)\n\n# weights:  284 (210 variable)\ninitial  value 498411.639069 \niter  10 value 429631.807781\niter  20 value 429261.427210\niter  30 value 428111.625547\niter  40 value 423807.031450\niter  50 value 421129.496196\niter  60 value 420475.833895\niter  70 value 419523.235916\niter  80 value 418621.612920\niter  90 value 418147.629782\niter 100 value 418036.670485\niter 110 value 417957.337016\niter 120 value 417908.465189\niter 130 value 417890.580843\niter 140 value 417874.839492\niter 150 value 417867.449342\niter 160 value 417862.626213\niter 170 value 417858.654628\nfinal  value 417858.031854 \nconverged\n\nanova(sfcrime.1, sfcrime.3)\n\n\n\n  \n\n\n\nThis time, we got to the end. (The maxit=300 gets passed on to multinom, and says “go around up to 300 times if needed”.) As you will see if you try it, this takes a bit of time to run.\nThis anova is also strongly significant, but in the light of the previous discussion, the differential effect of day of week in different districts might not be very big. We can even assess that; we have all the machinery for the predictions, and we just have to apply them to this model. The only thing is waiting for it to finish!\n\np &lt;- cbind(predictions(sfcrime.3, newdata = new, type = \"probs\"))\np\n\n\n\n  \n\n\n\n\np %&gt;% \n  select(group, estimate, DayOfWeek, PdDistrict) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\nIt doesn’t look much different. Maybe the Tenderloin has a larger weekend increase in assaults than Richmond does.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#crimes-in-san-francisco-the-data-1",
    "href": "nominal-response.html#crimes-in-san-francisco-the-data-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.11 Crimes in San Francisco – the data",
    "text": "26.11 Crimes in San Francisco – the data\nThe data in link is a huge dataset of crimes committed in San Francisco between 2003 and 2015. The variables are:\n\nDates: the date and time of the crime\nCategory: the category of crime, eg. “larceny” or “vandalism” (response).\nDescript: detailed description of crime.\nDayOfWeek: the day of the week of the crime.\nPdDistrict: the name of the San Francisco Police Department district in which the crime took place.\nResolution: how the crime was resolved\nAddress: approximate street address of crime\nX: longitude\nY: latitude\n\nOur aim is to see whether the category of crime depends on the day of the week and the district in which it occurred. However, there are a lot of crime categories, so we will focus on the top four “interesting” ones, which we will have to discover.\n\nRead in the data and verify that you have these columns and a lot of rows. (The data may take a moment to read in. You will see why.)\n\nSolution\n\nmy_url &lt;- \"http://utsc.utoronto.ca/~butler/d29/sfcrime.csv\"\nsfcrime &lt;- read_csv(my_url)\n\nRows: 878049 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): Category, Descript, DayOfWeek, PdDistrict, Resolution, Address\ndbl  (2): X, Y\ndttm (1): Dates\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsfcrime\n\n\n\n  \n\n\n\nThose columns indeed, and pushing a million rows! That’s why it took so long!\nThere are also 39 categories of crime, so we need to cut that down some. There are only ten districts, however, so we should be able to use that as is.\n\\(\\blacksquare\\)\n\nHow is the response variable here different to the one in the question about steak preferences (and therefore why would multinom from package nnet be the method of choice)?\n\nSolution\nSteak preferences have a natural order, while crime categories do not. Since they are unordered, multinom is better than polr.\n\\(\\blacksquare\\)\n\nFind out which crime categories there are, and arrange them in order of how many crimes there were in each category.\n\nSolution\nThis can be the one you know, a group-by and summarize, followed by arrange to sort:\n\nsfcrime %&gt;% group_by(Category) %&gt;%\n  summarize(count=n()) %&gt;%\n  arrange(desc(count))\n\n\n\n  \n\n\n\nor this one does the same thing and saves a step:\n\nsfcrime %&gt;% count(Category) %&gt;%\n  arrange(desc(n))\n\n\n\n  \n\n\n\nFor this one, do the count step first, to see what you get. It produces a two-column data frame with the column of counts called n. So now you know that the second line has to be arrange(desc(n)), whereas before you tried count, all you knew is that it was arrange-desc-something.\nYou need to sort in descending order so that the categories you want to see actually do appear at the top.\n\\(\\blacksquare\\)\n\nWhich are the four most frequent “interesting” crime categories, that is to say, not including “other offenses” and “non-criminal”? Get them into a vector called my.crimes. See if you can find a way of doing this that doesn’t involve typing them in (for full marks).\n\nSolution\nThe most frequent interesting ones are, in order, larceny-theft, assault, drug-narcotic and vehicle theft. The fact that “other offenses” is so big indicates that there are a lot of possible crimes out there, and even 39 categories of crime isn’t enough. “Non-criminal” means, I think, that the police were called, but on arriving at the scene, they found that no law had been broken.\nI think the easy way to get the “top four” crimes out is to pull them out of the data frame that count produces. They are rows 1, 4, 5 and 6, so add a slice to your pipeline:\n\nmy.rows &lt;- c(1,4,5,6)\nsfcrime %&gt;% count(Category) %&gt;%\n  arrange(desc(n)) %&gt;%\n  slice(my.rows) %&gt;% pull(Category) -&gt; my.crimes\nmy.crimes\n\n[1] \"LARCENY/THEFT\" \"ASSAULT\"       \"DRUG/NARCOTIC\" \"VEHICLE THEFT\"\n\n\nI just want the Category column (as a vector), and pull is the way to get that. (If I don’t do pull, I get a data frame.)\nIf you can’t think of anything, just type them into a vector with c, or better, copy-paste them from the console. But that’s a last resort, and would cost you a point. If you do this, they have to match exactly, UPPERCASE and all.\n\\(\\blacksquare\\)\n\n(Digression, but needed for the next part.) The R vector letters contains the lowercase letters from a to z. Consider the vector ('a','m',3,'Q'). Some of these are found amongst the lowercase letters, and some not. Type these into a vector v and explain briefly why v %in% letters produces what it does.\n\nSolution\nThis is the ultimate “try it and see”:\n\nv=c('a','m',3,'Q')\nv %in% letters\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nThe first two elements of the answer are TRUE because lowercase-a and lowercase-m can be found in the lowercase letters somewhere. The other two are false because the number 3 and the uppercase-Q cannot be found anywhere in the lowercase letters.\nThe name is %in% because it’s asking whether each element of the first vector (one at a time) is in the set defined by the second thing: “is a a lowercase letter?” … is “Q a lowercase letter?” and getting the answers “yes, yes, no, no”.\n\\(\\blacksquare\\)\n\nWe are going to filter only the rows of our data frame that have one of the crimes in my.crimes as their Category. Also, select only the columns Category, DayOfWeek and PdDistrict. Save the resulting data frame and display its structure. (You should have a lot fewer rows than you did before.)\n\nSolution\nThe hard part about this is to get the inputs to %in% the right way around. We are testing the things in Category one at a time for membership in the set in my.crimes, so this:\n\nsfcrime %&gt;% filter(Category %in% my.crimes) %&gt;%\n  select(c(Category,DayOfWeek,PdDistrict)) -&gt; sfcrimea\nsfcrimea\n\n\n\n  \n\n\n\nI had trouble thinking of a good name for this one, so I put an “a” on the end. (I would have used a number, but I prefer those for models.)\nDown to a “mere” 359,000 rows. Don’t be stressed that the Category factor still has 39 levels (the original 39 crime categories); only four of them have any data in them:\n\nsfcrimea %&gt;% count(Category)\n\n\n\n  \n\n\n\nSo all of the crimes that are left are one of the four Categories we want to look at.\n\\(\\blacksquare\\)\n\nSave these data in a file sfcrime1.csv.\n\nSolution\nThis is write_csv again:\n\nwrite_csv(sfcrimea,\"sfcrime1.csv\")\n\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#what-sports-do-these-athletes-play-1",
    "href": "nominal-response.html#what-sports-do-these-athletes-play-1",
    "title": "26  Logistic regression with nominal response",
    "section": "26.12 What sports do these athletes play?",
    "text": "26.12 What sports do these athletes play?\nThe data at link are physical and physiological measurements of 202 male and female Australian elite athletes. The data values are separated by tabs. We are going to see whether we can predict the sport an athlete plays from their height and weight.\nThe sports, if you care, are respectively basketball, “field athletics” (eg. shot put, javelin throw, long jump etc.), gymnastics, netball, rowing, swimming, 400m running, tennis, sprinting (100m or 200m running), water polo.\n\nRead in the data and display the first few rows.\n\nSolution\nThe data values are separated by tabs, so read_tsv is the thing:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ais.txt\"\nathletes &lt;- read_tsv(my_url)\n\nRows: 202 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nathletes\n\n\n\n  \n\n\n\nIf you didn’t remember that, this also works:\n\nathletes &lt;- read_delim(my_url, \"\\t\")\n\nRows: 202 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n(this is the R way of expressing “tab”.)\n\\(\\blacksquare\\)\n\nMake a scatterplot of height vs. weight, with the points coloured by what sport the athlete plays. Put height on the \\(x\\)-axis and weight on the \\(y\\)-axis.\n\nSolution\nI’m doing this to give you a little intuition for later:\n\nggplot(athletes, aes(x = Ht, y = Wt, colour = Sport)) + geom_point()\n\n\n\n\nThe reason for giving you the axes to use is (i) neither variable is really a response, so it doesn’t actually matter which one goes on which axis, and (ii) I wanted to give the grader something consistent to look at.\n\\(\\blacksquare\\)\n\nExplain briefly why a multinomial model (multinom from nnet) would be the best thing to use to predict sport played from the other variables.\n\nSolution\nThe categories of Sport are not in any kind of order, and there are more than two of them. That’s really all you needed, for which two marks was kind of generous.\n\\(\\blacksquare\\)\n\nFit a suitable model for predicting sport played from height and weight. (You don’t need to look at the results.) 100 steps isn’t quite enough, so set maxit equal to a larger number to allow the estimation to finish.\n\nSolution\n120 steps is actually enough, but any number larger than 110 is fine. It doesn’t matter if your guess is way too high. Like this:\n\nlibrary(nnet)\nsport.1 &lt;- multinom(Sport ~ Ht + Wt, data = athletes, maxit = 200)\n\n# weights:  40 (27 variable)\ninitial  value 465.122189 \niter  10 value 410.089598\niter  20 value 391.426721\niter  30 value 365.829150\niter  40 value 355.326457\niter  50 value 351.255395\niter  60 value 350.876479\niter  70 value 350.729699\niter  80 value 350.532323\niter  90 value 350.480130\niter 100 value 350.349271\niter 110 value 350.312029\nfinal  value 350.311949 \nconverged\n\n\nAs long as you see the word converged at the end, you’re good.\n\\(\\blacksquare\\)\n\nDemonstrate using anova that Wt should not be removed from this model.\n\nSolution\nThe idea is to fit a model without Wt, and then show that it fits significantly worse. This converges in less than 100 iterations, so you can have maxit or not as you prefer:\n\nsport.2 &lt;- update(sport.1, . ~ . - Wt)\n\n# weights:  30 (18 variable)\ninitial  value 465.122189 \niter  10 value 447.375728\niter  20 value 413.597441\niter  30 value 396.685596\niter  40 value 394.121380\niter  50 value 394.116993\niter  60 value 394.116434\nfinal  value 394.116429 \nconverged\n\nanova(sport.2, sport.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe P-value is very small indeed, so the bigger model sport.1 is definitely better (or the smaller model sport.2 is significantly worse, however you want to say it). So taking Wt out is definitely a mistake.\nThis is what I would have guessed (I actually wrote the question in anticipation of this being the answer) because weight certainly seems to help in distinguishing the sports. For example, the field athletes seem to be heavy for their height compared to the other athletes (look back at the graph you made).\ndrop1, the obvious thing, doesn’t work here:\n\ndrop1(sport.1, test = \"Chisq\", trace = T)\n\ntrying - Ht \n\n\nError in if (trace) {: argument is not interpretable as logical\n\n\nI gotta figure out what that error is. Does step?\n\nstep(sport.1, direction = \"backward\", test = \"Chisq\")\n\nStart:  AIC=754.62\nSport ~ Ht + Wt\n\ntrying - Ht \n# weights:  30 (18 variable)\ninitial  value 465.122189 \niter  10 value 441.367394\niter  20 value 381.021649\niter  30 value 380.326030\nfinal  value 380.305003 \nconverged\ntrying - Wt \n# weights:  30 (18 variable)\ninitial  value 465.122189 \niter  10 value 447.375728\niter  20 value 413.597441\niter  30 value 396.685596\niter  40 value 394.121380\niter  50 value 394.116993\niter  60 value 394.116434\nfinal  value 394.116429 \nconverged\n       Df      AIC\n&lt;none&gt; 27 754.6239\n- Ht   18 796.6100\n- Wt   18 824.2329\n\n\nCall:\nmultinom(formula = Sport ~ Ht + Wt, data = athletes, maxit = 200)\n\nCoefficients:\n        (Intercept)         Ht          Wt\nField      59.98535 -0.4671650  0.31466413\nGym       112.49889 -0.5027056 -0.57087657\nNetball    47.70209 -0.2947852  0.07992763\nRow        35.90829 -0.2517942  0.14164007\nSwim       36.82832 -0.2444077  0.10544986\nT400m      32.73554 -0.1482589 -0.07830622\nTennis     41.92855 -0.2278949 -0.01979877\nTSprnt     51.43723 -0.3359534  0.12378285\nWPolo      23.35291 -0.2089807  0.18819526\n\nResidual Deviance: 700.6239 \nAIC: 754.6239 \n\n\nCuriously enough, it does. The final model is the same as the initial one, telling us that neither variable should be removed.\n\\(\\blacksquare\\)\n\nMake a data frame consisting of all combinations of Ht 160, 180 and 200 (cm), and Wt 50, 75, and 100 (kg), and use it to obtain predicted probabilities of athletes of those heights and weights playing each of the sports. Display the results. You might have to display them smaller, or reduce the number of decimal places13 to fit them on the page.\n\nSolution\nTo get all combinations of those heights and weights, use datagrid:\n\nnew &lt;- datagrid(model = sport.1, Ht = c(160, 180, 200), Wt = c(50, 75, 100))\nnew\n\n\n\n  \n\n\n\n(check: \\(3 \\times 3 = 9\\) rows.)\nThen predict:\n\np &lt;- cbind(predictions(sport.1, newdata = new, type = \"probs\"))\np\n\n\n\n  \n\n\n\nI saved these, because I want to talk about what to do next. The normal procedure is to say that there is a prediction for each group (sport), so we want to grab only the columns we need and pivot wider. But, this time there are 10 sports (see how there are \\(9 \\times 10 = 90\\) rows, with a predicted probability for each height-weight combo for each sport). I suggested reducing the number of decimal places in the predictions; the time to do that is now, while they are all in one column (rather than trying to round a bunch of columns all at once, which is doable but more work). Hence:\n\np %&gt;% \n  mutate(estimate = round(estimate, 2)) %&gt;% \n  select(group, estimate, Ht, Wt) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate)\n\n\n\n  \n\n\n\nThis works (although even then it might not all fit onto the page and you’ll have to scroll left and right to see everything).\nIf you forget to round until the end, you’ll have to do something like this:\n\np %&gt;% \n  select(group, estimate, Ht, Wt) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate) %&gt;% \n  mutate(across(BBall:WPolo, \\(x) round(x, 2)))\n\n\n\n  \n\n\n\nIn words, “for each column from BBall through WPolo, replace it by itself rounded to 2 decimals”.\nThere’s nothing magic about two decimals; three or maybe even four would be fine. A small enough number that you can see most or all of the columns at once, and easily compare the numbers for size (which is hard when some of them are in scientific notation).\nExtra: you can even abbreviate the sport names, like this:\n\np %&gt;% \n  mutate(estimate = round(estimate, 2),\n         group = abbreviate(group, 3)) %&gt;% \n  select(group, estimate, Ht, Wt) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate) \n\n\n\n  \n\n\n\nOnce again, the time to do this is while everything is long, so that you only have one column to work with.14 Again, you can do this at the end, but it’s more work, because you are now renaming columns:\n\np %&gt;% \n  mutate(estimate = round(estimate, 2)) %&gt;% \n  select(group, estimate, Ht, Wt) %&gt;% \n  pivot_wider(names_from = group, values_from = estimate) %&gt;% \n  rename_with(\\(x) abbreviate(x, 3), everything())\n\n\n\n  \n\n\n\nrename doesn’t work with across (you will get an error if you try it), so you need to use rename_with15. This has syntax “how to rename” first (here, with an abbreviated version of itself), and “what to rename” second (here, everything, or BBall through WPolo if you prefer).\n\\(\\blacksquare\\)\n\nFor an athlete who is 180 cm tall and weighs 100 kg, what sport would you guess they play? How sure are you that you are right? Explain briefly.\n\nSolution\nFind this height and weight in your predictions (it’s row 6). Look along the line for the highest probability, which is 0.85 for Field (that is, field athletics). All the other probabilities are much smaller (the biggest of the others is 0.06). So this means we would guess the athlete to be a field athlete, and because the predicted probability is so big, we are very likely to be right. This kind of thought process is characteristic of discriminant analysis, which we’ll see more of later in the course. Compare that with the scatterplot you drew earlier: the field athletes do seem to be distinct from the rest in terms of height-weight combination. Some of the other height-weight combinations are almost equally obvious: for example, very tall people who are not very heavy are likely to play basketball. 400m runners are likely to be of moderate height but light weight. Some of the other sports, or height-weight combinations, are difficult to judge. Consider also that we have mixed up males and females in this data set. We might gain some clarity by including Sex in the model, and also in the predictions. But I wanted to keep this question relatively simple for you, and I wanted to stop from getting unreasonably long. (You can decide whether you think it’s already too long.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "nominal-response.html#footnotes",
    "href": "nominal-response.html#footnotes",
    "title": "26  Logistic regression with nominal response",
    "section": "",
    "text": "For this, use round.↩︎\nIf you do not take out the NA values, they are shown separately on the end of the summary for that column.↩︎\nThe American Green party is more libertarian than Green parties elsewhere.↩︎\nNorthern Ireland’s political parties distinguish themselves by politics and religion. Northern Ireland has always had political tensions between its Protestants and its Catholics.↩︎\nThere are three political parties; using the first as a baseline, there are therefore \\(3-1=2\\) coefficients for each variable.↩︎\nIt amuses me that Toronto’s current (2021) mayor, named Tory, is politically a Tory.↩︎\nWhen you have variables that are categories, you might have more than one individual with exactly the same categories; on the other hand, if they had measured Size as, say, length in centimetres, it would have been very unlikely to get two alligators of exactly the same size.↩︎\nDiscovered by me two minutes ago.↩︎\nI know I just said not to do this kind of thing, but counting is really fast, while unnecessary predictions are really slow.↩︎\nThere is only one district, so we can just put that in here.↩︎\nThe same thing applies if you are doing something like webscraping, that is to say downloading stuff from the web that you then want to do something with. Download it once and save it, then you can take as long as you need to decide what you’re doing with it.↩︎\nStatistical significance as an idea grew up in the days before “big data”.↩︎\nFor this, use round.↩︎\nabbreviate comes from base R; it returns an abbreviated version of its (text) input, with a minimum length as its second input. If two abbreviations come out the same, it makes them longer until they are different. It has some heuristics to get things short enough: remove vowels, then remove letters on the end.↩︎\nWhich I learned about today.↩︎"
  },
  {
    "objectID": "survival-analysis.html#the-worcester-survey",
    "href": "survival-analysis.html#the-worcester-survey",
    "title": "27  Survival analysis",
    "section": "27.1 The Worcester survey",
    "text": "27.1 The Worcester survey\nThe Worcester survey was a long-term study of all myocardial-infarction1 victims admitted to hospitals in the Worcester, Massachusetts area.2 The data have been well studied, and can be found in the file link.\n\nRead the data and display the first few rows of the data frame. You might get an extra column, which you can ignore. For your information, the variables are:\n\n\npatient ID code\nadmission date\ndate of last followup (this is the date of death if the patient died)\nlength of hospital stay (days)\nfollowup time (days) (time between admission and last followup)\nfollowup status: 1=dead, 0=alive\nAge in years (at admission)\ngender (0=male, 1=female)\nbody mass index (kg/m\\(^2\\))\n\n\nCreate a suitable response variable for a Cox proportional hazards model for time of survival, using the followup time and followup status.\nFit a Cox proportional hazards model predicting survival time from age, gender and BMI. Obtain the summary (but you don’t need to comment on it yet).\nTest the overall fit of the model. What does the result mean?\nCan any of your explanatory variables be removed from the model? Explain briefly.\nRemove your most non-significant explanatory variable from the model and fit again. Take a look at the results. Are all your remaining explanatory variables significant? (If all your explanatory variables were previously significant, you can skip this part.)\nCalculate the 1st quartile, median, and 3rd quartiles of age and BMI. (quantile.) Round these off to the nearest whole number. (Do the rounding off yourself, though R has a function round that does this, which you can investigate if you want.) As an alternative, you can get these by passing the whole data frame, or the columns of it you want, into summary.\nMake a data frame out of all the combinations of age and BMI values (that you obtained in the previous part) suitable for predicting with.\nObtain predicted survival probabilities for each of the values in your new data frame. Use your best model. (You don’t need to look at the results, though you can if you want to.)\nMake a graph depicting the survival curves from survfit with different colours distinguishing the different survival curves.\nWhat is the effect of age on survival? What is the effect of BMI on survival? Explain briefly. (You will have to disentangle the meaning of the different coloured lines on the plot to do this.)"
  },
  {
    "objectID": "survival-analysis.html#drug-treatment-programs",
    "href": "survival-analysis.html#drug-treatment-programs",
    "title": "27  Survival analysis",
    "section": "27.2 Drug treatment programs",
    "text": "27.2 Drug treatment programs\nOne of the goals of drug treatment programs is to lengthen the time until the patient returns to using drugs. (It is not generally possible to prevent patients from ever using drugs again.) In one study, over 600 former drug users took part. Two different programs, a short program and a long program, were offered at two different sites, labelled A and B. The data can be found in link. The variables are these:\n\nID: patient ID number\nage: patient age at enrollment into the study\nndrugtx: number of previous drug treatments\ntreat: 0 for short treatment program, 1 for long program\nsite: 0 for site A, 1 for site B\ntime: time until return to drug use\ncensor: whether the subject returned to drug use (1) or not (0) during the follow-up period\nherco: whether subject used heroine or cocaine in the last 3 months: 1 is both, 2 is one (either heroine or cocaine), 3 is neither.\n\n\nRead in the data and check in one way or another that you have what was promised above.\nThere are some missing values in the dataframe. Demonstrate this using summary. Pipe the dataframe into drop_na and show that they have gone. (drop_na removes all rows that have missing values in them.)\nSome of these variables are recorded as numbers but are actually categorical. Which ones? Re-define these variables in your data frame so that they have sensible (text) values.\nCreate a suitable reponse variable for a Cox proportional hazards regression that predicts time until return to drug use from the other variables. This requires some care, because you need to be sure about what the censoring variable actually represents and what you need it to represent.\nLook at the first few values of your response variable. Why is the fifth one marked with a +? Explain briefly.\nFit a Cox proportional hazards model, predicting from all the other variables (except for row and ID) that you haven’t used yet. Display the results.\nFind which explanatory variables can be removed at \\(\\alpha=0.05\\) (there should be two of them). Bear in mind that we have categorical variables, so that looking at the output from summary is not enough.\nRemove all the non-significant explanatory variables and re-fit your model. By carrying out a suitable test demonstrate that your smaller model is the better one.\n* Display your better model. Are all of the explanatory variables significant? Do their slope coefficients have sensible signs (plus or minus), based on what you know or can guess about drug treatments? Explain briefly.\nWe have three variables left in our model, age, ndrugtx and treat. The quartiles of age are 27 and 37, the quartiles of ndrugtx are 1 and 6, and the two possible values of treat are short and long. Create a data frame with variables of these names and all possible combinations of their values (so there should be 8 rows in the resulting data frame). Display the resulting data frame.\nObtain predicted survival probabilities for each of the values of age, ndrugtx and treat used in the previous part. You don’t need to display it (we are going to plot it shortly).\nPlot your predicted survival curves.\nWhich of your combinations of values is predicted to take the longest to return to drug use? Which is predicted to take the shortest time? Explain briefly.\nAre your survival curve plot and your conclusions from part (here) consistent, or not? Explain briefly."
  },
  {
    "objectID": "survival-analysis.html#multiple-myeloma",
    "href": "survival-analysis.html#multiple-myeloma",
    "title": "27  Survival analysis",
    "section": "27.3 Multiple myeloma",
    "text": "27.3 Multiple myeloma\nMultiple myeloma is a kind of cancer. It forms in a plasma cell (which is a type of white blood cell). It causes cancer cells to accumulate in the bone marrow, where they crowd out healthy blood cells. Plasma cells make antibodies (to help fight infections), while the cancer cells don’t: they produce abnormal proteins that can cause kidney problems. (This adapted from link.) The variables are:\n\ntime: survival time from diagnosis (months)\nvstatus: 0=alive, 1=dead at end of study\nlogbun: log of BUN test score (BUN test is a test of kidney function, not to be confused with cha siu bao3).\nhgb: hemoglobin (at diagnosis).\nplatelet: platelets: 1=normal, 0=abnormal (at diagnosis).\nage at diagnosis, in years\nlogwbc: log of WBC (white blood cell count, at diagnosis)\nfrac: fractures at diagnosis (0=absent, 1=present)\nlogpbm: log of percent of plasma cells in bone marrow\nprotein: proteinuria (protein in urine) at diagnosis. Most people have very little, so a larger than normal amount indicates illness of some kind.\nscalc: serum calcium at diagnosis.\n\nThe data, on 65 patients with multiple myeloma, are in link. Some of the variables are logs because they could take very large values.\nThere are a lot of parts here, but each part is supposed to be short.\n\nRead in the data and display (some of) the values. Confirm that you have the right number of observations and the right variables.\nCreate a suitable response variable for a Cox proportional-hazards survival model, bearing in mind that the “event” here is death. Display your response variable, and explain briefly what the + signs attached to some of the values mean, without using a technical term.\nWhat is the technical term for those patients that have a + by their values for the response variable?\nFit a Cox proportional-hazards survival model predicting your response variable from all the other variables (except for the ones that you used to make the response variable). Display the summary of your model.\nIn your model, which explanatory variables have a P-value less than 0.10? Fit a model containing only those and display the results.\nDo a test to compare the two models that you fit. Why do you prefer the second model? Explain briefly.\nThere should be two explanatory variables left in your model. These are both numerical variables. Find their first and third quartiles, any way you like.\nCreate a data frame containing all possible combinations of the two quartiles for each of the two variables, and display the result.\nObtain predicted survival probabilities for each of the combinations of variables you created above. You don’t need to look at the results (they are rather long).\nObtain a graph of the predicted survival curves for each combination of your variables.\nIs it better to have high or low values for each of the variables in your prediction? Explain briefly."
  },
  {
    "objectID": "survival-analysis.html#ovarian-cancer",
    "href": "survival-analysis.html#ovarian-cancer",
    "title": "27  Survival analysis",
    "section": "27.4 Ovarian cancer",
    "text": "27.4 Ovarian cancer\nR’s survival package contains several data sets. One of these is called ovarian; it comes from a study of 26 ovarian cancer patients. The major purpose of this study was to compare the effects of two treatments on survival time.\n\nObtain and display (all of) the data set. This is as simple as loading the package and typing the data set’s name.\nThe columns of interest to us are:\n\n\nfutime: the time for which a patient was followed-up: the number of days until either they died or the study ended (or they withdrew from the study for some other reason).\nfustat: follow-up status: 1 if the patient died of ovarian cancer, 0 if they were still alive when the study ended.\nage: of patient, at diagnosis, in years\nrx: treatment, numbered 1 or 2, but really labels for the two treatments.\n\nCreate and display a suitable response variable y for a Cox proportional-hazards model.\n\nIn the display of your response variable, some values are marked with a +. Why is that? Explain briefly. (If you use a technical term, you should explain what it means.)\nFit a Cox proportional-hazards model for predicting survival time from age and treatment. Note that the numeric values for treatment make sense only as labels for the two treatments, so in your model formula make treatment into a factor. Display the results.\nIs there a significant difference between the treatments in terms of their effects on survival (from ovarian cancer)?\nIs there a significant effect of age? If there is, describe the effect that age has on survival.\nMake a martingale residual plot for this model. Do you see any problems? Explain briefly.\nFind the quartiles of age, and make a data frame containing all combinations of those two ages and the two treatments. Display what you have. (Feel free to copy the values by hand, rather than trying to save them and use them.)\nObtain predicted survival probabilities for each of your age-treatment combinations, for each of a variety of survival times. (This is only one thing, despite it sounding like a lot.)\nDraw a plot that compares the survival probabilities at the different times.\nAccording to your plot, how would you describe the effects of treatment and of age?\n\nMy solutions follow:"
  },
  {
    "objectID": "survival-analysis.html#the-worcester-survey-1",
    "href": "survival-analysis.html#the-worcester-survey-1",
    "title": "27  Survival analysis",
    "section": "27.5 The Worcester survey",
    "text": "27.5 The Worcester survey\nThe Worcester survey was a long-term study of all myocardial-infarction4 victims admitted to hospitals in the Worcester, Massachusetts area.5 The data have been well studied, and can be found in the file link.\n\nRead the data and display the first few rows of the data frame. You might get an extra column, which you can ignore. For your information, the variables are:\n\n\npatient ID code\nadmission date\ndate of last followup (this is the date of death if the patient died)\nlength of hospital stay (days)\nfollowup time (days) (time between admission and last followup)\nfollowup status: 1=dead, 0=alive\nAge in years (at admission)\ngender (0=male, 1=female)\nbody mass index (kg/m\\(^2\\))\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/whas100.csv\"\nwhas100 &lt;- read_csv(my_url)\n\nNew names:\nRows: 100 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(2): admitdate, foldate dbl (8): ...1, id, los, lenfol, fstat, age, gender, bmi\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nwhas100\n\n\n\n  \n\n\n\nI seem to have an extra column called X1. This is because I saved my version of the data using the old write.csv, which comes with row names, and I forgot to get rid of them. These came back as an extra unnamed variable to which read_delim gave the name X1.\n\\(\\blacksquare\\)\n\nCreate a suitable response variable for a Cox proportional hazards model for time of survival, using the followup time and followup status.\n\nSolution\nSurv. The event here is death, so the two parts of the response variable are followup time lenfol and followup status, 1 being “dead”, fstat:\n\ny &lt;- with(whas100, Surv(lenfol, fstat == 1))\ny\n\n  [1]    6   374  2421    98  1205  2065  1002  2201   189  2719+ 2638+  492 \n [13]  302  2574+ 2610+ 2641+ 1669  2624  2578+ 2595+  123  2613+  774  2012 \n [25] 2573+ 1874  2631+ 1907   538   104     6  1401  2710   841   148  2137+\n [37] 2190+ 2173+  461  2114+ 2157+ 2054+ 2124+ 2137+ 2031  2003+ 2074+  274 \n [49] 1984+ 1993+ 1939+ 1172    89   128  1939+   14  1011  1497  1929+ 2084+\n [61]  107   451  2183+ 1876+  936   363  1048  1889+ 2072+ 1879+ 1870+ 1859+\n [73] 2052+ 1846+ 2061+ 1912+ 1836+  114  1557  1278  1836+ 1916+ 1934+ 1923+\n [85]   44  1922+  274  1860+ 1806  2145+  182  2013+ 2174+ 1624   187  1883+\n [97] 1577    62  1969+ 1054 \n\n\nJust using fstat alone as the second thing in Surv also works, because anything that gives TRUE or 1 when the event (death) occurs is equally good. (In R, TRUE as a number is 1 and FALSE as a number is 0.)\nI listed the values by way of checking. The ones with a + are censored: that is, the patient was still alive the last time the doctor saw them. Most of the censored values are longer times. Usually this happens because the patient was still alive at the end of the study.\nThis is perhaps now the old way of doing it, because you can now create y as a new column in your dataframe:\n\nwhas100 %&gt;% mutate(y = Surv(lenfol, fstat == 1))\n\n\n\n  \n\n\n\nIf you scroll across, this has a column y that contains the same values as the stand-alone y we defined earlier, including plus signs for censored ones. At the top of the column is an indication that this is a Surv object: that is, not just a column of numbers, but something that also contains censorship information.\n\\(\\blacksquare\\)\n\nFit a Cox proportional hazards model predicting survival time from age, gender and BMI. Obtain the summary (but you don’t need to comment on it yet).\n\nSolution\nThis, using the response variable that we just created:\n\nwhas100.1 &lt;- coxph(y ~ age + gender + bmi, data = whas100)\nsummary(whas100.1)\n\nCall:\ncoxph(formula = y ~ age + gender + bmi, data = whas100)\n\n  n= 100, number of events= 51 \n\n           coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nage     0.03713   1.03783  0.01272  2.918  0.00352 **\ngender  0.14325   1.15402  0.30604  0.468  0.63973   \nbmi    -0.07083   0.93162  0.03607 -1.964  0.04956 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\nage       1.0378     0.9636    1.0123    1.0640\ngender    1.1540     0.8665    0.6334    2.1024\nbmi       0.9316     1.0734    0.8680    0.9999\n\nConcordance= 0.683  (se = 0.037 )\nLikelihood ratio test= 21.54  on 3 df,   p=8e-05\nWald test            = 19.46  on 3 df,   p=2e-04\nScore (logrank) test = 20.82  on 3 df,   p=1e-04\n\n\n\\(\\blacksquare\\)\n\nTest the overall fit of the model. What does the result mean?\n\nSolution\nLook at those three P-values at the bottom. They are all small, so something in the model is helping to predict survival. As to what? Well, that’s the next part.\n\\(\\blacksquare\\)\n\nCan any of your explanatory variables be removed from the model? Explain briefly.\n\nSolution\ngender has a (very) large P-value, so that can be taken out of the model. The other two variables have small P-values (bmi only just under 0.05), so they need to stay. The other way to think about this is step, or drop1:\n\ndrop1(whas100.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThis is here equivalent to6 the output from summary, but where it scores is if you have a categorical explanatory variable like “treatment” with more than two levels: drop1 will tell you about keeping or dropping it as a whole.7\nIf you prefer:\n\nstep(whas100.1, trace = 0, test = \"Chisq\")\n\nCall:\ncoxph(formula = y ~ age + bmi, data = whas100)\n\n        coef exp(coef) se(coef)      z        p\nage  0.03927   1.04005  0.01187  3.309 0.000938\nbmi -0.07116   0.93131  0.03614 -1.969 0.048952\n\nLikelihood ratio test=21.32  on 2 df, p=2.346e-05\nn= 100, number of events= 51 \n\n\ngender comes out, but the others stay. As usual, put trace=1 or trace=2 to get more output, which will look like a sequence of drop1’s one after the other.\n\\(\\blacksquare\\)\n\nRemove your most non-significant explanatory variable from the model and fit again. Take a look at the results. Are all your remaining explanatory variables significant? (If all your explanatory variables were previously significant, you can skip this part.)\n\nSolution\nSo, take out gender:\n\nwhas100.2 &lt;- update(whas100.1, . ~ . - gender)\nsummary(whas100.2)\n\nCall:\ncoxph(formula = y ~ age + bmi, data = whas100)\n\n  n= 100, number of events= 51 \n\n        coef exp(coef) se(coef)      z Pr(&gt;|z|)    \nage  0.03927   1.04005  0.01187  3.309 0.000938 ***\nbmi -0.07116   0.93131  0.03614 -1.969 0.048952 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    exp(coef) exp(-coef) lower .95 upper .95\nage    1.0401     0.9615    1.0161    1.0645\nbmi    0.9313     1.0738    0.8676    0.9997\n\nConcordance= 0.681  (se = 0.037 )\nLikelihood ratio test= 21.32  on 2 df,   p=2e-05\nWald test            = 19  on 2 df,   p=7e-05\nScore (logrank) test = 19.99  on 2 df,   p=5e-05\n\n\nBoth explanatory variables are significant: age definitely, bmi only just. This is the same model as step gave me.\n\\(\\blacksquare\\)\n\nCalculate the 1st quartile, median, and 3rd quartiles of age and BMI. (quantile.) Round these off to the nearest whole number. (Do the rounding off yourself, though R has a function round that does this, which you can investigate if you want.) As an alternative, you can get these by passing the whole data frame, or the columns of it you want, into summary.\n\nSolution\n\nquantile(whas100$age)\n\n   0%   25%   50%   75%  100% \n32.00 59.75 71.00 80.25 92.00 \n\nquantile(whas100$bmi)\n\n      0%      25%      50%      75%     100% \n14.91878 23.53717 27.19158 30.34770 39.93835 \n\n\nor\n\nwhas100 %&gt;%\n  select(age, bmi) %&gt;%\n  summary()\n\n      age             bmi       \n Min.   :32.00   Min.   :14.92  \n 1st Qu.:59.75   1st Qu.:23.54  \n Median :71.00   Median :27.19  \n Mean   :68.25   Mean   :27.04  \n 3rd Qu.:80.25   3rd Qu.:30.35  \n Max.   :92.00   Max.   :39.94  \n\n\nOr, pure tidyverse: summarize all the columns (after you’ve done the select).\n\nwhas100 %&gt;%\n  select(age, bmi) %&gt;%\n  reframe(across(everything(), \\(x) quantile(x)))\n\n\n\n  \n\n\n\nThis one is reframe rather than summarize because the “summaries” are five numbers rather than one.\nUsing whichever of this multitude of ways appeals to you:\n60, 71 and 80 for age, 24, 27 and 30 for BMI.\n\\(\\blacksquare\\)\n\nMake a data frame out of all the combinations of age and BMI values (that you obtained in the previous part) suitable for predicting with.\n\nSolution\nThe inevitable datagrid. This is probably quickest, with the best model being the second one:\n\nwhas100.new &lt;- datagrid(model = whas100.2, age = c(60, 71, 80), bmi = c(24, 27, 30))\nwhas100.new\n\n\n\n  \n\n\n\nExtra: I set it up this way so that you would find the median and quartiles and then type the values into the datagrid (easier conceptually), but there is nothing stopping us doing it all in one step:\n\ndatagrid(model = whas100.2, \n         age = quantile(whas100$age, c(0.25, 0.5, 0.75)),\n         bmi = quantile(whas100$bmi, c(0.25, 0.5, 0.75))) %&gt;% \n  mutate(across(everything(), \\(x) round(x)))\n\n\n\n  \n\n\n\nThe last line rounds everything off to the (default) 0 decimal places. The repetitiousness of the preceding two lines makes me wonder whether I should have written a function.\n\\(\\blacksquare\\)\n\nObtain predicted survival probabilities for each of the values in your new data frame. Use your best model. (You don’t need to look at the results, though you can if you want to.)\n\nSolution\nThe magic word is survfit (which plays the role of predictions here). The best model is whas100.2, with the non-significant gender removed:\n\npp2 &lt;- survfit(whas100.2, whas100.new, data = whas100)\n\nThis doesn’t need the data= at the end (it works perfectly well without), but the plot (later) seems to need it to be there. I think the plot needs the information from the original data to be in the predictions somewhere.\nThis is kind of long to look at (summary(pp2) would be the thing), so we will need to make a graph of it. I gave it a name, since I want to use it again later.\n\\(\\blacksquare\\)\n\nMake a graph depicting the survival curves from survfit with different colours distinguishing the different survival curves.\n\nSolution\nThis is actually easy once you work out what to do:\n\nggsurvplot(pp2, conf.int = FALSE)\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nℹ Please use `gather()` instead.\nℹ The deprecated feature was likely used in the survminer package.\n  Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n\n\n\n\n\nWithout the conf.int thing, you get confidence intervals for each survival curve, which overlap each other and generally make the plot look messy.\nThe “strata” are the different age-BMI combinations that you predicted for, so it’s usually a good idea to list the “new” prediction data frame, either here or when you assess the effects of the variables (next part) so that you can see which is which:\n\nwhas100.new\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nWhat is the effect of age on survival? What is the effect of BMI on survival? Explain briefly. (You will have to disentangle the meaning of the different coloured lines on the plot to do this.)\n\nSolution\nBear in mind that up-and-to-the-right is best for a survival curve, since that means that people in the upper-right group have a higher chance of surviving for longer.\nThe best survival curve is therefore the olive-green one. According to the legend, this goes with stratum 3, which is (according to the listing of whas100.new) age 60 (the youngest) and BMI 30 (the highest). So it looks as if the best survival goes with a lower age (not surprising) and a higher BMI (surprising; see discussion about BMI below).\nYou can also leave one variable constant and see the effects of changing the other one. Let’s pick the oldest age 80: the BMI values are 24 (stratum 7, blue), 27 (stratum 8, purple), 30 (stratum 9, pink). These survival curves are the bottom one, the second bottom one, and the fourth bottom one. At this age, survival chances are not great, but having a higher BMI goes with a greater chance of surviving longer.\nOr pick a BMI, say 30. These are strata 3 (olive green), 6 (light blue) and 9 (pink) respectively for ages 60, 71 and 80. These are the best, 3rd best and 5th best survival curves; that is, as age increases, the chance of surviving a long time decreases.\nThe effect of BMI, though, seems backwards: a higher BMI is associated with a higher chance of survival.\nThat’s the end of what I wanted you to do, but:\nA higher BMI is usually associated with being obese (and therefore unhealthy), so you’d expect the effect of BMI to be the other way around. According to Wikipedia (link), the BMI values here are “overweight” or close to it. Maybe being heavier helps the body recover from a heart attack.\nLet’s start with the martingale residual plot:\n\nggcoxdiagnostics(whas100.2) + geom_smooth()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThere is a suspicion of bendiness here, though the left side of the curve is entirely because of that one positive residual on the left. In any case, this suggests that nonlinearity (evidently in terms of BMI, since that’s the relationship that currently makes no sense) would be worth exploring.\nThus:\n\nwhas100.3 &lt;- update(whas100.2, . ~ . + I(bmi^2))\nsummary(whas100.3)\n\nCall:\ncoxph(formula = y ~ age + bmi + I(bmi^2), data = whas100)\n\n  n= 100, number of events= 51 \n\n              coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nage       0.040542  1.041375  0.012035  3.369 0.000755 ***\nbmi      -0.848949  0.427864  0.231562 -3.666 0.000246 ***\nI(bmi^2)  0.014500  1.014606  0.004227  3.430 0.000603 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n         exp(coef) exp(-coef) lower .95 upper .95\nage         1.0414     0.9603    1.0171    1.0662\nbmi         0.4279     2.3372    0.2718    0.6736\nI(bmi^2)    1.0146     0.9856    1.0062    1.0230\n\nConcordance= 0.693  (se = 0.04 )\nLikelihood ratio test= 30.71  on 3 df,   p=1e-06\nWald test            = 32.56  on 3 df,   p=4e-07\nScore (logrank) test = 36.57  on 3 df,   p=6e-08\n\n\nAh, that seems to be it. The significant positive coefficient on bmi-squared means that the “hazard of dying” increases faster with increasing bmi, so there ought to be an optimal BMI beyond which survival chances decrease again. Have we improved the residuals by adding the squared term?\n\nggcoxdiagnostics(whas100.3) + geom_smooth()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI call those “inconsequential wiggles” now, so I think we are good. Let’s explore the quadratic relationship on a graph.\nI’m going to focus on a close-to-median age of 70, since, in this model, the effect of BMI is the same for all ages (to make it different, we would need an interaction term, ANOVA-style).\nFirst we create a data frame with a bunch of different BMIs in, and one age 70:\n\nbmis &lt;- seq(20, 36, 4)\nages &lt;- 70\nwhas100.new.2 &lt;- crossing(bmi = bmis, age = ages)\nwhas100.new.2\n\n\n\n  \n\n\n\nIt is rather absurd to have a plural ages with only one age in it, but that’s the way it goes, if you’re me and trying to avoid thinking.\nPredictions, using the model with the squared term in it:\n\npp3 &lt;- survfit(whas100.3, whas100.new.2, data = whas100)\n\nAnd then the plot:\n\nggsurvplot(pp3, conf.int = F)\n\n\n\n\nand the customary reminder of which stratum is which, with its rather ungainly name:\n\nwhas100.new.2\n\n\n\n  \n\n\n\nThis time, the green survival curve is best, stratum 3, which means that survival is best at BMI 28, and worse for both higher BMIs and lower BMIs. You can follow the sequence of colours: red, olive-green, green, blue, pink, that goes up and then down again. But it’s still true that having a very low BMI is worst, which is why our (linear) model said that having a higher BMI was better.\nIt would have been better to have you put a squared term in the model, but the question was already long and complicated enough, and I didn’t want to make your lives more of a nightmare than they are already becoming!\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "survival-analysis.html#drug-treatment-programs-1",
    "href": "survival-analysis.html#drug-treatment-programs-1",
    "title": "27  Survival analysis",
    "section": "27.6 Drug treatment programs",
    "text": "27.6 Drug treatment programs\nOne of the goals of drug treatment programs is to lengthen the time until the patient returns to using drugs. (It is not generally possible to prevent patients from ever using drugs again.) In one study, over 600 former drug users took part. Two different programs, a short program and a long program, were offered at two different sites, labelled A and B. The data can be found in link. The variables are these:\n\nID: patient ID number\nage: patient age at enrollment into the study\nndrugtx: number of previous drug treatments\ntreat: 0 for short treatment program, 1 for long program\nsite: 0 for site A, 1 for site B\ntime: time until return to drug use\ncensor: whether the subject returned to drug use (1) or not (0) during the follow-up period\nherco: whether subject used heroine or cocaine in the last 3 months: 1 is both, 2 is one (either heroine or cocaine), 3 is neither.\n\n\nRead in the data and check in one way or another that you have what was promised above.\n\nSolution\nThis:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/drugusers.txt\"\ndrugusers &lt;- read_delim(my_url, \" \")\n\nRows: 628 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (9): row, ID, age, ndrugtx, treat, site, time, censor, herco\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndrugusers\n\n\n\n  \n\n\n\nThis shows that you have over 600 rows and the variables described.\n\\(\\blacksquare\\)\n\nThere are some missing values in the dataframe. Demonstrate this using summary. Pipe the dataframe into drop_na and show that they have gone. (drop_na removes all rows that have missing values in them.)\n\nSolution\nFirst off, summary is a quick way to show how many missing values there are:8\n\nsummary(drugusers)\n\n      row              ID             age           ndrugtx      \n Min.   :  1.0   Min.   :  1.0   Min.   :20.00   Min.   : 0.000  \n 1st Qu.:157.8   1st Qu.:157.8   1st Qu.:27.00   1st Qu.: 1.000  \n Median :314.5   Median :314.5   Median :32.00   Median : 3.000  \n Mean   :314.5   Mean   :314.5   Mean   :32.37   Mean   : 4.574  \n 3rd Qu.:471.2   3rd Qu.:471.2   3rd Qu.:37.00   3rd Qu.: 6.000  \n Max.   :628.0   Max.   :628.0   Max.   :56.00   Max.   :40.000  \n                                 NA's   :5       NA's   :17      \n     treat             site            time            censor      \n Min.   :0.0000   Min.   :0.000   Min.   :   2.0   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:  79.0   1st Qu.:1.0000  \n Median :0.0000   Median :0.000   Median : 166.0   Median :1.0000  \n Mean   :0.4904   Mean   :0.293   Mean   : 234.7   Mean   :0.8089  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.: 365.2   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1172.0   Max.   :1.0000  \n                                                                   \n     herco      \n Min.   :1.000  \n 1st Qu.:1.000  \n Median :2.000  \n Mean   :1.898  \n 3rd Qu.:3.000  \n Max.   :3.000  \n                \n\n\nAge has five missing values and “number of previous drug treatments” has seventeen.\nFollowing the instructions, and saving back into the original dataframe:\n\ndrugusers %&gt;% drop_na() -&gt; drugusers\n\nand then\n\nsummary(drugusers)\n\n      row              ID             age           ndrugtx      \n Min.   :  1.0   Min.   :  1.0   Min.   :20.00   Min.   : 0.000  \n 1st Qu.:155.2   1st Qu.:155.2   1st Qu.:27.00   1st Qu.: 1.000  \n Median :312.5   Median :312.5   Median :32.00   Median : 3.000  \n Mean   :313.8   Mean   :313.8   Mean   :32.39   Mean   : 4.579  \n 3rd Qu.:473.8   3rd Qu.:473.8   3rd Qu.:37.00   3rd Qu.: 6.000  \n Max.   :628.0   Max.   :628.0   Max.   :56.00   Max.   :40.000  \n     treat             site             time            censor      \n Min.   :0.0000   Min.   :0.0000   Min.   :   2.0   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:  79.0   1st Qu.:1.0000  \n Median :0.0000   Median :0.0000   Median : 166.0   Median :1.0000  \n Mean   :0.4918   Mean   :0.2984   Mean   : 234.4   Mean   :0.8115  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 361.8   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1172.0   Max.   :1.0000  \n     herco     \n Min.   :1.00  \n 1st Qu.:1.00  \n Median :2.00  \n Mean   :1.89  \n 3rd Qu.:3.00  \n Max.   :3.00  \n\n\nNo NA left. Gosh, as they say, that was easy. Extra: how many rows did we lose?\n\nnrow(drugusers)\n\n[1] 610\n\n\nThere were 628 rows before, so we lost 18. (There were 22 missing values, but some of them were two on one row, so we only lost 18 rows.)\nThis is a very unsophisticated way of dealing with missing values. Another way is to “impute” them, that is, to guess what they would have been, and then fill in the guessed values and use them as if they were the truth, for example by regressing the columns with missing values on all the others, and using the regression predictions in place of the missing values.\n\\(\\blacksquare\\)\n\nSome of these variables are recorded as numbers but are actually categorical. Which ones? Re-define these variables in your data frame so that they have sensible (text) values.\n\nSolution\nThese variables are actually categorical rather than quantitative:\n\ntreat\nsite\ncensor\nherco\n\nMost of them have only two levels, so it doesn’t matter whether we make them categorical or leave them as numbers, but for herco it matters. Let’s give them all sensible values, mostly with ifelse,9 thus:\n\ndrugusers %&gt;% mutate(\n  treat = ifelse(treat == 0, \"short\", \"long\"),\n  site = ifelse(site == 0, \"A\", \"B\"),\n  censor = ifelse(censor == 1, \"returned\", \"no-return\"),\n  herco = case_when(\n    herco == 1 ~ \"both\",\n    herco == 2 ~ \"one\",\n    herco == 3 ~ \"neither\"\n  )\n) -&gt; drugusers\n\nI’m living on the edge and overwriting everything:\n\ndrugusers\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nCreate a suitable reponse variable for a Cox proportional hazards regression that predicts time until return to drug use from the other variables. This requires some care, because you need to be sure about what the censoring variable actually represents and what you need it to represent.\n\nSolution\nThis is Surv in package survival. The response variable needs to encode two things: the time until the event of interest (return to drug use) and whether or not that event happened for each patient.10 In this case, that is censor=\"returned\".\n\ny &lt;- with(drugusers, Surv(time, censor == \"returned\"))\n\nUse whatever name you gave to the level of censor that means “returned to drug use”.\nOnce again, there is no problem with adding a new column y to your dataframe, thus:\n\ndrugusers %&gt;% \n  mutate(y = Surv(time, censor == \"returned\"))\n\n\n\n  \n\n\n\nAdd your response as a stand-alone vector or as a new column; your choice.\n\\(\\blacksquare\\)\n\nLook at the first few values of your response variable. Why is the fifth one marked with a +? Explain briefly.\n\nSolution\nhead works as well with a vector (displaying the first six values) as it does with a data frame:\n\nhead(y)\n\n[1] 188   26  207  144  551+  32 \n\n\nThe fifth value is marked with a + because it is a censored value: this is a patient who was never observed to go back to drug use. You can tell this by looking at the head of the entire data frame:\n\nhead(drugusers)\n\n\n\n  \n\n\n\nsince this patient has censor=\"no-return\". The other ones have censor=\"returned\"; these are all “uncensored” in the jargon.\nIf you added a new column y to your dataframe, you can see all this in one go by looking at the fifth row of the dataframe.\nTypically, censored values will be bigger than uncensored ones, because (in general) the individual will be observed until the study ends, and studies of this kind carry on for years:\n\nggplot(drugusers, aes(x = censor, y = time)) + geom_boxplot()\n\n\n\n\nYep. The smallest time for a censored observation would be an upper outlier if it were observed for an uncensored observation.\nOne nice side-effect of turning censor into a categorical variable is that it can now distinguish groups as a boxplot requires.\nI discovered something rather amusing when I originally wrote this (a year ago). Suppose you want to compare times for the two treatment groups, and you also want to distinguish censored from non-censored observations. Then, this works:\n\nggplot(drugusers, aes(x = treat, y = time, colour = censor)) +\n  geom_boxplot()\n\n\n\n\nFor each treatment, you get side-by-side boxplots of the times for censored (red) and uncensored (blue) observations, and so you see for both treatments (short and long) the censored times are typically longer than the uncensored ones.\n(This you may recognize as a “grouped boxplot”, for when we have two categorical variables and one quantitative one.)\nI borrow this idea for two-way ANOVA (coming up later).\n\\(\\blacksquare\\)\n\nFit a Cox proportional hazards model, predicting from all the other variables (except for row and ID) that you haven’t used yet. Display the results.\n\nSolution\n\ndrugusers.1 &lt;- coxph(y ~ age + ndrugtx + treat + site + herco, data = drugusers)\nsummary(drugusers.1)\n\nCall:\ncoxph(formula = y ~ age + ndrugtx + treat + site + herco, data = drugusers)\n\n  n= 610, number of events= 495 \n\n                  coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nage          -0.023798  0.976483  0.007561 -3.148  0.00165 ** \nndrugtx       0.034815  1.035429  0.007755  4.490 7.14e-06 ***\ntreatshort    0.254606  1.289953  0.091006  2.798  0.00515 ** \nsiteB        -0.173021  0.841120  0.102105 -1.695  0.09016 .  \nherconeither  0.125779  1.134032  0.103075  1.220  0.22236    \nhercoone      0.247318  1.280586  0.122759  2.015  0.04394 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n             exp(coef) exp(-coef) lower .95 upper .95\nage             0.9765     1.0241    0.9621    0.9911\nndrugtx         1.0354     0.9658    1.0198    1.0513\ntreatshort      1.2900     0.7752    1.0792    1.5418\nsiteB           0.8411     1.1889    0.6886    1.0275\nherconeither    1.1340     0.8818    0.9266    1.3879\nhercoone        1.2806     0.7809    1.0067    1.6289\n\nConcordance= 0.581  (se = 0.014 )\nLikelihood ratio test= 35.08  on 6 df,   p=4e-06\nWald test            = 36.96  on 6 df,   p=2e-06\nScore (logrank) test = 37.36  on 6 df,   p=1e-06\n\n\nAnother way to handle “all the other \\(x\\)’s except row, ID, time and censor” is this:\n\ndrugusers.1a &lt;- coxph(y ~ . - row - ID - time - censor, data = drugusers)\ntidy(drugusers.1a)\n\n\n\n  \n\n\n\nSame. I used tidy from broom to shorten the output a bit.\n\\(\\blacksquare\\)\n\nFind which explanatory variables can be removed at \\(\\alpha=0.05\\) (there should be two of them). Bear in mind that we have categorical variables, so that looking at the output from summary is not enough.\n\nSolution\nThe hint is meant to suggest to you that looking at drop1 is the right way to go:\n\ndrop1(drugusers.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nNote that herco, a categorical variable with three levels, has 2 degrees of freedom here, since a test of “no effect of herco” is testing that survival is the same at all three levels of herco.\n\\(\\blacksquare\\)\n\nRemove all the non-significant explanatory variables and re-fit your model. By carrying out a suitable test demonstrate that your smaller model is the better one.\n\nSolution\nsite and herco are the two variables to come out.11 I like update, but there is no problem about copying-pasting your coxph and taking out what you no longer need.\n\ndrugusers.2 &lt;- update(drugusers.1, . ~ . - site - herco)\n\nHaving fit two models, we can use anova to compare them. The right test gets done, so no need for test=:\n\nanova(drugusers.2, drugusers.1)\n\n\n\n  \n\n\n\nThere is no significant difference between these two models,12 so we can go with the smaller, simpler one (with just age, ndrugtx and treat).\n\\(\\blacksquare\\)\n\n* Display your better model. Are all of the explanatory variables significant? Do their slope coefficients have sensible signs (plus or minus), based on what you know or can guess about drug treatments? Explain briefly.\n\nSolution\n\nsummary(drugusers.2)\n\nCall:\ncoxph(formula = y ~ age + ndrugtx + treat, data = drugusers)\n\n  n= 610, number of events= 495 \n\n                coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nage        -0.020801  0.979414  0.007419 -2.804  0.00505 ** \nndrugtx     0.035567  1.036207  0.007621  4.667 3.05e-06 ***\ntreatshort  0.231055  1.259929  0.090175  2.562  0.01040 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n           exp(coef) exp(-coef) lower .95 upper .95\nage           0.9794     1.0210    0.9653    0.9938\nndrugtx       1.0362     0.9651    1.0208    1.0518\ntreatshort    1.2599     0.7937    1.0558    1.5035\n\nConcordance= 0.572  (se = 0.014 )\nLikelihood ratio test= 27.87  on 3 df,   p=4e-06\nWald test            = 30.5  on 3 df,   p=1e-06\nScore (logrank) test = 30.62  on 3 df,   p=1e-06\n\n\nThe three remaining explanatory variables are all clearly significant: the patient’s age, the number of previous drug treatments, and whether the treatment was short or long. This is legit (we don’t need to run drop1 again) because the remaining explanatory variables are all quantitative or have only two levels, so that the single-df tests in summary are what we need.\nDo their slope coefficients have sensible signs? Well, this requires careful thought. A positive coefficient means that increasing that variable increases the hazard of the event: ie., it makes the event likelier to happen sooner. Here, the “event” is “return to drug use”:\n\nAge has a negative coefficient, so an older person is likely to take longer to return to drug use, other things being equal. This makes some kind of sense, if you imagine drug use as being related to maturity, or an older drug user as being more strongly committed to “turning their life around”, so that a drug treatment of any kind is going to be more effective on an older patient.\nThe number of previous treatments has a positive coefficient, so that a patient who has had a lot of previous treatments is likely to go back to drugs sooner. Such a person might be an “addict” for whom treatments really do not work, or might not be committed to giving up drugs.\ntreatshort has a positive coefficient. This says that if you give a patient a short treatment, they are more likely (other things being equal) to go back to drugs sooner, as compared to the baseline long treatment. That is, a longer treatment is more effective than a shorter one. Given a significant effect of treatment length, this is the way around you would expect it to be.\n\n\\(\\blacksquare\\)\n\nWe have three variables left in our model, age, ndrugtx and treat. The quartiles of age are 27 and 37, the quartiles of ndrugtx are 1 and 6, and the two possible values of treat are short and long. Create a data frame with variables of these names and all possible combinations of their values (so there should be 8 rows in the resulting data frame). Display the resulting data frame.\n\nSolution\nThis data frame is going to be used for prediction, so I will call it new and construct it in pieces as I did before (thus meaning that I don’t have to think too hard about what I’m doing):\n\nages &lt;- c(27, 37)\nndrugtxs &lt;- c(1, 6)\ntreats &lt;- c(\"short\", \"long\")\nnew &lt;- datagrid(model = drugusers.2, age = ages, ndrugtx = ndrugtxs, treat = treats)\nnew\n\n\n\n  \n\n\n\n8 rows as promised.\n\\(\\blacksquare\\)\n\nObtain predicted survival probabilities for each of the values of age, ndrugtx and treat used in the previous part. You don’t need to display it (we are going to plot it shortly).\n\nSolution\nsurvfit is the survival analysis version of predict and works the same way, so this is all you need:\n\npp &lt;- survfit(drugusers.2, new, data = drugusers)\n\nMake sure that you use your best model, ie. the second one. The data= is needed for the plot below, not in itself for the prediction.\n\\(\\blacksquare\\)\n\nPlot your predicted survival curves.\n\nSolution\nThis:\n\nggsurvplot(pp, conf.int = F)\n\n\n\n\nThe only thing to remember is that you plot your predictions, not the model from which they came.\nIf your plot didn’t come out, you may have to go back and re-do the survfit with the data= at the end.\nFor reference in a minute:\n\nnew\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nWhich of your combinations of values is predicted to take the longest to return to drug use? Which is predicted to take the shortest time? Explain briefly.\n\nSolution\nRemember that “up and to the right” is the best survival curve: that is, the people on this survival curve are predicted to take the longest to return to drug use. On my plot, this is the pale blue survival curve, stratum 5. Going back to my combinations data frame, this is 37-year-olds with only one previous drug treatment and the longer drug treatment this time.\nThe worst is my green survival curve, stratum 4, which is the exact opposite of this: 27-year-olds, 6 previous drug treatments, shorter treatment this time.\n“Returning to drug use” is like “death” in that you want it to be a long time before it happens, so “best” is top right on the plot of survival curves. In other circumstances, you might want the event to happen sooner, in which case the lower-left survival curve would be the “best” one.\n\\(\\blacksquare\\)\n\nAre your survival curve plot and your conclusions from part (here) consistent, or not? Explain briefly.\n\nSolution\nThe survival curves say that being older, having fewer previous treatments and being on the long treatment are better in terms of taking longer to return to drug use. Our analysis of whether the slope coefficients in drugusers.2 were positive or negative came to exactly the same conclusion. So the survival curves and part (here) are entirely consistent.\nOn my plot with the legend, you can assess the effects of the individual variables: for example, to assess the effect of age, find two combos that differ only in age, say strata 1 and 5, the red and light blue ones. Of these, the light blue survival curve is higher, so age 37 is better in terms of survival than age 27. This will work whichever such pair you pick: for example, strata 3 and 7, the olive green and purple curves, compare the same way.\nExtra: more comparisons for you to do: to assess the effect of number of previous treatments, compare eg. strata 1 and 3, red and olive green, and to assess the effect of treatment length, compare eg. strata 5 and 6, light blue and darker blue.\nAll this struggling to identify colours makes me think of link, in which the guy behind the webcomic XKCD did a survey where he showed people a whole bunch of different colours and asked the people to name the colours.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "survival-analysis.html#multiple-myeloma-1",
    "href": "survival-analysis.html#multiple-myeloma-1",
    "title": "27  Survival analysis",
    "section": "27.7 Multiple myeloma",
    "text": "27.7 Multiple myeloma\nMultiple myeloma is a kind of cancer. It forms in a plasma cell (which is a type of white blood cell). It causes cancer cells to accumulate in the bone marrow, where they crowd out healthy blood cells. Plasma cells make antibodies (to help fight infections), while the cancer cells don’t: they produce abnormal proteins that can cause kidney problems. (This adapted from link.) The variables are:\n\ntime: survival time from diagnosis (months)\nvstatus: 0=alive, 1=dead at end of study\nlogbun: log of BUN test score (BUN test is a test of kidney function, not to be confused with cha siu bao13).\nhgb: hemoglobin (at diagnosis).\nplatelet: platelets: 1=normal, 0=abnormal (at diagnosis).\nage at diagnosis, in years\nlogwbc: log of WBC (white blood cell count, at diagnosis)\nfrac: fractures at diagnosis (0=absent, 1=present)\nlogpbm: log of percent of plasma cells in bone marrow\nprotein: proteinuria (protein in urine) at diagnosis. Most people have very little, so a larger than normal amount indicates illness of some kind.\nscalc: serum calcium at diagnosis.\n\nThe data, on 65 patients with multiple myeloma, are in link. Some of the variables are logs because they could take very large values.\nThere are a lot of parts here, but each part is supposed to be short.\n\nRead in the data and display (some of) the values. Confirm that you have the right number of observations and the right variables.\n\nSolution\nThe usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/myeloma.csv\"\nmyeloma &lt;- read_csv(my_url)\n\nRows: 65 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (11): time, vstatus, logbun, hgb, platelet, age, logwbc, frac, logpbm, p...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmyeloma\n\n\n\n  \n\n\n\n65 observations, and all the variables listed. If you want to go further (not necessary here), you can check that the variables vstatus, platelet and frac that should be zero and one actually are zero and one, at least for the values shown (they are), and the ages look like ages (they do).\nThe tidyverse also offers:\n\nglimpse(myeloma)\n\nRows: 65\nColumns: 11\n$ time     &lt;dbl&gt; 1.25, 1.25, 2.00, 2.00, 2.00, 3.00, 5.00, 5.00, 6.00, 6.00, 6…\n$ vstatus  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ logbun   &lt;dbl&gt; 2.2175, 1.9395, 1.5185, 1.7482, 1.3010, 1.5441, 2.2355, 1.681…\n$ hgb      &lt;dbl&gt; 9.4, 12.0, 9.8, 11.3, 5.1, 6.7, 10.1, 6.5, 9.0, 10.2, 9.7, 10…\n$ platelet &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ age      &lt;dbl&gt; 67, 38, 81, 75, 57, 46, 50, 74, 77, 70, 60, 67, 48, 61, 53, 5…\n$ logwbc   &lt;dbl&gt; 3.6628, 3.9868, 3.8751, 3.8062, 3.7243, 4.4757, 4.9542, 3.732…\n$ frac     &lt;dbl&gt; 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1…\n$ logpbm   &lt;dbl&gt; 1.9542, 1.9542, 2.0000, 1.2553, 2.0000, 1.9345, 1.6628, 1.732…\n$ protein  &lt;dbl&gt; 12, 20, 2, 0, 3, 12, 4, 5, 1, 1, 0, 0, 5, 1, 1, 0, 0, 1, 1, 0…\n$ scalc    &lt;dbl&gt; 10, 18, 15, 12, 9, 10, 9, 9, 8, 8, 10, 8, 10, 10, 13, 12, 10,…\n\n\nwhich gives a bit more of a picture of the values.14 Or if you were serious about checking, you could do\n\nsummary(myeloma)\n\n      time          vstatus           logbun            hgb      \n Min.   : 1.25   Min.   :0.0000   Min.   :0.7782   Min.   : 4.9  \n 1st Qu.: 7.00   1st Qu.:0.0000   1st Qu.:1.1461   1st Qu.: 8.8  \n Median :15.00   Median :1.0000   Median :1.3222   Median :10.2  \n Mean   :24.01   Mean   :0.7385   Mean   :1.3929   Mean   :10.2  \n 3rd Qu.:35.00   3rd Qu.:1.0000   3rd Qu.:1.5682   3rd Qu.:12.0  \n Max.   :92.00   Max.   :1.0000   Max.   :2.2355   Max.   :14.6  \n    platelet           age            logwbc           frac       \n Min.   :0.0000   Min.   :38.00   Min.   :3.362   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:51.00   1st Qu.:3.643   1st Qu.:1.0000  \n Median :1.0000   Median :60.00   Median :3.732   Median :1.0000  \n Mean   :0.8615   Mean   :60.15   Mean   :3.769   Mean   :0.7538  \n 3rd Qu.:1.0000   3rd Qu.:67.00   3rd Qu.:3.875   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :82.00   Max.   :4.954   Max.   :1.0000  \n     logpbm          protein           scalc      \n Min.   :0.4771   Min.   : 0.000   Min.   : 7.00  \n 1st Qu.:1.3617   1st Qu.: 0.000   1st Qu.: 9.00  \n Median :1.6232   Median : 1.000   Median :10.00  \n Mean   :1.5497   Mean   : 3.615   Mean   :10.12  \n 3rd Qu.:1.8451   3rd Qu.: 4.000   3rd Qu.:10.00  \n Max.   :2.0000   Max.   :27.000   Max.   :18.00  \n\n\nwhich gives means and five-number summaries for each of the variables (the numeric ones, but they all are here, even the ones coded as 0 or 1 that are really categorical).\n\\(\\blacksquare\\)\n\nCreate a suitable response variable for a Cox proportional-hazards survival model, bearing in mind that the “event” here is death. Display your response variable, and explain briefly what the + signs attached to some of the values mean, without using a technical term.\n\nSolution\nI seem to call my response variables y, but you can call yours whatever you like. Two things to consider: the survival times, here time, and the indicator of the event, here vstatus being 1.\nThe modern way is to define the response variable right back into the dataframe, thus:\n\nmyeloma %&gt;% \n  mutate(y = Surv(time, vstatus == 1)) -&gt; myeloma\nmyeloma\n\n\n\n  \n\n\n\nbut you can also define y as a separate object outside the dataframe, thus:\n\ny &lt;- with(myeloma, Surv(time, vstatus == 1))\ny\n\n [1]  1.25   1.25   2.00   2.00   2.00   3.00   5.00   5.00   6.00   6.00 \n[11]  6.00   6.00   7.00   7.00   7.00   9.00  11.00  11.00  11.00  11.00 \n[21] 11.00  13.00  14.00  15.00  16.00  16.00  17.00  17.00  18.00  19.00 \n[31] 19.00  24.00  25.00  26.00  32.00  35.00  37.00  41.00  41.00  51.00 \n[41] 52.00  54.00  58.00  66.00  67.00  88.00  89.00  92.00   4.00+  4.00+\n[51]  7.00+  7.00+  8.00+ 12.00+ 11.00+ 12.00+ 13.00+ 16.00+ 19.00+ 19.00+\n[61] 28.00+ 41.00+ 53.00+ 57.00+ 77.00+\n\n\nOr use myeloma$ (twice) before the variable names.\nThe values of y that have a + by them go with patients who were never observed to die (or were still alive at the end of the study). There were 17 of these, listed at the end of the data frame. Usually, these values of the response will be higher than the others, but they weren’t here. (Maybe some of these patients were withdrawn from the study, or they joined it late.)\n\\(\\blacksquare\\)\n\nWhat is the technical term for those patients that have a + by their values for the response variable?\n\nSolution\nCensored. A quick one.\nI was trying to dissuade you from using the word “censored” in the previous part, since I wanted you to demonstrate that you understood what it meant. But you should know the technical term as well, which is why I asked you for it here.\nGrading note: if this part and the previous one contain, somewhere, the word “censored” and a clear explanation of what “censored” means, then I don’t mind what is where.\n\\(\\blacksquare\\)\n\nFit a Cox proportional-hazards survival model predicting your response variable from all the other variables (except for the ones that you used to make the response variable). Display the summary of your model.\n\nSolution\nThe obvious way to do this is to list all the other variables on the right side of the squiggle, but a faster way is this:\n\ny.1 &lt;- coxph(y ~ . - time - vstatus, data = myeloma)\nsummary(y.1)\n\nCall:\ncoxph(formula = y ~ . - time - vstatus, data = myeloma)\n\n  n= 65, number of events= 48 \n\n             coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nlogbun    1.85557   6.39536  0.65628  2.827  0.00469 **\nhgb      -0.12629   0.88136  0.07212 -1.751  0.07994 . \nplatelet -0.25488   0.77501  0.51194 -0.498  0.61858   \nage      -0.01306   0.98702  0.01957 -0.668  0.50439   \nlogwbc    0.35389   1.42460  0.71576  0.494  0.62101   \nfrac      0.34232   1.40821  0.40725  0.841  0.40059   \nlogpbm    0.38165   1.46470  0.48743  0.783  0.43364   \nprotein   0.01302   1.01311  0.02612  0.498  0.61817   \nscalc     0.12976   1.13856  0.10502  1.236  0.21659   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n         exp(coef) exp(-coef) lower .95 upper .95\nlogbun      6.3954     0.1564    1.7670    23.147\nhgb         0.8814     1.1346    0.7652     1.015\nplatelet    0.7750     1.2903    0.2841     2.114\nage         0.9870     1.0131    0.9499     1.026\nlogwbc      1.4246     0.7020    0.3503     5.794\nfrac        1.4082     0.7101    0.6339     3.128\nlogpbm      1.4647     0.6827    0.5634     3.808\nprotein     1.0131     0.9871    0.9625     1.066\nscalc       1.1386     0.8783    0.9268     1.399\n\nConcordance= 0.675  (se = 0.051 )\nLikelihood ratio test= 17.62  on 9 df,   p=0.04\nWald test            = 17.93  on 9 df,   p=0.04\nScore (logrank) test = 18.97  on 9 df,   p=0.03\n\n\nThe . in this model formula means “all the columns in the data frame” (except for the response variable if it was in the data frame, which here it was not). I used time and vstatus to make y, so I had to exclude them explicitly.\nIf you forget to exclude time and vstatus, you are in danger of having a model that fits perfectly:\n\ny.00 &lt;- coxph(y ~ ., data = myeloma)\n\nWarning in coxph.fit(X, Y, istrat, offset, init, control, weights = weights, :\nRan out of iterations and did not converge\n\nsummary(y.00)\n\nCall:\ncoxph(formula = y ~ ., data = myeloma)\n\n  n= 65, number of events= 48 \n\n               coef  exp(coef)   se(coef)       z Pr(&gt;|z|)    \ntime     -1.109e+01  1.526e-05  9.799e-01 -11.318   &lt;2e-16 ***\nvstatus   1.543e+01  5.031e+06  6.233e+02   0.025     0.98    \nlogbun    1.759e-04  1.000e+00  7.904e-01   0.000     1.00    \nhgb       6.861e-06  1.000e+00  9.143e-02   0.000     1.00    \nplatelet  9.226e-05  1.000e+00  6.221e-01   0.000     1.00    \nage      -4.311e-06  1.000e+00  2.245e-02   0.000     1.00    \nlogwbc   -1.220e-06  1.000e+00  8.940e-01   0.000     1.00    \nfrac     -6.694e-05  9.999e-01  6.262e-01   0.000     1.00    \nlogpbm    2.400e-04  1.000e+00  7.307e-01   0.000     1.00    \nprotein   3.472e-05  1.000e+00  6.776e-02   0.001     1.00    \nscalc    -1.447e-05  1.000e+00  1.091e-01   0.000     1.00    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n         exp(coef) exp(-coef) lower .95 upper .95\ntime     1.526e-05  6.554e+04 2.236e-06 0.0001041\nvstatus  5.031e+06  1.988e-07 0.000e+00       Inf\nlogbun   1.000e+00  9.998e-01 2.124e-01 4.7087742\nhgb      1.000e+00  1.000e+00 8.359e-01 1.1962633\nplatelet 1.000e+00  9.999e-01 2.955e-01 3.3849492\nage      1.000e+00  1.000e+00 9.569e-01 1.0449881\nlogwbc   1.000e+00  1.000e+00 1.734e-01 5.7673011\nfrac     9.999e-01  1.000e+00 2.930e-01 3.4119518\nlogpbm   1.000e+00  9.998e-01 2.389e-01 4.1885763\nprotein  1.000e+00  1.000e+00 8.757e-01 1.1420792\nscalc    1.000e+00  1.000e+00 8.075e-01 1.2384242\n\nConcordance= 1  (se = 0 )\nLikelihood ratio test= 277  on 11 df,   p=&lt;2e-16\nWald test            = 128.1  on 11 df,   p=&lt;2e-16\nScore (logrank) test = 87.4  on 11 df,   p=5e-14\n\n\nThe warning at the top is your clue that something has gone wrong. This kind of warning can happen with real data, but not often: it is usually an indication that something is wrong with the way you specified the model. If you look at the output, you’ll realize that predicting survival time from survival time makes no sense at all.\nThere is of course nothing wrong with typing out all the variable names, except that the first time you type them out, you will likely make a typo (unless you are more careful than I usually am).\n\\(\\blacksquare\\)\n\nIn your model, which explanatory variables have a P-value less than 0.10? Fit a model containing only those and display the results.\n\nSolution\nOnly logbun and hgb; the other P-values are larger, usually much larger. Because there are so many variables to remove, I am frightened away from update here (which I would normally try to use in this situation). I’m going to copy-and-paste my code for y.1 and edit it:\n\ny.2 &lt;- coxph(y ~ logbun + hgb, data = myeloma)\nsummary(y.2)\n\nCall:\ncoxph(formula = y ~ logbun + hgb, data = myeloma)\n\n  n= 65, number of events= 48 \n\n           coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nlogbun  1.71597   5.56209  0.61855  2.774  0.00553 **\nhgb    -0.11966   0.88722  0.05742 -2.084  0.03717 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\nlogbun    5.5621     0.1798    1.6547   18.6961\nhgb       0.8872     1.1271    0.7928    0.9929\n\nConcordance= 0.675  (se = 0.043 )\nLikelihood ratio test= 12.27  on 2 df,   p=0.002\nWald test            = 12.51  on 2 df,   p=0.002\nScore (logrank) test = 13.07  on 2 df,   p=0.001\n\n\nThat’s all I wanted, but you can note that hgb has become significant at \\(\\alpha=0.05\\). I suspect it was somewhat correlated with a variable that we removed, so that its value to the regression has become clearer.\n\\(\\blacksquare\\)\n\nDo a test to compare the two models that you fit. Why do you prefer the second model? Explain briefly.\n\nSolution\nComparing two models is anova, which also works here. The right test is Chisq:\n\nanova(y.2, y.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe usual logic here: this is far from significant, so the null hypothesis (that the two models are equally good) is not rejected, so we prefer the smaller model y.2 because it is simpler.\nI wasn’t sure about the Model 2 line of my anova output (what are time and vstatus doing there?), but the test has 7 degrees of freedom, which is correct since we started with 9 explanatory variables and finished with 2, so that we took out 7 of them. I checked what went off the right side of the page: there is a -time-vstatus on the end, so that it is correct. What happened is that the . got expanded out into all the variables separated by +, and then whatever else (the “minus” variables) were on the end.\nIn case you are curious, step also works on models like these:\n\ny.3 &lt;- step(y.1, direction = \"backward\", trace = 0)\nsummary(y.3)\n\nCall:\ncoxph(formula = y ~ logbun + hgb, data = myeloma)\n\n  n= 65, number of events= 48 \n\n           coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nlogbun  1.71597   5.56209  0.61855  2.774  0.00553 **\nhgb    -0.11966   0.88722  0.05742 -2.084  0.03717 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\nlogbun    5.5621     0.1798    1.6547   18.6961\nhgb       0.8872     1.1271    0.7928    0.9929\n\nConcordance= 0.675  (se = 0.043 )\nLikelihood ratio test= 12.27  on 2 df,   p=0.002\nWald test            = 12.51  on 2 df,   p=0.002\nScore (logrank) test = 13.07  on 2 df,   p=0.001\n\n\nThe same model as the one we found by brute force. You can change the value of trace to see the progress, but in this case it’s not very illuminating, since &lt;none&gt; and the variables we end up keeping are always at the bottom of the list to remove.\nstep is built on add1 and drop1. In this case, drop1 is run repeatedly and the variable with lowest AIC is removed. We had all numeric variables in this one, but if our model had something categorical like treatment with, let’s say, 4 levels, drop1 would contemplate dropping all four of these in one shot, the same way it works with a categorical variable in a regression of any other kind.\n\\(\\blacksquare\\)\n\nThere should be two explanatory variables left in your model. These are both numerical variables. Find their first and third quartiles, any way you like.\n\nSolution\nThe obvious way is probably this:\n\nquantile(myeloma$logbun)\n\n    0%    25%    50%    75%   100% \n0.7782 1.1461 1.3222 1.5682 2.2355 \n\nquantile(myeloma$hgb)\n\n  0%  25%  50%  75% 100% \n 4.9  8.8 10.2 12.0 14.6 \n\n\nSo the quartiles are 1.15 and 1.57 for logbun, and 8.8 and 12.0 for hgb.\nThere are (at least) three other ways to do it. This is the easiest:\n\nsummary(myeloma)\n\n      time          vstatus           logbun            hgb      \n Min.   : 1.25   Min.   :0.0000   Min.   :0.7782   Min.   : 4.9  \n 1st Qu.: 7.00   1st Qu.:0.0000   1st Qu.:1.1461   1st Qu.: 8.8  \n Median :15.00   Median :1.0000   Median :1.3222   Median :10.2  \n Mean   :24.01   Mean   :0.7385   Mean   :1.3929   Mean   :10.2  \n 3rd Qu.:35.00   3rd Qu.:1.0000   3rd Qu.:1.5682   3rd Qu.:12.0  \n Max.   :92.00   Max.   :1.0000   Max.   :2.2355   Max.   :14.6  \n    platelet           age            logwbc           frac       \n Min.   :0.0000   Min.   :38.00   Min.   :3.362   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:51.00   1st Qu.:3.643   1st Qu.:1.0000  \n Median :1.0000   Median :60.00   Median :3.732   Median :1.0000  \n Mean   :0.8615   Mean   :60.15   Mean   :3.769   Mean   :0.7538  \n 3rd Qu.:1.0000   3rd Qu.:67.00   3rd Qu.:3.875   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :82.00   Max.   :4.954   Max.   :1.0000  \n     logpbm          protein           scalc      \n Min.   :0.4771   Min.   : 0.000   Min.   : 7.00  \n 1st Qu.:1.3617   1st Qu.: 0.000   1st Qu.: 9.00  \n Median :1.6232   Median : 1.000   Median :10.00  \n Mean   :1.5497   Mean   : 3.615   Mean   :10.12  \n 3rd Qu.:1.8451   3rd Qu.: 4.000   3rd Qu.:10.00  \n Max.   :2.0000   Max.   :27.000   Max.   :18.00  \n       y.time              y.status      \n Min.   : 1.25000     Min.   :0.0000000  \n 1st Qu.: 7.00000     1st Qu.:0.0000000  \n Median :15.00000     Median :1.0000000  \n Mean   :24.00769     Mean   :0.7384615  \n 3rd Qu.:35.00000     3rd Qu.:1.0000000  \n Max.   :92.00000     Max.   :1.0000000  \n\n\nfrom which you pick out the ones you need. Or, you select the ones you need first:\n\nmyeloma %&gt;% select(logbun, hgb) %&gt;% summary()\n\n     logbun            hgb      \n Min.   :0.7782   Min.   : 4.9  \n 1st Qu.:1.1461   1st Qu.: 8.8  \n Median :1.3222   Median :10.2  \n Mean   :1.3929   Mean   :10.2  \n 3rd Qu.:1.5682   3rd Qu.:12.0  \n Max.   :2.2355   Max.   :14.6  \n\n\nThe obvious tidyverse way is actually a bit inelegant, because you have to calculate two things for two variables:15\n\nmyeloma %&gt;% summarize(\n  logbun_q1 = quantile(logbun, 0.25),\n  logbun_q3 = quantile(logbun, 0.75),\n  hgb_q1 = quantile(hgb, 0.25),\n  hgb_q3 = quantile(hgb, 0.75)\n)\n\n\n\n  \n\n\n\nNext is the tidyverse-approved way to get both quartiles for both variables at once. Use across to select the variables to use, and then write an anonymous function to say “do this on each of the columns selected in the across”. If you have a cleverer way to select those two columns without naming them, go for it. Read this in English as “for each of the columns logbun and hgb, work out the first and third quantiles of it”, where the dot is read as “it”:\n\nmyeloma %&gt;% \n  reframe(across(c(logbun, hgb), \n                   \\(x) quantile(x, c(0.25, 0.75))))\n\n\n\n  \n\n\n\nWe have lost which quartile is which, but of course the lower one must be Q1 and the higher one Q3 for each variable.16 This one is reframe rather than summarize because each “summary” (from quantile) is two numbers rather than one.\n\\(\\blacksquare\\)\n\nCreate a data frame containing all possible combinations of the two quartiles for each of the two variables, and display the result.\n\nSolution\nThe usual datagrid:\n\nlogbuns &lt;- c(1.1461, 1.5682)\nhgbs &lt;- c(8.8, 12)\nnew &lt;- datagrid(model = y.2, logbun = logbuns, hgb = hgbs)\n\nWarning: Matrix columns are not supported as predictors and are therefore\n  omitted. This may prevent computation of the quantities of interest. You\n  can construct your own prediction dataset and supply it explicitly to\n  the `newdata` argument.\n\nnew\n\n\n\n  \n\n\n\nIf you added y as a column to your dataframe, you will get a warning here that you cannot use a “matrix column”. This is because y encodes two things (survival time and censorship status), but this is not a problem because y is the response variable, not an explanatory variable.\nThe place you have to get to in the end is a data frame with columns called logbun and hgb, and the right four combinations of values. If you want to round the logbun values off more, for example to two decimals, that’s fine; it won’t affect the graph that’s coming up.\nThere is even one more way, which is to calculate the quartiles directly right in defining new:\n\nnew &lt;- datagrid(model = y.2, \n                logbun = quantile(myeloma$logbun, c(0.25,0.75)), \n                hgb = quantile(myeloma$hgb, c(0.25, 0.75)))\n\nWarning: Matrix columns are not supported as predictors and are therefore\n  omitted. This may prevent computation of the quantities of interest. You\n  can construct your own prediction dataset and supply it explicitly to\n  the `newdata` argument.\n\nnew\n\n\n\n  \n\n\n\nThis is more difficult to follow: you are doing two things at once, working out the quartiles and then all combinations of them. But if you prefer it, do it this way.\n\\(\\blacksquare\\)\n\nObtain predicted survival probabilities for each of the combinations of variables you created above. You don’t need to look at the results (they are rather long).\n\nSolution\nThis seems as if it ought to be predictions, but the survival version is called survfit:\n\ns &lt;- survfit(y.2, new, data = myeloma)\n\nIt works the same as predictions: a fitted model object (your smaller survival model), and a data frame of values to predict for. The data= is not strictly needed here, but if you want ggsurvplot to work right, you do need it to be here.\n\\(\\blacksquare\\)\n\nObtain a graph of the predicted survival curves for each combination of your variables.\n\nSolution\nThis is easier than you think: it’s just ggsurvplot from survminer:\n\nggsurvplot(s, conf.int = F)\n\n\n\n\n\\(\\blacksquare\\)\n\nIs it better to have high or low values for each of the variables in your prediction? Explain briefly.\n\nSolution\nThose four “strata” are the four rows in your prediction data frame (the four combinations). They are in the same order that they were in new (or whatever name you used):\n\nnew\n\n\n\n  \n\n\n\nThe best survival curve is the top-right green one. This is stratum17 2, from the legend at the top. In new, this goes with a low value of logbun and a high value of hgb.\nYou can check this by looking at the worst survival curve, which should be diametrically opposed. This is the blue one, stratum 3, which is high logbun and low hgb, indeed exactly the opposite of the best one.\nThings that are tests, like logbun, are often set up so that a high value is the abnormal one (so that an abnormal one will be easy to spot). Things that are measurements, like hgb, might have an ideal range, but the better value could be high or low, depending on what is being measured.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "survival-analysis.html#ovarian-cancer-1",
    "href": "survival-analysis.html#ovarian-cancer-1",
    "title": "27  Survival analysis",
    "section": "27.8 Ovarian cancer",
    "text": "27.8 Ovarian cancer\nR’s survival package contains several data sets. One of these is called ovarian; it comes from a study of 26 ovarian cancer patients. The major purpose of this study was to compare the effects of two treatments on survival time.\n\nObtain and display (all of) the data set. This is as simple as loading the package and typing the data set’s name.\n\nSolution\nThus. You may need to start with library(survival):\n\novarian\n\n\n\n  \n\n\n\nThere are indeed 26 rows. This is a data.frame rather than a tibble, so you might see the whole thing, in case you were expecting something like this:\n\novarian %&gt;% as_tibble()\n\n\n\n  \n\n\n\nwhich doesn’t change anything in ovarian, but changes what kind of thing it is (and thus how it displays). Usually when you read something in from a file, you use something like read_delim that makes a tibble, but this one wasn’t read in from a file. It was stored in the package as an old-fashioned data.frame, and so that’s how it stays.\n\\(\\blacksquare\\)\n\nThe columns of interest to us are:\n\n\nfutime: the time for which a patient was followed-up: the number of days until either they died or the study ended (or they withdrew from the study for some other reason).\nfustat: follow-up status: 1 if the patient died of ovarian cancer, 0 if they were still alive when the study ended.\nage: of patient, at diagnosis, in years\nrx: treatment, numbered 1 or 2, but really labels for the two treatments.\n\nCreate and display a suitable response variable y for a Cox proportional-hazards model.\nSolution\nThe idea is to use the appropriate one(s) of these columns in Surv. Remember that the response variable in a survival model encodes two things: the survival time, and whether or not the event (here death) actually happened to that patient or not. I always forget whether the second thing in Surv has to be 1 or 0 if the event happened. The help says that it needs to be 1 or TRUE if the event (death) happened, which is what fustat is, so we can use it as it is:\n\ny &lt;- with(ovarian, Surv(futime, fustat))\ny\n\n [1]   59   115   156   421+  431   448+  464   475   477+  563   638   744+\n[13]  769+  770+  803+  855+ 1040+ 1106+ 1129+ 1206+ 1227+  268   329   353 \n[25]  365   377+\n\n\nThis creates a separate variable y outside of any data frame. This is how, in the past, it had to be done, because although y looks as if it is 26 long (one per patient), it’s actually more complicated than that. But adding it as a column to a dataframe now works just fine:\n\novarian %&gt;% mutate(y = Surv(futime, fustat)) -&gt; ov2\nov2\n\n\n\n  \n\n\n\nInternally y is a matrix with two columns:\n\nprint.default(y)\n\n      time status\n [1,]   59      1\n [2,]  115      1\n [3,]  156      1\n [4,]  421      0\n [5,]  431      1\n [6,]  448      0\n [7,]  464      1\n [8,]  475      1\n [9,]  477      0\n[10,]  563      1\n[11,]  638      1\n[12,]  744      0\n[13,]  769      0\n[14,]  770      0\n[15,]  803      0\n[16,]  855      0\n[17,] 1040      0\n[18,] 1106      0\n[19,] 1129      0\n[20,] 1206      0\n[21,] 1227      0\n[22,]  268      1\n[23,]  329      1\n[24,]  353      1\n[25,]  365      1\n[26,]  377      0\nattr(,\"type\")\n[1] \"right\"\nattr(,\"class\")\n[1] \"Surv\"\n\n\nbut everything is handled properly when you add it to a dataframe (note the Surv at the top of the column to indicated that it is a survival-time object, encoding a survival time and survivorship status both).\n\\(\\blacksquare\\)\n\nIn the display of your response variable, some values are marked with a +. Why is that? Explain briefly. (If you use a technical term, you should explain what it means.)\n\nSolution\nThese are the censored observations. You can say this, but you also need to say what that means (this is the “technical term” referred to in the question). The observations with a + are individuals who were never observed to die, or who were still alive at the end of the study.\nI want you to demonstrate that you know what censored means, not just that you know when you have a censored observation.\nExtra: in a study like this, patients are typically “recruited” into the study at various different times. Patients who happened to be in the study near the beginning and who survived can have a large (censored) value of y (like those values over 1000 days). But a patient might join the study later on; if they survive, they might produce a censored observation with a small survival time, like the last value 377. I’m sure the doctor would have liked to follow them for longer, but the funding ran out, and the doctor had a paper to write. (There is some information in these small censored values, but not much, because most of the patients, even the ones who eventually died, survived for longer than 377 days.)\nThe other thing that might have happened is that a patient with the 377-censored value died from something else unrelated to ovarian cancer. The study is only concerned with deaths from ovarian cancer, so such a patient is treated as censored at their death time. After this point we cannot assess how long this patient survived ovarian cancer.\n\\(\\blacksquare\\)\n\nFit a Cox proportional-hazards model for predicting survival time from age and treatment. Note that the numeric values for treatment make sense only as labels for the two treatments, so in your model formula make treatment into a factor. Display the results.\n\nSolution\nThe hint suggests something like this:\n\ntime.1 &lt;- coxph(y ~ age + factor(rx), data = ovarian)\nsummary(time.1)\n\nCall:\ncoxph(formula = y ~ age + factor(rx), data = ovarian)\n\n  n= 26, number of events= 12 \n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nage          0.14733   1.15873  0.04615  3.193  0.00141 **\nfactor(rx)2 -0.80397   0.44755  0.63205 -1.272  0.20337   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nage            1.1587      0.863    1.0585     1.268\nfactor(rx)2    0.4475      2.234    0.1297     1.545\n\nConcordance= 0.798  (se = 0.076 )\nLikelihood ratio test= 15.89  on 2 df,   p=4e-04\nWald test            = 13.47  on 2 df,   p=0.001\nScore (logrank) test = 18.56  on 2 df,   p=9e-05\n\n\nAlternatively, define the factor version of rx in the data frame first. This is the slick way to do that:\n\ntime.1a &lt;- ovarian %&gt;%\n  mutate(rxf = factor(rx)) %&gt;%\n  coxph(y ~ age + rxf, data = .)\nsummary(time.1a)\n\nCall:\ncoxph(formula = y ~ age + rxf, data = .)\n\n  n= 26, number of events= 12 \n\n         coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nage   0.14733   1.15873  0.04615  3.193  0.00141 **\nrxf2 -0.80397   0.44755  0.63205 -1.272  0.20337   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     exp(coef) exp(-coef) lower .95 upper .95\nage     1.1587      0.863    1.0585     1.268\nrxf2    0.4475      2.234    0.1297     1.545\n\nConcordance= 0.798  (se = 0.076 )\nLikelihood ratio test= 15.89  on 2 df,   p=4e-04\nWald test            = 13.47  on 2 df,   p=0.001\nScore (logrank) test = 18.56  on 2 df,   p=9e-05\n\n\nThe answer is the same either way. The data=. means “use the data frame that came out of the previous step, the one with rxf in it.”\nAlternatively, use the dataframe I called ov2 with y in it:\n\ntime.1b &lt;- coxph(y ~ age + factor(rx), data = ov2)\nsummary(time.1b)\n\nCall:\ncoxph(formula = y ~ age + factor(rx), data = ov2)\n\n  n= 26, number of events= 12 \n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nage          0.14733   1.15873  0.04615  3.193  0.00141 **\nfactor(rx)2 -0.80397   0.44755  0.63205 -1.272  0.20337   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nage            1.1587      0.863    1.0585     1.268\nfactor(rx)2    0.4475      2.234    0.1297     1.545\n\nConcordance= 0.798  (se = 0.076 )\nLikelihood ratio test= 15.89  on 2 df,   p=4e-04\nWald test            = 13.47  on 2 df,   p=0.001\nScore (logrank) test = 18.56  on 2 df,   p=9e-05\n\n\n\\(\\blacksquare\\)\n\nIs there a significant difference between the treatments in terms of their effects on survival (from ovarian cancer)?\n\nSolution\nLook at the P-value for my factor(rx)2, 0.203. This is not small, so there is no evidence of a difference between treatments.\nExtra: the reason for the odd label is that we have turned treatment into a categorical variable; treatment 1 is used as the baseline, and the negative slope says that the “hazard of death” is lower for treatment 2 than for treatment 1: that is, people survive longer on treatment 2, but the difference is not big enough to be significant (we also have a smallish sample size).\nSince there are only two treatments, it would in fact have been OK to leave them as numbers (with two numbers one unit apart, the slope would have been the same size as here), but I think it’s a good idea to treat categorical variables as categorical. My own habit is to use letters or something non-numerical to distinguish categories. I might have used t1 and t2 in this case, or the names of the different treatments.\n\\(\\blacksquare\\)\n\nIs there a significant effect of age? If there is, describe the effect that age has on survival.\n\nSolution\nThe P-value for age is 0.0014, small, so age definitely has a significant effect on survival. As to what kind of effect, look at the slope coefficient, 0.15, positive, which means that increased age-at-diagnosis goes with an increased hazard of death, or that older patients do not survive as long.\nI would like you to get to the plain-English words at the end. Part of your job as a statistician is explaining what you got to people who are doctors, managers, etc., who won’t understand the terminology.\nThus, one mark for assessing significance via P-value, one for looking at the slope coefficient and noting that it is positive, and one for getting to “older patients do not survive as long”, or “older patients have a larger chance of dying sooner”. (Strictly, this is also “all else equal” as usual, since survival time might also have depended on treatment, but the point of this question is for you to get to “older patients do not survive as long”.)\n(The interpretation of the slope may seem backwards: a positive slope means a shorter survival time for a larger age. This is why I talk about “hazard of death”, since that guides us to the correct interpretation.)\nExtra: I was curious about what would happen if I just included rx in the model:\n\ntime.2 &lt;- update(time.1, . ~ . - age)\nsummary(time.2)\n\nCall:\ncoxph(formula = y ~ factor(rx), data = ovarian)\n\n  n= 26, number of events= 12 \n\n               coef exp(coef) se(coef)      z Pr(&gt;|z|)\nfactor(rx)2 -0.5964    0.5508   0.5870 -1.016     0.31\n\n            exp(coef) exp(-coef) lower .95 upper .95\nfactor(rx)2    0.5508      1.816    0.1743      1.74\n\nConcordance= 0.608  (se = 0.07 )\nLikelihood ratio test= 1.05  on 1 df,   p=0.3\nWald test            = 1.03  on 1 df,   p=0.3\nScore (logrank) test = 1.06  on 1 df,   p=0.3\n\n\nStill not significant, but this model is a lot worse because I took out the significant age. What this is doing is mixing up all the people of different ages (and we know that age has an effect on survival) and trying (and failing) to discern an effect of treatment.\nWe could have been badly misled by this model if one of the treatments had predominantly older patients. We know that older patients have worse survival, so the treatment with older patients would have looked worse, even if it actually wasn’t. The model time.1 which contained age properly adjusted for the effect of age, so that was the best way to see whether there was a difference between treatments.\nWhat you often see early on in a paper on this kind of stuff is a graph showing that the treatment groups are similar in terms of important things like age. Here, that could be a boxplot:\n\nggplot(ovarian, aes(x = factor(rx), y = age)) + geom_boxplot()\n\n\n\n\n0.14733\nI needed to do factor(rx) because geom_boxplot needs a genuine categorical variable, not just a numerical variable masquerading as one. If you just leave it as rx, as I discovered, you get one boxplot of all the ages together regardless of treatment. The key, for you as user of software, is not (necessarily) to get it right the first time, but to know what to do to fix up the errors you will inevitably get. If you have worked through the boxplot examples in C32 and D29, you will have enough experience to remember that a boxplot has to have a categorical x (text will do, but definitely not numbers). This is why I give you so many things to work through: so that you gain the experience to know how to fix up problems.\nTreatment 1 has a larger spread of ages and treatment 2 has a low outlier age, but the median ages are very similar.\n\\(\\blacksquare\\)\n\nMake a martingale residual plot for this model. Do you see any problems? Explain briefly.\n\nSolution\nThe plot is just the same idea as the one in the notes. Make sure you have survminer installed and loaded:\n\nggcoxdiagnostics(time.1) + geom_smooth()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nMake a call about whether you think the smooth trend deviates too much from the red dotted line going across at zero. Martingale residuals can get very negative (and that is OK), so that residual of \\(-2\\) is not a problem, and this is pulling the smooth trend down a bit (it’s the reason for the dip on the right side of the smooth trend). So I’d call this OK, but you can make whatever call you like as long as it’s supported by what you see here.\nI observe that the obvious fixable thing is where you have a curve here, one that looks like a parabola (at which point you add squared terms to your explanatory variables and see if that helps, as for bmi in one of the other problems). This one is too wiggly to be a parabola (it bends twice), and so is more like a cubic than anything.\nThe other thing you can note is that the grey envelope is “not significantly different from zero”, since 0 is clearly within the grey envelope all the way across.\n\\(\\blacksquare\\)\n\nFind the quartiles of age, and make a data frame containing all combinations of those two ages and the two treatments. Display what you have. (Feel free to copy the values by hand, rather than trying to save them and use them.)\n\nSolution\nI imagine you can guess what we are going to be doing with these: predictions, so we’ll call the data frame new when we get there.\nQuartiles first:\n\nquantile(ovarian$age)\n\n      0%      25%      50%      75%     100% \n38.89320 50.16712 56.84660 62.37810 74.50410 \n\n\nor, if you prefer,\n\novarian %&gt;%\n  summarize(\n    q1 = quantile(age, 0.25),\n    q3 = quantile(age, 0.75)\n  )\n\n\n\n  \n\n\n\nThe quartiles are 50.17 and 62.38 (rounding slightly).\nEither way is good.\nThen follow my standard procedure (or one of your own devising), remembering that “treatment” is called rx here:\n\nages &lt;- c(50.17, 62.38)\nrxs &lt;- c(1, 2)\nnew &lt;- datagrid(model = time.1, age = ages, rx = rxs)\n\n\\(\\blacksquare\\)\n\nObtain predicted survival probabilities for each of your age-treatment combinations, for each of a variety of survival times. (This is only one thing, despite it sounding like a lot.)\n\nSolution\nThe magic word here is survfit. The structure is the same as for predictions:\n\ns &lt;- survfit(time.1, new, data = ovarian)\nsummary(s)\n\nCall: survfit(formula = time.1, newdata = new, data = ovarian)\n\n time n.risk n.event survival1 survival2 survival3 survival4\n   59     26       1     0.993     0.997     0.959     0.981\n  115     25       1     0.985     0.993     0.911     0.959\n  156     24       1     0.973     0.988     0.846     0.928\n  268     23       1     0.959     0.981     0.777     0.893\n  329     22       1     0.932     0.969     0.653     0.826\n  353     21       1     0.905     0.956     0.548     0.764\n  365     20       1     0.877     0.943     0.452     0.701\n  431     17       1     0.843     0.926     0.356     0.630\n  464     15       1     0.805     0.908     0.271     0.557\n  475     14       1     0.768     0.888     0.202     0.489\n  563     12       1     0.701     0.853     0.117     0.382\n  638     11       1     0.634     0.816     0.064     0.292\n\n\nI didn’t ask you to display it, so doing so is optional. Also, you don’t need (here) that data=ovarian (the predictions will work just fine without it), but the plot coming up will not. So my recommendation is to put it in.\nExtra: The four columns survival1 through survival4 are the survival probabilities for the times shown in the left-hand column (these are numbers of days), for the four rows of my new, in the same order.\nThere are only a few different times, because these are the numbers of days at which somebody in the data set died, and the estimated survival probability does not change at the times in between these. (You’ll see this on a plot in a minute.)\nThese survival probabilities are pretty easy to eyeball: the best survival is in stratum 2, which is the younger patients in treatment 2. This we’ll come back to.\n\\(\\blacksquare\\)\n\nDraw a plot that compares the survival probabilities at the different times.\n\nSolution\nThus. The conf.int=F means to skip the confidence interval “envelopes” that I find make the plot rather messy:\n\nggsurvplot(s, conf.int = FALSE)\n\n\n\n\n\\(\\blacksquare\\)\n\nAccording to your plot, how would you describe the effects of treatment and of age? 50.17 1\n50.17 2\n\nSolution\nThe best survival curve, in terms of surviving longest, is up and to the right, so the green one is best and the blue one is worst.18 To figure out which those are, we have to go back to the data frame we created:\n\nnew\n\n\n\n  \n\n\n\nTo see the effect of age, compare strata 1 and 3 (or 2 and 4). This means comparing the red and blue curves; the red one is clearly better (in the sense of longer survival time), which means that age has a big effect on survival, with younger people living longer, other things being equal (that we saw earlier was significant). You could equally well compare the green and purple survival curves and come to the same conclusion.\nTo assess the effect of treatment, compare strata 1 and 2 (red and green), or strata 3 and 4 (blue and purple). In both cases, the stratum corresponding to treatment 2 has slightly better survival (has a higher chance of living for longer), but there is not as big an effect as for age. (You’ll recall that the treatment difference was not significant).\nState or imply that you know which stratum is which, say something about the effects of age and of treatment, including which one is better.\nExtra: recall the output from the Cox model:\n\nsummary(time.1)\n\nCall:\ncoxph(formula = y ~ age + factor(rx), data = ovarian)\n\n  n= 26, number of events= 12 \n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nage          0.14733   1.15873  0.04615  3.193  0.00141 **\nfactor(rx)2 -0.80397   0.44755  0.63205 -1.272  0.20337   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nage            1.1587      0.863    1.0585     1.268\nfactor(rx)2    0.4475      2.234    0.1297     1.545\n\nConcordance= 0.798  (se = 0.076 )\nLikelihood ratio test= 15.89  on 2 df,   p=4e-04\nWald test            = 13.47  on 2 df,   p=0.001\nScore (logrank) test = 18.56  on 2 df,   p=9e-05\n\n\nThe slope coefficient for treatment 2 (as compared to the baseline treatment 1) was \\(-0.83097\\), negative, which meant that patients on treatment 2 had a lower hazard of death than patients on treatment 1: that is, that treatment 2 was better for survival than treatment 1. That is what the plot said also (and the relatively small difference is consistent with that difference not being significant).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "survival-analysis.html#footnotes",
    "href": "survival-analysis.html#footnotes",
    "title": "27  Survival analysis",
    "section": "",
    "text": "Heart attack.↩︎\nWorcester is pronounced, by locals, Woo-stuh.↩︎\nBarbecued pork in a bun. A staple of Chinese dim sum and Chinese bakeries, such as Ding Dong bakery on Spadina.↩︎\nHeart attack.↩︎\nWorcester is pronounced, by locals, Woo-stuh.↩︎\nNot exactly the same as that output, because it is doing a test that would be the same if you had an infinitely large sample, but is slightly different with an ordinary finite number of observations.↩︎\nOur categorical variable gender has only two levels.↩︎\nIt doesn’t work with text columns, but it does work if you temporarily turn the text columns into factors, eg. by using mutate with where. However, we don’t have any text columns here, so what we do here is good for this data set.↩︎\ncase_when is much clearer than using nested if-elses when you have three or more categories, as for herco.↩︎\nSome people define the response variable right inside the coxph, in the same way as putting something like log(y) as a response in an lm, but I think, especially while you’re getting used to the process, it’s better to create the response variable first and look at it to make sure it’s the right thing.↩︎\nThe researchers were probably relieved that there was not quite a significant effect of site.↩︎\nNot at the 0.05 level, anyway.↩︎\nBarbecued pork in a bun. A staple of Chinese dim sum and Chinese bakeries, such as Ding Dong bakery on Spadina.↩︎\nDon’t confuse this with glance from broom, which gives a one-line summary of a model, containing things like R-squared and a test for the overall model significance.↩︎\nBecause summarize will only allow you to have a single-number answer.↩︎\nThe way, as we have seen elsewhere, is to use tidy(quantile) or enframe(quantile), which produce a two-column data frame with the percentiles shown.↩︎\nStrata is plural; the singular is stratum. Like data and datum.↩︎\nIn other cases, having the event happen sooner is better. For example, you might be poisoning rats, in which case you want them to die quicker. Or the event might be something desirable like becoming qualified to fly an airplane. In those cases, down and to the left is better.↩︎"
  },
  {
    "objectID": "anova-revisited.html#acid-rain",
    "href": "anova-revisited.html#acid-rain",
    "title": "28  Analysis of variance revisited",
    "section": "28.1 Acid rain",
    "text": "28.1 Acid rain\nOne of the first noticeable effects of climate change was “acid rain”. This is formed by the water vapour in clouds combining with nitrous oxide and sulfur dioxide, which come from coal and oil production. How does the acidity of rain affect the acidity of the soil on which it falls? (This will have an effect on the kind of plants that can be grown in that soil.) Acidity is measured using the pH scale, where a pH of 7 is chemically neutral, a number less than 7 is acidic, and a number greater than 7 is alkaline.\nAn experiment was conducted at the Florida Institute of Food and Agricultural Sciences, to determine how acidity of rain affects soil acidity. Experimental plots were irrigated with rainwater that was prepared to have one of two different pH levels, 3.7 and 4.5. The acidity of the soil was then measured at three different depths, 0–15, 15–30, and 30–46 centimetres. This was done on three different dates, in April and June 1991. The data are in link.\n\nRead in and display the data.\nDraw a grouped boxplot to show how soil acidity depends on the pH of the rain and the soil depth. (The first time you do this you might not get the boxplot you want. How can you fix that?)\nWhat does your grouped boxplot say about likely interactions? Explain briefly.\nFit an ANOVA with interaction. What do you conclude from it? You may want to create a column that is the factor version of rain_pH first."
  },
  {
    "objectID": "anova-revisited.html#treating-hay-fever",
    "href": "anova-revisited.html#treating-hay-fever",
    "title": "28  Analysis of variance revisited",
    "section": "28.2 Treating hay fever",
    "text": "28.2 Treating hay fever\nHay fever is an allergic reaction to things like grasses or pollen which can make it unpleasant to go outside when the weather is at its best. A research lab is developing a new compound to relieve severe cases of hay fever. There were two active ingredients, labelled A and B, with each one having three levels, low, medium and high. There were thus \\(3\\times 3=9\\) treatment combinations. 36 subjects were available, and these were randomly assigned to combinations of levels of factor A and factor B, so that each combination was tested on 4 people. The response variable was the number of hours of relief from symptoms (so a higher value is better). The data are in link.\n\nRead the data and display its structure. Verify that you have what you were expecting.\nCalculate the mean hours of relief for each combination of levels of the two active ingredients. Save your results in a data frame and display that.\nMake an interaction plot, showing how the mean pain relief depends on the combination of levels of A and B. It is probably easiest to use the data frame you obtained in the previous part.\nWhat do you conclude from your interaction plot? Explain briefly.\nRun an analysis of variance with interaction. What do you conclude?\nAnalyze the simple effects of B when A is medium. (This means doing an appropriate aov and an appropriate Tukey, if warranted.)\nAnalyze the simple effects of B when A is high."
  },
  {
    "objectID": "anova-revisited.html#focused-comparisons-of-the-effect-of-caffeine",
    "href": "anova-revisited.html#focused-comparisons-of-the-effect-of-caffeine",
    "title": "28  Analysis of variance revisited",
    "section": "28.3 Focused comparisons of the effect of caffeine",
    "text": "28.3 Focused comparisons of the effect of caffeine\nDoes caffeine help students do better on a certain test? To find out, 36 students were randomly allocated to three groups (12 in each group). Each student received a fixed number of cups of coffee while they were studying, but the students didn’t know whether they were receiving all full-strength coffee (“high”), all decaf coffee (“low”) or a 50-50 mixture of the two (“moderate”). For each subject, their group was recorded as well as their score on the test. The data are in link, as a .csv file.\nThis time we look at contrasts. Suppose I knew ahead of time that I wanted to compare moderate caffeine with high, and any caffeine with none. (In the latter case, we’re comparing “no caffeine” against the average of the other two groups.)\nIn the previous go-through of the caffeine data, we discovered that amount was actually text rather than a factor, but we also discovered that it didn’t matter. Here it does matter, so the first thing we have to do is to re-do the pivot_longer, creating a factor version of amount.\n\nRead in the data again, from link, and display it. This is the untidy format, so name it appropriately:\nCopy your pivot_longer from before, only this time add names_ptypes = list(amount=factor()) to the end of it. Take a look at the results. What has changed from before?\nUsing the newly tidied caffeine data, run the ANOVA as a regression (that is, using lm). Look at the summary of the output. What do you see?\nObtain the different values of amount, in the order that R has them.\nCreate a contrast that compares High with Moderate, ignoring None. That is, create a vector whose length is the same as the number of levels of amount, and which has a 1 to represent High and a \\(-1\\) to represent Moderate.\nCreate a contrast that compares “any caffeine” against “none” by comparing None against the average of Moderate and High.\nVerify that your two contrasts are orthogonal.\nArrange your contrasts as columns of a matrix (using cbind), and say that you want to use these as contrasts for amount (in data frame caffeine or whatever you called it).\nFit the ANOVA as an lm, and look at the summary. What do you conclude about your contrasts?\nWhat happens if you try to use high caffeine vs. moderate caffeine and moderate vs. none as your two contrasts?"
  },
  {
    "objectID": "anova-revisited.html#who-studies-the-most-outside-class",
    "href": "anova-revisited.html#who-studies-the-most-outside-class",
    "title": "28  Analysis of variance revisited",
    "section": "28.4 Who studies the most outside class?",
    "text": "28.4 Who studies the most outside class?\nA social scientist wanted to see how many hours students studied outside of class. She took a random sample of 75 students from three different majors: math, social science and English, and recorded the number of weekly outside-class study hours for each student. The data can be found at link. The social scientist had two research questions: whether math students study more outside of class than the other students, and whether English and social science students study different amounts of time outside class.\n\nExplain briefly why contrasts would be a better idea here than ANOVA with Tukey.\nRead in the data and make side-by-side boxplots of study hours by major. What do the boxplots suggest about the comparisons that the researcher wants to make?\nSet up contrasts for each of the researcher’s research questions, bearing in mind the levels of major and the order in which they come. (For the Math contrast, you want Math against the average of the other two.)\nVerify that your two contrasts are orthogonal.\nCreate a matrix of contrasts (using cbind) and let lm know that these are contrasts for major, in data frame studyhours (or whatever you called it).\nFit the model using lm. What do you conclude about your contrasts?\nAre you surprised by the significance or non-significance you found in the previous part? Explain briefly."
  },
  {
    "objectID": "anova-revisited.html#mental-context",
    "href": "anova-revisited.html#mental-context",
    "title": "28  Analysis of variance revisited",
    "section": "28.5 Mental context",
    "text": "28.5 Mental context\nIt is believed that being in the same mental context for learning and for testing leads to better test scores. An experiment was carried out to test this. During the learning phase, subjects learned a list of 80 words in a room painted orange and decorated with posters, paintings and other paraphernalia.1 A memory test was given to all subjects immediately after they had learned the words, to give the impression that the experiment was over. (The results of this test were discarded.) One day later, subjects were unexpectedly re-tested under different experimental conditions and asked to write down all the words of the original list that they could remember. The re-test took place in five different conditions, which subjects were randomly allocated to one of:\n\nSame context: in the original orange-painted room.\nDifferent context: in a very different room, painted grey and located in a different part of campus.\nImaginary context: in the grey room, but subjects are asked to remember the orange room in which they took the original test. To help them with this, the experimenter begins by asking them several questions about the orange room and the objects in it.\nPhotographed context: in the grey room, but subjects are shown photographs of the orange room.\nPlacebo context: in the grey room, with subjects first being asked to recall their living room.\n\nIn each case, the response variable was the number of words on the original list successfully recalled by each subject.\n50 subjects in total completed the experiment, 10 under each of the 5 conditions.\nThe researchers had four research hypotheses to test with the data:\n\nGroups where the context matches (is the same, or is simulated by imagining or photograph) will perform better than groups with different or placebo contexts.\nThe group with the same context will differ from the group with imaginary or photographed contexts.\nThe imaginary-context group will differ from the photographed-context group.\nThe different-context group will differ from the placebo-context group.\n\nThe research hypotheses are (as is usual) written as alternative hypotheses. We can rewrite them as null hypotheses, with some extra wording to facilitate converting them into contrasts, like this:\n\nThe mean of the same, imaginary and photograph groups (group means) is equal to the mean of the different and placebo groups (group means).\nThe mean of the imaginary and photograph groups is equal to the (mean of the) same group.\nThe imaginary and the photograph groups will have the same mean.\nThe different and the placebo groups will have the same mean.\n\nThe data are in link (the original researcher’s name was Smith).\n\nRead in the data and verify that you have a column called context that is text and a column called words that is a (whole) number.\nTurn context into a factor, within the data frame. (We are going to be doing contrasts). Display how the data frame now looks.\nWhat are the names of the five contexts in the data set (just display them), and what order are they in?\nWrite each of the four research hypotheses (in the null-hypothesis versions) as R vectors that can be used to make contrasts. (This will mean getting the contexts in the right order. If you get stuck, do the last two first, since they’re easiest. The first one is actually the hardest.)\nPick two of your contrasts (doesn’t matter which two) and demonstrate that they are orthogonal.\nCollect your contrasts together into a matrix, and tell lm that these are the contrasts for context.\nFit a model with lm, and display the results.\nFor each of the original research hypotheses, what do you infer about them?"
  },
  {
    "objectID": "anova-revisited.html#trying-on-shirts",
    "href": "anova-revisited.html#trying-on-shirts",
    "title": "28  Analysis of variance revisited",
    "section": "28.6 Trying on shirts",
    "text": "28.6 Trying on shirts\nDoes it make a difference if you see somebody else trying on a shirt before you do, and if so, does it matter what kind of person it is?\nAn experiment was carried out in a university bookstore, with students who wanted to try on a shirt serving as (unsuspecting) experimental subjects. When a student wanted to try on a shirt, the sales associate told them that there was only one shirt left, and it was being tried on by an “other customer”. This “other customer” was actually a “confederate” of the experimenter (that means, they were pretending to be a real customer but were actually part of the experiment). The “other customer” was always female: either an attractive well-dressed model, or an average-looking student wearing jeans. The “other customer” would come out of the dressing room and hand the shirt to the sales associate, who would give it to the student who wanted to try it on. When the student had tried on the shirt, they were asked to rate it on a 7-point scale (on five different dimensions, and those five scores were then averaged). Does a student’s evaluation depend on whether the student was male or female, and whether the “other customer” was a model or a student? There was also a control group, where the student was handed the shirt directly by the sales associate, without the “other customer” being involved at all.\nThere were thus five treatments: male students who saw a model, male students who saw a student, female students who saw a model, female students who saw a student, and the control group. There were 25 students in each treatment group.\nThe data from this experiment can be found at link.\n\nRead in and display the data. How many observations do you have, and is that what you should have?\nTurn treatment into a factor in your data frame. (You can use the same name as the text treatment that you read in from the file.)\nList the treatments in the order that they are in within your factor. (There are lots of ways to do this; any one of them except for distinct is good.)\n* Obtain a table of mean evaluation scores for each treatment group.\nThe experimenters wanted to compare four specific things in their analysis:\n\n\nevaluation scores between male students who saw a (female) model and male students who saw a (female) student\nevaluation scores between female students who saw a (female) model and female students who saw a (female) student\nevaluation scores for male and for female students (overall)\nevaluation scores for the (average of the) four genuine treatments and for the control group\n\nCreate contrasts, with suitable names, using vectors with appropriate values.\n\nPick two of your contrasts (doesn’t matter which two) and demonstrate that they are orthogonal.\nCollect all your contrasts together into a matrix and declare that they are contrasts for treatment within your data frame (whatever you called it).\nPredict evaluation score from treatment as a regression, and display the results.\nFor each of your contrasts, assess whether or not it is significant, and explain briefly what that means in the context of the data. If a contrast is significant, use your answer to part (here) to help in your interpretation.\n\nMy solutions follow:"
  },
  {
    "objectID": "anova-revisited.html#acid-rain-1",
    "href": "anova-revisited.html#acid-rain-1",
    "title": "28  Analysis of variance revisited",
    "section": "28.7 Acid rain",
    "text": "28.7 Acid rain\nOne of the first noticeable effects of climate change was “acid rain”. This is formed by the water vapour in clouds combining with nitrous oxide and sulfur dioxide, which come from coal and oil production. How does the acidity of rain affect the acidity of the soil on which it falls? (This will have an effect on the kind of plants that can be grown in that soil.) Acidity is measured using the pH scale, where a pH of 7 is chemically neutral, a number less than 7 is acidic, and a number greater than 7 is alkaline.\nAn experiment was conducted at the Florida Institute of Food and Agricultural Sciences, to determine how acidity of rain affects soil acidity. Experimental plots were irrigated with rainwater that was prepared to have one of two different pH levels, 3.7 and 4.5. The acidity of the soil was then measured at three different depths, 0–15, 15–30, and 30–46 centimetres. This was done on three different dates, in April and June 1991. The data are in link.\n\nRead in and display the data.\n\nSolution\nThis time, it’s a .csv:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/acidrain.csv\"\nacidrain &lt;- read_csv(my_url)\n\nRows: 18 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): soil_depth\ndbl  (2): rain_pH, soil_acidity\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nacidrain\n\n\n\n  \n\n\n\nThere are 4 columns, soil depth, date, pH of the rain that was applied (all explanatory) and soil acidity, the response. You’ll note that we have one soil acidity value per combination of the other things (it was a designed experiment).\nWe’re going to ignore the date for now, treating the observations on different dates as replicates.\n\\(\\blacksquare\\)\n\nDraw a grouped boxplot to show how soil acidity depends on the pH of the rain and the soil depth. (The first time you do this you might not get the boxplot you want. How can you fix that?)\n\nSolution\nThe problem is likely to be that either your x or your fill for your boxplot is numerical (rain_pH is dbl) rather than the categorical variable you need. Try to use one of the explanatory variables as x and the other one as fill (or colour):\n\nggplot(acidrain, aes(x = rain_pH, y = soil_acidity, fill = soil_depth)) +\n  geom_boxplot()\n\n\n\n\nThat looks as if it worked, but it didn’t. See the boxplot below for how it’s supposed to be. I need x for the boxplot needs to be categorical. The easiest way to make it such is to wrap it in factor:\n\nggplot(acidrain, aes(x = factor(rain_pH), y = soil_acidity, fill = soil_depth)) +\n  geom_boxplot()\n\n\n\n\nEven though soil_depth looks like numbers, the printout of the data frame reveals that it is text, so that is OK.\nIf you prefer, exchange x and fill:\n\nggplot(acidrain, aes(fill = factor(rain_pH), y = soil_acidity, x = soil_depth)) +\n  geom_boxplot()\n\n\n\n\n\\(\\blacksquare\\)\n\nWhat does your grouped boxplot say about likely interactions? Explain briefly.\n\nSolution\nThink about the effect of one of your explanatory variables, for each level of the other. For example, in the last plot, the effect of the rain pH pm on the soil acidity is very small at the largest and smallest depths, but at the middle soil depth 15–30, the average (median) soil acidity is a lot less when the rain pH is higher (which seems odd to me). The effect of rain pH being different according to soil pH is what suggests an interaction effect. Another way of thinking about this is imagining what an interaction plot would look like. This would be a trace going through the middle (strictly, mean rather than median) of each set of boxplots of one colour. In the last boxplot, the red trace would go close to straight across, while the blue one would dip in the middle. Not parallel, so suggesting an interaction. Either approach is good.\n\\(\\blacksquare\\)\n\nFit an ANOVA with interaction. What do you conclude from it? You may want to create a column that is the factor version of rain_pH first.\n\nSolution\nFollowing my own hint:\n\nacidrain &lt;- acidrain %&gt;% mutate(frph = factor(rain_pH))\nsoil.1 &lt;- aov(soil_acidity ~ frph * soil_depth, data = acidrain)\nsummary(soil.1)\n\n                Df Sum Sq Mean Sq F value Pr(&gt;F)\nfrph             1 0.0304 0.03042   0.759  0.401\nsoil_depth       2 0.0671 0.03357   0.838  0.457\nfrph:soil_depth  2 0.0078 0.00391   0.097  0.908\nResiduals       12 0.4810 0.04008               \n\n\nThe interaction is nowhere near significant, perhaps surprisingly. But bear in mind that there are only 18 observations in total, so each box on the boxplot is based on three observations only. So the interaction would have to be a lot bigger to be significant.\nThe usual procedure after finding a non-significant interaction is to take it out:\n\nsoil.2 &lt;- update(soil.1, . ~ . - frph:soil_depth)\nsummary(soil.2)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nfrph         1 0.0304 0.03042   0.871  0.366\nsoil_depth   2 0.0671 0.03357   0.962  0.406\nResiduals   14 0.4888 0.03492               \n\n\nThe P-values have come down a bit (the result of gaining two df for error while the error SS only got a tiny bit bigger), but not nearly enough to be significant.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "anova-revisited.html#treating-hay-fever-1",
    "href": "anova-revisited.html#treating-hay-fever-1",
    "title": "28  Analysis of variance revisited",
    "section": "28.8 Treating hay fever",
    "text": "28.8 Treating hay fever\nHay fever is an allergic reaction to things like grasses or pollen which can make it unpleasant to go outside when the weather is at its best. A research lab is developing a new compound to relieve severe cases of hay fever. There were two active ingredients, labelled A and B, with each one having three levels, low, medium and high. There were thus \\(3\\times 3=9\\) treatment combinations. 36 subjects were available, and these were randomly assigned to combinations of levels of factor A and factor B, so that each combination was tested on 4 people. The response variable was the number of hours of relief from symptoms (so a higher value is better). The data are in link.\n\nRead the data and display its structure. Verify that you have what you were expecting.\n\nSolution\nAligned columns separated by spaces, so read_table:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/hayfever.txt\"\nhayfever &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  relief = col_double(),\n  a = col_character(),\n  b = col_character(),\n  replicate = col_double()\n)\n\nhayfever\n\n\n\n  \n\n\n\nI have 36 observations (patients). There are two categorical columns a and b corresponding to the two active ingredients, and they each seem to have levels low, medium and high.2\nThe replicate column labels each observation within its A-B combination, so that each treatment combination was indeed replicated four times. We won’t be using this column in our analysis; I think it’s a refugee from the original untidy format the data was in before it came to you.\n\\(\\blacksquare\\)\n\nCalculate the mean hours of relief for each combination of levels of the two active ingredients. Save your results in a data frame and display that.\n\nSolution\nThis is a group-by and summarize, but there are two active ingredients and they both have to go in the group-by:\n\nhayfever %&gt;%\n  group_by(a, b) %&gt;%\n  summarize(m = mean(relief)) -&gt; d\n\n`summarise()` has grouped output by 'a'. You can override using the `.groups`\nargument.\n\nd\n\n\n\n  \n\n\n\nI’m using my usual name d for a temporary data frame. I could have put brackets around my whole pipeline to display its result, but I still need to save the data frame d to use in a moment.\nThese levels are in the wrong logical order, but they are in the right order in the original data frame, so we can use fct_inorder first, thus:\n\nhayfever %&gt;%\n  mutate(a = fct_inorder(a), b = fct_inorder(b)) %&gt;%\n  group_by(a, b) %&gt;%\n  summarize(m = mean(relief)) -&gt; d2\n\n`summarise()` has grouped output by 'a'. You can override using the `.groups`\nargument.\n\nd2\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake an interaction plot, showing how the mean pain relief depends on the combination of levels of A and B. It is probably easiest to use the data frame you obtained in the previous part.\n\nSolution\nMy column of mean relief values is called m. Use whatever name you gave it. I’m going to use my proper-order data frame for this:\n\nggplot(d2, aes(x = a, y = m, colour = b, group = b)) +\n  geom_point() + geom_line()\n\n\n\n\nOr, you probably had this:\n\nggplot(d, aes(x = a, y = m, colour = b, group = b)) +\n  geom_point() + geom_line()\n\n\n\n\nSince a and b both have three levels, you could just as well use them the other way around:\n\nggplot(d2, aes(x = b, y = m, colour = a, group = a)) +\n  geom_point() + geom_line()\n\n\n\n\nThe plot looks different depending on how you draw it, but the conclusion from it (below) will be the same.\n\\(\\blacksquare\\)\n\nWhat do you conclude from your interaction plot? Explain briefly.\n\nSolution\nThe three lines are not particularly close to being parallel, so I would expect to see an interaction between the two active ingredients (that is, the number of hours of pain relief depends on the combination of the two of them).\nExtra: It is always a judgment call to decide whether the lines are “approximately parallel” or not. It depends on how much variability there is in the data, which the interaction plot doesn’t show. Another idea is to add the data to the interaction plot, colour-coded in the same way. I would do this by taking out the geom_point for the means and add one instead for the data, taken from the original data frame:\n\nggplot(d2, aes(x = a, y = m, colour = b, group = b)) +\n  geom_line() +\n  geom_point(data = hayfever, aes(y = relief))\n\n\n\n\nTechnique: for that last geom_point, put in anything that changes: a new data frame, and a new y for the plot, but the x and colour and group are the same as they were before, so I don’t need to specify them.\nThe points are very close to the lines, so there is almost no residual variability. This makes it more convincing that the interaction is real and will be significant.\n\\(\\blacksquare\\)\n\nRun an analysis of variance with interaction. What do you conclude?\n\nSolution\n\nhayfever.1 &lt;- aov(relief ~ a * b, data = hayfever)\nsummary(hayfever.1)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \na            2 220.02  110.01  1827.9 &lt;2e-16 ***\nb            2 123.66   61.83  1027.3 &lt;2e-16 ***\na:b          4  29.42    7.36   122.2 &lt;2e-16 ***\nResiduals   27   1.63    0.06                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe interaction is (strongly) significant. The hours of relief depend on the combination of levels of the active ingredients A and B.\nDon’t try to interpret the main effects of A and B from here. That’s what simple effects are for, coming up.\n\\(\\blacksquare\\)\n\nAnalyze the simple effects of B when A is medium. (This means doing an appropriate aov and an appropriate Tukey, if warranted.)\n\nSolution\nFirst, we pull out only the data where A is medium, and then we do a one-way analysis of B on that data. This is the slick way, though you can certainly save the result of filter first:\n\nhayfever %&gt;%\n  filter(a == \"medium\") %&gt;%\n  aov(relief ~ b, data = .) -&gt; simple_medium\nsummary(simple_medium)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nb            2  34.16  17.081   262.8 1.04e-08 ***\nResiduals    9   0.59   0.065                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is definitely some effect of ingredient B when A is medium. What is it? Tukey will tell us:\n\nTukeyHSD(simple_medium)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = relief ~ b, data = .)\n\n$b\n              diff       lwr       upr     p adj\nlow-high    -3.675 -4.178336 -3.171664 0.0000000\nmedium-high -0.200 -0.703336  0.303336 0.5323662\nmedium-low   3.475  2.971664  3.978336 0.0000000\n\n\nThere is no difference between medium and high (levels of B), but both of these are better in terms of relief than low is.\n\\(\\blacksquare\\)\n\nAnalyze the simple effects of B when A is high.\n\nSolution\nSame idea: pull out only the data where A is high, do a one-way analysis of B, and do Tukey if needed:\n\nhayfever %&gt;%\n  filter(a == \"high\") %&gt;%\n  aov(relief ~ b, data = .) -&gt; simple_high\nsummary(simple_high)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nb            2 107.02   53.51     796 7.49e-11 ***\nResiduals    9   0.61    0.07                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(simple_high)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = relief ~ b, data = .)\n\n$b\n              diff       lwr       upr p adj\nlow-high    -7.275 -7.786868 -6.763132 0e+00\nmedium-high -2.975 -3.486868 -2.463132 2e-07\nmedium-low   4.300  3.788132  4.811868 0e+00\n\n\nWhen A is high, there is definitely an effect of B again, but this time high (for B) is better than medium is better than low. (All the P-values in Tukey are very small.)\nYou could guess this from the interaction plot as well (whichever one you came up with): when A is high, it is better for B to be high as well, but when A is medium or low, there is not much difference between B being medium or high.\nExtra: here, the effect of B is different, depending on what A is. This is what a significant interaction means. If there were no significant interaction, the effect of B would always be the same, no matter what A was.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "anova-revisited.html#focused-comparisons-of-the-effect-of-caffeine-1",
    "href": "anova-revisited.html#focused-comparisons-of-the-effect-of-caffeine-1",
    "title": "28  Analysis of variance revisited",
    "section": "28.9 Focused comparisons of the effect of caffeine",
    "text": "28.9 Focused comparisons of the effect of caffeine\nDoes caffeine help students do better on a certain test? To find out, 36 students were randomly allocated to three groups (12 in each group). Each student received a fixed number of cups of coffee while they were studying, but the students didn’t know whether they were receiving all full-strength coffee (“high”), all decaf coffee (“low”) or a 50-50 mixture of the two (“moderate”). For each subject, their group was recorded as well as their score on the test. The data are in link, as a .csv file.\nThis time we look at contrasts. Suppose I knew ahead of time that I wanted to compare moderate caffeine with high, and any caffeine with none. (In the latter case, we’re comparing “no caffeine” against the average of the other two groups.)\nIn the previous go-through of the caffeine data, we discovered that amount was actually text rather than a factor, but we also discovered that it didn’t matter. Here it does matter, so the first thing we have to do is to re-do the pivot_longer, creating a factor version of amount.\n\nRead in the data again, from link, and display it. This is the untidy format, so name it appropriately:\n\nSolution\n\nmy_url &lt;- \"http://individual.utoronto.ca/kbutler/stad29/caffeine.csv\"\ncaffeine.untidy &lt;- read_csv(my_url)\n\nRows: 12 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Sub, High, Moderate, None\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncaffeine.untidy\n\n\n\n  \n\n\n\nOne caffeine level per column, rather than a column of caffeine levels, so untidy.\n\\(\\blacksquare\\)\n\nCopy your pivot_longer from before, only this time add names_ptypes = list(amount=factor()) to the end of it. Take a look at the results. What has changed from before?\n\nSolution\nWe’ll save into caffeine again:\n\ncaffeine.untidy %&gt;%\n  pivot_longer(-Sub, names_to=\"amount\", values_to=\"score\", \n               names_ptypes = list(amount=factor())) -&gt; caffeine\ncaffeine\n\n\n\n  \n\n\n\nThe variable created for the “names” is now a factor: it was text before. Maybe we should have made it a factor before (it wouldn’t have done any harm), but we got away with not doing so.\n\\(\\blacksquare\\)\n\nUsing the newly tidied caffeine data, run the ANOVA as a regression (that is, using lm). Look at the summary of the output. What do you see?\n\nSolution\nExtra: I’m going to run the ANOVA the “old way” first, so that we can compare results. You don’t need to do this:\n\ncaffeine.old &lt;- aov(score ~ amount, data = caffeine)\nsummary(caffeine.old)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \namount       2  477.7  238.86   3.986 0.0281 *\nResiduals   33 1977.5   59.92                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(caffeine.old)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = score ~ amount, data = caffeine)\n\n$amount\n                   diff       lwr       upr     p adj\nModerate-High -4.750000 -12.50468  3.004679 0.3025693\nNone-High     -8.916667 -16.67135 -1.161987 0.0213422\nNone-Moderate -4.166667 -11.92135  3.588013 0.3952176\n\n\nEnd of digression.\nNow we’ll do it using lm, with contrasts later, and see how things change:\n\ncaffeine.2 &lt;- lm(score ~ amount, data = caffeine)\nsummary(caffeine.2)\n\n\nCall:\nlm(formula = score ~ amount, data = caffeine)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.833  -6.958  -2.458   6.354  15.167 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      76.833      2.235  34.383  &lt; 2e-16 ***\namountModerate   -4.750      3.160  -1.503  0.14234    \namountNone       -8.917      3.160  -2.821  0.00803 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.741 on 33 degrees of freedom\nMultiple R-squared:  0.1946,    Adjusted R-squared:  0.1458 \nF-statistic: 3.986 on 2 and 33 DF,  p-value: 0.02815\n\n\nLook at the slopes. They are amount followed by one of the amounts of caffeine. R is using “high” as a baseline (that’s the first level alphabetically), so the amountModerate line is testing high vs. moderate: high is not significantly higher, in terms of test scores, than moderate. That’s one of the things I wanted to test. What about the coefficient for amountNone? That’s none vs. high, since high was the baseline. This is, as we saw from Tukey, significant. But it is not what I said we wanted to test.\nExtra: in case you’re curious, you can also get the regular analysis of variance table as below. anova is multi-talented:\n\nanova(caffeine.2)\n\n\n\n  \n\n\n\nThe problem is that you can’t naturally do Tukey this way, which is often what you want to do next. That’s why we used aov before.\nSince we have a regression model (albeit a peculiar one), we can test whether we should remove amount (that is, whether it has any impact on test scores) this way too:\n\ndrop1(caffeine.2, test = \"F\")\n\n\n\n  \n\n\n\nSame conclusion: there is some effect of caffeine level on test score.\n\\(\\blacksquare\\)\n\nObtain the different values of amount, in the order that R has them.\n\nSolution\nCount them, or find the distinct ones:\n\ncaffeine %&gt;% group_by(amount) %&gt;% summarize(count = n())\n\n\n\n  \n\n\n\nor there is this shortcut to the above:\n\ncaffeine %&gt;% count(amount)\n\n\n\n  \n\n\n\nOr\n\ncaffeine %&gt;% distinct(amount)\n\n\n\n  \n\n\n\nsince we didn’t really need to know how many of each there were.\nThese would all have worked if amount had been text rather a factor. If you have a genuine factor, you can also ask for its levels:\n\nwith(caffeine, levels(amount))\n\n[1] \"High\"     \"Moderate\" \"None\"    \n\n\nor summary will count them up and list them:\n\ncaffeine %&gt;% select(amount) %&gt;% summary()\n\n      amount  \n High    :12  \n Moderate:12  \n None    :12  \n\n\nThis last won’t work if you have a categorical-variable-as-text. It has to be a genuine factor for it to work.\nThe categories are High, Moderate and None in that order. For working with contrasts, we need to have the thing we’re making contrasts for (see below) as a factor, otherwise it won’t work.\n\\(\\blacksquare\\)\n\nCreate a contrast that compares High with Moderate, ignoring None. That is, create a vector whose length is the same as the number of levels of amount, and which has a 1 to represent High and a \\(-1\\) to represent Moderate.\n\nSolution\nPut a 0 in for None:\n\nc.hm &lt;- c(1, -1, 0)\n\nHaving the 1 and the \\(-1\\) the other way around would also have been fine.\n\\(\\blacksquare\\)\n\nCreate a contrast that compares “any caffeine” against “none” by comparing None against the average of Moderate and High.\n\nSolution\n\nc.any &lt;- c(-0.5, -0.5, 1)\n\nNote that both our contrasts have coefficients that add up to zero, as they must:\n\nsum(c.hm)\n\n[1] 0\n\nsum(c.any)\n\n[1] 0\n\n\n\\(\\blacksquare\\)\n\nVerify that your two contrasts are orthogonal.\n\nSolution\nMultiply them together and check that what you get adds up to zero:\n\nsum(c.hm * c.any)\n\n[1] 0\n\n\nZero, so orthogonal. You can check that writing c.any as c(-1,-1,2) would also work (and still be orthogonal with c.hm), and so would writing it as c(1,1,-2).\n\\(\\blacksquare\\)\n\nArrange your contrasts as columns of a matrix (using cbind), and say that you want to use these as contrasts for amount (in data frame caffeine or whatever you called it).\n\nSolution\n\nm &lt;- cbind(c.hm, c.any)\ncontrasts(caffeine$amount) &lt;- m\n\n\\(\\blacksquare\\)\n\nFit the ANOVA as an lm, and look at the summary. What do you conclude about your contrasts?\n\nSolution\n\ncaff.3 &lt;- lm(score ~ amount, data = caffeine)\nsummary(caff.3)\n\n\nCall:\nlm(formula = score ~ amount, data = caffeine)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.833  -6.958  -2.458   6.354  15.167 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   72.278      1.290  56.022   &lt;2e-16 ***\namountc.hm     2.375      1.580   1.503   0.1423    \namountc.any   -4.361      1.825  -2.390   0.0227 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.741 on 33 degrees of freedom\nMultiple R-squared:  0.1946,    Adjusted R-squared:  0.1458 \nF-statistic: 3.986 on 2 and 33 DF,  p-value: 0.02815\n\n\nc.hm was the contrast between high and moderate caffeine. This is not significant (P-value 0.142), which is the same conclusion as Tukey, but the P-value here is quite a bit lower (and thus closer to being significant). There’s a reason for that: here we are focusing in on the two contrasts that we really wanted to test, and ignoring the \\(F\\)-test and the Tukey that tell us stuff that we don’t care about. By focusing our comparison, we get a better (smaller) P-value.\nc.any was none vs. average of any caffeine. This one is significant, with a P-value of 0.023. So this contrast tells us that having any caffeine is better than having none.\n\\(\\blacksquare\\)\n\nWhat happens if you try to use high caffeine vs. moderate caffeine and moderate vs. none as your two contrasts?\n\nSolution\n\nc.hm &lt;- c(1, -1, 0)\nc.mn &lt;- c(0, 1, -1)\n\nSo far so good: the coefficients add up to zero and they reflect the right comparisons. But now:\n\nsum(c.hm * c.mn)\n\n[1] -1\n\n\nThis does not add up to zero, so these two contrasts are not orthogonal, and we can’t do what we just did. R will give us an answer if we try it, but it’ll be the wrong answer.3\nThe best description I have seen of what to do here is by David Howell,4 at link (at the bottom). Let me try to follow his method.\nFirst we need a vector that is all 1’s, which I have called c0 below. Since each of our contrasts c.hm and c.mn have 3 things in them (3 groups), we need to add a “dummy” 3rd contrast to give us a \\(3\\times 3\\) array of numbers:5\n\nc0 &lt;- rep(1, 3)\nm &lt;- cbind(c0, c.hm, c.mn)\nm\n\n     c0 c.hm c.mn\n[1,]  1    1    0\n[2,]  1   -1    1\n[3,]  1    0   -1\n\n\nThis is what Howell calls an “augmented” matrix of contrasts, since it has our two contrasts as the second and third columns, plus the extra dummy one. Next we invert this matrix of contrasts, which we can do because it’s square. t(m) means “take the matrix transpose of m”, if you’re trying to keep up at the back, and solve finds a matrix inverse:\n\nminv &lt;- solve(t(m))\n\nand then we remove the first column, which represents the contrast that we didn’t want anyway (what Howell calls “deaugmenting”):6\n\nm.contr &lt;- minv[, -1]\nm.contr\n\n           c.hm       c.mn\n[1,]  0.6666667  0.3333333\n[2,] -0.3333333  0.3333333\n[3,] -0.3333333 -0.6666667\n\ncontrasts(caffeine$amount) &lt;- m.contr\n\nThe columns of m.contr are our new contrasts. Note that they appear to be something else: high vs. the average of moderate and none, and none vs. the average of moderate and high. They are actually not orthogonal, but if Howell is to be trusted,7 they can be used to test what we want. Now fit the model again:\n\ncaff.4 &lt;- lm(score ~ amount, data = caffeine)\nsummary(caff.4)\n\n\nCall:\nlm(formula = score ~ amount, data = caffeine)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.833  -6.958  -2.458   6.354  15.167 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   72.278      1.290  56.022   &lt;2e-16 ***\namountc.hm     4.750      3.160   1.503    0.142    \namountc.mn     4.167      3.160   1.318    0.196    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.741 on 33 degrees of freedom\nMultiple R-squared:  0.1946,    Adjusted R-squared:  0.1458 \nF-statistic: 3.986 on 2 and 33 DF,  p-value: 0.02815\n\n\nThe rows amountc.hm and amountc.mn are the proper tests for our contrasts c.hm and c.mn. c.hm is not significant (P-value 0.14) and c.mn is not significant either (P-value 0.20). This is the same significance as from Tukey, but note that the P-values for the non-significant tests are much lower than the corresponding ones from Tukey, once again because we have focused on just these comparisons, and not on any others. We decided ahead of time to test just these, and gave ourselves the best chance of finding significance that we could.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "anova-revisited.html#who-studies-the-most-outside-class-1",
    "href": "anova-revisited.html#who-studies-the-most-outside-class-1",
    "title": "28  Analysis of variance revisited",
    "section": "28.10 Who studies the most outside class?",
    "text": "28.10 Who studies the most outside class?\nA social scientist wanted to see how many hours students studied outside of class. She took a random sample of 75 students from three different majors: math, social science and English, and recorded the number of weekly outside-class study hours for each student. The data can be found at link. The social scientist had two research questions: whether math students study more outside of class than the other students, and whether English and social science students study different amounts of time outside class.\n\nExplain briefly why contrasts would be a better idea here than ANOVA with Tukey.\n\nSolution\nThe researcher is not interested in all the comparisons between the groups, but rather in two specific ones (as detailed in the question). So we should focus our attention on those comparisons. This is what contrasts are for.\n\\(\\blacksquare\\)\n\nRead in the data and make side-by-side boxplots of study hours by major. What do the boxplots suggest about the comparisons that the researcher wants to make?\n\nSolution\nSeparated by one space. There appear to be some strange quotes in there, but we’ll ignore those and see whether they cause any trouble:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/studyhours.txt\"\nstudyhours &lt;- read_delim(my_url, \" \")\n\nRows: 75 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): major\ndbl (2): id, hours\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstudyhours\n\n\n\n  \n\n\n\nSo far so good. 75 students, in tidy format.\n\nggplot(studyhours, aes(x = major, y = hours)) + geom_boxplot()\n\n\n\n\nThis suggests that the math students study more than the others, but the English and social science students study about the same amount.\nI see some rather consistent right-skewness here (but no outliers). We have 25 observations in each group, a decent amount of data, so I’m not too worried, but a transformation could be a good idea, something like square root, perhaps. The spreads are not that unequal, but it is true that the Math students have both the largest median and the largest spread, and the social science students are the lowest on both.\nExtra: Box-Cox also works on ANOVA-type data, since that is an lm kind of model:\n\nboxcox(hours ~ major, data = studyhours)\n\n\n\n\nThis suggests that some kind of transformation would be a good idea (no transformation, power 1, is not supported by the data). My guess of 0.5 is not bad; that, or something a bit less like 0.25 (fourth root) would be good. Even log is supportable. But we’ll proceed without transformation in this question to give you some practice with contrasts. Despite appearances, I do (sometimes) like to make things not so complicated!\n\\(\\blacksquare\\)\n\nSet up contrasts for each of the researcher’s research questions, bearing in mind the levels of major and the order in which they come. (For the Math contrast, you want Math against the average of the other two.)\n\nSolution\nWe need to know what the categories of major are, so that we can set up the contrasts correctly, so let’s start with that:\n\nstudyhours %&gt;% select(major) %&gt;% summary()\n\n    major          \n Length:75         \n Class :character  \n Mode  :character  \n\n\nOh. Yeah. We never turned major into a factor. We’d better do that, and then try again:\n\nstudyhours &lt;- studyhours %&gt;% mutate(major = factor(major))\nstudyhours %&gt;% select(major) %&gt;% summary()\n\n     major   \n english:25  \n math   :25  \n socsci :25  \n\n\nIn alphabetical order.\nAll right, contrasts. The English vs. social science one is easier, so we’ll do that first. Just set it up with a 1 for one of them and a \\(-1\\) for the other and 0 for math, in the right order according to what the levels are, viz:\n\nc.eng.socsci &lt;- c(1, 0, -1)\n\nOr c(-1,0,1) would be just as good.\nThe other one is a bit trickier, because you want math against the average of the others, like \\[\\mbox{math}-(\\mbox{English}+\\mbox{socsci})/2.\\] This translates into contrast-ese like this, making sure to get Math in the middle where it belongs:8\n\nc.math.others &lt;- c(-0.5, 1, -0.5)\n\nThe coefficients have to add up to zero, so I have to have two halves to balance my one 1. Or I can use two \\(-1\\)s and a 2.\n\\(\\blacksquare\\)\n\nVerify that your two contrasts are orthogonal.\n\nSolution\nMultiply them together (elementwise, which is how R does it) and show that what you get adds up to zero:\n\nsum(c.eng.socsci * c.math.others)\n\n[1] 0\n\n\nZero. Orthogonal.\nSo we are safely in “familiar” territory, not in the here-be-dragons9 land of non-orthogonal contrasts.\n\\(\\blacksquare\\)\n\nCreate a matrix of contrasts (using cbind) and let lm know that these are contrasts for major, in data frame studyhours (or whatever you called it).\n\nSolution\nSo, like the example in lecture (where the contrasts(model) was admittedly rather confusing):\n\nm &lt;- cbind(c.math.others, c.eng.socsci)\nm\n\n     c.math.others c.eng.socsci\n[1,]          -0.5            1\n[2,]           1.0            0\n[3,]          -0.5           -1\n\ncontrasts(studyhours$major) &lt;- m\n\n\\(\\blacksquare\\)\n\nFit the model using lm. What do you conclude about your contrasts?\n\nSolution\nPretend the ANOVA is a regression (though you can fit ANOVAs either way: we just used aov before because we were going to follow up with Tukey):\n\nstudyhours.1 &lt;- lm(hours ~ major, data = studyhours)\nsummary(studyhours.1)\n\n\nCall:\nlm(formula = hours ~ major, data = studyhours)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.44  -2.48  -0.48   2.52  10.56 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.3200     0.3980  18.392  &lt; 2e-16 ***\nmajorc.math.others   2.1200     0.5628   3.767 0.000335 ***\nmajorc.eng.socsci    0.7800     0.4874   1.600 0.113936    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.447 on 72 degrees of freedom\nMultiple R-squared:  0.1887,    Adjusted R-squared:  0.1662 \nF-statistic: 8.374 on 2 and 72 DF,  p-value: 0.0005375\n\n\nThere is a seriously significant difference between the math students and the others. There is not, however, any kind of difference between the mean study hours of the English and social science students.\n\\(\\blacksquare\\)\n\nAre you surprised by the significance or non-significance you found in the previous part? Explain briefly.\n\nSolution\nGo back and compare with the boxplots. The math students studied noticeably more than the others, so it is not surprising that this difference came out significant. The median study times for the English and social science students were similar, so it is not at all surprising that this difference failed to be significant.\nWhen you set up contrasts, these are the tests you are interested in, so there is no reason for following up with Tukey or anything else. But you have to be able to say ahead of time which contrasts you want to test. This is in contrast10 to Tukey, where you don’t have to decide which comparisons interest you until right at the end.\nAnother question you might have had is “how many contrasts can I do”? To answer this, go back and look at the ANOVA table. If you run an ANOVA using lm, you get the ANOVA table by passing the fitted model object into anova rather than summary:11\n\nanova(studyhours.1)\n\n\n\n  \n\n\n\nTwo degrees of freedom for major, so we can do 2 contrasts. (The example in class had four models of chainsaw, so 3 df for model, so 3 contrasts possible.) What if we don’t use all those contrasts up? Try it and see:\n\nm &lt;- cbind(c.math.others)\nm\n\n     c.math.others\n[1,]          -0.5\n[2,]           1.0\n[3,]          -0.5\n\ncontrasts(studyhours$major) &lt;- m\nstudyhours.2 &lt;- lm(hours ~ major, data = studyhours)\nsummary(studyhours.2)\n\n\nCall:\nlm(formula = hours ~ major, data = studyhours)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.44  -2.48  -0.48   2.52  10.56 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.3200     0.3980  18.392  &lt; 2e-16 ***\nmajorc.math.others   2.1200     0.5628   3.767 0.000335 ***\nmajor               -1.1031     0.6893  -1.600 0.113936    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.447 on 72 degrees of freedom\nMultiple R-squared:  0.1887,    Adjusted R-squared:  0.1662 \nF-statistic: 8.374 on 2 and 72 DF,  p-value: 0.0005375\n\n\nThe two P-values are identical, but the second one is not now identified with any contrast (so we just ignore it). If we were only interested in how math students compared with the rest, we could test it this way.\nOr, we could relabel the students as “math” and “other” and just do a two-sample \\(t\\)-test. To do this, we need to delve into forcats (which is a part of the tidyverse). It deals with categorical variables which are genuine factors, which we haven’t had much cause to do so far. But now we do. What we want to do is to recode the levels of major, which is done by fct_recode:\n\nstudyhoursx &lt;- studyhours %&gt;%\n  mutate(mathrest = fct_recode(major,\n    \"rest\" = \"english\",\n    \"rest\" = \"socsci\"\n  ))\nstudyhoursx %&gt;% count(mathrest)\n\n\n\n  \n\n\n\nThe way fct_recode works is that you first give a factor, here major, then you define a list of new levels in terms of the old ones. We want a student whose major is either English or Social Science to end up in the new not-math group that we’ll call rest, so we need two lines inside fct_recode, one that says that the new rest group includes the old english majors, and another that says that rest also includes the old socsci majors. Any levels not mentioned (here math) are left unchanged.\nI did this wrong the first time: I got the things before and after the = in fct_recode the wrong way around.\nThe last line is to check that the result makes sense. I previously had 25 students in each major. Now I have 25 students for whom mathrest is math (the math majors) and \\(50=25+25\\) students for whom mathrest is rest (everyone else).\nNow, the \\(t\\)-test.\n\nt.test(hours ~ mathrest, data = studyhoursx, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  hours by mathrest\nt = -3.7269, df = 73, p-value = 0.0003796\nalternative hypothesis: true difference in means between group rest and group math is not equal to 0\n95 percent confidence interval:\n -4.880528 -1.479472\nsample estimates:\nmean in group rest mean in group math \n              6.26               9.44 \n\n\nThis is very close to, but not the same as, the test with the contrasts, because we are not quite testing the same thing. Here, we are comparing the mean of the math students to the mean of everyone else, thrown together into one big bag, but with the contrasts, we are comparing the mean of the math students with the average of the means of the other two groups, treating them as different. (Not clear? Well, it’s not that clear to me either.)\nThat business with the var.equal=T? This is doing a pooled \\(t\\)-test. I did that here because the ANOVA is assuming equal spreads within the groups (which is what a pooled \\(t\\)-test does) and I thought I should do my best to be consistent. A pooled \\(t\\)-test is really a two-group one-way ANOVA, or at least it is if it is done two-sided.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "anova-revisited.html#mental-context-1",
    "href": "anova-revisited.html#mental-context-1",
    "title": "28  Analysis of variance revisited",
    "section": "28.11 Mental context",
    "text": "28.11 Mental context\nIt is believed that being in the same mental context for learning and for testing leads to better test scores. An experiment was carried out to test this. During the learning phase, subjects learned a list of 80 words in a room painted orange and decorated with posters, paintings and other paraphernalia.12 A memory test was given to all subjects immediately after they had learned the words, to give the impression that the experiment was over. (The results of this test were discarded.) One day later, subjects were unexpectedly re-tested under different experimental conditions and asked to write down all the words of the original list that they could remember. The re-test took place in five different conditions, which subjects were randomly allocated to one of:\n\nSame context: in the original orange-painted room.\nDifferent context: in a very different room, painted grey and located in a different part of campus.\nImaginary context: in the grey room, but subjects are asked to remember the orange room in which they took the original test. To help them with this, the experimenter begins by asking them several questions about the orange room and the objects in it.\nPhotographed context: in the grey room, but subjects are shown photographs of the orange room.\nPlacebo context: in the grey room, with subjects first being asked to recall their living room.\n\nIn each case, the response variable was the number of words on the original list successfully recalled by each subject.\n50 subjects in total completed the experiment, 10 under each of the 5 conditions.\nThe researchers had four research hypotheses to test with the data:\n\nGroups where the context matches (is the same, or is simulated by imagining or photograph) will perform better than groups with different or placebo contexts.\nThe group with the same context will differ from the group with imaginary or photographed contexts.\nThe imaginary-context group will differ from the photographed-context group.\nThe different-context group will differ from the placebo-context group.\n\nThe research hypotheses are (as is usual) written as alternative hypotheses. We can rewrite them as null hypotheses, with some extra wording to facilitate converting them into contrasts, like this:\n\nThe mean of the same, imaginary and photograph groups (group means) is equal to the mean of the different and placebo groups (group means).\nThe mean of the imaginary and photograph groups is equal to the (mean of the) same group.\nThe imaginary and the photograph groups will have the same mean.\nThe different and the placebo groups will have the same mean.\n\nThe data are in link (the original researcher’s name was Smith).\n\nRead in the data and verify that you have a column called context that is text and a column called words that is a (whole) number.\n\nSolution\nThe usual thing — read in the data appropriately and look at the data frame you got:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/smith.txt\"\nsmith &lt;- read_delim(my_url, \" \")\n\nRows: 50 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): context\ndbl (2): id, words\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsmith\n\n\n\n  \n\n\n\ndbl actually means “decimal number”, but these look like whole numbers, so I think we are good.\n\\(\\blacksquare\\)\n\nTurn context into a factor, within the data frame. (We are going to be doing contrasts). Display how the data frame now looks.\n\nSolution\nUse mutate and assign everything back to what it was before:\n\nsmith &lt;- smith %&gt;% mutate(context = factor(context))\nsmith\n\n\n\n  \n\n\n\nNote that context is now labelled as the factor that we made.\n\\(\\blacksquare\\)\n\nWhat are the names of the five contexts in the data set (just display them), and what order are they in?\n\nSolution\nYou have lots of choices now, since context is a factor (and can also be treated as text). The most obvious factor-based way is this:\n\nlevels(smith$context)\n\n[1] \"different\"  \"imaginary\"  \"photograph\" \"placebo\"    \"same\"      \n\n\nor use summary:\n\nsmith %&gt;% select(context) %&gt;% summary()\n\n       context  \n different :10  \n imaginary :10  \n photograph:10  \n placebo   :10  \n same      :10  \n\n\nOr treat context as text, such as this way:\n\nsmith %&gt;% count(context)\n\n\n\n  \n\n\n\nThese all display the contexts in alphabetical order.\nOr display the distinct values:\n\nsmith %&gt;% distinct(context)\n\n\n\n  \n\n\n\nor, without using the tidyverse,\n\nunique(smith$context)\n\n[1] same       different  imaginary  photograph placebo   \nLevels: different imaginary photograph placebo same\n\n\nThese two don’t display things in alphabetical order, because they display things as they appear in the file. This is not the order we want, though the second way does display the Levels in the right order.\n\\(\\blacksquare\\)\n\nWrite each of the four research hypotheses (in the null-hypothesis versions) as R vectors that can be used to make contrasts. (This will mean getting the contexts in the right order. If you get stuck, do the last two first, since they’re easiest. The first one is actually the hardest.)\n\nSolution\nAll right, let’s go from the bottom:\n\nDifferent and placebo have same means. These are 1st and 4th.\n\n\nc4 &lt;- c(1, 0, 0, -1, 0)\n\n\nImaginary and photograph have same means. 2nd and 3rd:\n\n\nc3 &lt;- c(0, 1, -1, 0, 0)\n\n\nImaginary and photograph together same as same. This time we have two means being compared with one, so we need to give the two means half weight. 2nd and 3rd against 5th:\n\n\nc2 &lt;- c(0, 1 / 2, 1 / 2, 0, -1)\n\n\n(Same and imaginary and photograph) vs. (different or placebo): 2nd, 3rd and 5th with weight \\(1/3\\) (three of them) against 1st and 4th with weight \\(1/2\\) (2 of them):\n\n\nc1 &lt;- c(-1 / 2, 1 / 3, 1 / 3, -1 / 2, 1 / 3)\n\nVariations: you can switch the sign on everything within a contrast (since it doesn’t matter which is plus and which is minus). You can also multiply through by anything to get rid of the fractions, for example these:\n\nc2 &lt;- c(0, 1, 1, 0, -2)\nc1 &lt;- c(-3, 2, 2, -3, 2)\n\nAll that matters is that the coefficients add up to zero, and that they are the right size and sign relative to each other.\n\\(\\blacksquare\\)\n\nPick two of your contrasts (doesn’t matter which two) and demonstrate that they are orthogonal.\n\nSolution\nMultiply your chosen contrasts together elementwise, and show that the results add to zero, eg. by showing the whole thing:\n\nc1 * c2\n\n[1]  0.0000000  0.1666667  0.1666667  0.0000000 -0.3333333\n\n\nwhich adds to zero because it is \\(2/6\\) minus \\(1/3\\), or by explicitly summing the elementwise product:\n\nsum(c1 * c3)\n\n[1] 0\n\n\nAny pair is good.\n\\(\\blacksquare\\)\n\nCollect your contrasts together into a matrix, and tell lm that these are the contrasts for context.\n\nSolution\nThis:\n\nm &lt;- cbind(c1, c2, c3, c4)\ncontrasts(smith$context) &lt;- m\n\nSlipping off into another aside, you might have been wondering whether there was a way to test all the contrasts for orthogonality at once. There is, and it depends on the rules for matrix multiplication. We want to test all the columns of m for orthogonality, but matrix multiplication works by combining a row with a column. No matter, transposing a matrix interchanges rows and columns, so that in math, we want to look at the matrix \\(M^T M\\). In R, %*% means “matrix multiply”.13 Thus,\n\nt(m) %*% m\n\n          c1  c2 c3 c4\nc1 0.8333333 0.0  0  0\nc2 0.0000000 1.5  0  0\nc3 0.0000000 0.0  2  0\nc4 0.0000000 0.0  0  2\n\n\nThis matrix-multiplies the transpose of m by m itself. There are numbers down the top-left to bottom-right diagonal, but don’t worry about these, since a contrast doesn’t have to be orthogonal with itself. The thing to note is that all the other elements of this matrix are zero: that means that each of the four contrasts is orthogonal to each of the other three.\n\\(\\blacksquare\\)\n\nFit a model with lm, and display the results.\n\nSolution\nWe’re past the hard part:\n\nsmith.1 &lt;- lm(words ~ context, data = smith)\nsummary(smith.1)\n\n\nCall:\nlm(formula = words ~ context, data = smith)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -9.0   -4.0   -1.5    4.6   11.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  14.8000     0.8129  18.207  &lt; 2e-16 ***\ncontextc1     8.6000     1.9912   4.319 8.52e-05 ***\ncontextc2    -0.3333     1.4841  -0.225    0.823    \ncontextc3    -0.1000     1.2853  -0.078    0.938    \ncontextc4     0.5000     1.2853   0.389    0.699    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.748 on 45 degrees of freedom\nMultiple R-squared:  0.2954,    Adjusted R-squared:  0.2327 \nF-statistic: 4.715 on 4 and 45 DF,  p-value: 0.002902\n\n\n\\(\\blacksquare\\)\n\nFor each of the original research hypotheses, what do you infer about them?\n\nSolution\nEven though I generated the contrasts backwards, I maintained the numbering so that they refer to the original numbered hypotheses. You might have named them something more mnemonic; that works too, and is quite possibly better. Anyway:\n\nMatching context better than non-matching context: strongly significant (P-value 0.000085). (Strictly, this is a two-sided test, but the way I constructed the contrast, this should be significantly positive if the research hypothesis is true, and it is.)\nSame context not different from imaginary/photographed context (P-value 0.823)\nImaginary context not different from photographed context (P-value 0.938).\nDifferent context not different from placebo context (P-value 0.699).\n\nI wanted you to match up the research hypotheses with the P-values, as above, and state a conclusion about each one. If you do that, I am happy.\nTaking this back to the original research, I think the first hypothesis was the most immediately important of the four: we were able to show that having (or faking up) the original room helped with recall. After that, it didn’t matter how it was done: being in the original room was not different from imagining the original room (by thinking about it or seeing a photo of it). Failing to recall the original room was equally bad, whether the subjects were in a different room and not asked to think about the original orange room, or whether they were literally asked to think about another room entirely.\nContrasts enabled us to tackle this problem and gain a very clear conclusion: recalling the original orange room is helpful, but it doesn’t matter how you do it.\nHow big of a difference does thinking about the orange room make? You can use the 2 SE thing to get a 95% confidence interval: the difference in the (mean of the) recall-orange-room means and the (mean of the) don’t-recall means is between about 4.6 and 12.6 words (out of 80). I leave it for you to decide whether that is practically important as well as being statistically significant.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "anova-revisited.html#trying-on-shirts-1",
    "href": "anova-revisited.html#trying-on-shirts-1",
    "title": "28  Analysis of variance revisited",
    "section": "28.12 Trying on shirts",
    "text": "28.12 Trying on shirts\nDoes it make a difference if you see somebody else trying on a shirt before you do, and if so, does it matter what kind of person it is?\nAn experiment was carried out in a university bookstore, with students who wanted to try on a shirt serving as (unsuspecting) experimental subjects. When a student wanted to try on a shirt, the sales associate told them that there was only one shirt left, and it was being tried on by an “other customer”. This “other customer” was actually a “confederate” of the experimenter (that means, they were pretending to be a real customer but were actually part of the experiment). The “other customer” was always female: either an attractive well-dressed model, or an average-looking student wearing jeans. The “other customer” would come out of the dressing room and hand the shirt to the sales associate, who would give it to the student who wanted to try it on. When the student had tried on the shirt, they were asked to rate it on a 7-point scale (on five different dimensions, and those five scores were then averaged). Does a student’s evaluation depend on whether the student was male or female, and whether the “other customer” was a model or a student? There was also a control group, where the student was handed the shirt directly by the sales associate, without the “other customer” being involved at all.\nThere were thus five treatments: male students who saw a model, male students who saw a student, female students who saw a model, female students who saw a student, and the control group. There were 25 students in each treatment group.\nThe data from this experiment can be found at link.\n\nRead in and display the data. How many observations do you have, and is that what you should have?\n\nSolution\nread_csv:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/shirts.csv\"\nshirts &lt;- read_csv(my_url)\n\nRows: 125 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): treatment\ndbl (1): score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nshirts\n\n\n\n  \n\n\n\nThere were 5 treatment groups with 25 students in each, so there should be, and are, \\(5 \\times 25=125\\) observations.\n\\(\\blacksquare\\)\n\nTurn treatment into a factor in your data frame. (You can use the same name as the text treatment that you read in from the file.)\n\nSolution\nThus:\n\nshirts &lt;- shirts %&gt;% mutate(treatment = factor(treatment))\n\nand for checking:\n\nshirts\n\n\n\n  \n\n\n\nsee that treatment is now definitely a factor rather than a categorical variable represented as a piece of text.\n\\(\\blacksquare\\)\n\nList the treatments in the order that they are in within your factor. (There are lots of ways to do this; any one of them except for distinct is good.)\n\nSolution\nThis is the most direct:\n\nlevels(shirts$treatment)\n\n[1] \"control\"               \"female_seeing_model\"   \"female_seeing_student\"\n[4] \"male_seeing_model\"     \"male_seeing_student\"  \n\n\nor you can do one of the tidyverse-style ways, such as\n\nshirts %&gt;% count(treatment)\n\n\n\n  \n\n\n\nor\n\nshirts %&gt;% distinct(treatment)\n\n\n\n  \n\n\n\nexcept that distinct shows you the values in the order that they appear in the data file, which is not the order that they appear in the factor. So that’s why I asked you not to do it that way.\nAlphabetical order, is the moral of the story.\nExtra: you can make them appear in the order they appear in the data, if you want to (and if you know what you are doing):\n\nshirts %&gt;% mutate(trt2 = fct_inorder(treatment)) -&gt; d\nlevels(d$trt2)\n\n[1] \"male_seeing_model\"     \"male_seeing_student\"   \"female_seeing_model\"  \n[4] \"female_seeing_student\" \"control\"              \n\n\nfct_inorder creates a factor whose levels are in the order that they appeared in the data. If this is what you want, you can go ahead and do this, and use this order when you create your contrasts. It will work, and it will be good even though your contrasts will look different from mine.\n\\(\\blacksquare\\)\n\n* Obtain a table of mean evaluation scores for each treatment group.\n\nSolution\nWe will use this later when assessing the significance of the contrasts. It’s the usual group-by and summarize:\n\nshirts %&gt;%\n  group_by(treatment) %&gt;%\n  summarize(m = mean(score))\n\n\n\n  \n\n\n\nExtra: how does this come out if you have your factor levels in a non-alphabetical order?\n\nd %&gt;% group_by(trt2) %&gt;% summarize(m = mean(score))\n\n\n\n  \n\n\n\nAh, it respects the order of the levels of the factor: that is to say, the treatment groups appear here in the order that they were in the data, because the factor trt2 was created to make that happen.\nforcats has other tools to re-order the levels of a factor, lump factor levels together to make a category “other”, and so on. These also work if you start with a categorical variable as text: it creates a factor with those properties.\n\\(\\blacksquare\\)\n\nThe experimenters wanted to compare four specific things in their analysis:\n\n\nevaluation scores between male students who saw a (female) model and male students who saw a (female) student\nevaluation scores between female students who saw a (female) model and female students who saw a (female) student\nevaluation scores for male and for female students (overall)\nevaluation scores for the (average of the) four genuine treatments and for the control group\n\nCreate contrasts, with suitable names, using vectors with appropriate values.\nSolution\nThese, in that order, are comparisons of treatments 4 and 5:\n\nc_mms &lt;- c(0, 0, 0, 1, -1)\n\ntreatments 2 and 3:\n\nc_fms &lt;- c(0, 1, -1, 0, 0)\n\n(in these two the 1 and \\(-1\\) can also be the other way around)\nthe average of 2 and 3 vs. the average of 4 and 5:\n\nc_mf &lt;- c(0, 0.5, 0.5, -0.5, -0.5)\n\nor you can use 1 and \\(-1\\) instead of the 0.5s, or you can switch the signs around. I like the 0.5 values to remind me it’s an average of two means.\nFinally\n\nc_tc &lt;- c(1, -0.25, -0.25, -0.25, -0.25)\n\nor multiply through by 4 to get rid of the fractions, or have the negative sign on the first one and positive signs on the others.\nI tried to give the contrasts mnemonic but short names, so that I would remember which was which.\n\\(\\blacksquare\\)\n\nPick two of your contrasts (doesn’t matter which two) and demonstrate that they are orthogonal.\n\nSolution\nMultiply them together (elementwise, which is what * does) and add them up, showing that you get zero, for example:\n\nsum(c_mf * c_tc)\n\n[1] 0\n\n\nCheck (mops brow).\n\\(\\blacksquare\\)\n\nCollect all your contrasts together into a matrix and declare that they are contrasts for treatment within your data frame (whatever you called it).\n\nSolution\nI called my data frame shirts, so I need to do this:\n\nm &lt;- cbind(c_mms, c_fms, c_mf, c_tc)\ncontrasts(shirts$treatment) &lt;- m\n\nThere’s no output here; we’ll see in a moment whether it worked.\nExtra: once you’ve created the matrix m, it gives you a second way to test the contrasts for orthogonality, all at once:\n\nt(m) %*% m\n\n      c_mms c_fms c_mf c_tc\nc_mms     2     0    0 0.00\nc_fms     0     2    0 0.00\nc_mf      0     0    1 0.00\nc_tc      0     0    0 1.25\n\n\nThis matrix-multiplies the transpose of m by m, so it’s testing each column of m to see whether it’s orthogonal to each other column. The columns don’t have to be orthogonal to themselves, hence the non-zeros down the diagonal, but all the off-diagonal entries are zero. Hence each contrast is orthogonal to each other contrast.\nOr even:\n\nz &lt;- t(m) %*% m\nall(z[row(z) != col(z)] == 0)\n\n[1] TRUE\n\n\nThat says (a little breathlessly) that it is true that all the elements of \\(M^TM\\) that are off the diagonal are zero.14\n\\(\\blacksquare\\)\n\nPredict evaluation score from treatment as a regression, and display the results.\n\nSolution\nOnce you have everything set up, it’s just a matter of going through the process:\n\nscore.1 &lt;- lm(score ~ treatment, data = shirts)\nsummary(score.1)\n\n\nCall:\nlm(formula = score ~ treatment, data = shirts)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.860 -0.760  0.032  0.840  3.640 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      4.0256     0.1231  32.705  &lt; 2e-16 ***\ntreatmentc_mms   1.0820     0.1946   5.560 1.66e-07 ***\ntreatmentc_fms   0.3540     0.1946   1.819   0.0714 .  \ntreatmentc_mf   -0.6200     0.2752  -2.253   0.0261 *  \ntreatmentc_tc    0.0064     0.2462   0.026   0.9793    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.376 on 120 degrees of freedom\nMultiple R-squared:  0.2467,    Adjusted R-squared:  0.2216 \nF-statistic: 9.823 on 4 and 120 DF,  p-value: 6.576e-07\n\n\nNote how the contrasts have appeared as “slopes” in our regression. (If this didn’t happen for you, go back and check what you did. Probably something gave you an error before, in that case.)\nYour slopes can differ in terms of sign and possibly value from mine, but the \\(t\\)-statistics and P-values should be the same as mine.\n\\(\\blacksquare\\)\n\nFor each of your contrasts, assess whether or not it is significant, and explain briefly what that means in the context of the data. If a contrast is significant, use your answer to part (here) to help in your interpretation.\n\nSolution\nI’ll take my four contrasts as they appear in the output from lm:\n\nComparing the male students who saw the model with those who saw the student: this is strongly significant, and the table of means shows that the males who saw the model rated the shirt significantly higher than those who saw the “average-looking” female student.\nComparing the female students who saw the model with those who saw the student: this difference is not quite significant, so we conclude that it didn’t matter who the female students saw coming out of the dressing room.\nThere is a significant difference between the average evaluation score that males gave to the shirt and that females did. The table of means reveals that the male average was higher, mostly because males really liked the shirt when they thought the model had been wearing it!\nThere is no significant difference overall between treatment and control groups in terms of average score. (This is mostly because the treatment groups differ among themselves, with the scores for students who saw the model being higher and those for students who saw the female student being lower than for the control group.)\n\nIt is enough, on the non-significant ones, to say what it is that the contrast is testing, and then to note that it is not significant. On the significant ones, I want you to say what is higher than what. You can also get these conclusions from the lm output, but it requires a bit more care:\n\nI set up c_mms with the score for the students who had seen the model as plus and those who saw the student as minus. My slope here is positive, so the score for male students who saw the model is higher.\nMy slope for c_fms is also positive, and I set this one up the same way with model as positive and student as negative, so the female students rated the shirt higher (but not significantly higher) when they saw the model.\nMy c_mf slope is significantly negative. I set up this contrast with females as plus and males as minus, so this means males gave a significantly higher score on average (for reasons discussed above).\nThe last slope is very close to 0, consistent with there being no difference (on average) between treatment and control.\n\nPerhaps the story to get from this analysis is that male students are such suckers!\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "anova-revisited.html#footnotes",
    "href": "anova-revisited.html#footnotes",
    "title": "28  Analysis of variance revisited",
    "section": "",
    "text": "This is a fancy word for stuff.↩︎\nIt’s important to be clear about the distinction between a categorical variable, that lives in a data frame column, and its levels, the values that appear in the column. This is especially important if you’re trying to decide whether a data frame is tidy, since typically an untidy data frame will have factor levels as column names rather than the factor itself, and you need to be able to tell the difference.↩︎\nSAS, for example, has a way of making non-orthogonal contrasts orthogonal in a way that the user doesn’t have to worry about, but in R, you are closer to the ground, so to speak, and you have to make it happen yourself.↩︎\nHowell is the author of a famous text on Statistics in Psychology.↩︎\nWhich we are going to invert, as a matrix. But I get ahead of myself.↩︎\nWe are working with R matrices here rather than data frames, so we access elements, rows and columns using the square bracket notation: inside the square brackets, we put first the numbers of the rows we want, then a comma, then the numbers of the columns. There are two special pieces of notation, both of which I use here: leaving the row or column slot blank means all the rows or all the columns, and using a negative row or column number means all the rows or columns except the one(s) named. Thus my notation here is all the rows, and all the columns except for the first one. You can access data frames this way too, but the Tidyverse makes it much easier.↩︎\nI think Howell is famous enough to be trusted.↩︎\nAs I failed to do the first time.↩︎\nOn ancient maps, people didn’t know what was in certain parts of the world, because no-one had ever explored them, so they wrote on the map “here be dragons”.↩︎\nIn contrast. Get it? No? Well, never mind then.↩︎\nanova is one of R’s multi-purpose tools; what it does depends on what you feed it.↩︎\nThis is a fancy word for stuff.↩︎\nIn R, percents around something mean that it is a special version of that something. Hence the notation for matrix-multiply and the pipe symbol. A regular * when used for multiplying matrices in R will multiply them element by element.↩︎\nThe thing inside the square brackets says only to look at the elements of \\(M^TM\\) whose row number and whose column number are different; it is perhaps easier to reason that elements of a matrix whose row number and column number are the same are on the diagonal, for example the element in row 2, column 2.↩︎"
  },
  {
    "objectID": "ancova.html#productivity-and-research-and-development",
    "href": "ancova.html#productivity-and-research-and-development",
    "title": "29  Analysis of covariance",
    "section": "29.1 Productivity and research-and-development",
    "text": "29.1 Productivity and research-and-development\nAn economist compiled data on productivity improvements for a sample of companies that produce electronic equipment. The companies were classified according to their expenditures on research and development in the last three years (classified as low, moderate, high). Productivity improvement is measured on a 0–100 scale, with most values being between 0 and 10. The economist also thought that last year’s productivity improvement would be relevant, and so recorded this as well.\nThe data set can be found at link.\nFeel free to use “R&D” as an abbreviation for “research and development” in your answers below.\n\nRead in and display at least some of the data.\nPlot productivity improvement for this year against last year, identifying the points by the level of research-and-development expenditure. On your plot, add regression lines for each expenditure group.\n* Look at your graph of the previous part. Do you think that (i) knowing last year’s productivity improvement tells you something about this year’s, (ii) the level of expenditure on research-and-development has an impact on this year’s productivity improvement, (iii) there is an interaction effect between R&D expenditure and last year’s productivity increase on this year’s productivity increase? In each case, explain briefly.\nFit an analysis of covariance model without interaction. Run the results through drop1 with test=\"F\" (the F must be in quotes). What do you conclude?\nNow look at the summary of your analysis of covariance model. Explain briefly how each of the last three numbers in the Estimate column are consistent with your graph.\nAdd an interaction between last and expenditure to your analysis of covariance model. Test it for significance using drop1. In the light of what you have seen, does this surprise you? Explain briefly why you are or are not surprised."
  },
  {
    "objectID": "ancova.html#treating-leprosy",
    "href": "ancova.html#treating-leprosy",
    "title": "29  Analysis of covariance",
    "section": "29.2 Treating leprosy",
    "text": "29.2 Treating leprosy\nTwo drugs are being tested in the treatment of leprosy. These are labelled A and D. There is also a control drug, labelled F. The response variable is a post-treatment score of leprosy bacilli (measured at six different sites on each patient). A lower score is better.\nThus far, we have a standard one-way analysis of variance. But the researchers wanted greater precision in assessing the effects (if any) of the drugs, so they also measured a pre-treatment score of leprosy bacilli. The data are in the file link. The pre-treatment and post-treatment scores are labelled pre and post respectively.\n\nRead in the data and check that you have apparently the right thing.\n* Make a scatterplot of post-treatment score against pre-treatment score, with the points for each drug drawn in a different colour.\nDoes it appear that including the pre-treatment score was a good idea? Explain briefly.\nWhat about this dataset suggests that analysis of covariance is a method worth trying?\nFit an analysis of covariance model to predict post-treatment score. Include an interaction between your explanatory variables. (You don’t need to look at the output from the model.)\nPass your fitted model of the last part into drop1. Is the interaction term significant?\nFit a model without the interaction. Is this a sensible thing to do (in addition, that is, to the fact that I just asked you to do it)?\nTake a look at the summary of your preferred model. Is there a significant effect of pre-treatment score? Describe the effects of the different drugs on the post-treatment score. (Don’t do any tests for drug.) Does your comparison of drugs make sense?\nObtain predicted values for post for each of the three drugs at pre scores 5, 12 and 20. To do this, obtain a new data frame that has all 9 combinations of drugs and pre scores, and then feed this into predict using your preferred model.1\nNow, plot the data with the fitted lines on.\nAre the lines on your plot parallel, with the same slopes? Is this what you would expect? Explain briefly.\n\nMy solutions follow:"
  },
  {
    "objectID": "ancova.html#productivity-and-research-and-development-1",
    "href": "ancova.html#productivity-and-research-and-development-1",
    "title": "29  Analysis of covariance",
    "section": "29.3 Productivity and research-and-development",
    "text": "29.3 Productivity and research-and-development\nAn economist compiled data on productivity improvements for a sample of companies that produce electronic equipment. The companies were classified according to their expenditures on research and development in the last three years (classified as low, moderate, high). Productivity improvement is measured on a 0–100 scale, with most values being between 0 and 10. The economist also thought that last year’s productivity improvement would be relevant, and so recorded this as well.\nThe data set can be found at link.\nFeel free to use “R&D” as an abbreviation for “research and development” in your answers below.\n\nRead in and display at least some of the data.\n\nSolution\nThe data values are separated by one space, so let’s use read_delim and display whatever displays (probably the first ten lines):\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/productivity.txt\"\nproductivity &lt;- read_delim(my_url, \" \")\n\nRows: 27 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): expenditure\ndbl (2): improvement, last\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nproductivity\n\n\n\n  \n\n\n\nThere is a column classifying expenditure along with numerical values for productivity improvement for this year and last year (labelled improvement and last), so it looks as if we are good.\nYou are free to give the data frame a shorter name to make it easier to keep typing it!\nExtra: those expenditure levels (that are currently text) will get sorted into alphabetical order unless we stop them. They are actually in the right order in the data file, so maybe we should turn them into a factor (with the levels in the right order) now:\n\nproductivity %&gt;%\n  mutate(expenditure = fct_inorder(expenditure)) -&gt; productivity\n\nIf you don’t do this, your answers below will be different from mine. Probably not in any kind of consequential way, but different nonetheless.\n\\(\\blacksquare\\)\n\nPlot productivity improvement for this year against last year, identifying the points by the level of research-and-development expenditure. On your plot, add regression lines for each expenditure group.\n\nSolution\nTwo quantitative and one categorical variable, so plot the categorical variable using colour (or shape etc., if you know about that, but colour is the most obvious thing):\n\nggplot(productivity, aes(x = last, y = improvement, colour = expenditure)) +\n  geom_point() + geom_smooth(method = \"lm\", se = F)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe se=F is optional; no problem if you don’t include it. (If you miss it out, you’ll get those CI-of-mean-response grey envelopes around each line.)\n\\(\\blacksquare\\)\n\n* Look at your graph of the previous part. Do you think that (i) knowing last year’s productivity improvement tells you something about this year’s, (ii) the level of expenditure on research-and-development has an impact on this year’s productivity improvement, (iii) there is an interaction effect between R&D expenditure and last year’s productivity increase on this year’s productivity increase? In each case, explain briefly.\n\nSolution\nTaking the three things in turn: (i) knowing about last year’s productivity increase definitely helps, because the three trends definitely go up (extra: and are pretty much linear). (ii) knowing about the level of expenditure helps because the coloured trends are in different places (low on the left and high on the right, whatever colours they are for you). (iii) I am guessing there is no interaction because the three lines look more or less parallel (a judgement call: if you think that there will be an interaction because you think the lines are definitely not “approximately parallel” and therefore there is an interaction, that’s OK too).\n\\(\\blacksquare\\)\n\nFit an analysis of covariance model without interaction. Run the results through drop1 with test=\"F\" (the F must be in quotes). What do you conclude?\n\nSolution\nThis looks exactly like a regression with a categorical variable, and is. Just the two main effects, thus:\n\nimprovement.1 &lt;- lm(improvement ~ last + expenditure, data = productivity)\ndrop1(improvement.1, test = \"F\")\n\n\n\n  \n\n\n\nBoth the P-values are small, so there is strong evidence of an effect on this year’s productivity increase of last year’s productivity increase and the level of R&D expenditure.\n\\(\\blacksquare\\)\n\nNow look at the summary of your analysis of covariance model. Explain briefly how each of the last three numbers in the Estimate column are consistent with your graph.\n\nSolution\nTo begin:\n\nsummary(improvement.1)\n\n\nCall:\nlm(formula = improvement ~ last + expenditure, data = productivity)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52812 -0.16385 -0.00046  0.08379  0.45730 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -1.00804    0.50991  -1.977   0.0602 .  \nlast                 1.11417    0.07116  15.658 9.27e-14 ***\nexpendituremoderate -1.83316    0.22372  -8.194 2.84e-08 ***\nexpenditurehigh     -3.14338    0.37115  -8.469 1.59e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2393 on 23 degrees of freedom\nMultiple R-squared:  0.9629,    Adjusted R-squared:  0.958 \nF-statistic: 198.8 on 3 and 23 DF,  p-value: &lt; 2.2e-16\n\n\nThe last three numbers in the Estimate column are slopes for the quantitative variable last and changes in intercept for the levels of the categorical variable expenditure. Specifically:\n\nthe slope for last is positive, so if last year’s productivity increase is higher, this year’s will be higher as well. This matches the upward trends within each expenditure group on the graph.\nexpenditure is categorical, so everything is measured relative to the baseline. Mine is low, but yours is probably high.\nMy two estimates for expenditure high and moderate are both negative, so when expenditure on R&D is high or moderate, the productivity increase will be lower this year (for the same productivity increase last year). Going back to the graph, if you extend the lines for moderate and high to a middling last value of something like 10, the productivity increase this year will be higher when expenditure is low or moderate then when it is high, which is consistent with those slopes both being positive. (You don’t need to go to this detail, but you should say something convincing from the graph about how this year’s productivity increase is higher if R&D expenditure is low or moderate compared to high.)\n\nExtra: the graph also indicates that the higher the expenditure on R&D is, the more likely last year’s productivity improvement will be higher also. So in practice we are not likely to be comparing actual companies with different expenditure but the same last. Nonetheless, what I said is what those positive coefficients for expenditure actually mean. Be careful not to say that low is lowest because it’s on the left; it’s whether it’s higher or lower than the others at the same value of last that matters.\nExtra extra: I asked you to do the test from drop1 because there are three levels of expenditure (rather than just two), and the summary output only gives you a comparison with the baseline level, rather than comparing all three levels. I prefer to do the right test (using drop1) first, and then use summary to interpret what I have, or at least the parts of it that are significant.\nExtra extra extra: you would (or at least I would) expect a larger productivity increase to go with a larger expenditure on R&D, but that’s not how it worked out. This is one of those cases where all else isn’t really equal.\n\\(\\blacksquare\\)\n\nAdd an interaction between last and expenditure to your analysis of covariance model. Test it for significance using drop1. In the light of what you have seen, does this surprise you? Explain briefly why you are or are not surprised.\n\nSolution\nI like update for this (writing out the whole model is an alternative):\n\nimprovement.2 &lt;- update(improvement.1, . ~ . + last:expenditure)\ndrop1(improvement.2, test = \"F\")\n\n\n\n  \n\n\n\nYes, I too forgot the test=\"F\" the first time.\nThe interaction term is actually significant, at least at \\(\\alpha=0.05\\). This is a surprise to me, because I thought those lines on the graph were pretty close to parallel, so I wasn’t expecting to see a significant interaction. (What I want from you here is to look back at your answer to (iii) in part (here), and to say how this small P-value is consistent or inconsistent with whatever you said there. If you said that the lines weren’t parallel enough, then this significant interaction should not be a surprise to you.)\nThat’s all I’m after. I don’t need you to speculate on why the test came out significant. I will, in a moment, but you don’t need to.\nExtra: I didn’t expect the interaction to come out significant, but the P-value is not nearly so small as the ones we had before. I guess the explanation for this is that the data cluster pretty tightly about the lines in the graph, so that even a small difference in slopes can be signficant. You might argue that my red line is not quite as steep as the others, but the difference does look very small.\nLet’s take a look at the summary for my interaction model:\n\nsummary(improvement.2)\n\n\nCall:\nlm(formula = improvement ~ last + expenditure + last:expenditure, \n    data = productivity)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.32417 -0.14885 -0.02465  0.13739  0.55556 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.27827    0.64967   0.428  0.67278    \nlast                      0.93243    0.09124  10.220 1.32e-09 ***\nexpendituremoderate      -4.50268    1.25959  -3.575  0.00179 ** \nexpenditurehigh          -7.14795    1.91223  -3.738  0.00121 ** \nlast:expendituremoderate  0.32217    0.14243   2.262  0.03444 *  \nlast:expenditurehigh      0.40858    0.17549   2.328  0.02997 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2135 on 21 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9666 \nF-statistic: 151.5 on 5 and 21 DF,  p-value: 9.763e-16\n\n\nThe last two Estimates, the ones beginning with last:, came from the interaction. Once again low is my baseline. These say how the slopes for last for the other groups differ from the slopes for last for low expenditure. (Yes, this is confusing. Read it again until you get it.)\nThe biggest-size difference in slopes, about \\(0.40\\), is between low expenditure and high expenditure. This says that the line for high expenditure is this much more steep than the line for low expenditure. That’s where the non-parallelism is on the graph, such as it is. A small but significant difference in slopes. You can tell that the effect is small by looking in the Estimate column; those changes in slopes, \\(0.40\\) and \\(0.32\\), are the smallest things in size out of everything in that column.\nHaving seen this, you now realize that I did this question wrong (or, at least, I led you through it wrong). The right way to do this would have been to fit the interaction model first, see that it is significant, and then done some predictions to assess the effects of things:\n\nplot_cap(improvement.2, condition = c(\"last\", \"expenditure\"))\n\n\n\n\nThe three lines are not quite parallel, and evidently non-parallel enough to be significant. (This graph does not give you any sense of how much variability there is in the slopes; the at-least partial non-overlapping of the confidence envelopes tells you that there is a significant effect of expenditure, but it doesn’t tell you about the interaction.)\nThe effect is more or less as we described it before: as last goes up (for fixed expenditure), the predicted productivity improvements for this year go up, and as expenditure level goes up, the predictions go down. But the rates at which they go up or down are different, which is the significant interaction coming into play. Having said that, they are not very different, so I cheated and pretended the interaction was not significant (or that I was using \\(\\alpha=0.01\\)), so that you would have something easier to interpret. Qualitatively, the story is the same either way, because the sizes of the interaction terms are small compared to the others. So, even though I cheated, you ended up with more or less the same conclusions doing it the way I asked you to do it, or doing it the way I just did it.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ancova.html#treating-leprosy-1",
    "href": "ancova.html#treating-leprosy-1",
    "title": "29  Analysis of covariance",
    "section": "29.4 Treating leprosy",
    "text": "29.4 Treating leprosy\nTwo drugs are being tested in the treatment of leprosy. These are labelled A and D. There is also a control drug, labelled F. The response variable is a post-treatment score of leprosy bacilli (measured at six different sites on each patient). A lower score is better.\nThus far, we have a standard one-way analysis of variance. But the researchers wanted greater precision in assessing the effects (if any) of the drugs, so they also measured a pre-treatment score of leprosy bacilli. The data are in the file link. The pre-treatment and post-treatment scores are labelled pre and post respectively.\n\nRead in the data and check that you have apparently the right thing.\n\nSolution\nTake a look at the data file. The values have multiple spaces between them, but they are aligned with each other and the column headings, so read_table is the thing:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/leprosy.txt\"\nlepro &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  drug = col_character(),\n  pre = col_double(),\n  post = col_double()\n)\n\n\nCall it what you like.\nThat looks good, with variables of the right names.\n\\(\\blacksquare\\)\n\n* Make a scatterplot of post-treatment score against pre-treatment score, with the points for each drug drawn in a different colour. Add a linear trend for each drug.\n\nSolution\nThis is the kind of thing that ggplot does without batting an eyelid:\n\nggplot(lepro, aes(x = pre, y = post, colour = drug)) + geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\\(\\blacksquare\\)\n\nDoes it appear that including the pre-treatment score was a good idea? Explain briefly.\n\nSolution\nThe overall trend on the scatterplot is that a higher pre tends to go with a higher post, regardless of drug, so including this information appears to be informative. I personally suspect that there’s some fan-out happening on the pre-post relationship, but I’m not planning to make you explore that.\n\\(\\blacksquare\\)\n\nWhat about this dataset suggests that analysis of covariance is a method worth trying?\n\nSolution\nThe key is a mixture of categorical and quantitative explanatory variables. Here we have a categorical variable drug and a quantitative one pre. If we had only one type of explanatory variable, we could do a regression or an ANOVA as appropriate. But we don’t. In some ways, it’s not worth making a fuss about the distinction, because regressions and ANOVAs are all linear models anyway. But you may see the term “analysis of covariance”, so it’s worth your while to know what it’s about.\n\\(\\blacksquare\\)\n\nFit an analysis of covariance model to predict post-treatment score. Include an interaction between your explanatory variables. (You don’t need to look at the output from the model.)\n\nSolution\nThis is what you’d guess. lm handles the interaction properly, even though pre is a quantitative variable.\n\nlepro.1 &lt;- lm(post ~ pre * drug, data = lepro)\n\nI wanted to take a look, so I did:\n\nsummary(lepro.1)\n\n\nCall:\nlm(formula = post ~ pre * drug, data = lepro)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.225 -2.437 -0.586  1.126  8.775 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  -1.6306     2.9455  -0.554   0.5850  \npre           0.7452     0.2849   2.616   0.0152 *\ndrugD        -2.9549     4.1246  -0.716   0.4806  \ndrugF        -1.4780     5.4678  -0.270   0.7892  \npre:drugD     0.3233     0.3846   0.841   0.4089  \npre:drugF     0.4492     0.4458   1.008   0.3236  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.07 on 24 degrees of freedom\nMultiple R-squared:  0.6915,    Adjusted R-squared:  0.6272 \nF-statistic: 10.76 on 5 and 24 DF,  p-value: 1.63e-05\n\n\nFor testing the interaction, there are two slope coefficients that should be zero if there is no interaction. So we have to test this with drop1, which is next.\n\\(\\blacksquare\\)\n\nPass your fitted model of the last part into drop1. Is the interaction term significant?\n\nSolution\nJust this:\n\ndrop1(lepro.1, test = \"F\")\n\n\n\n  \n\n\n\nThere is only a test for the interaction term because you can’t take out the main effects until you’ve taken out the interaction.\nThe P-value for the interaction is very large (0.5606) so it is nowhere near significant. We can drop the interaction.\n\\(\\blacksquare\\)\n\nFit a model without the interaction. Is this a sensible thing to do (in addition, that is, to the fact that I just asked you to do it)?\n\nSolution\nChange the * to a +:\n\nlepro.2 &lt;- lm(post ~ pre + drug, data = lepro)\n\nOr use update (not much in it, here):\n\nlepro.2a &lt;- update(lepro.1, . ~ . - pre:drug)\n\nWe just said that the interaction could come out, since it wasn’t significant, so this is exactly the model that we should be fitting.\n\\(\\blacksquare\\)\n\nTake a look at the summary of your preferred model. Is there a significant effect of pre-treatment score? Describe the effects of the different drugs on the post-treatment score. (Don’t do any tests for drug.) Does your comparison of drugs make sense?\n\nSolution\nMine was the no-interaction model lepro.2:\n\nsummary(lepro.2)\n\n\nCall:\nlm(formula = post ~ pre + drug, data = lepro)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4115 -2.3891 -0.5711  1.7237  8.5885 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -3.8808     1.9862  -1.954   0.0616 .  \npre           0.9872     0.1645   6.001 2.45e-06 ***\ndrugD         0.1090     1.7951   0.061   0.9521    \ndrugF         3.4461     1.8868   1.826   0.0793 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.006 on 26 degrees of freedom\nMultiple R-squared:  0.6763,    Adjusted R-squared:  0.6389 \nF-statistic:  18.1 on 3 and 26 DF,  p-value: 1.501e-06\n\n\nThe pre-treatment term is definitely significant, with a P-value of 0.0000025. So pre-treatment score definitely has an impact on post-treatment score.\nI didn’t ask you to test for significance of drugs. I just wanted you to assess their coefficients. Drug A is being used as the baseline, so its coefficient is zero. Drug D has a slightly positive coefficient (0.109) so its average bacilli score is slightly higher (for any pre-treatment score) than for drug A. Drug F, which was the placebo, has a slope of 3.446, so its average bacilli score is a fair bit higher than for either of the other drugs. This makes sense because a higher score is worse, and the two “real” drugs are both better than the fake one.\nWhether there is a real drug difference, I didn’t ask you to assess, but you could do it by drop1 again, this way:\n\ndrop1(lepro.2, test = \"F\")\n\n\n\n  \n\n\n\nThis is actually not significant.2 This is one of those cases where the non-significant drug has a slightly bigger AIC than &lt;none&gt;, so drop1 considers it best to leave it in the model.\n\\(\\blacksquare\\)\n\nObtain predicted values for post for each of the three drugs at pre scores 5, 12 and 20. To do this, obtain a new data frame that has all 9 combinations of drugs and pre scores, and then feed this into predict using your preferred model.3\n\nSolution\nFirst, make the new data frame for predicting from, using crossing. I’m doing this in small steps for clarity: first, I define all the drugs and pre values, and then I feed them into datagrid:\n\ndrugs &lt;- c(\"A\", \"D\", \"F\")\npres &lt;- c(5, 12, 20)\nnew &lt;- datagrid(model = lepro.2, drug = drugs, pre = pres)\nnew\n\n\n\n  \n\n\n\nNow I obtain the predictions, from my best model lepro.2:\n\npreds &lt;- cbind(predictions(lepro.2, newdata = new))\npreds %&gt;% select(drug, pre, estimate, conf.low, conf.high)\n\n\n\n  \n\n\n\nI gave this a name in case I feel like using it again later.\n\\(\\blacksquare\\)\n\nNow, plot the data with the fitted lines on.\n\nSolution\nThe starting point is to plot the predictions, which is plot_cap:\n\nplot_cap(lepro.2, condition = c(\"pre\", \"drug\"))\n\n\n\n\nThe olive-coloured line is actually the red and green lines right next to each other. You can check from the previous part that the predictions for drugs A and D are very close together, with those for drug F (the placebo) being higher (worse).\nThis is a ggplot, so we can add things to it. The idea is to say that the next thing to plot comes from some other dataframe, and to specify everything we need (that is, not to inherit from the original ggplot that is lurking within plot_cap):\n\nplot_cap(lepro.2, condition = c(\"pre\", \"drug\")) +\n  geom_point(data = lepro, aes(x = pre, y = post, colour = drug), \n             inherit.aes = FALSE)\n\n\n\n\nThere is quite a lot of variability (which is why those confidence bands are so wide), but at least some of the blue points from drug F are above the others (worse), and there is very little to choose between drugs A and D.\n\\(\\blacksquare\\)\n\nAre the lines on your plot parallel, with the same slopes? Is this what you would expect? Explain briefly.\n\nSolution\nMy lines are parallel. This is exactly what I would expect, since my best model has no interaction, and the interaction is what would make the lines not be parallel. If your best model did have the interaction term still in it, your predictions would have been these:\n\nplot_cap(lepro.1, condition = c(\"pre\", \"drug\")) +\n  geom_point(data = lepro, aes(x = pre, y = post, colour = drug), inherit.aes = FALSE)\n\n\n\n\nThere is, as you see, a substantial scatter in the points that would make it very difficult to prove that those three slopes are really different, even though the lines cross.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "ancova.html#footnotes",
    "href": "ancova.html#footnotes",
    "title": "29  Analysis of covariance",
    "section": "",
    "text": "Analysis of covariance is just a linear model, so predict works the same here as in regression.↩︎\nThis is why I didn’t ask you to test this, since it would have confused the story.↩︎\nAnalysis of covariance is just a linear model, so predict works the same here as in regression.↩︎"
  },
  {
    "objectID": "manova.html#fabricated-data",
    "href": "manova.html#fabricated-data",
    "title": "30  Multivariate analysis of variance",
    "section": "30.1 Fabricated data",
    "text": "30.1 Fabricated data\nThe data in link (probably made up) are measurements of two variables y1 and y2 on three groups, labelled a, b and c. We want to see whether y1 or y2 or some combination of them differ among the groups.\n\nRead in the data and check that you have 12 observations on 2 numeric variables from 3 different groups.\nRun a one-way ANOVA predicting y1 from group. Are there any significant differences in y1 among the groups? If necessary, run Tukey. What do you conclude?\nRepeat all the previous part for y2.\nMake a plot of y1 against y2, with the points distinguished by which group they are from.\nHow is it that group b is not distinguishable on either y1 or y2 individually (from the ANOVAs), but is distinguishable on your graph? Explain briefly.\nRun a one-way multivariate analysis of variance, obtaining a P-value. Note the very small P-value."
  },
  {
    "objectID": "manova.html#do-characteristics-of-urine-depend-on-obesity",
    "href": "manova.html#do-characteristics-of-urine-depend-on-obesity",
    "title": "30  Multivariate analysis of variance",
    "section": "30.2 Do characteristics of urine depend on obesity?",
    "text": "30.2 Do characteristics of urine depend on obesity?\nA study was made of the characteristics of urine of young men. The men were classified into four groups based on their degree of obesity. (The groups are labelled a, b, c, d.) Four variables were measured, x (which you can ignore), pigment creatinine, chloride and chlorine. The data are in link as a .csv file. There are 45 men altogether.\n\nRead in the data and check that you have the right number of observations and the right variables.\nMake boxplots of each of the three variables of interest against obesity group.\nHow, if at all, do the groups differ on the variables of interest?\nRun a multivariate analysis of variance, using the three variables of interest as response variables, and the obesity group as explanatory. (This is a so-called one-way MANOVA.)\nWhat do you conclude? Is this in line with what your boxplots said? Explain briefly."
  },
  {
    "objectID": "manova.html#how-do-height-and-weight-depend-on-sport-played-by-elite-athletes",
    "href": "manova.html#how-do-height-and-weight-depend-on-sport-played-by-elite-athletes",
    "title": "30  Multivariate analysis of variance",
    "section": "30.3 How do height and weight depend on sport played by elite athletes?",
    "text": "30.3 How do height and weight depend on sport played by elite athletes?\nThis question uses the data on Australian elite athletes, which can be found at link. 202 elite athletes had various physical and physiological measurements taken, and for each athlete, their gender (in the column Sex) and the sport they play (Sport) was also noted. The data values are separated by tabs, meaning that read_tsv will read them in.\n\nRead in the data and verify that you have 202 rows and 13 columns.\nWe are going to see whether the height-weight combination of an athlete depends significantly on which sport they play and which gender they are. Explain (very) briefly why a multivariate analysis of variance will be necessary to do this.\nCreate a response variable for a MANOVA. (You don’t need to show what you got, since it is rather big.) The columns you want are called Ht and Wt.\nRun a multivariate analysis of variance to see whether the height-weight combination depends significantly on gender, sport or the combination of both. Display the results. Use the small-m manova way to do this one.\nBriefly justify removing the interaction term from your previous model, and fit a model without it. You can use either manova or Manova for this; they should both work. (You only need to use one of them.) Display your results.\nSee if you can make a graph that shows what’s going on here. Bear in mind that you have two quantitative variables and two categorical ones, so you will need to find a way to display everything. Hint: what would you do with two quantitative variables and one categorical one? Can you find a way to generalize that idea by displaying the other categorical variable differently? I think we have seen a couple of ideas that will work, somewhere in the lecture notes (but in a different context). Or you can search for ideas, of course. For full marks, obtain a plot which does not give you any warnings. (The grader will know which of the possible plots give warnings, and will deduct extra marks if you assert that a plot gives no warnings when in fact it does. That is to say, if you get any warnings, you need to include those in what you hand in).\n\nMy solutions follow:"
  },
  {
    "objectID": "manova.html#fabricated-data-1",
    "href": "manova.html#fabricated-data-1",
    "title": "30  Multivariate analysis of variance",
    "section": "30.4 Fabricated data",
    "text": "30.4 Fabricated data\nThe data in link (probably made up) are measurements of two variables y1 and y2 on three groups, labelled a, b and c. We want to see whether y1 or y2 or some combination of them differ among the groups.\n\nRead in the data and check that you have 12 observations on 2 numeric variables from 3 different groups.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/simple-manova.txt\"\nsimple &lt;- read_delim(my_url, \" \")\n\nRows: 12 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): group\ndbl (2): y1, y2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsimple\n\n\n\n  \n\n\n\nAs promised: 12 observations, 2 numeric variables y1 and y2, and one categorical variable (as text) containing three groups a,b,c.\n\\(\\blacksquare\\)\n\nRun a one-way ANOVA predicting y1 from group. Are there any significant differences in y1 among the groups? If necessary, run Tukey. What do you conclude?\n\nSolution\n\nsimple.1 &lt;- aov(y1 ~ group, data = simple)\nsummary(simple.1)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ngroup        2  63.45   31.72    21.2 0.000393 ***\nResiduals    9  13.47    1.50                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(simple.1)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y1 ~ group, data = simple)\n\n$group\n        diff        lwr      upr     p adj\nb-a 1.666667 -0.9417891 4.275122 0.2289181\nc-a 5.200000  2.9089670 7.491033 0.0003548\nc-b 3.533333  1.0391725 6.027494 0.0083999\n\n\nThe \\(F\\)-test said that there were differences among the groups, so I ran Tukey and found that group c is significantly bigger on y1 than the other two groups.\n\\(\\blacksquare\\)\n\nRepeat all the previous part for y2.\n\nSolution\n\nsimple.2 &lt;- aov(y2 ~ group, data = simple)\nsummary(simple.2)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ngroup        2  19.05   9.525   9.318 0.00642 **\nResiduals    9   9.20   1.022                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(simple.2)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y2 ~ group, data = simple)\n\n$group\n    diff        lwr      upr     p adj\nb-a  3.0  0.8440070 5.155993 0.0093251\nc-a  2.4  0.5063697 4.293630 0.0157279\nc-b -0.6 -2.6615236 1.461524 0.7050026\n\n\nSame idea: there is a difference, and Tukey reveals that this time group a is significantly smaller than the other groups on y2.\n\\(\\blacksquare\\)\n\nMake a plot of y1 against y2, with the points distinguished by which group they are from.\n\nSolution\ny1 and y2 can be either way around, since they are both response variables (!):\n\nggplot(simple, aes(x = y1, y = y2, colour = group)) + geom_point()\n\n\n\n\n\\(\\blacksquare\\)\n\nHow is it that group b is not distinguishable on either y1 or y2 individually (from the ANOVAs), but is distinguishable on your graph? Explain briefly.\n\nSolution\nGroup b has the same kind of y1 values as group a and the same kind of y2 values as group c. This means looking at either variable singly is not enough to distinguish group b. But group b stands out on the graph as being “top left” of the picture. That means that y1 has to be low and y2 has to be high, both together, to pick out group b. That seems like a lot of work, but the important point that I want you to raise is that the points in group b are distinguished by a particular combination of values of y1 and y2.\n\\(\\blacksquare\\)\n\nRun a one-way multivariate analysis of variance, obtaining a P-value. Note the very small P-value.\n\nSolution\nThis is just like the seed weight and yield example in class (deliberately so):\n\nresponse &lt;- with(simple, cbind(y1, y2))\nsimple.3 &lt;- manova(response ~ group, data = simple)\nsummary(simple.3)\n\n          Df Pillai approx F num Df den Df    Pr(&gt;F)    \ngroup      2 1.3534   9.4196      4     18 0.0002735 ***\nResiduals  9                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe P-value here, 0.00027, is smaller than either of the P-values for the one-variable ANOVAs. This expresses the idea that the group difference is essentially multivariate (it depends on the combination of y1 and y2 values). I only wanted you to note that it is smaller than either of the other ANOVAs; you don’t need to say anything about why.\nFor completeness, we should do Box’s M test as well:\n\nsummary(BoxM(response, simple$group))\n\n       Box's M Test \n\nChi-Squared Value = 3.517357 , df = 6  and p-value: 0.742 \n\n\nNo problems here at all.\nWe were in the fortunate position of being able to draw a picture, because we had only two response variables, and we could plot them against each other with the groups labelled. (If we had three response variables, like the peanuts example, we wouldn’t be able to do this, barring some kind of 3D-plotting procedure.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "manova.html#do-characteristics-of-urine-depend-on-obesity-1",
    "href": "manova.html#do-characteristics-of-urine-depend-on-obesity-1",
    "title": "30  Multivariate analysis of variance",
    "section": "30.5 Do characteristics of urine depend on obesity?",
    "text": "30.5 Do characteristics of urine depend on obesity?\nA study was made of the characteristics of urine of young men. The men were classified into four groups based on their degree of obesity. (The groups are labelled a, b, c, d.) Four variables were measured, x (which you can ignore), pigment creatinine, chloride and chlorine. The data are in link as a .csv file. There are 45 men altogether.\n\nRead in the data and check that you have the right number of observations and the right variables.\n\nSolution\nread_csv:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/urine.csv\"\nurine &lt;- read_csv(my_url)\n\nRows: 45 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): obesity\ndbl (4): x, creatinine, chloride, chlorine\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nurine\n\n\n\n  \n\n\n\nThe variables are as promised, and we do indeed have 45 rows.\n\\(\\blacksquare\\)\n\nMake boxplots of each of the three variables of interest against obesity group.\n\nSolution\nJust churn through it:\n\nggplot(urine, aes(x = obesity, y = creatinine)) + geom_boxplot()\n\n\n\nggplot(urine, aes(x = obesity, y = chlorine)) + geom_boxplot()\n\n\n\nggplot(urine, aes(x = obesity, y = chloride)) + geom_boxplot()\n\n\n\n\nThis also works with facets, though it is different from the other ones we’ve done, in that we have to collect together the \\(y\\)-variables first rather than the \\(x\\)s:\n\nurine %&gt;%\n  pivot_longer(creatinine:chlorine, names_to=\"yname\", values_to=\"y\") %&gt;%\n  ggplot(aes(x = obesity, y = y)) + geom_boxplot() +\n  facet_wrap(~yname, scales = \"free\", ncol = 2)\n\n\n\n\nI decided to throw a couple of things in here: first, the scales=\"free\" thing since the \\(y\\)-variables (this time) are on different scales, and second, the ncol=2 to arrange the facets in (3 cells of) a \\(2\\times 2\\) grid, rather than having them come out tall and skinny.1 It’s unusual to have faceted boxplots, but this is one of those cases where it makes sense. (The key is different \\(y\\)’s but the same \\(x\\), I think.)\nThe conclusions about the boxplots are, of course, the same. I think it makes it easier to have the three boxplots side by side, but it’s up to you whether you think that gain is worth the extra coding.\nExtra: let’s take another look at that longer data frame:\n\nurine %&gt;%\n  pivot_longer(creatinine:chlorine, names_to=\"yname\", values_to=\"y\") \n\n\n\n  \n\n\n\nYou might say that there is one quantitative variable now, the thing we called y, and two categorical variables, obesity and yname. So why not make a grouped boxplot? All right:\n\nurine %&gt;%\n  pivot_longer(creatinine:chlorine, names_to=\"yname\", values_to=\"y\") %&gt;%\n  ggplot(aes(x=yname, y=y, colour=obesity)) + geom_boxplot()\n\n\n\n\nThis actually looks a lot like the facetted boxplots (if you imagine drawing a box around each group of four boxplots and removing the scales=\"free\" before). There is, as the saying goes, more than one way to skin a cat.\n\\(\\blacksquare\\)\n\nHow, if at all, do the groups differ on the variables of interest?\n\nSolution\nAny sensible comment here is good. You can take the approach that group D is lowest on creatinine and chloride, but there is not much to choose (given the amount of variability) on chlorine. Or you can say that groups B and C are generally highest, except for chloride where A is higher. Anything of this general kind is fine. The point here is that some of the groups appear to be different on some of the variables, which would make a multivariate analysis of variance (in a moment) come out significant.\n\\(\\blacksquare\\)\n\nRun a multivariate analysis of variance, using the three variables of interest as response variables, and the obesity group as explanatory. (This is a so-called one-way MANOVA.)\n\nSolution\nCreate the response variable and run manova:\n\nresponse &lt;- with(urine, cbind(creatinine, chlorine, chloride))\nurine.1 &lt;- manova(response ~ obesity, data = urine)\nsummary(urine.1)\n\n          Df  Pillai approx F num Df den Df  Pr(&gt;F)  \nobesity    3 0.43144   2.2956      9    123 0.02034 *\nResiduals 41                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAt some point, you should run Box’s M test, either here or at the start of the next part (before you get to your conclusions):\n\nsummary(BoxM(response, urine$obesity))\n\n       Box's M Test \n\nChi-Squared Value = 30.8322 , df = 18  and p-value: 0.0301 \n\n\nThis P-value is small, but not, for this test, small enough to be a problem.\n\\(\\blacksquare\\)\n\nWhat do you conclude? Is this in line with what your boxplots said? Explain briefly.\n\nSolution\nThe null hypothesis (that each of the variables have the same mean for each of the groups) is rejected: that is, not all of the groups have the same mean on all of the variables. Based on the idea that there seemed to be differences between the groups in the boxplots, this makes sense. (That’s what I saw, anyway. There was a lot of variability within the groups, which is why the P-value didn’t come out smaller.)\nExtra: the other way of doing this is the following, using Manova from car:\n\nresponse.1 &lt;- lm(response ~ obesity, data = urine)\nsummary(Manova(response.1))\n\n\nType II MANOVA Tests:\n\nSum of squares and products for error:\n            creatinine   chlorine   chloride\ncreatinine  496.768761 -225.02136   4.507476\nchlorine   -225.021364 1431.85432  86.226818\nchloride      4.507476   86.22682 152.854274\n\n------------------------------------------\n \nTerm: obesity \n\nSum of squares and products for the hypothesis:\n           creatinine  chlorine  chloride\ncreatinine  181.07035 -66.72530  40.03686\nchlorine    -66.72530 103.80568 -41.37182\nchloride     40.03686 -41.37182  20.03773\n\nMultivariate Tests: obesity\n                 Df test stat approx F num Df    den Df    Pr(&gt;F)   \nPillai            3 0.4314359 2.295559      9 123.00000 0.0203377 * \nWilks             3 0.6080320 2.395937      9  95.06636 0.0171207 * \nHotelling-Lawley  3 0.5800706 2.427703      9 113.00000 0.0146529 * \nRoy               3 0.4336849 5.927027      3  41.00000 0.0018659 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nmanova gives you the Pillai test, but Manova gives you all four; here, they are all significant, but to different degrees.\nThe result is the same, or at least consistent. You don’t need do this one here (though you can as an alternative), but when you come to repeated measures you will need to be able to use Manova.\nTo understand the differences in the variables due to the groups, we need to run a discriminant analysis (coming up later).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "manova.html#how-do-height-and-weight-depend-on-sport-played-by-elite-athletes-1",
    "href": "manova.html#how-do-height-and-weight-depend-on-sport-played-by-elite-athletes-1",
    "title": "30  Multivariate analysis of variance",
    "section": "30.6 How do height and weight depend on sport played by elite athletes?",
    "text": "30.6 How do height and weight depend on sport played by elite athletes?\nThis question uses the data on Australian elite athletes, which can be found at link. 202 elite athletes had various physical and physiological measurements taken, and for each athlete, their gender (in the column Sex) and the sport they play (Sport) was also noted. The data values are separated by tabs, meaning that read_tsv will read them in.\n\nRead in the data and verify that you have 202 rows and 13 columns.\n\nSolution\nAs in the hint:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ais.txt\"\nathletes &lt;- read_tsv(my_url)\n\nRows: 202 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nathletes\n\n\n\n  \n\n\n\n202 rows and 13 columns indeed.\n\\(\\blacksquare\\)\n\nWe are going to see whether the height-weight combination of an athlete depends significantly on which sport they play and which gender they are. Explain (very) briefly why a multivariate analysis of variance will be necessary to do this.\n\nSolution\nWe now have two response variables, height and weight, rather than just one. In order to do a test with more than one response variable, we need to use multivariate ANOVA rather than regular ANOVA. (The first sentence is enough.)\nExtra: the explanatory variables, sport and gender, are both categorical here, which makes a MANOVA work. If either of them had been quantitative, we would have needed to do a multivariate regression, which is beyond the scope of what we do in this course.\n\\(\\blacksquare\\)\n\nCreate a response variable for a MANOVA. (You don’t need to show what you got, since it is rather big.) The columns you want are called Ht and Wt.\n\nSolution\nUse cbind to glue the two columns together:\n\nresponse &lt;- with(athletes, cbind(Ht, Wt))\n\nEither way around, height or weight first, is good. Also good is the dollar sign, which you’ll need to do twice (and thus the with way is more elegant):\n\nresponse_a &lt;- cbind(athletes$Ht, athletes$Wt)\n\n\nathletes %&gt;% select(Ht, Wt) %&gt;% \n  as.matrix() -&gt; y \n\n\nresponse\n\n          Ht     Wt\n  [1,] 176.8  59.90\n  [2,] 172.6  63.00\n  [3,] 176.0  66.30\n  [4,] 169.9  60.70\n  [5,] 183.0  72.90\n  [6,] 178.2  67.90\n  [7,] 177.3  67.50\n  [8,] 174.1  74.10\n  [9,] 173.6  68.20\n [10,] 173.7  68.80\n [11,] 178.7  75.30\n [12,] 183.3  67.40\n [13,] 174.4  70.00\n [14,] 173.3  74.00\n [15,] 168.6  51.90\n [16,] 174.0  74.10\n [17,] 176.0  74.30\n [18,] 172.2  77.80\n [19,] 182.7  66.90\n [20,] 180.5  83.80\n [21,] 179.8  82.90\n [22,] 179.6  64.10\n [23,] 171.7  68.85\n [24,] 195.9  78.90\n [25,] 189.7  74.40\n [26,] 177.8  69.10\n [27,] 185.0  74.90\n [28,] 184.6  64.60\n [29,] 174.0  63.70\n [30,] 186.2  75.20\n [31,] 173.8  62.30\n [32,] 171.4  66.50\n [33,] 179.9  62.90\n [34,] 193.4  96.30\n [35,] 188.7  75.50\n [36,] 169.1  63.00\n [37,] 177.9  80.50\n [38,] 177.5  71.30\n [39,] 179.6  70.50\n [40,] 181.3  73.20\n [41,] 179.7  68.70\n [42,] 185.2  80.50\n [43,] 177.3  72.90\n [44,] 179.3  74.50\n [45,] 175.3  75.40\n [46,] 174.0  69.50\n [47,] 183.3  66.40\n [48,] 184.7  79.70\n [49,] 180.2  73.60\n [50,] 180.2  78.70\n [51,] 176.0  75.00\n [52,] 156.0  49.80\n [53,] 179.7  67.20\n [54,] 180.9  66.00\n [55,] 179.5  74.30\n [56,] 178.9  78.10\n [57,] 182.1  79.50\n [58,] 186.3  78.50\n [59,] 170.0  64.80\n [60,] 170.0  59.00\n [61,] 180.5  72.10\n [62,] 173.3  75.60\n [63,] 173.5  71.40\n [64,] 181.0  69.70\n [65,] 175.0  63.90\n [66,] 170.3  55.10\n [67,] 165.0  60.00\n [68,] 169.8  58.00\n [69,] 174.1  64.70\n [70,] 175.0  87.50\n [71,] 171.1  78.90\n [72,] 172.7  83.90\n [73,] 175.6  82.80\n [74,] 171.6  74.40\n [75,] 172.3  94.80\n [76,] 171.4  49.20\n [77,] 178.0  61.90\n [78,] 162.0  53.60\n [79,] 167.3  63.70\n [80,] 162.0  52.80\n [81,] 170.8  65.20\n [82,] 163.0  50.90\n [83,] 166.1  57.30\n [84,] 176.0  60.00\n [85,] 163.9  60.10\n [86,] 173.0  52.50\n [87,] 177.0  59.70\n [88,] 168.0  57.30\n [89,] 172.0  59.60\n [90,] 167.9  71.50\n [91,] 177.5  69.70\n [92,] 162.5  56.10\n [93,] 172.5  61.10\n [94,] 166.7  47.40\n [95,] 175.0  56.00\n [96,] 157.9  45.80\n [97,] 158.9  47.80\n [98,] 156.9  43.80\n [99,] 148.9  37.80\n[100,] 149.0  45.10\n[101,] 172.7  67.00\n[102,] 176.5  74.40\n[103,] 183.0  79.30\n[104,] 194.4  87.50\n[105,] 193.4  83.50\n[106,] 180.2  78.00\n[107,] 183.0  78.00\n[108,] 184.0  85.00\n[109,] 192.7  84.70\n[110,] 187.2  92.00\n[111,] 183.9  72.30\n[112,] 192.0  83.00\n[113,] 190.4  96.90\n[114,] 190.7  85.70\n[115,] 181.8  85.40\n[116,] 188.3  85.30\n[117,] 198.0  93.50\n[118,] 186.0  86.80\n[119,] 192.0  87.90\n[120,] 185.6  87.20\n[121,] 165.3  53.80\n[122,] 185.6  89.80\n[123,] 189.0  91.10\n[124,] 193.4  88.60\n[125,] 185.6  92.30\n[126,] 194.6  97.00\n[127,] 189.0  89.50\n[128,] 188.1  88.20\n[129,] 200.4  92.20\n[130,] 195.3  78.90\n[131,] 194.1  90.30\n[132,] 187.9  87.00\n[133,] 209.4 113.70\n[134,] 203.4  98.00\n[135,] 198.7 100.20\n[136,] 187.1  79.40\n[137,] 196.6  90.30\n[138,] 186.1  77.70\n[139,] 192.8  83.90\n[140,] 195.2  75.50\n[141,] 169.1  60.60\n[142,] 186.6  71.00\n[143,] 184.4  71.80\n[144,] 187.3  76.80\n[145,] 185.1 102.70\n[146,] 185.5  94.25\n[147,] 184.9  79.00\n[148,] 175.0  66.60\n[149,] 185.4  71.80\n[150,] 181.0  74.80\n[151,] 176.0  68.20\n[152,] 176.2  62.30\n[153,] 174.0  61.00\n[154,] 191.0  77.50\n[155,] 171.0  57.40\n[156,] 174.0  71.40\n[157,] 180.2  70.30\n[158,] 178.5  80.20\n[159,] 190.3  84.20\n[160,] 185.0 111.30\n[161,] 189.0  80.70\n[162,] 180.1  97.90\n[163,] 189.2 123.20\n[164,] 182.6  72.90\n[165,] 186.0  83.00\n[166,] 174.9  75.90\n[167,] 180.6  70.70\n[168,] 178.6  67.10\n[169,] 173.0  69.20\n[170,] 179.7  67.05\n[171,] 174.6  70.50\n[172,] 178.0  70.80\n[173,] 178.5  71.00\n[174,] 171.3  69.10\n[175,] 178.0  62.90\n[176,] 189.1  94.80\n[177,] 195.4  94.60\n[178,] 179.1 108.20\n[179,] 180.1  97.90\n[180,] 179.6  75.20\n[181,] 174.7  74.80\n[182,] 192.7  94.20\n[183,] 179.3  76.10\n[184,] 197.5  94.70\n[185,] 182.7  86.20\n[186,] 190.5  79.60\n[187,] 191.0  85.30\n[188,] 179.6  74.40\n[189,] 192.6  93.50\n[190,] 194.1  87.60\n[191,] 193.0  85.40\n[192,] 193.9 101.00\n[193,] 187.7  74.90\n[194,] 185.3  87.30\n[195,] 191.5  90.00\n[196,] 184.6  94.70\n[197,] 179.9  76.30\n[198,] 183.9  93.20\n[199,] 183.5  80.00\n[200,] 183.1  73.80\n[201,] 178.4  71.10\n[202,] 190.8  76.70\n\n\n\\(\\blacksquare\\)\n\nRun a multivariate analysis of variance to see whether the height-weight combination depends significantly on gender, sport or the combination of both. Display the results. Use the small-m manova way to do this one.\n\nSolution\nThe straightforward way is small-m manova. I don’t know what I should be calling my fitted model object, so I’m making something up. Don’t forget to use the right names for the variables, and to include an interaction:\n\nhtwt.1 &lt;- manova(response ~ Sex * Sport, data = athletes)\nsummary(htwt.1)\n\n           Df  Pillai approx F num Df den Df Pr(&gt;F)    \nSex         1 0.52412  101.325      2    184 &lt;2e-16 ***\nSport       9 0.87914   16.123     18    370 &lt;2e-16 ***\nSex:Sport   6 0.04105    0.646     12    370 0.8023    \nResiduals 185                                          \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nathletes %&gt;% \n  unite(\"combo\", c(Sex, Sport)) -&gt; athletes_combo\nsummary(BoxM(response, athletes_combo$combo))\n\n       Box's M Test \n\nChi-Squared Value = 86.605 , df = 48  and p-value: 0.000537 \n\n\nThis test gives just small enough a P-value to be worried about (remembering that less than 0.001 is the problem zone). It seems unlikely, though, that the right test (whatever it is) would produce a significant interaction.\nThe other way is apparently to use large-M Manova from package car. This goes as a two-stage process: fit an lm first with the same model formula as above, and then pass that into Manova. This way, you just display the final result, rather than passing it into summary:\n\nhtwt.2 &lt;- lm(response ~ Sex * Sport, data = athletes)\nManova(htwt.2)\n\nError in Anova.mlm(mod, ...): model is singular\n\n\nExcept that this doesn’t work, for reasons that I don’t understand. htwt.2 is fine; it’s the Manova that is not working.\n\\(\\blacksquare\\)\n\nBriefly justify removing the interaction term from your previous model, and fit a model without it. You can use either manova or Manova for this; they should both work. (You only need to use one of them.) Display your results.\n\nSolution\nThe interaction should come out, since it is not significant. (This is what I meant by “briefly.”) To do that, in each case, replace the * with a +, or use update. Most of those work:\n\nhtwt.4 &lt;- manova(response ~ Sex + Sport, data = athletes)\nsummary(htwt.4)\n\n           Df  Pillai approx F num Df den Df    Pr(&gt;F)    \nSex         1 0.51888  102.454      2    190 &lt; 2.2e-16 ***\nSport       9 0.86923   16.314     18    382 &lt; 2.2e-16 ***\nResiduals 191                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nor\n\nhtwt.5 &lt;- update(htwt.1, . ~ . - Sex:Sport)\n\nError in if (projections) qr &lt;- lmcall$qr &lt;- TRUE: the condition has length &gt; 1\n\n\nIt seems that manova things don’t like update. If that happened to you, try it another way.\nor, with Manova, the same two ways:\n\nhtwt.6 &lt;- lm(response ~ Sex + Sport, data = athletes)\nhtwt.7 &lt;- Manova(htwt.6)\nsummary(htwt.7)\n\n\nType II MANOVA Tests:\n\nSum of squares and products for error:\n         Ht        Wt\nHt 6871.978  5842.771\nWt 5842.771 13613.021\n\n------------------------------------------\n \nTerm: Sex \n\nSum of squares and products for the hypothesis:\n         Ht       Wt\nHt 4582.744 6146.078\nWt 6146.078 8242.718\n\nMultivariate Tests: Sex\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1 0.4427781 75.48862      2    190 &lt; 2.22e-16 ***\nWilks             1 0.5572219 75.48862      2    190 &lt; 2.22e-16 ***\nHotelling-Lawley  1 0.7946171 75.48862      2    190 &lt; 2.22e-16 ***\nRoy               1 0.7946171 75.48862      2    190 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: Sport \n\nSum of squares and products for the hypothesis:\n         Ht        Wt\nHt 6162.455  7070.085\nWt 7070.085 13727.950\n\nMultivariate Tests: Sport\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            9 0.8692278 16.31358     18    382 &lt; 2.22e-16 ***\nWilks             9 0.3132928 16.60578     18    380 &lt; 2.22e-16 ***\nHotelling-Lawley  9 1.6093145 16.89780     18    378 &lt; 2.22e-16 ***\nRoy               9 1.0593835 22.48247      9    191 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nor\n\nhtwt.8 &lt;- update(htwt.2, . ~ . - Sex:Sport)\nhtwt.9 &lt;- Manova(htwt.8)\nsummary(htwt.9)\n\n\nType II MANOVA Tests:\n\nSum of squares and products for error:\n         Ht        Wt\nHt 6871.978  5842.771\nWt 5842.771 13613.021\n\n------------------------------------------\n \nTerm: Sex \n\nSum of squares and products for the hypothesis:\n         Ht       Wt\nHt 4582.744 6146.078\nWt 6146.078 8242.718\n\nMultivariate Tests: Sex\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1 0.4427781 75.48862      2    190 &lt; 2.22e-16 ***\nWilks             1 0.5572219 75.48862      2    190 &lt; 2.22e-16 ***\nHotelling-Lawley  1 0.7946171 75.48862      2    190 &lt; 2.22e-16 ***\nRoy               1 0.7946171 75.48862      2    190 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: Sport \n\nSum of squares and products for the hypothesis:\n         Ht        Wt\nHt 6162.455  7070.085\nWt 7070.085 13727.950\n\nMultivariate Tests: Sport\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            9 0.8692278 16.31358     18    382 &lt; 2.22e-16 ***\nWilks             9 0.3132928 16.60578     18    380 &lt; 2.22e-16 ***\nHotelling-Lawley  9 1.6093145 16.89780     18    378 &lt; 2.22e-16 ***\nRoy               9 1.0593835 22.48247      9    191 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThese both work, and give the same answer. I think this is because the thing being updated is a regular lm; the Manova bit comes after, and is calculated from the lm fit.\nAny of these is good, and gets you the same answer.\nThe way you run Box’s M test on these is on each explanatory variable separately:\n\nsummary(BoxM(response, athletes$Sex))\n\n       Box's M Test \n\nChi-Squared Value = 3.98602 , df = 3  and p-value: 0.263 \n\nsummary(BoxM(response, athletes$Sport))\n\n       Box's M Test \n\nChi-Squared Value = 68.16926 , df = 27  and p-value: 2.04e-05 \n\n\nThe second of these, the one for Sport, is small enough to be a problem, but the P-value in the MANOVA for Sport is extremely small, so we should be safe.\nExtra: more about the summary output from Manova:\n\nsummary(htwt.9)\n\n\nType II MANOVA Tests:\n\nSum of squares and products for error:\n         Ht        Wt\nHt 6871.978  5842.771\nWt 5842.771 13613.021\n\n------------------------------------------\n \nTerm: Sex \n\nSum of squares and products for the hypothesis:\n         Ht       Wt\nHt 4582.744 6146.078\nWt 6146.078 8242.718\n\nMultivariate Tests: Sex\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1 0.4427781 75.48862      2    190 &lt; 2.22e-16 ***\nWilks             1 0.5572219 75.48862      2    190 &lt; 2.22e-16 ***\nHotelling-Lawley  1 0.7946171 75.48862      2    190 &lt; 2.22e-16 ***\nRoy               1 0.7946171 75.48862      2    190 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: Sport \n\nSum of squares and products for the hypothesis:\n         Ht        Wt\nHt 6162.455  7070.085\nWt 7070.085 13727.950\n\nMultivariate Tests: Sport\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            9 0.8692278 16.31358     18    382 &lt; 2.22e-16 ***\nWilks             9 0.3132928 16.60578     18    380 &lt; 2.22e-16 ***\nHotelling-Lawley  9 1.6093145 16.89780     18    378 &lt; 2.22e-16 ***\nRoy               9 1.0593835 22.48247      9    191 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere are actually four competing tests that generalize the one-response-variable \\(F\\) statistic in different ways. If you go looking in a text on multivariate analysis, you’ll also see about all four of them, especially Wilks’s \\(\\Lambda\\) (that’s a capital lambda). They will generally agree in terms of conclusion, but you should probably at least glance at all of them.\nI am in danger of running out of numbers for my fitted models (and am definitely in danger of forgetting which one is which)!\n\\(\\blacksquare\\)\n\nSee if you can make a graph that shows what’s going on here. Bear in mind that you have two quantitative variables and two categorical ones, so you will need to find a way to display everything. Hint: what would you do with two quantitative variables and one categorical one? Can you find a way to generalize that idea by displaying the other categorical variable differently? I think we have seen a couple of ideas that will work, somewhere in the lecture notes (but in a different context). Or you can search for ideas, of course. For full marks, obtain a plot which does not give you any warnings. (The grader will know which of the possible plots give warnings, and will deduct extra marks if you assert that a plot gives no warnings when in fact it does. That is to say, if you get any warnings, you need to include those in what you hand in).\n\nSolution\nThis will probably involve a bit of digging. The starting point would be a scatterplot (the two quantitative variables) with the points distinguished by colour (one of the categorical variables). What other options do we have?\nIf you look eg. at the navigation on the left of link, you’ll see that there are two likely possibilities: shape and size. shape uses a different plotting symbol for each value of a categorical variable; size draws the plotting symbol bigger or smaller according to the value of a categorical variable.\nThe remaining question is the one of which aesthetic to “map” to which variable? To guide you in this, note that we have ten different sports and two different genders. In my opinion, colours are the easiest to distinguish a lot of, though even then 10 is pushing it, then plotting symbols. Symbol sizes are hard to distinguish a lot of, so it’s best to limit this (if you use it at all) to gender. With that in mind, I would go for this one:\n\nggplot(athletes, aes(x = Ht, y = Wt, colour = Sport, shape = Sex)) + geom_point()\n\n\n\n\nIf you investigate, you’ll find that ggplot can handle about six different shapes (and we have ten sports), and it doesn’t care for using size to distinguish values of a categorical variable. It will try to draw the graph, but will give you a warning, such as these:\n\nggplot(athletes, aes(x = Ht, y = Wt, colour = Sport, size = Sex)) + geom_point()\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\nggplot(athletes, aes(x = Ht, y = Wt, shape = Sport, size = Sex)) + geom_point()\n\nWarning: Using size for a discrete variable is not advised.\n\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because\nmore than 6 becomes difficult to discriminate; you have 10. Consider\nspecifying shapes manually if you must have them.\n\n\nWarning: Removed 72 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nggplot(athletes, aes(x = Ht, y = Wt, shape = Sport, colour = Sex)) + geom_point()\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because\nmore than 6 becomes difficult to discriminate; you have 10. Consider\nspecifying shapes manually if you must have them.\n\n\nWarning: Removed 72 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWhatever plot you come up with, you can put height and weight on the other axes, since neither one of them is uniquely a response variable.\nAny of these graphs with warnings do the job, kind of, so they get 2 points.\nOne of the many well-thought-out things about ggplot2 is that making this graph was actually not difficult once you had figured out that you also had shape and size at your disposal. (In addition, ggplot2 will help you make some aesthetic decisions about which variable is distinguished by what kind of thing. This will help you work towards a plot that doesn’t have any warnings. These warnings are pretty clear about what not to do, and by implication what you need to do instead. Colour, for example, is always safe warning-wise.)\nThe model for this kind of graph that we saw is the plot of probability (of preferring different brands of a product) as it depended on age (two quantitative variables) for each brand and gender (two categorical variables). I had three brands and two genders there, so I used colour for brand and shape for gender, but it would have worked about as well if I’d used shape for brand and colour for gender.\nThe warning message for shape contains the words ``specify shapes manually if you must have them’’. I was wondering how you can avoid that. Here’s how:\n\nggplot(athletes, aes(x = Ht, y = Wt, shape = Sport, colour = Sex)) + geom_point() +\n  scale_shape_manual(values = 1:10)\n\n\n\n\nI agree with ggplot2 that this many shapes are hard to tell apart,2 but if you can figure this out, you achieve the goal of producing a plot with no warnings, so you get full marks. (We need 10 shapes because there are 10 different sports, so we have to specify 10 different values in values=: any 10 different values will do. A list of the possible shapes is in link, at the bottom. You can even use regular letters, but you have to refer to them by numeric code:\n\nggplot(athletes, aes(x = Ht, y = Wt, shape = Sport, colour = Sex)) + geom_point() +\n  scale_shape_manual(values = c(66, 70, 71, 78, 82, 83, 52, 84, 3, 87))\n\n\n\n\nI had to find something for TSprnt, since all the letters appeared to be taken.\nOn consideration, I like this last graph the best, because the letters are more or less reminders of which sport each point is (they are “mnemonic”, so you don’t have to look them up all the time, as with colours or shapes: you can eventually remember which is which).\nWhatever you come up with, expect: 3 marks if you get a plot with no warnings, 2 marks if you get a plot with warnings, 1 mark if you get a plot that would produce warnings but you don’t indicate that there are warnings, either by saying so or by including the warnings in what you hand in.\nI didn’t ask you to interpret your plot (just getting it was enough work), but there seems to be a common theme that, for sports played by both genders, that the athletes are in about the same place relative to the other ones in their gender (eg., the basketball players are tall and heavy, the field athletes are really heavy but not very tall), but male athletes are both taller and heavier than female athletes playing the same sport. This consistency of pattern was probably why the interaction was nowhere near significant: there are additive separate effects of sport and of gender on the height-weight combination.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "manova.html#footnotes",
    "href": "manova.html#footnotes",
    "title": "30  Multivariate analysis of variance",
    "section": "",
    "text": "Like one of those crazy drinks at Starbucks.↩︎\nHaving said that, the shapes are less anbiguous than the colours, because if you’re willing to study the legend, you can work out exactly which sport a shape belongs to, whereas the colours might be hard to tell apart at all.↩︎"
  },
  {
    "objectID": "repeated-measures.html#effect-of-drug-on-rat-weight",
    "href": "repeated-measures.html#effect-of-drug-on-rat-weight",
    "title": "31  Repeated measures",
    "section": "31.1 Effect of drug on rat weight",
    "text": "31.1 Effect of drug on rat weight\nBox (1950) gives data on the weights of three groups of rats. One group was given thyroxin in their drinking water, one group thiouracil, and the third group was a control. (This description comes from Christensen (2001).)1 Weights are measured in grams at weekly intervals (over a 4-week period, so that each rat is measured 5 times). The data are in link as a .csv file.\n\nRead in the data and check that you have a column of drug and five columns of rat weights at different times.\nWhy would it be wrong to use something like pivot_longer to create one column of weights, and separate columns of drug and time, and then to run a two-way ANOVA? Explain briefly.\nCreate a suitable response variable and fit a suitable lm as the first step of the repeated-measures analysis.\nLoad the package car and run a suitable Manova. To do this, you will need to set up the right thing for idata and idesign.\nTake a look at the output from the MANOVA. Is there a significant interaction? What does its significance (or lack thereof) mean?\nWe are going to draw an interaction plot in a moment. To set that up, use pivot_longer as in the lecture notes to create one column of weights and a second column of times. (You don’t need to do the separate thing that I did in class, though if you want to try it, go ahead.)\nObtain an interaction plot. Putting time as the x will put time along the horizontal axis, which is the way we’re used to seeing such things. Begin by calculating the mean weight for each time-drug combination.\nHow does this plot show why the interaction was significant? Explain briefly."
  },
  {
    "objectID": "repeated-measures.html#social-interaction-among-old-people",
    "href": "repeated-measures.html#social-interaction-among-old-people",
    "title": "31  Repeated measures",
    "section": "31.2 Social interaction among old people",
    "text": "31.2 Social interaction among old people\nA graduate student wrote a thesis comparing different treatments for increasing social interaction among geriatric patients. He recruited 21 patients at a state mental hospital and randomly assigned them to treatments: Reality Orientation (ro), Behavior Therapy (bt) or no treatment (ctrl). Each subject was observed at four times, labelled t1 through t4 in the data file link. The response variable was the percentage of time that the subject was “engaging in the relevant social interaction”, so that a higher value is better.\nThe principal aim of the study was to see whether there were differences among the treatments (one would hope that the real treatments were better than the control one), and whether there were any patterns over time.\n\nRead in the data and display at least some of it.\nCreate a response variable and fit a suitable lm as the first stage of the repeated-measures analysis.\nRun a suitable Manova. There is some setup first. Make sure you do that.\nDisplay the results of your repeated-measures analysis. What do you conclude? Explain briefly.\nTo understand the results that you got from the repeated measures analysis, you are going to draw a picture (or two). To do that, we are going to need the data in “long” format with one response value per line (instead of four). Use pivot_longer suitably to get the data in that format, and demonstrate that you have done so.\nCalculate and save the mean interaction percents for each time-treatment combination.\nMake an interaction plot. Arrange things so that time goes across the page. Use your data frame of means that you just calculated.\nDescribe what you see on your interaction plot, and what it says about why your repeated-measures analysis came out as it did.\nDraw a spaghetti plot of these data. That is, use ggplot to plot the interaction percent against time for each subject, joining the points for the same subject by lines whose colour shows what treatment they were on. Use the “long” data frame for this (not the data frame of means)."
  },
  {
    "objectID": "repeated-measures.html#childrens-stress-levels-and-airports",
    "href": "repeated-measures.html#childrens-stress-levels-and-airports",
    "title": "31  Repeated measures",
    "section": "31.3 Children’s stress levels and airports",
    "text": "31.3 Children’s stress levels and airports\nIf you did STAC32, you might remember this question, which we can now do properly. Some of this question is a repeat from there.\nThe data in link are based on a 1998 study of stress levels in children as a result of the building of a new airport in Munich, Germany. A total of 200 children had their epinephrine levels (a stress indicator) measured at each of four different times: before the airport was built, and 6, 18 and 36 months after it was built. The four measurements are labelled epi_1 through epi_4. Out of the children, 100 were living near the new airport (location 1 in the data set), and could be expected to suffer stress because of the new airport. The other 100 children lived in the same city, but outside the noise impact zone. These children thus serve as a control group. The children are identified with numbers 1 through 200.\n\nIf we were testing for the effect of time, explain briefly what it is about the structure of the data that would make an analysis of variance inappropriate.\nRead the data into R and demonstrate that you have the right number of observations and variables.\nCreate and save a “longer” data frame with all the epinephrine values collected together into one column.\nMake a “spaghetti plot” of these data: that is, a plot of epinephrine levels against time, with the locations identified by colour, and the points for the same child joined by lines. To do this: (i) from the long data frame, create a new column containing only the numeric values of time (1 through 4), (ii) plot epinephrine level against time with the points grouped by child and coloured by location (which you may have to turn from a number into a factor.)\nWhat do you see on your spaghetti plot? We are looking ahead to possible effects of time, location and their interaction.\nThe spaghetti plot was hard to interpret because there are so many children. Calculate the mean epinephrine levels for each location-time combination, and make an interaction plot with time on the \\(x\\)-axis and location as the second factor.\nWhat do you conclude from your interaction plot? Is your conclusion clearer than from the spaghetti plot?\nRun a repeated-measures analysis of variance and display the results. Go back to your original data frame, the one you read in from the file, for this. You’ll need to make sure your numeric location gets treated as a factor.\nWhat do you conclude from the MANOVA? Is that consistent with your graphs? Explain briefly."
  },
  {
    "objectID": "repeated-measures.html#body-fat-as-repeated-measures",
    "href": "repeated-measures.html#body-fat-as-repeated-measures",
    "title": "31  Repeated measures",
    "section": "31.4 Body fat as repeated measures",
    "text": "31.4 Body fat as repeated measures\nThis one is also stolen from STAC32. Athletes are concerned with measuring their body fat percentage. Two different methods are available: one using ultrasound, and the other using X-ray technology. We are interested in whether there is a difference in the mean body fat percentage as measured by these two methods, and if so, how big that difference is. Data on 16 athletes are at link.\n\nRead in the data and check that you have a sensible number of rows and columns.\nCarry out a suitable (matched-pairs) \\(t\\)-test to determine whether the means are the same or different.\nWhat do you conclude from the test?\nRun a repeated-measures analysis of variance, treating the two methods of measuring body fat as the repeated measures (ie., playing the role of “time” that you have seen in most of the other repeated measures analyses). There is no “treatment” here, so there is nothing to go on the right side of the squiggle. Insert a 1 there to mean “just an intercept”. Display the results.\nCompare your repeated-measures analysis to your matched-pairs one. Do you draw the same conclusions?"
  },
  {
    "objectID": "repeated-measures.html#investigating-motor-activity-in-rats",
    "href": "repeated-measures.html#investigating-motor-activity-in-rats",
    "title": "31  Repeated measures",
    "section": "31.5 Investigating motor activity in rats",
    "text": "31.5 Investigating motor activity in rats\nA researcher named King was investigating the effect of the drug midazolam on motor activity in rats. Typically, the first time the drug is injected, a rat’s motor activity decreases substantially, but rats typically develop a “tolerance”, so that further injections of the drug have less impact on the rat’s motor activity.\nThe data shown in link were all taken in one day, called the “experiment day” below. 24 different rats were used. Each rat, on the experiment day, was injected with a fixed amount of midazolam, and at each of six five-minute intervals after being injected, the rat’s motor activity was measured (these are labelled i1 through i6 in the data). The rats differed in how they had been treated before the experiment day. The control group of rats had previously been injected repeatedly with a saline solution (no active ingredient), so the experiment day was the first time this group of rats had received midazolam. The other two groups of rats had both received midazolam repeatedly before the experiment day: the “same” group was injected on experiment day in the same environment that the previous injections had taken place (this is known in psychology as a “conditioned tolerance”), but the “different” group had the previous injections in a different environment than on experiment day.\nThe column id identifies the rat from which each sequence of values was obtained.\n\nExplain briefly why we need to use a repeated measures analysis for these data.\nRead in the data and note that you have what was promised in the question.\nWe are going to do a repeated-measures analysis using the “profile” method shown in class. Create a suitable response variable for this method.\nSet up the “within-subjects” part of the analysis. That means getting hold of the names of the columns that hold the different times, saving them, and also making a data frame out of them:\nFit the repeated-measures ANOVA. This will involve fitting an lm first, if you have not already done so.\nWhat do you conclude from your repeated-measures ANOVA? Explain briefly, in the context of the data.\nTo understand the results of the previous part, we are going to make a spaghetti plot. In preparation for that, we need to save the data in “long format” with one observation on one time point in each row. Arrange that, and show by displaying (some of) the data that you have done so.\nMake a spaghetti plot: that is, plot motor activity against the time points, joining the points for each rat by lines, and colouring the points and lines according to the context.\nLooking at your spaghetti plot, why do you think your repeated-measures ANOVA came out as it did? Explain briefly."
  },
  {
    "objectID": "repeated-measures.html#repeated-measures-with-no-background",
    "href": "repeated-measures.html#repeated-measures-with-no-background",
    "title": "31  Repeated measures",
    "section": "31.6 Repeated measures with no background",
    "text": "31.6 Repeated measures with no background\nNine people are randomly chosen to receive one of three treatments, labelled A, B and C. Each person has their response y to the treatment measured at three times, labelled T1, T2 and T3. The main aim of the study is to properly assess the effects of the treatments. A higher value of y is better.\nThe data are in link.\n\nThere are \\(9 \\times 3=27\\) observations of y in this study. Why would it be wrong to treat these as 27 independent observations? Explain briefly.\nRead in the data values. Are they tidy or untidy? Explain briefly. (The data values are separated by tabs, like the Australian athlete data.)\nMake a spaghetti plot: that is, a plot of y against time, with the observations for the same individual joined by lines which are coloured according to the treatment that individual received.\nOn your spaghetti plot, how do the values of y for the treatments compare over time?\nExplain briefly how the data are in the wrong format for a repeated-measures ANOVA (done using MANOVA, as in class), and use pivot_wider to get the data set into the right format.\nRun a repeated-measures ANOVA the Manova way. What do you conclude from it?\nHow is your conclusion from the previous part consistent with your spaghetti plot? Explain briefly.\n\nMy solutions follow:"
  },
  {
    "objectID": "repeated-measures.html#effect-of-drug-on-rat-weight-1",
    "href": "repeated-measures.html#effect-of-drug-on-rat-weight-1",
    "title": "31  Repeated measures",
    "section": "31.7 Effect of drug on rat weight",
    "text": "31.7 Effect of drug on rat weight\nBox (1950) gives data on the weights of three groups of rats.2 One group was given thyroxin in their drinking water, one group thiouracil, and the third group was a control. (This description comes from Christensen (2001).)3 Weights are measured in grams at weekly intervals (over a 4-week period, so that each rat is measured 5 times). The data are in link as a .csv file.\n\nRead in the data and check that you have a column of drug and five columns of rat weights at different times.\n\nSolution\nA .csv file, so read_csv. (I typed the data from Christensen (2001) into a spreadsheet.)\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/ratweight.csv\"\nweights &lt;- read_csv(my_url)\n\nRows: 27 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): drug\ndbl (6): rat, Time0, Time1, Time2, Time3, Time4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nweights\n\n\n\n  \n\n\n\nThere are 27 rats altogether, each measured five times (labelled time 0 through 4). The rats are also labelled rat (the first column), which will be useful later.\n\\(\\blacksquare\\)\n\nWhy would it be wrong to use something like pivot_longer to create one column of weights, and separate columns of drug and time, and then to run a two-way ANOVA? Explain briefly.\n\nSolution\nSuch a solution would assume that we have measurements on different rats, one for each drug-time combination. But we have sets of five measurements all on the same rat: that is to say, we have repeated measures, and the proper analysis will take that into account.\n\\(\\blacksquare\\)\n\nCreate a suitable response variable and fit a suitable lm as the first step of the repeated-measures analysis.\n\nSolution\nThe response variable is the same idea as for any MANOVA: just glue the columns together:\n\nresponse &lt;- with(weights, cbind(Time0, Time1, Time2, Time3, Time4))\nweights.1 &lt;- lm(response ~ drug, data = weights)\n\n\nweights %&gt;% select(starts_with(\"Time\")) %&gt;% \n  as.matrix() -&gt; y\n\nNow, we don’t look at weights.1, but we do use it as input to Manova in a moment.\n\\(\\blacksquare\\)\n\nLoad the package car and run a suitable Manova. To do this, you will need to set up the right thing for idata and idesign.\n\nSolution\nSomething like this:\n\ntimes &lt;- colnames(response)\ntimes\n\n[1] \"Time0\" \"Time1\" \"Time2\" \"Time3\" \"Time4\"\n\ntimes.df &lt;- data.frame(times=factor(times))\ntimes.df\n\n\n\n  \n\n\nweights.2 &lt;- Manova(weights.1, idata = times.df, idesign = ~times)\n\nThe thought process is that the columns of the response (Time.0 through Time.4) are all times. This is the “within-subject design” part of it: within a rat, the different response values are at different times. That’s the only part of it that is within subjects. The different drugs are a “between-subjects” factor: each rat only gets one of the drugs.4\n\\(\\blacksquare\\)\n\nTake a look at all the output from the MANOVA. Is there a significant interaction? What does its significance (or lack thereof) mean?\n\nSolution\nLook at the summary, which is rather long:\n\nsummary(weights.2)\n\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nTime0           1\nTime1           1\nTime2           1\nTime3           1\nTime4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     6875579\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1   0.99257 3204.089      1     24 &lt; 2.22e-16 ***\nWilks             1   0.00743 3204.089      1     24 &lt; 2.22e-16 ***\nHotelling-Lawley  1 133.50372 3204.089      1     24 &lt; 2.22e-16 ***\nRoy               1 133.50372 3204.089      1     24 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug \n\n Response transformation matrix:\n      (Intercept)\nTime0           1\nTime1           1\nTime2           1\nTime3           1\nTime4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    33193.27\n\nMultivariate Tests: drug\n                 Df test stat approx F num Df den Df    Pr(&gt;F)   \nPillai            2 0.3919186 7.734199      2     24 0.0025559 **\nWilks             2 0.6080814 7.734199      2     24 0.0025559 **\nHotelling-Lawley  2 0.6445166 7.734199      2     24 0.0025559 **\nRoy               2 0.6445166 7.734199      2     24 0.0025559 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3 times4\nTime0      1      0      0      0\nTime1      0      1      0      0\nTime2      0      0      1      0\nTime3      0      0      0      1\nTime4     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1 times2    times3   times4\ntimes1 235200.00 178920 116106.67 62906.67\ntimes2 178920.00 136107  88324.00 47854.00\ntimes3 116106.67  88324  57316.15 31053.93\ntimes4  62906.67  47854  31053.93 16825.04\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1   0.98265 297.3643      4     21 &lt; 2.22e-16 ***\nWilks             1   0.01735 297.3643      4     21 &lt; 2.22e-16 ***\nHotelling-Lawley  1  56.64082 297.3643      4     21 &lt; 2.22e-16 ***\nRoy               1  56.64082 297.3643      4     21 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug:times \n\n Response transformation matrix:\n      times1 times2 times3 times4\nTime0      1      0      0      0\nTime1      0      1      0      0\nTime2      0      0      1      0\nTime3      0      0      0      1\nTime4     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n         times1   times2   times3   times4\ntimes1 9192.071 8948.843 6864.676 3494.448\ntimes2 8948.843 8787.286 6740.286 3381.529\ntimes3 6864.676 6740.286 5170.138 2594.103\ntimes4 3494.448 3381.529 2594.103 1334.006\n\nMultivariate Tests: drug:times\n                 Df test stat  approx F num Df den Df     Pr(&gt;F)    \nPillai            2 0.8779119  4.303151      8     44 0.00069308 ***\nWilks             2 0.2654858  4.939166      8     42 0.00023947 ***\nHotelling-Lawley  2 2.2265461  5.566365      8     40 9.3465e-05 ***\nRoy               2 1.9494810 10.722146      4     22 5.6277e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n             Sum Sq num Df Error SS den Df   F value    Pr(&gt;F)    \n(Intercept) 1375116      1  10300.2     24 3204.0892 &lt; 2.2e-16 ***\ndrug           6639      2  10300.2     24    7.7342  0.002556 ** \ntimes        146292      4   4940.7     96  710.6306 &lt; 2.2e-16 ***\ndrug:times     6777      8   4940.7     96   16.4606 4.185e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n           Test statistic   p-value\ntimes           0.0072565 1.781e-19\ndrug:times      0.0072565 1.781e-19\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n            GG eps Pr(&gt;F[GG])    \ntimes      0.33165  &lt; 2.2e-16 ***\ndrug:times 0.33165  2.539e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n              HF eps   Pr(&gt;F[HF])\ntimes      0.3436277 9.831975e-26\ndrug:times 0.3436277 1.757377e-06\n\n\nStart near the bottom with Mauchly’s test. This is strongly significant (for the interaction, which is our focus here) and means that sphericity fails and the P-value for the interaction in the univariate test is not to be trusted (it is much too small). Look instead at the Huynh-Feldt adjusted P-value at the very bottom, \\(1.76 \\times 10^{-6}\\). This is strongly significant still, but it is a billion times bigger than the one in the univariate table! For comparison, the test for interaction in the multivariate analysis has a P-value of 0.0007 or less, depending on which of the four tests you look at (this time, they are not all the same). As usual, the multivariate tests have bigger P-values than the appropriately adjusted univariate tests, but the P-values are all pointing in the same direction.\nThe significant interaction means that the effect of time on growth is different for the different drugs: that is, the effect of drug is over the whole time profile, not just something like “a rat on Thyroxin is on average 10 grams heavier than a control rat, over all times”.\nSince the interaction is significant, that’s where we stop, as far as interpretation is concerned.\n\\(\\blacksquare\\)\n\nWe are going to draw an interaction plot in a moment. To set that up, use pivot_longer as in the lecture notes to create one column of weights and a second column of times. (You don’t need to do the separate thing that I did in class, though if you want to try it, go ahead.)\n\nSolution\nLike this:\n\nweights %&gt;% \n  pivot_longer(starts_with(\"Time\"), names_to=\"time\", values_to=\"weight\") -&gt; weights.long\nweights.long\n\n\n\n  \n\n\n\nMy data frame was called weights, so I was OK with having a variable called weight. Watch out for that if you call the data frame weight, though.\nSince the piece of the time we want is the number, parse_number (from readr, part of the tidyverse) should also work:\n\nweights %&gt;% \n  pivot_longer(starts_with(\"Time\"), names_to=\"timex\", values_to=\"weight\") %&gt;% \n  mutate(time = parse_number(timex)) -&gt; weights2.long\nweights2.long %&gt;% sample_n(20)\n\n\n\n  \n\n\n\nI decided to show you a random collection of rows, so that you can see that parse_number worked for various different times.\n\\(\\blacksquare\\)\n\nObtain an interaction plot. Putting time as the x will put time along the horizontal axis, which is the way we’re used to seeing such things. Begin by calculating the mean weight for each time-drug combination.\n\nSolution\ngroup_by, summarize and ggplot, the latter using the data frame that came out of the summarize. The second factor drug goes as the colour and group both, since time has grabbed the x spot:\n\nweights.long %&gt;%\n  group_by(time, drug) %&gt;%\n  summarize(mean.weight = mean(weight)) %&gt;%\n  ggplot(aes(x = time, y = mean.weight, colour = drug, group = drug)) +\n  geom_point() + geom_line()\n\n`summarise()` has grouped output by 'time'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\\(\\blacksquare\\)\n\nHow does this plot show why the interaction was significant? Explain briefly.\n\nSolution\nAt the beginning, all the rats have the same average growth, but from time 2 (or maybe even 1) or so, the rats on thiouracil grew more slowly. The idea is not just that thiouracil has a constant effect over all times, but that the pattern of growth is different for the different drugs: whether or not thiouracil inhibits growth, and, if so, by how much, depends on what time point you are looking at.\nRats on thyroxin or the control drug grew at pretty much the same rate over all times, so I wouldn’t concern myself with any differences there.\nWhat I thought would be interesting is to plot the growth curves for all the rats individually, colour-coded by which drug the rat was on. This is the repeated-measures version of the ANOVA interaction plot with the data on it, a so-called spaghetti plot. (We don’t use the lines for the means, here, instead using them for joining the measurements belonging to the same subject.)\nWhen I first used this data set, it didn’t have a column identifying which rat was which, which made this plot awkward, but now it does (the column rat). So we can start directly from the dataframe I created above called weights.long:\n\nweights.long\n\n\n\n  \n\n\n\nEach rat is identified by rat``, which repeats 5 times, once for each value oftime`:\n\nweights.long %&gt;% count(rat)\n\n\n\n  \n\n\n\nIn the data frame weights.long, we plot time (\\(x\\)) against weight (\\(y\\)), grouping the points according to rat and colouring them according to drug.\n\nlibrary(ggplot2)\nggplot(weights.long, aes(time, weight, group = rat, colour = drug)) + geom_line()\n\n\n\n\nAs you see, “spaghetti plot” is a rather apt name for this kind of thing.\nI like this plot because, unlike the interaction plot, which shows only means, this gives a sense of variability as well. The blue and red lines (thyroxin and control) are all intermingled and they go straight up. So there is nothing to choose between these. The green lines, though, start off mixed up with the red and blue ones but finish up at the bottom: the pattern of growth of the thiouracil rats is different from the others, which is why we had a significant interaction between drug and time.\ndrug is categorical, so ggplot uses a set of distinguishable colours to mark the levels. If our colour had been a numerical variable, ggplot would have used a range of colours like light blue to dark blue, with lighter being higher, for example.\nWhat, you want to see that? All right. This one is kind of silly, but you see the point:\n\nggplot(weights.long, aes(time, weight, group = rat, colour = weight)) + geom_line()\n\n\n\n\nThe line segments get lighter as you go up the page.\nSince we went to the trouble of making the “long” data frame, we can also run a repeated measures analysis using the mixed-model idea (described more fully in the problem of the children near the new airport):\n\nwt.1 &lt;- lmer(weight ~ drug * time + (1 | rat), data = weights.long)\ndrop1(wt.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe drug-by-time interaction is even more strongly significant than in the profile analysis. (The output from drop1 reminds us that the only thing we should be thinking about now is that interaction.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "repeated-measures.html#social-interaction-among-old-people-1",
    "href": "repeated-measures.html#social-interaction-among-old-people-1",
    "title": "31  Repeated measures",
    "section": "31.8 Social interaction among old people",
    "text": "31.8 Social interaction among old people\nA graduate student wrote a thesis comparing different treatments for increasing social interaction among geriatric patients. He recruited 21 patients at a state mental hospital and randomly assigned them to treatments: Reality Orientation (ro), Behavior Therapy (bt) or no treatment (ctrl). Each subject was observed at four times, labelled t1 through t4 in the data file link. The response variable was the percentage of time that the subject was “engaging in the relevant social interaction”, so that a higher value is better.\nThe principal aim of the study was to see whether there were differences among the treatments (one would hope that the real treatments were better than the control one), and whether there were any patterns over time.\n\nRead in the data and display at least some of it.\n\nSolution\nThe usual, separated by a single space:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/geriatrics.txt\"\ngeriatrics &lt;- read_delim(my_url, \" \")\n\nRows: 21 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): treatment\ndbl (5): subject, t1, t2, t3, t4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngeriatrics\n\n\n\n  \n\n\n\nCorrectly 21 observations measured at 4 different times. We also have subject numbers, which might be useful later.\n\\(\\blacksquare\\)\n\nCreate a response variable and fit a suitable lm as the first stage of the repeated-measures analysis.\n\nSolution\nThis:\n\nresponse &lt;- with(geriatrics, cbind(t1, t2, t3, t4))\ngeriatrics.1 &lt;- lm(response ~ treatment, data = geriatrics)\n\nThere is no need to look at this, since we are going to feed it into Manova in a moment, but in case you’re curious, you see (in summary) a regression of each of the four columns in response on treatment, one by one.\n\\(\\blacksquare\\)\n\nRun a suitable Manova. There is some setup first. Make sure you do that.\n\nSolution\nMake sure car is loaded, and do the idata and idesign thing:\n\ntimes &lt;- colnames(response)\ntimes.df &lt;- data.frame(times=factor(times))\ngeriatrics.2 &lt;- Manova(geriatrics.1, idata = times.df, idesign = ~times)\n\nIn case you’re curious, response is an R matrix:\n\nclass(response)\n\n[1] \"matrix\" \"array\" \n\n\nand not a data frame (because it was created by cbind which makes a matrix out of vectors). So, to pull the names off the top, we really do need colnames (applied to a matrix) rather than just names (which applies to a data frame).\n\\(\\blacksquare\\)\n\nDisplay the results of your repeated-measures analysis. What do you conclude? Explain briefly.\n\nSolution\nIts summary will get you what you want:\n\nsummary(geriatrics.2)\n\nWarning in summary.Anova.mlm(geriatrics.2): HF eps &gt; 1 treated as 1\n\n\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    3286.252\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1 0.7458921 52.83606      1     18 9.3318e-07 ***\nWilks             1 0.2541079 52.83606      1     18 9.3318e-07 ***\nHotelling-Lawley  1 2.9353366 52.83606      1     18 9.3318e-07 ***\nRoy               1 2.9353366 52.83606      1     18 9.3318e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    360.6695\n\nMultivariate Tests: treatment\n                 Df test stat approx F num Df den Df   Pr(&gt;F)  \nPillai            2 0.2436597 2.899406      2     18 0.080994 .\nWilks             2 0.7563403 2.899406      2     18 0.080994 .\nHotelling-Lawley  2 0.3221562 2.899406      2     18 0.080994 .\nRoy               2 0.3221562 2.899406      2     18 0.080994 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1     times2    times3\ntimes1  0.5833333  -8.366667 -1.666667\ntimes2 -8.3666667 120.001905 23.904762\ntimes3 -1.6666667  23.904762  4.761905\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df   Pr(&gt;F)    \nPillai            1 0.7214276  13.8119      3     16 0.000105 ***\nWilks             1 0.2785724  13.8119      3     16 0.000105 ***\nHotelling-Lawley  1 2.5897315  13.8119      3     16 0.000105 ***\nRoy               1 2.5897315  13.8119      3     16 0.000105 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment:times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1    times2    times3\ntimes1   8.166667 -27.33333 -4.933333\ntimes2 -27.333333  91.61524 17.569524\ntimes3  -4.933333  17.56952 11.443810\n\nMultivariate Tests: treatment:times\n                 Df test stat  approx F num Df den Df     Pr(&gt;F)    \nPillai            2 0.9258067  4.883886      6     34 0.00107288 ** \nWilks             2 0.2190296  6.062534      6     32 0.00025426 ***\nHotelling-Lawley  2 2.9043306  7.260827      6     30 7.4555e-05 ***\nRoy               2 2.6552949 15.046671      3     17 4.8948e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                Sum Sq num Df Error SS den Df F value    Pr(&gt;F)    \n(Intercept)     821.56      1   279.89     18 52.8361 9.332e-07 ***\ntreatment        90.17      2   279.89     18  2.8994   0.08099 .  \ntimes            87.07      3    72.25     54 21.6933 2.378e-09 ***\ntreatment:times  90.77      6    72.25     54 11.3067 3.827e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                Test statistic p-value\ntimes                  0.85209 0.75008\ntreatment:times        0.85209 0.75008\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                 GG eps Pr(&gt;F[GG])    \ntimes           0.90848  1.108e-08 ***\ntreatment:times 0.90848  1.434e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                  HF eps   Pr(&gt;F[HF])\ntimes           1.086414 2.377839e-09\ntreatment:times 1.086414 3.826914e-08\n\n\nAs is the way, start at the bottom and go up to Mauchly’s test for sphericity. No problem here, so you can use the P-value for interaction on the univariate test as is ($3.8 ^{-8}). By way of comparison, the Huynh-Feldt adjusted P-value is exactly the same (not actually adjusted at all), which makes sense because there was no lack of sphericity. The multivariate tests for the interaction have P-values that vary, but they are all (i) a bit bigger than the univariate one, and (ii) still significant.\nThus, the interaction is significant, so the effects of the treatments are different at different times. (It makes most sense to say it this way around, since treatment is something that was controlled and time was not.)\nYou, I hope, know better than to look at the main effects when there is a significant interaction!\n\\(\\blacksquare\\)\n\nTo understand the results that you got from the repeated measures analysis, you are going to draw a picture (or two). To do that, we are going to need the data in “long” format with one response value per line (instead of four). Use pivot_longer suitably to get the data in that format, and demonstrate that you have done so.\n\nSolution\nThe usual layout:\n\ngeriatrics %&gt;% \n  pivot_longer(t1:t4, names_to=\"time\", values_to = \"intpct\") -&gt; geriatrics.long\ngeriatrics.long\n\n\n\n  \n\n\n\nI have one column of interaction percents, and one column of times. If you check the whole thing, you’ll see that pivot_longer gives all the measurements for subject 1, then subject 2, and so on.\nThe long data frame is, well, long.\nIt’s not necessary to pull out the numeric time values, though you could if you wanted to, by using parse_number.\n\\(\\blacksquare\\)\n\nCalculate and save the mean interaction percents for each time-treatment combination.\n\nSolution\ngroup_by followed by summarize, as ever:\n\ngeriatrics.long %&gt;%\n  group_by(treatment, time) %&gt;%\n  summarize(mean = mean(intpct)) -&gt; means\n\n`summarise()` has grouped output by 'treatment'. You can override using the\n`.groups` argument.\n\nmeans\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nMake an interaction plot. Arrange things so that time goes across the page. Use your data frame of means that you just calculated.\n\nSolution\nOnce you have the means, this is not too bad:\n\nggplot(means, aes(x = time, y = mean, group = treatment, colour = treatment)) +\n  geom_point() + geom_line()\n\n\n\n\nThe “second factor” treatment appears as both group and colour.\n\\(\\blacksquare\\)\n\nDescribe what you see on your interaction plot, and what it says about why your repeated-measures analysis came out as it did.\n\nSolution\nThe two “real” treatments bt and ro both go up sharply between time 1 and time 2, and then come back down so that by time 4 they are about where they started. The control group basically didn’t change at all, and if anything went down between times 1 and 2, a completely different pattern to the others. The two treatments bt and ro are not exactly parallel, but they do at least have qualitatively the same pattern.5 It is, I think, the fact that the control group has a completely different pattern over time that makes the interaction come out significant.6 I’m going to explore that some more later, but first I want to get you to draw a spaghetti plot.\n\\(\\blacksquare\\)\n\nDraw a spaghetti plot of these data. That is, use ggplot to plot the interaction percent against time for each subject, joining the points for the same subject by lines whose colour shows what treatment they were on. Use the “long” data frame for this (not the data frame of means).\n\nSolution\nThis is almost easier to do than it is to ask you to do:\n\nggplot(geriatrics.long, aes(x = time, y = intpct, colour = treatment, group = subject)) +\n  geom_line()\n\n\n\n\nThe basic difficulty here is to get all the parts. We need both a colour and a group; the latter controls the joining of points by lines (if you have both). Fortunately we already had subject numbers in the original data; if we had not had them, we would have had to create them. dplyr has a function row_number that we could have used for that; we’d apply the row numbers to the original wide data frame, before we made it long, so that the correct subject numbers would get carried along.\nWhether you add a geom_point() to plot the data points, or not, is up to you. Logically, it makes sense to include the actual data, but aesthetically, it looks more like spaghetti if you leave the points out. Either way is good, as far as I’m concerned.\nI didn’t ask you to comment on the spaghetti plot, because the story is much the same as the interaction plot. There is a lot of variability, but the story within each group is basically what we already said: the red lines go sharply up and almost as sharply back down again, the blue lines do something similar, only not as sharply up and down, and the green lines do basically nothing.\nI said that the control subjects’ time pattern was noticeably different from the others. Which made me think: what if we remove the control subjects? Would there still be an interaction?7\nAll right, we need to start with the original wide data frame, and from that select everything but ctrl:\n\ngg &lt;- geriatrics %&gt;% filter(treatment != \"ctrl\")\ngg\n\n\n\n  \n\n\n\nSo now there are two treatments left, seven people on each:8\n\ngg %&gt;% count(treatment)\n\n\n\n  \n\n\n\nThen we do the same stuff over again: construct the response, run the lm, create the stuff for idata and idesign, and run the Manova. There’s really nothing new here:\n\nresponse &lt;- with(gg, cbind(t1, t2, t3, t4))\ngg.1 &lt;- lm(response ~ treatment, data = gg)\ntimes &lt;- colnames(response)\ntimes.df &lt;- data.frame(times=factor(times))\ngg.2 &lt;- Manova(gg.1, idata = times.df, idesign = ~times)\nsummary(gg.2)\n\nWarning in summary.Anova.mlm(gg.2): HF eps &gt; 1 treated as 1\n\n\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    2920.346\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1  0.765026 39.06941      1     12 4.2509e-05 ***\nWilks             1  0.234974 39.06941      1     12 4.2509e-05 ***\nHotelling-Lawley  1  3.255785 39.06941      1     12 4.2509e-05 ***\nRoy               1  3.255785 39.06941      1     12 4.2509e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    203.6829\n\nMultivariate Tests: treatment\n                 Df test stat approx F num Df den Df Pr(&gt;F)\nPillai            1 0.1850562 2.724941      1     12 0.1247\nWilks             1 0.8149438 2.724941      1     12 0.1247\nHotelling-Lawley  1 0.2270784 2.724941      1     12 0.1247\nRoy               1 0.2270784 2.724941      1     12 0.1247\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1    times2   times3\ntimes1    3.5 -24.80000 -6.80000\ntimes2  -24.8 175.72571 48.18286\ntimes3   -6.8  48.18286 13.21143\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1  0.817303 14.91185      3     10 0.00050726 ***\nWilks             1  0.182697 14.91185      3     10 0.00050726 ***\nHotelling-Lawley  1  4.473555 14.91185      3     10 0.00050726 ***\nRoy               1  4.473555 14.91185      3     10 0.00050726 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment:times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1 times2    times3\ntimes1    3.5 -11.20  2.000000\ntimes2  -11.2  35.84 -6.400000\ntimes3    2.0  -6.40  1.142857\n\nMultivariate Tests: treatment:times\n                 Df test stat approx F num Df den Df   Pr(&gt;F)  \nPillai            1 0.5816827 4.635099      3     10 0.027959 *\nWilks             1 0.4183173 4.635099      3     10 0.027959 *\nHotelling-Lawley  1 1.3905298 4.635099      3     10 0.027959 *\nRoy               1 1.3905298 4.635099      3     10 0.027959 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                Sum Sq num Df Error SS den Df F value    Pr(&gt;F)    \n(Intercept)     730.09      1  224.243     12 39.0694 4.251e-05 ***\ntreatment        50.92      1  224.243     12  2.7249 0.1247005    \ntimes           136.04      3   60.551     36 26.9595 2.560e-09 ***\ntreatment:times  38.16      3   60.551     36  7.5629 0.0004777 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                Test statistic p-value\ntimes                  0.66019 0.48791\ntreatment:times        0.66019 0.48791\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                 GG eps Pr(&gt;F[GG])    \ntimes           0.82418  5.012e-08 ***\ntreatment:times 0.82418   0.001217 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                HF eps   Pr(&gt;F[HF])\ntimes           1.0546 2.560037e-09\ntreatment:times 1.0546 4.777491e-04\n\n\nThe procedure, as before: Mauchly’s test is not significant, so you can look at the univariate test for interaction. For comparison, the P-values for interaction in the multivariate test (all the same) are quite a bit bigger but still significant.\nThere is still an interaction, but it’s not as significant as it was before. I think it is still significant because the shape of the two time trends is not the same: the red bt group goes up further and down further. I was musing that the higher values are also more variable, which would suggest a transformation, but I haven’t explored that.\nIf the interaction had turned out to be nonsignificant this way? You might think about trying to remove it from the model, except that in this kind of model, treatment is a “between-subjects factor” and times is a “within-subjects factor”, so they are different kinds of things. What you do in that case is to ignore the non-significant interaction and interpret the main effects: there is no way to “gain df for error” like we did in two-way ANOVA.\nSupposing, in this case, that we were using \\(\\alpha=0.01\\), we would say that the interaction is not significant. Then we look at the main effects: there is no effect of treatment, but there is an effect of time. Or, to put it another way, once you allow for an effect of time, there is no difference between the two remaining treatments.9\nThinking back to our spaghetti plot, we are now comparing the red and blue treatments. They both go up at time 2 and down afterwards, which is the time effect, but even once you allow for this time trend, there is too much scatter to be able to infer a difference between the treatments.\nExtra (maybe I could branch off into another question sometime?) I was thinking that this is not terribly clear, so I thought I would fake up some data where there is a treatment effect and a time effect (but no interaction), and draw a spaghetti plot, so you can see the difference, idealized somewhat of course. Let’s try to come up with something with the same kind of time effect, up at time 2 and then down afterwards, that is the same for two drugs A and B. Here’s what I came up with:\n\nfake &lt;- read.csv(\"fake.csv\", header = T)\nfake\n\n\n\n  \n\n\n\nYou can kind of get the idea that the pattern over time is up and then down, so that it finishes about where it starts, but the numbers for drug A are usually bigger than the ones for drug B, consistently over time. So there ought not to be an interaction, but there ought to be both a time effect and a drug effect.\nLet’s see whether we can demonstrate that. First, a spaghetti plot, which involves getting the data in long format first. I’m saving the long format to use again later.\n\nfake %&gt;% \n  pivot_longer(t1:t4, names_to=\"times\", values_to=\"score\") -&gt; fake.long\nfake.long %&gt;%\n  ggplot(aes(x = times, y = score, colour = drug, group = subject)) +\n  geom_point() + geom_line()\n\n\n\n\nThe reds are consistently higher than the blues (drug effect), the pattern over time goes up and then down (time effect), but the time effect is basically the same for both drugs (no interaction).\nI got the plot wrong the first time, because I forgot whether I was doing an interaction plot (where group= and colour= are the same) or a spaghetti plot (where group has to be subject and the colour represents the treatment, usually).\nLet’s do the repeated-measures ANOVA and see whether my guess above is right:\n\nresponse &lt;- with(fake, cbind(t1, t2, t3, t4))\nfake.1 &lt;- lm(response ~ drug, data = fake)\ntimes &lt;- colnames(response)\ntimes.df &lt;- data.frame(times=factor(times))\nfake.2 &lt;- Manova(fake.1, idata = times.df, idesign = ~times)\n\nAfter typing this kind of stuff out a few too many times, I hope you’re getting the idea “function”. Also, the construction of the response is kind of annoying, where you have to list all the time columns. The trouble is, response has to be a matrix, which it is:\n\nclass(response)\n\n[1] \"matrix\" \"array\" \n\n\nbut if you do the obvious thing of selecting the columns of the data frame that you want:\n\nfake %&gt;% select(t1:t4) -&gt; r\nclass(r)\n\n[1] \"data.frame\"\n\n\nyou get a data frame instead. I think this would work:\n\nr &lt;- fake %&gt;% select(t1:t4) %&gt;% as.matrix()\nclass(r)\n\n[1] \"matrix\" \"array\" \n\n\nThe idea is that you select the columns you want as a data frame first (with select), and then turn it into a matrix at the end.\nThis is the kind of thing you’d have to do in a function, I think, since you’d have to have some way of telling the function which are the “time” columns. Anyway, hope you haven’t forgotten what we were doing:10\n\nsummary(fake.2)\n\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     15920.1\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1   0.98478 517.7268      1      8 1.4752e-08 ***\nWilks             1   0.01522 517.7268      1      8 1.4752e-08 ***\nHotelling-Lawley  1  64.71585 517.7268      1      8 1.4752e-08 ***\nRoy               1  64.71585 517.7268      1      8 1.4752e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)       532.9\n\nMultivariate Tests: drug\n                 Df test stat approx F num Df den Df    Pr(&gt;F)   \nPillai            1   0.68417 17.33008      1      8 0.0031525 **\nWilks             1   0.31583 17.33008      1      8 0.0031525 **\nHotelling-Lawley  1   2.16626 17.33008      1      8 0.0031525 **\nRoy               1   2.16626 17.33008      1      8 0.0031525 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1 times2 times3\ntimes1    1.6   15.2    6.8\ntimes2   15.2  144.4   64.6\ntimes3    6.8   64.6   28.9\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1   0.98778 161.7086      3      6 3.9703e-06 ***\nWilks             1   0.01222 161.7086      3      6 3.9703e-06 ***\nHotelling-Lawley  1  80.85428 161.7086      3      6 3.9703e-06 ***\nRoy               1  80.85428 161.7086      3      6 3.9703e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug:times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1 times2 times3\ntimes1    0.4    0.8   -0.2\ntimes2    0.8    1.6   -0.4\ntimes3   -0.2   -0.4    0.1\n\nMultivariate Tests: drug:times\n                 Df test stat approx F num Df den Df   Pr(&gt;F)  \nPillai            1 0.6490046  3.69808      3      6 0.081108 .\nWilks             1 0.3509954  3.69808      3      6 0.081108 .\nHotelling-Lawley  1 1.8490401  3.69808      3      6 0.081108 .\nRoy               1 1.8490401  3.69808      3      6 0.081108 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n            Sum Sq num Df Error SS den Df  F value    Pr(&gt;F)    \n(Intercept) 3980.0      1     61.5      8 517.7268 1.475e-08 ***\ndrug         133.2      1     61.5      8  17.3301  0.003152 ** \ntimes         87.9      3     14.9     24  47.1812 3.233e-10 ***\ndrug:times     1.5      3     14.9     24   0.7919  0.510323    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n           Test statistic p-value\ntimes             0.18708 0.04852\ndrug:times        0.18708 0.04852\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n            GG eps Pr(&gt;F[GG])    \ntimes      0.54943  1.886e-06 ***\ndrug:times 0.54943     0.4505    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n              HF eps   Pr(&gt;F[HF])\ntimes      0.6735592 1.708486e-07\ndrug:times 0.6735592 4.709652e-01\n\n\nThe usual procedure: check for sphericity first. Here, that is just rejected, but since the P-value on the sphericity test is only just less than 0.05, you would expect the P-values on the univariate test for interaction and the Huynh-Feldt adjustment to be similar, and they are (0.510 and 0.471 respectively). Scrolling up a bit further, the multivariate test for interaction only just fails to be significant, with a P-value of 0.081. It is a mild concern that this one differs so much from the others; normally the multivariate test(s) would tell a similar story to the others.\nThe drug-by-time interaction is not significant, so we go ahead and interpret the main effects: there is a time effect (the increase at time 2 that I put in on purpose), and, allowing for the time effect, there is also a difference between the drugs (because the drug A scores are a bit higher than the drug B scores). The procedure is to look at the Huynh-Feldt adjusted P-value for time (\\(1.71 \\times 10^{-7}\\)), expecting it to be a bit bigger than the one in the univariate table (it is) and comparable to the one for time in the appropriate multivariate analysis (\\(3.97 \\times 10^{-6}\\); it is, but remember to scroll back enough). In this kind of analysis, the effect of drug is averaged over time,11 so the test for the drug main effect is unaffected by sphericity. Its P-value, 0.0032, is identical in the univariate and multivariate tables, and you see that the drug main effect is not part of the sphericity testing.\nWhat if we ignored the time effect? You’d think we could do something like this, treating the measurements at different times as replicates:\n\nhead(fake.long)\n\n\n\n  \n\n\nfake.3 &lt;- aov(score ~ drug, data = fake.long)\nsummary(fake.3)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndrug         1  133.2  133.22   30.54 2.54e-06 ***\nResiduals   38  165.8    4.36                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nbut this would be wrong, because we are acting as if we have 40 independent observations, which we don’t (this is the point of doing repeated measures in the first place). It looks as if we have achieved something by getting a lower P-value for drug, but we haven’t really, because we have done so by cheating.\nWhat we could do instead is to average the scores for each subject over all the times,12 for which we go back to the original data frame:\n\nfake\n\n\n\n  \n\n\nfake %&gt;%\n  mutate(avg.score = (t1 + t2 + t3 + t4) / 4) %&gt;%\n  aov(avg.score ~ drug, data = .) %&gt;%\n  summary()\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndrug         1  33.31   33.31   17.33 0.00315 **\nResiduals    8  15.37    1.92                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAh, now, this is very interesting. I was hoping that by throwing away the time information (which is useful), we would have diminished the significance of the drug effect. By failing to include the time-dependence in our model, we ought to have introduced some extra variability, which ought to weaken our test. But this test gives exactly the same P-value as the ones in the MANOVA, and it looks like exactly the same test (the \\(F\\)-value is the same too). So it looks as if this is what the MANOVA is doing, to assess the drug effect: it’s averaging over the times. Since the same four (here) time points are being used to compute the average for each subject, we are comparing like with like at least, and even if there is a large time effect, I suppose it’s going to have the same effect on each average. For example, if as here the scores at time 2 are typically highest, all the averages are going to be composed of one high score and three lower ones. So maybe I have to go back and dilute my conclusions about the significance of treatments earlier: it’s actually saying that there is a difference between the two remaining treatments averaged over time rather than allowing for time as I said earlier.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "repeated-measures.html#childrens-stress-levels-and-airports-1",
    "href": "repeated-measures.html#childrens-stress-levels-and-airports-1",
    "title": "31  Repeated measures",
    "section": "31.9 Children’s stress levels and airports",
    "text": "31.9 Children’s stress levels and airports\nIf you did STAC32, you might remember this question, which we can now do properly. Some of this question is a repeat from there.\nThe data in link are based on a 1998 study of stress levels in children as a result of the building of a new airport in Munich, Germany. A total of 200 children had their epinephrine levels (a stress indicator) measured at each of four different times: before the airport was built, and 6, 18 and 36 months after it was built. The four measurements are labelled epi_1 through epi_4. Out of the children, 100 were living near the new airport (location 1 in the data set), and could be expected to suffer stress because of the new airport. The other 100 children lived in the same city, but outside the noise impact zone. These children thus serve as a control group. The children are identified with numbers 1 through 200.\n\nIf we were testing for the effect of time, explain briefly what it is about the structure of the data that would make an analysis of variance inappropriate.\n\nSolution\nIt is the fact that each child was measured four times, rather than each measurement being on a different child (with thus \\(4\\times 200=800\\) observations altogether). It’s the same distinction as between matched pairs and a two-sample \\(t\\) test.\n\\(\\blacksquare\\)\n\nRead the data into R and demonstrate that you have the right number of observations and variables.\n\nSolution\nThe usual, data values separated by one space:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/airport.txt\"\nairport &lt;- read_delim(my_url, \" \")\n\nRows: 200 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (6): epi_1, epi_2, epi_3, epi_4, location, child\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nairport\n\n\n\n  \n\n\n\nThere are 200 rows (children), with four epi measurements, a location and a child identifier, so that looks good.\n(I am mildly concerned about the negative epi measurements, but I don’t know what the scale is, so presumably they are all right. Possibly epinephrine is measured on a log scale, so that a negative value here is less than 1 on the original scale that we don’t see.)\n\\(\\blacksquare\\)\n\nCreate and save a “longer” data frame with all the epinephrine values collected together into one column.\n\nSolution\npivot_longer:\n\nairport %&gt;% pivot_longer(starts_with(\"epi\"), names_to=\"when\", values_to=\"epinephrine\") -&gt; airport.long\nairport.long\n\n\n\n  \n\n\n\nSuccess. I’m saving the name time for later, so I’ve called the time points when for now. There were 4 measurements on each of 200 children, so the long data frame should (and does) have \\(200\\times 4 = 800\\) rows.\n\\(\\blacksquare\\)\n\nMake a “spaghetti plot” of these data: that is, a plot of epinephrine levels against time, with the locations identified by colour, and the points for the same child joined by lines. To do this: (i) from the long data frame, create a new column containing only the numeric values of time (1 through 4), (ii) plot epinephrine level against time with the points grouped by child and coloured by location (which you may have to turn from a number into a factor.)\n\nSolution\nNote the use of the different things for colour and group, as usual for a spaghetti plot. Also, note that the locations are identified by number, but the number is only a label, and we want to use different colours for the different locations, so we need to turn location into a factor for this.\n\nairport.long %&gt;%\n  mutate(time = parse_number(when)) %&gt;%\n  ggplot(aes(x = time, y = epinephrine, colour = factor(location), group = child)) +\n  geom_point() + geom_line()\n\n\n\n\nThis13 is different from the plot we had in C32, where I had you use a different colour for each child, and we ended up with a huge legend of all the children (which we then removed).\nIf you forget to turn location into a factor, ggplot will assume that you want location to be on a continuous scale, and you’ll get two shades of blue.\nAnother problem with this plot is that there are so many children, you can’t see the ones underneath because the ones on top are overwriting them. The solution to that is to make the lines (partly) transparent, which is controlled by a parameter alpha:14\n\nairport.long %&gt;%\n  mutate(time = parse_number(when)) %&gt;%\n  ggplot(aes(x = time, y = epinephrine, colour = factor(location), group = child)) +\n  geom_point() + geom_line(alpha = 0.2)\n\n\n\n\nIt seems to make the lines skinnier, so they look more like threads. Even given the lesser thickness, they seem to be a little bit see-through as well. You can experiment with adding transparency to the points in addition.\n\\(\\blacksquare\\)\n\nWhat do you see on your spaghetti plot? We are looking ahead to possible effects of time, location and their interaction.\n\nSolution\nThis is not clear, so it’s very much your call. I see the red spaghetti strands as going up further (especially) and maybe down further than the blue ones. The epinephrine levels of the children near the new airport are definitely more spread out, and maybe have a higher mean, than those of the control group of children not near the airport. The red spaghetti strands show something of an increase over time, at least up to time 3, after which they seem to drop again. The blue strands, however, don’t show any kind of trend over time. Since the time trend is different for the two locations, I would expect to see a significant interaction.\n\\(\\blacksquare\\)\n\nThe spaghetti plot was hard to interpret because there are so many children. Calculate the mean epinephrine levels for each location-time combination, and make an interaction plot with time on the \\(x\\)-axis and location as the second factor.\n\nSolution\nWe’ve done this before:\n\nairport.long %&gt;%\n  mutate(time = parse_number(when)) %&gt;%\n  mutate(floc = factor(location)) %&gt;%\n  group_by(floc, time) %&gt;%\n  summarize(mean.epi = mean(epinephrine)) %&gt;%\n  ggplot(aes(x = time, y = mean.epi, group = floc, colour = floc)) +\n  geom_point() + geom_line()\n\n`summarise()` has grouped output by 'floc'. You can override using the\n`.groups` argument.\n\n\n\n\n\nI wanted the actual numerical times, so I made them again. Also, it seemed to be easier to create a factor version of the numeric location up front, and then use it several times later. I’m actually not sure that you need it here, since group_by works with the distinct values of a variable, whatever they are, and group in a boxplot may or may not insist on something other than a number. I should try it:\n\nairport.long %&gt;%\n  mutate(time = parse_number(when)) %&gt;%\n  group_by(location, time) %&gt;%\n  summarize(mean.epi = mean(epinephrine)) %&gt;%\n  ggplot(aes(x = time, y = mean.epi, group = location, colour = location)) +\n  geom_point() + geom_line()\n\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n\n\n\n\n\nIt seems that colour requires a non-number:\n\nairport.long %&gt;%\n  mutate(time = parse_number(when)) %&gt;%\n  group_by(location, time) %&gt;%\n  summarize(mean.epi = mean(epinephrine)) %&gt;%\n  ggplot(aes(x = time, y = mean.epi, group = location, colour = factor(location))) +\n  geom_point() + geom_line()\n\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n\n\n\n\n\nWith a long pipeline like this, none of us get it right the first time (I certainly didn’t), so be prepared to debug it one line at a time. The way I like to do this is to take the pipe symbol and move it down to the next line (moving the cursor to just before it and hitting Enter). This ends the pipe at the end of this line and displays what it produces so far. When you are happy with that, go to the start of the next line (that currently has a pipe symbol by itself) and hit Backspace to move the pipe symbol back where it was. Then go to the end of the next line (where the next pipe symbol is), move that to a line by itself, and so on. Keep going until each line produces what you want, and when you are finished, the whole pipeline will do what you want.\n\\(\\blacksquare\\)\n\nWhat do you conclude from your interaction plot? Is your conclusion clearer than from the spaghetti plot?\n\nSolution\nThe two “traces” are not parallel, so I would expect to see an interaction between location and time. The big difference seems to be between times 1 and 2; the traces are the same at time 1, and more or less parallel after time 2. Between times 1 and 2, the mean epinephrine level of the children near the new airport increases sharply, whereas for the children in the control group it increases much less. This, to my mind, is very much easier to interpret than the spaghetti plot, even the second version with the thinner strands, because there is a lot of variability there that obscures the overall pattern. The interaction plot is plain as day, but it might be an oversimplification because it doesn’t show variability.\n\\(\\blacksquare\\)\n\nRun a repeated-measures analysis of variance and display the results. Go back to your original data frame, the one you read in from the file, for this. You’ll need to make sure your numeric location gets treated as a factor.\n\nSolution\nThe usual process. I’ll try the other way I used of making the response:\n\nairport %&gt;%\n  select(epi_1:epi_4) %&gt;%\n  as.matrix() -&gt; response\nairport.1 &lt;- lm(response ~ factor(location), data = airport)\ntimes &lt;- colnames(response)\ntimes.df &lt;- data.frame(times=factor(times))\nairport.2 &lt;- Manova(airport.1, idata = times.df, idesign = ~times)\nsummary(airport.2)\n\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   268516272\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1  0.920129     2281      1    198 &lt; 2.22e-16 ***\nWilks             1  0.079871     2281      1    198 &lt; 2.22e-16 ***\nHotelling-Lawley  1 11.520204     2281      1    198 &lt; 2.22e-16 ***\nRoy               1 11.520204     2281      1    198 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: factor(location) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     3519790\n\nMultivariate Tests: factor(location)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1 0.1311980 29.90002      1    198 1.3611e-07 ***\nWilks             1 0.8688020 29.90002      1    198 1.3611e-07 ***\nHotelling-Lawley  1 0.1510102 29.90002      1    198 1.3611e-07 ***\nRoy               1 0.1510102 29.90002      1    198 1.3611e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1     times2     times3\ntimes1  497500.84 -113360.08 -56261.667\ntimes2 -113360.08   25830.12  12819.731\ntimes3  -56261.67   12819.73   6362.552\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1 0.3274131 31.80405      3    196 &lt; 2.22e-16 ***\nWilks             1 0.6725869 31.80405      3    196 &lt; 2.22e-16 ***\nHotelling-Lawley  1 0.4867966 31.80405      3    196 &lt; 2.22e-16 ***\nRoy               1 0.4867966 31.80405      3    196 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: factor(location):times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1    times2      times3\ntimes1 533081.68 206841.01 -14089.6126\ntimes2 206841.01  80256.38  -5466.9104\ntimes3 -14089.61  -5466.91    372.3954\n\nMultivariate Tests: factor(location):times\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1 0.2373704 20.33516      3    196 1.6258e-11 ***\nWilks             1 0.7626296 20.33516      3    196 1.6258e-11 ***\nHotelling-Lawley  1 0.3112525 20.33516      3    196 1.6258e-11 ***\nRoy               1 0.3112525 20.33516      3    196 1.6258e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                         Sum Sq num Df Error SS den Df  F value    Pr(&gt;F)    \n(Intercept)            67129068      1  5827073    198 2281.000 &lt; 2.2e-16 ***\nfactor(location)         879947      1  5827073    198   29.900 1.361e-07 ***\ntimes                    475671      3  3341041    594   28.190 &lt; 2.2e-16 ***\nfactor(location):times   366641      3  3341041    594   21.728 2.306e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                       Test statistic  p-value\ntimes                          0.9488 0.066194\nfactor(location):times         0.9488 0.066194\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                        GG eps Pr(&gt;F[GG])    \ntimes                  0.96685  &lt; 2.2e-16 ***\nfactor(location):times 0.96685   5.35e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                          HF eps   Pr(&gt;F[HF])\ntimes                  0.9827628 8.418534e-17\nfactor(location):times 0.9827628 3.571900e-13\n\n\n\\(\\blacksquare\\)\n\nWhat do you conclude from the MANOVA? Is that consistent with your graphs? Explain briefly.\n\nSolution\nStart with Mauchly’s test near the bottom. This is not quite significant (for the interaction), so we are entitled to look at the univariate test for interaction, which is \\(2.3 \\times 10^{-13}\\), extremely significant. If you want a comparison, look at the Huynh-Feldt adjustment for the interaction, which is almost exactly the same (\\(3.57 \\times 10^{-13}\\)), or the multivariate tests for the interaction (almost the same again).\nSo, we start and end with the significant interaction: there is an effect of location, but the nature of that effect depends on time. This is the same as we saw in the interaction plot: from time 2 on, the mean epinephrine levels for the children near the new airport were clearly higher.\nIf you stare at the spaghetti plot, you might come to the same conclusion. Or you might not! I suppose those red dots at time 2 are mostly at the top, and generally so afterwards, whereas at time 1 they are all mixed up with the blue ones.\nInteractions of this sort in this kind of analysis are very common. There is an “intervention” or “treatment”, and the time points are chosen so that the first one is before the treatment happens, and the other time points are after. Then, the results are very similar for the first time point, and very different after that, rather than being (say) always higher for the treatment group by about the same amount for all times (in which case there would be no interaction).\nSo, you have some choices in practice as to how you might go. You might do the MANOVA, get a significant interaction, and draw an interaction plot to see why. You might stop there, or you might do something like what we did in class: having seen that the first time point is different from the others for reasons that you can explain, do the analysis again, but omitting the first time point. For the MANOVA, that means tweaking your definition of your response to omit the first time point. The rest of it stays the same, though you might want to change your model numbers rather than re-using the old ones as I did:\n\nairport %&gt;%\n  select(epi_2:epi_4) %&gt;%\n  as.matrix() -&gt; response\nairport.1 &lt;- lm(response ~ factor(location), data = airport)\ntimes &lt;- colnames(response)\ntimes.df &lt;- data.frame(times=factor(times))\nairport.2 &lt;- Manova(airport.1, idata = times.df, idesign = ~times)\nsummary(airport.2)\n\nWarning in summary.Anova.mlm(airport.2): HF eps &gt; 1 treated as 1\n\n\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   165867956\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1  0.918531 2232.383      1    198 &lt; 2.22e-16 ***\nWilks             1  0.081469 2232.383      1    198 &lt; 2.22e-16 ***\nHotelling-Lawley  1 11.274662 2232.383      1    198 &lt; 2.22e-16 ***\nRoy               1 11.274662 2232.383      1    198 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: factor(location) \n\n Response transformation matrix:\n      (Intercept)\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     3567099\n\nMultivariate Tests: factor(location)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1 0.1951509 48.00886      1    198 5.8219e-11 ***\nWilks             1 0.8048491 48.00886      1    198 5.8219e-11 ***\nHotelling-Lawley  1 0.2424690 48.00886      1    198 5.8219e-11 ***\nRoy               1 0.2424690 48.00886      1    198 5.8219e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2\nepi_2      1      0\nepi_3      0      1\nepi_4     -1     -1\n\nSum of squares and products for the hypothesis:\n         times1    times2\ntimes1 25830.12 12819.731\ntimes2 12819.73  6362.552\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df  Pr(&gt;F)\nPillai            1 0.0123563 1.232325      2    197 0.29385\nWilks             1 0.9876437 1.232325      2    197 0.29385\nHotelling-Lawley  1 0.0125109 1.232325      2    197 0.29385\nRoy               1 0.0125109 1.232325      2    197 0.29385\n\n------------------------------------------\n \nTerm: factor(location):times \n\n Response transformation matrix:\n      times1 times2\nepi_2      1      0\nepi_3      0      1\nepi_4     -1     -1\n\nSum of squares and products for the hypothesis:\n         times1     times2\ntimes1 80256.38 -5466.9104\ntimes2 -5466.91   372.3954\n\nMultivariate Tests: factor(location):times\n                 Df test stat approx F num Df den Df    Pr(&gt;F)   \nPillai            1 0.0508561 5.277736      2    197 0.0058507 **\nWilks             1 0.9491439 5.277736      2    197 0.0058507 **\nHotelling-Lawley  1 0.0535811 5.277736      2    197 0.0058507 **\nRoy               1 0.0535811 5.277736      2    197 0.0058507 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                         Sum Sq num Df Error SS den Df   F value    Pr(&gt;F)    \n(Intercept)            55289319      1  4903856    198 2232.3830 &lt; 2.2e-16 ***\nfactor(location)        1189033      1  4903856    198   48.0089 5.822e-11 ***\ntimes                     12915      2  2281728    396    1.1207  0.327070    \nfactor(location):times    57397      2  2281728    396    4.9807  0.007306 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                       Test statistic p-value\ntimes                         0.99068 0.39746\nfactor(location):times        0.99068 0.39746\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                        GG eps Pr(&gt;F[GG])   \ntimes                  0.99076   0.326702   \nfactor(location):times 0.99076   0.007483 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                         HF eps  Pr(&gt;F[HF])\ntimes                  1.000731 0.327069634\nfactor(location):times 1.000731 0.007305714\n\n\nThe interaction is still significant. The sphericity test is not significant, so you can use the 0.0073 in the univariate tests as your P-value (note that the Huynh-Feldt adjustment is actually not adjusted at all).\nSo there is still not a consistent effect of being near the new airport on epinephrine levels: that is to say, the effect of the new airport still varies over time. That might be because (looking at the interaction plot) for the children near the new airport, the mean epinephrine level went up again between times 2 and 3, whereas for the control children it (for some reason) went dramatically down over the same time period.\nWe have lots of data here (200 children), so the significant interaction effect also might not be very big.\nExperimental designs like this are kind of awkward, because you expect there to be some kind of pattern over time for the treatment group, that will vary over time, whereas for the control group, you expect there to be no pattern over time. So a significant difference shows up as an interaction, which is messier to interpret than you would like.\nExtra: the other way to analyze repeated measures data 15 is to treat them as “mixed models”, which requires a different kind of analysis using the lme4 package. I always forget how these go, and I have to look them up when I need them, but the idea is this: the treatments you observe, and the time points at which you observe them, are typically the only ones you care about (a so-called “fixed effect”), but the individuals (children, here) which you happen to observe are something like a random sample of all the children you might have observed (a so-called “random effect”). Models with random effects in them are called “mixed models” (or, I suppose, models with both fixed and random effects). This matters because you have repeated observations on the same child. Some people like to think of this in terms of “sources of variability”: the epinephrine levels vary because of the location and time at which they were observed, but also because of the particular child they happen to have been observed for: each child has a “random effect” that they carry with them through all the time points at which they are observed.\nLet’s see if we can make it fly for this example. We need the data in “long” format, the way we arranged it for graphing: the data frame airport.long. I’d like to convert things to factors first:\n\nairport.long %&gt;% mutate(\n  fchild = factor(child),\n  flocation = factor(location)\n) -&gt; fairport\n\nairport.3 &lt;- lmer(epinephrine ~ flocation * when + (1 | fchild), data = fairport)\nanova(airport.3)\n\n\n\n  \n\n\ndrop1(airport.3, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe anova doesn’t actually give us any tests, but what you see in the ANOVA table are the fixed effects. These are testable. The easiest way to see what you can get rid of is drop1; the chi-squared test appears to be the right one (more on that below). This says that the interaction is strongly significant, and we should not consider removing it, the same conclusion as our “profile analysis” before. The other choice for testing is to fit a model without what you are testing and use anova to compare the two models:\n\nairport.4 &lt;- update(airport.3, . ~ . - flocation:when)\nanova(airport.4, airport.3)\n\nrefitting model(s) with ML (instead of REML)\n\n\n\n\n  \n\n\n\nThere are technical considerations involved in comparing the fit of two models (which is the reason for the “refitting models with…”): there is one method for estimating and a different method for testing. The test is based on “likelihood ratio”, which means that the right test for the drop1 above is Chisq.\nIf you omit the test in drop1, it just gives you AIC values, which you can use for an informal assessment. In this case, &lt;none&gt; has a much smaller AIC than the interaction (smaller by over 50), so there’s no way we should entertain taking out the interaction. However, if it had not been significant, we would just take it out by fitting a model like airport4: there is no distinction here between “within-subject” and “between-subject” factors that prevented us from taking the interaction out in profile analysis.\nAs ever when an interaction is significant, we might think about simple effects: that is, look at the two locations separately. That makes sense here because of the kind of experimental design it is: we expect a different kind of relationship with time for the “treatment” children (the ones living near the new airport) as compared to the control children, the ones who live farther away. That approach would work with either the profile-analysis way using Manova or the mixed-modelling way using lmer. In either case, we’d expect to see a time effect at location 1 but not at location 2. (Having separated out locations, only the time effect would be left for testing.) I guess I have to show you that, but I have to get ready for class first.\nLater\nThe nice thing about Wednesday evenings is that I am so tired from class that I have energy for almost nothing except playing with these things. So let’s have a crack at it.\nLet’s start with location 1, at which we expect there to be something happening. This is really a simple effect of time at location 1, but in repeated measures guise. The awkwardness is that the profile analysis needs the wide-format data, while the mixed-model analysis needs long format, so we’ll have to repeat our process, once for each format of data set:\n\nloc1 &lt;- airport %&gt;% filter(location == 1)\nresponse &lt;- loc1 %&gt;% select(epi_1:epi_4) %&gt;% as.matrix()\nloc1.1 &lt;- lm(response ~ 1, data = loc1)\ntimes &lt;- colnames(response)\ntimes.df &lt;- data.frame(times=factor(times))\nloc1.2 &lt;- Manova(loc1.1, idata = times.df, idesign = ~times)\n\nNote: model has only an intercept; equivalent type-III tests substituted.\n\nsummary(loc1.2)\n\nWarning in summary.Anova.mlm(loc1.2): HF eps &gt; 1 treated as 1\n\n\n\nType III Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   166760848\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1  0.897288 864.8584      1     99 &lt; 2.22e-16 ***\nWilks             1  0.102712 864.8584      1     99 &lt; 2.22e-16 ***\nHotelling-Lawley  1  8.735944 864.8584      1     99 &lt; 2.22e-16 ***\nRoy               1  8.735944 864.8584      1     99 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1    times2     times3\ntimes1 1030275.32 87978.051 -71100.691\ntimes2   87978.05  7512.688  -6071.484\ntimes3  -71100.69 -6071.484   4906.755\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1 0.4542272 26.90988      3     97 9.4963e-13 ***\nWilks             1 0.5457728 26.90988      3     97 9.4963e-13 ***\nHotelling-Lawley  1 0.8322643 26.90988      3     97 9.4963e-13 ***\nRoy               1 0.8322643 26.90988      3     97 9.4963e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n              Sum Sq num Df Error SS den Df F value    Pr(&gt;F)    \n(Intercept) 41690212      1  4772262     99 864.858 &lt; 2.2e-16 ***\ntimes         776618      3  2774062    297  27.716 7.869e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n      Test statistic p-value\ntimes        0.95865 0.53129\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n       GG eps Pr(&gt;F[GG])    \ntimes 0.97423  1.748e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        HF eps   Pr(&gt;F[HF])\ntimes 1.007064 7.869406e-16\n\n\nThis is one of those times where a pipe doesn’t quite do it. We need to grab the data for location 1, but then we need to do two things with it: one, create a response variable, and two, feed the response variable and the location 1 data into lm. Pipes are for linear sequences of things to do, and this one diverges. So, save what you need to save, and then do it without a pipe.\nThis is one of those explanatory-variable-less repeated measures where you only have things varying over time. Here, the treatment was location, and we’re only looking at one of those locations. We still have a within-subject design (the four times), but there is no between-subject design left.\nThe conclusion is that there is very strong evidence of a time effect for location 1, as we would have guessed.\nNow, let’s see whether the mixed-model analysis comes to the same conclusion. This time, we have to start from the long data set (the one I’m using is the one called fairport with things as factors) and pull out the rows for location 1. The only remaining fixed effect is time:\n\nfairport %&gt;%\n  filter(location == 1) %&gt;%\n  lmer(epinephrine ~ when + (1 | fchild), data = .) %&gt;%\n  drop1(test = \"Chisq\")\n\n\n\n  \n\n\n\nThe effect of time is, once again, substantial. The P-value is not the same, because the test is not the same, but it is very similar and points to exactly the same conclusion.\nI should perhaps point out that you don’t have to do these models in a pipe. I just did because it was easier for me. But there’s nothing wrong with doing it like this:\n\ntmp &lt;- fairport %&gt;% filter(location == 1)\ntmp.1 &lt;- lmer(epinephrine ~ when + (1 | child), data = tmp)\ndrop1(tmp.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nand it gives exactly the same result.\nThis last model (in either of its versions) is a so-called random intercepts model. What it says is that epinephrine level for a child depends on time, but the effect of being one child rather than another is to shunt the mean epinephrine level up or down by a fixed amount for all times. It seems to me that this is a very reasonable way to model the child-to-child variation in this case, but in other cases, things might be different. lmer allows more sophisticated things, like for example the random child effect depending linearly on time. To do that, you’d rewrite the above like this:\n\nfairport %&gt;%\n  filter(location == 1) %&gt;%\n  lmer(epinephrine ~ when + (1 + when | fchild), data = .) %&gt;%\n  drop1(test = \"Chisq\")\n\nThe change is on the lmer line: the bit in brackets has a linear model in when for each child. I didn’t run that, so I’m not certain it works,16 but that’s the idea.\nAll right, let’s get on to location 2. This was the control location, so we expect to see no dependence of epinephrine level on time, either in the profile analysis or in the mixed-model analysis. There is a large amount of copying and pasting coming up:\n\nairport %&gt;% filter(location == 2) -&gt; loc2\nloc2 %&gt;% select(epi_1:epi_4) %&gt;% as.matrix() -&gt; response\nloc2.1 &lt;- lm(response ~ 1, data = loc1)\ntimes &lt;- colnames(response)\ntimes.df &lt;- data.frame(times=factor(times))\nloc2.2 &lt;- Manova(loc2.1, idata = times.df, idesign = ~times)\n\nNote: model has only an intercept; equivalent type-III tests substituted.\n\nsummary(loc2.2)\n\n\nType III Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   105275213\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1  0.961466 2470.169      1     99 &lt; 2.22e-16 ***\nWilks             1  0.038534 2470.169      1     99 &lt; 2.22e-16 ***\nHotelling-Lawley  1 24.951203 2470.169      1     99 &lt; 2.22e-16 ***\nRoy               1 24.951203 2470.169      1     99 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1    times2     times3\ntimes1  307.1984  5502.882   749.4117\ntimes2 5502.8823 98573.811 13424.3048\ntimes3  749.4117 13424.305  1828.1931\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1 0.3023560 14.01313      3     97 1.1613e-07 ***\nWilks             1 0.6976440 14.01313      3     97 1.1613e-07 ***\nHotelling-Lawley  1 0.4333957 14.01313      3     97 1.1613e-07 ***\nRoy               1 0.4333957 14.01313      3     97 1.1613e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n              Sum Sq num Df Error SS den Df  F value    Pr(&gt;F)    \n(Intercept) 26318803      1  1054811     99 2470.169 &lt; 2.2e-16 ***\ntimes          65694      3   566979    297   11.471 3.884e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n      Test statistic    p-value\ntimes        0.77081 0.00011477\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n       GG eps Pr(&gt;F[GG])    \ntimes 0.84304  2.433e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n         HF eps   Pr(&gt;F[HF])\ntimes 0.8669702 1.838348e-06\n\n\nI think I changed everything I needed to.\nThere actually is still an effect of time. This is true even though sphericity fails and so the Huynh-Feldt adjustment to the P-value is fairly substantial; even so, it is still clearly significant.\nWhat kind of effect? We can look at that by finding epinephrine means at each time. That would be easier if we had long format, but I think we can still do it. The magic word is across:\n\nairport %&gt;%\n  filter(location == 2) %&gt;%\n  summarize(across(starts_with(\"epi\"), \\(epi) mean(epi)))\n\n\n\n  \n\n\n\n(in words, “for each column that starts with epi, find the mean of it.”)\nThe message here is that the mean epinephrine levels at time 2 were higher than the others, so that’s what seems to be driving the time effect. And remember, these were the control children, so they had no new airport nearby.\nI want to try this:\n\nairport %&gt;% nest_by(location)\n\n\n\n  \n\n\n\nThis is like the idea in the Spilich problem: I want to write a function that calculates the means of the epi_1 through epi_4 columns of a data frame like airport, and then apply it to each of those “nested” data frames.\n\nepi.means &lt;- function(x) {\n  x %&gt;% summarize(across(starts_with(\"epi\"), \\(epi) mean(epi)))\n}\n\nepi.means(airport)\n\n\n\n  \n\n\n\nOK, and then:\n\nairport %&gt;%\n  nest_by(location) %&gt;%\n  rowwise() %&gt;% \n  mutate(means = list(epi.means(data))) %&gt;%\n  unnest(means)\n\n\n\n  \n\n\n\nOoh, nice. There are the means for the four time points at both of the two locations. At location 1, epinephrine starts out low, jumps high after the airport is built, and stays there, while at location 2, the mean is mysteriously higher at time 2 and then epinephrine levels go back down again.\nEnough of that. Back to the mixed model analysis, with more copying and pasting. Here is the “simple time effect” at location 2:\n\nfairport %&gt;%\n  filter(location == 2) %&gt;%\n  lmer(epinephrine ~ when + (1 | fchild), data = .) %&gt;%\n  drop1(test = \"Chisq\")\n\n\n\n  \n\n\n\nThis also finds a time effect at location 2, with a very similar P-value.\nSo which method is preferred? They are, I think, two different ways of approaching the same problem, and so there is no good reason to prefer one way over the other. The profile-analysis way is easier to follow if you are coming from a MANOVA direction: set it up the same way as you would a MANOVA, with your repeated measures as your multivariate response, and distinguish between the within-subject design (times, for us) and the between-subject design (treatments, stuff like that). As for mixed models: when you get your head around that crazy way of specifying the random effects, which are typically “subjects” in this kind of analysis, the actual model statement is brief: response depends on fixed effects plus random effects with the brackets and vertical bar. I always find the lmer models look very simple once you have figured them out (like ggplot in that regard). I also like the fact that lmer uses “tidy data”, so that you can make graphs and do this flavour of analysis with the same data frame. Having gotten my head around getting my data tidy, it seems odd to me that profile analysis requires the data to be untidy, although it does so for a good reason: if you were genuinely doing a MANOVA, you would want your multiple response variables each in their own column. Doing repeated measures this way is thinking of your responses at different times like different variables.\nThe approach I’ve taken in this course is for historical reasons. That is to say, my history, not because the historical perspective is necessarily the best one. It began with doing repeated measures in SAS (this was like nine years ago), and the only way I could get my head around that was the profile-analysis way. I’m not even sure there was software for doing mixed models in those days, certainly not in SAS. Besides, random effects scared me back then. Anyway, when I moved things over to R (this must have been about six years ago), I realized I was using basically the same idea in R: make it into a MANOVA and tweak things as needed. So I developed things by showing you MANOVA first as ANOVA when you have lots of \\(y\\)-variables instead of just one, and then using it for repeated measures by thinking of the repeated measures as “different-but-related” \\(y\\)-variables. To my mind, this is a reasonable way of doing it in a course like this: you get to see MANOVA, which is worth knowing about anyway, and then you get to apply it to a very common situation.\nMixed models are actually quite recent, as a way of doing this kind of modelling. Back when I was learning this stuff, you learned to distinguish between “fixed effects”, whose levels were the only ones you cared about (like the two locations here), and “random effects”, where the ones you observed were only a sample of the ones you might have seen (like “children” or in general “subjects”). Then you would test the fixed effects by hypothesizing that their mean effect was zero and seeing whether you could reject that (in the same way that you do in regression or regular ANOVA: in the latter case, you test that the effects of all the different groups are zero, which comes down to saying that all the groups have the same mean). The way you handled random effects was to estimate their variance, and then you would test whether a random effect exists or not by testing whether the variance of that random effect is zero (does not exist) or is greater than zero. Zero is the smallest a variance (or SD) can be, which means that testing for it has always been kind of flakey and the standard theory doesn’t work for it (the technical term is “on the boundary of the parameter space”, and the theory applies when the null-hypothesis value is strictly inside the set of possible values the parameter can take). Back when we did this stuff by hand,17 we had to figure out whether we were testing a fixed or a random effect, and there were rules, involving things called “expected mean squares”, that told you how to get the tests right. Anyway, that is all considered rather old-fashioned now, and mixed models are where it is at. In these, you typically only test the fixed effects, while estimating the size of the random effects, taking it for granted that they exist. This is an active area of research; the things that lmer fits are called “linear mixed models”, and there are also now “generalized linear mixed models”, things like logistic regression with random effects.\nWe haven’t had to worry about this up to now because in most of the experimental designs we have used, each subject only contributes one measurement. In that case, you cannot separate out a subject-specific “random effect” from random error, and so we lump it all into random error. The one experimental design where a subject gives us more than one measurement is matched pairs, and the way we have handled that so far is to turn the two measurements into one by taking the difference between them; even in that case, each subject contributes only one difference to the analysis. Repeated measures, though, is genuinely different: you can’t wish away multiple measurements on the same subject by taking differences any more, so you have to face up to the issue at last. Either we think of it as several different measurements that might be correlated (the profile-analysis MANOVA way), or we face up to the idea that different subjects bring their own random effect to the table that shows up in every measurement that the subject provides (mixed models).\nI did analysis of covariance as a separate mini-section, thinking of it as a regression, but if the numerical covariate is the same thing as the response but measured at a different time (eg., before rather than after), that would also respond to a repeated-measures approach, or to the taking of differences as matched pairs does. There is almost always more than one way to look at these things.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "repeated-measures.html#body-fat-as-repeated-measures-1",
    "href": "repeated-measures.html#body-fat-as-repeated-measures-1",
    "title": "31  Repeated measures",
    "section": "31.10 Body fat as repeated measures",
    "text": "31.10 Body fat as repeated measures\nThis one is also stolen from STAC32. Athletes are concerned with measuring their body fat percentage. Two different methods are available: one using ultrasound, and the other using X-ray technology. We are interested in whether there is a difference in the mean body fat percentage as measured by these two methods, and if so, how big that difference is. Data on 16 athletes are at link.\n\nRead in the data and check that you have a sensible number of rows and columns.\n\nSolution\nThis kind of thing:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/bodyfat.txt\"\nbodyfat &lt;- read_delim(my_url, \" \")\n\nRows: 16 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): athlete, xray, ultrasound\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbodyfat\n\n\n\n  \n\n\n\n16 rows (athletes) and 3 columns, one for each measurement method and one labelling the athletes. All good.\n\\(\\blacksquare\\)\n\nCarry out a suitable (matched-pairs) \\(t\\)-test to determine whether the means are the same or different.\n\nSolution\nFeed the two columns into t.test along with paired=T. Remember that this is (in effect) a one-sample \\(t\\)-test, so that you can’t use a data=. You therefore have to wrap everything in a with:\n\nwith(bodyfat, t.test(xray, ultrasound, paired = TRUE))\n\n\n    Paired t-test\n\ndata:  xray and ultrasound\nt = -0.30801, df = 15, p-value = 0.7623\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.7425068  0.5550068\nsample estimates:\nmean difference \n       -0.09375 \n\n\nThe test we want is two-sided, so we didn’t have to take any special steps to get that.\n\\(\\blacksquare\\)\n\nWhat do you conclude from the test?\n\nSolution\nThe P-value of 0.7623 is not at all small, so there is no way we can reject the null hypothesis.18 There is no evidence of a difference in means; we can act as if the two methods produce the same mean body fat percentage. That is to say, on this evidence we can use either method, whichever one is cheaper or more convenient.\n\\(\\blacksquare\\)\n\nRun a repeated-measures analysis of variance, treating the two methods of measuring body fat as the repeated measures (ie., playing the role of “time” that you have seen in most of the other repeated measures analyses). There is no “treatment” here, so there is nothing to go on the right side of the squiggle. Insert a 1 there to mean “just an intercept”. Display the results.\n\nSolution\nConstruct the response variable, run lm, construct the within-subjects part of the design, run Manova:\n\nbodyfat %&gt;%\n  select(xray:ultrasound) %&gt;%\n  as.matrix() -&gt; response\nbodyfat.1 &lt;- lm(response ~ 1)\nmethods &lt;- colnames(response)\nmethods.df &lt;- data.frame(methods=factor(methods))\nbodyfat.2 &lt;- Manova(bodyfat.1, idata = methods.df, idesign = ~methods)\n\nNote: model has only an intercept; equivalent type-III tests substituted.\n\nsummary(bodyfat.2)\n\n\nType III Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n           (Intercept)\nxray                 1\nultrasound           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)        9025\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1  0.797341 59.01586      1     15 1.4135e-06 ***\nWilks             1  0.202659 59.01586      1     15 1.4135e-06 ***\nHotelling-Lawley  1  3.934390 59.01586      1     15 1.4135e-06 ***\nRoy               1  3.934390 59.01586      1     15 1.4135e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: methods \n\n Response transformation matrix:\n           methods1\nxray             -1\nultrasound        1\n\nSum of squares and products for the hypothesis:\n         methods1\nmethods1 0.140625\n\nMultivariate Tests: methods\n                 Df test stat   approx F num Df den Df  Pr(&gt;F)\nPillai            1 0.0062849 0.09486999      1     15 0.76231\nWilks             1 0.9937151 0.09486999      1     15 0.76231\nHotelling-Lawley  1 0.0063247 0.09486999      1     15 0.76231\nRoy               1 0.0063247 0.09486999      1     15 0.76231\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n            Sum Sq num Df Error SS den Df F value    Pr(&gt;F)    \n(Intercept) 4512.5      1  1146.94     15 59.0159 1.413e-06 ***\nmethods        0.1      1    11.12     15  0.0949    0.7623    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis is an unusual one in that the repeated measures variable is not time but method, and there is no actual treatment. The consequence (with there being only two methods) is that sphericity is automatically good, and the univariate test for methods (at the bottom) has exactly the same P-value as all the multivariate tests, so that the conclusion is the same either way. (Also, see below about the matched pairs.)\n\\(\\blacksquare\\)\n\nCompare your repeated-measures analysis to your matched-pairs one. Do you draw the same conclusions?\n\nSolution\nThe P-value for methods, which is testing the same thing as the matched pairs, is 0.7623, which is actually identical to the matched pairs \\(t\\)-test, and so the conclusion is identical also. That is, there is no difference between the two methods for measuring body fat. This goes to show that repeated measures gives the same answer as a matched-pairs \\(t\\)-test in the situation where they both apply. But repeated measures is, as we have seen, a lot more general.\nSince this really is repeated measures, we ought to be able to use a mixed model here too. We need “long” or “tidy” format, which we don’t have yet. One pipe to save them all, to paraphrase Lord of the Rings19 — put all the fat measurements in one column with a label saying which method they were obtained with; create a column which is the athlete number as a factor; fit the linear mixed model; see what we can drop from it:\n\nbodyfat\n\n\n\n  \n\n\nbodyfat %&gt;%\n  pivot_longer(-athlete, names_to=\"method\", values_to=\"fat\") %&gt;%\n  mutate(fathlete = factor(athlete)) %&gt;%\n  lmer(fat ~ method + (1 | fathlete), data = .) %&gt;%\n  drop1(test = \"Chisq\")\n\n\n\n  \n\n\n\nOnce again, there is no difference between methods, and though the P-value is different from the matched pairs or profile analysis, it is very close to those.\nIf you’re not clear about the tidy data frame used for input to lmer, pull the top two lines off the pipeline and see what they produce:\n\nbodyfat %&gt;%\n  pivot_longer(-athlete, names_to=\"method\", values_to=\"fat\") %&gt;%\n  mutate(fathlete = factor(athlete))\n\n\n\n  \n\n\n\nEach athlete now appears twice: once with their fat measured by xray, and again with it measured by ultrasound. The column fathlete is a factor.\nThe mixed model took me two goes to get right: I forgot that I needed the data=. in lmer, because it works like lm with the model formula first, not the input data. If the pipeline is going too fast for you, create the tidy data frame and save it, then use the saved data frame as input to lmer.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "repeated-measures.html#investigating-motor-activity-in-rats-1",
    "href": "repeated-measures.html#investigating-motor-activity-in-rats-1",
    "title": "31  Repeated measures",
    "section": "31.11 Investigating motor activity in rats",
    "text": "31.11 Investigating motor activity in rats\nA researcher named King was investigating the effect of the drug midazolam on motor activity in rats. Typically, the first time the drug is injected, a rat’s motor activity decreases substantially, but rats typically develop a “tolerance”, so that further injections of the drug have less impact on the rat’s motor activity.\nThe data shown in link were all taken in one day, called the “experiment day” below. 24 different rats were used. Each rat, on the experiment day, was injected with a fixed amount of midazolam, and at each of six five-minute intervals after being injected, the rat’s motor activity was measured (these are labelled i1 through i6 in the data). The rats differed in how they had been treated before the experiment day. The control group of rats had previously been injected repeatedly with a saline solution (no active ingredient), so the experiment day was the first time this group of rats had received midazolam. The other two groups of rats had both received midazolam repeatedly before the experiment day: the “same” group was injected on experiment day in the same environment that the previous injections had taken place (this is known in psychology as a “conditioned tolerance”), but the “different” group had the previous injections in a different environment than on experiment day.\nThe column id identifies the rat from which each sequence of values was obtained.\n\nExplain briefly why we need to use a repeated measures analysis for these data.\n\nSolution\nEach rat is measured at six different times (i1 through i6): that is to say, each row of the data set consists of repeated measurements on the same rat. (If each row had used six different rats to obtain the six measurements, we would have been back in familiar territory and could have used a regular analysis of variance.)\n\\(\\blacksquare\\)\n\nRead in the data and note that you have what was promised in the question.\n\nSolution\nThe usual:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/king.csv\"\nking &lt;- read_csv(my_url)\n\nRows: 24 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): context\ndbl (7): id, i1, i2, i3, i4, i5, i6\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nking\n\n\n\n  \n\n\n\nThere are 24 rats (rows). The columns label the rat (id) and the times at which motor activity was measured (i1 through i6). The remaining column, context, describes how the rats were treated before experiment day, with the levels being the same ones given in the question.\n\\(\\blacksquare\\)\n\nWe are going to do a repeated-measures analysis using the “profile” method shown in class. Create a suitable response variable for this method.\n\nSolution\ncbind the appropriate columns together, to make a matrix:\n\nresponse &lt;- with(king, cbind(i1, i2, i3, i4, i5, i6))\n\nThis is the “simple” but tedious way, and produces a matrix because the i1 through i6 are vectors (single columns):\n\nclass(response)\n\n[1] \"matrix\" \"array\" \n\n\ni1:i6 does not work here, because we are outside of the tidyverse, and in that world, : only means “through” (as in “this through that”) when the things on either side of it are or represent numbers.\nThe clever way to get the response is to select the columns and then turn them into a matrix. This does permit the colon because we are now in the tidyverse:\n\nresponse &lt;- king %&gt;%\n  select(i1:i6) %&gt;%\n  as.matrix()\nclass(response)\n\n[1] \"matrix\" \"array\" \n\n\nIt is indeed a matrix.\nI tried to be extra-clever and use starts_with, but I have another column id that starts with i that I do not want to be part of the response. So I had to abandon that idea, but not before trying this:\n\nresponse &lt;- king %&gt;%\n  select(matches(\"i[0-9]\")) %&gt;%\n  as.matrix()\nhead(response)\n\n      i1  i2  i3  i4  i5  i6\n[1,] 150  44  71  59 132  74\n[2,] 335 270 156 160 118 230\n[3,] 149  52  91 115  43 154\n[4,] 159  31 127 212  71 224\n[5,] 292 125 184 246 225 170\n[6,] 297 187  66  96 209  74\n\n\nhead displays the first six lines (of anything). We don’t normally need it because we are typically dealing with tibble-like data frames that display only ten rows of themselves by default. But this worked. The matches part takes a so-called “regular expression” which is a very flexible way of matching anything: in this case, a column whose name starts with i followed by exactly one digit (something between 0 and 9 inclusive).\n\\(\\blacksquare\\)\n\nSet up the “within-subjects” part of the analysis. That means getting hold of the names of the columns that hold the different times, saving them, and also making a data frame out of them:\n\nSolution\n\ntimes &lt;- colnames(response)\ntimes.df &lt;- data.frame(times=factor(times))\n\n\\(\\blacksquare\\)\n\nFit the repeated-measures ANOVA. This will involve fitting an lm first, if you have not already done so.\n\nSolution\nFit the lm first, and then pass that into Manova from car:\n\nking.1 &lt;- lm(response ~ context, data = king)\nking.2 &lt;- Manova(king.1, idata = times.df, idesign = ~times)\nsummary(king.2)\n\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\ni1           1\ni2           1\ni3           1\ni4           1\ni5           1\ni6           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    24399650\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df    Pr(&gt;F)    \nPillai            1  0.913261 221.1069      1     21 1.273e-12 ***\nWilks             1  0.086739 221.1069      1     21 1.273e-12 ***\nHotelling-Lawley  1 10.528902 221.1069      1     21 1.273e-12 ***\nRoy               1 10.528902 221.1069      1     21 1.273e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: context \n\n Response transformation matrix:\n   (Intercept)\ni1           1\ni2           1\ni3           1\ni4           1\ni5           1\ni6           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     1611524\n\nMultivariate Tests: context\n                 Df test stat approx F num Df den Df    Pr(&gt;F)   \nPillai            2 0.4101695 7.301725      2     21 0.0039141 **\nWilks             2 0.5898305 7.301725      2     21 0.0039141 **\nHotelling-Lawley  2 0.6954024 7.301725      2     21 0.0039141 **\nRoy               2 0.6954024 7.301725      2     21 0.0039141 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3 times4 times5\ni1      1      0      0      0      0\ni2      0      1      0      0      0\ni3      0      0      1      0      0\ni4      0      0      0      1      0\ni5      0      0      0      0      1\ni6     -1     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n            times1      times2       times3      times4      times5\ntimes1 463982.0417 18075.41667 -17241.16667 -47691.2917  973.291667\ntimes2  18075.4167   704.16667   -671.66667  -1857.9167   37.916667\ntimes3 -17241.1667  -671.66667    640.66667   1772.1667  -36.166667\ntimes4 -47691.2917 -1857.91667   1772.16667   4902.0417 -100.041667\ntimes5    973.2917    37.91667    -36.16667   -100.0417    2.041667\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1  0.856997 20.37569      5     17 1.2326e-06 ***\nWilks             1  0.143003 20.37569      5     17 1.2326e-06 ***\nHotelling-Lawley  1  5.992849 20.37569      5     17 1.2326e-06 ***\nRoy               1  5.992849 20.37569      5     17 1.2326e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: context:times \n\n Response transformation matrix:\n   times1 times2 times3 times4 times5\ni1      1      0      0      0      0\ni2      0      1      0      0      0\ni3      0      0      1      0      0\ni4      0      0      0      1      0\ni5      0      0      0      0      1\ni6     -1     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1    times2    times3    times4    times5\ntimes1 40376.583 44326.333 28404.667 -6315.458  9666.458\ntimes2 44326.333 93153.083 56412.417  3080.292 24377.208\ntimes3 28404.667 56412.417 34289.083  1235.458 14606.042\ntimes4 -6315.458  3080.292  1235.458  3241.583  1586.167\ntimes5  9666.458 24377.208 14606.042  1586.167  6573.083\n\nMultivariate Tests: context:times\n                 Df test stat approx F num Df den Df    Pr(&gt;F)   \nPillai            2 0.8033948 2.417022     10     36 0.0256284 * \nWilks             2 0.3347058 2.476886     10     34 0.0237616 * \nHotelling-Lawley  2 1.5750953 2.520152     10     32 0.0230293 * \nRoy               2 1.2432100 4.475556      5     18 0.0079604 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n               Sum Sq num Df Error SS den Df  F value    Pr(&gt;F)    \n(Intercept)   4066608      1   386233     21 221.1069 1.273e-12 ***\ncontext        268587      2   386233     21   7.3017 0.0039141 ** \ntimes          407439      5   273592    105  31.2736 &lt; 2.2e-16 ***\ncontext:times   88901     10   273592    105   3.4119 0.0006746 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n              Test statistic  p-value\ntimes                0.24793 0.022324\ncontext:times        0.24793 0.022324\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n               GG eps Pr(&gt;F[GG])    \ntimes         0.70016  3.152e-14 ***\ncontext:times 0.70016   0.003269 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                 HF eps   Pr(&gt;F[HF])\ntimes         0.8573896 6.269000e-17\ncontext:times 0.8573896 1.422381e-03\n\n\n\\(\\blacksquare\\)\n\nWhat do you conclude from your repeated-measures ANOVA? Explain briefly, in the context of the data.\n\nSolution\nThe interaction term is significant, with a P-value less than 0.05. This is where we start and stop looking.\nTo be precise, the sphericity test fails (P-value 0.022), so that the P-value in the univariate test for interaction is too small, and we should trust the Huynh-Feldt one of 0.0014 below it. This is still significant, but not as significant as you might have thought. For comparison, the multivariate test P-values vary between 0.008 and 0.026, a bit bigger but still significant.\nThis means that the effect of time on motor activity (that is, the way the motor activity depends on time) is different for each context. That’s all we can say now. Grading note: as long as the setup and MANOVA are done somewhere, I don’t mind which part they are labelled with. But you need to do the setup, initial lm and Manova somewhere so that everything comes out right in the end.\n\\(\\blacksquare\\)\n\nTo understand the results of the previous part, we are going to make a spaghetti plot. In preparation for that, we need to save the data in “long format” with one observation on one time point in each row. Arrange that, and show by displaying (some of) the data that you have done so.\n\nSolution\nThis is pivot_longer yet again: gather up columns i1 through i6 and call them something like activity:\n\nking %&gt;% \n  pivot_longer(i1:i6, names_to=\"time\", values_to=\"activity\") -&gt; king.long\nking.long\n\n\n\n  \n\n\n\nDisplaying the top 10 rows of the resulting data frame is a good way to display “some of” it. You can always look at more if you like. There are more rows and fewer columns than there were before, which is encouraging. pivot_longer collects up all the time and activity values for the first rat, then the second, and so on.\n\\(\\blacksquare\\)\n\nMake a spaghetti plot: that is, plot motor activity against the time points, joining the points for each rat by lines, and colouring the points and lines according to the context.\n\nSolution\nThat means this, using group to indicate which points to join by lines, since it’s different from the colour:\n\nggplot(king.long, aes(x = time, y = activity, colour = context, group = id)) +\n  geom_point() + geom_line()\n\n\n\n\nI’d say the geom_point is optional, so that this is also good, perhaps better even:\n\nggplot(king.long, aes(x = time, y = activity, colour = context, group = id)) +\n  geom_line()\n\n\n\n\n\\(\\blacksquare\\)\n\nLooking at your spaghetti plot, why do you think your repeated-measures ANOVA came out as it did? Explain briefly.\n\nSolution\nWhat you’re after is an explanation of how the patterns over time are different for the three contexts. If you can find something that says that, I’m good. For example, even though all of the rats experienced a decrease in motor activity between times 1 and 2, the rats in the same group didn’t decrease as much. Or, the rats in the same group continued to decrease beyond time 2, whereas the rats in the control and different groups tended to level off after time 2, not decreasing so much after that. If you like, you can draw an interaction plot by working out the means for each context-time group first:\n\nking.long %&gt;%\n  group_by(context, time) %&gt;%\n  summarize(m = mean(activity)) %&gt;%\n  ggplot(aes(x = time, y = m, colour = context, group = context)) +\n  geom_point() + geom_line()\n\n`summarise()` has grouped output by 'context'. You can override using the\n`.groups` argument.\n\n\n\n\n\nThis seems to illustrate the same things as I found on the spaghetti plot. It gains in clarity by only looking at means, but loses by not considering the variability. Your call.\nThis kind of thing also runs with lmer from package lme4. It uses the long data frame, thus, treating id (identifying the rats) as a random effect:\n\nking.3 &lt;- lmer(activity ~ context * time + (1 | id), data = king.long)\n\nWhat can we drop? The only thing under consideration is the interaction:\n\ndrop1(king.3, test = \"Chisq\")\n\n\n\n  \n\n\n\nand we get the same conclusion as before, but with a much smaller P-value.\nWith this kind of modelling, there is no distinction between “within” and “between”, so that even though time is a within-subjects factor and context is between subjects, if the interaction had not been significant, we could have dropped it from the model, and then we would have had an effect of time and an effect of context, independent of each other. I was actually looking for an example with a non-significant interaction, but I couldn’t find one.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "repeated-measures.html#repeated-measures-with-no-background-1",
    "href": "repeated-measures.html#repeated-measures-with-no-background-1",
    "title": "31  Repeated measures",
    "section": "31.12 Repeated measures with no background",
    "text": "31.12 Repeated measures with no background\nNine people are randomly chosen to receive one of three treatments, labelled A, B and C. Each person has their response y to the treatment measured at three times, labelled T1, T2 and T3. The main aim of the study is to properly assess the effects of the treatments. A higher value of y is better.\nThe data are in link.\n\nThere are \\(9 \\times 3=27\\) observations of y in this study. Why would it be wrong to treat these as 27 independent observations? Explain briefly.\n\nSolution\nThere are only 9 people with 3 observations on each person. The three observations on the same person are likely to be correlated with each other, and so treating them as independent would be a mistake. This is repeated-measures data. If you say that, that’s useful, but you also need to demonstrate that you know what repeated measures means and why it needs to be handled differently from one-observation-per-individual data. Another way to look at it is that individuals will differ from each other, and so there ought to be an “individual” effect included in the model, in the same way that you would include a block effect in a randomized block design: not because you care about differences among individuals, but because you are pretty sure they’ll be there and you want to account for them.\n\\(\\blacksquare\\)\n\nRead in the data values. Are they tidy or untidy? Explain briefly. (The data values are separated by tabs, like the Australian athlete data.)\n\nSolution\nWe therefore need read_tsv. I’m not quite sure what to call this one:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/rm.txt\"\ntreatments &lt;- read_tsv(my_url)\n\nRows: 27 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): trt, time\ndbl (2): subject, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntreatments\n\n\n\n  \n\n\n\nFind a way to display what you have, so you can decide whether it is tidy or not. Each observation of y is in a row by itself, so this is tidy, or long format. You might even call this extra-tidy, because each person is spread over three rows, one for each time point. Looking ahead, this is ideal for making a graph, or for doing the advanced version of the analysis with lme4, but it is not so good for our MANOVA way of doing a repeated measures analysis. That we will have to prepare for.\n\\(\\blacksquare\\)\n\nMake a spaghetti plot: that is, a plot of y against time, with the observations for the same individual joined by lines which are coloured according to the treatment that individual received.\n\nSolution\nThe individuals are labelled in subject and the treatments are in trt, which means we need to do this:\n\nggplot(treatments, aes(x = time, y = y, colour = trt, group = subject)) +\n  geom_point() + geom_line()\n\n\n\n\nI’m going to be all smug and tell you that I got this right first time. (I’m telling you this because it doesn’t happen often.) If it didn’t come out that way for you, no stress: figure out what went wrong, fix it, and no one is any the wiser. On a spaghetti plot, you want to join together the points that belong to the same subject, and you want to colour them by treatment, so colour needs to be trt and group needs to be subject, since otherwise all the observations on the same treatment will get connected by lines, which is not what you want. You can use group in other contexts too, but for us we would normally distinguish groups by colour, and here we have two grouping variables (subject and treatment), which we want to distinguish in different ways.\n\\(\\blacksquare\\)\n\nOn your spaghetti plot, how do the values of y for the treatments compare over time?\n\nSolution\nThe most obvious thing is that the values of y all go up over time, regardless of which treatment they were from. At the initial time T1, the treatments are all about the same, but at the second and third time points, y is bigger for treatment C than for the other two treatments (which are about the same as each other). If you like, say that the gap between treatment C and the others is increasing over time, or that the lines for treatment C are steeper than for the other treatments. Any of those ways of saying it comes to the same conclusion. Extra: if you look at the lines of the same colour (treatment), they don’t seem to cross over very much. That suggests that an individual who starts with a larger value of y (relatively, compared to the other individuals on the same treatment) tends to stay larger than the other individuals on the same treatment all the way through. This would be another thing you’d see if the measurements for the individuals are correlated, or if there is an “individual effect” to go along with a treatment effect (and a time effect). If you think of this like an individual-level version of an interaction plot (which would be obtained by plotting the means for each treatment at each time), there is a suggestion here of an interaction between treatment and time, as well as a treatment effect (the latter because treatment C appears better than the rest).\n\\(\\blacksquare\\)\n\nExplain briefly how the data are in the wrong format for a repeated-measures ANOVA (done using MANOVA, as in class), and use pivot_wider to get the data set into the right format.\n\nSolution\nFor MANOVA, we want the three responses (here, the values of y at the three different times) in three separate columns, with all the measurements for one subject in one row (rather than on three separate rows, as here). pivot_wider is the flip-side of pivot_longer: instead of making different columns that all measure the same thing into one column, we split one column that contains things that are (slightly) different from each other (here, y at different times). It needs two inputs: names_from, the current single column that contains the column names you are going to make, and values_from, the values to carry along with them:\n\ntreatments %&gt;% pivot_wider(names_from = time, values_from = y) -&gt; tr2\ntr2\n\n\n\n  \n\n\n\n(I got this right the first time too. I must be having a good day!)\nTechnical detail: in pivot_longer, the column names are in quotes because they don’t exist yet, but in pivot_wider, they are not in quotes because they are columns that already exist.\nNote that the time and y columns have disappeared: the columns labelled with the time points are where those values of y have gone. The nine subjects make up the nine rows of the new “wide” data set, which is in the format we want.\n\\(\\blacksquare\\)\n\nRun a repeated-measures ANOVA the Manova way. What do you conclude from it?\n\nSolution\nCreate the response variable first, and use it in an lm:\n\nresponse &lt;- with(tr2, cbind(T1, T2, T3))\ntreatment.1 &lt;- lm(response ~ trt, data = tr2)\n\nNow we have to construct the within-subject stuff, for which we need to get the different times we have. You can type them in again (fine here), or get them from the response you just made:\n\ntimes &lt;- colnames(response)\ntimes.df &lt;- data.frame(times=factor(times))\n\nThis is where the possible time effect is accounted for. Because time is within-subjects (each subject is measured at several different times) but treatment is between subjects (each subject only gets one treatment), the two things have to be treated separately, in this approach at least.\nThen, uppercase-M Manova:\n\ntreatment.2 &lt;- Manova(treatment.1, idata = times.df, idesign = ~times)\nsummary(treatment.2)\n\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nT1           1\nT2           1\nT3           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    32520.11\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1    0.9975 2399.025      1      6 4.8568e-09 ***\nWilks             1    0.0025 2399.025      1      6 4.8568e-09 ***\nHotelling-Lawley  1  399.8374 2399.025      1      6 4.8568e-09 ***\nRoy               1  399.8374 2399.025      1      6 4.8568e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: trt \n\n Response transformation matrix:\n   (Intercept)\nT1           1\nT2           1\nT3           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    193.5556\n\nMultivariate Tests: trt\n                 Df test stat approx F num Df den Df   Pr(&gt;F)  \nPillai            2 0.7041229 7.139344      2      6 0.025902 *\nWilks             2 0.2958771 7.139344      2      6 0.025902 *\nHotelling-Lawley  2 2.3797814 7.139344      2      6 0.025902 *\nRoy               2 2.3797814 7.139344      2      6 0.025902 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2\nT1      1      0\nT2      0      1\nT3     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1    times2\ntimes1   2601 1258.0000\ntimes2   1258  608.4444\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            1    0.9988 2010.298      2      5 5.4369e-08 ***\nWilks             1    0.0012 2010.298      2      5 5.4369e-08 ***\nHotelling-Lawley  1  804.1190 2010.298      2      5 5.4369e-08 ***\nRoy               1  804.1190 2010.298      2      5 5.4369e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: trt:times \n\n Response transformation matrix:\n   times1 times2\nT1      1      0\nT2      0      1\nT3     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1   times2\ntimes1     72 38.00000\ntimes2     38 28.22222\n\nMultivariate Tests: trt:times\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            2  1.345125  6.16206      4     12 0.00620585 ** \nWilks             2  0.023398 13.84359      4     10 0.00043842 ***\nHotelling-Lawley  2 25.988095 25.98810      4      8 0.00012292 ***\nRoy               2 25.367215 76.10165      2      6 5.4552e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n             Sum Sq num Df Error SS den Df   F value    Pr(&gt;F)    \n(Intercept) 10840.0      1   27.111      6 2399.0246 4.857e-09 ***\ntrt            64.5      2   27.111      6    7.1393 0.0259021 *  \ntimes        1301.0      2   12.889     12  605.6207 8.913e-13 ***\ntrt:times      41.5      4   12.889     12    9.6552 0.0009899 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n          Test statistic  p-value\ntimes            0.29964 0.049149\ntrt:times        0.29964 0.049149\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n           GG eps Pr(&gt;F[GG])    \ntimes     0.58811  3.114e-08 ***\ntrt:times 0.58811   0.008332 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n             HF eps   Pr(&gt;F[HF])\ntimes     0.6461293 7.092455e-09\ntrt:times 0.6461293 6.137921e-03\n\n\n(Since I call things by the same names every time, my code for one of these looks a lot like my code for any of the others.)\nStart near the bottom with the Mauchly tests for sphericity. These are just significant, so that instead of looking at the univariate tests, look at the bottom for the Huynh-Feldt adjusted P-values, the one for the interaction being 0.0061. This is a bit bigger than the one for the interaction in the univariate tests (0.0010), the latter being deceptively small because sphericity is actually no good.\nCompare this, if you like, with the multivariate tests for the interaction. In this case, these are actually not all the same; the largest P-value is the Pillai one, which at 0.0062 is very close to the properly adjusted univariate test.\nFinally, interpretation. We look only at the interaction. This is significant, so the effect of treatment is different at the different times. And we stop there.\n\\(\\blacksquare\\)\n\nHow is your conclusion from the previous part consistent with your spaghetti plot? Explain briefly.\n\nSolution\nThe thing that the interaction means is that the effect of treatment is different over different times. That shows up in the spaghetti plot by treatment C being the same as the others at the beginning, but clearly better than the others at the later times. That is to say, you can’t talk about “an” effect of treatment, because whether or not treatment C is better than the others depends on which time you’re looking at.\nExtra: we used the MANOVA way of doing the repeated-measures analysis. There is another way, “mixed models”, which is in some ways less familiar and in some ways more.\nIn any analysis of variance, there are two kinds of effects of things you may care about: fixed effects and random effects. Fixed effects are things like the treatment and time here, where the ones you look at are the only ones you are interested in (in this study at least). If you had wanted to assess another treatment, you would have designed it into the study; if you cared about other times, you would have measured y at those times too. The subjects, though, are different: they are a random sample of all possible people, and you want your results to generalize to the population of all people of whom your subjects are a sample.20 So subjects are a different kind of thing and they have what are called random effects. When each subject only gives one measurement, as in all the things we’ve seen so far,21 it doesn’t matter how you treat (statistically) the subjects, but when each subject gives more than one measurement, it does matter. Which is why we have to do the idesign stuff in the MANOVA, or what you will see below.\nA model with both fixed and random effects is called a mixed model.\nWe’re going to make the assumption that the effect of being one subject rather than another is to move the value of y up or down by a fixed amount regardless of treatment or time, on average (each subject is different, but within a subject the random effect is the same size). That seems reasonable, given the spaghetti plot, where some subjects seemed to give consistently larger or smaller values of y than others. This is a so-called “random-intercepts” model. In the package lme4, there is a function lmer that looks like lm, except for the way in which you specify the random effects. It looks like this, noting that it works with the tidy data frame that we read in from the file and made the spaghetti plot out of:\n\ntreatment.4 &lt;- lmer(y ~ trt * time + (1 | subject), data = treatments)\ndrop1(treatment.4, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe way to read that model is “y depends on the combination of treatment and time and also on a random intercept for each subject”. This is the way in which the model captures the idea that each subject is different.\nYou don’t get a test for the random effects; you are assuming that the subjects will be different from each other and you want to adjust for that, rather than testing for it.22 All you get is tests for the fixed effects that are currently up for grabs, in this case the interaction, which is strongly significant.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "repeated-measures.html#footnotes",
    "href": "repeated-measures.html#footnotes",
    "title": "31  Repeated measures",
    "section": "",
    "text": "References: Box, George EP, Problems in the analysis of growth and wear curves, Biometrics vol. 6, 362-369, 1950; Christensen R, Advanced Linear Modeling, 2nd edition, Springer, 2001. Yes, that’s the Box-Cox Box.↩︎\nThis is the same Box as Box-Cox and the M test.↩︎\nReferences: Box, George EP, Problems in the analysis of growth and wear curves, Biometrics vol. 6, 362-369, 1950; Christensen R, Advanced Linear Modeling, 2nd edition, Springer, 2001. Yes, that’s the Box-Cox Box.↩︎\nThings would be a lot more complicated if each rat got a different drug at a different time! But the rats each got one drug once, at the beginning, and the issue was the effect of that drug on all the growth that followed.↩︎\nThat is to say, it’s the same kind of shape.↩︎\nI am kind of wrong about that, as we see.↩︎\nThis is rather like removing time zero in the example in lecture.↩︎\nThe factor treatment still has three levels, but only two of them have any remaining data.↩︎\nThere is often an effect of time, which is why you would be taking multiple time measurements, but the issue is when you take that into account, you are giving yourself an improved chance, in general, to find a treatment effect. This is exactly the same idea as using a matched pairs design to give yourself an improved chance of finding a treatment effect, even though the subjects might be quite different from each other. In fact, repeated measures is matched pairs with more than two measurements on each person. Which makes me think, I should have you do a matched pairs as repeated measures.↩︎\nI got sidetracked, surprise surprise.↩︎\nSee below.↩︎\nThis would be allowable, since we are averaging over the time-dependence; we are creating 10 independent averages, from the 10 subjects. People do this kind of thing, instead of having to deal with the repeated measures.↩︎\nThe term “accidental aRt” is sometimes used for graphs that cross the boundary between being informative and looking like a piece of art, particularly if it was not done on purpose. This one is a bit like that.↩︎\nThis is different from the 0.05 \\(\\alpha\\).↩︎\nThis is something I want to understand, so I will share my findings with you. You can read them or not, as you choose.↩︎\nI’m suspicious about when needing to be the numerical time inside. Not sure.↩︎\nI understand psychology students still do this kind of stuff by hand.↩︎\nMy hat stays on my head.↩︎\nThe movies of Lord of the Rings were filmed in New Zealand, which is also the country in which R was first designed.↩︎\nIn practice, things are usually fuzzier than this, because the subjects in your study are typically the ones you could get, rather than being a physical random sample of all possible people, but we usually act as if our subjects are a random sample of all possible subjects.↩︎\nIncluding matched pairs, because what we do there is to take the difference between the two measurements for each person and throw away the actual measurements themselves, so that each subject still only gives us one measurement.↩︎\nThis is rather like the test for blocks in a randomized block design: you want to allow for differences among blocks, but you don’t especially care to test that there are any. In fact, blocks are a lot like subjects, in that they are typically things like different experimental plots in which plants are grown, or different days on which the experiment is conducted, and you want to generalize from the blocks you observed, which are certainly not all possible blocks, to the population of all possible blocks.↩︎"
  },
  {
    "objectID": "discriminant-analysis.html#telling-whether-a-banknote-is-real-or-counterfeit",
    "href": "discriminant-analysis.html#telling-whether-a-banknote-is-real-or-counterfeit",
    "title": "32  Discriminant analysis",
    "section": "32.1 Telling whether a banknote is real or counterfeit",
    "text": "32.1 Telling whether a banknote is real or counterfeit\n* A Swiss bank collected a number of known counterfeit (fake) bills over time, and sampled a number of known genuine bills of the same denomination. Is it possible to tell, from measurements taken from a bill, whether it is genuine or not? We will explore that issue here. The variables measured were:\n\nlength\nright-hand width\nleft-hand width\ntop margin\nbottom margin\ndiagonal\n\n\nRead in the data from link, and check that you have 200 rows and 7 columns altogether.\nRun a multivariate analysis of variance. What do you conclude? Is it worth running a discriminant analysis? (This is the same procedure as with basic MANOVAs before.)\nRun a discriminant analysis. Display the output.\nHow many linear discriminants did you get? Is that making sense? Explain briefly.\n* Using your output from the discriminant analysis, describe how each of the linear discriminants that you got is related to your original variables. (This can, maybe even should, be done crudely: “does each variable feature in each linear discriminant: yes or no?”.)\nWhat values of your variable(s) would make LD1 large and positive?\n* Find the means of each variable for each group (genuine and counterfeit bills). You can get this from your fitted linear discriminant object.\nPlot your linear discriminant(s), however you like. Bear in mind that there is only one linear discriminant.\nWhat kind of score on LD1 do genuine bills typically have? What kind of score do counterfeit bills typically have? What characteristics of a bill, therefore, would you look at to determine if a bill is genuine or counterfeit?"
  },
  {
    "objectID": "discriminant-analysis.html#urine-and-obesity-what-makes-a-difference",
    "href": "discriminant-analysis.html#urine-and-obesity-what-makes-a-difference",
    "title": "32  Discriminant analysis",
    "section": "32.2 Urine and obesity: what makes a difference?",
    "text": "32.2 Urine and obesity: what makes a difference?\nA study was made of the characteristics of urine of young men. The men were classified into four groups based on their degree of obesity. (The groups are labelled a, b, c, d.) Four variables were measured, x (which you can ignore), pigment creatinine, chloride and chlorine. The data are in link as a .csv file. There are 45 men altogether.\nYes, you may have seen this one before. What you found was something like this, probably also with the Box M test (which has a P-value that is small, but not small enough to be a concern):\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/urine.csv\"\nurine &lt;- read_csv(my_url)\n\nRows: 45 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): obesity\ndbl (4): x, creatinine, chloride, chlorine\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nresponse &lt;- with(urine, cbind(creatinine, chlorine, chloride))\nurine.1 &lt;- manova(response ~ obesity, data = urine)\nsummary(urine.1)\n\n          Df  Pillai approx F num Df den Df  Pr(&gt;F)  \nobesity    3 0.43144   2.2956      9    123 0.02034 *\nResiduals 41                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOur aim is to understand why this result was significant.\n\nRead in the data again (copy the code from above) and obtain a discriminant analysis.\nHow many linear discriminants were you expecting? Explain briefly.\nWhy do you think we should pay attention to the first two linear discriminants but not the third? Explain briefly.\nPlot the first two linear discriminant scores (against each other), with each obesity group being a different colour.\n* Looking at your plot, discuss how (if at all) the discriminants separate the obesity groups. (Where does each obesity group fall on the plot?)\n* Obtain a table showing observed and predicted obesity groups. Comment on the accuracy of the predictions.\nDo your conclusions from (here) and (here) appear to be consistent?"
  },
  {
    "objectID": "discriminant-analysis.html#understanding-a-manova",
    "href": "discriminant-analysis.html#understanding-a-manova",
    "title": "32  Discriminant analysis",
    "section": "32.3 Understanding a MANOVA",
    "text": "32.3 Understanding a MANOVA\nOne use of discriminant analysis is to understand the results of a MANOVA. This question is a followup to a previous MANOVA that we did, the one with two variables y1 and y2 and three groups a through c. The data were in link.\n\nRead the data in again and run the MANOVA that you did before.\nRun a discriminant analysis “predicting” group from the two response variables. Display the output.\n* In the output from the discriminant analysis, why are there exactly two linear discriminants LD1 and LD2?\n* From the output, how would you say that the first linear discriminant LD1 compares in importance to the second one LD2: much more important, more important, equally important, less important, much less important? Explain briefly.\nObtain a plot of the discriminant scores.\nDescribe briefly how LD1 and/or LD2 separate the groups. Does your picture confirm the relative importance of LD1 and LD2 that you found back in part (here)? Explain briefly.\nWhat makes group a have a low score on LD1? There are two steps that you need to make: consider the means of group a on variables y1 and y2 and how they compare to the other groups, and consider how y1 and y2 play into the score on LD1.\nObtain predictions for the group memberships of each observation, and make a table of the actual group memberships against the predicted ones. How many of the observations were wrongly classified?"
  },
  {
    "objectID": "discriminant-analysis.html#what-distinguishes-people-who-do-different-jobs",
    "href": "discriminant-analysis.html#what-distinguishes-people-who-do-different-jobs",
    "title": "32  Discriminant analysis",
    "section": "32.4 What distinguishes people who do different jobs?",
    "text": "32.4 What distinguishes people who do different jobs?\n2441 people work at a certain company. They each have one of three jobs: customer service, mechanic, dispatcher. In the data set, these are labelled 1, 2 and 3 respectively. In addition, they each are rated on scales called outdoor, social and conservative. Do people with different jobs tend to have different scores on these scales, or, to put it another way, if you knew a person’s scores on outdoor, social and conservative, could you say something about what kind of job they were likely to hold? The data are in link.\n\nRead in the data and display some of it.\nNote the types of each of the variables, and create any new variables that you need to.\nRun a multivariate analysis of variance to convince yourself that there are some differences in scale scores among the jobs.\nRun a discriminant analysis and display the output.\nWhich is the more important, LD1 or LD2? How much more important? Justify your answer briefly.\nDescribe what values for an individual on the scales will make each of LD1 and LD2 high.\nThe first group of employees, customer service, have the highest mean on social and the lowest mean on both of the other two scales. Would you expect the customer service employees to score high or low on LD1? What about LD2?\nPlot your discriminant scores (which you will have to obtain first), and see if you were right about the customer service employees in terms of LD1 and LD2. The job names are rather long, and there are a lot of individuals, so it is probably best to plot the scores as coloured circles with a legend saying which colour goes with which job (rather than labelling each individual with the job they have).\n* Obtain predicted job allocations for each individual (based on their scores on the three scales), and tabulate the true jobs against the predicted jobs. How would you describe the quality of the classification? Is that in line with what the plot would suggest?\nConsider an employee with these scores: 20 on outdoor, 17 on social and 8 on conservative What job do you think they do, and how certain are you about that? Use predict, first making a data frame out of the values to predict for.\nSince I am not making you hand this one in, I’m going to keep going. Re-run the analysis to incorporate cross-validation, and make a table of the predicted group memberships. Is it much different from the previous one you had? Why would that be?"
  },
  {
    "objectID": "discriminant-analysis.html#observing-children-with-adhd",
    "href": "discriminant-analysis.html#observing-children-with-adhd",
    "title": "32  Discriminant analysis",
    "section": "32.5 Observing children with ADHD",
    "text": "32.5 Observing children with ADHD\nA number of children with ADHD were observed by their mother or their father (only one parent observed each child). Each parent was asked to rate occurrences of behaviours of four different types, labelled q1 through q4 in the data set. Also recorded was the identity of the parent doing the observation for each child: 1 is father, 2 is mother.\nCan we tell (without looking at the parent column) which parent is doing the observation? Research suggests that rating the degree of impairment in different categories depends on who is doing the rating: for example, mothers may feel that a child has difficulty sitting still, while fathers, who might do more observing of a child at play, might think of such a child as simply being “active” or “just being a kid”. The data are in link.\n\nRead in the data and confirm that you have four ratings and a column labelling the parent who made each observation.\nRun a suitable discriminant analysis and display the output.\nWhich behaviour item or items seem to be most helpful at distinguishing the parent making the observations? Explain briefly.\nObtain the predictions from the lda, and make a suitable plot of the discriminant scores, bearing in mind that you only have one LD. Do you think there will be any misclassifications? Explain briefly.\nObtain the predicted group memberships and make a table of actual vs. predicted. Were there any misclassifications? Explain briefly.\nRe-run the discriminant analysis using cross-validation, and again obtain a table of actual and predicted parents. Is the pattern of misclassification different from before? Hints: (i) Bear in mind that there is no predict step this time, because the cross-validation output includes predictions; (ii) use a different name for the predictions this time because we are going to do a comparison in a moment.\nDisplay the original data (that you read in from the data file) side by side with two sets of posterior probabilities: the ones that you obtained with predict before, and the ones from the cross-validated analysis. Comment briefly on whether the two sets of posterior probabilities are similar. Hints: (i) use data.frame rather than cbind, for reasons that I explain elsewhere; (ii) round the posterior probabilities to 3 decimals before you display them. There are only 29 rows, so look at them all. I am going to add the LD1 scores to my output and sort by that, but you don’t need to. (This is for something I am going to add later.)\nRow 17 of your (original) data frame above, row 5 of the output in the previous part, is the mother that was misclassified as a father. Why is it that the cross-validated posterior probabilities are 1 and 0, while the previous posterior probabilities are a bit less than 1 and a bit more than 0?\nFind the parents where the cross-validated posterior probability of being a father is “non-trivial”: that is, not close to zero and not close to 1. (You will have to make a judgement about what “close to zero or 1” means for you.) What do these parents have in common, all of them or most of them?"
  },
  {
    "objectID": "discriminant-analysis.html#growing-corn",
    "href": "discriminant-analysis.html#growing-corn",
    "title": "32  Discriminant analysis",
    "section": "32.6 Growing corn",
    "text": "32.6 Growing corn\nA new type of corn seed has been developed. The people developing it want to know if the type of soil the seed is planted in has an impact on how well the seed performs, and if so, what kind of impact. Three outcome measures were used: the yield of corn produced (from a fixed amount of seed), the amount of water needed, and the amount of herbicide needed. The data are in link. 32 fields were planted with the seed, 8 fields with each soil type.\n\nRead in the data and verify that you have 32 observations with the correct variables.\nRun a multivariate analysis of variance to see whether the type of soil has any effect on any of the variables. What do you conclude from it?\nRun a discriminant analysis on these data, “predicting” soil type from the three response variables. Display the results.\n* Which linear discriminants seem to be worth paying attention to? Why did you get three linear discriminants? Explain briefly.\nWhich response variables do the important linear discriminants depend on? Answer this by extracting something from your discriminant analysis output.\nObtain predictions for the discriminant analysis. (You don’t need to do anything with them yet.)\nPlot the first two discriminant scores against each other, coloured by soil type. You’ll have to start by making a data frame containing what you need.\nOn your plot that you just made, explain briefly how LD1 distinguishes at least one of the soil types.\nOn your plot, does LD2 appear to do anything to separate the groups? Is this surprising given your earlier findings? Explain briefly.\nMake a table of actual and predicted soil group. Which soil type was classified correctly the most often?"
  },
  {
    "objectID": "discriminant-analysis.html#understanding-athletes-height-weight-sport-and-gender",
    "href": "discriminant-analysis.html#understanding-athletes-height-weight-sport-and-gender",
    "title": "32  Discriminant analysis",
    "section": "32.7 Understanding athletes’ height, weight, sport and gender",
    "text": "32.7 Understanding athletes’ height, weight, sport and gender\nOn a previous assignment, we used MANOVA on the athletes data to demonstrate that there was a significant relationship between the combination of the athletes’ height and weight, with the sport they play and the athlete’s gender. The problem with MANOVA is that it doesn’t give any information about the kind of relationship. To understand that, we need to do discriminant analysis, which is the purpose of this question.\nThe data can be found at link.\n\nOnce again, read in and display (some of) the data, bearing in mind that the data values are separated by tabs. (This ought to be a free two marks.)\nUse unite to make a new column in your data frame which contains the sport-gender combination. Display it. (You might like to display only a few columns so that it is clear that you did the right thing.) Hint: you’ve seen unite in the peanuts example in class.\nRun a discriminant analysis “predicting” sport-gender combo from height and weight. Display the results. (No comment needed yet.)\nWhat kind of height and weight would make an athlete have a large (positive) score on LD1? Explain briefly.\nMake a guess at the sport-gender combination that has the highest score on LD1. Why did you choose the combination you did?\n What combination of height and weight would make an athlete have a small* (that is, very negative) score on LD2? Explain briefly.\nObtain predictions for the discriminant analysis, and use these to make a plot of LD1 score against LD2 score, with the individual athletes distinguished by what sport they play and gender they are. (You can use colour to distinguish them, or you can use shapes. If you want to go the latter way, there are clues in my solutions to the MANOVA question about these athletes.)\nLook on your graph for the four athletes with the smallest (most negative) scores on LD2. What do they have in common? Does this make sense, given your answer to part (here)? Explain briefly.\nObtain a (very large) square table, or a (very long) table with frequencies, of actual and predicted sport-gender combinations. You will probably have to make the square table very small to fit it on the page. For that, displaying the columns in two or more sets is OK (for example, six columns and all the rows, six more columns and all the rows, then the last five columns for all the rows). Are there any sport-gender combinations that seem relatively easy to classify correctly? Explain briefly.\n\nMy solutions follow:"
  },
  {
    "objectID": "discriminant-analysis.html#telling-whether-a-banknote-is-real-or-counterfeit-1",
    "href": "discriminant-analysis.html#telling-whether-a-banknote-is-real-or-counterfeit-1",
    "title": "32  Discriminant analysis",
    "section": "32.8 Telling whether a banknote is real or counterfeit",
    "text": "32.8 Telling whether a banknote is real or counterfeit\n* A Swiss bank collected a number of known counterfeit (fake) bills over time, and sampled a number of known genuine bills of the same denomination. Is it possible to tell, from measurements taken from a bill, whether it is genuine or not? We will explore that issue here. The variables measured were:\n\nlength\nright-hand width\nleft-hand width\ntop margin\nbottom margin\ndiagonal\n\n\nRead in the data from link, and check that you have 200 rows and 7 columns altogether.\n\nSolution\nCheck the data file first. It’s aligned in columns, thus:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/swiss1.txt\"\nswiss &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  length = col_double(),\n  left = col_double(),\n  right = col_double(),\n  bottom = col_double(),\n  top = col_double(),\n  diag = col_double(),\n  status = col_character()\n)\n\nswiss\n\n\n\n  \n\n\n\nYep, 200 rows and 7 columns.\n\\(\\blacksquare\\)\n\nRun a multivariate analysis of variance. What do you conclude? Is it worth running a discriminant analysis? (This is the same procedure as with basic MANOVAs before.)\n\nSolution\nSmall-m manova will do here:\n\nresponse &lt;- with(swiss, cbind(length, left, right, bottom, top, diag))\nswiss.1 &lt;- manova(response ~ status, data = swiss)\nsummary(swiss.1)\n\n           Df  Pillai approx F num Df den Df    Pr(&gt;F)    \nstatus      1 0.92415   391.92      6    193 &lt; 2.2e-16 ***\nResiduals 198                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(BoxM(response, swiss$status))\n\n       Box's M Test \n\nChi-Squared Value = 121.8991 , df = 21  and p-value: 3.2e-16 \n\n\nThat is a very significant Box’s M test, which means that we shouldn’t trust the MANOVA at all. It is only the fact that the MANOVA is so significant that provides any evidence that the discriminant analysis is worth doing.\nExtra: you might be wondering whether you had to go to all that trouble to make the response variable. Would this work?\n\nresponse2 &lt;- swiss %&gt;% select(length:diag)\nswiss.1a &lt;- manova(response2 ~ status, data = swiss)\n\nError in model.frame.default(formula = response2 ~ status, data = swiss, : invalid type (list) for variable 'response2'\n\n\nNo, because response2 needs to be an R matrix, and it isn’t:\n\nclass(response2)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe error message was a bit cryptic (nothing unusual there), but a data frame (to R) is a special kind of list, so that R didn’t like response2 being a data frame, which it thought was a list.\nThis, however, works, since it turns the data frame into a matrix:\n\nresponse4 &lt;- swiss %&gt;% select(length:diag) %&gt;% as.matrix()\nswiss.2a &lt;- manova(response4 ~ status, data = swiss)\nsummary(swiss.2a)\n\n           Df  Pillai approx F num Df den Df    Pr(&gt;F)    \nstatus      1 0.92415   391.92      6    193 &lt; 2.2e-16 ***\nResiduals 198                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnyway, the conclusion: the status of a bill (genuine or counterfeit) definitely has an influence on some or all of those other variables, since the P-value \\(2.2 \\times 10^{-16}\\) (or less) is really small. So it is apparently worth running a discriminant analysis to figure out where the differences lie.\nAs a piece of strategy, for creating the response matrix, you can always either use cbind, which creates a matrix directly, or you can use select, which is often easier but creates a data frame, and then turn that into a matrix using as.matrix. As long as you end up with a matrix, it’s all good.\n\\(\\blacksquare\\)\n\nRun a discriminant analysis. Display the output.\n\nSolution\nNow we forget about all that response stuff. For a discriminant analysis, the grouping variable (or combination of the grouping variables) is the “response”, and the quantitative ones are “explanatory”:\n\nswiss.3 &lt;- lda(status ~ length + left + right + bottom + top + diag, data = swiss)\nswiss.3\n\nCall:\nlda(status ~ length + left + right + bottom + top + diag, data = swiss)\n\nPrior probabilities of groups:\ncounterfeit     genuine \n        0.5         0.5 \n\nGroup means:\n             length    left   right bottom    top    diag\ncounterfeit 214.823 130.300 130.193 10.530 11.133 139.450\ngenuine     214.969 129.943 129.720  8.305 10.168 141.517\n\nCoefficients of linear discriminants:\n                LD1\nlength  0.005011113\nleft    0.832432523\nright  -0.848993093\nbottom -1.117335597\ntop    -1.178884468\ndiag    1.556520967\n\n\n\\(\\blacksquare\\)\n\nHow many linear discriminants did you get? Is that making sense? Explain briefly.\n\nSolution\nI got one discriminant, which makes sense because there are two groups, and the smaller of 6 (variables, not counting the grouping one) and \\(2-1\\) is 1.\n\\(\\blacksquare\\)\n\n* Using your output from the discriminant analysis, describe how each of the linear discriminants that you got is related to your original variables. (This can, maybe even should, be done crudely: “does each variable feature in each linear discriminant: yes or no?”.)\n\nSolution\nThis is the Coefficients of Linear Discriminants. Make a call about whether each of those coefficients is close to zero (small in size compared to the others), or definitely positive or definitely negative. These are judgement calls: either you can say that LD1 depends mainly on diag (treating the other coefficients as “small” or close to zero), or you can say that LD1 depends on everything except length.\n\\(\\blacksquare\\)\n\nWhat values of your variable(s) would make LD1 large and positive?\n\nSolution\nDepending on your answer to the previous part: If you said that only diag was important, diag being large would make LD1 large and positive. If you said that everything but length was important, then it’s a bit more complicated: left and diag large, right, bottom and top small (since their coefficients are negative).\n\\(\\blacksquare\\)\n\n* Find the means of each variable for each group (genuine and counterfeit bills). You can get this from your fitted linear discriminant object.\n\nSolution\n\nswiss.3$means\n\n             length    left   right bottom    top    diag\ncounterfeit 214.823 130.300 130.193 10.530 11.133 139.450\ngenuine     214.969 129.943 129.720  8.305 10.168 141.517\n\n\n\\(\\blacksquare\\)\n\nPlot your linear discriminant(s), however you like. Bear in mind that there is only one linear discriminant.\n\nSolution\nWith only one linear discriminant, we can plot LD1 scores on the \\(y\\)-axis and the grouping variable on the \\(x\\)-axis. How you do that is up to you.\nBefore we start, though, we need the LD1 scores. This means doing predictions. The discriminant scores are in there. We take the prediction output and make a data frame with all the things in the original data. My current preference (it changes) is to store the predictions, and then cbind them with the original data, thus:\n\nswiss.pred &lt;- predict(swiss.3)\nd &lt;- cbind(swiss, swiss.pred)\nhead(d)\n\n\n\n  \n\n\n\nI needed head because cbind makes an old-fashioned data.frame rather than a tibble, so if you display it, you get all of it.\nThis gives the LD1 scores, predicted groups, and posterior probabilities as well. That saves us having to pick out the other things later. The obvious thing is a boxplot. By examining d above (didn’t you?), you saw that the LD scores were in a column called LD1:\n\nggplot(d, aes(x = status, y = LD1)) + geom_boxplot()\n\n\n\n\nThis shows that positive LD1 scores go (almost without exception) with genuine bills, and negative ones with counterfeit bills. It also shows that there are three outlier bills, two counterfeit ones with unusually high LD1 score, and one genuine one with unusually low LD1 score, at least for a genuine bill.\nThis goes to show that (the Box M test notwithstanding) the two types of bill really are different in a way that is worth investigating.\nOr you could do faceted histograms of LD1 by status:\n\nggplot(d, aes(x = LD1)) + geom_histogram(bins = 10) + facet_grid(status ~ .)\n\n\n\n\nThis shows much the same thing as plot(swiss.3) does (try it).\n\\(\\blacksquare\\)\n\nWhat kind of score on LD1 do genuine bills typically have? What kind of score do counterfeit bills typically have? What characteristics of a bill, therefore, would you look at to determine if a bill is genuine or counterfeit?\n\nSolution\nThe genuine bills almost all have a positive score on LD1, while the counterfeit ones all have a negative one. This means that the genuine bills (depending on your answer to (here)) have a large diag, or they have a large left and diag, and a small right, bottom and top. If you look at your table of means in (here), you’ll see that the genuine bills do indeed have a large diag, or, depending on your earlier answer, a small right, bottom and top, but not actually a small left (the left values are very close for the genuine and counterfeit coins).\nExtra: as to that last point, this is easy enough to think about. A boxplot seems a nice way to display it:\n\nggplot(d, aes(y = left, x = status)) + geom_boxplot()\n\n\n\n\nThere is a fair bit of overlap: the median is higher for the counterfeit bills, but the highest value actually belongs to a genuine one.\nCompare that to diag:\n\nggplot(d, aes(y = diag, x = status)) + geom_boxplot()\n\n\n\n\nHere, there is an almost complete separation of the genuine and counterfeit bills, with just one low outlier amongst the genuine bills spoiling the pattern. I didn’t look at the predictions (beyond the discriminant scores), since this question (as set on an assignment a couple of years ago) was already too long, but there is no difficulty in doing so. Everything is in the data frame I called d:\n\nwith(d, table(obs = status, pred = class))\n\n             pred\nobs           counterfeit genuine\n  counterfeit         100       0\n  genuine               1      99\n\n\n(this labels the rows and columns, which is not necessary but is nice.)\nThe tidyverse way is to make a data frame out of the actual and predicted statuses, and then count what’s in there:\n\nd %&gt;% count(status, class)\n\n\n\n  \n\n\n\nThis gives a “long” table, with frequencies for each of the combinations for which anything was observed.\nFrequency tables are usually wide, and we can make this one so by pivot-wider-ing pred:\n\nd %&gt;%\n  count(status, class) %&gt;%\n  pivot_wider(names_from = class, values_from = n)\n\n\n\n  \n\n\n\nOne of the genuine bills is incorrectly classified as a counterfeit one (evidently that low outlier on LD1), but every single one of the counterfeit bills is classified correctly. That missing value is actually a frequency that is zero, which you can fix up thus:\n\nd %&gt;%\n  count(status, class) %&gt;%\n  pivot_wider(names_from = class, values_from = n, values_fill = 0) \n\n\n\n  \n\n\n\nwhich turns any missing values into the zeroes they should be in this kind of problem. It would be interesting to see what the posterior probabilities look like for that misclassified bill:\n\nd %&gt;% filter(status != class)\n\n\n\n  \n\n\n\nOn the basis of the six measured variables, this looks a lot more like a counterfeit bill than a genuine one. Are there any other bills where there is any doubt? One way to find out is to find the maximum of the two posterior probabilities. If this is small, there is some doubt about whether the bill is real or fake. 0.99 seems like a very stringent cutoff, but let’s try it and see:\n\nd %&gt;%\n  mutate(max.post = pmax(posterior.counterfeit, posterior.genuine)) %&gt;%\n  filter(max.post &lt; 0.99) %&gt;%\n  dplyr::select(-c(length:diag))\n\n\n\n  \n\n\n\nThe only one is the bill that was misclassified: it was actually genuine, but was classified as counterfeit. The posterior probabilities say that it was pretty unlikely to be genuine, but it was the only bill for which there was any noticeable doubt at all.\nI had to use pmax rather than max there, because I wanted max.post to contain the larger of the two corresponding entries: that is, the first entry in max.post is the larger of the first entry of counterfeit and the first entry in genuine. If I used max instead, I’d get the largest of all the entries in counterfeit and all the entries in genuine, repeated 200 times. (Try it and see.) pmax stands for “parallel maximum”, that is, for each row separately. This also should work:\n\nd %&gt;%\n  rowwise() %&gt;% \n  mutate(max.post = max(posterior.counterfeit, posterior.genuine)) %&gt;%\n  filter(max.post &lt; 0.99) %&gt;%\n  select(-c(length:diag))\n\n\n\n  \n\n\n\nBecause we’re using rowwise, max is applied to the pairs of values of posterior.counterfeit and posterior.genuine, taken one row at a time.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "discriminant-analysis.html#urine-and-obesity-what-makes-a-difference-1",
    "href": "discriminant-analysis.html#urine-and-obesity-what-makes-a-difference-1",
    "title": "32  Discriminant analysis",
    "section": "32.9 Urine and obesity: what makes a difference?",
    "text": "32.9 Urine and obesity: what makes a difference?\nA study was made of the characteristics of urine of young men. The men were classified into four groups based on their degree of obesity. (The groups are labelled a, b, c, d.) Four variables were measured, x (which you can ignore), pigment creatinine, chloride and chlorine. The data are in link as a .csv file. There are 45 men altogether.\nYes, you may have seen this one before. What you found was something like this:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/urine.csv\"\nurine &lt;- read_csv(my_url)\n\nRows: 45 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): obesity\ndbl (4): x, creatinine, chloride, chlorine\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nresponse &lt;- with(urine, cbind(creatinine, chlorine, chloride))\nurine.1 &lt;- manova(response ~ obesity, data = urine)\nsummary(urine.1)\n\n          Df  Pillai approx F num Df den Df  Pr(&gt;F)  \nobesity    3 0.43144   2.2956      9    123 0.02034 *\nResiduals 41                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(BoxM(response, urine$obesity))\n\n       Box's M Test \n\nChi-Squared Value = 30.8322 , df = 18  and p-value: 0.0301 \n\n\nOur aim is to understand why this result was significant. (Remember that the P-value on Box’s M test is not small enough to be worried about.)\n\nRead in the data again (copy the code from above) and obtain a discriminant analysis.\n\nSolution\nAs above, plus:\n\nurine.1 &lt;- lda(obesity ~ creatinine + chlorine + chloride, data = urine)\nurine.1\n\nCall:\nlda(obesity ~ creatinine + chlorine + chloride, data = urine)\n\nPrior probabilities of groups:\n        a         b         c         d \n0.2666667 0.3111111 0.2444444 0.1777778 \n\nGroup means:\n  creatinine chlorine chloride\na   15.89167 5.275000 6.012500\nb   17.82143 7.450000 5.214286\nc   16.34545 8.272727 5.372727\nd   11.91250 9.675000 3.981250\n\nCoefficients of linear discriminants:\n                   LD1        LD2         LD3\ncreatinine  0.24429462 -0.1700525 -0.02623962\nchlorine   -0.02167823 -0.1353051  0.11524045\nchloride    0.23805588  0.3590364  0.30564592\n\nProportion of trace:\n   LD1    LD2    LD3 \n0.7476 0.2430 0.0093 \n\n\n\\(\\blacksquare\\)\n\nHow many linear discriminants were you expecting? Explain briefly.\n\nSolution\nThere are 3 variables and 4 groups, so the smaller of 3 and \\(4-1=3\\): that is, 3.\n\\(\\blacksquare\\)\n\nWhy do you think we should pay attention to the first two linear discriminants but not the third? Explain briefly.\n\nSolution\nThe first two “proportion of trace” values are a lot bigger than the third (or, the third one is close to 0).\n\\(\\blacksquare\\)\n\nPlot the first two linear discriminant scores (against each other), with each obesity group being a different colour.\n\nSolution\nFirst obtain the predictions, and then make a data frame out of the original data and the predictions.\n\nurine.pred &lt;- predict(urine.1)\nd &lt;- cbind(urine, urine.pred)\nhead(d)\n\n\n\n  \n\n\n\nurine produced the first five columns and urine.pred produced the rest.\nTo go a more tidyverse way, we can combine the original data frame and the predictions using bind_cols, but we have to be more careful that the things we are gluing together are both data frames:\n\nclass(urine)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\nclass(urine.pred)\n\n[1] \"list\"\n\n\nurine is a tibble all right, but urine.pred is a list. What does it look like?\n\nglimpse(urine.pred)\n\nList of 3\n $ class    : Factor w/ 4 levels \"a\",\"b\",\"c\",\"d\": 2 1 2 2 1 1 3 3 1 1 ...\n $ posterior: num [1:45, 1:4] 0.233 0.36 0.227 0.294 0.477 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:45] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:4] \"a\" \"b\" \"c\" \"d\"\n $ x        : num [1:45, 1:3] 0.393 -0.482 0.975 2.188 2.018 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:45] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:3] \"LD1\" \"LD2\" \"LD3\"\n\n\nA data frame is a list for which all the items are the same length, but some of the things in here are matrices. You can tell because they have a number of rows, 45, and a number of columns, 3 or 4. They do have the right number of rows, though, so something like as.data.frame (a base R function) will smoosh them all into one data frame, grabbing the columns from the matrices:\n\nhead(as.data.frame(urine.pred))\n\n\n\n  \n\n\n\nYou see that the columns that came from matrices have gained two-part names, the first part from the name of the matrix, the second part from the column name within that matrix. Then we can do this:\n\ndd &lt;- bind_cols(urine, as.data.frame(urine.pred))\ndd\n\n\n\n  \n\n\n\nIf you want to avoid base R altogether, though, and go straight to bind_cols, you have to be more careful about the types of things. bind_cols only works with vectors and data frames, not matrices, so that is what it is up to you to make sure you have. That means pulling out the pieces, turning them from matrices into data frames, and then gluing everything back together:\n\npost &lt;- as_tibble(urine.pred$posterior)\nld &lt;- as_tibble(urine.pred$x)\nddd &lt;- bind_cols(urine, class = urine.pred$class, ld, post)\nddd\n\n\n\n  \n\n\n\nThat’s a lot of work, but you might say that it’s worth it because you are now absolutely sure what kind of thing everything is. I also had to be slightly careful with the vector of class values; in ddd it has to have a name, so I have to make sure I give it one.2 Any of these ways (in general) is good. The last way is a more careful approach, since you are making sure things are of the right type rather than relying on R to convert them for you, but I don’t mind which way you go. Now make the plot, making sure that you are using columns with the right names. I’m using my first data frame, with the two-part names:\n\nggplot(d, aes(x = x.LD1, y = x.LD2, colour = obesity)) + geom_point()\n\n\n\n\n\\(\\blacksquare\\)\n\n* Looking at your plot, discuss how (if at all) the discriminants separate the obesity groups. (Where does each obesity group fall on the plot?)\n\nSolution\nMy immediate reaction was “they don’t much”. If you look a bit more closely, the b group, in green, is on the right (high LD1) and the d group (purple) is on the left (low LD1). The a group, red, is mostly at the top (high LD2) but the c group, blue, really is all over the place.\nThe way to tackle interpreting a plot like this is to look for each group individually and see if that group is only or mainly found on a certain part of the plot.\nThis can be rationalized by looking at the “coefficients of linear discriminants” on the output. LD1 is low if creatinine and chloride are low (it has nothing much to do with chlorine since that coefficient is near zero). Group d is lowest on both creatinine and chloride, so that will be lowest on LD1. LD2 is high if chloride is high, or creatinine and chlorine are low. Out of the groups a, b, c, a has the highest mean on chloride and lowest means on the other two variables, so this should be highest on LD2 and (usually) is.\nLooking at the means is only part of the story; if the individuals within a group are very variable, as they are here (especially group c), then that group will appear all over the plot. The table of means only says how the average individual within a group stacks up.\n\nggbiplot(urine.1, groups = urine$obesity)\n\n\n\n\nThis shows (in a way that is perhaps easier to see) how the linear discriminants are related to the original variables, and thus how the groups differ in terms of the original variables.3 Most of the B’s are high creatinine and high chloride (on the right); most of the D’s are low on both (on the left). LD2 has a bit of chloride, but not much of anything else. Extra: the way we used to do this was with “base graphics”, which involved plotting the lda output itself:\n\nplot(urine.1)\n\n\n\n\nwhich is a plot of each discriminant score against each other one. You can plot just the first two, like this:\n\nplot(urine.1, dimen = 2)\n\n\n\n\nThis is easier than using ggplot, but (i) less flexible and (ii) you have to figure out how it works rather than doing things the standard ggplot way. So I went with constructing a data frame from the predictions, and then ggplotting that. It’s a matter of taste which way is better.\n\\(\\blacksquare\\)\n\n* Obtain a table showing observed and predicted obesity groups. Comment on the accuracy of the predictions.\n\nSolution\nMake a table, one way or another:\n\ntab &lt;- with(d, table(obesity, class))\ntab\n\n       class\nobesity a b c d\n      a 7 3 2 0\n      b 2 9 2 1\n      c 3 4 1 3\n      d 2 0 1 5\n\n\nclass is always the predicted group in these. You can also name things in table. Or, if you prefer (equally good), the tidyverse way of counting all the combinations of true obesity and predicted class, which can be done all in one go, or in two steps by saving the data frame first. I’m saving my results for later:\n\nd %&gt;% count(obesity, class) -&gt; tab\ntab\n\n\n\n  \n\n\n\nor if you prefer to make it look more like a table of frequencies:\n\ntab %&gt;% pivot_wider(names_from=class, values_from=n, values_fill = list(n=0))\n\n\n\n  \n\n\n\nThe thing on the end fills in zero frequencies as such (they would otherwise be NA, which they are not: we know they are zero). My immediate reaction to this is “it’s terrible”! But at least some of the men have their obesity group correctly predicted: 7 of the \\(7+3+2+0=12\\) men that are actually in group a are predicted to be in a; 9 of the 14 actual b’s are predicted to be b’s; 5 of the 8 actual d’s are predicted to be d’s. These are not so awful. But only 1 of the 11 c’s is correctly predicted to be a c!\nAs for what I want to see: I am looking for some kind of statement about how good you think the predictions are (the word “terrible” is fine for this) with some kind of support for your statement. For example, “the predictions are not that good, but at least group B is predicted with some accuracy (9 out of 14).”\nI think looking at how well the individual groups were predicted is the most incisive way of getting at this, because the c men are the hardest to get right and the others are easier, but you could also think about an overall misclassification rate. This comes most easily from the “tidy” table:\n\ntab %&gt;% count(correct = (obesity == class), wt = n)\n\n\n\n  \n\n\n\nYou can count anything, not just columns that already exist. This one is a kind of combined mutate-and-count to create the (logical) column called correct.\nIt’s a shortcut for this:\n\ntab %&gt;%\n  mutate(is_correct = (obesity == class)) %&gt;%\n  count(is_correct, wt = n)\n\n\n\n  \n\n\n\nIf I don’t put the wt, count counts the number of rows for which the true and predicted obesity group is the same. But that’s not what I want here: I want the number of observations totalled up, which is what the wt= does. It says “use the things in the given column as weights”, which means to total them up rather than count up the number of rows.\nThis says that 22 men were classified correctly and 23 were gotten wrong. We can find the proportions correct and wrong:\n\ntab %&gt;%\n  count(correct = (obesity == class), wt = n) %&gt;%\n  mutate(proportion = n / sum(n))\n\n\n\n  \n\n\n\nand we see that 51% of men had their obesity group predicted wrongly. This is the overall misclassification rate, which is a simple summary of how good a job the discriminant analysis did.\nThere is a subtlety here. n has changed its meaning in the middle of this calculation! In tab, n is counting the number of obesity observed and predicted combinations, but now it is counting the number of men classified correctly and incorrectly. The wt=n uses the first n, but the mutate line uses the new n, the result of the count line here. (I think count used to use nn for the result of the second count, so that you could tell them apart, but it no longer seems to do so.)\nI said above that the obesity groups were not equally easy to predict. A small modification of the above will get the misclassification rates by (true) obesity group. This is done by putting an appropriate group_by in at the front, before we do any summarizing:\n\ntab %&gt;%\n  group_by(obesity) %&gt;%\n  count(correct = (obesity == class), wt = n) %&gt;%\n  mutate(proportion = n / sum(n))\n\n\n\n  \n\n\n\nThis gives the proportion wrong and correct for each (true) obesity group. I’m going to do the one more cosmetic thing to make it easier to read, a kind of “untidying”:\n\ntab %&gt;%\n  group_by(obesity) %&gt;%\n  count(correct = (obesity == class), wt = n) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  select(-n) %&gt;%\n  pivot_wider(names_from=correct, values_from=proportion)\n\n\n\n  \n\n\n\nLooking down the TRUE column, groups A, B and D were gotten about 60% correct (and 40% wrong), but group C is much worse. The overall misclassification rate is made bigger by the fact that C is so hard to predict.\nFind out for yourself what happens if I fail to remove the n column before doing the pivot_wider.\nA slightly more elegant look is obtained this way, by making nicer values than TRUE and FALSE:\n\ntab %&gt;%\n  group_by(obesity) %&gt;%\n  mutate(prediction_stat = ifelse(obesity == class, \"correct\", \"wrong\")) %&gt;%\n  count(prediction_stat, wt = n) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  select(-n) %&gt;%\n  pivot_wider(names_from=prediction_stat, values_from=proportion)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nDo your conclusions from (here) and (here) appear to be consistent?\n\nSolution\nOn the plot of (here), we said that there was a lot of scatter, but that groups a, b and d tended to be found at the top, right and left respectively of the plot. That suggests that these three groups should be somewhat predictable. The c’s, on the other hand, were all over the place on the plot, and were mostly predicted wrong.\nThe idea is that the stories you pull from the plot and the predictions should be more or less consistent. There are several ways you might say that: another approach is to say that the observations are all over the place on the plot, and the predictions are all bad. This is not as insightful as my comments above, but if that’s what the plot told you, that’s what the predictions would seem to be saying as well. (Or even, the predictions are not so bad compared to the apparently random pattern on the plot, if that’s what you saw. There are different ways to say something more or less sensible.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "discriminant-analysis.html#understanding-a-manova-1",
    "href": "discriminant-analysis.html#understanding-a-manova-1",
    "title": "32  Discriminant analysis",
    "section": "32.10 Understanding a MANOVA",
    "text": "32.10 Understanding a MANOVA\nOne use of discriminant analysis is to understand the results of a MANOVA. This question is a followup to a previous MANOVA that we did, the one with two variables y1 and y2 and three groups a through c. The data were in link.\n\nRead the data in again and run the MANOVA that you did before.\n\nSolution\nThis is an exact repeat of what you did before:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/simple-manova.txt\"\n# my_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/simple-manova.txt\"\nsimple &lt;- read_delim(my_url, \" \")\n\nRows: 12 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): group\ndbl (2): y1, y2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsimple\n\n\n\n  \n\n\nresponse &lt;- with(simple, cbind(y1, y2))\nsimple.3 &lt;- manova(response ~ group, data = simple)\nsummary(simple.3)\n\n          Df Pillai approx F num Df den Df    Pr(&gt;F)    \ngroup      2 1.3534   9.4196      4     18 0.0002735 ***\nResiduals  9                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis P-value is small, so there is some way in which some of the groups differ on some of the variables.4\nWe should check that we believe this, using Box’s M test:5\n\nsummary(BoxM(response, simple$group))\n\n       Box's M Test \n\nChi-Squared Value = 3.517357 , df = 6  and p-value: 0.742 \n\n\nThere is no problem here: no evidence that any of the response variables differ in spread across the groups.\n\\(\\blacksquare\\)\n\nRun a discriminant analysis “predicting” group from the two response variables. Display the output.\n\nSolution\nThis:\n\nsimple.4 &lt;- lda(group ~ y1 + y2, data = simple)\nsimple.4\n\nCall:\nlda(group ~ y1 + y2, data = simple)\n\nPrior probabilities of groups:\n        a         b         c \n0.3333333 0.2500000 0.4166667 \n\nGroup means:\n        y1  y2\na 3.000000 4.0\nb 4.666667 7.0\nc 8.200000 6.4\n\nCoefficients of linear discriminants:\n         LD1        LD2\ny1 0.7193766  0.4060972\ny2 0.3611104 -0.9319337\n\nProportion of trace:\n   LD1    LD2 \n0.8331 0.1669 \n\n\nNote that this is the other way around from MANOVA: here, we are “predicting the group” from the response variables, in the same manner as one of the flavours of logistic regression: “what makes the groups different, in terms of those response variables?”.\n\\(\\blacksquare\\)\n\n* In the output from the discriminant analysis, why are there exactly two linear discriminants LD1 and LD2?\n\nSolution\nThere are two linear discriminants because there are 3 groups and two variables, so there are the smaller of \\(3-1\\) and 2 discriminants.\n\\(\\blacksquare\\)\n\n* From the output, how would you say that the first linear discriminant LD1 compares in importance to the second one LD2: much more important, more important, equally important, less important, much less important? Explain briefly.\n\nSolution\nLook at the Proportion of trace at the bottom of the output. The first number is much bigger than the second, so the first linear discriminant is much more important than the second. (I care about your reason; you can say it’s “more important” rather than “much more important” and I’m good with that.)\n\\(\\blacksquare\\)\n\nObtain a plot of the discriminant scores.\n\nSolution\nThis was the old-fashioned way:\n\nplot(simple.4)\n\n\n\n\nIt needs cajoling to produce colours, but we can do better. The first thing is to obtain the predictions:\n\nsimple.pred &lt;- predict(simple.4)\n\nThen we make a data frame out of the discriminant scores and the true groups, using cbind:\n\nd &lt;- cbind(simple, simple.pred)\nhead(d)\n\n\n\n  \n\n\n\nor like this, for fun:6\n\nld &lt;- as_tibble(simple.pred$x)\npost &lt;- as_tibble(simple.pred$posterior)\ndd &lt;- bind_cols(simple, class = simple.pred$class, ld, post)\ndd\n\n\n\n  \n\n\n\nAfter that, we plot the first one against the second one, colouring by true groups:\n\nggplot(d, aes(x = x.LD1, y = x.LD2, colour = group)) + geom_point()\n\n\n\n\nI wanted to compare this plot with the original plot of y1 vs. y2, coloured by groups:\n\nggplot(simple, aes(x = y1, y = y2, colour = group)) + geom_point()\n\n\n\n\nThe difference between this plot and the one of LD1 vs.\nLD2 is that things have been rotated a bit so that most of the separation of groups is done by LD1. This is reflected in the fact that LD1 is quite a bit more important than LD2: the latter doesn’t help much in separating the groups.\nWith that in mind, we could also plot just LD1, presumably against groups via boxplot:\n\nggplot(d, aes(x = group, y = x.LD1)) + geom_boxplot()\n\n\n\n\nThis shows that LD1 does a pretty fine job of separating the groups, and LD2 doesn’t really have much to add to the picture.\n\\(\\blacksquare\\)\n\nDescribe briefly how LD1 and/or LD2 separate the groups. Does your picture confirm the relative importance of LD1 and LD2 that you found back in part (here)? Explain briefly.\n\nSolution\nLD1 separates the groups left to right: group a is low on LD1, b is in the middle and c is high on LD1. (There is no intermingling of the groups on LD1, so it separates the groups perfectly.)\nAs for LD2, all it does (possibly) is to distinguish b (low) from a and c (high). Or you can, just as reasonably, take the view that it doesn’t really separate any of the groups.\nBack in part (here), you said (I hope) that LD1 was (very) important compared to LD2. This shows up here in that LD1 does a very good job of distinguishing the groups, while LD2 does a poor to non-existent job of separating any groups. (If you didn’t say that before, here is an invitation to reconsider what you did say there.)\n\\(\\blacksquare\\)\n\nWhat makes group a have a low score on LD1? There are two steps that you need to make: consider the means of group a on variables y1 and y2 and how they compare to the other groups, and consider how y1 and y2 play into the score on LD1.\n\nSolution\nThe information you need is in the big output.\nThe means of y1 and y2 for group a are 3 and 4 respectively, which are the lowest of all the groups. That’s the first thing.\nThe second thing is the coefficients of LD1 in terms of y1 and y2, which are both positive. That means, for any observation, if its y1 and y2 values are large, that observation’s score on LD1 will be large as well. Conversely, if its values are small, as the ones in group a are, its score on LD1 will be small.\nYou need these two things.\nThis explains why the group a observations are on the left of the plot. It also explains why the group c observations are on the right: they are large on both y1 and y2, and so large on LD1.\nWhat about LD2? This is a little more confusing (and thus I didn’t ask you about that). Its “coefficients of linear discriminant” are positive on y1 and negative on y2, with the latter being bigger in size. Group b is about average on y1 and distinctly high on y2; the second of these coupled with the negative coefficient on y2 means that the LD2 score for observations in group b will be negative.\nFor LD2, group a has a low mean on both variables and group c has a high mean, so for both groups there is a kind of cancelling-out happening, and neither group a nor group c will be especially remarkable on LD2.\n\\(\\blacksquare\\)\n\nObtain predictions for the group memberships of each observation, and make a table of the actual group memberships against the predicted ones. How many of the observations were wrongly classified?\n\nSolution\nUse the simple.pred that you got earlier. This is the table way:\n\nwith(d, table(obs = group, pred = class))\n\n   pred\nobs a b c\n  a 4 0 0\n  b 0 3 0\n  c 0 0 5\n\n\nEvery single one of the 12 observations has been classified into its correct group. (There is nothing off the diagonal of this table.) The alternative to table is the tidyverse way:\n\nd %&gt;% count(group, class)\n\n\n\n  \n\n\n\nor\n\nd %&gt;%\n  count(group, class) %&gt;%\n  pivot_wider(names_from=class, values_from=n, values_fill = list(n=0))\n\n\n\n  \n\n\n\nif you want something that looks like a frequency table. All the as got classified as a, and so on. That’s the end of what I asked you to do, but as ever I wanted to press on. The next question to ask after getting the predicted groups is “what are the posterior probabilities of being in each group for each observation”: that is, not just which group do I think it belongs in, but how sure am I about that call? The posterior probabilities in my d start with posterior. These have a ton of decimal places which I like to round off first before I display them, eg. to 3 decimals here:\n\nd %&gt;%\n  select(y1, y2, group, class, starts_with(\"posterior\")) %&gt;%\n  mutate(across(starts_with(\"posterior\"), \\(post) round(post, 3)))\n\n\n\n  \n\n\n\nYou see that the posterior probability of an observation being in the group it actually was in is close to 1 all the way down. The only one with any doubt at all is observation #6, which is actually in group b, but has “only” probability 0.814 of being a b based on its y1 and y2 values. What else could it be? Well, it’s about equally split between being a and c. Let me see if I can display this observation on the plot in a different way. First I need to make a new column picking out observation 6, and then I use this new variable as the size of the point I plot:\n\nsimple %&gt;%\n  mutate(is6 = (row_number() == 6)) %&gt;%\n  ggplot(aes(x = y1, y = y2, colour = group, size = is6)) +\n  geom_point()\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\nThat makes it stand out. As the legend indicates, observation #6 is plotted as a big circle, with the rest being plotted as small circles as usual. Since observation #6 is in group b, it appears as a big green circle. What makes it least like a b? Well, it has the smallest y2 value of any of the b’s (which makes it most like an a of any of the b’s), and it has the largest y1 value (which makes it most like a c of any of the b’s). But still, it’s nearer the greens than anything else, so it’s still more like a b than it is like any of the other groups.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "discriminant-analysis.html#what-distinguishes-people-who-do-different-jobs-1",
    "href": "discriminant-analysis.html#what-distinguishes-people-who-do-different-jobs-1",
    "title": "32  Discriminant analysis",
    "section": "32.11 What distinguishes people who do different jobs?",
    "text": "32.11 What distinguishes people who do different jobs?\n2447 people work at a certain company. They each have one of three jobs: customer service, mechanic, dispatcher. In the data set, these are labelled 1, 2 and 3 respectively. In addition, they each are rated on scales called outdoor, social and conservative. Do people with different jobs tend to have different scores on these scales, or, to put it another way, if you knew a person’s scores on outdoor, social and conservative, could you say something about what kind of job they were likely to hold? The data are in link.\n\nRead in the data and display some of it.\n\nSolution\nThe usual. This one is aligned columns. I’m using a “temporary” name for my read-in data frame, since I’m going to create the proper one in a moment.\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/jobs.txt\"\njobs0 &lt;- read_table(my_url)\n\nWarning: Missing column names filled in: 'X6' [6]\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  outdoor = col_double(),\n  social = col_double(),\n  conservative = col_double(),\n  job = col_double(),\n  id = col_double(),\n  X6 = col_character()\n)\n\n\nWarning: 244 parsing failures.\nrow col  expected    actual                                                                 file\n  1  -- 6 columns 5 columns 'https://raw.githubusercontent.com/nxskok/datafiles/master/jobs.txt'\n  2  -- 6 columns 5 columns 'https://raw.githubusercontent.com/nxskok/datafiles/master/jobs.txt'\n  3  -- 6 columns 5 columns 'https://raw.githubusercontent.com/nxskok/datafiles/master/jobs.txt'\n  4  -- 6 columns 5 columns 'https://raw.githubusercontent.com/nxskok/datafiles/master/jobs.txt'\n  5  -- 6 columns 5 columns 'https://raw.githubusercontent.com/nxskok/datafiles/master/jobs.txt'\n... ... ......... ......... ....................................................................\nSee problems(...) for more details.\n\njobs0\n\n\n\n  \n\n\n\nWe got all that was promised, plus a label id for each employee, which we will from here on ignore.8\n\\(\\blacksquare\\)\n\nNote the types of each of the variables, and create any new variables that you need to.\n\nSolution\nThese are all int or whole numbers. But, the job ought to be a factor: the labels 1, 2 and 3 have no meaning as such, they just label the three different jobs. (I gave you a hint of this above.) So we need to turn job into a factor. I think the best way to do that is via mutate, and then we save the new data frame into one called jobs that we actually use for the analysis below:\n\njob_labels &lt;- c(\"custserv\", \"mechanic\", \"dispatcher\")\njobs0 %&gt;%\n  mutate(job = factor(job, labels = job_labels)) -&gt; jobs\n\nI lived on the edge and saved my factor job into a variable with the same name as the numeric one. I should check that I now have the right thing:\n\njobs\n\n\n\n  \n\n\n\nI like this better because you see the actual factor levels rather than the underlying numeric values by which they are stored.\nAll is good here. If you forget the labels thing, you’ll get a factor, but its levels will be 1, 2, and 3, and you will have to remember which jobs they go with. I’m a fan of giving factors named levels, so that you can remember what stands for what.9\nExtra: another way of doing this is to make a lookup table, that is, a little table that shows which job goes with which number:\n\nlookup_tab &lt;- tribble(\n  ~job, ~jobname,\n  1, \"custserv\",\n  2, \"mechanic\",\n  3, \"dispatcher\"\n)\nlookup_tab\n\n\n\n  \n\n\n\nI carefully put the numbers in a column called job because I want to match these with the column called job in jobs0:\n\njobs0 %&gt;%\n  left_join(lookup_tab) %&gt;%\n  sample_n(20)\n\nJoining with `by = join_by(job)`\n\n\n\n\n  \n\n\n\nYou see that each row has the name of the job that employee has, in the column jobname, because the job id was looked up in our lookup table. (I displayed some random rows so you could see that it worked.)\n\\(\\blacksquare\\)\n\nRun a multivariate analysis of variance to convince yourself that there are some differences in scale scores among the jobs.\n\nSolution\nYou know how to do this, right? This one is the easy way:\n\nresponse &lt;- with(jobs, cbind(social, outdoor, conservative))\nresponse.1 &lt;- manova(response ~ job, data = jobs)\nsummary(response.1)\n\n           Df  Pillai approx F num Df den Df    Pr(&gt;F)    \njob         2 0.76207   49.248      6    480 &lt; 2.2e-16 ***\nResiduals 241                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOr you can use Manova. That is mostly for practice here, since there is no reason to make things difficult for yourself:\n\nlibrary(car)\nresponse.2 &lt;- lm(response ~ job, data = jobs)\nsummary(Manova(response.2))\n\n\nType II MANOVA Tests:\n\nSum of squares and products for error:\n                social    outdoor conservative\nsocial       4406.2994  334.90273    235.46301\noutdoor       334.9027 4082.46302     64.76334\nconservative  235.4630   64.76334   2683.25695\n\n------------------------------------------\n \nTerm: job \n\nSum of squares and products for the hypothesis:\n                 social   outdoor conservative\nsocial        2889.1228 -794.3945   -1405.8401\noutdoor       -794.3945 1609.7993     283.1711\nconservative -1405.8401  283.1711     691.7594\n\nMultivariate Tests: job\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            2 0.7620657 49.24757      6    480 &lt; 2.22e-16 ***\nWilks             2 0.3639880 52.38173      6    478 &lt; 2.22e-16 ***\nHotelling-Lawley  2 1.4010307 55.57422      6    476 &lt; 2.22e-16 ***\nRoy               2 1.0805270 86.44216      3    240 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis version gives the four different versions of the test (rather than just the Pillai test that manova gives), but the results are in this case identical for all of them.\nSo: oh yes, there are differences (on some or all of the variables, for some or all of the groups). So we need something like discriminant analysis to understand the differences.\nWe really ought to follow this up with Box’s M test, to be sure that the variances and correlations for each variable are equal enough across the groups, but we note off the top that the P-values (all of them) are really small, so there ought not to be much doubt about the conclusion anyway:\n\nsummary(BoxM(response, jobs$job))\n\n       Box's M Test \n\nChi-Squared Value = 25.64176 , df = 12  and p-value: 0.0121 \n\n\nThis is small, but not (for this test) small enough to worry about (it’s not less than 0.001).\nThis, and the lda below, actually works perfectly well if you use the original (integer) job, but then you have to remember which job number is which.\n\\(\\blacksquare\\)\n\nRun a discriminant analysis and display the output.\n\nSolution\nNow job is the “response”:\n\njob.1 &lt;- lda(job ~ social + outdoor + conservative, data = jobs)\njob.1\n\nCall:\nlda(job ~ social + outdoor + conservative, data = jobs)\n\nPrior probabilities of groups:\n  custserv   mechanic dispatcher \n 0.3483607  0.3811475  0.2704918 \n\nGroup means:\n             social  outdoor conservative\ncustserv   24.22353 12.51765     9.023529\nmechanic   21.13978 18.53763    10.139785\ndispatcher 15.45455 15.57576    13.242424\n\nCoefficients of linear discriminants:\n                     LD1         LD2\nsocial       -0.19427415 -0.04978105\noutdoor       0.09198065 -0.22501431\nconservative  0.15499199  0.08734288\n\nProportion of trace:\n   LD1    LD2 \n0.7712 0.2288 \n\n\n\\(\\blacksquare\\)\n\nWhich is the more important, LD1 or LD2? How much more important? Justify your answer briefly.\n\nSolution\nLook at the “proportion of trace” at the bottom. The value for LD1 is quite a bit higher, so LD1 is quite a bit more important when it comes to separating the groups. LD2 is, as I said, less important, but is not completely worthless, so it will be worth taking a look at it.\n\\(\\blacksquare\\)\n\nDescribe what values for an individual on the scales will make each of LD1 and LD2 high.\n\nSolution\nThis is a two-parter: decide whether each scale makes a positive, negative or zero contribution to the linear discriminant (looking at the “coefficients of linear discriminants”), and then translate that into what would make each LD high. Let’s start with LD1:\nIts coefficients on the three scales are respectively negative (\\(-0.19\\)), zero (0.09; my call) and positive (0.15). Where you draw the line is up to you: if you want to say that outdoor’s contribution is positive, go ahead. This means that LD1 will be high if social is low and if conservative is high. (If you thought that outdoor’s coefficient was positive rather than zero, if outdoor is high as well.)\nNow for LD2: I’m going to call outdoor’s coefficient of \\(-0.22\\) negative and the other two zero, so that LD2 is high if outdoor is low. Again, if you made a different judgement call, adapt your answer accordingly.\n\\(\\blacksquare\\)\n\nThe first group of employees, customer service, have the highest mean on social and the lowest mean on both of the other two scales. Would you expect the customer service employees to score high or low on LD1? What about LD2?\n\nSolution\nIn the light of what we said in the previous part, the customer service employees, who are high on social and low on conservative, should be low (negative) on LD1, since both of these means are pointing that way. As I called it, the only thing that matters to LD2 is outdoor, which is low for the customer service employees, and thus LD2 for them will be high (negative coefficient).\n\\(\\blacksquare\\)\n\nPlot your discriminant scores (which you will have to obtain first), and see if you were right about the customer service employees in terms of LD1 and LD2. The job names are rather long, and there are a lot of individuals, so it is probably best to plot the scores as coloured circles with a legend saying which colour goes with which job (rather than labelling each individual with the job they have).\n\nSolution\nPredictions first, then make a data frame combining the predictions with the original data:\n\np &lt;- predict(job.1)\nd &lt;- cbind(jobs, p)\nd\n\n\n\n  \n\n\n\nFollowing my suggestion, plot these the standard way with colour distinguishing the jobs:\n\nggplot(d, aes(x = x.LD1, y = x.LD2, colour = job)) + geom_point()\n\n\n\nggplot(d, aes(x = x.LD1, y = x.LD2, colour = class)) + geom_point()\n\n\n\n# ggplot(d, aes(x = job, y = x.LD1)) + geom_boxplot()\n\nI was mostly right about the customer service people: small LD1 definitely, large LD2 kinda. I wasn’t more right because the group means don’t tell the whole story: evidently, the customer service people vary quite a bit on outdoor, so the red dots are all over the left side of the plot.\nThere is quite a bit of intermingling of the three employee groups on the plot, but the point of the MANOVA is that the groups are (way) more separated than you’d expect by chance, that is if the employees were just randomly scattered across the plot.\nTo think back to that trace thing: here, it seems that LD1 mainly separates customer service (left) from dispatchers (right); the mechanics are all over the place on LD1, but they tend to be low on LD2. So LD2 does have something to say.\n\\(\\blacksquare\\)\n\n* Obtain predicted job allocations for each individual (based on their scores on the three scales), and tabulate the true jobs against the predicted jobs. How would you describe the quality of the classification? Is that in line with what the plot would suggest?\n\nSolution\nUse the predictions that you got before and saved in d:\n\nwith(d, table(obs = job, pred = class))\n\n            pred\nobs          custserv mechanic dispatcher\n  custserv         68       13          4\n  mechanic         16       67         10\n  dispatcher        3       13         50\n\n\nOr, the tidyverse way:\n\nd %&gt;% count(job, class)\n\n\n\n  \n\n\n\nor:\n\nd %&gt;%\n  count(job, class) %&gt;%\n  pivot_wider(names_from=class, values_from=n, values_fill = list(n=0))\n\n\n\n  \n\n\n\nI didn’t really need the values_fill since there are no missing frequencies, but I’ve gotten used to putting it in. There are a lot of misclassifications, but there are a lot of people, so a large fraction of people actually got classified correctly. The biggest frequencies are of people who got classified correctly. I think this is about what I was expecting, looking at the plot: the people top left are obviously customer service, the ones top right are in dispatch, and most of the ones at the bottom are mechanics. So there will be some errors, but the majority of people should be gotten right. The easiest pairing to get confused is customer service and mechanics, which you might guess from the plot: those customer service people with a middling LD1 score and a low LD2 score (that is, high on outdoor) could easily be confused with the mechanics. The easiest pairing to distinguish is customer service and dispatchers: on the plot, left and right, that is, low and high respectively on LD1.\nWhat fraction of people actually got misclassified? You could just pull out the numbers and add them up, but you know me: I’m too lazy to do that.\nWe can work out the total number and fraction who got misclassified. There are different ways you might do this, but the tidyverse way provides the easiest starting point. For example, we can make a new column that indicates whether a group is the correct or wrong classification:\n\nd %&gt;%\n  count(job, class) %&gt;%\n  mutate(job_stat = ifelse(job == class, \"correct\", \"wrong\"))\n\n\n\n  \n\n\n\nFrom there, we count up the correct and wrong ones, recognizing that we want to total up the frequencies in n, not just count the number of rows:\n\nd %&gt;%\n  count(job, class) %&gt;%\n  mutate(job_stat = ifelse(job == class, \"correct\", \"wrong\")) %&gt;%\n  count(job_stat, wt = n)\n\n\n\n  \n\n\n\nand turn these into proportions:\n\nd %&gt;%\n  count(job, class) %&gt;%\n  mutate(job_stat = ifelse(job == class, \"correct\", \"wrong\")) %&gt;%\n  count(job_stat, wt = n) %&gt;%\n  mutate(proportion = n / sum(n))\n\n\n\n  \n\n\n\nThere is a count followed by another count of the first lot of counts, so the second count column has taken over the name n.\n24% of all the employees got classified into the wrong job, based on their scores on outdoor, social and conservative.\nThis is actually not bad, from one point of view: if you just guessed which job each person did, without looking at their scores on the scales at all, you would get \\({1\\over 3}=33\\%\\) of them right, just by luck, and \\({2\\over3}=67\\%\\) of them wrong. From 67% to 24% error is a big improvement, and that is what the MANOVA is reacting to.\nTo figure out whether some of the groups were harder to classify than others, squeeze a group_by in early to do the counts and proportions for each (true) job:\n\nd %&gt;%\n  count(job, class) %&gt;%\n  mutate(job_stat = ifelse(job == class, \"correct\", \"wrong\")) %&gt;%\n  group_by(job) %&gt;%\n  count(job_stat, wt = n) %&gt;%\n  mutate(proportion = n / sum(n))\n\n\n\n  \n\n\n\nor even split out the correct and wrong ones into their own columns:\n\nd %&gt;%\n  count(job, class) %&gt;%\n  mutate(job_stat = ifelse(job == class, \"correct\", \"wrong\")) %&gt;%\n  group_by(job) %&gt;%\n  count(job_stat, wt = n) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  select(-n) %&gt;%\n  pivot_wider(names_from=job_stat, values_from=proportion)\n\n\n\n  \n\n\n\nThe mechanics were hardest to get right and easiest to get wrong, though there isn’t much in it. I think the reason is that the mechanics were sort of “in the middle” in that a mechanic could be mistaken for either a dispatcher or a customer service representative, but but customer service and dispatchers were more or less distinct from each other.\nIt’s up to you whether you prefer to do this kind of thing by learning enough about table to get it to work, or whether you want to use tidy-data mechanisms to do it in a larger number of smaller steps. I immediately thought of table because I knew about it, but the tidy-data way is more consistent with the way we have been doing things.\n\\(\\blacksquare\\)\n\nConsider an employee with these scores: 20 on outdoor, 17 on social and 8 on conservative What job do you think they do, and how certain are you about that? Use predict, first making a data frame out of the values to predict for.\n\nSolution\nThis is in fact exactly the same idea as the data frame that I generally called new when doing predictions for other models. I think the clearest way to make one of these is with tribble:\n\nnew &lt;- tribble(\n  ~outdoor, ~social, ~conservative,\n  20, 17, 8\n)\nnew\n\n\n\n  \n\n\n\n\nsummary(jobs)\n\n    outdoor          social       conservative           job    \n Min.   : 0.00   Min.   : 7.00   Min.   : 0.00   custserv  :85  \n 1st Qu.:13.00   1st Qu.:17.00   1st Qu.: 8.00   mechanic  :93  \n Median :16.00   Median :21.00   Median :11.00   dispatcher:66  \n Mean   :15.64   Mean   :20.68   Mean   :10.59                  \n 3rd Qu.:19.00   3rd Qu.:25.00   3rd Qu.:13.00                  \n Max.   :28.00   Max.   :35.00   Max.   :20.00                  \n       id             X6           \n Min.   : 1.00   Length:244        \n 1st Qu.:21.00   Class :character  \n Median :41.00   Mode  :character  \n Mean   :41.95                     \n 3rd Qu.:61.25                     \n Max.   :93.00                     \n\n\nThere’s no need for crossing here because I’m not doing combinations of things. (I might have done that here, to get a sense for example of “what effect does a higher score on outdoor have on the likelihood of a person doing each job?”. But I didn’t.)\nThen feed this into predict as the second thing:\n\npp1 &lt;- predict(job.1, new)\n\nOur predictions are these:\n\ncbind(new, pp1)\n\n\n\n  \n\n\n\nThe class thing gives our predicted job, and the posterior probabilities say how sure we are about that. So we reckon there’s a 78% chance that this person is a mechanic; they might be a dispatcher but they are unlikely to be in customer service. Our best guess is that they are a mechanic.10\nDoes this pass the sanity-check test? First figure out where our new employee stands compared to the others:\n\nsummary(jobs)\n\n    outdoor          social       conservative           job    \n Min.   : 0.00   Min.   : 7.00   Min.   : 0.00   custserv  :85  \n 1st Qu.:13.00   1st Qu.:17.00   1st Qu.: 8.00   mechanic  :93  \n Median :16.00   Median :21.00   Median :11.00   dispatcher:66  \n Mean   :15.64   Mean   :20.68   Mean   :10.59                  \n 3rd Qu.:19.00   3rd Qu.:25.00   3rd Qu.:13.00                  \n Max.   :28.00   Max.   :35.00   Max.   :20.00                  \n       id             X6           \n Min.   : 1.00   Length:244        \n 1st Qu.:21.00   Class :character  \n Median :41.00   Mode  :character  \n Mean   :41.95                     \n 3rd Qu.:61.25                     \n Max.   :93.00                     \n\n\nTheir score on outdoor is above average, but their scores on the other two scales are below average (right on the 1st quartile in each case).\nGo back to the table of means from the discriminant analysis output. The mechanics have the highest average for outdoor, they’re in the middle on social and they are lowish on conservative. Our new employee is at least somewhat like that.\nOr, we can figure out where our new employee sits on the plot. The output from predict gives the predicted LD1 and LD2, which are 0.71 and \\(-1.02\\) respectively. This employee would sit to the right of and below the middle of the plot: in the greens, but with a few blues nearby: most likely a mechanic, possibly a dispatcher, but likely not customer service, as the posterior probabilities suggest.\nExtra: I can use the same mechanism to predict for a combination of values. This would allow for the variability of each of the original variables to differ, and enable us to assess the effect of, say, a change in conservative over its “typical range”, which we found out above with summary(jobs). I’ll take the quartiles, in my usual fashion:\n\noutdoors &lt;- c(13, 19)\nsocials &lt;- c(17, 25)\nconservatives &lt;- c(8, 13)\n\nThe IQRs are not that different, which says that what we get here will not be that different from the ``coefficients of linear discriminants’’ above:\n\nnew &lt;- crossing(\n  outdoor = outdoors, social = socials,\n  conservative = conservatives\n)\npp2 &lt;- predict(job.1, new)\npx &lt;- round(pp2$x, 2)\ncbind(new, pp2$class, px)\n\n\n\n  \n\n\n\nThe highest (most positive) LD1 score goes with high outdoor, low social, high conservative (and being a dispatcher). It is often interesting to look at the second-highest one as well: here that is low outdoor, and the same low social and high conservative as before. That means that outdoor has nothing much to do with LD1 score. Being low social is strongly associated with LD1 being positive, so that’s the important part of LD1.\nWhat about LD2? The most positive LD2 are these:\n\nLD2    outdoor  social  conservative\n====================================\n0.99   low      low     high\n0.59   low      high    high\n0.55   low      low     low\n\nThese most consistently go with outdoor being low.\nIs that consistent with the “coefficients of linear discriminants”?\n\njob.1$scaling\n\n                     LD1         LD2\nsocial       -0.19427415 -0.04978105\noutdoor       0.09198065 -0.22501431\nconservative  0.15499199  0.08734288\n\n\nVery much so: outdoor has nothing much to do with LD1 and everything to do with LD2.\n\\(\\blacksquare\\)\n\nSince I am not making you hand this one in, I’m going to keep going. Re-run the analysis to incorporate cross-validation, and make a table of the predicted group memberships. Is it much different from the previous one you had? Why would that be?\n\nSolution\nStick a CV=T in the lda:\n\njob.3 &lt;- lda(job ~ social + outdoor + conservative, data = jobs, CV = T)\nglimpse(job.3)\n\nList of 5\n $ class    : Factor w/ 3 levels \"custserv\",\"mechanic\",..: 1 2 1 1 1 2 1 1 1 1 ...\n $ posterior: num [1:244, 1:3] 0.902 0.352 0.71 0.805 0.766 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:244] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:3] \"custserv\" \"mechanic\" \"dispatcher\"\n $ terms    :Classes 'terms', 'formula'  language job ~ social + outdoor + conservative\n  .. ..- attr(*, \"variables\")= language list(job, social, outdoor, conservative)\n  .. ..- attr(*, \"factors\")= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. ..- attr(*, \"term.labels\")= chr [1:3] \"social\" \"outdoor\" \"conservative\"\n  .. ..- attr(*, \"order\")= int [1:3] 1 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(job, social, outdoor, conservative)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:4] \"factor\" \"numeric\" \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:4] \"job\" \"social\" \"outdoor\" \"conservative\"\n $ call     : language lda(formula = job ~ social + outdoor + conservative, data = jobs, CV = T)\n $ xlevels  : Named list()\n\n\nThis directly contains a class (no need for a predict), so we make a data frame, with a different name since I shortly want to compare this one with the previous one:\n\ndcv &lt;- cbind(jobs, class = job.3$class, posterior = job.3$posterior)\nhead(dcv)\n\n\n\n  \n\n\n\nThis is a bit fiddlier than before because job.3 contains some things of different lengths and we can’t just cbind them all together.\nThen go straight to the table:\n\nwith(dcv, table(job, class))\n\n            class\njob          custserv mechanic dispatcher\n  custserv         67       14          4\n  mechanic         16       67         10\n  dispatcher        3       14         49\n\n\nThis is almost exactly the same as we had in part (here): the cross-validation has made almost no difference. The reason for that is that here, we have lots of data (you can predict for one mechanic, say, and there are still lots of others to say that the mechanics are “over there”. This is in sharp contrast to the example in class with the bellydancers, where if you try to predict for one of the extreme ones, the notion of “where are the bellydancers” changes substantially. Here, I suspect that the few people whose predictions changed were ones where the posterior probabilities were almost equal for two jobs, and the cross-validation was just enough to tip the balance. You can check this, but there are a lot of posterior probabilities to look at!\nThis is another way of saying that with small data sets, your conclusions are more “fragile” or less likely to be generalizable. With a larger data set like this one, cross-validation, which is the right thing to do, makes almost no difference.11\nAll right, I suppose I do want to investigate the individuals whose predicted jobs changed, and look at their posterior probabilities. I think I have the machinery to do that.\nLet’s start by gluing together the dataframes with the predictions from the regular lda (in d) and the ones from the cross-validation (in dcv). I think I can do that like this:\n\nd %&gt;% left_join(dcv, by=c(\"id\", \"job\"))\n\n\n\n  \n\n\n\nThere’s already subtlety. The people are numbered separately within each actual job, so the thing that uniquely identifies each person (what database people call a “key”) is the combination of the actual job they do plus their id within that job. You might also think of using bind_cols, except that this adds a number to all the column names which is a pain to deal with. I don’t really need to look up the people in the second dataframe, since I know where they are (in the corresponding rows), but doing so seems to make everything else easier.\nThe columns with an x on the end of their names came from d, that is, the predictions without cross-validation, and the ones with a y came from cross-validation. Let’s see if we can keep only the columns we need so that it’s a bit less unwieldy:\n\nd %&gt;% left_join(dcv, by=c(\"id\", \"job\")) %&gt;% \n  select(outdoor = outdoor.x,\n         social = social.x,\n         conservative = conservative.x,\n         job, id, starts_with(\"class\"),\n         starts_with(\"posterior\")) -&gt; all\nall\n\n\n\n  \n\n\n\nThat’s not too bad. We could shorten some variable names and reduce some decimal places, but that’ll do for now.\nHow many individuals were predicted differently? We have columns called class.x (predicted group membership from original LDA) and class.y (from cross-validation), and so:\n\nall %&gt;% filter(class.x != class.y)\n\n\n\n  \n\n\n\nThere are exactly two individuals that were predicted differently. Under cross-validation, they both got called mechanics. How do their posterior probabilities compare? These are all in columns beginning with posterior. We could scrutinize the output above, or try to make things simpler. Let’s round them to three decimals, and then display only some of the columns:\n\nall %&gt;% filter(class.x != class.y) %&gt;% \n  mutate(across(starts_with(\"posterior\"), \\(post) round(post, 3))) %&gt;%\n  select(id, job, starts_with(\"posterior\"))\n\n\n\n  \n\n\n\nAnd then, because I can, let’s re-format that to make it easier to read, x being regular LDA and y being cross-validation:\n\nall %&gt;% filter(class.x != class.y) %&gt;% \n  mutate(across(starts_with(\"posterior\"), \\(post) round(post, 3))) %&gt;%\n  select(id, job, starts_with(\"posterior\")) %&gt;% \n  pivot_longer(starts_with(\"posterior\"), names_to = c(\"post_job\", \"method\"), \n               names_pattern = \"posterior\\\\.(.*)\\\\.(.)\", values_to = \"prob\") %&gt;% \n  pivot_wider(names_from = method, values_from = prob)\n\n\n\n  \n\n\n\nAs I suspected, the posterior probabilities in each case are almost identical, but different ones happen to be slightly higher in the two cases. For the first individual (actually in customer service), cross-validation just tweaked the posterior probabilities enough to call that individual a mechanic, and for the second one, actually a dispatcher, the first analysis was almost too close to call, and things under cross-validation got nudged onto the mechanic side again.\nAll right, what about those people who got misclassified (say, by the LDA rather than the cross-validation, since it seems not to make much difference)?\nLet’s count them first:\n\nall %&gt;% mutate(is_correct = ifelse(job == class.x, \"correct\", \"wrong\")) -&gt; all.mis\nall.mis %&gt;%\n  count(is_correct == \"wrong\") %&gt;%\n  mutate(proportion = n / sum(n))\n\n\n\n  \n\n\n\n24% of them. There are a lot of them, so we’ll pick a random sample to look at, rounding the posterior probabilities to 3 decimals first and reducing the number of columns to look at:\n\nset.seed(457299)\nall.mis %&gt;%\n  filter(is_correct == \"wrong\") %&gt;%\n  mutate(across(starts_with(\"posterior\"), \\(post) round(post, 3))) %&gt;%\n  select(\n    id, job, class.x, outdoor, social, conservative,\n    starts_with(\"posterior\")\n  ) %&gt;%\n  sample_n(15)\n\n\n\n  \n\n\n\nI put the set.seed in so that this will come out the same each time I do it, and so that the discussion below always makes sense.\nNow we can look at the true and predicted jobs for these people, and the posterior probabilities (which I rounded earlier).\n\nThe first one, id 6, is badly wrong; this was actually a mechanic, but the posterior probabilities say that it is a near-certain dispatcher.\nThe second one, id 65, is a little better, but the posterior probability of actually being a mechanic is only 0.269; the probability of being a dispatcher is much higher at 0.593, so that’s what it gets classified as.\nThe third one, though, id #61, is a very close call: posterior probability 0.438 of being in customer service (correct), 0.453 of being a dispatcher, only slightly higher, but enough to make the prediction wrong.\n\nThe implication from looking at our sample of 15 people is that some of them are “badly” misclassified (with a high posterior probability of having a different job from the one they actually hold), but a lot of them came out on the wrong end of a close call. This suggests that a number of the correct classifications came out right almost by chance as well, with (hypothesizing) two close posterior probabilities of which their actual job came out slightly higher.\nFurther further analysis would look at the original variables social, outdoor and conservative for the misclassified people, and try to find out what was unusual about them. But I think now would be an excellent place for me to stop.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "discriminant-analysis.html#observing-children-with-adhd-1",
    "href": "discriminant-analysis.html#observing-children-with-adhd-1",
    "title": "32  Discriminant analysis",
    "section": "32.12 Observing children with ADHD",
    "text": "32.12 Observing children with ADHD\nA number of children with ADHD were observed by their mother or their father (only one parent observed each child). Each parent was asked to rate occurrences of behaviours of four different types, labelled q1 through q4 in the data set. Also recorded was the identity of the parent doing the observation for each child: 1 is father, 2 is mother.\nCan we tell (without looking at the parent column) which parent is doing the observation? Research suggests that rating the degree of impairment in different categories depends on who is doing the rating: for example, mothers may feel that a child has difficulty sitting still, while fathers, who might do more observing of a child at play, might think of such a child as simply being “active” or “just being a kid”. The data are in link.\n\nRead in the data and confirm that you have four ratings and a column labelling the parent who made each observation.\n\nSolution\nAs ever:\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/adhd-parents.txt\"\nadhd &lt;- read_delim(my_url, \" \")\n\nRows: 29 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): parent\ndbl (4): q1, q2, q3, q4\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nadhd\n\n\n\n  \n\n\n\nYes, exactly that.\n\\(\\blacksquare\\)\n\nRun a suitable discriminant analysis and display the output.\n\nSolution\nThis is as before:\n\nadhd.1 &lt;- lda(parent ~ q1 + q2 + q3 + q4, data = adhd)\nadhd.1\n\nCall:\nlda(parent ~ q1 + q2 + q3 + q4, data = adhd)\n\nPrior probabilities of groups:\n   father    mother \n0.1724138 0.8275862 \n\nGroup means:\n          q1       q2       q3    q4\nfather 1.800 1.000000 1.800000 1.800\nmother 2.375 2.791667 1.958333 1.625\n\nCoefficients of linear discriminants:\n          LD1\nq1 -0.3223454\nq2  2.3219448\nq3  0.1411360\nq4  0.1884613\n\n\n\\(\\blacksquare\\)\n\nWhich behaviour item or items seem to be most helpful at distinguishing the parent making the observations? Explain briefly.\n\nSolution\nLook at the Coefficients of Linear Discriminants. The coefficient of q2, 2.32, is much larger in size than the others, so it’s really q2 that distinguishes mothers and fathers. Note also that the group means for fathers and mothers are fairly close on all the items except for q2, which are a long way apart. So that’s another hint that it might be q2 that makes the difference. But that might be deceiving: one of the other qs, even though the means are close for mothers and fathers, might actually do a good job of distinguishing mothers from fathers, because it has a small SD overall.\n\\(\\blacksquare\\)\n\nObtain the predictions from the lda, and make a suitable plot of the discriminant scores, bearing in mind that you only have one LD. Do you think there will be any misclassifications? Explain briefly.\n\nSolution\nThe prediction is the obvious thing. I take a quick look at it (using glimpse), but only because I feel like it:\n\nadhd.2 &lt;- predict(adhd.1)\nglimpse(adhd.2)\n\nList of 3\n $ class    : Factor w/ 2 levels \"father\",\"mother\": 1 2 1 2 2 2 2 2 2 2 ...\n $ posterior: num [1:29, 1:2] 9.98e-01 5.57e-06 9.98e-01 4.97e-02 4.10e-05 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:29] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:2] \"father\" \"mother\"\n $ x        : num [1:29, 1] -3.327 1.357 -3.327 -0.95 0.854 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:29] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr \"LD1\"\n\n\nThe discriminant scores are in the thing called x in there. There is only LD1 (only two groups, mothers and fathers), so the right way to plot it is against the true groups, eg. by a boxplot, first making a data frame, using data.frame, containing what you need:\n\nd &lt;- cbind(adhd, adhd.2)\nhead(d)\n\n\n\n  \n\n\nggplot(d, aes(x = parent, y = LD1)) + geom_boxplot()\n\n\n\n\nThe fathers look to be a very compact group with LD1 score around \\(-3\\), so I don’t foresee any problems there. The mothers, on the other hand, have outliers: there is one with LD1 score beyond \\(-3\\) that will certainly be mistaken for a father. There are a couple of other unusual LD1 scores among the mothers, but a rule like “anything above \\(-2\\) is called a mother, anything below is called a father” will get these two right. So I expect that the one very low mother will get misclassified, but that’s the only one.\n\\(\\blacksquare\\)\n\nObtain the predicted group memberships and make a table of actual vs. predicted. Were there any misclassifications? Explain briefly.\n\nSolution\nUse the predictions from the previous part, and the observed parent values from the original data frame. Then use either table or tidyverse to summarize.\n\nwith(d, table(obs = parent, pred = class))\n\n        pred\nobs      father mother\n  father      5      0\n  mother      1     23\n\n\nOr,\n\nd %&gt;% count(parent, class)\n\n\n\n  \n\n\n\nor\n\nd %&gt;%\n  count(parent, class) %&gt;%\n  pivot_wider(names_from=class, values_from=n, values_fill = list(n=0))\n\n\n\n  \n\n\n\nOne of the mothers got classified as a father (evidently that one with a very negative LD1 score), but everything else is correct.\nThis time, by “explain briefly” I mean something like “tell me how you know there are or are not misclassifications”, or “describe any misclassifications that occur” or something like that.\nExtra: I was curious — what is it about that one mother that caused her to get misclassified? (I didn’t ask you to think further about this, but in case you are curious as well.)\nFirst, which mother was it? Let’s begin by adding the predicted classification to the data frame, and then we can query it by asking to see only the rows where the actual parent and the predicted parent were different. I’m also going to create a column id that will give us the row of the original data frame:\n\nd %&gt;%\n  mutate(id = row_number()) %&gt;%\n  filter(parent != class)\n\n\n\n  \n\n\n\nIt was the original row 17. So what was unusual about this? We know from earlier that behaviour q2 was the one that generally distinguished mothers from fathers, so maybe we should find the mean and SD of scores for mothers and fathers on q2:\n\nadhd %&gt;% group_by(parent) %&gt;% summarize(m2 = mean(q2), s2 = sd(q2))\n\n\n\n  \n\n\n\nThe fathers’ scores on q2 were all 1, but the mothers’ scores on q2 were on average much higher. So it’s not really a surprise that this mother was mistaken for a father.\n\\(\\blacksquare\\)\n\nRe-run the discriminant analysis using cross-validation, and again obtain a table of actual and predicted parents. Is the pattern of misclassification different from before? Hints: (i) Bear in mind that there is no predict step this time, because the cross-validation output includes predictions; (ii) use a different name for the predictions this time because we are going to do a comparison in a moment.\n\nSolution\nSo, this, with different name:\n\nadhd.3 &lt;- lda(parent ~ q1 + q2 + q3 + q4, data = adhd, CV = T)\ndd &lt;- cbind(adhd, class = adhd.3$class, posterior = adhd.3$posterior)\nwith(dd, table(parent, class))\n\n        class\nparent   father mother\n  father      5      0\n  mother      1     23\n\n\nIt’s exactly the same pattern of misclassification. (In fact, it’s exactly the same mother being misclassified as a father.)\nThis one is the same not because of having lots of data. In fact, as you see below, having a small data set makes quite a bit of difference to the posterior probabilities (where they are not close to 1 or 0), but the decisions about whether the parents are a mother or a father are clear-cut enough that none of those change. Even though (some of) the posterior probabilities are noticeably changed, which one is the bigger has not changed at all.\n\\(\\blacksquare\\)\n\nDisplay the original data (that you read in from the data file) side by side with two sets of posterior probabilities: the ones that you obtained with predict before, and the ones from the cross-validated analysis. Comment briefly on whether the two sets of posterior probabilities are similar. Hints: (i) use data.frame rather than cbind, for reasons that I explain elsewhere; (ii) round the posterior probabilities to 3 decimals before you display them. There are only 29 rows, so look at them all. I am going to add the LD1 scores to my output and sort by that, but you don’t need to. (This is for something I am going to add later.)\n\nSolution\nWe have two data frames, d and dd12\nthat respectively contain everything from the (original) lda output and the cross-validated output. Let’s glue them together, look at what we have, and then pull out what we need:\n\nall &lt;- data.frame(d, dd)\nhead(all)\n\n\n\n  \n\n\n\nThe ones with a 1 on the end are the cross-validated ones. We need the posterior probabilities, rounded, and they need to have shorter names:\n\nall %&gt;%\n  select(parent, starts_with(\"posterior\"), LD1) %&gt;%\n  mutate(across(starts_with(\"posterior\"), \\(x) round(x, 3))) %&gt;%\n  rename_with(\n    ~ str_replace(., \"posterior\", \"p\"),\n    starts_with(\"posterior\"),\n  ) %&gt;%\n  arrange(LD1)\n\n\n\n  \n\n\n\nThe rename_with changes the names of the columns that start with posterior to start with p instead (shorter). I learned about this today (having wondered whether it existed or not), and it took about three goes for me to get it right.13 The first column is the actual parent; the other five columns are: the posterior probabilities from before, for father and for mother (two columns), and the posterior probabilities from cross-validation for father and for mother (two more columns), and the LD1 scores from before, sorted into order. You might have these the other way around from me, but in any case you ought to make it clear which is which. I included the LD1 scores for my discussion below; you don’t need to. Are the two sets of posterior probabilities similar? Only kinda. The ones at the top and bottom of the list are without doubt respectively fathers at the top of the list (top 5 rows on my sorted output, except that one of those is actually a mother), or mothers at the bottom, from row 10 down. But for rows 6 through 9, the posterior probabilities are not that similar. The most dissimilar ones are in row 4, where the regular lda gives a posterior probability of 0.050 that the parent is a father, but under cross-validation that goes all the way up to 0.236. I think this is one of those mothers that is a bit like a father: her score on q2 was only 2, compared to 3 for most of the mothers. If you take out this mother, as cross-validation does, there are noticeably fewer q2=2 mothers left, so the observation looks more like a father than it would otherwise.\n\\(\\blacksquare\\)\n\nRow 17 of your (original) data frame above, row 5 of the output in the previous part, is the mother that was misclassified as a father. Why is it that the cross-validated posterior probabilities are 1 and 0, while the previous posterior probabilities are a bit less than 1 and a bit more than 0?\n\nSolution\nIn making the classification, the non-cross-validated procedure uses all the data, so that parent #17 suggests that the mothers are very variable on q2, so it is conceivable (though still unlikely) that this parent actually is a mother. Under cross-validation, however, parent #17 is omitted. This mother is nothing like any of the other mothers, or, to put it another way, the remaining mothers as a group are very far away from this one, so #17 doesn’t look like a mother at all.\n\\(\\blacksquare\\)\n\nFind the parents where the cross-validated posterior probability of being a father is “non-trivial”: that is, not close to zero and not close to 1. (You will have to make a judgement about what “close to zero or 1” means for you.) What do these parents have in common, all of them or most of them?\n\nSolution\nLet’s add something to the output we had before: the original scores on q1 through q4:\n\nall %&gt;%\n  select(q1:q4, parent, starts_with(\"posterior\"), LD1) %&gt;%\n  mutate(across(starts_with(\"posterior\"), \\(x) round(x, 3))) %&gt;%\n  rename_with(\n    \\(y) str_replace(y, \"posterior\", \"p\"),\n    starts_with(\"posterior\")) %&gt;%\n  arrange(LD1)\n\n\n\n  \n\n\n\nTo my mind, the “non-trivial” posterior probabilities are in rows 5 through 9. (You might have drawn the line in a different place.) These are the ones where there was some doubt, though maybe only a little, about which parent actually gave the ratings. For three of these, the parent (that was actually a mother) gave a rating of 2 on q2. These were the only 2’s on q2. The others were easy to call: “mother” if 3 and “father” if 1, and you’d get them all right except for that outlying mother. The clue in looking at q2 was that we found earlier that LD1 contained mostly q2, so that it was mainly q2 that separated the fathers and mothers. If you found something else that the “non-trivial” rows had in common, that is good too, but I think looking at q2 is your quickest route to an answer. (q1=1 picks out some of these, but not all of them.) This is really the same kind of issue as we discussed when comparing the posterior probabilities for lda and cross-validation above: there were only a few parents with q2=2, so the effect there is that under cross-validation, there are even fewer when you take one of them out.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "discriminant-analysis.html#growing-corn-1",
    "href": "discriminant-analysis.html#growing-corn-1",
    "title": "32  Discriminant analysis",
    "section": "32.13 Growing corn",
    "text": "32.13 Growing corn\nA new type of corn seed has been developed. The people developing it want to know if the type of soil the seed is planted in has an impact on how well the seed performs, and if so, what kind of impact. Three outcome measures were used: the yield of corn produced (from a fixed amount of seed), the amount of water needed, and the amount of herbicide needed. The data are in link. 32 fields were planted with the seed, 8 fields with each soil type.\n\nRead in the data and verify that you have 32 observations with the correct variables.\n\nSolution\nThe usual:\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/cornseed.csv\"\ncornseed &lt;- read_csv(my_url)\n\nRows: 32 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): soil\ndbl (4): field, yield, water, herbicide\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncornseed\n\n\n\n  \n\n\n\nWe have 32 rows; we have a categorical soil type, three numerical columns containing the yield, water and herbicide values, and we also have a label for each of the 32 fields (which is actually a number, but we don’t have to worry about that, since we won’t be using field for anything).\n\\(\\blacksquare\\)\n\nRun a multivariate analysis of variance to see whether the type of soil has any effect on any of the variables. What do you conclude from it?\n\nSolution\nThe usual thing: create the response, use manova (or Manova from car if you like, but it’s not necessary):\n\nresponse &lt;- with(cornseed, cbind(yield, water, herbicide))\ncornseed.1 &lt;- manova(response ~ soil, data = cornseed)\nsummary(cornseed.1)\n\n          Df Pillai approx F num Df den Df  Pr(&gt;F)  \nsoil       3 0.5345   2.0234      9     84 0.04641 *\nResiduals 28                                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWith a P-value (just) less than 0.05, soil type has some effect on the response variables: that is, it affects one or more of the three responses, or some combination of them. ANOVA conclusions are usually vague, and MANOVA conclusions are vaguer than most. We will try to improve on this. But with an only-just-significant P-value, we should not be expecting miracles.\nWe ought to check Box’s M test:\n\nsummary(BoxM(response, cornseed$soil))\n\n       Box's M Test \n\nChi-Squared Value = 12.07432 , df = 18  and p-value: 0.843 \n\n\nNo problems with unequal variances or correlations, at least.\nHere and below, field is neither a response variable nor an explanatory variable; it is an experimental unit, so field acts as an ID rather than anything else. So field should not be part of any of the analyses; if it did appear, the only way it could is as a factor, for example if this was somehow a repeated measures analysis over the three response variables. In that case, lmer, if you were going that way, would use field as a random effect.\nThe variables to include are the yield, water and herbicide as measured response variables, and soil type, as the categorical explanatory variable. (For the discriminant analysis, these get turned around: the grouping variable soil acts like a response and the others act as explanatory.)\n\\(\\blacksquare\\)\n\nRun a discriminant analysis on these data, “predicting” soil type from the three response variables. Display the results.\n\nSolution\n\ncornseed.2 &lt;- lda(soil ~ yield + water + herbicide, data = cornseed)\ncornseed.2\n\nCall:\nlda(soil ~ yield + water + herbicide, data = cornseed)\n\nPrior probabilities of groups:\n clay  loam salty sandy \n 0.25  0.25  0.25  0.25 \n\nGroup means:\n        yield   water herbicide\nclay  58.8375 33.0875    4.0875\nloam  69.7125 32.7375    6.3875\nsalty 55.3125 30.6375    3.8625\nsandy 62.5750 28.2000    4.3500\n\nCoefficients of linear discriminants:\n                  LD1         LD2         LD3\nyield     -0.08074845 -0.02081174 -0.04822432\nwater      0.03759961  0.09598577 -0.03231897\nherbicide -0.50654017  0.06979662  0.27281743\n\nProportion of trace:\n   LD1    LD2    LD3 \n0.9487 0.0456 0.0057 \n\n\nNo field in here, for reasons discussed above. (I’m not even sure how you can run a discriminant analysis with a factor variable on the right of the squiggle.) The fields were numbered by soil type:\n\ncornseed %&gt;% select(field, soil)\n\n\n\n  \n\n\n\nso evidently if you know the field number you can guess the field type, but we didn’t care about that: we cared about whether you can distinguish the fields by yield, water, herbicide or combination thereof.\n\\(\\blacksquare\\)\n\n* Which linear discriminants seem to be worth paying attention to? Why did you get three linear discriminants? Explain briefly.\n\nSolution\nLook for “proportion of trace” in the output.\nThe first one is way bigger than the others, which says that the first linear discriminant is way more important (at separating the groups) than either of the other two.\nAs to why we got three: there are 3 variables and 4 groups (soil types), and the smaller of 3 and \\(4-1\\) is 3.\n\\(\\blacksquare\\)\n\nWhich response variables do the important linear discriminants depend on? Answer this by extracting something from your discriminant analysis output.\n\nSolution\nThe table “coefficients of linear discriminants”. We said earlier that the only important discriminant is LD1. On that, the only notably non-zero coefficient is for herbicide; the ones for yield and water are close to zero. That is to say, the effects of the soil types play out through herbicide and not either of the other two variables.\nI didn’t ask you to, but you could check this by seeing how herbicide differs according to soil type:\n\nggplot(cornseed, aes(x = soil, y = herbicide)) + geom_boxplot()\n\n\n\n\nThe fields in loam soil needed more herbicide than the others.\nOr by water:\n\nggplot(cornseed, aes(x = soil, y = water)) + geom_boxplot()\n\n\n\n\nThere isn’t much difference in the amount of water needed between any of the fields, no matter what soil type.\nThis confirms that water is not distinguished by soil type, while herbicide is (at least to some extent).\n\\(\\blacksquare\\)\n\nObtain predictions for the discriminant analysis. (You don’t need to do anything with them yet.)\n\nSolution\nJust this, therefore:\n\ncornseed.pred &lt;- predict(cornseed.2)\n\n\\(\\blacksquare\\)\n\nPlot the first two discriminant scores against each other, coloured by soil type. You’ll have to start by making a data frame containing what you need.\n\nSolution\nI changed my mind from the past about how to do this. I make a big data frame out of the data and predictions (with cbind) and go from there:\n\nd &lt;- cbind(cornseed, cornseed.pred)\nhead(d)\n\n\n\n  \n\n\n\nThen we use this as input to ggplot:\n\nggplot(d, aes(x = x.LD1, y = x.LD2, colour = soil)) + geom_point()\n\n\n\n\n\\(\\blacksquare\\)\n\nOn your plot that you just made, explain briefly how LD1 distinguishes at least one of the soil types.\n\nSolution\nFind a soil type that is typically high (or low or average) on LD1. Any one or more of these will do: loam soils are typically high on LD1, clay soils or salty soils are typically low on LD1; sandy soils are typically average on LD1. (There are exceptions, but I’m looking for “typically”.)\n\\(\\blacksquare\\)\n\nOn your plot, does LD2 appear to do anything to separate the groups? Is this surprising given your earlier findings? Explain briefly.\n\nSolution\nAll the soil groups appear go to about the full height of the plot: that is to say, none of the groups appear to be especially at the top or the bottom. That means that LD2 does not separate the groups at all.\nBack in part (here), we said that the first linear discriminant is way more important than either of the other two, and here we see what that means: LD2 does nothing to separate the groups. So it’s not a surprising finding at all.\nI thought earlier about asking you to plot only the first linear discriminant, and now we see why: only the first one separates the groups. If you wanted to do that, you could make a boxplot of the discriminant scores by soil group, thus:\n\nggplot(d, aes(x = soil, y = x.LD1)) + geom_boxplot()\n\n\n\n\nThis says more or less the same thing as your plot of LD1 and LD2: loam has the highest LD1 score, sandy is about in the middle, and clay and salty have typically negative LD1 scores, similar to each other, though there is one outlying salty that looks a lot more like a loam.\n\\(\\blacksquare\\)\n\nMake a table of actual and predicted soil group. Which soil type was classified correctly the most often?\n\nSolution\n\nwith(d, table(obs = soil, pred = class))\n\n       pred\nobs     clay loam salty sandy\n  clay     3    0     3     2\n  loam     0    6     0     2\n  salty    1    1     5     1\n  sandy    2    1     1     4\n\n\nOr, the tidyverse way, which is below.\nThere were 8 fields of each soil type. The soil type that has the most of its fields classified correctly (based on the values of the response variables) has the biggest number down the diagonal of the table: looking at 3, 6, 5 and 4, we see that the loam soil type had the most of its fields classified correctly, so this was the most distinct from the others. (We also saw this on the plot of LD1 vs. LD2: the loam fields were all over on the right.)\nThis was easier because we had the same number of fields of each type. If we didn’t have that, the right way to go then would be to work out row percentages: “out of the fields that were actually sandy, what percent of them got classified as sandy”, and so on.\nThis is not a perfect classification, though, which is about what you would expect from the soil types being intermingled on the plot of LD1 vs. LD2. If you look at the table, salty and sandy are fairly distinct also, but clay is often confused with both of them. On the plot of LD1 and LD2, salty is generally to the left of sandy, but clay is mixed up with them both. The tidyverse way of doing this is equally good. This is the tidied-up way:\n\nd %&gt;% count(soil, class) %&gt;% \n  pivot_wider(names_from = class, values_from = n, values_fill = 0)\n\n\n\n  \n\n\n\nSix out of eight loams were correctly classified, which is better than anything else.\nExtra: we can calculate misclassification rates, first overall, which is easier:\n\nd %&gt;%\n  count(soil, class) %&gt;%\n  mutate(soil_stat = ifelse(soil == class, \"correct\", \"wrong\")) %&gt;%\n  count(soil_stat, wt = n)\n\n\n\n  \n\n\n\n\nd %&gt;%\n  count(soil, class) %&gt;%\n  mutate(soil_stat = ifelse(soil == class, \"correct\", \"wrong\")) %&gt;%\n  count(soil_stat, wt = n) %&gt;%\n  mutate(prop = nn / sum(nn))\n\nError in `mutate()`:\nℹ In argument: `prop = nn/sum(nn)`.\nCaused by error:\n! object 'nn' not found\n\n\nNote the use of wt on the second count to count the number of observations from the first count, not the number of rows.\nThis shows that 44% of the soil types were misclassified, which sounds awful, but is actually not so bad, considering. Bear in mind that if you were just guessing, you’d get 75% of them wrong, so getting 44% wrong is quite a bit better than that. The variables (especially herbicide) are at least somewhat informative about soil type; it’s better to know them than not to.\nOr do it by actual soil type:\n\nd %&gt;%\n  count(soil, class) %&gt;%\n  group_by(soil) %&gt;%\n  mutate(soil_stat = ifelse(soil == class, \"correct\", \"wrong\")) %&gt;%\n  count(soil_stat, wt = n)\n\n\n\n  \n\n\n\n\nd %&gt;%\n  count(soil, class) %&gt;%\n  group_by(soil) %&gt;%\n  mutate(soil_stat = ifelse(soil == class, \"correct\", \"wrong\")) %&gt;%\n  count(soil_stat, wt = n) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  select(-n) %&gt;%\n  pivot_wider(names_from=soil_stat, values_from=prop)\n\n\n\n  \n\n\n\nLoam soil was the easiest to get right, and clay was easiest to get wrong. However, these proportions were each based on only eight observations, so it’s probably wise not to say that loam is always easiest to get right.\nI didn’t have you look at posterior probabilities here.14 With 32 fields, this is rather a lot to list them all, but what we can do is to look at the ones that were misclassified (the true soil type differs from the predicted soil type). Before that, though, we need to make a data frame with the stuff in it that we want to look at. And before that, I want to round the posterior probabilities to a small number of decimals.\nThen, we can fire away with this:\n\nd %&gt;%\n  mutate(across(starts_with(\"posterior\"), \n                \\(post) round(post, 3))) %&gt;%\n  mutate(row = row_number()) -&gt; dd\ndd %&gt;% filter(soil != class)\n\n\n\n  \n\n\n\nMost of the posterior probabilities are neither especially small nor especially large, which adds to the impression that things are really rather uncertain. For example, field 8 could have been either loam (0.352) or sandy (0.373). There was one field that was actually salty but looked like a loam one (with LD1 score around 2); this is field 20, that needed a lot of herbicide; it was rated to have an 82% chance of being loam and only 1% chance of salty.\nLet’s remind ourselves of why we were doing this: the MANOVA was significant, so at least some of the fields were different on some of the variables from some of the others. What we found by doing the discriminant analysis was that only the first discriminant was of any value in distinguishing the soil types by the variables we measured, and that was mostly herbicide. So the major effect that soil type had was on the amount of herbicide needed, with the loam soils needing most.\nI wanted to finish with one more thing, which was to look again at the soils that were actually loam:\n\ndd %&gt;%\n  filter(soil == \"loam\") %&gt;%\n  select(soil, yield, water, herbicide, class, starts_with(\"posterior\"))\n\n\n\n  \n\n\n\nFields 7 and 8 could have been pretty much any type of soil; sandy came out with the highest posterior probability, so that’s what they were predicted (wrongly) to be. Some of the fields, 1, 3 and 5, were clearly (and correctly) loam. For 1 and 5, you can clearly see that this is because herbicide was high, but field 3 is more of a mystery. For this field, herbicide is not high, so one or more of the other variables must be pointing towards loam.\nWe can obtain predicted LD1 scores for various combinations of “typical” values of the response variables and see what has what effect on LD1:\n\nsummary(cornseed)\n\n     field           soil               yield           water      \n Min.   : 1.00   Length:32          Min.   :45.00   Min.   :14.50  \n 1st Qu.: 8.75   Class :character   1st Qu.:50.58   1st Qu.:25.75  \n Median :16.50   Mode  :character   Median :61.40   Median :29.60  \n Mean   :16.50                      Mean   :61.61   Mean   :31.17  \n 3rd Qu.:24.25                      3rd Qu.:67.00   3rd Qu.:36.83  \n Max.   :32.00                      Max.   :96.10   Max.   :54.20  \n   herbicide     \n Min.   : 1.100  \n 1st Qu.: 3.075  \n Median : 4.750  \n Mean   : 4.672  \n 3rd Qu.: 5.825  \n Max.   :11.700  \n\n\nThe problem is that the variables have different spreads. Let’s do some predictions (ie. calculations) of LD1 score for combinations of quartiles of our response variables. I like quartiles because these are “representative” values of the variables, typical of how far up and down they go. This process is one you’ve seen before:\n\nyields &lt;- c(51, 67)\nwaters &lt;- c(26, 37)\nherbicides &lt;- c(3, 6)\nnew &lt;- crossing(yield = yields, water = waters, herbicide = herbicides)\npred &lt;- predict(cornseed.2, new)\ncbind(new, pred$x) %&gt;% arrange(desc(LD1))\n\n\n\n  \n\n\n\nI arranged the predicted LD1 scores in descending order, so the most loam-like combinations are at the top. The top two combinations look like loam; they both have high herbicide, as we figured before. But they also have high yield. That might go some way towards explaining why field 3, with its non-high herbicide, was confidently predicted to be loam:\n\ncornseed %&gt;% filter(field == 3)\n\n\n\n  \n\n\n\nThis has a very high yield, and that is what is making us (correctly) think it is loam.\nI suddenly remembered that I hadn’t done a biplot of this one, which I could, since it’s a discriminant analysis:\n\nggbiplot(cornseed.2, groups = cornseed$soil)\n\n\n\n\nThis shows the dominant influence of herbicide on LD1 score (more herbicide is more positive), and that water has nothing to say (in terms of distinguishing soil types) and yield has not much to say, their arrows being short. That observation with a non-high herbicide that was predicted to be had the highest yield of all, so even the small influence of yield on LD1 made a big difference here.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "discriminant-analysis.html#understanding-athletes-height-weight-sport-and-gender-1",
    "href": "discriminant-analysis.html#understanding-athletes-height-weight-sport-and-gender-1",
    "title": "32  Discriminant analysis",
    "section": "32.14 Understanding athletes’ height, weight, sport and gender",
    "text": "32.14 Understanding athletes’ height, weight, sport and gender\nOn a previous assignment, we used MANOVA on the athletes data to demonstrate that there was a significant relationship between the combination of the athletes’ height and weight, with the sport they play and the athlete’s gender. The problem with MANOVA is that it doesn’t give any information about the kind of relationship. To understand that, we need to do discriminant analysis, which is the purpose of this question.\nThe data can be found at link.\n\nOnce again, read in and display (some of) the data, bearing in mind that the data values are separated by tabs. (This ought to be a free two marks.)\n\nSolution\nNothing new here:\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/ais.txt\"\nathletes &lt;- read_tsv(my_url)\n\nRows: 202 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nathletes\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nUse unite to make a new column in your data frame which contains the sport-gender combination. Display it. (You might like to display only a few columns so that it is clear that you did the right thing.) Hint: you’ve seen unite in the peanuts example in class.\n\nSolution\nThe columns to combine are called Sport and Sex, with Capital Letters. The syntax for unite is that you give the name of the new combo column first, and then the names of the columns you want to combine, either by listing them or by using a select-helper. They will be separated by an underscore by default, which is usually easiest to handle.15 In unite, you can group the columns to “unite” with c(), as in class, or not, as here. Either way is good.16 We’ll be using height and weight in the analysis to come, so I decided to display just those:\n\nathletes %&gt;%\n  unite(combo, Sport, Sex) -&gt; athletesc\nathletesc %&gt;% select(combo, Ht, Wt)\n\n\n\n  \n\n\n\nI gave the data frame a new name, since I might want to come back to the original later. Also, displaying only those columns gives more width for the display of my combo, so that I can be sure I got it right.\nExtra: there is another column, SSF, that begins with S, so the select-helper thing is not so obviously helpful here. But the two columns we want start with S followed by either e or p, so we could do this:\n\nathletes %&gt;%\n  unite(combo, matches(\"^S(e|p)\")) %&gt;%\n  select(combo, Ht, Wt)\n\n\n\n  \n\n\n\nThe matches takes a so-called regular expression. This one says ``starting at the beginning of the column name, find an uppercase S followed by either a lowercase e or a lowercase p’’. This picks out the columns and only the columns we want. In the opposite order, though (either order is fine).\nI have a feeling we can also take advantage of the fact that the two columns we want to unite are the only two text ones:\n\nathletes %&gt;% \n  unite(combo, where(is.character))\n\n\n\n  \n\n\n\nI wasn’t expecting that to work!\n\\(\\blacksquare\\)\n\nRun a discriminant analysis “predicting” sport-gender combo from height and weight. Display the results. (No comment needed yet.)\n\nSolution\nThat would be this. I’m having my familiar trouble with names:\n\ncombo.1 &lt;- lda(combo ~ Ht + Wt, data = athletesc)\n\nIf you used a new name for the data frame with the sport-gender combinations in it, use that new name here.\nThe output:\n\ncombo.1\n\nCall:\nlda(combo ~ Ht + Wt, data = athletesc)\n\nPrior probabilities of groups:\n  BBall_female     BBall_male   Field_female     Field_male     Gym_female \n    0.06435644     0.05940594     0.03465347     0.05940594     0.01980198 \nNetball_female     Row_female       Row_male    Swim_female      Swim_male \n    0.11386139     0.10891089     0.07425743     0.04455446     0.06435644 \n  T400m_female     T400m_male  Tennis_female    Tennis_male  TSprnt_female \n    0.05445545     0.08910891     0.03465347     0.01980198     0.01980198 \n   TSprnt_male     WPolo_male \n    0.05445545     0.08415842 \n\nGroup means:\n                     Ht       Wt\nBBall_female   182.2692 71.33077\nBBall_male     195.5833 88.92500\nField_female   172.5857 80.04286\nField_male     185.2750 95.76250\nGym_female     153.4250 43.62500\nNetball_female 176.0870 69.59348\nRow_female     178.8591 72.90000\nRow_male       187.5333 86.80667\nSwim_female    173.1778 65.73333\nSwim_male      185.6462 81.66154\nT400m_female   169.3364 57.23636\nT400m_male     179.1889 68.20833\nTennis_female  168.5714 58.22857\nTennis_male    183.9500 75.40000\nTSprnt_female  170.4750 59.72500\nTSprnt_male    178.5364 75.79091\nWPolo_male     188.2235 86.72941\n\nCoefficients of linear discriminants:\n          LD1        LD2\nHt 0.08898971 -0.1888615\nWt 0.06825230  0.1305246\n\nProportion of trace:\n   LD1    LD2 \n0.7877 0.2123 \n\n\nI comment here that there are two linear discriminants because there are two variables (height and weight) and actually 17 groups (not quite \\(2\\times 10\\) because some sports are played by athletes of only one gender). The smaller of 2 and \\(17-1\\) is 2. (I often ask about this, but am choosing not to here.)\n\\(\\blacksquare\\)\n\nWhat kind of height and weight would make an athlete have a large (positive) score on LD1? Explain briefly.\n\nSolution\nThe Coefficients of Linear Discriminants for LD1 are both positive, so an athlete with a large positive score on LD1 has a large height and weight: that is to say, they are tall and heavy.\n\\(\\blacksquare\\)\n\nMake a guess at the sport-gender combination that has the highest score on LD1. Why did you choose the combination you did?\n\nSolution\nI could have made you guess the smallest score on LD1, but that would have been too easy (female gymnasts). For this one, you want a sport-gender combination that is typically tall and heavy, and you can look in the table of Group Means to help you find a candidate group. I think the two best guesses are male basketball players (tallest and nearly the heaviest) and male field athletes (heaviest and among the group of athletes that are second-tallest behind the male basketball players). I don’t so much mind what you guess, as long as you make a sensible call about a group that is reasonably tall and reasonably heavy (or, I suppose, that matches with what you said in the previous part, whatever that was).\n\\(\\blacksquare\\)\n\n What combination of height and weight would make an athlete have a small* (that is, very negative) score on LD2? Explain briefly.\n\nSolution\nThe italics in the question are something to do with questions that have a link to them in Bookdown. I don’t know how to fix that. Going back to the Coefficients of Linear Discriminants, the coefficient for Height is negative, and the one for Weight is positive. What will make an athlete come out small (very negative) on this is if they have a large height and a small weight. To clarify your thinking on this, think of the heights and weights as being standardized, so that a big one will be positive and a small one will be negative. To make LD2 very negative, you want a “plus” height to multiply the minus sign, and a “minus” weight multiplying the plus sign. Extra: what is happening here is that LD1 gives the most important way in which the groups differ, and LD2 the next-most important. There is generally a positive correlation between height and weight (taller athletes are generally heavier), so the most important “dimension” is the big-small one with tall heavy athletes at one end and short light athletes at the other. The Proportion of trace in the output says that LD1 is definitely more important, in terms of separating the groups, than LD2 is, but the latter still has some value.\n\\(\\blacksquare\\)\n\nObtain predictions for the discriminant analysis, and use these to make a plot of LD1 score against LD2 score, with the individual athletes distinguished by what sport they play and gender they are. (You can use colour to distinguish them, or you can use shapes. If you want to go the latter way, there are clues in my solutions to the MANOVA question about these athletes.)\n\nSolution\nThe prediction part is only one step:\n\np &lt;- predict(combo.1)\n\nOne point for this.\nThis, in case you are wondering, is obtaining predicted group membership and LD scores for the original data, that is, for our 202 athletes.\nI prefer (no obligation) to take a look at what I have. My p is actually a list:\n\nclass(p)\n\n[1] \"list\"\n\nglimpse(p)\n\nList of 3\n $ class    : Factor w/ 17 levels \"BBall_female\",..: 12 6 6 6 7 6 6 6 6 6 ...\n $ posterior: num [1:202, 1:17] 0.1235 0.0493 0.084 0.0282 0.1538 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:202] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:17] \"BBall_female\" \"BBall_male\" \"Field_female\" \"Field_male\" ...\n $ x        : num [1:202, 1:2] -1.325 -1.487 -0.96 -1.885 0.114 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:202] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:2] \"LD1\" \"LD2\"\n\n\nOur standard procedure is to cbind the predictions together with the original data (including the combo), and get a huge data frame (in this case):\n\nd &lt;- cbind(athletesc, p)\nhead(d)\n\n\n\n  \n\n\n\nAnd so, to the graph:\n\nggplot(d, aes(x = x.LD1, y = x.LD2, colour = combo)) + geom_point()\n\n\n\n\nIf you can distinguish seventeen different colours, your eyes are better than mine! You might prefer to use seventeen different shapes, although I wonder how much better that will be:\n\nggplot(d, aes(x = x.LD1, y = x.LD2, shape = combo)) + geom_point() +\n  scale_shape_manual(values = 1:17)\n\n\n\n\nYou have to do something special to get as many as seventeen shapes. This idea came from the MANOVA question in the last assignment.\nOr even this:\n\nggplot(d, aes(x = x.LD1, y = x.LD2, shape = combo, colour = combo)) + geom_point() +\n  scale_shape_manual(values = 1:17)\n\n\n\n\nPerhaps having colours and shapes makes the combos easier to distinguish. We’re beginning to stray onto the boundary between statistics and aesthetics here!\nExtra: earlier, I asked you to guess which group(s) of athletes had a high (positive) score on LD1. These are the ones on the right side of this plot: male basketball players bottom right and male field athletes top right. Was that what you guessed? What about the other guesses you might have made?\n\\(\\blacksquare\\)\n\nLook on your graph for the four athletes with the smallest (most negative) scores on LD2. What do they have in common? Does this make sense, given your answer to part (here)? Explain briefly.\n\nSolution\nThese are the four athletes at the bottom of the plot. If you can distinguish the colours, two of these are red and two of them are orange, so they are all basketball players (two male and two female). If you plotted the shapes, and you used the same shapes I did, two of them are circles and the other two are upward-facing triangles, leading you to the same conclusion. (You could also denote each combo by a letter and plot with those letters, as per the solutions to the last assignment.) Back in part (here), I said that what would make an athlete come out very negative on LD2 is if they were tall and not heavy. This is the stereotypical description of a basketball player, so it makes perfect sense to me.\nExtra: some basketball players are tall and heavier; these are the ones on the right of the plot, with a larger LD1 score, to reflect that they are both tall and heavy, but with an LD2 score closer to zero, reflecting that, given how tall they are, their weight is about what you’d expect. LD2 is really saying something like “weight relative to height”, with someone at the top of the picture being unusually heavy and someone at the bottom unusually light.\n\\(\\blacksquare\\)\n\nObtain a (very large) square table, or a (very long) table with frequencies, of actual and predicted sport-gender combinations. You will probably have to make the square table very small to fit it on the page. For that, displaying the columns in two or more sets is OK (for example, six columns and all the rows, six more columns and all the rows, then the last five columns for all the rows). Are there any sport-gender combinations that seem relatively easy to classify correctly? Explain briefly.\n\nSolution\nLet’s see what happens:\n\ntab &lt;- with(d, table(combo, class))\ntab\n\n                class\ncombo            BBall_female BBall_male Field_female Field_male Gym_female\n  BBall_female              3          1            0          0          0\n  BBall_male                0          9            0          0          0\n  Field_female              0          0            5          0          0\n  Field_male                0          1            0          7          0\n  Gym_female                0          0            0          0          4\n  Netball_female            0          0            1          0          0\n  Row_female                0          0            0          0          1\n  Row_male                  0          2            0          1          0\n  Swim_female               0          0            0          0          0\n  Swim_male                 0          4            0          0          0\n  T400m_female              0          0            0          0          0\n  T400m_male                3          1            0          0          0\n  Tennis_female             0          0            1          0          1\n  Tennis_male               1          0            0          0          0\n  TSprnt_female             0          0            0          0          0\n  TSprnt_male               0          0            0          0          0\n  WPolo_male                1          3            0          2          0\n                class\ncombo            Netball_female Row_female Row_male Swim_female Swim_male\n  BBall_female                5          1        0           0         0\n  BBall_male                  0          0        0           0         2\n  Field_female                1          0        0           0         0\n  Field_male                  0          2        0           0         0\n  Gym_female                  0          0        0           0         0\n  Netball_female             13          4        0           0         0\n  Row_female                  5         10        0           0         1\n  Row_male                    0          0        1           0         0\n  Swim_female                 4          1        0           0         0\n  Swim_male                   2          3        0           0         0\n  T400m_female                3          0        0           0         0\n  T400m_male                  5          3        0           0         0\n  Tennis_female               2          0        0           0         0\n  Tennis_male                 0          3        0           0         0\n  TSprnt_female               1          0        0           0         0\n  TSprnt_male                 6          3        0           0         0\n  WPolo_male                  0          3        1           0         0\n                class\ncombo            T400m_female T400m_male Tennis_female Tennis_male\n  BBall_female              0          2             0           0\n  BBall_male                0          0             0           0\n  Field_female              1          0             0           0\n  Field_male                0          0             0           0\n  Gym_female                0          0             0           0\n  Netball_female            1          4             0           0\n  Row_female                0          4             0           0\n  Row_male                  1          0             0           0\n  Swim_female               3          1             0           0\n  Swim_male                 0          1             0           0\n  T400m_female              6          2             0           0\n  T400m_male                1          5             0           0\n  Tennis_female             2          1             0           0\n  Tennis_male               0          0             0           0\n  TSprnt_female             2          1             0           0\n  TSprnt_male               0          0             0           0\n  WPolo_male                0          0             0           0\n                class\ncombo            TSprnt_female TSprnt_male WPolo_male\n  BBall_female               0           0          1\n  BBall_male                 0           0          1\n  Field_female               0           0          0\n  Field_male                 0           0          2\n  Gym_female                 0           0          0\n  Netball_female             0           0          0\n  Row_female                 0           0          1\n  Row_male                   0           0         10\n  Swim_female                0           0          0\n  Swim_male                  0           0          3\n  T400m_female               0           0          0\n  T400m_male                 0           0          0\n  Tennis_female              0           0          0\n  Tennis_male                0           0          0\n  TSprnt_female              0           0          0\n  TSprnt_male                0           0          2\n  WPolo_male                 0           0          7\n\n\nThat’s kind of long.\nFor combos that are easy to classify, you’re looking for a largish number on the diagonal of the table (classified correctly), bearing in mind that you only see about four columns of the table at once, and (much) smaller numbers in the rest of the row and column. I don’t mind which ones you pick out, but see if you can find a few:\n\nMale basketball players (9 out of 12 classified correctly)\nMale field athletes (7 out of 10 classified correctly)\nFemale netball players (13 out of about 23)\nFemale rowers (10 out of about 22)\n\nOr you can turn it into a tibble:\n\ntab %&gt;% as_tibble()\n\n\n\n  \n\n\n\nThis makes the tidyverse output, with frequencies. You probably want to omit the zero ones:\n\ntab %&gt;% as_tibble() %&gt;% filter(n &gt; 0)\n\n\n\n  \n\n\n\nThis is the same output as below. See there for comments.\nThe other, perhaps easier, way to tackle this one is the tidyverse way, making a “long” table of frequencies. Here is some of it. You’ll be able to click to see more:\n\nd %&gt;% count(combo, class)\n\n\n\n  \n\n\n\nThe zeroes never show up here. The combo column is the truth, and the class column is the prediction. Again, you can see where the big frequencies are; a lot of the female netball players were gotten right, but there were a lot of them to begin with.\nExtra: let’s see if we can work out proportions correct. I’ve changed my mind from how I originally wrote this. I still use count, but I start with the overall misclassification. Let’s take it in steps:\n\nd %&gt;%\n  count(combo, class) %&gt;%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\"))\n\n\n\n  \n\n\n\nThat makes a new column stat that contains whether the predicted sport-gender combination was correct or wrong. For an overall misclassification rate we have to count these, but not simply counting the number of rows; rather, we need to total up the things in the n column:\n\nd %&gt;%\n  count(combo, class) %&gt;%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %&gt;%\n  count(stat, wt = n)\n\n\n\n  \n\n\n\nThis tells us how many predictions overall were right and how many wrong.\nTo make those into proportions, another mutate, dividing by the total of n:\n\nd %&gt;%\n  count(combo, class) %&gt;%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %&gt;%\n  count(stat, wt = n) %&gt;%\n  mutate(proportion = n / sum(n))\n\n\n\n  \n\n\n\n65% of the sport-gender combinations were misclassified. This is awful, but is a lot better than guessing (we’d then get about 5% of them right and about 95% wrong).\nThere’s a subtlety here that will make sense when we do the corresponding calculation by sport-gender combination. To do that, we put a group_by(combo) either before or after we define stat (it doesn’t matter which way):\n\nd %&gt;%\n  count(combo, class) %&gt;%\n  group_by(combo) %&gt;%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %&gt;%\n  count(stat, wt = n) %&gt;%\n  mutate(proportion = n / sum(n))\n\n\n\n  \n\n\n\nThat last sum(n): what is it summing over? The answer is “within combo”, since that is the group_by. You see that the two proportion values within, say, BBall_female, add up to 1.\nWe don’t actually see all the answers, because there are too many of them. Let’s try to get the proportion correct and wrong in their own columns. This almost works:\n\nd %&gt;%\n  count(combo, class) %&gt;%\n  group_by(combo) %&gt;%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %&gt;%\n  count(stat, wt = n) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  pivot_wider(names_from=stat, values_from=proportion)\n\n\n\n  \n\n\n\nThis doesn’t work because everything outside of the pivot_wider is tested for uniqueness; if it’s unique, it gets its own row. Thus, BBall_male and 3 is different from BBall_male and 9. But we only want one row of BBall_male. I think the easiest way around this is to get rid of n, since it has served its purpose:\n\nd %&gt;%\n  count(combo, class) %&gt;%\n  group_by(combo) %&gt;%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %&gt;%\n  count(stat, wt = n) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  select(-n) %&gt;%\n  pivot_wider(names_from=stat, values_from=proportion, values_fill = list(proportion=0))\n\n\n\n  \n\n\n\nOne extra thing: some of the proportion values were missing, because there weren’t any misclassified (or maybe correctly-classified!) athletes. The values_fill sets any missings in proportion to zero.\nWhile we’re about it, let’s arrange in order of misclassification probability:\n\nd %&gt;%\n  count(combo, class) %&gt;%\n  group_by(combo) %&gt;%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %&gt;%\n  count(stat, wt = n) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  select(-n) %&gt;%\n  pivot_wider(names_from=stat, values_from=proportion, values_fill = list(proportion=0)) %&gt;% \n  replace_na(list(correct = 0, wrong = 0)) %&gt;%\n  arrange(wrong)\n\n\n\n  \n\n\n\nThe most distinctive athletes were the female gymnasts (tiny!), followed by the male basketball players (tall) and the female field athletes (heavy). These were easiest to predict from their height and weight. The ones at the bottom of the list were very confusible since the discriminant analysis guessed them all wrong! So what were the most common misclassifications? Let’s go back to this:\n\nhead(d)\n\n\n\n  \n\n\nd %&gt;%\n  count(combo, class) %&gt;%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\"))\n\n\n\n  \n\n\n\nWe want to express those n values as proportions out of their actual sport-gender combo, so we group by combo before defining the proportions:\n\nd %&gt;%\n  count(combo, class) %&gt;%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %&gt;%\n  group_by(combo) %&gt;%\n  mutate(proportion = n / sum(n))\n\n\n\n  \n\n\n\nOnly pick out the ones that were gotten wrong, and arrange the remaining proportions in descending order:\n\nd %&gt;%\n  count(combo, class) %&gt;%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %&gt;%\n  group_by(combo) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  filter(stat == \"wrong\") %&gt;%\n  arrange(desc(proportion))\n\n\n\n  \n\n\n\nThe embarrassment champion is the three male tennis players that were taken to be — female rowers! Most of the other mistakes are more forgivable: the male rowers being taken for male water polo players, for example.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "discriminant-analysis.html#footnotes",
    "href": "discriminant-analysis.html#footnotes",
    "title": "32  Discriminant analysis",
    "section": "",
    "text": "Grammatically, I am supposed to write this as “two hundred and forty-four” in words, since I am not supposed to start a sentence with a number. But, I say, deal with it. Or, I suppose, “there are 244 people who work…”.↩︎\nIf you run into an error like “Argument 2 must have names” here, that means that the second thing, class, needs to have a name and doesn’t have one.↩︎\nThis was why we were doing discriminant analysis in the first place.↩︎\nThat sounds like the ultimate in evasiveness!↩︎\nEvery time I do this, I forget to put the summary around the outside, so I get the long ugly version of the output rather than the short pretty version.↩︎\nFor suitable definitions of fun.↩︎\nGrammatically, I am supposed to write this as “two hundred and forty-four” in words, since I am not supposed to start a sentence with a number. But, I say, deal with it. Or, I suppose, “there are 244 people who work…”.↩︎\nUntil much later.↩︎\nWhen you’re recording the data, you may find it convenient to use short codes to represent the possibly long factor levels, but in that case you should also use a codebook so that you know what the codes represent. When I read the data into R, I would create a factor with named levels, like I did here, if I don’t already have one.↩︎\nI discovered that I used pp twice, and I want to use the first one again later, so I had to rename this one.↩︎\nSo we should do it, when assessing how good the classification is.↩︎\nI have to learn to come up with better names.↩︎\nstr-replace is from stringr, and takes three inputs: a piece of text, the text to look for, and the text to replace it with. The piece of text in this case is one of the columns whose name starts with posterior; the dot represents it, that is to say “one of the columns whose name starts with posterior.↩︎\nRest assured that I did on the final exam!↩︎\nThe opposite of unite is separate, which splits a combined column like my combo into separate columns; it too uses underscore as the default separator.↩︎\nYou used to have to group them, but you don’t any more. Hence my old code has them grouped, but my new code does not.↩︎"
  },
  {
    "objectID": "cluster.html#sites-on-the-sea-bed",
    "href": "cluster.html#sites-on-the-sea-bed",
    "title": "33  Hierarchical cluster analysis",
    "section": "33.1 Sites on the sea bed",
    "text": "33.1 Sites on the sea bed\nBiologists investigate the prevalence of species of organism by sampling sites where the organisms might be, taking a “grab” from the site, and sending the grabs to a laboratory for analysis. The data in this question come from the sea bed. There were 30 sites, labelled s1 through s30. At each site, five species of organism, labelled a through e, were of interest; the data shown in those columns of the data set were the number of organisms of that species identified in the grab from that site. There are some other columns in the (original) data set that will not concern us. Our interest is in seeing which sites are similar to which other sites, so that a cluster analysis will be suitable.\nWhen the data are counts of different species, as they are here, biologists often measure the dissimilarity in species prevalence profiles between two sites using something called the Bray-Curtis dissimilarity. It is not important to understand this for this question (though I explain it in my solutions). I calculated the Bray-Curtis dissimilarity between each pair of sites and stored the results in link.\n\nRead in the dissimilarity data and check that you have 30 rows and 30 columns of dissimilarities.\nCreate a distance object out of your dissimilarities, bearing in mind that the values are distances (well, dissimilarities) already.\nFit a cluster analysis using single-linkage, and display a dendrogram of the results.\nNow fit a cluster analysis using Ward’s method, and display a dendrogram of the results.\n* On the Ward’s method clustering, how many clusters would you choose to divide the sites into? Draw rectangles around those clusters.\n* The original data is in link. Read in the original data and verify that you again have 30 sites, variables called a through e and some others.\nGo back to your Ward method dendrogram with the red rectangles and find two sites in the same cluster. Display the original data for your two sites and see if you can explain why they are in the same cluster. It doesn’t matter which two sites you choose; the grader will merely check that your results look reasonable.\nObtain the cluster memberships for each site, for your preferred number of clusters from part (here). Add a column to the original data that you read in, in part (here), containing those cluster memberships, as a factor. Obtain a plot that will enable you to assess the relationship between those clusters and pollution. (Once you have the cluster memberships, you can add them to the data frame and make the graph using a pipe.) What do you see?"
  },
  {
    "objectID": "cluster.html#dissimilarities-between-fruits",
    "href": "cluster.html#dissimilarities-between-fruits",
    "title": "33  Hierarchical cluster analysis",
    "section": "33.2 Dissimilarities between fruits",
    "text": "33.2 Dissimilarities between fruits\nConsider the fruits apple, orange, banana, pear, strawberry, blueberry. We are going to work with these four properties of fruits:\n\nhas a round shape\nIs sweet\nIs crunchy\nIs a berry\n\n\nMake a table with fruits as columns, and with rows “round shape”, “sweet”, “crunchy”, “berry”. In each cell of the table, put a 1 if the fruit has the property named in the row, and a 0 if it does not. (This is your opinion, and may not agree with mine. That doesn’t matter, as long as you follow through with whatever your choices were.)\nWe’ll define the dissimilarity between two fruits to be the number of qualities they disagree on. Thus, for example, the dissimilarity between Apple and Orange is 1 (an apple is crunchy and an orange is not, but they agree on everything else). Calculate the dissimilarity between each pair of fruits, and make a square table that summarizes the results. (To save yourself some work, note that the dissimilarity between a fruit and itself must be zero, and the dissimilarity between fruits A and B is the same as that between B and A.) Save your table of dissimilarities into a file for the next part.\nDo a hierarchical cluster analysis using complete linkage. Display your dendrogram.\nHow many clusters, of what fruits, do you seem to have? Explain briefly.\nPick a pair of clusters (with at least 2 fruits in each) from your dendrogram. Verify that the complete-linkage distance on your dendrogram is correct."
  },
  {
    "objectID": "cluster.html#similarity-of-species",
    "href": "cluster.html#similarity-of-species",
    "title": "33  Hierarchical cluster analysis",
    "section": "33.3 Similarity of species",
    "text": "33.3 Similarity of species\nTwo scientists assessed the dissimilarity between a number of species by recording the number of positions in the protein molecule cytochrome-\\(c\\) where the two species being compared have different amino acids. The dissimilarities that they recorded are in link.\n\nRead the data into a data frame and take a look at it.\nBearing in mind that the values you read in are already dissimilarities, convert them into a dist object suitable for running a cluster analysis on, and display the results. (Note that you need to get rid of any columns that don’t contain numbers.)\nRun a cluster analysis using single-linkage and obtain a dendrogram.\nRun a cluster analysis using Ward’s method and obtain a dendrogram.\nDescribe how the two dendrograms from the last two parts look different.\nLooking at your clustering for Ward’s method, what seems to be a sensible number of clusters? Draw boxes around those clusters.\nList which cluster each species is in, for your preferred number of clusters (from Ward’s method)."
  },
  {
    "objectID": "cluster.html#bridges-in-pittsburgh",
    "href": "cluster.html#bridges-in-pittsburgh",
    "title": "33  Hierarchical cluster analysis",
    "section": "33.4 Bridges in Pittsburgh",
    "text": "33.4 Bridges in Pittsburgh\nThe city of Pittsburgh, Pennsylvania, lies where three rivers, the Allegheny, Monongahela, and Ohio, meet.1 It has long been important to build bridges there, to enable its residents to cross the rivers safely. See link for a listing (with pictures) of the bridges. The data at link contains detail for a large number of past and present bridges in Pittsburgh. All the variables we will use are categorical. Here they are:\n\nid identifying the bridge (we ignore)\nriver: initial letter of river that the bridge crosses\nlocation: a numerical code indicating the location within Pittsburgh (we ignore)\nerected: time period in which the bridge was built (a name, from CRAFTS, earliest, to MODERN, most recent.\npurpose: what the bridge carries: foot traffic (“walk”), water (aqueduct), road or railroad.\nlength categorized as long, medium or short.\nlanes of traffic (or number of railroad tracks): a number, 1, 2, 4 or 6, that we will count as categorical.\nclear_g: whether a vertical navigation requirement was included in the bridge design (that is, ships of a certain height had to be able to get under the bridge). I think G means “yes”.\nt_d: method of construction. DECK means the bridge deck is on top of the construction, THROUGH means that when you cross the bridge, some of the bridge supports are next to you or above you.\nmaterial the bridge is made of: iron, steel or wood.\nspan: whether the bridge covers a short, medium or long distance.\nrel_l: Relative length of the main span of the bridge (between the two central piers) to the total crossing length. The categories are S, S-F and F. I don’t know what these mean.\ntype of bridge: wood, suspension, arch and three types of truss bridge: cantilever, continuous and simple.\n\nThe website link is an excellent source of information about bridges. (That’s where I learned the difference between THROUGH and DECK.) Wikipedia also has a good article at link. I also found link which is the best description I’ve seen of the variables.\n\nThe bridges are stored in CSV format. Some of the information is not known and was recorded in the spreadsheet as ?. Turn these into genuine missing values by adding na=\"?\" to your file-reading command. Display some of your data, enough to see that you have some missing data.\nVerify that there are missing values in this dataset. To see them, convert the text columns temporarily to factors using mutate, and pass the resulting data frame into summary.\nUse drop_na to remove any rows of the data frame with missing values in them. How many rows do you have left?\nWe are going to assess the dissimilarity between two bridges by the number of the categorical variables they disagree on. This is called a “simple matching coefficient”, and is the same thing we did in the question about clustering fruits based on their properties. This time, though, we want to count matches in things that are rows of our data frame (properties of two different bridges), so we will need to use a strategy like the one I used in calculating the Bray-Curtis distances. First, write a function that takes as input two vectors v and w and counts the number of their entries that differ (comparing the first with the first, the second with the second, , the last with the last. I can think of a quick way and a slow way, but either way is good.) To test your function, create two vectors (using c) of the same length, and see whether it correctly counts the number of corresponding values that are different.\nWrite a function that has as input two row numbers and a data frame to take those rows from. The function needs to select all the columns except for id and location, select the rows required one at a time, and turn them into vectors. (There may be some repetitiousness here. That’s OK.) Then those two vectors are passed into the function you wrote in the previous part, and the count of the number of differences is returned. This is like the code in the Bray-Curtis problem. Test your function on rows 3 and 4 of your bridges data set (with the missings removed). There should be six variables that are different.\nCreate a matrix or data frame of pairwise dissimilarities between each pair of bridges (using only the ones with no missing values). Use loops, or crossing and rowwise, as you prefer. Display the first six rows of your matrix (using head) or the first few rows of your data frame. (The whole thing is big, so don’t display it all.)\nTurn your matrix or data frame into a dist object. (If you couldn’t create a matrix or data frame of dissimilarities, read them in from link.) Do not display your distance object.\nRun a cluster analysis using Ward’s method, and display a dendrogram. The labels for the bridges (rows of the data frame) may come out too big; experiment with a cex less than 1 on the plot so that you can see them.\nHow many clusters do you think is reasonable for these data? Draw them on your plot.\nPick three bridges in the same one of your clusters (it doesn’t matter which three bridges or which cluster). Display the data for these bridges. Does it make sense that these three bridges ended up in the same cluster? Explain briefly.\n\nMy solutions follow:"
  },
  {
    "objectID": "cluster.html#sites-on-the-sea-bed-1",
    "href": "cluster.html#sites-on-the-sea-bed-1",
    "title": "33  Hierarchical cluster analysis",
    "section": "33.5 Sites on the sea bed",
    "text": "33.5 Sites on the sea bed\nBiologists investigate the prevalence of species of organism by sampling sites where the organisms might be, taking a “grab” from the site, and sending the grabs to a laboratory for analysis. The data in this question come from the sea bed. There were 30 sites, labelled s1 through s30. At each site, five species of organism, labelled a through e, were of interest; the data shown in those columns of the data set were the number of organisms of that species identified in the grab from that site. There are some other columns in the (original) data set that will not concern us. Our interest is in seeing which sites are similar to which other sites, so that a cluster analysis will be suitable.\nWhen the data are counts of different species, as they are here, biologists often measure the dissimilarity in species prevalence profiles between two sites using something called the Bray-Curtis dissimilarity. It is not important to understand this for this question (though I explain it in my solutions). I calculated the Bray-Curtis dissimilarity between each pair of sites and stored the results in link.\n\nRead in the dissimilarity data and check that you have 30 rows and 30 columns of dissimilarities.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/seabed1.csv\"\nseabed &lt;- read_csv(my_url)\n\nRows: 30 Columns: 30\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (30): s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14, s15, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nseabed\n\n\n\n  \n\n\n\nCheck. The columns are labelled with the site names. (As I originally set this question, the data file was read in with read.csv instead, and the site names were read in as row names as well: see discussion elsewhere about row names. But in the tidyverse we don’t have row names.)\n\\(\\blacksquare\\)\n\nCreate a distance object out of your dissimilarities, bearing in mind that the values are distances (well, dissimilarities) already.\n\nSolution\nThis one needs as.dist to convert already-distances into a dist object. (dist would have calculated distances from things that were not distances/dissimilarities yet.)\n\nd &lt;- as.dist(seabed)\n\nIf you check, you’ll see that the site names are being used to label rows and columns of the dissimilarity matrix as displayed. The lack of row names is not hurting us.\n\\(\\blacksquare\\)\n\nFit a cluster analysis using single-linkage, and display a dendrogram of the results.\n\nSolution\nThis:\n\nd.1 &lt;- hclust(d, method = \"single\")\nplot(d.1)\n\n\n\n\nThis is a base-graphics plot, it not having any of the nice ggplot things. But it does the job.\nSingle-linkage tends to produce “stringy” clusters, since the individual being added to a cluster only needs to be close to one thing in the cluster. Here, that manifests itself in sites getting added to clusters one at a time: for example, sites 25 and 26 get joined together into a cluster, and then in sequence sites 6, 16, 27, 30 and 22 get joined on to it (rather than any of those sites being formed into clusters first).\nYou might2 be wondering what else is in that hclust object, and what it’s good for. Let’s take a look:\n\nglimpse(d.1)\n\nList of 7\n $ merge      : int [1:29, 1:2] -3 -25 -6 -9 -28 -16 -27 -1 -30 -24 ...\n $ height     : num [1:29] 0.1 0.137 0.152 0.159 0.159 ...\n $ order      : int [1:30] 21 14 29 23 15 1 19 18 2 7 ...\n $ labels     : chr [1:30] \"s1\" \"s2\" \"s3\" \"s4\" ...\n $ method     : chr \"single\"\n $ call       : language hclust(d = d, method = \"single\")\n $ dist.method: NULL\n - attr(*, \"class\")= chr \"hclust\"\n\n\nYou might guess that labels contains the names of the sites, and you’d be correct. Of the other things, the most interesting are merge and height. Let’s display them side by side:\n\nwith(d.1, cbind(height, merge))\n\n         height        \n [1,] 0.1000000  -3 -20\n [2,] 0.1369863 -25 -26\n [3,] 0.1523179  -6   2\n [4,] 0.1588785  -9 -12\n [5,] 0.1588785 -28   4\n [6,] 0.1617647 -16   3\n [7,] 0.1633987 -27   6\n [8,] 0.1692308  -1 -19\n [9,] 0.1807229 -30   7\n[10,] 0.1818182 -24   5\n[11,] 0.1956522  -5  10\n[12,] 0.2075472 -15   8\n[13,] 0.2083333 -14 -29\n[14,] 0.2121212  -7  11\n[15,] 0.2142857 -11   1\n[16,] 0.2149533  -2  14\n[17,] 0.2191781 -18  16\n[18,] 0.2205882 -22   9\n[19,] 0.2285714  17  18\n[20,] 0.2307692  12  19\n[21,] 0.2328767 -10  15\n[22,] 0.2558140  20  21\n[23,] 0.2658228 -23  22\n[24,] 0.2666667  13  23\n[25,] 0.3023256  -4 -13\n[26,] 0.3333333  24  25\n[27,] 0.3571429 -21  26\n[28,] 0.4285714  -8 -17\n[29,] 0.6363636  27  28\n\n\nheight is the vertical scale of the dendrogram. The first height is 0.1, and if you look at the bottom of the dendrogram, the first sites to be joined together are sites 3 and 20 at height 0.1 (the horizontal bar joining sites 3 and 20 is what you are looking for). In the last two columns, which came from merge, you see what got joined together, with negative numbers meaning individuals (individual sites), and positive numbers meaning clusters formed earlier. So, if you look at the third line, at height 0.152, site 6 gets joined to the cluster formed on line 2, which (looking back) we see consists of sites 25 and 26. Go back now to the dendrogram; about \\({3\\over 4}\\) of the way across, you’ll see sites 25 and 26 joined together into a cluster, and a little higher up the page, site 6 joins that cluster.\nI said that single linkage produces stringy clusters, and the way that shows up in merge is that you often get an individual site (negative number) joined onto a previously-formed cluster (positive number). This is in contrast to Ward’s method, below.\n\\(\\blacksquare\\)\n\nNow fit a cluster analysis using Ward’s method, and display a dendrogram of the results.\n\nSolution\nSame thing, with small changes. The hard part is getting the name of the method right:\n\nd.2 &lt;- hclust(d, method = \"ward.D\")\nplot(d.2, cex = 0.7)\n\n\n\n\nThe site numbers were a bit close together, so I printed them out smaller than usual size (which is what the cex and a number less than 1 is doing: 70% of normal size).3 This time, there is a greater tendency for sites to be joined into small clusters first, then these small clusters are joined together. It’s not perfect, but there is a greater tendency for it to happen here.\nThis shows up in merge too:\n\nd.2$merge\n\n      [,1] [,2]\n [1,]   -3  -20\n [2,]  -25  -26\n [3,]   -9  -12\n [4,]  -28    3\n [5,]   -1  -19\n [6,]   -6    2\n [7,]  -14  -29\n [8,]   -5   -7\n [9,]  -18  -24\n[10,]  -27    6\n[11,]  -16  -22\n[12,]   -2    4\n[13,]  -30   10\n[14,]  -15    5\n[15,]  -23    8\n[16,]   -4  -13\n[17,]  -11    1\n[18,]    9   12\n[19,]  -10   17\n[20,]   -8  -17\n[21,]   11   13\n[22,]  -21   15\n[23,]    7   22\n[24,]   14   19\n[25,]   16   24\n[26,]   18   21\n[27,]   20   23\n[28,]   26   27\n[29,]   25   28\n\n\nThere are relatively few instances of a site being joined to a cluster of sites. Usually, individual sites get joined together (negative with a negative, mainly at the top of the list), or clusters get joined to clusters (positive with positive, mainly lower down the list).\n\\(\\blacksquare\\)\n\n* On the Ward’s method clustering, how many clusters would you choose to divide the sites into? Draw rectangles around those clusters.\n\nSolution\nYou may need to draw the plot again. In any case, a second line of code draws the rectangles. I think three clusters is good, but you can have a few more than that if you like:\n\nplot(d.2, cex = 0.7)\nrect.hclust(d.2, 3)\n\n\n\n\nWhat I want to see is a not-unreasonable choice of number of clusters (I think you could go up to about six), and then a depiction of that number of clusters on the plot. This is six clusters:\n\nplot(d.2, cex = 0.7)\nrect.hclust(d.2, 6)\n\n\n\n\nIn all your plots, the cex is optional, but you can compare the plots with it and without it and see which you prefer.\nLooking at this, even seven clusters might work, but I doubt you’d want to go beyond that. The choice of the number of clusters is mainly an aesthetic4 decision.\n\\(\\blacksquare\\)\n\n* The original data is in link. Read in the original data and verify that you again have 30 sites, variables called a through e and some others.\n\nSolution\nThus:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/seabed.csv\"\nseabed.z &lt;- read_csv(my_url)\n\nRows: 30 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): site, sediment\ndbl (8): a, b, c, d, e, depth, pollution, temp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nseabed.z\n\n\n\n  \n\n\n\n30 observations of 10 variables, including a through e. Check.\nI gave this a weird name so that it didn’t overwrite my original seabed, the one I turned into a distance object, though I don’t think I really needed to worry.\nThese data came from link,5 from which I also got the definition of the Bray-Curtis dissimilarity that I calculated for you. The data are in Exhibit 1.1 of that book.\n\\(\\blacksquare\\)\n\nGo back to your Ward method dendrogram with the red rectangles and find two sites in the same cluster. Display the original data for your two sites and see if you can explain why they are in the same cluster. It doesn’t matter which two sites you choose; the grader will merely check that your results look reasonable.\n\nSolution\nI want my two sites to be very similar, so I’m looking at two sites that were joined into a cluster very early on, sites s3 and s20. As I said, I don’t mind which ones you pick, but being in the same cluster will be easiest to justify if you pick sites that were joined together early. Then you need to display just those rows of the original data (that you just read in), which is a filter with an “or” in it:\n\nseabed.z %&gt;% filter(site == \"s3\" | site == \"s20\")\n\n\n\n  \n\n\n\nI think this odd-looking thing also works:\n\nseabed.z %&gt;% filter(site %in% c(\"s3\", \"s20\"))\n\n\n\n  \n\n\n\nI’ll also take displaying the lines one at a time, though it is easier to compare them if they are next to each other.\nWhy are they in the same cluster? To be similar (that is, have a low dissimilarity), the values of a through e should be close together. Here, they certainly are: a and e are both zero for both sites, and b, c and d are around 10 for both sites. So I’d call that similar.\nYou will probably pick a different pair of sites, and thus your detailed discussion will differ from mine, but the general point of it should be the same: pick a pair of sites in the same cluster (1 mark), display those two rows of the original data (1 mark), some sensible discussion of how the sites are similar (1 mark). As long as you pick two sites in the same one of your clusters, I don’t mind which ones you pick. The grader will check that your two sites were indeed in the same one of your clusters, then will check that you do indeed display those two sites from the original data.\nWhat happens if you pick sites from different clusters? Let’s pick two very dissimilar ones, sites 4 and 7 from opposite ends of my dendrogram:\n\nseabed.z %&gt;% filter(site == \"s4\" | site == \"s7\")\n\n\n\n  \n\n\n\nSite s4 has no a or b at all, and site s7 has quite a few; site s7 has no c at all, while site s4 has a lot. These are very different sites.\nExtra: now that you’ve seen what the original data look like, I should explain how I got the Bray-Curtis dissimilarities. As I said, only the counts of species a through e enter into the calculation; the other variables have nothing to do with it.\nLet’s simplify matters by pretending that we have only two species (we can call them A and B), and a vector like this:\n\nv1 &lt;- c(10, 3)\n\nwhich says that we have 10 organisms of species A and 3 of species B at a site. This is rather similar to this site:\n\nv2 &lt;- c(8, 4)\n\nbut very different from this site:\n\nv3 &lt;- c(0, 7)\n\nThe way you calculate the Bray-Curtis dissimilarity is to take the absolute difference of counts of organisms of each species:\n\nabs(v1 - v2)\n\n[1] 2 1\n\n\nand add those up:\n\nsum(abs(v1 - v2))\n\n[1] 3\n\n\nand then divide by the total of all the frequencies:\n\nsum(abs(v1 - v2)) / sum(v1 + v2)\n\n[1] 0.12\n\n\nThe smaller this number is, the more similar the sites are. So you might imagine that v1 and v3 would be more dissimilar:\n\nsum(abs(v1 - v3)) / sum(v1 + v3)\n\n[1] 0.7\n\n\nand so it is. The scaling of the Bray-Curtis dissimilarity is that the smallest it can be is 0, if the frequencies of each of the species are exactly the same at the two sites, and the largest it can be is 1, if one site has only species A and the other has only species B. (I’ll demonstrate that in a moment.) You might imagine that we’ll be doing this calculation a lot, and so we should define a function to automate it. Hadley Wickham (in “R for Data Science”) says that you should copy and paste some code (as I did above) no more than twice; if you need to do it again, you should write a function instead. The thinking behind this is if you copy and paste and change something (like a variable name), you’ll need to make the change everywhere, and it’s so easy to miss one. So, my function is (copying and pasting my code from above into the body of the function, which is Wickham-approved since it’s only my second time):\n\nbraycurtis &lt;- function(v1, v2) {\n  sum(abs(v1 - v2)) / sum(v1 + v2)\n}\n\nLet’s test it on my made-up sites, making up one more:\n\nbraycurtis(v1, v2)\n\n[1] 0.12\n\nbraycurtis(v1, v3)\n\n[1] 0.7\n\nbraycurtis(v2, v2)\n\n[1] 0\n\nv4 &lt;- c(4, 0)\nbraycurtis(v3, v4)\n\n[1] 1\n\n\nThese all check out. The first two are repeats of the ones we did before. The third one says that if you calculate Bray-Curtis for two sites with the exact same frequencies all the way along, you get the minimum value of 0; the fourth one says that when site v3 only has species B and site v4 only has species A, you get the maximum value of 1.\nBut note this:\n\nv2\n\n[1] 8 4\n\n2 * v2\n\n[1] 16  8\n\nbraycurtis(v2, 2 * v2)\n\n[1] 0.3333333\n\n\nYou might say that v2 and 2*v2 are the same distribution, and so they are, proportionately. But Bray-Curtis is assessing whether the frequencies are the same (as opposed to something like a chi-squared test that is assessing proportionality).6\nSo far so good. Now we have to do this for the actual data. The first issue7 is that the data is some of the row of the original data frame; specifically, it’s columns 2 through 6. For example, sites s3 and s20 of the original data frame look like this:\n\nseabed.z %&gt;% filter(site == \"s3\" | site == \"s20\")\n\n\n\n  \n\n\n\nand we don’t want to feed the whole of those into braycurtis, just the second through sixth elements of them. So let’s write another function that extracts the columns a through e of its inputs for given rows, and passes those on to the braycurtis that we wrote before. This is a little fiddly, but bear with me. The input to the function is the data frame, then the two sites that we want: First, though, what happens if filter site s3?\n\nseabed.z %&gt;% filter(site == \"s3\")\n\n\n\n  \n\n\n\nThis is a one-row data frame, not a vector as our function expects. Do we need to worry about it? First, grab the right columns, so that we will know what our function has to do:\n\nseabed.z %&gt;%\n  filter(site == \"s3\") %&gt;%\n  select(a:e)\n\n\n\n  \n\n\n\nThat leads us to this function, which is a bit repetitious, but for two repeats I can handle it. I haven’t done anything about the fact that x and y below are actually data frames:\n\nbraycurtis.spec &lt;- function(d, i, j) {\n  d %&gt;% filter(site == i) %&gt;% select(a:e) -&gt; x\n  d %&gt;% filter(site == j) %&gt;% select(a:e) -&gt; y\n  braycurtis(x, y)\n}\n\nThe first time I did this, I had the filter and the select in the opposite order, so I was neatly removing the column I wanted to filter by before I did the filter!\nThe first two lines pull out columns a through e of (respectively) sites i and j.\nIf I were going to create more than two things like x and y, I would have hived that off into a separate function as well, but I didn’t.\nSites 3 and 20 were the two sites I chose before as being similar ones (in the same cluster). So the dissimilarity should be small:\n\nbraycurtis.spec(seabed.z, \"s3\", \"s20\")\n\n[1] 0.1\n\n\nand so it is. Is it about right? The c differ by 5, the d differ by one, and the total frequency in both rows is about 60, so the dissimilarity should be about \\(6/60=0.1\\), as it is (exactly, in fact).\nThis, you will note, works. I think R has taken the attitude that it can treat these one-row data frames as if they were vectors. This is the cleaned-up version of my function. When I first wrote it, I printed out x and y, so that I could check that they were what I was expecting (they were).8 We have almost all the machinery we need. Now what we have to do is to compare every site with every other site and compute the dissimilarity between them. If you’re used to Python or another similar language, you’ll recognize this as two loops, one inside the other. This can be done in R (and I’ll show you how), but I’d rather show you the Tidyverse way first.\nThe starting point is to make a vector containing all the sites, which is easier than you would guess:\n\nsites &lt;- str_c(\"s\", 1:30)\nsites\n\n [1] \"s1\"  \"s2\"  \"s3\"  \"s4\"  \"s5\"  \"s6\"  \"s7\"  \"s8\"  \"s9\"  \"s10\" \"s11\" \"s12\"\n[13] \"s13\" \"s14\" \"s15\" \"s16\" \"s17\" \"s18\" \"s19\" \"s20\" \"s21\" \"s22\" \"s23\" \"s24\"\n[25] \"s25\" \"s26\" \"s27\" \"s28\" \"s29\" \"s30\"\n\n\nNext, we need to make all possible pairs of sites, which we also know how to do:\n\nsite_pairs &lt;- crossing(site1 = sites, site2 = sites)\nsite_pairs\n\n\n\n  \n\n\n\nNow, think about what to do in English first: “for each of the sites in site1, and for each of the sites in site2, taken in parallel, work out the Bray-Curtis distance.” This is, I hope, making you think of rowwise:\n\nsite_pairs %&gt;%\n  rowwise() %&gt;% \n  mutate(bray_curtis = braycurtis.spec(seabed.z, site1, site2)) -&gt; bc\nbc\n\n\n\n  \n\n\n\n(you might notice that this takes a noticeable time to run.)\nThis is a “long” data frame, but for the cluster analysis, we need a wide one with sites in rows and columns, so let’s create that:\n\n(bc %&gt;% pivot_wider(names_from=site2, values_from=bray_curtis) -&gt; bc2)\n\n\n\n  \n\n\n\nThat’s the data frame I shared with you.\nThe more Python-like way of doing it is a loop inside a loop. This works in R, but it has more housekeeping and a few possibly unfamiliar ideas. We are going to work with a matrix, and we access elements of a matrix with two numbers inside square brackets, a row number and a column number. We also have to initialize our matrix that we’re going to fill with Bray-Curtis distances; I’ll fill it with \\(-1\\) values, so that if any are left at the end, I’ll know I missed something.\n\nm &lt;- matrix(-1, 30, 30)\nfor (i in 1:30) {\n  for (j in 1:30) {\n    m[i, j] &lt;- braycurtis.spec(seabed.z, sites[i], sites[j])\n  }\n}\nrownames(m) &lt;- sites\ncolnames(m) &lt;- sites\nhead(m)\n\n          s1        s2        s3        s4        s5        s6        s7\ns1 0.0000000 0.4567901 0.2962963 0.4666667 0.4769231 0.5221239 0.4545455\ns2 0.4567901 0.0000000 0.4814815 0.5555556 0.3478261 0.2285714 0.4146341\ns3 0.2962963 0.4814815 0.0000000 0.4666667 0.5076923 0.5221239 0.4909091\ns4 0.4666667 0.5555556 0.4666667 0.0000000 0.7857143 0.6923077 0.8695652\ns5 0.4769231 0.3478261 0.5076923 0.7857143 0.0000000 0.4193548 0.2121212\ns6 0.5221239 0.2285714 0.5221239 0.6923077 0.4193548 0.0000000 0.5087719\n          s8        s9       s10       s11       s12       s13       s14\ns1 0.9333333 0.3333333 0.4029851 0.3571429 0.3750000 0.5769231 0.6326531\ns2 0.9298246 0.2222222 0.4468085 0.5662651 0.2149533 0.6708861 0.4210526\ns3 1.0000000 0.4074074 0.3432836 0.2142857 0.3250000 0.6538462 0.6734694\ns4 1.0000000 0.6388889 0.3793103 0.5319149 0.5492958 0.3023256 0.8500000\ns5 0.8536585 0.1956522 0.5641026 0.3731343 0.3186813 0.7142857 0.2666667\ns6 0.9325843 0.2428571 0.5714286 0.5304348 0.2374101 0.6756757 0.5925926\n         s15       s16       s17       s18       s19       s20       s21\ns1 0.2075472 0.8571429 1.0000000 0.5689655 0.1692308 0.3333333 0.7333333\ns2 0.3750000 0.4720000 0.8620690 0.3146853 0.3695652 0.4022989 0.6666667\ns3 0.3584906 0.7346939 1.0000000 0.5344828 0.3230769 0.1000000 0.8222222\ns4 0.4090909 0.9325843 1.0000000 0.6635514 0.4642857 0.3333333 0.8333333\ns5 0.4687500 0.5045872 0.8095238 0.5118110 0.3947368 0.5211268 0.3571429\ns6 0.5357143 0.2484076 0.9111111 0.2571429 0.3870968 0.4621849 0.6730769\n         s22       s23       s24       s25       s26       s27       s28\ns1 0.7346939 0.4411765 0.5714286 0.7037037 0.6956522 0.6363636 0.3250000\ns2 0.3760000 0.5368421 0.2432432 0.3925926 0.3277311 0.3809524 0.2149533\ns3 0.6326531 0.5294118 0.3809524 0.6666667 0.6086957 0.6363636 0.5000000\ns4 0.9325843 0.8644068 0.5200000 0.9393939 0.9277108 0.9333333 0.5774648\ns5 0.3761468 0.2658228 0.4105263 0.5294118 0.4174757 0.3818182 0.3186813\ns6 0.2993631 0.4488189 0.3006993 0.1856287 0.1523179 0.2151899 0.2949640\n         s29       s30\ns1 0.4339623 0.6071429\ns2 0.3500000 0.3669065\ns3 0.4339623 0.5892857\ns4 0.5454545 0.8446602\ns5 0.3125000 0.4796748\ns6 0.5357143 0.2163743\n\n\nBecause my loops work with site numbers and my function works with site names, I have to remember to refer to the site names when I call my function. I also have to supply row and column names (the site names).\nThat looks all right. Are all my Bray-Curtis distances between 0 and 1? I can smoosh my matrix into a vector and summarize it:\n\nsummary(as.vector(m))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.3571  0.5023  0.5235  0.6731  1.0000 \n\n\nAll the dissimilarities are correctly between 0 and 1. We can also check the one we did before:\n\nbc2 %&gt;% filter(site1 == \"s3\") %&gt;% select(s20)\n\n\n\n  \n\n\n\nor\n\nm[3, 20]\n\n[1] 0.1\n\n\nCheck.\n\\(\\blacksquare\\)\n\nObtain the cluster memberships for each site, for your preferred number of clusters from part (here). Add a column to the original data that you read in, in part (here), containing those cluster memberships, as a factor. Obtain a plot that will enable you to assess the relationship between those clusters and pollution. (Once you have the cluster memberships, you can add them to the data frame and make the graph using a pipe.) What do you see?\n\nSolution\nStart by getting the clusters with cutree. I’m going with 3 clusters, though you can use the number of clusters you chose before. (This is again making the grader’s life a misery, but her instructions from me are to check that you have done something reasonable, with the actual answer being less important.)\n\ncluster &lt;- cutree(d.2, 3)\ncluster\n\n s1  s2  s3  s4  s5  s6  s7  s8  s9 s10 s11 s12 s13 s14 s15 s16 s17 s18 s19 s20 \n  1   2   1   1   3   2   3   3   2   1   1   2   1   3   1   2   3   2   1   1 \ns21 s22 s23 s24 s25 s26 s27 s28 s29 s30 \n  3   2   3   2   2   2   2   2   3   2 \n\n\nNow, we add that to the original data, the data frame I called seabed.z, and make a plot. The best one is a boxplot:\n\nseabed.z %&gt;%\n  mutate(cluster = factor(cluster)) %&gt;%\n  ggplot(aes(x = cluster, y = pollution)) + geom_boxplot()\n\n\n\n\nThe clusters differ substantially in terms of the amount of pollution, with my cluster 1 being highest and my cluster 2 being lowest. (Cluster 3 has a low outlier.)\nAny sensible plot will do here. I think boxplots are the best, but you could also do something like vertically-faceted histograms:\n\nseabed.z %&gt;%\n  mutate(cluster = factor(cluster)) %&gt;%\n  ggplot(aes(x = pollution)) + geom_histogram(bins = 8) +\n  facet_grid(cluster ~ .)\n\n\n\n\nwhich to my mind doesn’t show the differences as dramatically. (The bins are determined from all the data together, so that each facet actually has fewer than 8 bins. You can see where the bins would be if they had any data in them.)\nHere’s how 5 clusters looks:\n\ncluster &lt;- cutree(d.2, 5)\ncluster\n\n s1  s2  s3  s4  s5  s6  s7  s8  s9 s10 s11 s12 s13 s14 s15 s16 s17 s18 s19 s20 \n  1   2   1   1   3   4   3   5   2   1   1   2   1   3   1   4   5   2   1   1 \ns21 s22 s23 s24 s25 s26 s27 s28 s29 s30 \n  3   4   3   2   4   4   4   2   3   4 \n\nseabed.z %&gt;%\n  mutate(cluster = factor(cluster)) %&gt;%\n  ggplot(aes(x = cluster, y = pollution)) + geom_boxplot()\n\n\n\n\nThis time, the picture isn’t quite so clear-cut, but clusters 1 and 5 are the highest in terms of pollution and cluster 4 is the lowest. I’m guessing that whatever number of clusters you choose, you’ll see some differences in terms of pollution.\nWhat is interesting is that pollution had nothing to do with the original formation of the clusters: that was based only on which species were found at each site. So, what we have shown here is that the amount of pollution has some association with what species are found at a site.\nA way to go on with this is to use the clusters as “known groups” and predict the cluster membership from depth, pollution and temp using a discriminant analysis. Then you could plot the sites, colour-coded by what cluster they were in, and even though you had three variables, you could plot it in two dimensions (or maybe even one dimension, depending how many LD’s were important).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "cluster.html#dissimilarities-between-fruits-1",
    "href": "cluster.html#dissimilarities-between-fruits-1",
    "title": "33  Hierarchical cluster analysis",
    "section": "33.6 Dissimilarities between fruits",
    "text": "33.6 Dissimilarities between fruits\nConsider the fruits apple, orange, banana, pear, strawberry, blueberry. We are going to work with these four properties of fruits:\n\nhas a round shape\nIs sweet\nIs crunchy\nIs a berry\n\n\nMake a table with fruits as columns, and with rows “round shape”, “sweet”, “crunchy”, “berry”. In each cell of the table, put a 1 if the fruit has the property named in the row, and a 0 if it does not. (This is your opinion, and may not agree with mine. That doesn’t matter, as long as you follow through with whatever your choices were.)\n\nSolution\nSomething akin to this:\n\nFruit        Apple Orange Banana Pear Strawberry Blueberry\nRound shape    1      1      0     0       0         1\nSweet          1      1      0     0       1         0\nCrunchy        1      0      0     1       0         0\nBerry          0      0      0     0       1         1\n\nYou’ll have to make a choice about “crunchy”. I usually eat pears before they’re fully ripe, so to me, they’re crunchy.\n\\(\\blacksquare\\)\n\nWe’ll define the dissimilarity between two fruits to be the number of qualities they disagree on. Thus, for example, the dissimilarity between Apple and Orange is 1 (an apple is crunchy and an orange is not, but they agree on everything else). Calculate the dissimilarity between each pair of fruits, and make a square table that summarizes the results. (To save yourself some work, note that the dissimilarity between a fruit and itself must be zero, and the dissimilarity between fruits A and B is the same as that between B and A.) Save your table of dissimilarities into a file for the next part.\n\nSolution\nI got this, by counting them:\n \nFruit         Apple  Orange   Banana   Pear  Strawberry  Blueberry\nApple           0       1       3       2        3          3\nOrange          1       0       2       3        2          2\nBanana          3       2       0       1        2          2\nPear            2       3       1       0        3          3\nStrawberry      3       2       2       3        0          2\nBlueberry       3       2       2       3        2          0\n\nI copied this into a file fruits.txt. Note that (i) I have aligned my columns, so that I will be able to use read_table later, and (ii) I have given the first column a name, since read_table wants the same number of column names as columns.\nExtra: yes, you can do this in R too. We’ve seen some of the tricks before.\nLet’s start by reading in my table of fruits and properties, which I saved in link:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/fruit1.txt\"\nfruit1 &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  Property = col_character(),\n  Apple = col_double(),\n  Orange = col_double(),\n  Banana = col_double(),\n  Pear = col_double(),\n  Strawberry = col_double(),\n  Blueberry = col_double()\n)\n\nfruit1\n\n\n\n  \n\n\n\nWe don’t need the first column, so we’ll get rid of it:\n\nfruit2 &lt;- fruit1 %&gt;% select(-Property)\nfruit2\n\n\n\n  \n\n\n\nThe loop way is the most direct. We’re going to be looking at combinations of fruits and other fruits, so we’ll need two loops one inside the other. It’s easier for this to work with column numbers, which here are 1 through 6, and we’ll make a matrix m with the dissimilarities in it, which we have to initialize first. I’ll initialize it to a \\(6\\times 6\\) matrix of -1, since the final dissimilarities are 0 or bigger, and this way I’ll know if I forgot anything.\nHere’s where we are at so far:\n\nfruit_m &lt;- matrix(-1, 6, 6)\nfor (i in 1:6) {\n  for (j in 1:6) {\n    fruit_m[i, j] &lt;- 3 # dissim between fruit i and fruit j\n  }\n}\n\nThis, of course, doesn’t run yet. The sticking point is how to calculate the dissimilarity between two columns. I think that is a separate thought process that should be in a function of its own. The inputs are the two column numbers, and a data frame to get those columns from:\n\ndissim &lt;- function(i, j, d) {\n  x &lt;- d %&gt;% select(i)\n  y &lt;- d %&gt;% select(j)\n  sum(x != y)\n}\ndissim(1, 2, fruit2)\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(i)\n\n  # Now:\n  data %&gt;% select(all_of(i))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(j)\n\n  # Now:\n  data %&gt;% select(all_of(j))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\n[1] 1\n\n\nApple and orange differ by one (not being crunchy). The process is: grab the \\(i\\)-th column and call it x, grab the \\(j\\)-th column and call it y. These are two one-column data frames with four rows each (the four properties). x!=y goes down the rows, and for each one gives a TRUE if they’re different and a FALSE if they’re the same. So x!=y is a collection of four T-or-F values. This seems backwards, but I was thinking of what we want to do: we want to count the number of different ones. Numerically, TRUE counts as 1 and FALSE as 0, so we should make the thing we’re counting (the different ones) come out as TRUE. To count the number of TRUEs (1s), add them up.\nThat was a complicated thought process, so it was probably wise to write a function to do it. Now, in our loop, we only have to call the function (having put some thought into getting it right):\n\nfruit_m &lt;- matrix(-1, 6, 6)\nfor (i in 1:6) {\n  for (j in 1:6) {\n    fruit_m[i, j] &lt;- dissim(i, j, fruit2)\n  }\n}\nfruit_m\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    0    1    3    2    3    3\n[2,]    1    0    2    3    2    2\n[3,]    3    2    0    1    2    2\n[4,]    2    3    1    0    3    3\n[5,]    3    2    2    3    0    2\n[6,]    3    2    2    3    2    0\n\n\nThe last step is re-associate the fruit names with this matrix. This is a matrix so it has a rownames and a colnames. We set both of those, but first we have to get the fruit names from fruit2:\n\nfruit_names &lt;- names(fruit2)\nrownames(fruit_m) &lt;- fruit_names\ncolnames(fruit_m) &lt;- fruit_names\nfruit_m\n\n           Apple Orange Banana Pear Strawberry Blueberry\nApple          0      1      3    2          3         3\nOrange         1      0      2    3          2         2\nBanana         3      2      0    1          2         2\nPear           2      3      1    0          3         3\nStrawberry     3      2      2    3          0         2\nBlueberry      3      2      2    3          2         0\n\n\nThis is good to go into the cluster analysis (happening later).\nThere is a tidyverse way to do this also. It’s actually a lot like the loop way in its conception, but the coding looks different. We start by making all combinations of the fruit names with each other, which is crossing:\n\ncombos &lt;- crossing(fruit = fruit_names, other = fruit_names)\ncombos\n\n\n\n  \n\n\n\nNow, we want a function that, given any two fruit names, works out the dissimilarity between them. A happy coincidence is that we can use the function we had before, unmodified! How? Take a look:\n\ndissim &lt;- function(i, j, d) {\n  x &lt;- d %&gt;% select(i)\n  y &lt;- d %&gt;% select(j)\n  sum(x != y)\n}\ndissim(\"Apple\", \"Orange\", fruit2)\n\n[1] 1\n\n\nselect can take a column number or a column name, so that running it with column names gives the right answer.\nNow, we want to run this function for each of the pairs in combos. This is rowwise, since our function takes only one fruit and one other fruit at a time, not all of them at once:\n\ncombos %&gt;% \n  rowwise() %&gt;% \n  mutate(dissim = dissim(fruit, other, fruit2))\n\n\n\n  \n\n\n\nThis would work just as well using fruit1, with the column of properties, rather than fruit2, since we are picking out the columns by name rather than number.\nTo make this into something we can turn into a dist object later, we need to pivot-wider the column other to make a square array:\n\ncombos %&gt;% \n  rowwise() %&gt;% \n  mutate(dissim = dissim(fruit, other, fruit2)) %&gt;% \n  pivot_wider(names_from = other, values_from = dissim) -&gt; fruit_spread\nfruit_spread\n\n\n\n  \n\n\n\nDone!\n\\(\\blacksquare\\)\n\nDo a hierarchical cluster analysis using complete linkage. Display your dendrogram.\n\nSolution\nFirst, we need to take one of our matrices of dissimilarities and turn it into a dist object. Since I asked you to save yours into a file, let’s start from there. Mine is aligned columns:\n\ndissims &lt;- read_table(\"fruits.txt\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  fruit = col_character(),\n  Apple = col_double(),\n  Orange = col_double(),\n  Banana = col_double(),\n  Pear = col_double(),\n  Strawberry = col_double(),\n  Blueberry = col_double()\n)\n\ndissims\n\n\n\n  \n\n\n\nThen turn it into a dist object. The first step is to take off the first column, since as.dist can get the names from the columns:\n\ndissims %&gt;%\n  select(-fruit) %&gt;%\n  as.dist() -&gt; d\nd\n\n           Apple Orange Banana Pear Strawberry\nOrange         1                              \nBanana         3      2                       \nPear           2      3      1                \nStrawberry     3      2      2    3           \nBlueberry      3      2      2    3          2\n\n\nIf you forget to take off the first column, this happens:\n\nas.dist(dissims)\n\nWarning in storage.mode(m) &lt;- \"numeric\": NAs introduced by coercion\n\n\nWarning in as.dist.default(dissims): non-square matrix\n\n\nError in dimnames(df) &lt;- if (is.null(labels)) list(seq_len(size), seq_len(size)) else list(labels, : length of 'dimnames' [1] not equal to array extent\n\n\nYou have one more column than you have rows, since you have a column of fruit names.\nAside: what is that stuff about dimnames?\n\ndimnames(dissims)\n\n[[1]]\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\n[[2]]\n[1] \"fruit\"      \"Apple\"      \"Orange\"     \"Banana\"     \"Pear\"      \n[6] \"Strawberry\" \"Blueberry\" \n\n\nDataframes have column names (the second element of that list), literally the names of the columns. But they can also have “row names”. This is more part of the old-fashioned data.frame thinking, because in the tidyverse, row names are ignored. If your dataframe doesn’t explicitly have row names (ours doesn’t), the values 1 through the number of rows are used instead. If you like to think of it this way, a dataframe has two dimensions (rows and columns), and so dimnames for a dataframe is a list of length two.\nNow, to that error message. A dist object is square (well, half of a square, as it displays), so if you use as.dist to make it from a dataframe, that dataframe had better be square as well. The way as.dist checks your dataframe for squareness is to see whether it has the same number of rows as columns, and the way it does that is to look at its dimnames and checks whether they have the same length. Here, there are six row names but seven9 column names. Hence the error message.\nThere is, I suppose, one more thing to say: internally, a dataframe is a list of columns:\n\ndissims %&gt;% as.list()\n\n$fruit\n[1] \"Apple\"      \"Orange\"     \"Banana\"     \"Pear\"       \"Strawberry\"\n[6] \"Blueberry\" \n\n$Apple\n[1] 0 1 3 2 3 3\n\n$Orange\n[1] 1 0 2 3 2 2\n\n$Banana\n[1] 3 2 0 1 2 2\n\n$Pear\n[1] 2 3 1 0 3 3\n\n$Strawberry\n[1] 3 2 2 3 0 2\n\n$Blueberry\n[1] 3 2 2 3 2 0\n\nattr(,\"spec\")\ncols(\n  fruit = col_character(),\n  Apple = col_double(),\n  Orange = col_double(),\n  Banana = col_double(),\n  Pear = col_double(),\n  Strawberry = col_double(),\n  Blueberry = col_double()\n)\n\n\nSince there are seven columns, this dataframe has seven “things” in it. This is the “array extent” that the error message talks about. But the first thing in dimnames, the row names, which the error message calls 'dimnames' [1], only has six things in it. End of aside.\nThis one is as.dist rather than dist since you already have dissimilarities and you want to arrange them into the right type of thing. dist is for calculating dissimilarities, which we did before, so we don’t want to do that now.\nNow, after all that work, the actual cluster analysis and dendrogram:\n\nfruits.1 &lt;- hclust(d, method = \"complete\")\nplot(fruits.1)\nrect.hclust(fruits.1, 3)\n\n\n\n\n\\(\\blacksquare\\)\n\nHow many clusters, of what fruits, do you seem to have? Explain briefly.\n\nSolution\nI reckon I have three clusters: strawberry and blueberry in one, apple and orange in the second, and banana and pear in the third. (If your dissimilarities were different from mine, your dendrogram will be different also.)\n\\(\\blacksquare\\)\n\nPick a pair of clusters (with at least 2 fruits in each) from your dendrogram. Verify that the complete-linkage distance on your dendrogram is correct.\n\nSolution\nI’ll pick strawberry-blueberry and and apple-orange. I’ll arrange the dissimilarities like this:\n\n           apple   orange\nstrawberry    3       2\nblueberry     3       2\n\nThe largest of those is 3, so that’s the complete-linkage distance. That’s also what the dendrogram says. (Likewise, the smallest of those is 2, so 2 is the single-linkage distance.) That is to say, the largest distance or dissimilarity from anything in one cluster to anything in the other is 3, and the smallest is 2. I don’t mind which pair of clusters you take, as long as you spell out the dissimilarity (distance) between each fruit in each cluster, and take the maximum of those. Besides, if your dissimilarities are different from mine, your complete-linkage distance could be different from mine also. The grader will have to use her judgement!10\nThe important point is that you assess the dissimilarities between fruits in one cluster and fruits in the other. The dissimilarities between fruits in the same cluster don’t enter into it.11 As it happens, all my complete-linkage distances between clusters (of at least 2 fruits) are 3. The single-linkage ones are different, though:\n\nfruits.2 &lt;- hclust(d, method = \"single\")\nplot(fruits.2)\n\n\n\n\nAll the single-linkage cluster distances are 2. (OK, so this wasn’t a very interesting example, but I wanted to give you one where you could calculate what was going on.)\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "cluster.html#similarity-of-species-1",
    "href": "cluster.html#similarity-of-species-1",
    "title": "33  Hierarchical cluster analysis",
    "section": "33.7 Similarity of species",
    "text": "33.7 Similarity of species\nTwo scientists assessed the dissimilarity between a number of species by recording the number of positions in the protein molecule cytochrome-\\(c\\) where the two species being compared have different amino acids. The dissimilarities that they recorded are in link.\n\nRead the data into a data frame and take a look at it.\n\nSolution\nNothing much new here:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/species.txt\"\nspecies &lt;- read_delim(my_url, \" \")\n\nRows: 8 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): what\ndbl (8): Man, Monkey, Horse, Pig, Pigeon, Tuna, Mould, Fungus\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nspecies\n\n\n\n  \n\n\n\nThis is a square array of dissimilarities between the eight species.\nThe data set came from the 1960s, hence the use of “Man” rather than “human”. It probably also came from the UK, judging by the spelling of Mould.\n(I gave the first column the name what so that you could safely use species for the whole data frame.)\n\\(\\blacksquare\\)\n\nBearing in mind that the values you read in are already dissimilarities, convert them into a dist object suitable for running a cluster analysis on, and display the results. (Note that you need to get rid of any columns that don’t contain numbers.)\n\nSolution\nThe point here is that the values you have are already dissimilarities, so no conversion of the numbers is required. Thus this is a job for as.dist, which merely changes how it looks. Use a pipeline to get rid of the first column first:\n\nspecies %&gt;%\n  select(-what) %&gt;%\n  as.dist() -&gt; d\nd\n\n       Man Monkey Horse Pig Pigeon Tuna Mould\nMonkey   1                                   \nHorse   17     16                            \nPig     13     12     5                      \nPigeon  16     15    16  13                  \nTuna    31     32    27  25     27           \nMould   63     62    64  64     59   72      \nFungus  66     65    68  67     66   69    61\n\n\nThis doesn’t display anything that it doesn’t need to: we know that the dissimilarity between a species and itself is zero (no need to show that), and that the dissimilarity between B and A is the same as between A and B, so no need to show everything twice. It might look as if you are missing a row and a column, but one of the species (Fungus) appears only in a row and one of them (Man) only in a column.\nThis also works, to select only the numerical columns:\n\nspecies %&gt;%\n  select(where(is.numeric)) %&gt;%\n  as.dist()\n\n       Man Monkey Horse Pig Pigeon Tuna Mould\nMonkey   1                                   \nHorse   17     16                            \nPig     13     12     5                      \nPigeon  16     15    16  13                  \nTuna    31     32    27  25     27           \nMould   63     62    64  64     59   72      \nFungus  66     65    68  67     66   69    61\n\n\nExtra: data frames officially have an attribute called “row names”, that is displayed where the row numbers display, but which isn’t actually a column of the data frame. In the past, when we used read.table with a dot, the first column of data read in from the file could be nameless (that is, you could have one more column of data than you had column names) and the first column would be treated as row names. People used row names for things like identifier variables. But row names have this sort of half-existence, and when Hadley Wickham designed the tidyverse, he decided not to use row names, taking the attitude that if it’s part of the data, it should be in the data frame as a genuine column. This means that when you use a read_ function, you have to have exactly as many column names as columns.\nFor these data, I previously had the column here called what as row names, and as.dist automatically got rid of the row names when formatting the distances. Now, it’s a genuine column, so I have to get rid of it before running as.dist. This is more work, but it’s also more honest, and doesn’t involve thinking about row names at all. So, on balance, I think it’s a win.\n\\(\\blacksquare\\)\n\nRun a cluster analysis using single-linkage and obtain a dendrogram.\n\nSolution\nSomething like this:\n\nspecies.1 &lt;- hclust(d, method = \"single\")\nplot(species.1)\n\n\n\n\n\\(\\blacksquare\\)\n\nRun a cluster analysis using Ward’s method and obtain a dendrogram.\n\nSolution\nNot much changes here in the code, but the result is noticeably different:\n\nspecies.2 &lt;- hclust(d, method = \"ward.D\")\nplot(species.2)\n\n\n\n\nDon’t forget to take care with the method: it has to be ward in lowercase (even though it’s someone’s name) followed by a D in uppercase.\n\\(\\blacksquare\\)\n\nDescribe how the two dendrograms from the last two parts look different.\n\nSolution\nThis is (as ever with this kind of thing) a judgement call. Your job is to come up with something reasonable. For myself, I was thinking about how single-linkage tends to produce “stringy” clusters that join single objects (species) onto already-formed clusters. Is that happening here? Apart from the first two clusters, man and monkey, horse and pig, everything that gets joined on is a single species joined on to a bigger cluster, including mould and fungus right at the end. Contrast that with the output from Ward’s method, where, for the most part, groups are formed first and then joined onto other groups. For example, in Ward’s method, mould and fungus are joined earlier, and also the man-monkey group is joined to the pigeon-horse-pig group.12 You might prefer to look at the specifics of what gets joined. I think the principal difference from this angle is that mould and fungus get joined together (much) earlier in Ward. Also, pigeon gets joined to horse and pig first under Ward, but after those have been joined to man and monkey under single-linkage. This is also a reasonable kind of observation.\n\\(\\blacksquare\\)\n\nLooking at your clustering for Ward’s method, what seems to be a sensible number of clusters? Draw boxes around those clusters.\n\nSolution\nPretty much any number of clusters bigger than 1 and smaller than 8 is ok here, but I would prefer to see something between 2 and 5, because a number of clusters of that sort offers (i) some insight (“these things are like these other things”) and (ii) a number of clusters of that sort is supported by the data. To draw those clusters, you need rect.hclust, and before that you’ll need to plot the cluster object again. For 2 clusters, that would look like this:\n\nplot(species.2)\nrect.hclust(species.2, 2)\n\n\n\n\nThis one is “mould and fungus vs. everything else”. (My red boxes seem to have gone off the side, sorry.)\nOr we could go to the other end of the scale:\n\nplot(species.2)\nrect.hclust(species.2, 5)\n\n\n\n\nFive is not really an insightful number of clusters with 8 species, but it seems to correspond (for me at least) with a reasonable division of these species into “kinds of living things”. That is, I am bringing some outside knowledge into my number-of-clusters division.\n\\(\\blacksquare\\)\n\nList which cluster each species is in, for your preferred number of clusters (from Ward’s method).\n\nSolution\nThis is cutree. For 2 clusters it would be this:\n\ncutree(species.2, 2)\n\n   Man Monkey  Horse    Pig Pigeon   Tuna  Mould Fungus \n     1      1      1      1      1      1      2      2 \n\n\nFor 5 it would be this:\n\ncutree(species.2, 5)\n\n   Man Monkey  Horse    Pig Pigeon   Tuna  Mould Fungus \n     1      1      2      2      2      3      4      5 \n\n\nand anything in between is in between.\nThese ones came out sorted, so there is no need to sort them (so you don’t need the methods of the next question).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "cluster.html#bridges-in-pittsburgh-1",
    "href": "cluster.html#bridges-in-pittsburgh-1",
    "title": "33  Hierarchical cluster analysis",
    "section": "33.8 Bridges in Pittsburgh",
    "text": "33.8 Bridges in Pittsburgh\nThe city of Pittsburgh, Pennsylvania, lies where three rivers, the Allegheny, Monongahela, and Ohio, meet.13 It has long been important to build bridges there, to enable its residents to cross the rivers safely. See link for a listing (with pictures) of the bridges. The data at link contains detail for a large number of past and present bridges in Pittsburgh. All the variables we will use are categorical. Here they are:\n\nid identifying the bridge (we ignore)\nriver: initial letter of river that the bridge crosses\nlocation: a numerical code indicating the location within Pittsburgh (we ignore)\nerected: time period in which the bridge was built (a name, from CRAFTS, earliest, to MODERN, most recent.\npurpose: what the bridge carries: foot traffic (“walk”), water (aqueduct), road or railroad.\nlength categorized as long, medium or short.\nlanes of traffic (or number of railroad tracks): a number, 1, 2, 4 or 6, that we will count as categorical.\nclear_g: whether a vertical navigation requirement was included in the bridge design (that is, ships of a certain height had to be able to get under the bridge). I think G means “yes”.\nt_d: method of construction. DECK means the bridge deck is on top of the construction, THROUGH means that when you cross the bridge, some of the bridge supports are next to you or above you.\nmaterial the bridge is made of: iron, steel or wood.\nspan: whether the bridge covers a short, medium or long distance.\nrel_l: Relative length of the main span of the bridge (between the two central piers) to the total crossing length. The categories are S, S-F and F. I don’t know what these mean.\ntype of bridge: wood, suspension, arch and three types of truss bridge: cantilever, continuous and simple.\n\nThe website link is an excellent source of information about bridges. (That’s where I learned the difference between THROUGH and DECK.) Wikipedia also has a good article at link. I also found link which is the best description I’ve seen of the variables.\n\nThe bridges are stored in CSV format. Some of the information is not known and was recorded in the spreadsheet as ?. Turn these into genuine missing values by adding na=\"?\" to your file-reading command. Display some of your data, enough to see that you have some missing data.\n\nSolution\nThis sort of thing:\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/bridges.csv\"\nbridges0 &lt;- read_csv(my_url, na = \"?\")\n\nRows: 108 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): id, river, erected, purpose, length, clear_g, t_d, material, span,...\ndbl  (2): location, lanes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbridges0\n\n\n\n  \n\n\n\nI have some missing values in the length column. (You sometimes see &lt;NA&gt; instead of NA, as you do here; this means the missing value is a missing piece of text rather than a missing number.)14\nThere are 108 bridges in the data set.\nI’m saving the name bridges for my final data set, after I’m finished organizing it.\n\\(\\blacksquare\\)\n\nVerify that there are missing values in this dataset. To see them, convert the text columns temporarily to factors using mutate, and pass the resulting data frame into summary.\n\nSolution\nI called my data frame bridges0, so this:\n\nbridges0 %&gt;%\n  mutate(across(where(is.character), \\(x) factor(x))) %&gt;%\n  summary()\n\n       id      river     location         erected       purpose      length  \n E1     :  1   A:49   Min.   : 1.00   CRAFTS  :18   AQUEDUCT: 4   LONG  :21  \n E10    :  1   M:41   1st Qu.:15.50   EMERGING:15   HIGHWAY :71   MEDIUM:48  \n E100   :  1   O:15   Median :27.00   MATURE  :54   RR      :32   SHORT :12  \n E101   :  1   Y: 3   Mean   :25.98   MODERN  :21   WALK    : 1   NA's  :27  \n E102   :  1          3rd Qu.:37.50                                          \n E103   :  1          Max.   :52.00                                          \n (Other):102          NA's   :1                                              \n     lanes      clear_g        t_d      material      span     rel_l   \n Min.   :1.00   G   :80   DECK   :15   IRON :11   LONG  :30   F   :58  \n 1st Qu.:2.00   N   :26   THROUGH:87   STEEL:79   MEDIUM:53   S   :30  \n Median :2.00   NA's: 2   NA's   : 6   WOOD :16   SHORT : 9   S-F :15  \n Mean   :2.63                          NA's : 2   NA's  :16   NA's: 5  \n 3rd Qu.:4.00                                                          \n Max.   :6.00                                                          \n NA's   :16                                                            \n       type   \n SIMPLE-T:44  \n WOOD    :16  \n ARCH    :13  \n CANTILEV:11  \n SUSPEN  :11  \n (Other) :11  \n NA's    : 2  \n\n\nThere are missing values all over the place. length has the most, but lanes and span also have a fair few.\nmutate requires across and a logical condition, something that is true or false, about each column, and then something to do with it. In words, “for each column that is text, replace it (temporarily) with the factor version of itself.”\nExtra: I think the reason summary doesn’t handle text stuff very well is that, originally, text columns that were read in from files got turned into factors, and if you didn’t want that to happen, you had to explicitly stop it yourself. Try mentioning stringsAsFactors=F to a veteran R user, and watch their reaction, or try it yourself by reading in a data file with text columns using read.table instead of read_delim. (This will read in an old-fashioned data frame, so pipe it through as_tibble to see what the columns are.)\nWhen Hadley Wickham designed readr, the corner of the tidyverse where the read_ functions live, he deliberately chose to keep text as text (on the basis of being honest about what kind of thing we have), with the result that we sometimes have to create factors when what we are using requires them rather than text.\n\\(\\blacksquare\\)\n\nUse drop_na to remove any rows of the data frame with missing values in them. How many rows do you have left?\n\nSolution\nThis is as simple as:\n\nbridges0 %&gt;% drop_na() -&gt; bridges\nbridges\n\n\n\n  \n\n\n\nI have 70 rows left (out of the original 108).\n\\(\\blacksquare\\)\n\nWe are going to assess the dissimilarity between two bridges by the number of the categorical variables they disagree on. This is called a “simple matching coefficient”, and is the same thing we did in the question about clustering fruits based on their properties. This time, though, we want to count matches in things that are rows of our data frame (properties of two different bridges), so we will need to use a strategy like the one I used in calculating the Bray-Curtis distances. First, write a function that takes as input two vectors v and w and counts the number of their entries that differ (comparing the first with the first, the second with the second, , the last with the last. I can think of a quick way and a slow way, but either way is good.) To test your function, create two vectors (using c) of the same length, and see whether it correctly counts the number of corresponding values that are different.\n\nSolution\nThe slow way is to loop through the elements of each vector, using square brackets to pull out the ones you want, checking them for differentness, then updating a counter which gets returned at the end. If you’ve done Python, this is exactly the strategy you’d use there:\n\ncount_diff &lt;- function(v, w) {\n  n &lt;- length(v)\n  stopifnot(length(v) == length(w)) # I explain this below\n  count &lt;- 0\n  for (i in 1:n) {\n    if (v[i] != w[i]) count &lt;- count + 1\n  }\n  count\n}\n\nThis function makes no sense if v and w are of different lengths, since we’re comparing corresponding elements of them. The stopifnot line checks to see whether v and w have the same number of things in them, and stops with an informative error if they are of different lengths. (The thing inside the stopifnot is what has to be true.)\nDoes it work?\n\nv &lt;- c(1, 1, 0, 0, 0)\nw &lt;- c(1, 2, 0, 2, 0)\ncount_diff(v, w)\n\n[1] 2\n\n\nThree of the values are the same and two are different, so this is right.\nWhat happens if my two vectors are of different lengths?\n\nv1 &lt;- c(1, 1, 0, 0)\nw &lt;- c(1, 2, 0, 2, 0)\ncount_diff(v1, w)\n\nError in count_diff(v1, w): length(v) == length(w) is not TRUE\n\n\nError, as produced by stopifnot. See how it’s perfectly clear what went wrong?\nR, though, is a “vectorized” language: it’s possible to work with whole vectors at once, rather than pulling things out of them one at a time. Check out this (which is like what I did with the fruits):\n\nv != w\n\n[1] FALSE  TRUE FALSE  TRUE FALSE\n\n\nThe second and fourth values are different, and the others are the same. But we can go one step further:\n\nsum(v != w)\n\n[1] 2\n\n\nThe true values count as 1 and the false ones as zero, so the sum is counting up how many values are different, exactly what we want. So the function can be as simple as:\n\ncount_diff &lt;- function(v, w) {\n  sum(v != w)\n}\n\nI still think it’s worth writing a function do this, though, since count_diff tells you what it does and sum(v!=w) doesn’t, unless you happen to know.\n\\(\\blacksquare\\)\n\nWrite a function that has as input two row numbers and a data frame to take those rows from. The function needs to select all the columns except for id and location, select the rows required one at a time, and turn them into vectors. (There may be some repetitiousness here. That’s OK.) Then those two vectors are passed into the function you wrote in the previous part, and the count of the number of differences is returned. This is like the code in the Bray-Curtis problem. Test your function on rows 3 and 4 of your bridges data set (with the missings removed). There should be six variables that are different.\n\nSolution\nThis is just like my function braycurtis.spec, except that instead of calling braycurtis at the end, I call count_diff:\n\nrow_diff &lt;- function(i, j, d) {\n  d1 &lt;- d %&gt;% select(-id, -location)\n  x &lt;- d1 %&gt;% slice(i) %&gt;% unlist()\n  y &lt;- d1 %&gt;% slice(j) %&gt;% unlist()\n  count_diff(x, y)\n}\nrow_diff(3, 4, bridges)\n\n[1] 6\n\n\nThat’s what I said.\nExtra: is that right, though? Let’s print out those rows and count:\n\nbridges %&gt;% slice(c(3, 4)) \n\n\n\n  \n\n\n\nOut of the ones we’re counting, I see differences in purpose, length, lanes, material, span and type. Six.\nI actually think the unlist is not needed:\n\nrow_diff2 &lt;- function(i, j, d) {\n  d1 &lt;- d %&gt;% select(-id, -location)\n  x &lt;- d1 %&gt;% slice(i)\n  y &lt;- d1 %&gt;% slice(j)\n  count_diff(x, y)\n}\nrow_diff2(3, 4, bridges)\n\n[1] 6\n\n\nHere, x and y are one-row data frames, but R (via count_diff) is sufficiently flexible to be able to cope with these rather than vectors (it checks “corresponding elements” of x and y for differentness). To my mind, though, having the unlist in is clearer, since it makes it unambiguous that x and y are vectors, and we know that count_diff works for vectors since that’s what we tested it with.\n\\(\\blacksquare\\)\n\nCreate a matrix or data frame of pairwise dissimilarities between each pair of bridges (using only the ones with no missing values). Use loops, or crossing and rowwise, as you prefer. Display the first six rows of your matrix (using head) or the first few rows of your data frame. (The whole thing is big, so don’t display it all.)\n\nSolution\nFirst thing, either way, is to find out how many bridges we have left:\n\nbridges\n\n\n\n  \n\n\n\n\nSo the loops (and the crossing) will go up to 70. Loops first:\n\n\nm &lt;- matrix(-1, 70, 70)\nfor (i in 1:70) {\n  for (j in 1:70) {\n    m[i, j] &lt;- row_diff(i, j, bridges)\n  }\n}\n\nEven just the top six rows (of all 70 columns) takes up a lot of space. The grader would only check that it looks about right:\n\nhead(m)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n[1,]    0    0    2    4    3    1    2    4    3     1     2     3     7     6\n[2,]    0    0    2    4    3    1    2    4    3     1     2     3     7     6\n[3,]    2    2    0    6    5    1    2    4    5     1     2     5     7     6\n[4,]    4    4    6    0    3    5    6    4    3     5     6     6     8     8\n[5,]    3    3    5    3    0    4    3    3    3     4     5     6     6     5\n[6,]    1    1    1    5    4    0    1    3    4     0     1     4     6     5\n     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n[1,]     7     9     6     7     3     8     7     9     8     6     7     6\n[2,]     7     9     6     7     3     8     7     9     8     6     7     6\n[3,]     7     9     6     7     5     8     8     9     7     6     6     6\n[4,]     8    10     8     8     3    10     9    10     9     8     9     8\n[5,]     6     9     7     6     6     7     6    10     7     7     8     7\n[6,]     6     9     5     6     4     8     7     9     7     5     6     5\n     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n[1,]     7     7     8     8     9     6     8     7     9     7     8     8\n[2,]     7     7     8     8     9     6     8     7     9     7     8     8\n[3,]     7     6     8     8     7     7     9     8     9     8     8     7\n[4,]     8     8     9     9     9     8     9     9    10     8     9     9\n[5,]     6     8     7     7    10     7     8     7     9     8     9     9\n[6,]     6     6     7     7     8     6     8     7     9     7     8     7\n     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n[1,]     8     7     8     8     7     9     8     9     9     8     7     8\n[2,]     8     7     8     8     7     9     8     9     9     8     7     8\n[3,]     6     6     6     6     8     8     9     8     8     6     6     9\n[4,]     9     8     8     8     9    10     9    10    11     8     9     9\n[5,]     9     8     8     8     7     8     7    10     8     8     6     8\n[6,]     7     6     7     7     7     8     8     8     8     7     6     8\n     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n[1,]     7     8     8     8     8     8     6     8     7     9    10     8\n[2,]     7     8     8     8     8     8     6     8     7     9    10     8\n[3,]     8     9     9     8     8     6     6     8     7     7    10     9\n[4,]     8     8     9     9     9     9     8    10     9    10    11     9\n[5,]     7     6     7     9     9     9     7     8     6     8     9     7\n[6,]     7     8     8     7     7     7     5     8     6     8    10     8\n     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70]\n[1,]     8     7     8     9    10     8     9     9\n[2,]     8     7     8     9    10     8     9     9\n[3,]     8     8     9     7    10     6     9     8\n[4,]     9     9     9    10    11     9    10    10\n[5,]     9     7     7    10     9     9     9     8\n[6,]     8     7     8     8    10     7     9     9\n\n\nA cursory glance at this shows that the bridges in the small-numbered rows of the data frame are similar to each other and different from the others. This suggests that these small-numbered bridges will end up in the same cluster (later).\nThe tidyverse way is really similar in conception. First use crossing to create all combinations of i and j:\n\nmm &lt;- crossing(i = 1:70, j = 1:70)\n\nand then use rowwise:\n\nmm %&gt;% \n  rowwise() %&gt;% \n  mutate(diff = row_diff(i, j, bridges)) -&gt; mm\nmm\n\n\n\n  \n\n\n\nIf you forget the rowwise, the answers (in diff) will all be the same, which should make you suspicious.\nThis is long format, though, so we need to pivot_wider the j column to get a square array of dissimilarities:\n\nmm %&gt;% pivot_wider(names_from=j, values_from=diff) -&gt; mm\nmm\n\n\n\n  \n\n\n\nThis shows what we found before, that bridges 3 and 4 differ on 6 variables.\n\\(\\blacksquare\\)\n\nTurn your matrix or data frame into a dist object. (If you couldn’t create a matrix or data frame of dissimilarities, read them in from link.) Do not display your distance object.\n\nSolution\nThe only tricky thing is that the data frame that I called mm has an extra column called i that needs to be removed first. This also applies if you need to read it in from the file. Thus:\n\nd1 &lt;- as.dist(m)\n\nor\n\nmm %&gt;% select(-i) %&gt;% as.dist() -&gt; d2\n\nor, if you got stuck,\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/mm.csv\"\nmmm &lt;- read_csv(my_url)\n\nRows: 70 Columns: 71\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (71): i, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmmm\n\n\n\n  \n\n\nmmm %&gt;% select(-i) %&gt;% as.dist() -&gt; d3\nd3\n\n    1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n2   0                                                                        \n3   2  2                                                                     \n4   4  4  6                                                                  \n5   3  3  5  3                                                               \n6   1  1  1  5  4                                                            \n7   2  2  2  6  3  1                                                         \n8   4  4  4  4  3  3  4                                                      \n9   3  3  5  3  3  4  5  4                                                   \n10  1  1  1  5  4  0  1  3  4                                                \n11  2  2  2  6  5  1  2  4  5  1                                             \n12  3  3  5  6  6  4  5  7  6  4  3                                          \n13  7  7  7  8  6  6  5  7  5  6  5  6                                       \n14  6  6  6  8  5  5  4  6  7  5  4  5  2                                    \n15  7  7  7  8  6  6  5  7  5  6  5  6  0  2                                 \n16  9  9  9 10  9  9  9  9  7  9  8  8  4  6  4                              \n17  6  6  6  8  7  5  6  5  6  5  5  6  4  4  4  5                           \n18  7  7  7  8  6  6  5  7  5  6  6  7  1  3  1  5  3                        \n19  3  3  5  3  6  4  5  7  5  4  4  4  8  8  8 10  7  7                     \n20  8  8  8 10  7  8  7  8  8  8  8  8  5  5  5  3  3  4  9                  \n21  7  7  8  9  6  7  6  7  8  7  7  7  5  3  5  5  3  4  8  2               \n22  9  9  9 10 10  9 10  9  7  9  9  9  6  8  6  3  4  5  9  3  5            \n23  8  8  7  9  7  7  6  8  6  7  7  8  2  4  2  4  4  1  8  3  5  4         \n24  6  6  6  8  7  5  6  5  6  5  5  6  4  4  4  5  0  3  7  3  3  4  4      \n25  7  7  6  9  8  6  7  6  7  6  6  7  5  5  5  4  1  4  8  2  4  3  3  1   \n26  6  6  6  8  7  5  6  5  7  5  5  6  5  4  5  6  1  4  7  4  3  5  5  1  2\n27  7  7  7  8  6  6  5  7  5  6  6  7  1  3  1  5  3  0  7  4  4  5  1  3  4\n28  7  7  6  8  8  6  7  7  5  6  6  7  3  5  3  4  3  2  7  4  6  3  1  3  2\n29  8  8  8  9  7  7  6  7  6  7  7  8  2  4  2  4  2  1  8  3  3  4  2  2  3\n30  8  8  8  9  7  7  6  6  6  7  7  8  2  4  2  5  3  1  8  4  4  5  2  3  4\n31  9  9  7  9 10  8  9  7  7  8  8  7  5  7  5  6  4  4  8  6  7  5  4  4  4\n32  6  6  7  8  7  6  7  6  6  6  6  6  5  5  5  4  1  4  7  2  2  3  5  1  2\n33  8  8  9  9  8  8  8  8  7  8  8  8  5  6  5  3  4  4  8  4  3  4  5  4  5\n34  7  7  8  9  7  7  7  7  8  7  7  7  6  5  6  4  3  5  8  3  2  5  6  3  4\n35  9  9  9 10  9  9  9  9  7  9  9  9  5  7  5  1  4  4  9  2  4  2  3  4  3\n36  7  7  8  8  8  7  8  7  7  7  7  5  6  6  6  5  2  5  7  3  3  4  6  2  3\n37  8  8  8  9  9  8  9  8  7  8  8 10  8  9  8  5  6  7  8  5  6  2  6  6  5\n38  8  8  7  9  9  7  8  7  6  7  7  8  4  6  4  3  2  3  8  3  5  2  2  2  1\n39  8  8  6  9  9  7  8  7  9  7  7  6  7  5  7  7  3  6  8  5  4  6  6  3  3\n40  7  7  6  8  8  6  7  7  8  6  6  5  6  4  6  7  4  5  7  5  5  6  4  4  3\n41  8  8  6  8  8  7  8  5  9  7  7  6  7  6  7  8  4  6  8  6  6  7  6  4  4\n42  8  8  6  8  8  7  8  5  9  7  7  6  7  6  7  8  4  6  8  6  6  7  6  4  4\n43  7  7  8  9  7  7  7  6  8  7  7  7  6  5  6  5  4  5  8  4  3  6  6  4  5\n44  9  9  8 10  8  8  7  7 10  8  8  9  8  7  8  9  7  7  9  6  7  7  6  7  6\n45  8  8  9  9  7  8  7  8  9  8  8  6  6  5  6  6  4  5  8  3  2  6  6  4  5\n46  9  9  8 10 10  8  9  8 10  8  8  7  8  6  8  7  4  7  9  5  5  4  6  4  3\n47  9  9  8 11  8  8  7  7 10  8  8  9  6  5  6  7  5  5 10  4  5  5  4  5  4\n48  8  8  6  8  8  7  8  5  9  7  7  6  7  6  7  8  4  6  8  6  6  7  6  4  4\n49  7  7  6  9  6  6  5  7  7  6  6  7  3  3  3  5  3  2  8  2  4  5  1  3  2\n50  8  8  9  9  8  8  8  8  9  8  8  6  7  5  7  5  4  6  8  4  2  6  7  4  5\n51  7  7  8  8  7  7  7  7  8  7  7  7  8  6  8  6  5  7  7  5  3  7  8  5  6\n52  8  8  9  8  6  8  7  7  9  8  8  6  6  5  6  6  4  5  8  3  2  6  6  4  5\n53  8  8  9  9  7  8  7  7  9  8  8  8  8  7  8  9  7  7  8  6  5  7  8  7  8\n54  8  8  8  9  9  7  8  7  9  7  7  6  7  6  7  8  3  6  8  6  5  5  7  3  4\n55  8  8  8  9  9  7  8  6  9  7  7  6  7  6  7  9  5  7  9  8  7  7  8  5  6\n56  8  8  6  9  9  7  8  6  6  7  7  8  4  6  4  5  4  4  9  6  7  5  4  4  4\n57  6  6  6  8  7  5  6  4  7  5  5  8  7  6  7  9  5  7  8  8  7  7  8  5  6\n58  8  8  8 10  8  8  8  8  9  8  8  8  7  6  7  3  5  7 10  3  4  5  6  5  4\n59  7  7  7  9  6  6  5  5  7  6  6  7  3  3  3  6  3  3  9  4  4  7  4  3  4\n60  9  9  7 10  8  8  7  8 10  8  8  7  6  5  6  7  5  6 10  5  5  8  6  5  5\n61 10 10 10 11  9 10  9 10 11 10 10  8  8  7  8  6  7  8 11  4  5  5  7  7  6\n62  8  8  9  9  7  8  7  8  9  8  8  6  6  5  6  6  5  6  9  4  3  7  7  5  6\n63  8  8  8  9  9  8  9  8  9  8  8  8 10  9 10  7  7 10  9  6  7  5  9  7  6\n64  7  7  8  9  7  7  7  7  8  7  7  7  6  5  6  4  4  6  9  4  3  6  7  4  5\n65  8  8  9  9  7  8  7  8  9  8  8  7  6  4  6  6  5  6  9  4  2  7  7  5  6\n66  9  9  7 10 10  8  9  7 10  8  8  7  8  7  8  9  6  8 10  8  8  7  8  6  6\n67 10 10 10 11  9 10  9 10 11 10 10  8  8  7  8  6  7  8 11  4  5  5  7  7  6\n68  8  8  6  9  9  7  8  7  9  7  7  7  7  5  7  7  4  7  9  6  5  7  7  4  4\n69  9  9  9 10  9  9  9  9 10  9  9  8  8  6  8  4  6  8 10  4  4  6  7  6  5\n70  9  9  8 10  8  9  8  9 10  9  9  8  7  5  7  6  6  7 10  4  3  7  7  6  6\n   26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n2                                                                            \n3                                                                            \n4                                                                            \n5                                                                            \n6                                                                            \n7                                                                            \n8                                                                            \n9                                                                            \n10                                                                           \n11                                                                           \n12                                                                           \n13                                                                           \n14                                                                           \n15                                                                           \n16                                                                           \n17                                                                           \n18                                                                           \n19                                                                           \n20                                                                           \n21                                                                           \n22                                                                           \n23                                                                           \n24                                                                           \n25                                                                           \n26                                                                           \n27  4                                                                        \n28  4  2                                                                     \n29  3  1  3                                                                  \n30  4  1  3  1                                                               \n31  5  4  3  4  3                                                            \n32  2  4  4  3  4  5                                                         \n33  3  4  5  3  4  6  3                                                      \n34  2  5  6  4  5  7  2  1                                                   \n35  5  4  3  3  4  5  3  2  3                                                \n36  3  5  5  4  5  4  1  4  3  4                                             \n37  6  7  5  6  7  7  5  5  6  4  6                                          \n38  3  3  1  2  3  3  3  4  5  2  4  4                                       \n39  3  6  5  5  6  3  4  6  5  6  3  7  4                                    \n40  4  5  3  6  6  4  5  7  6  6  4  7  4  2                                 \n41  4  6  5  6  5  2  5  7  6  7  4  8  5  2  3                              \n42  4  6  5  6  5  2  5  7  6  7  4  8  5  2  3  0                           \n43  3  5  6  5  4  6  3  2  1  4  4  7  6  6  6  5  5                        \n44  6  7  7  7  6  6  8  8  7  8  7  6  7  6  5  5  5  6                     \n45  3  5  7  4  5  6  3  3  2  5  2  7  6  4  5  5  5  3  5                  \n46  4  7  5  6  7  5  5  7  6  6  4  5  4  2  2  4  4  7  4  5               \n47  4  5  5  5  4  6  6  6  5  6  7  6  5  6  5  5  5  4  2  5  4            \n48  4  6  5  6  5  2  5  7  6  7  4  8  5  2  3  0  0  5  5  5  4  5         \n49  4  2  2  3  3  5  4  6  5  4  5  7  3  5  3  5  5  5  5  5  5  3  5      \n50  4  6  7  5  6  6  3  3  2  4  2  7  6  3  4  5  5  3  7  2  4  7  5  6   \n51  5  7  8  6  7  7  4  4  3  5  3  6  7  4  5  6  6  4  6  3  5  8  6  7  1\n52  4  5  7  4  5  6  3  4  3  5  2  7  6  4  5  4  4  4  6  1  5  6  4  5  2\n53  7  7  9  7  6  7  6  7  6  8  5  5  9  7  7  6  6  5  3  4  6  5  6  7  5\n54  2  6  6  5  6  5  4  5  4  7  3  6  5  3  4  4  4  5  4  3  2  4  4  6  4\n55  5  7  7  7  6  5  6  8  7  9  5  7  7  5  5  4  4  6  5  6  4  5  4  7  6\n56  5  4  3  4  3  2  5  6  7  5  6  7  3  5  6  4  4  6  8  8  7  6  4  5  8\n57  5  7  7  7  6  7  6  8  7  9  7  5  7  7  7  6  6  6  5  8  6  5  6  7  8\n58  4  7  6  6  7  8  4  3  2  3  5  6  5  6  6  7  7  3  7  4  6  5  7  5  4\n59  4  3  5  3  2  5  4  6  5  6  5  9  5  6  6  5  5  4  6  5  7  4  5  3  6\n60  5  6  7  5  6  5  6  7  6  7  5  8  6  3  5  4  4  7  6  4  5  6  4  5  5\n61  7  8  8  7  8  8  6  7  6  6  5  5  7  6  6  7  7  7  5  4  4  5  7  6  5\n62  4  6  8  5  6  7  4  4  3  6  3  8  7  5  6  6  6  4  6  1  6  6  6  6  3\n63  7 10  8  9 10  8  6  8  7  7  5  3  7  6  6  7  7  8  5  6  4  7  7  8  6\n64  4  6  7  5  6  8  3  3  2  4  4  6  6  6  7  7  7  3  9  4  7  7  7  6  3\n65  5  6  8  5  6  8  4  5  4  6  4  8  7  5  6  7  7  5  8  3  6  7  7  6  3\n66  6  8  7  8  7  4  7  9  8  9  6  7  7  4  5  3  3  7  5  7  4  5  3  7  7\n67  7  8  8  7  8  8  6  7  6  6  5  5  7  6  6  7  7  7  5  4  4  5  7  6  5\n68  4  7  6  6  7  5  5  7  6  7  5  8  5  2  4  4  4  7  8  6  4  7  4  6  5\n69  6  8  7  7  8  8  5  5  4  4  5  7  6  5  5  7  7  5  8  5  5  7  7  6  3\n70  6  7  8  6  7  7  5  6  5  6  5  8  7  4  6  6  6  6  8  4  6  7  6  6  4\n   51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69\n2                                                          \n3                                                          \n4                                                          \n5                                                          \n6                                                          \n7                                                          \n8                                                          \n9                                                          \n10                                                         \n11                                                         \n12                                                         \n13                                                         \n14                                                         \n15                                                         \n16                                                         \n17                                                         \n18                                                         \n19                                                         \n20                                                         \n21                                                         \n22                                                         \n23                                                         \n24                                                         \n25                                                         \n26                                                         \n27                                                         \n28                                                         \n29                                                         \n30                                                         \n31                                                         \n32                                                         \n33                                                         \n34                                                         \n35                                                         \n36                                                         \n37                                                         \n38                                                         \n39                                                         \n40                                                         \n41                                                         \n42                                                         \n43                                                         \n44                                                         \n45                                                         \n46                                                         \n47                                                         \n48                                                         \n49                                                         \n50                                                         \n51                                                         \n52  3                                                      \n53  4  4                                                   \n54  5  4  5                                                \n55  7  6  4  3                                             \n56  9  8  9  7  5                                          \n57  7  8  4  5  2  5                                       \n58  5  5  8  6  7  6  7                                    \n59  7  5  6  6  4  3  4  5                                 \n60  6  4  6  5  4  5  6  5  4                              \n61  6  4  4  5  4  8  6  4  6  3                           \n62  4  2  5  4  5  7  7  3  4  3  3                        \n63  5  6  4  5  4  8  4  5  8  5  2  5                     \n64  4  4  6  6  5  6  5  2  4  4  4  3  5                  \n65  4  3  6  6  6  7  7  4  4  4  4  2  6  3               \n66  8  7  5  4  1  4  3  7  5  3  4  6  4  6  7            \n67  6  4  4  5  4  8  6  4  6  3  0  3  2  4  4  4         \n68  6  6  9  5  5  4  6  5  5  3  6  5  6  5  3  4  6      \n69  4  5  8  7  7  7  8  2  6  5  4  4  5  3  2  7  4  3   \n70  5  4  7  7  7  6  8  4  5  3  4  3  6  4  1  6  4  2  2\n\n\n\\(\\blacksquare\\)\n\nRun a cluster analysis using Ward’s method, and display a dendrogram. The labels for the bridges (rows of the data frame) may come out too big; experiment with a cex less than 1 on the plot so that you can see them.\n\nSolution\nHome stretch now. I found that cex=0.3 was good for me, though I had to enlarge the graph to see the bridge numbers:\n\nbridges.1 &lt;- hclust(d1, method = \"ward.D\")\nplot(bridges.1, cex = 0.3)\n\n\n\n\ncex stands for “character expansion”. This is one of the myriad of things you could adjust on the old base graphics (of which this is an example). If you wanted to make text bigger, you’d set cex to a value bigger than 1.\n\\(\\blacksquare\\)\n\nHow many clusters do you think is reasonable for these data? Draw them on your plot.\n\nSolution\nI think you could go with any number from about 3 to something like 15, but my choice is 5. What you want is for the bridges within a cluster to be similar and bridges in different clusters to be different, however you judge that.\n\nplot(bridges.1, cex = 0.3)\nrect.hclust(bridges.1, 5)\n\n\n\n\nWhatever number of clusters you go with, draw a corresponding number of red rectangles.\nNote that the low-numbered bridges are all in my first cluster, as I suspected they would be.\n\\(\\blacksquare\\)\n\nPick three bridges in the same one of your clusters (it doesn’t matter which three bridges or which cluster). Display the data for these bridges. Does it make sense that these three bridges ended up in the same cluster? Explain briefly.\n\nSolution\nWhat I want you to do is to display the data for your chosen three bridges and make the case that they are “similar”. I’m picking 41, 42 and 48 from my third cluster. On yours, scroll right to see the other variables.\n\nbridges %&gt;% slice(c(41, 42, 48))\n\n\n\n  \n\n\n\nThese bridges are identical on everything except location (which we weren’t considering anyway). So it makes perfect sense that they would be in the same cluster.\nYou might not have been so lucky, for example, 10, 12 and 19, which got joined together further up:\n\nbridges %&gt;% slice(c(10, 12, 19))\n\n\n\n  \n\n\n\nThese differ on 3 or 4 variables and are the same on all the others, so you can certainly say that they are more alike than different.\nExtra: to get a feel for how different they might be, let’s compare three bridges in different clusters:\n\nbridges %&gt;% slice(c(8, 24, 52))\n\n\n\n  \n\n\n\nThese are not as different as I was expecting, but they are indeed different on more of the variables.\nIt would be interesting to plot these bridges on a map of Pittsburgh, colour-coded by which cluster they are in. This might give us some insight about how bridges are alike or different.\nI also remark that the discriminant analysis idea, using the clusters as known groups, would not work here because we don’t have any quantitative variables to use for the discriminant analysis.\nThe most interesting way I can think of is to cross-classify the bridges by cluster and values of the other variables. These would be complicated multi-way tables, though, which makes me wonder whether a “classification tree” like rpart would be worth thinking about.\nI am curious enough to have a crack at this. First we need a data set with the clusters in it. I’m going with my 5 clusters:\n\nbridges.rpart &lt;- bridges %&gt;% mutate(cluster = cutree(bridges.1, 5))\n\nand then\n\nlibrary(rpart)\nbridges.tree &lt;- rpart(factor(cluster) ~ river + erected + purpose + length + lanes + clear_g +\n  t_d + material + span + rel_l + type, data = bridges.rpart, method = \"class\")\nprint(bridges.tree)\n\nn= 70 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 70 47 3 (0.19 0.16 0.33 0.2 0.13)  \n   2) span=MEDIUM,SHORT 45 31 4 (0.29 0.24 0.044 0.31 0.11)  \n     4) material=IRON,WOOD 13  0 1 (1 0 0 0 0) *\n     5) material=STEEL 32 18 4 (0 0.34 0.062 0.44 0.16)  \n      10) river=M 13  3 2 (0 0.77 0.077 0 0.15) *\n      11) river=A 19  5 4 (0 0.053 0.053 0.74 0.16) *\n   3) span=LONG 25  4 3 (0 0 0.84 0 0.16)  \n     6) type=ARCH,CANTILEV,SIMPLE-T 18  0 3 (0 0 1 0 0) *\n     7) type=CONT-T,SUSPEN 7  3 5 (0 0 0.43 0 0.57) *\n\n\nThis takes some making sense of, so let’s grab a couple of bridges to predict the cluster of:\n\nbridges.rpart %&gt;% slice(c(20, 29)) \n\n\n\n  \n\n\n\nLet’s start with bridge 20 (with ID E42). The line in the printed output marked 1 and “root” says that there are 70 bridges altogether, and the best guess (if you know nothing) is to guess cluster 3, which would guess 47 of the bridges wrong. The five numbers in the brackets are the proportions of bridges in each cluster at this point. But we do know more about bridge E42. We go to number 2, which says to look at the span. For this bridge it is LONG, so we go down to number 3. Under 3 are 6 and 7, which say to look at type. Bridge E42 is of type SIMPLE-T, so we go to 6. There is nothing under this (lower down in the tree), and the line ends with a *, so we are ready to guess the cluster. There are 18 bridges with this span and one of these types, and they are all in cluster 3, no errors. Cluster 3 contains long-span bridges of one of those types.\nBridge 29, with ID E51, now. The first thing to look at is span again; this one is medium, so we are at 2. Under 2 is 4 and 5; we are invited to look at material, which is STEEL, number 5. We have another thing to look at, 10 and 11, which is river; in this case, river is M, number 10. We are now at the end, guessing cluster 2 (which is also correct); there are 13 bridges of this span, material and river and only 3 of them were in some cluster other than 2.\nBy looking at the tree output, we can describe what makes a bridge be predicted to land up in each cluster:\n\nSpan is medium or short, material is iron or wood. (This suggests old bridges.)\nSpan is medium or short, material is steel, river is M.\nSpan is long, type is arch, cantilever or simple-truss\nSpan is medium or short, material is steel, river is A.\nSpan is long, type is continuous-truss or suspension.\n\nThis story is telling us that the clusters are determined by only a few of our variables. span seems to be the most important, then either type (if the span is long) or material and maybe river (otherwise).\nThis is an example of a “classification tree” which is a nice easy-to-follow version of logistic regression.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "cluster.html#footnotes",
    "href": "cluster.html#footnotes",
    "title": "33  Hierarchical cluster analysis",
    "section": "",
    "text": "For a long time, the Pittsburgh Steelers football team played at the Three Rivers Stadium.↩︎\nConceivably.↩︎\nThis is base-graphics code, which I learned a long time ago. There are a lot of options with weird names that are hard to remember, and that are sometimes inconsistent with each other. There is a package ggdendro that makes nice ggplot dendrograms, and another called dendextend that does all kinds of stuff with dendrograms. I decided that it wasn’t worth the trouble of teaching you (and therefore me) ggdendro, since the dendrograms look much the same.↩︎\nThis, I think, is the British spelling, with the North American one being esthetic. My spelling is where the aes in a ggplot comes from.↩︎\nIf you are a soccer fan, you might recognize BBVA as a former sponsor of the top Spanish soccer league, La Liga BBVA (as it was). BBVA is a Spanish bank that also has a Foundation that published this book.↩︎\nYou could make a table out of the sites and species, and use the test statistic from a chi-squared test as a measure of dissimilarity: the smallest it can be is zero, if the species counts are exactly proportional at the two sites. It doesn’t have an upper limit.↩︎\nThere are more issues.↩︎\nI am a paid-up member of the print all the things school of debugging. You probably know how to do this better.↩︎\nCount them.↩︎\nThat’s two cups of coffee I owe the grader now.↩︎\nI now have a mental image of John Cleese saying “it don’t enter into it” in the infamous Dead Parrot sketch. Not to mention How to defend yourself against an assailant armed with fresh fruit↩︎\nTuna is an exception, but usually Ward tends to join fairly dissimilar things that are nonetheless more similar to each other than to anything else. This is like Hungarian and Finnish in the example in lecture: they are very dissimilar languages, but they are more similar to each other than to anything else.↩︎\nFor a long time, the Pittsburgh Steelers football team played at the Three Rivers Stadium.↩︎\nSometimes it’s necessary to distinguish between the different types of missing value; if that’s the case, you can use eg. NA_real_ and NA_character_- to distinguish missing decimal numbers from missing text.↩︎"
  },
  {
    "objectID": "kmeans-cluster.html#clustering-the-australian-athletes",
    "href": "kmeans-cluster.html#clustering-the-australian-athletes",
    "title": "34  K-means cluster analysis",
    "section": "34.1 Clustering the Australian athletes",
    "text": "34.1 Clustering the Australian athletes\nRecall the Australian athlete data (that we’ve seen so many times before). This time, we’ll do some K-means clustering, and then see whether athletes of certain genders and certain sports tend to end up in the same cluster.\n\nRead in the data from link, recalling that the data values are separated by tabs. Display (some of) the data set.\nFrom your data frame, select only the columns that are numbers (or get rid of the ones that are text), and standardize all of the columns you have left. This is, done the best way, a slick piece of code. Display what you get.\nMake a data frame that contains the total within-cluster sum of squares from a K-means clustering for each number of clusters from 2 to 20.\nUse the data frame you just created to make a scree plot. What does the scree plot tell you?\nUsing a sensible number of clusters as deduced from your scree plot, run a K-means cluster analysis. Don’t forget the nstart!\nMake a data frame consisting of the athletes’ sport and gender, and which of your clusters they belong to, taking the appropriate things from the appropriate one of your data frames.\nUsing the data frame you created in the previous part, display all the athletes in some of your clusters. Do the athletes within a cluster appear to have anything in common? (If a cluster has more than 10 athletes in it, make sure to look at them all.)\nAdd the cluster membership to the data frame you read in from the file, and do a discriminant analysis treating the clusters as known groups. You can display the output.\nHow many linear discriminants do you have? How many do you think are important?\nWhich variables seem to be important in distinguishing the clusters? Look only at the linear discriminants that you judged to be important.\nDraw a biplot (which shows the first two LDs), drawing the clusters in different colours. Comment briefly on anything especially consistent or inconsistent with what you’ve seen so far."
  },
  {
    "objectID": "kmeans-cluster.html#running-jumping-and-throwing",
    "href": "kmeans-cluster.html#running-jumping-and-throwing",
    "title": "34  K-means cluster analysis",
    "section": "34.2 Running, jumping, and throwing",
    "text": "34.2 Running, jumping, and throwing\nThe decathlon is a men’s1 track-and-field competition in which competitors complete 10 events over two days as follows, requiring the skills shown:\n\n\n\n\n\nEvent\nSkills\n\n\n\n\n100m\nRunning, speed\n\n\nLong jump\nJumping, speed\n\n\nShot put\nThrowing, strength\n\n\nHigh jump\nJumping, agility\n\n\n400m\nRunning, speed\n\n\n110m hurdles\nRunning, jumping, speed\n\n\nDiscus\nThrowing, agility (and maybe strength)\n\n\nPole vault\nJumping, agility\n\n\nJavelin\nThrowing, agility\n\n\n1500m\nRunning, endurance\n\n\n\n\n\n(note: in the pdf version, this table might appear twice.)\nThese are a mixture of running, jumping and throwing disciplines. The performance (time, distance or height) achieved in each event is converted to a number of points using standard tables. and the winner of the entire decathlon is the competitor with the largest total of points. The basic idea is that a “good” performance in an event is worth 1000 points, and the score decreases if the athlete takes more seconds (running) or achieves fewer metres (jumping/throwing). A good decathlete has to be at least reasonably good at all the disciplines.\nFor the decathlon competition at the 2013 Track and Field World Championship, a record was kept of each competitor’s performance in each event (for the competitors that competed in all ten events). These values are in link.\n\nRead in the data and verify that you have the right number of variables.\nSome of the performances are times in seconds, and some of them are distances (or heights) in metres. Also, some of the columns are more variable than others. Produce a matrix of standardized performances in each event, making sure not to try to standardize the names!\nWe are going to make a scree plot to decide on the number of clusters our K-means clustering should use. Using a loop, or otherwise,2 obtain the total within-cluster sum of squares for these data for each number of clusters for 2 up to 20.\nUsing what you calculated in the previous part, draw a scree plot. How does your scree plot tell you that 5 is a possible number of clusters? Explain briefly.\nRun K-means with 5 clusters. Produce an output that shows which competitors are in which cluster.\nDisplay the cluster means for all of the events. (This has already been calculated; you just have to display it.) Find the cluster mean, looking at all of the events, that is farthest from zero, and see if you can describe the strengths and weaknesses of the athletes in that cluster (look at all the events for the cluster that has that extreme mean). Bear in mind (i) that these are the original performances standardized, and (ii) for a running event, a smaller value is better."
  },
  {
    "objectID": "kmeans-cluster.html#clustering-the-swiss-bills",
    "href": "kmeans-cluster.html#clustering-the-swiss-bills",
    "title": "34  K-means cluster analysis",
    "section": "34.3 Clustering the Swiss bills",
    "text": "34.3 Clustering the Swiss bills\nThis question is about the Swiss bank counterfeit bills again. This time we’re going to ignore whether each bill is counterfeit or not, and see what groups they break into. Then, at the end, we’ll see whether cluster analysis was able to pick out the counterfeit ones or not.\n\nRead the data in again (just like last time), and look at the first few rows. This is just the same as before.\nThe variables in this data frame are on different scales. Standardize them so that they all have mean 0 and standard deviation 1. (Don’t try to standardize the status column!)\nWe are going to make a scree plot. First, calculate the total within-cluster SS for each number of clusters from 2 to 10.\n* Make a scree plot (creating a data frame first if you need). How many clusters do you think you should use?\nRun K-means with the number of clusters that you found in (here). How many bills are in each cluster?\nMake a table showing cluster membership against actual status (counterfeit or genuine). Are the counterfeit bills mostly in certain clusters?"
  },
  {
    "objectID": "kmeans-cluster.html#grouping-similar-cars",
    "href": "kmeans-cluster.html#grouping-similar-cars",
    "title": "34  K-means cluster analysis",
    "section": "34.4 Grouping similar cars",
    "text": "34.4 Grouping similar cars\nThe file link contains information on seven variables for 32 different cars. The variables are:\n\nCarname: name of the car (duh!)\nmpg: gas consumption in miles per US gallon (higher means the car uses less gas)\ndisp: engine displacement (total volume of cylinders in engine): higher is more powerful\nhp: engine horsepower (higher means a more powerful engine)\ndrat: rear axle ratio (higher means more powerful but worse gas mileage)\nwt: car weight in US tons\nqsec: time needed for the car to cover a quarter mile (lower means faster)\n\n\nRead in the data and display its structure. Do you have the right number of cars and variables?\nThe variables are all measured on different scales. Use scale to produce a matrix of standardized (\\(z\\)-score) values for the columns of your data that are numbers.\nRun a K-means cluster analysis for these data, obtaining 3 clusters, and display the results. Take whatever action you need to obtain the best (random) result from a number of runs.\nDisplay the car names together with which cluster they are in. If you display them all at once, sort by cluster so that it’s easier to see which clusters contain which cars. (You may have to make a data frame first.)\nI have no idea whether 3 is a sensible number of clusters. To find out, we will draw a scree plot (in a moment). Write a function that accepts the number of clusters and the (scaled) data, and returns the total within-cluster sum of squares.\nCalculate the total within-group sum of squares for each number of clusters from 2 to 10, using the function you just wrote.\nMake a scree plot, using the total within-cluster sums of squares values that you calculated in the previous part.\nWhat is a suitable number of clusters for K-means, based on your scree plot?\nRun a K-means analysis using the number of clusters suggested by your scree plot, and list the car names together with the clusters they belong to, sorted by cluster."
  },
  {
    "objectID": "kmeans-cluster.html#rating-beer",
    "href": "kmeans-cluster.html#rating-beer",
    "title": "34  K-means cluster analysis",
    "section": "34.5 Rating beer",
    "text": "34.5 Rating beer\nThirty-two students each rated 10 brands of beer:\n\nAnchor Steam\nBass\nBeck’s\nCorona\nGordon Biersch\nGuinness\nHeineken\nPete’s Wicked Ale\nSam Adams\nSierra Nevada\n\nThe ratings are on a scale of 1 to 9, with a higher rating being better. The data are in link. I abbreviated the beer names for the data file. I hope you can figure out which is which.\n\nRead in the data, and look at the first few rows.\nThe researcher who collected the data wants to see which beers are rated similarly to which other beers. Try to create a distance matrix from these data and explain why it didn’t do what you wanted. (Remember to get rid of the student column first.)\nThe R function t() transposes a matrix: that is, it interchanges rows and columns. Feed the transpose of your read-in beer ratings into dist. Does this now give distances between beers?\nTry to explain briefly why I used as.dist in the class example (the languages one) but dist here. (Think about the form of the input to each function.)\n* Obtain a clustering of the beers, using Ward’s method. Show the dendrogram.\nWhat seems to be a sensible number of clusters? Which beers are in which cluster?\nRe-draw your dendrogram with your clusters indicated.\nObtain a K-means clustering with 2 clusters.3 Note that you will need to use the (transposed) original data, not the distances. Use a suitably large value of nstart. (The data are ratings all on the same scale, so there is no need for scale here. In case you were wondering.)\nHow many beers are in each cluster?\nWhich beers are in each cluster? You can do this simply by obtaining the cluster memberships and using sort as in the last question, or you can do it as I did in class by obtaining the names of the things to be clustered and picking out the ones of them that are in cluster 1, 2, 3, .)\n\nMy solutions follow:"
  },
  {
    "objectID": "kmeans-cluster.html#clustering-the-australian-athletes-1",
    "href": "kmeans-cluster.html#clustering-the-australian-athletes-1",
    "title": "34  K-means cluster analysis",
    "section": "34.6 Clustering the Australian athletes",
    "text": "34.6 Clustering the Australian athletes\nRecall the Australian athlete data (that we’ve seen so many times before). This time, we’ll do some K-means clustering, and then see whether athletes of certain genders and certain sports tend to end up in the same cluster.\n\nRead in the data from link, recalling that the data values are separated by tabs. Display (some of) the data set.\n\nSolution\nSo, read_tsv.\n\nmy_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/ais.txt\"\nathletes &lt;- read_tsv(my_url)\n\nRows: 202 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nathletes\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nFrom your data frame, select only the columns that are numbers (or get rid of the ones that are text), and standardize all of the columns you have left. This is, done the best way, a slick piece of code. Display what you get.\n\nSolution\nThis first one is a bit too slick:\n\nathletes %&gt;% mutate(across(where(is.numeric), \\(x) scale(x)))\n\n\n\n  \n\n\n\nIt standardizes all the columns that are numeric all right, but any other columns it finds it leaves as they are, while we want to get rid of them first. So do it in two steps: get the numeric columns, and standardize all of those:\n\nathletes %&gt;% select(where(is.numeric)) %&gt;% \n  mutate(across(everything(), \\(x) scale(x))) -&gt; athletes.s\n\nThis, in fact:\n\nathletes.s\n\n\n\n  \n\n\n\nThe columns might have weird names, possibly because scale expects a matrix or data frame (to standardize each column), and here it’s getting the columns one at a time.\nElsewhere, I stuck scale() on the end, which produces a matrix, which I should then display the top of (it has 200-plus rows):\n\nathletes %&gt;% select(where(is.numeric)) %&gt;% scale() %&gt;% head()\n\n            RCC        WCC         Hc         Hg        Ferr        BMI\n[1,] -0.3463363  3.4385826 -0.2434034 -0.7092631 -1.19736325 -1.3254121\n[2,] -1.2415791 -0.6157363 -1.3900079 -1.3698371 -0.37633203 -0.6305634\n[3,] -1.2197439  0.2728816 -1.5265084 -1.6634256 -1.15525908 -0.5432708\n[4,] -0.8703809 -0.3935818 -1.4719082 -1.6634256 -0.98684242 -0.6724638\n[5,] -1.4380958 -0.7268135 -1.1989072 -1.2964400  0.02365754 -0.4140778\n[6,] -1.3070846 -0.5601977 -1.7722094 -2.0304111 -1.17631117 -0.5502542\n            SSF      %Bfat        LBM         Ht         Wt\n[1,] -0.6148189 -0.3582372 -0.8977457 -0.3394075 -1.0849225\n[2,]  1.2644802  1.8986922 -1.3606308 -0.7708629 -0.8623105\n[3,]  0.6134811  0.9503618 -0.8747927 -0.4215895 -0.6253364\n[4,]  0.8990609  0.9891351 -1.2313290 -1.0482270 -1.0274742\n[5,]  1.6298994  1.5513480 -0.6751017  0.2975028 -0.1513883\n[6,]  0.6564716  0.5416266 -0.6444978 -0.1955890 -0.5104399\n\n\nThe first athlete has a WCC value that is very large compared to the others.\nExtra: for those keeping track, sometimes you need an across and sometimes you don’t. The place where you need across is when you want to apply something to a bunch of columns all at once. select doesn’t need it, but something like mutate or summarize does, because you are changing the values in or summarizing several columns all at once.\nOne more: if the columns you are acting on in across are selected using a select helper (or by naming them or in some other way that depends on their names), you put that directly inside across (as in across(everything()) above), but if you are choosing the columns to act on by a property of them (eg. that they are numbers), you have a where inside the across, as in across(where(is.numeric)). You typically will be closing several brackets at the end. In R Studio, when you type a close-bracket, it briefly shows you the matching open-bracket so that you can keep track.\n\\(\\blacksquare\\)\n\nMake a data frame that contains the total within-cluster sum of squares from a K-means clustering for each number of clusters from 2 to 20.\n\nSolution\nI’m going to attempt a slick way of doing this, and then I’ll talk about how I’d expect you to tackle this. First, though, I set the random number seed so that everything comes out the same every time I run it:\n\nset.seed(457299)\n\nHere we go:\n\nwithinss &lt;- tibble(clusters = 2:20) %&gt;%\n  rowwise() %&gt;% \n  mutate(wss = kmeans(athletes.s, clusters, nstart = 20)$tot.withinss)\nwithinss\n\n\n\n  \n\n\n\nA one-liner, kinda. Remember that kmeans expects a single number of clusters, a value like 5, rather than a collection of possible numbers of clusters in a vector, so to do each of them, we need to work rowwise (and do one row at a time).\nThe advantage to this is that it looks exactly like the kmeans that you would write.\nAll right then, what is a better way to do this? First write a function to take a number of clusters and a data frame and return the total within-cluster sum of squares:\n\ntwss &lt;- function(i, x) {\n  ans &lt;- kmeans(x, i, nstart = 20)\n  ans$tot.withinss\n}\n\nand test it (against my answer above):\n\ntwss(3, athletes.s)\n\n[1] 1201.346\n\n\nCheck (with a few extra decimals).\nThen calculate all the total within-cluster sum of squares values by making a little data frame with all your numbers of clusters:\n\ntibble(clusters = 2:20)\n\n\n\n  \n\n\n\nand then make a pipeline and save it, using rowwise and your function:\n\ntibble(clusters = 2:20) %&gt;%\n  rowwise() %&gt;% \n  mutate(wss = twss(clusters, athletes.s)) -&gt; withinss\nwithinss\n\n\n\n  \n\n\n\nThis is better because the mutate line is simpler; you have off-loaded the details of the thinking to your function. Read this as “for each number of clusters, work out the total within-cluster sum of squares for that number of clusters.” The important thing here is what you are doing, not how you are doing it; if you care about the how-you-are-doing-it, go back and look at your function. Remember that business about how you can only keep track of seven things, plus or minus two, at once? When you write a function, you are saving some of the things you have to keep track of.\n\\(\\blacksquare\\)\n\nUse the data frame you just created to make a scree plot. What does the scree plot tell you?\n\nSolution\nggscreeplot is for principal components; this one you can plot directly, with the points joined by lines:\n\nggplot(withinss, aes(x = clusters, y = wss)) + geom_point() + geom_line()\n\n\n\n\nOn this plot, you are looking for “elbows”, but ones sufficiently far down the mountain. For example, that’s an elbow at 4 clusters, but it’s still up the mountain, which means that the total within-cluster sum of squares is quite large and that the athletes within those 4 clusters might be quite dissimilar from each other. I see an elbow at 12 clusters and possibly others at 14, 16 and 19; these are nearer the bottom of the mountain, so that the athletes within a cluster will be quite similar to each other. With over 200 athletes, there’s no problem having as many as 19 clusters, because that will still offer you some insight.\nSo I’m thinking 12 clusters (because I want to have a fairly small number of clusters to interpret later).\nThe other thing I’m thinking is I could have put a bigger number of clusters on the scree plot. The wss axis should go all the way down to 0 for 202 clusters, with each athlete in one cluster. So you could make the point that even 20 clusters is still a fair way up the mountain.\n\\(\\blacksquare\\)\n\nUsing a sensible number of clusters as deduced from your scree plot, run a K-means cluster analysis. Don’t forget the nstart!\n\nSolution\nThis:\n\nathletes.km &lt;- kmeans(athletes.s, 11, nstart = 20)\n\nor for your chosen number of clusters.\nI don’t think there’s any great need to display the output, since the most interesting thing is which athletes are in which cluster, which we’ll get to next.\n\\(\\blacksquare\\)\n\nMake a data frame consisting of the athletes’ sport and gender, and which of your clusters they belong to, taking the appropriate things from the appropriate one of your data frames.\n\nSolution\n\nathletes2 &lt;- tibble(\n  gender = athletes$Sex,\n  sport = athletes$Sport,\n  cluster = athletes.km$cluster\n)\nathletes2\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nUsing the data frame you created in the previous part, display all the athletes in some of your clusters. Do the athletes within a cluster appear to have anything in common? (If a cluster has more than 10 athletes in it, make sure to look at them all.)\n\nSolution\nLet’s start with my cluster 1:\n\nathletes2 %&gt;% filter(cluster == 4)\n\n\n\n  \n\n\n\nThese are almost all female, and if you remember back to our study of height and weight for these data, these are the kinds of sport that are played by shorter, lighter people. Cluster 2:\n\nathletes2 %&gt;% filter(cluster == 2) \n\n\n\n  \n\n\n\nMales, apparently some of the more muscular ones, but not the field athletes.\n\nathletes2 %&gt;% filter(cluster == 3) \n\n\n\n  \n\n\n\nThis is an odd one, since there is one male rower (rowers tend to be fairly big) along with a bunch of females mostly from sports involving running. I have a feeling this rower is a “cox”, whose job is not to row, but to sit in the boat and keep everybody in time by yelling out “stroke” in rhythm. Since the cox is not rowing, they need to be light in weight.\nLet’s investigate:\n\nathletes %&gt;%\n  select(gender = Sex, sport = Sport, ht = Ht, wt = Wt) %&gt;%\n  mutate(cluster = athletes.km$cluster) -&gt; athletes2a\nathletes2a %&gt;% filter(sport == \"Row\", cluster == 3)\n\n\n\n  \n\n\n\nHow does this athlete compare to the other rowers?\n\nathletes2a %&gt;%\n  filter(sport == \"Row\") %&gt;%\n  select(ht, wt) %&gt;%\n  summary()\n\n       ht              wt       \n Min.   :156.0   Min.   :49.80  \n 1st Qu.:179.3   1st Qu.:72.90  \n Median :181.8   Median :78.70  \n Mean   :182.4   Mean   :78.54  \n 3rd Qu.:186.3   3rd Qu.:87.20  \n Max.   :198.0   Max.   :97.00  \n\n\nThe rower that is in cluster 3 is almost the lightest, and also almost the shortest, of all the rowers. Cluster 4:\n\nathletes2 %&gt;% filter(cluster == 4) \n\n\n\n  \n\n\n\nMales, but possibly more muscular ones.\n\nathletes2 %&gt;% filter(cluster == 5) \n\n\n\n  \n\n\n\nMore males, from similar sports. I wonder what makes these last two clusters different?\nOne more:\n\nathletes2 %&gt;% filter(cluster == 6) \n\n\n\n  \n\n\n\nThese are three of our “big guys”, by the looks of it.\n\\(\\blacksquare\\)\n\nAdd the cluster membership to the data frame you read in from the file, and do a discriminant analysis treating the clusters as known groups. You can display the output.\n\nSolution\nMASS is already loaded (for me), so:\n\nathletes.3 &lt;- athletes %&gt;%\n  mutate(cluster = athletes.km$cluster) %&gt;%\n  lda(cluster ~ RCC + WCC + Hc + Hg + Ferr + BMI + SSF + `%Bfat` + LBM + Ht + Wt, data = .)\n\nWe can display all the output now. The problem here, with 12 groups and 11 variables, is that there is rather a lot of it:\n\nathletes.3\n\nCall:\nlda(cluster ~ RCC + WCC + Hc + Hg + Ferr + BMI + SSF + `%Bfat` + \n    LBM + Ht + Wt, data = .)\n\nPrior probabilities of groups:\n         1          2          3          4          5          6          7 \n0.01980198 0.12376238 0.05445545 0.13366337 0.08415842 0.15841584 0.04950495 \n         8          9         10         11 \n0.04455446 0.14356436 0.13366337 0.05445545 \n\nGroup means:\n        RCC      WCC       Hc       Hg      Ferr      BMI       SSF   `%Bfat`\n1  6.000000 8.150000 52.37500 17.87500  52.50000 23.91750  45.27500  8.655000\n2  4.292800 6.390000 39.96000 13.52400  70.08000 19.71680  52.53600 11.629600\n3  4.963636 8.054545 44.40000 14.63636  54.18182 19.73091  48.29091 10.690909\n4  4.592963 7.292593 42.65926 14.35926  46.03704 22.74333  86.31111 18.587778\n5  4.960000 9.729412 45.55294 15.51176 119.29412 25.20706  62.37059 11.151176\n6  4.962500 6.440625 45.20312 15.39375  63.15625 22.21344  39.48438  7.270625\n7  5.182000 7.210000 46.44000 15.93000 200.30000 22.96800  47.22000  8.737000\n8  5.077778 7.255556 46.32222 16.03333 136.88889 31.03222  94.40000 16.624444\n9  4.167241 5.993103 38.24828 12.68966  53.48276 22.05897  96.97241 19.652069\n10 4.964444 6.607407 44.95926 15.21481  78.62963 24.22037  54.69259  9.559630\n11 4.336364 8.818182 39.13636 13.21818  70.00000 25.03727 150.16364 26.948182\n        LBM       Ht        Wt\n1  71.00000 180.5250  77.85000\n2  47.89800 165.5560  54.21200\n3  52.16818 172.0909  58.42727\n4  58.60296 178.0926  72.02963\n5  79.23529 188.2059  89.25588\n6  68.31250 182.1719  73.64844\n7  68.60000 180.8200  75.27000\n8  85.20444 181.1667 102.02222\n9  56.45069 178.5828  70.30862\n10 81.96296 193.4630  90.67407\n11 57.36364 177.1273  78.66364\n\nCoefficients of linear discriminants:\n                LD1         LD2           LD3         LD4          LD5\nRCC     -1.28721668 -0.12082768 -0.1800745307 -1.32581822 -1.297736838\nWCC     -0.17572030  0.04445962  0.1815301070 -0.06148203  0.255180891\nHc      -0.06887672 -0.01316703  0.0006390582 -0.06578884  0.157668660\nHg      -0.22783310 -0.23151901  0.1206000078 -0.40505906  0.064329099\nFerr    -0.01265103  0.00215038  0.0204843448  0.00902723 -0.017202106\nBMI     -0.03888179  0.60898248 -0.7520786724 -2.05384021 -1.888427011\nSSF     -0.02326379  0.01808028  0.0068966220  0.05181218 -0.022649744\n`%Bfat`  0.33439665  0.31999943 -0.5379290003 -0.05571142 -0.008652337\nLBM      0.02714856  0.27988180 -0.7311870862  0.41363393 -0.137853460\nHt      -0.01345762  0.12483039 -0.2835260888 -0.48139186 -0.542575152\nWt      -0.09727561 -0.30961926  0.8755149125  0.28575160  0.770957102\n                 LD6         LD7          LD8           LD9         LD10\nRCC     -1.355071630 -0.68078007 -3.768561032 -1.5595553389  2.018076802\nWCC     -0.403452734  0.24436067  0.273515110 -0.1562834202  0.105214336\nHc       0.056928597  0.14865448 -0.020949089 -0.3197302898 -0.348771286\nHg       0.194535578  0.04096553  0.865388620  1.9932096086  0.167943193\nFerr     0.003547754  0.01070359 -0.002033483 -0.0006917844 -0.002233497\nBMI      0.357652569 -0.84389019  1.406309919 -0.9413566093  0.548574435\nSSF     -0.106308127 -0.08997047  0.002423251  0.0450725511  0.012458031\n`%Bfat`  0.372222136  0.97871158 -0.137349422  0.2769699401  0.752495767\nLBM     -0.170985948  0.57183247 -0.040113894  0.5003467041  1.165555975\nHt       0.008320643 -0.12624874  0.344376866 -0.1813212974  0.020106963\nWt       0.138538684 -0.27999325 -0.388631504 -0.2200445172 -1.130827203\n\nProportion of trace:\n   LD1    LD2    LD3    LD4    LD5    LD6    LD7    LD8    LD9   LD10 \n0.5062 0.2313 0.0903 0.0717 0.0307 0.0291 0.0265 0.0089 0.0044 0.0010 \n\n\n\\(\\blacksquare\\)\n\nHow many linear discriminants do you have? How many do you think are important?\n\nSolution\nProportion of trace, at the bottom of the output.\nIt’s hard to draw the line here. The first two, or maybe the first seven, or something like that. Your call.\n\\(\\blacksquare\\)\n\nWhich variables seem to be important in distinguishing the clusters? Look only at the linear discriminants that you judged to be important.\n\nSolution\nLook at the coefficients of linear discriminants. This is rather large, since I had 12 clusters, and thus there are 11 LDs.\nIf we go back to my thought of only using two linear discriminants: LD1 is mostly RCC positively and BMI negatively, in that an athlete with large RCC and small BMI will tend to score high (positive) on LD1. BMI is the familiar body fat index. LD2 depends on RCC again, but this time negatively, and maybe percent body fat and LBM. And so on, if you went on.\nIt may be that RCC is just very variable anyway, since it seems to appear just about everywhere.\nExtra: we can also look at the means on each variable by cluster, which is part of the output, in “Group Means”. Perhaps the easiest thing to eyeball here is the cluster in which a variable is noticeably biggest (or possibly smallest). For example, WCC is highest in cluster 4, and while Ferritin is high there, it is higher still in cluster 5. BMI is highest in cluster 6 and lowest in clusters 1 and 3. Height is smallest in cluster 1, with weight being smallest there as well, and weight is much the biggest in cluster 6.\n\\(\\blacksquare\\)\n\nDraw a biplot (which shows the first two LDs), drawing the clusters in different colours. Comment briefly on anything especially consistent or inconsistent with what you’ve seen so far.\n\nSolution\nThe thing to get the colours is to feed a groups into ggbiplot. I suspect I need the factor in there because the clusters are numbers and I want them treated as categorical (the numbers are labels). Also, note that we will have a lot of colours here, so I am trying to make them more distinguishable using scale_colour_brewer from the RColorBrewer package (loaded at the beginning):\n\nggbiplot(athletes.3, groups = factor(athletes2$cluster)) +\n  scale_colour_brewer(palette = \"Set3\")\n\n\n\n\nWhat the biplot shows, that we haven’t seen any hint of so far, is that the clusters are pretty well separated on LD1 and LD2: there is not a great deal of overlap.\nAnyway, low LD1 means high on BMI and low on RCC, as we saw before. The arrow for RCC points down as well as right, so it’s part of LD2 as well. There isn’t much else that points up or down, but percent body fat and LBM do as much as anything. This is all pretty much what we saw before.\nAs to where the clusters fall on the picture:\n\nCluster 1 in light blue was “small and light”: small BMI, so ought to be on the right. This cluster’s RCC was also small, which on balance puts them on the left, but then they should be top left because RCC points down. I dunno.\nCluster 2 in dark blue was “more muscular males”, mid-right, so above average on LD1 but about average on LD2.\nCluster 3, light green, was “running females” (mostly), lower left, so below average on both LD1 and LD2.\nCluster 4, dark green, “more muscular males” again. There is a lot of overlap with cluster 2.\nCluster 5, pink, was “yet more males”. Mostly above average on LD1 and below average on LD2. The latter was what distinguished these from clusters 4 and 2.\nCluster 6, red, was “big guys”. The biggest on LD1 and almost the biggest on LD2.\n\nThere is something a bit confusing in LD1, which contrasts RCC and BMI. You would expect, therefore, RCC and BMI to be negatively correlated, but if you look at the cluster means, that isn’t really the story: for example, cluster 1 has almost the lowest mean on both variables, and the highest RCC, in cluster 11, goes with a middling BMI.\nI like these colours much better than the default ones. Much easier to tell apart. In any case, RCC and BMI seem to be important, so let’s plot them against each other, coloured by cluster:\n\nathletes %&gt;%\n  mutate(cluster = factor(athletes2$cluster)) %&gt;%\n  ggplot(aes(x = RCC, y = BMI, colour = cluster)) +\n  geom_point() + scale_colour_brewer(palette = \"Paired\")\n\n\n\n\nI decided to create a column called cluster in the data frame, so that the legend would have a nice clear title. (If you do the factor(athletes2$cluster) in the ggplot, that is what will appear as the legend title.)\nThere seems to be very little relationship here, in terms of an overall trend on the plot. But at least these two variables do something to distinguish the clusters. It’s not as clear as using LD1 and LD2 (as it won’t be, since they’re designed to be the best at separating the groups), but you can see that the clusters are at least somewhat distinct.\nThe “paired” part of the colour palette indicates that successive colours come in pairs: light and dark of blue, green, red, orange, purple and brown (if you think of yellow as being “light brown” or brown as being “dark yellow”, like bananas).\nA good resource for RColorBrewer is link. The “qualitative palettes” shown there are for distinguishing groups (what we want here); the sequential palettes are for distinguishing values on a continuous scale, and the diverging palettes are for drawing attention to high and low.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "kmeans-cluster.html#running-jumping-and-throwing-1",
    "href": "kmeans-cluster.html#running-jumping-and-throwing-1",
    "title": "34  K-means cluster analysis",
    "section": "34.7 Running, jumping, and throwing",
    "text": "34.7 Running, jumping, and throwing\nThe decathlon is a men’s4 track-and-field competition in which competitors complete 10 events over two days as follows, requiring the skills shown:\n\n\n\n\n\nEvent\nSkills\n\n\n\n\n100m\nRunning, speed\n\n\nLong jump\nJumping, speed\n\n\nShot put\nThrowing, strength\n\n\nHigh jump\nJumping, agility\n\n\n400m\nRunning, speed\n\n\n110m hurdles\nRunning, jumping, speed\n\n\nDiscus\nThrowing, agility (and maybe strength)\n\n\nPole vault\nJumping, agility\n\n\nJavelin\nThrowing, agility\n\n\n1500m\nRunning, endurance\n\n\n\n\n\n(note: in the pdf version, this table might appear twice.)\nThese are a mixture of running, jumping and throwing disciplines. The performance (time, distance or height) achieved in each event is converted to a number of points using standard tables. and the winner of the entire decathlon is the competitor with the largest total of points. The basic idea is that a “good” performance in an event is worth 1000 points, and the score decreases if the athlete takes more seconds (running) or achieves fewer metres (jumping/throwing). A good decathlete has to be at least reasonably good at all the disciplines.\nFor the decathlon competition at the 2013 Track and Field World Championship, a record was kept of each competitor’s performance in each event (for the competitors that competed in all ten events). These values are in link.\n\nRead in the data and verify that you have the right number of variables.\n\nSolution\nChecking the file, this is delimited by single spaces. You might be concerned by the quotes; we’ll read them in and see what happens to them.\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/dec2013.txt\"\ndecathlon0 &lt;- read_delim(my_url, \" \")\n\nRows: 24 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr  (1): name\ndbl (10): x100m, long.jump, shot.put, high.jump, x400m, x110mh, discus, pole...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndecathlon0\n\n\n\n  \n\n\n\nThe names got shortened for display, but the quotes seem to have properly disappeared.\nNote that the columns that would otherwise start with digits have x on the front of their names, so as to guarantee that the column names are legal variable names (and thus won’t require any special treatment later).\n\\(\\blacksquare\\)\n\nSome of the performances are times in seconds, and some of them are distances (or heights) in metres. Also, some of the columns are more variable than others. Produce a matrix of standardized performances in each event, making sure not to try to standardize the names!\n\nSolution\nscale is what I am trying to hint towards. Leave off the first column. I would rather specify this by name than by number. (People have an annoying habit of changing the order of columns, but the column name is more work to change and thus it is less likely that it will change.)\n\ndecathlon0 %&gt;%\n  select(-name) %&gt;%\n  scale() -&gt; decathlon\nround(decathlon, 2)\n\n      x100m long.jump shot.put high.jump x400m x110mh discus pole.vault javelin x1500m\n [1,] -2.21      1.24     0.29     -0.95 -2.43  -1.77   0.27       1.10    0.55  -0.49\n [2,] -1.92      0.16     0.03      0.74 -0.46  -1.23  -0.06      -0.39    0.52  -0.46\n [3,] -1.33     -0.38     0.96     -0.11 -0.75  -1.37   1.71      -0.02   -1.17   0.63\n [4,] -1.08      0.54    -1.24     -0.53 -1.02   0.17  -0.09      -0.02   -0.60  -0.93\n [5,] -0.87      1.62     0.57     -0.11 -1.08  -0.50   0.82       0.36    0.72  -1.10\n [6,] -0.69      0.64     0.46     -0.53 -0.13  -1.03   0.59       0.73   -0.42   0.37\n [7,] -0.48      1.46     0.77      2.01 -0.33   0.13  -0.73      -1.14   -0.82   0.35\n [8,] -0.45      0.99    -0.21      0.32 -0.59  -0.74  -1.95       1.48   -1.06  -1.20\n [9,] -0.10     -0.47     2.68     -0.11 -0.46  -0.12   0.53      -0.76    1.00   0.54\n[10,] -0.10      0.32    -0.54      0.74 -0.53  -0.47  -0.40      -1.51    1.45  -1.21\n[11,] -0.02      0.03    -0.54      0.74 -0.47  -0.39  -0.09       1.85   -0.52   0.50\n[12,] -0.02      0.26    -0.10     -0.53 -0.13   0.69   1.42      -1.14   -2.26   0.06\n[13,]  0.01     -0.12    -0.02     -1.37  1.80   1.60   0.37      -1.51    1.46   1.82\n[14,]  0.33     -0.03    -0.02     -1.37 -0.62   0.24   0.81      -0.02    1.30  -1.31\n[15,]  0.40      0.95    -1.04      0.74  0.03   0.33  -1.20       0.73    0.65   0.64\n[16,]  0.47     -0.79     0.36     -0.11  0.04  -0.68  -0.09       0.36   -0.05  -0.05\n[17,]  0.57     -0.19    -0.60      0.32  1.07   1.51  -0.69      -1.51   -0.95   0.72\n[18,]  0.61     -2.09    -1.63     -1.37 -0.24  -0.32  -2.39       0.73    0.46   0.36\n[19,]  0.75      0.16     1.03      1.59  0.57  -0.16   0.70       0.73   -0.42   0.88\n[20,]  0.82     -0.25    -0.86      1.59  1.36   1.74  -0.94      -0.02   -0.49   2.07\n[21,]  0.89      0.51    -0.73      0.74  0.47  -0.68   0.41       1.10    0.80  -1.14\n[22,]  1.24     -0.69     1.07     -0.11  0.73   0.06   1.26       0.36    0.16  -0.02\n[23,]  1.56     -2.28     0.98     -1.80  1.32   1.18  -0.69      -1.51   -1.44  -1.83\n[24,]  1.63     -1.58    -1.69     -0.53  1.85   1.80   0.39      -0.02    1.11   0.78\nattr(,\"scaled:center\")\n     x100m  long.jump   shot.put  high.jump      x400m     x110mh     discus pole.vault \n 10.977083   7.339167  14.209583   1.997500  48.960000  14.512500  44.288333   4.904167 \n   javelin     x1500m \n 62.069583 273.306667 \nattr(,\"scaled:scale\")\n     x100m  long.jump   shot.put  high.jump      x400m     x110mh     discus pole.vault \n0.28433720 0.31549708 0.61480629 0.07091023 1.20878667 0.44795429 2.60828224 0.26780779 \n   javelin     x1500m \n5.01529875 7.22352899 \n\n\nI think the matrix of standardized values is small enough to look at all of, particularly if I round off the values to a small number of decimals. (Note that the means and SDs appear at the bottom as “attributes”.)\n\\(\\blacksquare\\)\n\nWe are going to make a scree plot to decide on the number of clusters our K-means clustering should use. Using a loop, or otherwise,5 obtain the total within-cluster sum of squares for these data for each number of clusters for 2 up to 20.\n\nSolution\nHaving kind of given the game away in the footnote, I guess I now have to keep up my end of the deal and show you the obvious way and the clever way. The obvious way is to do a Python-style loop, thus:\n\nmaxclust\n\n[1] 20\n\nw &lt;- numeric(0)\nfor (i in 2:maxclust) {\n  sol &lt;- kmeans(decathlon, i, nstart = 20)\n  w[i] &lt;- sol$tot.withinss\n}\nw\n\n [1]        NA 175.03246 151.08750 131.30247 113.59681 100.26941  89.51098  78.89089\n [9]  68.99662  60.77665  54.29991  47.64227  41.55046  35.39181  29.52008  25.05344\n[17]  21.02841  17.28444  13.80627  10.44197\n\n\nI defined maxclust earlier, surreptitiously. (Actually, what happened was that I changed my mind about how many clusters I wanted you to go up to, so that instead of hard-coding the maximum number of clusters, I decided to put it in a variable so that I only had to change it once if I changed my mind again.)\nI decided to split the stuff within the loop into two lines, first getting the \\(i\\)-cluster solution, and then pulling out the total within-cluster sum of squares from it and saving it in the right place in w. You can do it in one step or two; I don’t mind.\nThe first value in w is missing, because we didn’t calculate a value for 1 cluster (so that this w has 20 values, one of which is missing).\nNot that there’s anything wrong with this,6 and if it works, it’s good, but the True R Way7 is not to use a loop, but get the whole thing in one shot. The first stage is to figure out what you want to do for some number of clusters. In this case, it’s something like this:\n\nkmeans(decathlon, 3, nstart = 20)$tot.withinss\n\n[1] 151.0875\n\n\nThere’s nothing special about 3; any number will do.\nThe second stage is to run this for each desired number of clusters, without using a loop. There are two parts to this, in my favoured way of doing it. First, write a function that will get the total within-group sum of squares for any K-means analysis for any number of clusters (input) for any dataframe (also input):\n\ntwss &lt;- function(i, d) {\n  ans &lt;- kmeans(d, i, nstart = 20)\n  ans$tot.withinss\n}\n\nThe value of doing it this way is that you only ever have to write this function once, and you can use it for any K-means analysis you ever do afterwards. Or, copy this one and use it yourself.\nLet’s make sure it works:\n\ntwss(3, decathlon)\n\n[1] 151.0875\n\n\nCheck.\nSecond, use rowwise to work out the total within-group sum of squares for a variety of numbers of clusters. What you use depends on how much data you have, and therefore how many clusters you think it would be able to support (a smallish fraction of the total number of observations). I went from 2 to 20 before, so I’ll do that again:\n\ntibble(clusters = 2:20) %&gt;% \n  rowwise() %&gt;% \n  mutate(wss = twss(clusters, decathlon)) -&gt; wsss\nwsss\n\n\n\n  \n\n\n\nThis got a name that was wss with an extra s. Sometimes my imagination runs out.\nThere was (still is) also a function sapply that does the same thing. I learned sapply and friends a long time ago, and now, with the arrival of rowwise, I think I need to unlearn them.8\nExtra: I made a post on Twitter, link. To which Malcolm Barrett replied with this: link and this: link. So now you know all about the Four Noble R Truths.\n\\(\\blacksquare\\)\n\nUsing what you calculated in the previous part, draw a scree plot. How does your scree plot tell you that 5 is a possible number of clusters? Explain briefly.\n\nSolution\nThis requires a teeny bit of care. If you went the loop way, what I called w has a missing value first (unless you were especially careful), so you have to plot it against 1 through 20:\n\ntibble(clusters = 1:maxclust, wss = w) %&gt;%\n  ggplot(aes(x = clusters, y = wss)) +\n  geom_point() + geom_line()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nThe warning message is to say that you don’t have a total within-cluster sum of squares for 1 cluster, which you knew already.\nOr you can save the data frame first and then feed it into ggplot.\nIf you went the rowwise way, you will have the wss values for 2 through 20 clusters already in a data frame, so it is a fair bit simpler:\n\nwsss %&gt;%\n  ggplot(aes(x = clusters, y = wss)) +\n  geom_point() + geom_line()\n\n\n\n\nThere is, I suppose, the tiniest elbow at 5 clusters. It’s not very clear, though. I would have liked it to be clearer.\n\\(\\blacksquare\\)\n\nRun K-means with 5 clusters. Produce an output that shows which competitors are in which cluster.\n\nSolution\nIf you’re using R Markdown, you might like to start with this:\n\nset.seed(457299)\n\nor some other random number seed of your choice. Using nstart=20 or similar will give you the same clustering, but which cluster is cluster 1 might vary between runs. So if you talk about cluster 1 (below), and re-knit the document, you might otherwise find that cluster 1 has changed identity since the last time you knitted it. (I just remembered that for these solutions.)\nRunning the kmeans itself is a piece of cake, since you have done it a bunch of times already (in your loop or rowwise):\n\ndecathlon.1 &lt;- kmeans(decathlon, 5, nstart = 20)\ndecathlon.1\n\nK-means clustering with 5 clusters of sizes 4, 8, 5, 6, 1\n\nCluster means:\n        x100m   long.jump   shot.put     high.jump      x400m     x110mh     discus\n1  0.75760985 -0.53619092 -0.7922550 -1.554312e-15  1.5180512  1.6631161 -0.2159787\n2 -0.02051555  0.02245134  0.9034011  2.644188e-01 -0.0589434 -0.3097414  0.6739749\n3  0.28457995  0.07871177 -0.8288519  2.326886e-01 -0.1588370 -0.3582955 -1.0406594\n4 -0.97448850  0.64184430 -0.1484207 -2.467909e-01 -1.0216857 -0.5934385  0.2274805\n5  1.55771620 -2.27947172  0.9765949 -1.798048e+00  1.3236413  1.1775755 -0.6894704\n   pole.vault     javelin     x1500m\n1 -0.76236268  0.28321676  1.3477946\n2 -0.10890895 -0.49565010  0.3441300\n3  1.17932839  0.06548297 -0.1670467\n4 -0.07779211  0.65707285 -0.9136808\n5 -1.50916693 -1.43751822 -1.8269002\n\nClustering vector:\n [1] 4 4 2 4 4 2 2 3 2 4 3 2 1 4 3 2 1 3 2 1 3 2 5 1\n\nWithin cluster sum of squares by cluster:\n[1] 18.86978 41.40072 26.24500 27.08131  0.00000\n (between_SS / total_SS =  50.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nI displayed the result, so that I would know which of the things I needed later. The Available components at the bottom is a big hint with this.\nTo display who is in which cluster, it’s easiest to make a data frame of names and clusters and sort it:\n\ntibble(name = decathlon0$name, cluster = decathlon.1$cluster) %&gt;%\n  arrange(cluster) \n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nDisplay the cluster means for all of the events. (This has already been calculated; you just have to display it.) Find the cluster mean, looking at all of the events, that is farthest from zero, and see if you can describe the strengths and weaknesses of the athletes in that cluster (look at all the events for the cluster that has that extreme mean). Bear in mind (i) that these are the original performances standardized, and (ii) for a running event, a smaller value is better.\n\nSolution\nThis is the thing called centers:9\n\ndecathlon.1$centers\n\n        x100m   long.jump   shot.put     high.jump      x400m     x110mh     discus\n1  0.75760985 -0.53619092 -0.7922550 -1.554312e-15  1.5180512  1.6631161 -0.2159787\n2 -0.02051555  0.02245134  0.9034011  2.644188e-01 -0.0589434 -0.3097414  0.6739749\n3  0.28457995  0.07871177 -0.8288519  2.326886e-01 -0.1588370 -0.3582955 -1.0406594\n4 -0.97448850  0.64184430 -0.1484207 -2.467909e-01 -1.0216857 -0.5934385  0.2274805\n5  1.55771620 -2.27947172  0.9765949 -1.798048e+00  1.3236413  1.1775755 -0.6894704\n   pole.vault     javelin     x1500m\n1 -0.76236268  0.28321676  1.3477946\n2 -0.10890895 -0.49565010  0.3441300\n3  1.17932839  0.06548297 -0.1670467\n4 -0.07779211  0.65707285 -0.9136808\n5 -1.50916693 -1.43751822 -1.8269002\n\n\nMy most extreme value is the \\(-2.28\\) in the long jump column, cluster 4. Yours may well be different, since the formation of clusters is random: it will probably not be the same number cluster, and it might not even be the same value. Use whatever you have. (I asked you to find the most extreme one so that the other events in the same cluster are likely to be extreme as well and you have something to say.)\nSo I have to look along my cluster 4 row. I see:\n\n100m run high (bad)\nlong jump low (bad)\nshot put high (good)\nhigh jump low (bad)\n400m run high (bad)\n110m hurdles run high (bad)\ndiscus lowish (bad)\npole vault low (bad)\njavelin low (bad)\n1500m low (good)\n\nThe only two good events here are shot put (throwing a heavy ball) and 1500m (a long run). So what these athletes have in common is good strength and endurance, and bad speed and agility. (You can use my “skills required” in the table at the top of the question as a guide.)\nI said “these athletes”. I actually meant “this athlete”, since this is the cluster with just Marcus Nilsson in it. I ought to have checked that we were looking at a cluster with several athletes in it, and then this question would have made more sense, but the thought process is the same, so it doesn’t matter so much.\nYour cluster may well be different; I’m looking for some sensible discussion based on the values you have. I’m hoping that the athletes in your cluster will tend to be good at something and bad at something else, and the things they are good at (or bad at) will have something in common.\nWhat would have made more sense would have been to take the biggest cluster:\n\ndecathlon.1$size\n\n[1] 4 8 5 6 1\n\n\nwhich in this case is cluster 3, and then\n\ndecathlon.1$centers\n\n        x100m   long.jump   shot.put     high.jump      x400m     x110mh     discus\n1  0.75760985 -0.53619092 -0.7922550 -1.554312e-15  1.5180512  1.6631161 -0.2159787\n2 -0.02051555  0.02245134  0.9034011  2.644188e-01 -0.0589434 -0.3097414  0.6739749\n3  0.28457995  0.07871177 -0.8288519  2.326886e-01 -0.1588370 -0.3582955 -1.0406594\n4 -0.97448850  0.64184430 -0.1484207 -2.467909e-01 -1.0216857 -0.5934385  0.2274805\n5  1.55771620 -2.27947172  0.9765949 -1.798048e+00  1.3236413  1.1775755 -0.6894704\n   pole.vault     javelin     x1500m\n1 -0.76236268  0.28321676  1.3477946\n2 -0.10890895 -0.49565010  0.3441300\n3  1.17932839  0.06548297 -0.1670467\n4 -0.07779211  0.65707285 -0.9136808\n5 -1.50916693 -1.43751822 -1.8269002\n\n\nwhich says that the eight athletes in cluster 3 are a bit above average for shot put and discus, and below average for javelin, and, taking a decision, about average for everything else. This is kind of odd, since these are all throwing events, but the javelin is propelled a long way by running fast, and the other two are propelled mainly using strength rather than speed, so it makes some kind of sense (after the fact, at least).\nMy guess is that someone good at javelin is likely to be good at sprint running and possibly also the long jump, since that depends primarily on speed, once you have enough technique. Well, one way to figure out whether I was right:\n\ncor(decathlon)\n\n                 x100m   long.jump    shot.put   high.jump        x400m      x110mh\nx100m       1.00000000 -0.61351932 -0.17373396 -0.03703619  0.789091241  0.67372152\nlong.jump  -0.61351932  1.00000000  0.08369570  0.46379852 -0.548197160 -0.39484085\nshot.put   -0.17373396  0.08369570  1.00000000  0.02012049 -0.172516054 -0.28310469\nhigh.jump  -0.03703619  0.46379852  0.02012049  1.00000000  0.015217204 -0.08356323\nx400m       0.78909124 -0.54819716 -0.17251605  0.01521720  1.000000000  0.80285420\nx110mh      0.67372152 -0.39484085 -0.28310469 -0.08356323  0.802854203  1.00000000\ndiscus     -0.14989960  0.12891051  0.46449586 -0.11770266 -0.068778203 -0.13777771\npole.vault -0.12087966  0.21976890 -0.19328449  0.13565269 -0.361823592 -0.51871733\njavelin     0.02363715  0.01969302 -0.11313467 -0.12454417 -0.005823468 -0.05246857\nx1500m      0.14913949 -0.11672283 -0.06156793  0.27779220  0.446949386  0.39800522\n                discus  pole.vault      javelin       x1500m\nx100m      -0.14989960 -0.12087966  0.023637150  0.149139491\nlong.jump   0.12891051  0.21976890  0.019693022 -0.116722829\nshot.put    0.46449586 -0.19328449 -0.113134672 -0.061567926\nhigh.jump  -0.11770266  0.13565269 -0.124544175  0.277792195\nx400m      -0.06877820 -0.36182359 -0.005823468  0.446949386\nx110mh     -0.13777771 -0.51871733 -0.052468568  0.398005215\ndiscus      1.00000000 -0.10045072  0.020977427  0.019890861\npole.vault -0.10045072  1.00000000  0.052377148 -0.059888360\njavelin     0.02097743  0.05237715  1.000000000 -0.008858031\nx1500m      0.01989086 -0.05988836 -0.008858031  1.000000000\n\n\nor, for this, maybe better:\n\ncor(decathlon) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"event\") %&gt;%\n  pivot_longer(-event, names_to=\"event2\", values_to=\"corr\") %&gt;%\n  filter(event &lt; event2) %&gt;%\n  arrange(desc(abs(corr)))\n\n\n\n  \n\n\n\nI should probably talk about the code:\n\nI want to grab the event names from the row names of the matrix. This is a bit awkward, because I want to turn the matrix into a data frame, but if I turn it into a tibble, the row names will disappear.\nThus, I turn it into an old-fashioned data.frame, and then it has row names, which I can grab and put into a column called event.\nThen I make the data frame longer, creating a column event2 which is the second thing that each correlation will be between.\nThe correlations between an event and itself will be 1, and between events B and A will be the same as between A and B. So I take only the rows where the first event is alphabetically less than the second one.\nThen I arrange them in descending order of absolute correlation, since a large negative correlation is also interesting.\n\nThere are actually only a few high correlations:\n\n100m with long jump, 400m and 110m hurdles\nlong jump with 100m, high jump and 400m\nshot put with discus\nhigh jump with long jump\n400m with all the other running events plus long jump\n110m hurdles with the other running events plus pole vault\ndiscus with shot put\npole vault with 110m hurdles and maybe 400m\njavelin with nothing\n1500m with 400m\n\nSome of the correlations are negative as expected, since they are between a running event and a jumping/throwing event (that is, a long distance goes with a small time, both of which are good).\nI was wrong about javelin. It seems to be a unique skill in the decathlon, which is presumably why it’s there: you want 10 events that are as disparate as possible, rather than things that are highly correlated.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "kmeans-cluster.html#clustering-the-swiss-bills-1",
    "href": "kmeans-cluster.html#clustering-the-swiss-bills-1",
    "title": "34  K-means cluster analysis",
    "section": "34.8 Clustering the Swiss bills",
    "text": "34.8 Clustering the Swiss bills\nThis question is about the Swiss bank counterfeit bills again. This time we’re going to ignore whether each bill is counterfeit or not, and see what groups they break into. Then, at the end, we’ll see whether cluster analysis was able to pick out the counterfeit ones or not.\n\nRead the data in again (just like last time), and look at the first few rows. This is just the same as before.\n\nSolution\nThe data file was aligned in columns, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/swiss1.txt\"\nswiss &lt;- read_table(my_url)\n\n\n── Column specification ──────────────────────────────────────────────────────────────────\ncols(\n  length = col_double(),\n  left = col_double(),\n  right = col_double(),\n  bottom = col_double(),\n  top = col_double(),\n  diag = col_double(),\n  status = col_character()\n)\n\nswiss\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nThe variables in this data frame are on different scales. Standardize them so that they all have mean 0 and standard deviation 1. (Don’t try to standardize the status column!)\n\nSolution\n\nswiss.s &lt;- swiss %&gt;%\n  select(-status) %&gt;%\n  scale()\n\nWhat kind of thing do we have?\n\nclass(swiss.s)\n\n[1] \"matrix\" \"array\" \n\n\nso something like this is needed to display some of it (rather than all of it):\n\nhead(swiss.s)\n\n         length      left      right     bottom        top      diag\n[1,] -0.2549435  2.433346  2.8299417 -0.2890067 -1.1837648 0.4482473\n[2,] -0.7860757 -1.167507 -0.6347880 -0.9120152 -1.4328473 1.0557460\n[3,] -0.2549435 -1.167507 -0.6347880 -0.4966762 -1.3083061 1.4896737\n[4,] -0.2549435 -1.167507 -0.8822687 -1.3273542 -0.3119759 1.3161027\n[5,]  0.2761888 -1.444496 -0.6347880  0.6801176 -3.6745902 1.1425316\n[6,]  2.1351516  1.879368  1.3450576 -0.2890067 -0.6855997 0.7953894\n\n\n\\(\\blacksquare\\)\n\nWe are going to make a scree plot. First, calculate the total within-cluster SS for each number of clusters from 2 to 10.\n\nSolution\nWhen I first made this problem (some years ago), I thought the obvious answer was a loop, but now that I’ve been steeped in the Tidyverse a while, I think rowwise is much clearer, so I’ll do that first. Start by making a tibble that has one column called clusters containing the numbers 2 through 10:\n\ntibble(clusters = 2:10)\n\n\n\n  \n\n\n\nNow, for each of these numbers of clusters, calculate the total within-cluster sum of squares for it (that number of clusters). To do that, think about how you’d do it for something like three clusters:\n\nkmeans(swiss.s, 3, nstart = 20)$tot.withinss\n\n[1] 576.1284\n\n\nand then use that within your rowwise:\n\ntibble(clusters = 2:10) %&gt;%\n  rowwise() %&gt;% \n  mutate(wss = kmeans(swiss.s, clusters, nstart = 20)$tot.withinss) -&gt; wssq\nwssq\n\n\n\n  \n\n\n\nAnother way is to save all the output from the kmeans, in a list-column, and then extract the thing you want, thus:\n\ntibble(clusters = 2:10) %&gt;%\n  rowwise() %&gt;% \n  mutate(km = list(kmeans(swiss.s, clusters, nstart = 20))) %&gt;%\n  mutate(wss = km$tot.withinss) -&gt; wssq.2\nwssq.2\n\n\n\n  \n\n\n\nThe output from kmeans is a collection of things, not just a single number, so when you create the column km, you need to put list around the kmeans, and then you’ll create a list-column. wss, on the other hand, is a single number each time, so no list is needed, and wss is an ordinary column of numbers, labelled dbl at the top.\nThe most important thing in both of these is to remember the rowwise. Without it, everything will go horribly wrong! This is because kmeans expects a single number for the number of clusters, and rowwise will provide that single number (for the row you are looking at). If you forget the rowwise, the whole column clusters will get fed into kmeans all at once, and kmeans will get horribly confused.\nIf you insist, do it Python-style as a loop, like this:\n\nclus &lt;- 2:10\nwss.1 &lt;- numeric(0)\nfor (i in clus)\n{\n  wss.1[i] &lt;- kmeans(swiss.s, i, nstart = 20)$tot.withinss\n}\nwss.1\n\n [1]       NA 701.2054 576.4660 491.7085 449.3900 413.1265 381.5568 355.3900 334.6444\n[10] 312.8958\n\n\nNote that there are 10 wss values, but the first one is missing, since we didn’t do one cluster.10\nThe numeric(0) says “wss has nothing in it, but if it had anything, it would be numbers”. Or, you can initialize wss to however long it’s going to be (here 10), which is actually more efficient (R doesn’t have to keep making it “a bit longer”). If you initialize it to length 10, the 10 values will have NAs in them when you start. It doesn’t matter what nstart is: Ideally, big enough to have a decent chance of finding the best clustering, but small enough that it doesn’t take too long to run. Whichever way you create your total within-cluster sums of squares, you can use it to make a scree plot (next part).\n\\(\\blacksquare\\)\n\n* Make a scree plot (creating a data frame first if you need). How many clusters do you think you should use?\n\nSolution\nThe easiest is to use the output from the rowwise, which I called wssq, this already being a dataframe:\n\nggplot(wssq, aes(x = clusters, y = wss)) + geom_point() + geom_line()\n\n\n\n\nIf you did it the loop way, you’ll have to make a data frame first, which you can then pipe into ggplot:\n\ntibble(clusters = 1:10, wss = wss.1) %&gt;%\n  ggplot(aes(x = clusters, y = wss)) + geom_point() + geom_line()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nIf you started at 2 clusters, your wss will start at 2 clusters also, and you’ll need to be careful to have something like clusters=2:10 (not 1:10) in the definition of your data frame.\nInterpretation: I see a small elbow at 4 clusters, so that’s how many I think we should use. Any place you can reasonably see an elbow is good.\nThe warning is about the missing within-cluster total sum of squares for one cluster, since the loop way didn’t supply a total within-cluster sum of squares for one cluster.\n\\(\\blacksquare\\)\n\nRun K-means with the number of clusters that you found in (here). How many bills are in each cluster?\n\nSolution\nI’m going to start by setting the random number seed (so that my results don’t change every time I run this). You don’t need to do that, though you might want to in something like R Markdown code (for example, in an R Notebook):\n\nset.seed(457299)\n\nNow, down to business:\n\nswiss.7 &lt;- kmeans(swiss.s, 4, nstart = 20)\nswiss.7$size\n\n[1] 50 32 68 50\n\n\nThis many. Note that my clusters 1 and 4 (and also 2 and 3) add up to 100 bills. There were 100 genuine and 100 counterfeit bills in the original data set. I don’t know why “7”. I just felt like it. Extra: you might remember that back before I actually ran K-means on each of the numbers of clusters from 2 to 10. How can we extract that output? Something like this. Here’s where the output was:\n\nwssq.2\n\n\n\n  \n\n\n\nNow we need to pull out the 4th row and the km column. We need the output as an actual thing, not a data frame, so:\n\nwssq.2 %&gt;%\n  filter(clusters == 4) %&gt;%\n  pull(km) -&gt; swiss.7a\n\nIs that the right thing?\n\nswiss.7a\n\n[[1]]\nK-means clustering with 4 clusters of sizes 50, 50, 68, 32\n\nCluster means:\n      length       left      right     bottom         top       diag\n1  0.1062264  0.6993965  0.8352473  0.1927865  1.18251937 -0.9316427\n2 -0.5683115  0.2617543  0.3254371  1.3197396  0.04670298 -0.8483286\n3 -0.2002681 -1.0290130 -0.9878119 -0.8397381 -0.71307204  0.9434354\n4  1.1475776  0.6848546  0.2855308 -0.5788787 -0.40538184  0.7764051\n\nClustering vector:\n  [1] 4 3 3 3 3 4 3 3 3 4 4 3 4 3 3 3 3 3 3 3 3 4 4 4 3 4 4 4 4 3 4 3 3 4 4 4 4 3 4 3 3 3\n [43] 3 4 3 3 3 3 3 3 3 4 3 4 3 3 4 3 4 3 3 3 3 3 3 4 3 3 3 1 3 3 3 3 3 3 3 3 4 3 3 3 3 4\n [85] 4 3 3 3 4 3 3 4 3 3 3 4 4 3 3 3 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 1 2 2 2 1 1 2 1 1 2 1\n[127] 1 1 1 1 2 2 1 1 2 2 2 1 2 2 1 2 2 1 2 2 2 1 2 1 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 1 4 1\n[169] 1 2 1 2 2 2 2 2 2 1 1 1 2 1 1 1 2 2 1 2 1 2 1 1 2 1 2 1 1 1 1 2\n\nWithin cluster sum of squares by cluster:\n[1] 137.68573  95.51948 166.12573  92.37757\n (between_SS / total_SS =  58.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nLooks like it. But I should check:\n\nswiss.7a$centers\n\nNULL\n\n\nAh. swiss.7a is actually a list, as evidenced by the [[1]] at the top of the output, so I get things from it thus:\n\nswiss.7a[[1]]$centers\n\n      length       left      right     bottom         top       diag\n1  0.1062264  0.6993965  0.8352473  0.1927865  1.18251937 -0.9316427\n2 -0.5683115  0.2617543  0.3254371  1.3197396  0.04670298 -0.8483286\n3 -0.2002681 -1.0290130 -0.9878119 -0.8397381 -0.71307204  0.9434354\n4  1.1475776  0.6848546  0.2855308 -0.5788787 -0.40538184  0.7764051\n\n\nThis would be because it came from a list-column; using pull removed the data-frameness from swiss.7a, but not its listness.\n\\(\\blacksquare\\)\n\nMake a table showing cluster membership against actual status (counterfeit or genuine). Are the counterfeit bills mostly in certain clusters?\n\nSolution\ntable. swiss.7$cluster shows the actual cluster numbers:\n\ntable(swiss$status, swiss.7$cluster)\n\n             \n               1  2  3  4\n  counterfeit 50  1  0 49\n  genuine      0 31 68  1\n\n\nOr, if you prefer,\n\ntibble(obs = swiss$status, pred = swiss.7$cluster) %&gt;%\n  count(obs, pred)\n\n\n\n  \n\n\n\nor even\n\ntibble(obs = swiss$status, pred = swiss.7$cluster) %&gt;%\n  count(obs, pred) %&gt;%\n  pivot_wider(names_from = obs, values_from = n, values_fill = 0)\n\n\n\n  \n\n\n\nIn my case (yours might be different), 99 of the 100 counterfeit bills are in clusters 1 and 4, and 99 of the 100 genuine bills are in clusters 2 and 3.11 So the clustering has done a very good job of distinguishing the genuine bills from the counterfeit ones. (You could imagine, if you were an employee at the bank, saying that a bill in cluster 1 or 4 is counterfeit, and being right 99% of the time.) This is kind of a by-product of the clustering, though: we weren’t trying to distinguish counterfeit bills (that would have been the discriminant analysis that we did before); we were just trying to divide them into groups of different ones, and part of what made them different was that some of them were genuine bills and some of them were counterfeit.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "kmeans-cluster.html#grouping-similar-cars-1",
    "href": "kmeans-cluster.html#grouping-similar-cars-1",
    "title": "34  K-means cluster analysis",
    "section": "34.9 Grouping similar cars",
    "text": "34.9 Grouping similar cars\nThe file link contains information on seven variables for 32 different cars. The variables are:\n\nCarname: name of the car (duh!)\nmpg: gas consumption in miles per US gallon (higher means the car uses less gas)\ndisp: engine displacement (total volume of cylinders in engine): higher is more powerful\nhp: engine horsepower (higher means a more powerful engine)\ndrat: rear axle ratio (higher means more powerful but worse gas mileage)\nwt: car weight in US tons\nqsec: time needed for the car to cover a quarter mile (lower means faster)\n\n\nRead in the data and display its structure. Do you have the right number of cars and variables?\n\nSolution\n\n# my_url &lt;- \"https://raw.githubusercontent.com/nxskok/datafiles/master/car-cluster.csv\"\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/car-cluster.csv\"\ncars &lt;- read_csv(my_url)\n\nRows: 32 Columns: 7\n── Column specification ──────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Carname\ndbl (6): mpg, disp, hp, drat, wt, qsec\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncars\n\n\n\n  \n\n\n\nCheck, both on number of cars and number of variables.\n\\(\\blacksquare\\)\n\nThe variables are all measured on different scales. Use scale to produce a matrix of standardized (\\(z\\)-score) values for the columns of your data that are numbers.\n\nSolution\nAll but the first column needs to be scaled, so:\n\ncars %&gt;% select(-Carname) %&gt;% scale() -&gt; cars.s\n\nThis is a matrix, as we’ve seen before.\nAnother way is like this:\n\ncars %&gt;% select(where(is.numeric)) %&gt;% scale() -&gt; h\n\nI would prefer to have a look at my result, so that I can see that it has sane things in it:\n\nhead(cars.s)\n\n            mpg        disp         hp       drat           wt       qsec\n[1,]  0.1508848 -0.57061982 -0.5350928  0.5675137 -0.610399567 -0.7771651\n[2,]  0.1508848 -0.57061982 -0.5350928  0.5675137 -0.349785269 -0.4637808\n[3,]  0.4495434 -0.99018209 -0.7830405  0.4739996 -0.917004624  0.4260068\n[4,]  0.2172534  0.22009369 -0.5350928 -0.9661175 -0.002299538  0.8904872\n[5,] -0.2307345  1.04308123  0.4129422 -0.8351978  0.227654255 -0.4637808\n[6,] -0.3302874 -0.04616698 -0.6080186 -1.5646078  0.248094592  1.3269868\n\n\nor,\n\nhead(h)\n\n            mpg        disp         hp       drat           wt       qsec\n[1,]  0.1508848 -0.57061982 -0.5350928  0.5675137 -0.610399567 -0.7771651\n[2,]  0.1508848 -0.57061982 -0.5350928  0.5675137 -0.349785269 -0.4637808\n[3,]  0.4495434 -0.99018209 -0.7830405  0.4739996 -0.917004624  0.4260068\n[4,]  0.2172534  0.22009369 -0.5350928 -0.9661175 -0.002299538  0.8904872\n[5,] -0.2307345  1.04308123  0.4129422 -0.8351978  0.227654255 -0.4637808\n[6,] -0.3302874 -0.04616698 -0.6080186 -1.5646078  0.248094592  1.3269868\n\n\nThese look right. Or, perhaps better, this:\n\nsummary(cars.s)\n\n      mpg               disp               hp               drat        \n Min.   :-1.6079   Min.   :-1.2879   Min.   :-1.3810   Min.   :-1.5646  \n 1st Qu.:-0.7741   1st Qu.:-0.8867   1st Qu.:-0.7320   1st Qu.:-0.9661  \n Median :-0.1478   Median :-0.2777   Median :-0.3455   Median : 0.1841  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.4495   3rd Qu.: 0.7688   3rd Qu.: 0.4859   3rd Qu.: 0.6049  \n Max.   : 2.2913   Max.   : 1.9468   Max.   : 2.7466   Max.   : 2.4939  \n       wt               qsec         \n Min.   :-1.7418   Min.   :-1.87401  \n 1st Qu.:-0.6500   1st Qu.:-0.53513  \n Median : 0.1101   Median :-0.07765  \n Mean   : 0.0000   Mean   : 0.00000  \n 3rd Qu.: 0.4014   3rd Qu.: 0.58830  \n Max.   : 2.2553   Max.   : 2.82675  \n\n\nThe mean is exactly zero, for all variables, which is as it should be. Also, the standardized values look about as they should; even the extreme ones don’t go beyond \\(\\pm 3\\).\nThis doesn’t show the standard deviation of each variable, though, which should be exactly 1 (since that’s what “standardizing” means). To get that, this:\n\nas_tibble(cars.s) %&gt;%\n  summarize(across(everything(), \\(x) sd(x)))\n\n\n\n  \n\n\n\nThe idea here is “take the matrix cars.s, turn it into a data frame, and for each column, calculate the SD of it”.12\nAs you realize now, the same idea will get the mean of each column too:\n\nas_tibble(cars.s) %&gt;%\n  summarize(across(everything(), \\(x) mean(x)))\n\n\n\n  \n\n\n\nand we see that the means are all zero, to about 15 decimals, anyway.\n\\(\\blacksquare\\)\n\nRun a K-means cluster analysis for these data, obtaining 3 clusters, and display the results. Take whatever action you need to obtain the best (random) result from a number of runs.\n\nSolution\nThe hint at the end says “use nstart”, so something like this:\n\nset.seed(457299)\ncars.1 &lt;- kmeans(cars.s, 3, nstart = 20)\ncars.1\n\nK-means clustering with 3 clusters of sizes 12, 6, 14\n\nCluster means:\n         mpg       disp         hp       drat         wt       qsec\n1  0.1384407 -0.5707543 -0.5448163  0.1887816 -0.2454544  0.5491221\n2  1.6552394 -1.1624447 -1.0382807  1.2252295 -1.3738462  0.3075550\n3 -0.8280518  0.9874085  0.9119628 -0.6869112  0.7991807 -0.6024854\n\nClustering vector:\n [1] 1 1 1 1 3 1 3 1 1 1 1 3 3 3 3 3 3 2 2 2 1 3 3 3 3 2 2 2 3 1 3 1\n\nWithin cluster sum of squares by cluster:\n[1] 24.95528  7.76019 33.37849\n (between_SS / total_SS =  64.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nYou don’t need the set.seed, but if you run again, you’ll get a different answer. With the nstart, you’ll probably get the same clustering every time you run, but the clusters might have different numbers, so that when you talk about “cluster 1” and then re-run, what you were talking about might have moved to cluster 3, say.\nIf you are using R Markdown, for this reason, having a set.seed before anything involving random number generation is a smart move.13\n\\(\\blacksquare\\)\n\nDisplay the car names together with which cluster they are in. If you display them all at once, sort by cluster so that it’s easier to see which clusters contain which cars. (You may have to make a data frame first.)\n\nSolution\nAs below. The car names are in the Carname column of the original cars data frame, and the cluster numbers are in the cluster part of the output from kmeans. You’ll need to take some action to display everything (there are only 32 cars, so it’s perfectly all right to display all of them):\n\ntibble(car = cars$Carname, cluster = cars.1$cluster) %&gt;%\n  arrange(cluster) \n\n\n\n  \n\n\n\nOr start from the original data frame as read in from the file and grab only what you want:\n\ncars %&gt;%\n  select(Carname) %&gt;%\n  mutate(cluster = cars.1$cluster) %&gt;%\n  arrange(cluster) \n\n\n\n  \n\n\n\nThis time we want to keep the car names and throw away everything else.\n\\(\\blacksquare\\)\n\nI have no idea whether 3 is a sensible number of clusters. To find out, we will draw a scree plot (in a moment). Write a function that accepts the number of clusters and the (scaled) data, and returns the total within-cluster sum of squares.\n\nSolution\nI failed to guess (in conversation with students, back when this was a question to be handed in) what you might do. There are two equally good ways to tackle this part and the next:\n\nWrite a function to calculate the total within-cluster sum of squares (in this part) and somehow use it in the next part, eg. via rowwise, to get the total within-cluster sum of squares for each number of clusters.\nSkip the function-writing part and go directly to a loop in the next part.\n\nI’m good with either approach: as long as you obtain, somehow, the total within-cluster sum of squares for each number of clusters, and use them for making a scree plot, I think you should get the points for this part and the next. I’ll talk about the function way here and the loop way in the next part.\nThe function way is just like the one in the previous question:\n\nwss &lt;- function(howmany, data, nstart = 20) {\n  kmeans(data, howmany, nstart = 20)$tot.withinss\n}\n\nThe data and number of clusters can have any names, as long as you use whatever input names you chose within the function.\nI should probably check that this works, at least on 3 clusters. Before we had\n\ncars.1$tot.withinss\n\n[1] 66.09396\n\n\nand the function gives\n\nwss(3, cars.s)\n\n[1] 66.09396\n\n\nCheck. I need to make sure that I used my scaled cars data, but I don’t need to say anything about nstart, since that defaults to the perfectly suitable 20.\n\\(\\blacksquare\\)\n\nCalculate the total within-group sum of squares for each number of clusters from 2 to 10, using the function you just wrote.\n\nSolution\nThe loop way. I like to define my possible numbers of clusters into a vector first:\n\nw &lt;- numeric(0)\nnclus &lt;- 2:10\nfor (i in nclus) {\n  w[i] &lt;- wss(i, cars.s)\n}\nw\n\n [1]       NA 87.29448 66.09396 50.94273 38.22004 29.28816 24.23138 20.76061 17.97491\n[10] 15.19850\n\n\nNow that I look at this again, it occurs to me that there is no great need to write a function to do this: you can just do what you need to do within the loop, like this:\n\nw &lt;- numeric(0)\nnclus &lt;- 2:10\nfor (i in nclus) {\n  w[i] &lt;- kmeans(cars.s, i, nstart = 20)$tot.withinss\n}\nw\n\n [1]       NA 87.29448 66.09396 50.94273 38.22004 29.28816 24.23138 20.76061 17.33653\n[10] 15.19850\n\n\nYou ought to have an nstart somewhere to make sure that kmeans gets run a number of times and the best result taken.\nIf you initialize your w with numeric(10) rather than numeric(0), it apparently gets filled with zeroes rather than NA values. This means, later, when you come to plot your w-values, the within-cluster total sum of squares will appear to be zero, a legitimate value, for one cluster, even though it is definitely not. (Or, I suppose, you could start your loop at 1 cluster, and get a legitimate, though very big, value for it.)\nIn both of the above cases, the curly brackets are optional because there is only one line within the loop.14\nWhat is actually happening here is an implicit loop-within-a-loop. There is a loop over i that goes over all clusters, and then there is a loop over another variable, j say, that loops over the nstart runs that we’re doing for i clusters, where we find the tot.withinss for i clusters on the jth run, and if it’s the best one so far for i clusters, we save it. Or, at least, kmeans saves it.\nOr, using rowwise, which I like better:\n\ntibble(clusters = 2:10) %&gt;%\n  rowwise() %&gt;% \n  mutate(ss = wss(clusters, cars.s)) -&gt; wwx\nwwx\n\n\n\n  \n\n\n\nNote that w starts at 1, but wwx starts at 2. For this way, you have to define a function first to calculate the total within-cluster sum of squares for a given number of clusters. If you must, you can do the calculation in the mutate rather than writing a function, but I find that very confusing to read, so I’d rather define the function first, and then use it later. (The principle is to keep the mutate simple, and put the complexity in the function where it belongs.)\nAs I say, if you must:\n\ntibble(clusters = 2:10) %&gt;%\n  rowwise() %&gt;% \n  mutate(wss = kmeans(cars.s, \n                      clusters, \n                      nstart = 20)$tot.withinss) -&gt; wwx\nwwx\n\n\n\n  \n\n\n\nThe upshot of all of this is that if you had obtained a total within-cluster sum of squares for each number of clusters, somehow, and it’s correct, you should have gotten some credit15 for this part and the last part. This is a common principle of mine, and works on exams as well as assignments; it goes back to the idea of “get the job done first” that you first saw in C32.\n\\(\\blacksquare\\)\n\nMake a scree plot, using the total within-cluster sums of squares values that you calculated in the previous part.\n\nSolution\nIf you did this the loop way, it’s tempting to leap into this:\n\nd &lt;- data.frame(clusters = nclus, wss = w)\n\nError in data.frame(clusters = nclus, wss = w): arguments imply differing number of rows: 9, 10\n\n\nand then wonder why it doesn’t work. The problem is that w has 10 things in it, including an NA at the front (as a placeholder for 1 cluster):\n\nw\n\n [1]       NA 87.29448 66.09396 50.94273 38.22004 29.28816 24.23138 20.76061 17.33653\n[10] 15.19850\n\nnclus\n\n[1]  2  3  4  5  6  7  8  9 10\n\n\nwhile nclus only has 9. So do something like this instead:\n\ntibble(clusters = 1:10, wss = w) %&gt;%\n  ggplot(aes(x = clusters, y = wss)) + geom_point() + geom_line()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nThis gives a warning because there is no 1-cluster w-value, but the point is properly omitted from the plot, so the plot you get is fine.\nOr plot the output from rowwise, which is easier since it’s already a data frame:\n\nwwx %&gt;% ggplot(aes(x = clusters, y = wss)) + geom_point() + geom_line()\n\n\n\n\n\\(\\blacksquare\\)\n\nWhat is a suitable number of clusters for K-means, based on your scree plot?\n\nSolution\nThat seems to me to have a clear elbow at 6, suggesting six clusters.16 Look for where the plot “turns the corner” from going down to going out, or the point that is the “last one on the mountain and the first one on the scree”. This mountainside goes down to 6, and from there it seems to turn the corner and go out after that.\nThis is a judgement call, but this particular one is about as clear as you can expect to see.\nI wanted a picture of some real scree. This one shows what I mean:\n\nNote the rock face and the loose rock below, which is the scree. Imagine looking at the rock face and scree from side-on. This is in north Wales, the other end of Wales from Llanederyn/Llanedeyrn and Caldicot.\nThe above photo is from link.\n\\(\\blacksquare\\)\n\nRun a K-means analysis using the number of clusters suggested by your scree plot, and list the car names together with the clusters they belong to, sorted by cluster.\n\nSolution\nThis is the same idea as above. The arrange idea from above seems to be the cleanest way to arrange the output: The K-means analysis is thus:\n\ncars.2 &lt;- kmeans(cars.s, 6, nstart = 20)\n\nor use whatever number of clusters you thought was good from your scree plot.\nThen display them:\n\ncars %&gt;%\n  select(Carname) %&gt;%\n  mutate(cluster = cars.2$cluster) %&gt;%\n  arrange(cluster) \n\n\n\n  \n\n\n\nThe logic to this is the same as above. I don’t have a good feeling for what the cars within a cluster have in common, by eyeballing the names, except for possibly a couple of things: my cluster 1 seems to be mostly family cars, and my cluster 3 appears to contain “boats” (large cars that consume a lot of gas). Your clusters ought to be about the same in terms of membership, but might be numbered differently.\nExtra: to understand these clusters further, we can use them as input to a discriminant analysis. There isn’t any real need to run a MANOVA first, since we kind of know that these groups will be different (that’s why we ran a cluster analysis).\nSo, first we’ll make a data frame with the whole original data set plus the clusters that came out of the K-means. We are adding the clusters to cars, so it makes sense to use the same ideas as I used above (without the arrange, that being only for looking at, and without the select, since this time I want all the variables that were in cars):\n\ncarsx &lt;- cars %&gt;% mutate(cluster = cars.2$cluster)\n\nNow we fire away:\n\ncarsx.1 &lt;- lda(cluster ~ mpg + disp + hp + drat + wt + qsec, data = carsx)\ncarsx.1\n\nCall:\nlda(cluster ~ mpg + disp + hp + drat + wt + qsec, data = carsx)\n\nPrior probabilities of groups:\n      1       2       3       4       5       6 \n0.18750 0.21875 0.12500 0.15625 0.21875 0.09375 \n\nGroup means:\n       mpg     disp       hp     drat       wt     qsec\n1 30.06667  86.6500  75.5000 4.251667 1.873000 18.39833\n2 20.41429 147.0286 120.4286 3.888571 2.892143 17.62714\n3 14.60000 340.5000 272.2500 3.675000 3.537500 15.08750\n4 21.64000 178.1200  93.8000 3.430000 3.096000 20.51400\n5 16.78571 315.6286 170.0000 3.050000 3.688571 17.32000\n6 11.83333 457.3333 216.6667 3.053333 5.339667 17.74000\n\nCoefficients of linear discriminants:\n             LD1           LD2         LD3          LD4          LD5\nmpg  -0.19737944 -0.0155769096  0.27978549 -0.353766928  0.035582922\ndisp  0.01950855 -0.0001094137  0.02090998 -0.001034719  0.001680201\nhp    0.02804348  0.0251253160  0.01727355  0.015955928 -0.017220548\ndrat  0.94348424  1.8928372037 -0.56645563 -1.264185553 -2.015644662\nwt    0.39068831 -1.3973097325 -1.84808828 -2.963377419 -0.300573153\nqsec  0.33992344 -0.3010056176  0.66690927  0.755053279 -0.738889640\n\nProportion of trace:\n   LD1    LD2    LD3    LD4    LD5 \n0.7977 0.1234 0.0368 0.0299 0.0122 \n\n\nAt the bottom (in trace) you see that LD1 is clearly the most important thing for splitting into groups, LD2 might be slightly relevant, and the other LDs are basically meaningless. So a plot of the first two LDs should tell the story.\nBefore we get to that, however, we can take a look at the Coefficients of Linear Discriminants, for LD1 and LD2 anyway. LD1 depends principally on drat, wt and qsec (positively) and maybe negatively on mpg. That means LD1 will be large if the car is powerful, heavy, slow (since a larger qsec means the car takes longer to go a quarter mile) and consumes a lot of gas. I think I can summarize this as “powerful”.\nLD2 also depends on drat and wt, but note the signs: it is contrasting drat (displacement ratio) with wt (weight), so that a car with a large displacement ratio relative to its weight would be large (plus) on LD2. That is, LD2 is “powerful for its weight”.\nAll right, now for a plot, with the points colour-coded by cluster. There are two ways to do this; the easy one is ggbiplot. The only weirdness here is that the clusters are numbered, so you have to turn that into a factor first (unless you like shades of blue). I didn’t load the package first, so I call it here with the package name and the two colons:\n\nggbiplot::ggbiplot(carsx.1, groups = factor(carsx$cluster))\n\n\n\n\nOr you can do the predictions, then plot LD1 against LD2, coloured by cluster:\n\np &lt;- predict(carsx.1)\ndata.frame(p$x, cluster = factor(carsx$cluster)) %&gt;%\n  ggplot(aes(x = LD1, y = LD2, colour = cluster)) + geom_point() +\n  coord_fixed()\n\n\n\n\nThe pattern of coloured points is the same. The advantage to the biplot is that you see which original variables contribute to the LD scores and thus distinguish the clusters; on the second plot, you have to figure out for yourself which original variables contribute, and how, to the LD scores.\nYou should include coord_fixed to make the axis scales the same, since allowing them to be different will distort the picture (the picture should come out square). You do the same thing in multidimensional scaling.\nAs you see, LD1 is doing the best job of separating the clusters, but LD2 is also doing something: separating clusters 1 and 5, and also 2 and 4 (though 4 is a bit bigger than 2 on LD1 also).\nI suggested above that LD1 seems to be “powerful” (on the right) vs. not (on the left). The displacement ratio is a measure of the power of an engine, so a car that is large on LD2 is powerful for its weight.\nLet’s find the clusters I mentioned before. Cluster 3 was the “boats”: big engines and heavy cars, but not fast. So they should be large LD1 and small (negative) LD2. Cluster 1 I called “family cars”: they are not powerful, but have moderate-to-good power for their weight.\nWith that in mind, we can have a crack at the other clusters. Cluster 2 is neither powerful nor powerful-for-weight (I don’t know these cars, so can’t comment further) while cluster 5 is powerful and also powerful for their weight, so these might be sports cars. Clusters 6 and 4 are less and more powerful, both averagely powerful for their size."
  },
  {
    "objectID": "kmeans-cluster.html#rating-beer-1",
    "href": "kmeans-cluster.html#rating-beer-1",
    "title": "34  K-means cluster analysis",
    "section": "34.10 Rating beer",
    "text": "34.10 Rating beer\nThirty-two students each rated 10 brands of beer:\n\nAnchor Steam\nBass\nBeck’s\nCorona\nGordon Biersch\nGuinness\nHeineken\nPete’s Wicked Ale\nSam Adams\nSierra Nevada\n\nThe ratings are on a scale of 1 to 9, with a higher rating being better. The data are in link. I abbreviated the beer names for the data file. I hope you can figure out which is which.\n\nRead in the data, and look at the first few rows.\n\nSolution\nData values are aligned in columns, so read_table:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/beer.txt\"\nbeer &lt;- read_table(my_url)\n\n\n── Column specification ──────────────────────────────────────────────────────────────────\ncols(\n  student = col_character(),\n  AnchorS = col_double(),\n  Bass = col_double(),\n  Becks = col_double(),\n  Corona = col_double(),\n  GordonB = col_double(),\n  Guinness = col_double(),\n  Heineken = col_double(),\n  PetesW = col_double(),\n  SamAdams = col_double(),\n  SierraN = col_double()\n)\n\nbeer\n\n\n\n  \n\n\n\n32 rows (students), 11 columns (10 beers, plus a column of student IDs). All seems to be kosher. If beer can be kosher.17\n\\(\\blacksquare\\)\n\nThe researcher who collected the data wants to see which beers are rated similarly to which other beers. Try to create a distance matrix from these data and explain why it didn’t do what you wanted. (Remember to get rid of the student column first.)\n\nSolution\nThe obvious thing is to feed these ratings into dist (we are creating distances rather than re-formatting things that are already distances). We need to skip the first column, since those are student identifiers:\n\nbeer %&gt;%\n  select(-student) %&gt;%\n  dist() -&gt; d\nglimpse(d)\n\n 'dist' num [1:496] 9.8 8.49 6.56 8.89 8.19 ...\n - attr(*, \"Size\")= int 32\n - attr(*, \"Diag\")= logi FALSE\n - attr(*, \"Upper\")= logi FALSE\n - attr(*, \"method\")= chr \"euclidean\"\n - attr(*, \"call\")= language dist(x = .)\n\n\nThe 496 distances are:\n\n32 * 31 / 2\n\n[1] 496\n\n\nthe number of ways of choosing 2 objects out of 32, when order does not matter. Feel free to be offended by my choice of the letter d to denote both data frames (that I didn’t want to give a better name to) and dissimilarities in dist objects.\nYou can look at the whole thing if you like, though it is rather large. A dist object is stored internally as a long vector (here of 496 values); it’s displayed as a nice triangle. The clue here is the thing called Size, which indicates that we have a \\(32\\times 32\\) matrix of distances between the 32 students, so that if we were to go on and do a cluster analysis based on this d, we’d get a clustering of the students rather than of the beers, as we want. (If you just print out d, you’ll see that is of distances between 32 (unlabelled) objects, which by inference must be the 32 students.)\nIt might be interesting to do a cluster analysis of the 32 students (it would tell you which of the students have similar taste in beer), but that’s not what we have in mind here.\n\\(\\blacksquare\\)\n\nThe R function t() transposes a matrix: that is, it interchanges rows and columns. Feed the transpose of your read-in beer ratings into dist. Does this now give distances between beers?\n\nSolution\nAgain, omit the first column. The pipeline code looks a bit weird:\n\nbeer %&gt;%\n  select(-student) %&gt;%\n  t() %&gt;%\n  dist() -&gt; d\n\nso you should feel free to do it in a couple of steps. This way shows that you can also refer to columns by number:\n\nbeer %&gt;% select(-1) -&gt; beer2\nd &lt;- dist(t(beer2))\n\nEither way gets you to the same place:\n\nd\n\n          AnchorS     Bass    Becks   Corona  GordonB Guinness Heineken   PetesW SamAdams\nBass     15.19868                                                                        \nBecks    16.09348 13.63818                                                               \nCorona   20.02498 17.83255 17.54993                                                      \nGordonB  13.96424 11.57584 14.42221 13.34166                                             \nGuinness 14.93318 13.49074 16.85230 20.59126 14.76482                                    \nHeineken 20.66398 15.09967 13.78405 14.89966 14.07125 18.54724                           \nPetesW   11.78983 14.00000 16.37071 17.72005 11.57584 14.28286 19.49359                  \nSamAdams 14.62874 11.61895 14.73092 14.93318 10.90871 15.90597 14.52584 14.45683         \nSierraN  12.60952 15.09967 17.94436 16.97056 11.74734 13.34166 19.07878 13.41641 12.12436\n\n\nThere are 10 beers with these names, so this is good.\n\\(\\blacksquare\\)\n\nTry to explain briefly why I used as.dist in the class example (the languages one) but dist here. (Think about the form of the input to each function.)\n\nSolution\nas.dist is used if you already have dissimilarities (and you just want to format them right), but dist is used if you have data on variables and you want to calculate dissimilarities.\n\\(\\blacksquare\\)\n\n* Obtain a clustering of the beers, using Ward’s method. Show the dendrogram.\n\nSolution\nThis:\n\nbeer.1 &lt;- hclust(d, method = \"ward.D\")\nplot(beer.1)\n\n\n\n\n\\(\\blacksquare\\)\n\nWhat seems to be a sensible number of clusters? Which beers are in which cluster?\n\nSolution\nThis is a judgement call. Almost anything sensible is reasonable. I personally think that two clusters is good, beers Anchor Steam, Pete’s Wicked Ale, Guinness and Sierra Nevada in the first, and Bass, Gordon Biersch, Sam Adams, Corona, Beck’s, and Heineken in the second. You could make a case for three clusters, splitting off Corona, Beck’s and Heineken into their own cluster, or even about 5 clusters as Anchor Steam, Pete’s Wicked Ale; Guinness, Sierra Nevada; Bass, Gordon Biersch, Sam Adams; Corona; Beck’s, Heineken.\nThe idea is to have a number of clusters sensibly smaller than the 10 observations, so that you are getting some actual insight. Having 8 clusters for 10 beers wouldn’t be very informative! (This is where you use your own knowledge about beer to help you rationalize your choice of number of clusters.)\nExtra: as to why the clusters split up like this, I think the four beers on the left of my dendrogram are “dark” and the six on the right are “light” (in colour), and I would expect the students to tend to like all the beers of one type and not so much all the beers of the other type.\nYou knew I would have to investigate this, didn’t you? Let’s aim for a scatterplot of all the ratings for the dark beers, against the ones for the light beers.\nStart with the data frame read in from the file:\n\nbeer\n\n\n\n  \n\n\n\nThe aim is to find the average rating for a dark beer and a light beer for each student, and then plot them against each other. Does a student who likes dark beer tend not to like light beer, and vice versa?\nLet’s think about what to do first.\nWe need to: pivot_longer all the rating columns into one, labelled by name of beer. Then create a variable that is dark if we’re looking at one of the dark beers and light otherwise. ifelse works like “if” in a spreadsheet: a logical thing that is either true or false, followed by a value if true and a value if false. There is a nice R command %in% which is TRUE if the thing in the first variable is to be found somewhere in the list of things given next (here, one of the apparently dark beers). (Another way to do this, which will appeal to you more if you like databases, is to create a second data frame with two columns, the first being the beer names, and the second being dark or light as appropriate for that beer. Then you use a “left join” to look up beer type from beer name.)\nNext, group by beer type within student. Giving two things to group_by does it this way: the second thing within (or “for each of”) the first.\nThen calculate the mean rating within each group. This gives one column of students, one column of beer types, and one column of rating means.\nThen we need to pivot_wider beer type into two columns so that we can make a scatterplot of the mean ratings for light and dark against each other.\nFinally, we make a scatterplot.\nYou’ll see the final version of this that worked, but rest assured that there were many intervening versions of this that didn’t!\nI urge you to examine the chain one line at a time and see what each line does. That was how I debugged it.\nOff we go:\n\nbeer %&gt;%\n  pivot_longer(-student, names_to=\"name\", values_to=\"rating\") %&gt;%\n  mutate(beer.type = ifelse(name %in%\n    c(\"AnchorS\", \"PetesW\", \"Guinness\", \"SierraN\"), \"dark\", \"light\")) %&gt;%\n  group_by(student, beer.type) %&gt;%\n  summarize(mean.rat = mean(rating)) %&gt;%\n  pivot_wider(names_from=beer.type, values_from=mean.rat) %&gt;%\n  ggplot(aes(x = dark, y = light)) + geom_point()\n\n\n\n\nAfter all that work, not really. There are some students who like light beer but not dark beer (top left), there is a sort of vague straggle down to the bottom right, where some students like dark beer but not light beer, but there are definitely students at the top right, who just like beer!\nThe only really empty part of this plot is the bottom left, which says that these students don’t hate both kinds of beer; they like either dark beer, or light beer, or both.\nThe reason a ggplot fits into this “workflow” is that the first thing you feed into ggplot is a data frame, the one created by the chain here. Because it’s in a pipeline, you don’t have the first thing on ggplot, so you can concentrate on the aes (“what to plot”) and then the “how to plot it”. Now back to your regularly-scheduled programming.\n\\(\\blacksquare\\)\n\nRe-draw your dendrogram with your clusters indicated.\n\nSolution\nrect.hclust, with your chosen number of clusters:\n\nplot(beer.1)\nrect.hclust(beer.1, 2)\n\n\n\n\nOr if you prefer 5 clusters, like this:\n\nplot(beer.1)\nrect.hclust(beer.1, 5)\n\n\n\n\nSame idea with any other number of clusters. If you follow through with your preferred number of clusters from the previous part, I’m good.\n\\(\\blacksquare\\)\n\nObtain a K-means clustering with 2 clusters.18 Note that you will need to use the (transposed) original data, not the distances. Use a suitably large value of nstart. (The data are ratings all on the same scale, so there is no need for scale here. In case you were wondering.)\n\nSolution\nI used 20 for nstart. This is the pipe way:\n\nbeer.2 &lt;- beer %&gt;%\n  select(-1) %&gt;%\n  t() %&gt;%\n  kmeans(2, nstart = 20)\n\nNot everyone (probably) will get the same answer, because of the random nature of the procedure, but the above code should be good whatever output it produces.\n\\(\\blacksquare\\)\n\nHow many beers are in each cluster?\n\nSolution\nOn mine:\n\nbeer.2$size\n\n[1] 6 4\n\n\nYou might get the same numbers the other way around.\n\\(\\blacksquare\\)\n\nWhich beers are in each cluster? You can do this simply by obtaining the cluster memberships and using sort as in the last question, or you can do it as I did in class by obtaining the names of the things to be clustered and picking out the ones of them that are in cluster 1, 2, 3, .)\n\nSolution\nThe cluster numbers of each beer are these:\n\nbeer.2$cluster\n\n AnchorS     Bass    Becks   Corona  GordonB Guinness Heineken   PetesW SamAdams  SierraN \n       2        1        1        1        1        2        1        2        1        2 \n\n\nThis is what is known in the business as a “named vector”: it has values (the cluster numbers) and each value has a name attached to it (the name of a beer).\nNamed vectors are handily turned into a data frame with enframe:\n\nx &lt;- enframe(beer.2$cluster)\nx\n\n\n\n  \n\n\n\nOr, to go back the other way, deframe:\n\ndeframe(x)\n\n AnchorS     Bass    Becks   Corona  GordonB Guinness Heineken   PetesW SamAdams  SierraN \n       2        1        1        1        1        2        1        2        1        2 \n\n\nor, give the columns better names and arrange them by cluster:\n\nenframe(beer.2$cluster, name = \"beer\", value = \"cluster\") %&gt;%\n  arrange(cluster)\n\n\n\n  \n\n\n\nThese happen to be the same clusters as in my 2-cluster solution using Ward’s method.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "kmeans-cluster.html#footnotes",
    "href": "kmeans-cluster.html#footnotes",
    "title": "34  K-means cluster analysis",
    "section": "",
    "text": "Women compete in a similar competition called the heptathlon with seven events.↩︎\nI grew up in the UK, and when I saw that in an exam, it was code for “the way you’d think is obvious but long, and the otherwise-way is clever but short”. I think this is one of those.↩︎\nIf you haven’t gotten to K-means clustering yet, leave this and save it for later.↩︎\nWomen compete in a similar competition called the heptathlon with seven events.↩︎\nI grew up in the UK, and when I saw that in an exam, it was code for “the way you’d think is obvious but long, and the otherwise-way is clever but short”. I think this is one of those.↩︎\nI have to sneak a Seinfeld quote in there somewhere.↩︎\nLike Buddhism. I keep feeling that R should have something called the Eight Noble Truths or similar. See the Extra at the end of this part.↩︎\nI wrote that a few years ago, and you may be pleased to learn that I have indeed completely forgotten about apply, sapply, lapply and all the others. I remember struggling through them when I first learned R, but you are in the happy position of never having to worry about them.↩︎\nWe are no longer in the tidyverse, so you no longer have the option of using British or American spelling.↩︎\nR vectors start from 1, unlike C arrays or Python lists, which start from 0.↩︎\nThis is again where set.seed is valuable: write this text once and it never needs to change.↩︎\nThe scale function can take a data frame, as here, but always produces a matrix. That’s why we had to turn it back into a data frame.↩︎\nI forgot this, and then realized that I would have to rewrite a whole paragraph. In case you think I remember everything the first time.↩︎\nI am accustomed to using the curly brackets all the time, partly because my single-line loops have a habit of expanding to more than one line as I embellish what they do, and partly because I’m used to the programming language Perl where the curly brackets are obligatory even with only one line. Curly brackets in Perl serve the same purpose as indentation serves in Python: figuring out what is inside a loop or an if and what is outside.↩︎\nWhen this was a question to hand in, which it is not any more.↩︎\nWe do something similar on scree plots for principal components later, but then, for reasons that will become clear then, we take elbow minus 1.↩︎\nI investigated. It can; in fact, I found a long list of kosher beers that included Anchor Steam.↩︎\nIf you haven’t gotten to K-means clustering yet, leave this and save it for later.↩︎"
  },
  {
    "objectID": "maps.html#making-a-map-of-wisconsin",
    "href": "maps.html#making-a-map-of-wisconsin",
    "title": "35  Drawing maps with Leaflet",
    "section": "35.1 Making a map of Wisconsin",
    "text": "35.1 Making a map of Wisconsin\nThe file link contains the road distances (in miles) between 12 cities in Wisconsin and neighbouring states. We are going to try to draw a map of the area using Leaflet.\n\nRead in the data, displaying the data that you read in.\nMake a dataframe containing the names of the locations (get rid of the columns containing distances), and add a column of the abbreviations of the states they are in. All of them are in Wisconsin (WI), except for the last three: Dubuque is in Iowa (IA), St. Paul is in Minnesota (MN) and Chicago is in Illinois (IL).\nCreate a new column in which the abbreviation for the state is glued on to the end of each location, separated by a space.\nLook up the latitudes and longitudes of these twelve places.\nObtain a Leaflet map of the area containing these twelve cities."
  },
  {
    "objectID": "maps.html#the-cross-city-line",
    "href": "maps.html#the-cross-city-line",
    "title": "35  Drawing maps with Leaflet",
    "section": "35.2 The Cross-City Line",
    "text": "35.2 The Cross-City Line\nWhen I went to university (in Birmingham, England, a long time ago), I was very excited because I would be travelling to campus by train. My journey was on the Cross-City Line, a metro-type service with lots of stops short distances apart, but run in those days by diesel trains (the electrification came later).\nA list of the stations on the line is in http://ritsokiguess.site/datafiles/cross-city.csv. There is one column in the data file, called station. We are going to draw a map of these.\n\nRead in and display (some of) the station names.\nIn preparation for geocoding, create a second column in the dataframe that consists of the station names with “station UK” on the end. (This is to improve the chances of the geocoder finding the actual railway station.)\nLook up the longitudes and latitudes of all the stations, organizing your dataframe so that they are visible.\nMake a Leaflet map of the stations. Use circle markers or the “pin” markers as you prefer.\nZoom in to see whether the geocoding did indeed find each of the stations. Comment briefly on what you find."
  },
  {
    "objectID": "maps.html#the-brain-of-a-cat-revisited",
    "href": "maps.html#the-brain-of-a-cat-revisited",
    "title": "35  Drawing maps with Leaflet",
    "section": "35.3 The brain of a cat, revisited",
    "text": "35.3 The brain of a cat, revisited\nEarlier, we looked at an ethics study that had to do with a fictional brain of a fictional cat. I said there was actually a town called Catbrain. It’s in England, near Bristol, and seems to be home to a street of car dealerships.\n\nFind the latitude and longitude of “Catbrain UK” (though I don’t think there are any others).\nDraw a map of Catbrain using Leaflet.\nMake a dataframe containing some other British cities as well as Catbrain, and find their latitudes and longitudes.\nDraw a map containing the places you picked.\n\nMy solutions follow:"
  },
  {
    "objectID": "maps.html#making-a-map-of-wisconsin-1",
    "href": "maps.html#making-a-map-of-wisconsin-1",
    "title": "35  Drawing maps with Leaflet",
    "section": "35.4 Making a map of Wisconsin",
    "text": "35.4 Making a map of Wisconsin\nThe file link contains the road distances (in miles) between 12 cities in Wisconsin and neighbouring states. We are going to try to draw a map of the area using Leaflet.\n\nRead in the data, displaying the data that you read in.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/wisconsin.txt\"\nwisc &lt;- read_table(my_url)\n\nWarning: Missing column names filled in: 'X14' [14]\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  location = col_character(),\n  Appleton = col_double(),\n  Beloit = col_double(),\n  Fort.Atkinson = col_double(),\n  Madison = col_double(),\n  Marshfield = col_double(),\n  Milwaukee = col_double(),\n  Monroe = col_double(),\n  Superior = col_double(),\n  Wausau = col_double(),\n  Dubuque = col_double(),\n  St.Paul = col_double(),\n  Chicago = col_double(),\n  X14 = col_logical()\n)\n\nwisc\n\n\n\n  \n\n\n\nThe first time I did this, I had a blank line on the end of the data file, so I had a blank location and missing values for all the distances for it. I tidied that up before sharing the file with you, though.\n\\(\\blacksquare\\)\n\nMake a dataframe containing the names of the locations (get rid of the columns containing distances), and add a column of the abbreviations of the states they are in. All of them are in Wisconsin (WI), except for the last three: Dubuque is in Iowa (IA), St. Paul is in Minnesota (MN) and Chicago is in Illinois (IL).\n\nSolution\nThere seems to be a bit of base R attached to this, however you do it. I am going to create a dataframe pretending they are all in Wisconsin, and then fix it up afterwards:\n\nwisc %&gt;% \n  select(!where(is.numeric)) %&gt;% \n  mutate(state = \"WI\") -&gt; wisc\nwisc\n\n\n\n  \n\n\n\n(I checked that in this question I didn’t need the road distances for anything, so I saved it back into the original dataframe. Also, the select is unnecessarily fancy: I could have just selected the location column, but this one says “don’t select any of the columns that are numeric”.)\nNow we have to fix up the states of the last three places, which is where the base R seems to come in (but see the Extra):\n\nwisc$state[10] &lt;- \"IA\"\nwisc$state[11] &lt;- \"MN\"\nwisc$state[12] &lt;- \"IL\"\nwisc\n\n\n\n  \n\n\n\nThe states of the last three locations are now correct.\nExtra: I didn’t know about the following until literally just now, but there is a tidyverse way to do this as well (that may look familiar to those of you that know about SQL). Let’s start by pretending again that everything is in Wisconsin:\n\nwisc %&gt;% \n  mutate(state = \"WI\") -&gt; wisc2\nwisc2\n\n\n\n  \n\n\n\nand then change the ones that need changing. What you do is to make a little dataframe of the ones that need changing:\n\nchanges &lt;- tribble(\n  ~location, ~state,\n  \"Dubuque\", \"IA\",\n  \"St.Paul\", \"MN\",\n  \"Chicago\", \"IL\"\n)\nchanges\n\n\n\n  \n\n\n\nNote that the columns in here have exactly the same names as the ones in the original dataframe where everything was in Wisconsin.\nI did this by copy-pasting the city names whose states needed changing out of the display of my wisc2. You might think that a left join is what we need now, and it almost is. Note that I want to match the locations but not the states:\n\nwisc2 %&gt;% left_join(changes, by = \"location\")\n\n\n\n  \n\n\n\nand the story here is that if state.y has a value, use that, otherwise use the value in state.x. This can even be done: there is a function coalesce1 that will do exactly that:\n\nwisc2 %&gt;% left_join(changes, by = \"location\") %&gt;% \n  mutate(state=coalesce(state.y, state.x))\n\n\n\n  \n\n\n\nThe idea behind coalesce is that you give it a list of columns, and the first one of those that is not missing gives its value to the new column. So I feed it state.y first, and then state.x, and the new state contains the right things. (Can you explain what happens if you do it the other way around?)\nBut, there is a better way. Let’s go back to our all-Wisconsin dataframe:\n\nwisc2\n\n\n\n  \n\n\n\nand our dataframe of corrections to make:\n\nchanges\n\n\n\n  \n\n\n\nWe can make those changes in one step, thus:\n\nwisc2 %&gt;% \n  rows_update(changes) -&gt; wisc\n\nMatching, by = \"location\"\n\nwisc\n\n\n\n  \n\n\n\nThis works because the first column of changes, namely location, is the one that is looked up in wisc2. (rows_update has a by in the same way that left_join does if you want to change this.) So all three locations in changes are looked up in wisc2, and any that match have their state changed to the one in changes.\nIn database terms, the location is known as a “key” column. That means that each city appears once only in the column, and so the replacements in wisc are only happening once. To a statistician, location is an “identifier variable”, identifying the individuals in the dataset. Unless you are doing something like repeated measures, each individual will only give you one measurement, but even then, if you have wide format, the identifier variables will still only appear once.\nMagic. Now that I have learned about this, I will be using it a lot.\n\\(\\blacksquare\\)\n\nCreate a new column in which the abbreviation for the state is glued on to the end of each location, separated by a space.\n\nSolution\nA couple of ways: a literal gluing, using paste:\n\nwisc %&gt;% \n  mutate(lookup = paste(location, state))\n\n\n\n  \n\n\n\nor the same idea using str_c from stringr (part of the tidyverse), only this time you have to supply the space yourself:\n\nwisc %&gt;% \n  mutate(lookup = str_c(location, \" \", state))\n\n\n\n  \n\n\n\nor a way you might have forgotten, using unite (which is the inverse of separate):\n\nwisc %&gt;% \n  unite(lookup, c(location, state), sep = \" \") -&gt; wisc\nwisc\n\n\n\n  \n\n\n\nThis last one is my favourite, because it gets rid of the two constituent columns location and state that we don’t need any more. The syntax is the name of the new column, a vector of columns to unite together, and (optionally) what to separate the joined-together values with. The default for that is an underscore, but here we want a space because that’s what the geocoder (coming up) expects.\n\\(\\blacksquare\\)\n\nLook up the latitudes and longitudes of these twelve places.\n\nSolution\nThis is geocoding, with the disentangling afterwards that is (I hope) becoming familiar:\n\nwisc %&gt;% \n  rowwise() %&gt;% \n  mutate(ll = list(geocode_OSM(lookup))) %&gt;% \n  unnest_wider(ll) %&gt;% \n  unnest_wider(coords) -&gt; wisc\nwisc\n\n\n\n  \n\n\n\nYes, I forgot the rowwise as well the first time.\n\\(\\blacksquare\\)\n\nObtain a Leaflet map of the area containing these twelve cities.\n\nSolution\nThe usual:\n\nleaflet(data = wisc) %&gt;% \n  addTiles() %&gt;% \n  addCircleMarkers(lng = ~x, lat = ~y) -&gt; locs\nlocs\n\n\n\n\n\nThe nice thing about this map is that you can play with it: zoom it (using the plus/minus on the map or your mouse wheel), or move it around by clicking and dragging. To identify the cities: well, the big ones are obvious, and you can zoom in to identify the others. (You have to zoom in quite a long way to find Monroe, and the geocoder seems to have found its airport, which is not actually in the city.)\nI like identifying the cities with circles, but there are other possibilities, such as “icon markers” that look like Google map pins:\n\nleaflet(data = wisc) %&gt;% \n  addTiles() %&gt;% \n  addMarkers(lng = ~x, lat = ~y) -&gt; locs\nlocs\n\n\n\n\n\nYou can even attach text to the markers that appears when you click on them, but that’s farther than I would go here.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "maps.html#the-cross-city-line-1",
    "href": "maps.html#the-cross-city-line-1",
    "title": "35  Drawing maps with Leaflet",
    "section": "35.5 The Cross-City Line",
    "text": "35.5 The Cross-City Line\nWhen I went to university (in Birmingham, England, a long time ago), I was very excited because I would be travelling to campus by train. My journey was on the Cross-City Line, a metro-type service with lots of stops short distances apart, but run in those days by diesel trains (the electrification came later).\nA list of the stations on the line is in http://ritsokiguess.site/datafiles/cross-city.csv. There is one column in the data file, called station. We are going to draw a map of these.\n\nRead in and display (some of) the station names.\n\nSolution\nNothing terribly unexpected here:\n\nstations &lt;- read_csv(my_url)\n\nRows: 24 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): station\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstations\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nIn preparation for geocoding, create a second column in the dataframe that consists of the station names with “station UK” on the end. (This is to improve the chances of the geocoder finding the actual railway station.)\n\nSolution\nI wrote this back into the original dataframe. Create a new one if you prefer:\n\nstations %&gt;% \n  mutate(longname = str_c(station, \" station UK\")) -&gt; stations\nstations\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nLook up the longitudes and latitudes of all the stations, organizing your dataframe so that they are visible.\n\nSolution\nTwo steps: the first is to do the geocoding, and the second is to disentangle what comes back.\nFirst, then:\n\nstations %&gt;% \n  rowwise() %&gt;% \n  mutate(ll = list(geocode_OSM(longname))) -&gt; stations\n\nNo results found for \"King's Norton station UK\".\n\n\nNo results found for \"Butler's Lane station UK\".\n\nstations\n\n\n\n  \n\n\n\nThe longitudes and latitudes are hidden in the list-column that I called ll, so the second step is to get them out:\n\nstations %&gt;% unnest_wider(ll) %&gt;% \n  unnest_wider(coords) -&gt; stations\nstations\n\n\n\n  \n\n\n\nThe two unnest_widers are because the longitudes and latitudes are hidden inside a thing called coords which is itself nested within ll. Do the first unnest_wider, and see what you have. This will tell you that you need to do another one.\nThe values seem reasonable; this is the UK, most of which is slightly west of the Greenwich meridian, and the latitudes look sensible given that the UK is north of southern Ontario.\n\\(\\blacksquare\\)\n\nMake a Leaflet map of the stations. Use circle markers or the “pin” markers as you prefer.\n\nSolution\nI used the pin markers (with addMarkers), though addCircleMarkers is as good. The code for drawing the map is always the same; the work here is in setting up the geocoding:\n\nleaflet(data = stations) %&gt;% \n  addTiles() %&gt;% \n  addMarkers(lng = ~x, lat = ~y)\n\nWarning in validateCoords(lng, lat, funcName): Data contains 2 rows with either\nmissing or invalid lat/lon values and will be ignored\n\n\n\n\n\n\nYou might find that some of the markers are off a straight line through Birmingham. In these cases, the geocoder found a different place with (apparently) the same name, or a name that was close enough to similar. This seems to be different each time you run it, so you can try again.\nThis (mostly) seems to extend across the city of Birmingham, as it should. There are quite a lot of stations, so the pins overlap each other. Zoom in to see them in a bit more detail, or zoom out to orient yourself if your UK geography needs some work.\n\\(\\blacksquare\\)\n\nZoom in to see whether the geocoding did indeed find each of the stations. Comment briefly on what you find.\n\nSolution\nThe stations go south to north, so the most southerly one you find should be Redditch and the most northerly is Lichfield Trent Valley.\nIf you zoom in enough, you’ll see where the railway line goes (a grey line). The points seem to be mainly close to it. But if you zoom in a bit more, some of the pins are right on the railway, and some of them are a bit off, because the geocoder found the centre of the place (or something else named after the place) rather than its railway station. For example, when I did it, Gravelly Hill station was right where it should be, but Aston was not.2\nExtra: geocode_OSM uses a free geocoder called Nominatim. This has some options. The defaults are to return only the first “hit”, and to return just the coordinates and the “bounding box”. These can be changed. Let’s see what we can find for Aston:\n\ntibble(where = \"Aston UK\") %&gt;% \n  mutate(info = list(geocode_OSM(where, return.first.only = FALSE,\n                            details = TRUE))) -&gt; d\nd         \n\n\n\n  \n\n\n\nThere are now 10 things returned. Let’s unnest this and see what we have:\n\nd %&gt;% unnest(info) %&gt;% \n  unnest_wider(info)\n\n\n\n  \n\n\n\nThere are 10 locations it found matching “Aston UK”, and for each of those there is the information you see, a total of 12 columns’ worth in addition to the name of the place we are looking up. Perhaps the most interesting are the columns class and type near the end (keep scrolling right):\n\nd %&gt;% unnest(info) %&gt;% \n  unnest_wider(info) %&gt;% \n  select(where, class, type)\n\n\n\n  \n\n\n\nYou can see which one is the railway station.\nThis makes me think that with sufficient patience we could do this for all our places, and pick out the one that is the station:\n\nstations &lt;- read_csv(my_url)\n\nRows: 24 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): station\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstations %&gt;% \n  mutate(longname = str_c(station, \" UK\")) %&gt;% \n  rowwise() %&gt;% \n  mutate(ll = list(geocode_OSM(longname, \n                   return.first.only = FALSE,\n                   details = TRUE))) -&gt; stations2\n\n\nstations2\n\n\n\n  \n\n\n\nThe number in the ll column tells you how many things the geocoder found that match the input longname. One of each of these is, we hope, a railway station.\n\nstations2 %&gt;% unnest(ll) %&gt;% \n  unnest_wider(ll, names_sep = \"_\") %&gt;% \n  select(station, ll_coords, ll_class, ll_type) %&gt;% \n  filter(ll_class == \"railway\", ll_type == \"station\") %&gt;% \n  unnest_wider(ll_coords) -&gt; d\nd\n\n\n\n  \n\n\n\nIf you want to see how this works, run it one line at a time.\nWe almost got there. We’re missing University3 and Lichfield City stations, but it looks as if we got the rest:\n\nleaflet(data = d) %&gt;% \n  addTiles() %&gt;% \n  addMarkers(lng = ~x, lat = ~y)\n\n\n\n\n\nIf you zoom in, you’ll see that the ones we got are all the actual stations, and not the area from which the station takes its name.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "maps.html#the-brain-of-a-cat-revisited-1",
    "href": "maps.html#the-brain-of-a-cat-revisited-1",
    "title": "35  Drawing maps with Leaflet",
    "section": "35.6 The brain of a cat, revisited",
    "text": "35.6 The brain of a cat, revisited\nEarlier, we looked at an ethics study that had to do with a fictional brain of a fictional cat. I said there was actually a town called Catbrain. It’s in England, near Bristol, and seems to be home to a street of car dealerships.\n\nFind the latitude and longitude of “Catbrain UK” (though I don’t think there are any others).\n\nSolution\nMake sure you have these two packages loaded:\n\nlibrary(leaflet)\nlibrary(tmaptools)\n\nTo find the latitude and longitude of Catbrain:\n\ncatbrain &lt;- tibble(place = \"Catbrain UK\")\ncatbrain %&gt;% mutate(ll = list(geocode_OSM(place))) %&gt;% \n  unnest_wider(ll) %&gt;% \n  unnest_wider(coords) -&gt; catbrain\n\n\ncatbrain\n\n\n\n  \n\n\n\nRemember that the output from geocode_OSM is a list, and it has in it a thing called coords that is the longitude and latitude together, and another thing called bbox that we don’t use. So we have to unnest twice to get the longitude (as x) and the latitude (as y) out for drawing in a moment.\n\\(\\blacksquare\\)\n\nDraw a map of Catbrain using Leaflet.\n\nSolution\nThat goes this way:\n\nleaflet(data = catbrain) %&gt;% \n  addTiles() %&gt;% \n  addCircleMarkers(lng = ~x, lat = ~y) -&gt; catbrain_map\ncatbrain_map\n\n\n\n\n\nThere are car dealerships are along Lysander Road. Zoom in to see them. Or zoom out to see where this is. You can keep zooming out until you know where you are, using the plus and minus buttons, or your mouse wheel.\nThe name Catbrain, according to link, means “rough stony soil”, from Middle English, and has nothing to do with cats or their brains at all.\nExtra: I was actually surprised that this worked at all, because with only one point, how does it know what scale to draw the map? Also, unless your UK geography is really good, you won’t have any clue about exactly where this is. That’s the reason for the next part.\n\\(\\blacksquare\\)\n\nMake a dataframe containing some other British cities as well as Catbrain, and find their latitudes and longitudes.\n\nSolution\nI chose the cities below, mostly somewhere near Catbrain. You could fire up a Google map, zoom it out until it contains something you know, and pick some places you’ve heard of. (I happen to know British geography pretty well, so I just picked some mostly nearby places out of my head. I didn’t really want to pick London, but I figured this was the one you might know.)\n\ncatbrain2 &lt;- tribble(\n  ~where,\n  \"Catbrain UK\",\n  \"Bristol UK\",\n  \"Taunton UK\",\n  \"Newport UK\",\n  \"Gloucester UK\",\n  \"Cardiff UK\",\n  \"Birmingham UK\",\n  \"London UK\",\n  \"Caldicot UK\"\n)\ncatbrain2 %&gt;%\n  rowwise() %&gt;% \n  mutate(ll = list(geocode_OSM(where))) %&gt;% \n  unnest_wider(ll) %&gt;% \n  unnest_wider(coords) -&gt; catbrain2\n\n\ncatbrain2\n\n\n\n  \n\n\n\nThe first time I did this, I forgot the rowwise, which we didn’t need the first time (there was only one place), but here, it causes odd problems if you omit it.\n\\(\\blacksquare\\)\n\nDraw a map containing the places you picked.\n\nSolution\nThe map-drawing is almost the same, just changing the dataframe:\n\nleaflet(data = catbrain2) %&gt;% \n  addTiles() %&gt;% \n  addCircleMarkers(lng = ~x, lat = ~y)\n\n\n\n\n\nNow, if you have any sense of the geography of the UK, you know where you are. The big river (the Severn) is the border between England and Wales, so the places above and to the left of it are in Wales, including Caldicot (see question about Roman pottery).\nYou can zoom this map in (once you have figured out which of the circles is Catbrain) and find Lysander Road again, and also the M5 (see below).\nMore irrelevant extra: the M5 is one of the English “motorways” (like 400-series highways or US Interstates). The M5 goes from Birmingham to Exeter. You can tell that this is England because of the huge number of traffic circles, known there as “roundabouts”. One of the first things they teach you in British driving schools is how to handle roundabouts: which lane to approach them in, which (stick-shift) gear to be in, and when you’re supposed to signal where you’re going. I hope I still remember all that for when I next drive in England!\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "maps.html#footnotes",
    "href": "maps.html#footnotes",
    "title": "35  Drawing maps with Leaflet",
    "section": "",
    "text": "I knew this existed, but I couldn’t remember what it was called, which made it hard to search for. My first port of call was na_if, which converts values to NA, the opposite of what I wanted. But from its See Also I got na_replace, and from the See Also of that, I found out what coalesce does.↩︎\nIf you’re a soccer fan, this Aston is what Aston Villa is named after. See if you can find the team’s stadium Villa Park on your map, which is actually closer to Witton station on another line.↩︎\nThe station is called just University, not Birmingham University, which makes it hard to find.↩︎"
  },
  {
    "objectID": "mds.html#making-a-map-of-wisconsin",
    "href": "mds.html#making-a-map-of-wisconsin",
    "title": "36  Multidimensional Scaling",
    "section": "36.1 Making a map of Wisconsin",
    "text": "36.1 Making a map of Wisconsin\nThe file link contains the road distances (in miles) between 12 cities in Wisconsin and neighbouring states. We are going to try to reproduce a map of the area using multidimensional scaling.\n\nRead in the data and create a dist object, bearing in mind that the data in the file are already distances. Display your dist object. Probably, displaying the data that you read in is a good idea also.\nObtain a vector containing the city names. (This is easy, and not really necessary, but it was here before when I did things a different way.)\nRun a (metric) multidimensional scaling on the data, to obtain a two-dimensional representation of the cities. (You don’t need to look at the results yet.)\nPlot the results of the multidimensional scaling, labelling the cities with their names. Use your judgement to decide where to place the city names, and how to make sure the whole city names are shown on the map.\nAre cities close together on your map also close together in real life? Give an example or two.\nObtain a Google (or other) map of the area containing these twelve cities. (The way I expected you to do this when this was a hand-in problem was to take a screenshot or similar and include that in your document.)\nDiscuss how the map that came out of the multidimensional scaling corresponds to the actual (Google) map.\nCalculate something that demonstrates that a one-dimensional map of the cities is a much worse representation than the two-dimensional one that we made before. (I’m planning to get to this in class, but if we skip it, don’t worry about this part.)"
  },
  {
    "objectID": "mds.html#things-that-feel-similar-to-each-other",
    "href": "mds.html#things-that-feel-similar-to-each-other",
    "title": "36  Multidimensional Scaling",
    "section": "36.2 Things that feel similar to each other",
    "text": "36.2 Things that feel similar to each other\nWhich objects feel similar to one another and which ones feel different? Can we draw them on a “map”? 30 subjects1 were each given 17 different objects to feel, for example “inner surface of pine bark”, “rubber eraser” and “nylon scouring pad”. The subjects had to group the objects into a maximum of 8 groups such that the objects within a group felt similar, and the ones in different groups felt different.2 A dissimilarity matrix was formed by counting how many subjects put each pair of objects into different groups, so that the dissimilarity ranged from 0 (the objects were always put together in the same group) to 30 (the objects were not put together into the same group by any of the subjects).\nThe data are in link.\n\nLook at the data, and read in the file appropriately. Do you have something square in shape, apart from any columns of object names? Do your columns have the same names as the objects?\nObtain the names of the objects. Note that they are rather long.\nSet the column names of your data frame to be your object names, using names. Before you rush to do that, figure out how many column names you need, and supply values for any extra ones. Check that your data frame now has the right column names.\n*\nConvert your data frame into a distance object. Take a look at the (rather long) result.\nObtain and plot a (metric) multidimensional scaling map of these data. Label the points with the name of the object they represent. (Note that geom_text_repel has an option size that controls the size of the text.)\nFind a pair of objects that are close together on your plot. Look back at your answer to part~(here): is the distance between those objects small? Explain briefly.\nObtain a measure of goodness of fit for this multidimensional scaling map.\nObtain a cluster analysis for the same data, using Ward’s method. Obtain a dendrogram. What seems to be an appropriate number of clusters? Mark them on your dendrogram too.\nDiscuss as briefly as seems reasonable whether your clusters tell the same story as the map that came from multidimensional scaling. (There were only three marks for this, so you don’t need to go overboard.) Optionally, think about creating a plot that will make it easier to see the correspondence between your clusters and the MDS map."
  },
  {
    "objectID": "mds.html#confusing-letters",
    "href": "mds.html#confusing-letters",
    "title": "36  Multidimensional Scaling",
    "section": "36.3 Confusing letters",
    "text": "36.3 Confusing letters\nTwo researchers studied how often people mistook one letter for another one. They did this by showing their subjects each (uppercase) letter for a few milliseconds only, and asking them which letter (out of a list) they just saw. The researchers obtain a “confusion matrix” in which, for example, the W-Q entry is the total of W’s taken as Q and Q’s taken as W. This confusion matrix is in link. Note that the confusions are actually similarities rather than dissimilarities: a large number means that the two letters concerned are easily confused, that is, similar. Similarities can be converted to dissimilarities by subtracting them from a larger number than anything in the table (that is, subtracting from 21 is good here).\n\nRead in the confusion matrix and convert it to dissimilarities and then to a dist object, displaying the dist object that you obtain.\nRun a multidimensional scaling, with the default two dimensions, on your dist object, and display the results. (Making a graph is coming later.)\nObtain a vector of the names of the letters that were confused in this study.\nPlot your multidimensional scaling map. To do this, first create a data frame containing the points you want to plot and their labels, and then plot the points labelled by the right thing.\nPick a pair of letters appear close together on your map. Does it make sense, from looking at these letters, that they would be easy to confuse? Explain briefly.\nVerify that your chosen pair of letters was often confused in the data."
  },
  {
    "objectID": "mds.html#more-beer-please",
    "href": "mds.html#more-beer-please",
    "title": "36  Multidimensional Scaling",
    "section": "36.4 More beer please",
    "text": "36.4 More beer please\nPreviously, you did a cluster analysis of ten brands of beer, as rated by 32 students. This time, we will do a non-metric multidimensional scaling of those same brands of beer. The data are in link.\n\nNoting that we want to assess distances between brands of beer, read in the data and do whatever you need to do to work out distances between the beers. Show your result.\nObtain a non-metric multidimensional scaling of the beers. (Comment coming up in a moment.)\nObtain the stress value of the map, and comment on it.\nObtain a map of the beers, labelled with the names of the beers.\nFind a pair of beers close together on your map. Are they similar in terms of student ratings? Explain briefly.\nIn our cluster analysis, we found that Anchor Steam, Pete’s Wicked Ale, Guinness and Sierra Nevada were all in the same cluster. Would you expect them to be close together on your map? Are they? Explain briefly."
  },
  {
    "objectID": "mds.html#feeling-similar-again",
    "href": "mds.html#feeling-similar-again",
    "title": "36  Multidimensional Scaling",
    "section": "36.5 Feeling similar, again",
    "text": "36.5 Feeling similar, again\nPreviously, we looked at an experiment about which objects feel similar to one another and which ones feel different.\n30 subjects were each given 17 different objects to feel, for example “inner surface of pine bark”, “rubber eraser” and “nylon scouring pad”. The subjects had to group the objects into a maximum of 8 groups such that the objects within a group felt similar, and the ones in different groups felt different. A dissimilarity matrix was formed by counting how many subjects put each pair of objects into different groups, so that the dissimilarity ranged from 0 (the objects were always put together in the same group) to 30 (the objects were not put together into the same group by any of the subjects).\nThe data are in link. These have no column names.\nThis time we are going to try non-metric multidimensional scaling, to see whether that produces a more reasonable map. The reading in of the data is the same as before (so I have reduced the marks given for it).\n\nObtain and display the distance matrix that you used last time for these data. (I don’t care whether you run all the code again, or whether you just produce the matrix from where you had it before on R Studio. Correct is all I care about.)\nObtain a non-metric multidimensional scaling map of the objects. (No plotting or comments yet.)\n* Obtain a number that says how well the map reproduces the distances in the data, and comment on that number.\nPlot the non-metric multidimensional scaling map. Label each point with its (full) object name, sized suitably.\n* Find all the distances in your distance matrix that are 10 or less (there should be three of them). Find these pairs of objects on your map. Describe where they are on your map. Do they appear to be the three closest pairs of objects?\nConsider again your conclusions in parts (here) and (here). Explain briefly how they are consistent.\nWould a three-dimensional map work better than a two-dimensional one, bearing in mind that a three-dimensional map will need something like rgl to interpret? Calculate something that will help you decide, and explain what you think.\n\nMy solutions follow:"
  },
  {
    "objectID": "mds.html#making-a-map-of-wisconsin-1",
    "href": "mds.html#making-a-map-of-wisconsin-1",
    "title": "36  Multidimensional Scaling",
    "section": "36.6 Making a map of Wisconsin",
    "text": "36.6 Making a map of Wisconsin\nThe file link contains the road distances (in miles) between 12 cities in Wisconsin and neighbouring states. We are going to try to reproduce a map of the area using multidimensional scaling.\n\nRead in the data and create a dist object, bearing in mind that the data in the file are already distances. Display your dist object. Probably, displaying the data that you read in is a good idea also.\n\nSolution\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/wisconsin.txt\"\nwisc &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  location = col_character(),\n  Appleton = col_double(),\n  Beloit = col_double(),\n  Fort.Atkinson = col_double(),\n  Madison = col_double(),\n  Marshfield = col_double(),\n  Milwaukee = col_double(),\n  Monroe = col_double(),\n  Superior = col_double(),\n  Wausau = col_double(),\n  Dubuque = col_double(),\n  St.Paul = col_double(),\n  Chicago = col_double()\n)\n\nwisc\n\n\n\n  \n\n\n\nThe first time I did this, I had a blank line on the end of the data file, so I had a blank location and missing values for all the distances for it. I tidied that up before sharing the file with you, though.\nSo, the first column is the names of the places, which we should get rid of before we make a dist object using as.dist (since what we read in is already distances). The columns are also the names of the places, so we won’t lose anything by getting rid of the location column:\n\nd &lt;- wisc %&gt;%\n  select(-location) %&gt;%\n  as.dist()\nd\n\n              Appleton Beloit Fort.Atkinson Madison Marshfield Milwaukee Monroe\nBeloit             130                                                         \nFort.Atkinson       98     33                                                  \nMadison            102     50            36                                    \nMarshfield         103    185           164     138                            \nMilwaukee          100     73            54      77        184                 \nMonroe             149     33            58      47        170       107       \nSuperior           315    377           359     330        219       394    362\nWausau              91    186           166     139         45       181    186\nDubuque            196     94           119      95        186       168     61\nSt.Paul            257    304           287     258        161       322    289\nChicago            186     97           113     146        276        93    130\n              Superior Wausau Dubuque St.Paul\nBeloit                                       \nFort.Atkinson                                \nMadison                                      \nMarshfield                                   \nMilwaukee                                    \nMonroe                                       \nSuperior                                     \nWausau             223                       \nDubuque            351    215                \nSt.Paul            162    175     274        \nChicago            467    275     184     395\n\n\n\\(\\blacksquare\\)\n\nObtain a vector containing the city names. (This is easy, and not really necessary, but it was here before when I did things a different way.)\n\nSolution\nThe location column of the data frame you read in from the file:\n\ncities &lt;- wisc$location\ncities\n\n [1] \"Appleton\"      \"Beloit\"        \"Fort.Atkinson\" \"Madison\"      \n [5] \"Marshfield\"    \"Milwaukee\"     \"Monroe\"        \"Superior\"     \n [9] \"Wausau\"        \"Dubuque\"       \"St.Paul\"       \"Chicago\"      \n\n\n\\(\\blacksquare\\)\n\nRun a (metric) multidimensional scaling on the data, to obtain a two-dimensional representation of the cities. (You don’t need to look at the results yet.)\n\nSolution\n\nwisc.1 &lt;- cmdscale(d)\n\n\\(\\blacksquare\\)\n\nPlot the results of the multidimensional scaling, labelling the cities with their names. Use your judgement to decide where to place the city names, and how to make sure the whole city names are shown on the map.\n\nSolution\nLet’s see what wisc.1 has in it, and make a data frame of the right thing:\n\nhead(wisc.1)\n\n                    [,1]       [,2]\nAppleton       -13.37076  85.067148\nBeloit         -92.94157 -20.205916\nFort.Atkinson  -74.07473   4.804039\nMadison        -44.68148 -11.252521\nMarshfield      80.61250  27.097882\nMilwaukee     -102.87582  49.849552\n\n\nTwo unnamed columns (the display indicates that it’s a matrix rather than a data frame). As we saw in class, if we make a data frame out of this, the columns will get names X1 and X2. Those are perfectly good names for coordinates. The city names on the left of wisc.1 are actually row names rather than an actual genuine column. It’s probably best not to assume that they will make it through the transition to a data frame, so we’ll explicitly create a column called city with the city names (that we saved before):\n\ndd &lt;- data.frame(wisc.1, city = cities)\ndd\n\n\n\n  \n\n\n\nThere are only 12 rows, so it’s fine to display them all.\nI’m calling this one dd since I have another d elsewhere that I want to keep. I should use better names.\nI think the best use of your judgement is to go straight to geom_text_repel from package ggrepel:\n\nggplot(dd, aes(x = X1, y = X2, label = city)) +\n  geom_point() +\n  geom_text_repel() +\n  coord_fixed()\n\n\n\n\nYour map may come out different from mine, but subject to the usual stuff about rotation and reflection it should be equivalent to mine. You should include the coord_fixed to get the scales the same (a corresponding distance on the two scales should take the same space). This one didn’t come out quite square because the MDS says the points should be in a rectangle (stretching further one way than the other).\n\\(\\blacksquare\\)\n\nAre cities close together on your map also close together in real life? Give an example or two.\n\nSolution\nOn the map, the trio of cities Madison, Beloit and Fort Atkinson are closest together. How far apart are they actually? Well, you can go back to the original file (or display of what I called d) and find them, or you can do this:\n\ncities\n\n [1] \"Appleton\"      \"Beloit\"        \"Fort.Atkinson\" \"Madison\"      \n [5] \"Marshfield\"    \"Milwaukee\"     \"Monroe\"        \"Superior\"     \n [9] \"Wausau\"        \"Dubuque\"       \"St.Paul\"       \"Chicago\"      \n\n\nCities 2, 3 and 4, so:\n\nwisc %&gt;% slice(2:4) %&gt;% select(c(1, 3:5))\n\n\n\n  \n\n\n\nThe column numbers are off by one, since the first column is the names of the cities, which I decided to display here. It came out right, anyway.\nThese are all less than 50 miles or less apart. There are some others this close in the original data: Monroe and Madison are 47 miles apart, Wausau and Marshfield are 45 miles apart, but these appear further apart on the map. Extra: the slice-select thing doesn’t work on d because that is not a data frame. It is actually stored internally as a one-dimensional vector that displays nicely, but if you want to pull things out of it you have to figure out where in the vector they are:\n\nprint.default(d)\n\n [1] 130  98 102 103 100 149 315  91 196 257 186  33  50 185  73  33 377 186  94\n[20] 304  97  36 164  54  58 359 166 119 287 113 138  77  47 330 139  95 258 146\n[39] 184 170 219  45 186 161 276 107 394 181 168 322  93 362 186  61 289 130 223\n[58] 351 162 467 215 175 275 274 184 395\nattr(,\"Labels\")\n [1] \"Appleton\"      \"Beloit\"        \"Fort.Atkinson\" \"Madison\"      \n [5] \"Marshfield\"    \"Milwaukee\"     \"Monroe\"        \"Superior\"     \n [9] \"Wausau\"        \"Dubuque\"       \"St.Paul\"       \"Chicago\"      \nattr(,\"Size\")\n[1] 12\nattr(,\"call\")\nas.dist.default(m = .)\nattr(,\"class\")\n[1] \"dist\"\nattr(,\"Diag\")\n[1] FALSE\nattr(,\"Upper\")\n[1] FALSE\n\n\nIf you compare that with the usual display of d, this way goes all the way down the first column ending at 130, then all the way down the second column (which has one fewer entry), ending at 467, and so on. Thus the three entries we picked out are at \\(11+1=12\\), \\(11+2=13\\), and \\(11+10+1=22\\):\n\nd[12]\n\n[1] 33\n\nd[13]\n\n[1] 50\n\nd[22]\n\n[1] 36\n\n\nIt’s very easy to be off by one in this sort of calculation. There are 12 cities altogether, so 11 distances in the first column, 10 in the second, and so on. This was about my third attempt.\nI don’t much care which cities you look at. Finding some cities that are reasonably close on the map and doing some kind of critical assessment of their actual distances apart is all I want.\n\\(\\blacksquare\\)\n\nObtain a Google (or other) map of the area containing these twelve cities. (The way I expected you to do this when this was a hand-in problem was to take a screenshot or similar and include that in your document.)\n\nSolution\nI do this using R in the maps chapter. See there for how to do it this way.\n\\(\\blacksquare\\)\n\nDiscuss how the map that came out of the multidimensional scaling corresponds to the actual (Google) map.\n\nSolution\nLet’s pick a few places from the actual map, and make a table of where they are on the actual map and the cmdscale map:\n\n Place        Real              Cmdscale       \n----------------------------------------------\n Superior     northwest         central east   \n St. Paul     central west      southeast      \n Dubuque      central south     central south  \n Chicago      southeast         central west   \n Appleton     central east      central north  \n\n\nThis is a bit tricky. Dubuque is the only one in the right place, and the others that were west have become east and vice versa. So I think there is a flipping across a line going through Dubuque. That seems to be the most important thing; if you imagine the other points being flipped across a line going north-south through Dubuque, they all end up in about the right place. There might be a little rotation as well, but I’ll call that close enough.\n(For you, any comment along the lines of “flipped around this line” or “rotated about this much” that seems to describe what has happened, is fine.)\n\\(\\blacksquare\\)\n\nCalculate something that demonstrates that a one-dimensional map of the cities is a much worse representation than the two-dimensional one that we made before. (I’m planning to get to this in class, but if we skip it, don’t worry about this part.)\n\nSolution\nRun again with eig=T and take a look at GOF (uppercase):\n\ncmdscale(d, 2, eig = T)$GOF\n\n[1] 0.9129333 0.9315871\n\ncmdscale(d, 1, eig = T)$GOF\n\n[1] 0.7916925 0.8078690\n\n\nThe goodness-of-fit of the two-dimensional solution is pretty good,3 but that of the one-dimensional solution (which arranges all the cities along a line) is pretty awful in comparison.\nHow awful? Let’s find out. I should have saved it from just above, but now I have to do it again. For the plot, ones is a string of ones, as many as there are cities.\n\nones &lt;- rep(1, 12)\nv &lt;- cmdscale(d, 1, eig = T)\nddd &lt;- as_tibble(v$points) %&gt;%\n  mutate(one = ones, city = cities)\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\nddd\n\n\n\n  \n\n\n\n(the one-column matrix of points didn’t have a name, so it acquired the name V1), and the plot:\n\nggplot(ddd, aes(x = one, y = V1, label = city)) +\n  geom_point() + geom_text_repel()\n\n\n\n\nThe cities get mapped onto a line that goes northwest (top) to southeast (bottom). This is not completely terrible, since there aren’t really any cities in the northeast of the state, but it is pretty awful.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mds.html#things-that-feel-similar-to-each-other-1",
    "href": "mds.html#things-that-feel-similar-to-each-other-1",
    "title": "36  Multidimensional Scaling",
    "section": "36.7 Things that feel similar to each other",
    "text": "36.7 Things that feel similar to each other\nWhich objects feel similar to one another and which ones feel different? Can we draw them on a “map”? 30 subjects4 were each given 17 different objects to feel, for example “inner surface of pine bark”, “rubber eraser” and “nylon scouring pad”. The subjects had to group the objects into a maximum of 8 groups such that the objects within a group felt similar, and the ones in different groups felt different.5 A dissimilarity matrix was formed by counting how many subjects put each pair of objects into different groups, so that the dissimilarity ranged from 0 (the objects were always put together in the same group) to 30 (the objects were not put together into the same group by any of the subjects).\nThe data are in link.\n\nLook at the data, and read in the file appropriately. Do you have something square in shape, apart from any columns of object names? Do your columns have the same names as the objects?\n\nSolution\nLooking at the file first, there are no column names. So we have to tell read_delim that:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/stimuli.txt\"\nstimuli &lt;- read_delim(my_url, \" \", col_names = FALSE)\n\nRows: 17 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr  (1): X1\ndbl (17): X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstimuli\n\n\n\n  \n\n\n\nI have 17 rows and 18 columns, but one of the columns is the column of object names. So I really do have the same number of rows and columns of dissimilarities: that is, it is square in shape apart from the names.\nThe columns do not have the same names as the objects; R has used the X1, X2, ... names that it uses when you do not have column headers.\nI could have supplied the object names to col_names, but that is a lot more work than what we do below.\n\\(\\blacksquare\\)\n\nObtain the names of the objects. Note that they are rather long.\n\nSolution\nThe object names are in the first column, X1, of the data frame:\n\nobjects &lt;- stimuli$X1\nobjects\n\n [1] \"innersurfaceofpinebark\" \"brick\"                  \"cardboard\"             \n [4] \"cork\"                   \"rubbereraser\"           \"felt\"                  \n [7] \"leatherwallet\"          \"rigidplasticsheet\"      \"veryfinesandpaper\"     \n[10] \"nylonscouringpad\"       \"cellulosekitchensponge\" \"wovenstraw\"            \n[13] \"blockofstyrofoam\"       \"unglazedceramictile\"    \"velvet\"                \n[16] \"waxpaper\"               \"glossypaintedwood\"     \n\n\nI’m saving these for later.\n\\(\\blacksquare\\)\n\nSet the column names of your data frame to be your object names, using names. Before you rush to do that, figure out how many column names you need, and supply values for any extra ones. Check that your data frame now has the right column names.\n\nSolution\nI have 18 columns to name (including the column of object names), but only 17 names, so I need to supply an extra one:\n\nnames(stimuli) &lt;- c(\"object\", objects)\nstimuli\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\n*\nConvert your data frame into a distance object. Take a look at the (rather long) result.\n\nSolution\nThis is as.dist, since we have distances (dissimilarities) already. Don’t forget to take off the first column!\n\nd &lt;- stimuli %&gt;% select(-1) %&gt;% as.dist()\n\nI can try and show it all here, tiny, but even then it’s long because the column names are long:\n\nw &lt;- getOption(\"width\")\noptions(width = 132)\nd\n\n                       innersurfaceofpinebark brick cardboard cork rubbereraser felt leatherwallet rigidplasticsheet\nbrick                                      22                                                                       \ncardboard                                  23    27                                                                 \ncork                                       24    27        18                                                       \nrubbereraser                               26    27        19   15                                                  \nfelt                                       27    29        28   28           28                                     \nleatherwallet                              26    29        23   25           20   24                                \nrigidplasticsheet                          23    28        24   26           27   28            22                  \nveryfinesandpaper                          24    16        24   28           24   29            28                27\nnylonscouringpad                           23    18        29   28           27   26            28                29\ncellulosekitchensponge                     23    28        27   20           24   26            27                29\nwovenstraw                                 18    25        28   27           25   29            26                27\nblockofstyrofoam                           23    24        21   10           19   28            25                25\nunglazedceramictile                        21    10        26   26           24   29            29                25\nvelvet                                     28    29        28   28           29    4            24                29\nwaxpaper                                   24    28        24   28           24   28            21                12\nglossypaintedwood                          22    27        23   29           28   29            20                13\n                       veryfinesandpaper nylonscouringpad cellulosekitchensponge wovenstraw blockofstyrofoam unglazedceramictile\nbrick                                                                                                                           \ncardboard                                                                                                                       \ncork                                                                                                                            \nrubbereraser                                                                                                                    \nfelt                                                                                                                            \nleatherwallet                                                                                                                   \nrigidplasticsheet                                                                                                               \nveryfinesandpaper                                                                                                               \nnylonscouringpad                      21                                                                                        \ncellulosekitchensponge                24               22                                                                       \nwovenstraw                            26               16                     19                                                \nblockofstyrofoam                      25               25                     21         26                                     \nunglazedceramictile                   12               24                     26         26               25                    \nvelvet                                29               27                     27         28               29                  29\nwaxpaper                              29               29                     29         27               26                  28\nglossypaintedwood                     27               28                     27         25               29                  26\n                       velvet waxpaper\nbrick                                 \ncardboard                             \ncork                                  \nrubbereraser                          \nfelt                                  \nleatherwallet                         \nrigidplasticsheet                     \nveryfinesandpaper                     \nnylonscouringpad                      \ncellulosekitchensponge                \nwovenstraw                            \nblockofstyrofoam                      \nunglazedceramictile                   \nvelvet                                \nwaxpaper                   27         \nglossypaintedwood          26       12\n\noptions(width = w)\n\nThe stuff with width was to make it display lots of columns, and then setting it back afterwards so as not to mess things up later. If you try and take head of this, you’ll lose the structure. I don’t know of a good way to display part of one of these.\n\\(\\blacksquare\\)\n\nObtain and plot a (metric) multidimensional scaling map of these data. Label the points with the name of the object they represent. (Note that geom_text_repel has an option size that controls the size of the text.)\n\nSolution\nThis is the procedure. Talking about it is coming in a minute.\n\nd.1 &lt;- cmdscale(d, 2)\ndata.frame(d.1, stim = objects) %&gt;%\n  ggplot(aes(x = X1, y = X2, label = stim)) + geom_point() +\n  geom_text_repel(size = 2)\n\n\n\n\ncmdscale gets the coordinates to plot, then we plot them, and then we place the object names next to the points. I’m not quite sure what the scale is for size, but size=2 worked for me here, making the text a bit smaller (so that the labels don’t overlap), but not so small that you can’t read it. You’ll probably have to experiment to find a size that you like.\nIf you forget the 2 after the distance matrix in cmdscale, you’ll get a two-dimensional solution anyway (since two dimensions is the default). The output is an array of coordinates in two dimensions:\n\nd.1\n\n                              [,1]       [,2]\ninnersurfaceofpinebark  -2.7066290   1.694420\nbrick                  -12.2011332   5.147970\ncardboard                3.8630322  -9.322759\ncork                    -0.8424358 -14.884926\nrubbereraser             0.1676463 -11.733873\nfelt                     5.1803473   9.328562\nleatherwallet           10.4636668  -1.016525\nrigidplasticsheet       11.0208731   1.201504\nveryfinesandpaper      -11.0869483   2.800988\nnylonscouringpad       -10.4469053   7.232787\ncellulosekitchensponge  -5.3886609  -2.770991\nwovenstraw              -5.2762142   3.836948\nblockofstyrofoam        -2.9950151 -11.927717\nunglazedceramictile    -10.5902291   2.926805\nvelvet                   6.3768882  10.477972\nwaxpaper                13.1702265   1.677039\nglossypaintedwood       11.2914904   5.331796\n\n\nI note that any map that is the same as this apart from rotation and reflection is also fine (since the distances will be the same). I saw a lot of maps that were like mine but upside down (with cork at the top).6 No problem there.\n\\(\\blacksquare\\)\n\nFind a pair of objects that are close together on your plot. Look back at your answer to part~(here): is the distance between those objects small? Explain briefly.\n\nSolution\nI don’t mind (much) which objects you pick (as long as they are reasonably close together). Find the actual distance between them from what I called d in part~(here). Some possibilities:\n\nFelt and velvet, Distance 4.\nBlock of styrofoam and rubber eraser. Distance 19 (not that small, but one of the smaller ones).\nRigid plastic sheet and wax paper. Distance 12. Smallish.\nunglazed ceramic tile7 and very fine sandpaper. Distance 12. Smallish.\n\nThere won’t be a perfect relationship between distance in the distance matrix and on the map. In this case, there is an upper limit on distance (30, because 30 people rated the objects for similarity) and that upper limit is approached by many of the distances. (This suggests that non-metric multidimensional scaling, that just reproduces the order of the distances, might be a better idea for these data.) If objects A and B, and also B and C, are each close to 30 apart, then objects A and C will also be close to 30 apart, and that constrains them to be nearly in a triangle on the map. There are some 10s in the distance matrix, for example between block of styrofoam and cork, and also between unglazed ceramic tile and brick; these are a bit further apart on the map, but still close.\n\\(\\blacksquare\\)\n\nObtain a measure of goodness of fit for this multidimensional scaling map.\n\nSolution\nThis means fitting again, but this time with eig=T, and pulling out the thing called GOF. You can omit the 2, since that’s the default 2 dimensions:\n\nd.2 &lt;- cmdscale(d, 2, eig = T)\nd.2$GOF\n\n[1] 0.4019251 0.4110603\n\n\nI didn’t ask you to comment on this, but the adjective that came to my mind was “disappointing”. I think that’s to do with the upper-limit-30 thing again. Also, this time (unlike with Wisconsin) there was no “right answer”, so maybe it just isn’t going to be very good. If you looked at several pairs of points above, you might have noticed that the correspondence between map distance and actual distance isn’t very good; this is the same issue.\nI was curious about whether 3 dimensions would be any better:\n\nd.2a &lt;- cmdscale(d, 3, eig = T)\nd.2a$GOF\n\n[1] 0.5775841 0.5907117\n\n\nThat is quite a bit better. The problem with this, though, is that we need something like rgl to explore it with.\n\\(\\blacksquare\\)\n\nObtain a cluster analysis for the same data, using Ward’s method. Obtain a dendrogram. What seems to be an appropriate number of clusters? Mark them on your dendrogram too.\n\nSolution\nThis seems to be a throwback to last week, but I have my reasons, which you’ll see in a moment:\n\nd.3 &lt;- hclust(d, method = \"ward.D\")\nplot(d.3)\n\n\n\n\nThat looks like 5 clusters to me (chopping the tree at a height of 30). Drawing them:\n\nplot(d.3)\nrect.hclust(d.3, 5)\n\n\n\n\n\\(\\blacksquare\\)\n\nDiscuss as briefly as seems reasonable whether your clusters tell the same story as the map that came from multidimensional scaling. (There were only three marks for this, so you don’t need to go overboard.) Optionally, think about creating a plot that will make it easier to see the correspondence between your clusters and the MDS map.\n\nSolution\nI would take my clusters and think about whether the objects in them are (a) close together on the map and (b) distinct from other objects on the map. Working from left to right of my dendrogram:\n\nTogether at the bottom of the plot.\nTogether at the top left.\nTogether at the top left, but rather mixed up with the previous cluster (in particular, nylonscouringpad looks as if it belongs in the previous cluster).\nTogether at the top of the plot.\nTogether at the top right.\n\nIf you had a different number of clusters, work with what you have, and if you have done that, you’ll be good.\nMy general conclusion from the above is that my five clusters are mostly distinct from each other, but the MDS map really only has four. (If you look at the dendrogram, the two clusters that are not really distinct from each other were the last ones to be split.) This is a lot more detail than you need. What I want to see is some kind of association of clusters with (hopefully) nearby points on the map, some kind of discussion of whether the clusters are really distinct groupings on the map, and a sense of whether you feel the clusters are consistent with what appears on the map.\nExtra: this is where I have to say I cheated. I thought this would be hard to do by trying to match those names in the clusters with the ones on the MDS map. So I put myself through some short-term R coding pain for some substantial long-term gain. I was thinking, “can I extract the clusters from this analysis, and plot them on the MDS map in different colours?” That would go like this:\n\nclusters &lt;- cutree(d.3, 5)\ndata.frame(d.1, names = stimuli[, 1], cluster = factor(clusters)) %&gt;%\n  ggplot(aes(x = X1, y = X2, label = objects, colour = cluster)) +\n  geom_point() +\n  geom_text_repel(size = 2)\n\n\n\n\nThe first line obtains the clusters as numbers, which I then had to turn into a factor (to make the plot different colours). I copied the rest from above, and then I added to them, colouring the points according to what cluster they belonged to. Three of my “rough” objects have broken off into a cluster of their own; they are, kind of, in their own little area on the map.\nAnother, non-graphical, way of doing this is to list the MDS coordinates along with the clusters, probably sorted by cluster:\n\ndata.frame(d.1, object = stimuli[, 1], clusters) %&gt;% arrange(clusters)\n\n\n\n  \n\n\n\nThen you can think about what makes the clusters different in terms of X1 and X2. For me, clusters 1 and 2 are kind of mixed up, with X1 and (usually) X2 negative; cluster 3 has strongly positive X2; cluster 4 has very negative X2;8 and cluster 5 has strongly positive X1.\nI wonder whether our three-dimensional solution distinguishes clusters 1 and 2 at all? Same approach again:\n\nsave.3d &lt;- data.frame(d.2a$points, objects, clusters) %&gt;%\n  arrange(clusters)\nsave.3d\n\n\n\n  \n\n\n\nIt looks as if the combo of negative X1 and positive X3 distinguishes cluster 2 from cluster 1.\nAs I was writing this, I was thinking that we should throw the coordinates and the clusters into a discriminant analysis, to find out what distinguishes the clusters. Which is way too weird, and therefore deserves to be explored (with the three-dimensional solution, for example):\n\nlibrary(MASS)\nsave.3d.lda &lt;- lda(clusters ~ X1 + X2 + X3, data = save.3d)\nppp &lt;- predict(save.3d.lda)\n\nI snuck a look at the output and found that LD3 is basically worthless, so I can plot LD1 against LD2, coloured by cluster:\n\ndata.frame(ppp$x, cluster = factor(save.3d$clusters)) %&gt;%\n  ggplot(aes(x = LD1, y = LD2, colour = cluster)) +\n  geom_point()\n\n\n\n\nThis says that the three-dimensional MDS has separated clusters 1 and 2, and if we were to plot d.2a in rgl and rotate it the right way, we would be able to see a difference between those two clusters as well. (They wouldn’t look mixed up as they do on the two-dimensional map.)\nSo we can look at the three-dimensional map coordinates and the discriminant analysis and ask “what distinguishes the clusters?”. Map coordinates first. I need points since I ran the scaling with eig=T:\n\nsave.3d\n\n\n\n  \n\n\n\nand the business end of the LDA output:\n\nsave.3d.lda$svd\n\n[1] 12.593922  5.767636  3.007893\n\nsave.3d.lda$scaling\n\n          LD1         LD2        LD3\nX1  0.5463715  0.22819994 0.03521147\nX2  0.4370065 -0.28250040 0.10269853\nX3 -0.3512713  0.09616153 0.19930252\n\n\nThe plot said that cluster 2 was lowest of all on LD1, a bit lower than cluster 1. What would make LD1 small (negative) would be if X1 was small and X2 and X3 were large. The cluster 2 observations are the smallest on X1 (smaller than cluster 1) and larger on X3 than cluster 1. So we can enumerate what makes clusters 1 and 2 different. The plot of the first two LDs says, in fact, that under the 3-dimensional multidimensional scaling, all five groups are distinct.\nA biplot would be another way to look at that:\n\nggbiplot(save.3d.lda, groups = factor(save.3d$clusters))\n\n\n\n\nPoints on the right have X1 and X2 large, and X3 small (cluster 4, and to a lesser extent, cluster 5). Points on the left are the other way around. None of the arrows point up or down really, but X1 points up a bit and X2 down a bit, so points at the top of the plot are likely to be high on X1 and low on X2, like cluster 5.\nWas that all confusing enough for you?\nAnyway, the key to gaining some insight here is to find a way to combine the output from the cluster analysis and the multidimensional scaling. That way you can see both in the same place, and see how they compare.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mds.html#confusing-letters-1",
    "href": "mds.html#confusing-letters-1",
    "title": "36  Multidimensional Scaling",
    "section": "36.8 Confusing letters",
    "text": "36.8 Confusing letters\nTwo researchers studied how often people mistook one letter for another one. They did this by showing their subjects each (uppercase) letter for a few milliseconds only, and asking them which letter (out of a list) they just saw. The researchers obtain a “confusion matrix” in which, for example, the W-Q entry is the total of W’s taken as Q and Q’s taken as W. This confusion matrix is in link. Note that the confusions are actually similarities rather than dissimilarities: a large number means that the two letters concerned are easily confused, that is, similar. Similarities can be converted to dissimilarities by subtracting them from a larger number than anything in the table (that is, subtracting from 21 is good here).\n\nRead in the confusion matrix and convert it to dissimilarities and then to a dist object, displaying the dist object that you obtain.\n\nSolution\nread_table to read in the data, having first noted that we have aligned columns with spaces between:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/letterrec.txt\"\nletters &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  C = col_character(),\n  D = col_double(),\n  G = col_double(),\n  H = col_double(),\n  M = col_double(),\n  N = col_double(),\n  Q = col_double(),\n  W = col_double()\n)\n\n\nWarning: 8 parsing failures.\nrow col  expected     actual                                               file\n  1  -- 8 columns 10 columns 'http://ritsokiguess.site/datafiles/letterrec.txt'\n  2  -- 8 columns 9 columns  'http://ritsokiguess.site/datafiles/letterrec.txt'\n  3  -- 8 columns 9 columns  'http://ritsokiguess.site/datafiles/letterrec.txt'\n  4  -- 8 columns 9 columns  'http://ritsokiguess.site/datafiles/letterrec.txt'\n  5  -- 8 columns 9 columns  'http://ritsokiguess.site/datafiles/letterrec.txt'\n... ... ......... .......... ..................................................\nSee problems(...) for more details.\n\nletters\n\n\n\n  \n\n\n\nThe first column didn’t have a heading, so read_table filled in X1 for it (there was data down below, so it needed to have a name). This is in contrast to something like read_delim, where you have to have as many headers as columns. In read_table, you know where the headers have to be, so if any are missing they can be filled in, but with read_delim you don’t have that knowledge, so you have to have exactly the right number of headers, one per data column exactly.\nThese are similarities, so convert to dissimilarities by subtracting from 21. This is a shortcut way to do that, once you’ve gotten rid of everything that is not numbers:\n\nletters %&gt;% dplyr::select(-X1) -&gt; letters2\n\nError in `dplyr::select()`:\n! Can't subset columns that don't exist.\n✖ Column `X1` doesn't exist.\n\nletters2 &lt;- 21 - letters2\n\nError in eval(expr, envir, enclos): object 'letters2' not found\n\nletters2\n\nError in eval(expr, envir, enclos): object 'letters2' not found\n\n\nThis looks weird, or at least the stuff above the diagonal does, but dist works with the stuff below the diagonal (unless you tell it otherwise), so all will be good.\nas.dist comes next:\n\nd &lt;- as.dist(letters2)\n\nError in eval(expr, envir, enclos): object 'letters2' not found\n\nd\n\n                       innersurfaceofpinebark brick cardboard cork rubbereraser\nbrick                                      22                                  \ncardboard                                  23    27                            \ncork                                       24    27        18                  \nrubbereraser                               26    27        19   15             \nfelt                                       27    29        28   28           28\nleatherwallet                              26    29        23   25           20\nrigidplasticsheet                          23    28        24   26           27\nveryfinesandpaper                          24    16        24   28           24\nnylonscouringpad                           23    18        29   28           27\ncellulosekitchensponge                     23    28        27   20           24\nwovenstraw                                 18    25        28   27           25\nblockofstyrofoam                           23    24        21   10           19\nunglazedceramictile                        21    10        26   26           24\nvelvet                                     28    29        28   28           29\nwaxpaper                                   24    28        24   28           24\nglossypaintedwood                          22    27        23   29           28\n                       felt leatherwallet rigidplasticsheet veryfinesandpaper\nbrick                                                                        \ncardboard                                                                    \ncork                                                                         \nrubbereraser                                                                 \nfelt                                                                         \nleatherwallet            24                                                  \nrigidplasticsheet        28            22                                    \nveryfinesandpaper        29            28                27                  \nnylonscouringpad         26            28                29                21\ncellulosekitchensponge   26            27                29                24\nwovenstraw               29            26                27                26\nblockofstyrofoam         28            25                25                25\nunglazedceramictile      29            29                25                12\nvelvet                    4            24                29                29\nwaxpaper                 28            21                12                29\nglossypaintedwood        29            20                13                27\n                       nylonscouringpad cellulosekitchensponge wovenstraw\nbrick                                                                    \ncardboard                                                                \ncork                                                                     \nrubbereraser                                                             \nfelt                                                                     \nleatherwallet                                                            \nrigidplasticsheet                                                        \nveryfinesandpaper                                                        \nnylonscouringpad                                                         \ncellulosekitchensponge               22                                  \nwovenstraw                           16                     19           \nblockofstyrofoam                     25                     21         26\nunglazedceramictile                  24                     26         26\nvelvet                               27                     27         28\nwaxpaper                             29                     29         27\nglossypaintedwood                    28                     27         25\n                       blockofstyrofoam unglazedceramictile velvet waxpaper\nbrick                                                                      \ncardboard                                                                  \ncork                                                                       \nrubbereraser                                                               \nfelt                                                                       \nleatherwallet                                                              \nrigidplasticsheet                                                          \nveryfinesandpaper                                                          \nnylonscouringpad                                                           \ncellulosekitchensponge                                                     \nwovenstraw                                                                 \nblockofstyrofoam                                                           \nunglazedceramictile                  25                                    \nvelvet                               29                  29                \nwaxpaper                             26                  28     27         \nglossypaintedwood                    29                  26     26       12\n\n\nThis works because the letters that are confused are actually column names of the data frame.\nYou can check (and probably should, at least for yourself) that the distances in d correspond properly to the ones in letters. For example, the letters C and G were confused 12 times, \\(21-12=9\\), and the entry for C and G in d is indeed 9.\nNote that the actual confusion numbers were in the data file as the bottom half of the matrix, with the top half being zeroes. as.dist handled this with no problem. (You can check the help for as.dist to find out how it deals with this kind of thing.)\n\\(\\blacksquare\\)\n\nRun a multidimensional scaling, with the default two dimensions, on your dist object, and display the results. (Making a graph is coming later.)\n\nSolution\nThis:\n\nd.1 &lt;- cmdscale(d)\nd.1\n\n                              [,1]       [,2]\ninnersurfaceofpinebark  -2.7066290   1.694420\nbrick                  -12.2011332   5.147970\ncardboard                3.8630322  -9.322759\ncork                    -0.8424358 -14.884926\nrubbereraser             0.1676463 -11.733873\nfelt                     5.1803473   9.328562\nleatherwallet           10.4636668  -1.016525\nrigidplasticsheet       11.0208731   1.201504\nveryfinesandpaper      -11.0869483   2.800988\nnylonscouringpad       -10.4469053   7.232787\ncellulosekitchensponge  -5.3886609  -2.770991\nwovenstraw              -5.2762142   3.836948\nblockofstyrofoam        -2.9950151 -11.927717\nunglazedceramictile    -10.5902291   2.926805\nvelvet                   6.3768882  10.477972\nwaxpaper                13.1702265   1.677039\nglossypaintedwood       11.2914904   5.331796\n\n\nOr you can do it with eig=T, which gets you some more information:\n\nd.1a &lt;- cmdscale(d, eig = T)\nd.1a\n\n$points\n                              [,1]       [,2]\ninnersurfaceofpinebark  -2.7066290   1.694420\nbrick                  -12.2011332   5.147970\ncardboard                3.8630322  -9.322759\ncork                    -0.8424358 -14.884926\nrubbereraser             0.1676463 -11.733873\nfelt                     5.1803473   9.328562\nleatherwallet           10.4636668  -1.016525\nrigidplasticsheet       11.0208731   1.201504\nveryfinesandpaper      -11.0869483   2.800988\nnylonscouringpad       -10.4469053   7.232787\ncellulosekitchensponge  -5.3886609  -2.770991\nwovenstraw              -5.2762142   3.836948\nblockofstyrofoam        -2.9950151 -11.927717\nunglazedceramictile    -10.5902291   2.926805\nvelvet                   6.3768882  10.477972\nwaxpaper                13.1702265   1.677039\nglossypaintedwood       11.2914904   5.331796\n\n$eig\n [1]  1.181313e+03  9.394546e+02  9.268689e+02  6.314409e+02  3.467493e+02\n [6]  2.705989e+02  2.575474e+02  1.937616e+02  1.485443e+02  1.186640e+02\n[11]  8.449108e+01  4.651045e+01  1.331767e+01 -1.421085e-13 -1.739199e+01\n[16] -3.204147e+01 -6.782875e+01\n\n$x\nNULL\n\n$ac\n[1] 0\n\n$GOF\n[1] 0.4019251 0.4110603\n\n\nThe most interesting thing here is the GOF at the bottom, which is not that high (whichever one of the two values you take), suggesting that the two-dimensional representation is not very good. Further evidence for that is in eig, the “eigenvalues”, which are about 493, 232, 140, 50, 40, 0 and some negative ones, which suggests that three dimensions might be better than two for representing the data, because the first three eigenvalues seem noticeably bigger than the others. (This is the same thinking as the svd or percent of trace in discriminant analysis.) But that’s by the way: we’ll stick with two dimensions.\nThe important thing to remember is that if you go the eig=T way, you have to pull out the points to plot from the thing called points, so that you plot d.1 itself but d.1a$points.\n\\(\\blacksquare\\)\n\nObtain a vector of the names of the letters that were confused in this study.\n\nSolution\nEasiest way is to pull out the first column of the data frame that you read in from the file (if you can remember what it was called):\n\nletter_names &lt;- letters$X1\n\nWarning: Unknown or uninitialised column: `X1`.\n\n\nThat silly column name X1 that read_table supplied.\nExtra: You can even get the letter names from the thing I called d, but I can’t remember how, so I have to cheat. I said that d has a “print method”9 that controls how it looks:\n\nd\n\n                       innersurfaceofpinebark brick cardboard cork rubbereraser\nbrick                                      22                                  \ncardboard                                  23    27                            \ncork                                       24    27        18                  \nrubbereraser                               26    27        19   15             \nfelt                                       27    29        28   28           28\nleatherwallet                              26    29        23   25           20\nrigidplasticsheet                          23    28        24   26           27\nveryfinesandpaper                          24    16        24   28           24\nnylonscouringpad                           23    18        29   28           27\ncellulosekitchensponge                     23    28        27   20           24\nwovenstraw                                 18    25        28   27           25\nblockofstyrofoam                           23    24        21   10           19\nunglazedceramictile                        21    10        26   26           24\nvelvet                                     28    29        28   28           29\nwaxpaper                                   24    28        24   28           24\nglossypaintedwood                          22    27        23   29           28\n                       felt leatherwallet rigidplasticsheet veryfinesandpaper\nbrick                                                                        \ncardboard                                                                    \ncork                                                                         \nrubbereraser                                                                 \nfelt                                                                         \nleatherwallet            24                                                  \nrigidplasticsheet        28            22                                    \nveryfinesandpaper        29            28                27                  \nnylonscouringpad         26            28                29                21\ncellulosekitchensponge   26            27                29                24\nwovenstraw               29            26                27                26\nblockofstyrofoam         28            25                25                25\nunglazedceramictile      29            29                25                12\nvelvet                    4            24                29                29\nwaxpaper                 28            21                12                29\nglossypaintedwood        29            20                13                27\n                       nylonscouringpad cellulosekitchensponge wovenstraw\nbrick                                                                    \ncardboard                                                                \ncork                                                                     \nrubbereraser                                                             \nfelt                                                                     \nleatherwallet                                                            \nrigidplasticsheet                                                        \nveryfinesandpaper                                                        \nnylonscouringpad                                                         \ncellulosekitchensponge               22                                  \nwovenstraw                           16                     19           \nblockofstyrofoam                     25                     21         26\nunglazedceramictile                  24                     26         26\nvelvet                               27                     27         28\nwaxpaper                             29                     29         27\nglossypaintedwood                    28                     27         25\n                       blockofstyrofoam unglazedceramictile velvet waxpaper\nbrick                                                                      \ncardboard                                                                  \ncork                                                                       \nrubbereraser                                                               \nfelt                                                                       \nleatherwallet                                                              \nrigidplasticsheet                                                          \nveryfinesandpaper                                                          \nnylonscouringpad                                                           \ncellulosekitchensponge                                                     \nwovenstraw                                                                 \nblockofstyrofoam                                                           \nunglazedceramictile                  25                                    \nvelvet                               29                  29                \nwaxpaper                             26                  28     27         \nglossypaintedwood                    29                  26     26       12\n\n\nbut its innards are a whole lot more complicated than that:\n\nprint.default(d)\n\n  [1] 22 23 24 26 27 26 23 24 23 23 18 23 21 28 24 22 27 27 27 29 29 28 16 18 28\n [26] 25 24 10 29 28 27 18 19 28 23 24 24 29 27 28 21 26 28 24 23 15 28 25 26 28\n [51] 28 20 27 10 26 28 28 29 28 20 27 24 27 24 25 19 24 29 24 28 24 28 29 26 26\n [76] 29 28 29  4 28 29 22 28 28 27 26 25 29 24 21 20 27 29 29 27 25 25 29 12 13\n[101] 21 24 26 25 12 29 29 27 22 16 25 24 27 29 28 19 21 26 27 29 27 26 26 28 27\n[126] 25 25 29 26 29 29 28 26 27 26 12\nattr(,\"Labels\")\n [1] \"innersurfaceofpinebark\" \"brick\"                  \"cardboard\"             \n [4] \"cork\"                   \"rubbereraser\"           \"felt\"                  \n [7] \"leatherwallet\"          \"rigidplasticsheet\"      \"veryfinesandpaper\"     \n[10] \"nylonscouringpad\"       \"cellulosekitchensponge\" \"wovenstraw\"            \n[13] \"blockofstyrofoam\"       \"unglazedceramictile\"    \"velvet\"                \n[16] \"waxpaper\"               \"glossypaintedwood\"     \nattr(,\"Size\")\n[1] 17\nattr(,\"call\")\nas.dist.default(m = .)\nattr(,\"class\")\n[1] \"dist\"\nattr(,\"Diag\")\n[1] FALSE\nattr(,\"Upper\")\n[1] FALSE\n\n\nor\n\nunclass(d)\n\n  [1] 22 23 24 26 27 26 23 24 23 23 18 23 21 28 24 22 27 27 27 29 29 28 16 18 28\n [26] 25 24 10 29 28 27 18 19 28 23 24 24 29 27 28 21 26 28 24 23 15 28 25 26 28\n [51] 28 20 27 10 26 28 28 29 28 20 27 24 27 24 25 19 24 29 24 28 24 28 29 26 26\n [76] 29 28 29  4 28 29 22 28 28 27 26 25 29 24 21 20 27 29 29 27 25 25 29 12 13\n[101] 21 24 26 25 12 29 29 27 22 16 25 24 27 29 28 19 21 26 27 29 27 26 26 28 27\n[126] 25 25 29 26 29 29 28 26 27 26 12\nattr(,\"Labels\")\n [1] \"innersurfaceofpinebark\" \"brick\"                  \"cardboard\"             \n [4] \"cork\"                   \"rubbereraser\"           \"felt\"                  \n [7] \"leatherwallet\"          \"rigidplasticsheet\"      \"veryfinesandpaper\"     \n[10] \"nylonscouringpad\"       \"cellulosekitchensponge\" \"wovenstraw\"            \n[13] \"blockofstyrofoam\"       \"unglazedceramictile\"    \"velvet\"                \n[16] \"waxpaper\"               \"glossypaintedwood\"     \nattr(,\"Size\")\n[1] 17\nattr(,\"call\")\nas.dist.default(m = .)\nattr(,\"Diag\")\n[1] FALSE\nattr(,\"Upper\")\n[1] FALSE\n\n\nThis one gets rid of any special kind of thing that d is, and displays it like a thing without any special properties.\nIt’s the “attribute” called Labels that we need to grab:\n\nattributes(d)$Labels\n\n [1] \"innersurfaceofpinebark\" \"brick\"                  \"cardboard\"             \n [4] \"cork\"                   \"rubbereraser\"           \"felt\"                  \n [7] \"leatherwallet\"          \"rigidplasticsheet\"      \"veryfinesandpaper\"     \n[10] \"nylonscouringpad\"       \"cellulosekitchensponge\" \"wovenstraw\"            \n[13] \"blockofstyrofoam\"       \"unglazedceramictile\"    \"velvet\"                \n[16] \"waxpaper\"               \"glossypaintedwood\"     \n\n\n\\(\\blacksquare\\)\n\nPlot your multidimensional scaling map. To do this, first create a data frame containing the points you want to plot and their labels, and then plot the points labelled by the right thing.\n\nSolution\nThe “labels” need to be the letter names, which is why I made you find them in the previous part. I’m going to do this with a pipe, or else I’ll create another thing called d and overwrite the one I wanted to keep, again:\n\ndata.frame(d.1, names = letter_names)\n\nError in data.frame(d.1, names = letter_names): arguments imply differing number of rows: 17, 0\n\n\nSo far so good. The coordinates have gained names X1 and X2 (as they did before, but I wanted to check). This is what happens when you turn a matrix with nameless columns into a data frame. So I can proceed:\n\ndata.frame(d.1, names = letter_names) %&gt;%\n  ggplot(aes(x = X1, y = X2, label = names)) +\n  geom_point() + geom_text_repel() +\n  coord_fixed()\n\nError in data.frame(d.1, names = letter_names): arguments imply differing number of rows: 17, 0\n\n\nI need the last line for the same reason as before: I want to treat the two axes equally. If you don’t have that, you have a distorted map where one of the coordinates appears to be more important than the other.\nIf you can’t get ggrepel to behave itself, an equally good alternative is to plot the letter names instead of the labelled dots. To do that, take out the geom_point and add an ordinary geom_text. This will plot, at the location given by the x and y, the text in label, centred at the location. (If you have geom_point() as well, you’ll get a black dot in the middle of each piece of text.) geom_text has options to justify the text relative to the point, so you can see both, but I’ve always found these rather finicky, so I’d rather let geom_text_repel do the work of figuring out where to put the text relative to the point.\n\ndata.frame(d.1, names = letter_names) %&gt;%\n  ggplot(aes(x = X1, y = X2, label = names)) +\n  geom_text() + coord_fixed()\n\nError in data.frame(d.1, names = letter_names): arguments imply differing number of rows: 17, 0\n\n\nThen pick (in the next part) a pair of letters that is close together, and proceed. I like the labelled dots better (as a matter of taste), but this way is a perfectly good way to answer the question, so is perfectly acceptable here.\n\\(\\blacksquare\\)\n\nPick a pair of letters appear close together on your map. Does it make sense, from looking at these letters, that they would be easy to confuse? Explain briefly.\n\nSolution\nMy map has four pairs of close-together letters: C and G, H and M, N and W, Q and D. Pick one of those pairs. I don’t mind which pair you pick. (If you got something different, pick from what your map shows.) I think it is not at all surprising that these pairs of letters got confused, because the letters of each pair have similar shapes (which is all you’d have to react to if you see them for “a few milliseconds” each): C and G circular with a hole on the right, H, M, N and W vertical lines on the outside with something across the middle, Q and D almost circular. (It’s up to you whether you consider H, M, N and W as a group of four or as two pairs.)\n\\(\\blacksquare\\)\n\nVerify that your chosen pair of letters was often confused in the data.\n\nSolution\nThe data we read in from the file was this:\n\nletters\n\n\n\n  \n\n\n\nThen look for your pair of letters:\nThese are four of the biggest numbers in the matrix, which is as it should be. You need to find the number of confusions for your pair of letters and assert that it is large (compared to the others).\nThese are actually not all the large ones: M and W, H and N, M and N are also large (which lends some support to these letters being a foursome10 rather than two pairs).\nIf you prefer, you can work from your dist object, the thing I called d:\n\nd\n\n                       innersurfaceofpinebark brick cardboard cork rubbereraser\nbrick                                      22                                  \ncardboard                                  23    27                            \ncork                                       24    27        18                  \nrubbereraser                               26    27        19   15             \nfelt                                       27    29        28   28           28\nleatherwallet                              26    29        23   25           20\nrigidplasticsheet                          23    28        24   26           27\nveryfinesandpaper                          24    16        24   28           24\nnylonscouringpad                           23    18        29   28           27\ncellulosekitchensponge                     23    28        27   20           24\nwovenstraw                                 18    25        28   27           25\nblockofstyrofoam                           23    24        21   10           19\nunglazedceramictile                        21    10        26   26           24\nvelvet                                     28    29        28   28           29\nwaxpaper                                   24    28        24   28           24\nglossypaintedwood                          22    27        23   29           28\n                       felt leatherwallet rigidplasticsheet veryfinesandpaper\nbrick                                                                        \ncardboard                                                                    \ncork                                                                         \nrubbereraser                                                                 \nfelt                                                                         \nleatherwallet            24                                                  \nrigidplasticsheet        28            22                                    \nveryfinesandpaper        29            28                27                  \nnylonscouringpad         26            28                29                21\ncellulosekitchensponge   26            27                29                24\nwovenstraw               29            26                27                26\nblockofstyrofoam         28            25                25                25\nunglazedceramictile      29            29                25                12\nvelvet                    4            24                29                29\nwaxpaper                 28            21                12                29\nglossypaintedwood        29            20                13                27\n                       nylonscouringpad cellulosekitchensponge wovenstraw\nbrick                                                                    \ncardboard                                                                \ncork                                                                     \nrubbereraser                                                             \nfelt                                                                     \nleatherwallet                                                            \nrigidplasticsheet                                                        \nveryfinesandpaper                                                        \nnylonscouringpad                                                         \ncellulosekitchensponge               22                                  \nwovenstraw                           16                     19           \nblockofstyrofoam                     25                     21         26\nunglazedceramictile                  24                     26         26\nvelvet                               27                     27         28\nwaxpaper                             29                     29         27\nglossypaintedwood                    28                     27         25\n                       blockofstyrofoam unglazedceramictile velvet waxpaper\nbrick                                                                      \ncardboard                                                                  \ncork                                                                       \nrubbereraser                                                               \nfelt                                                                       \nleatherwallet                                                              \nrigidplasticsheet                                                          \nveryfinesandpaper                                                          \nnylonscouringpad                                                           \ncellulosekitchensponge                                                     \nwovenstraw                                                                 \nblockofstyrofoam                                                           \nunglazedceramictile                  25                                    \nvelvet                               29                  29                \nwaxpaper                             26                  28     27         \nglossypaintedwood                    29                  26     26       12\n\n\nThis time, you’re looking for a small dissimilarity between your pair of letters:\nThese, again, are smaller than most, though not the smallest overall. So, if you go this way, you need to assert that the corresponding number in your dist object is small.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mds.html#more-beer-please-1",
    "href": "mds.html#more-beer-please-1",
    "title": "36  Multidimensional Scaling",
    "section": "36.9 More beer please",
    "text": "36.9 More beer please\nPreviously, you did a cluster analysis of ten brands of beer, as rated by 32 students. This time, we will do a non-metric multidimensional scaling of those same brands of beer. The data are in link.\n\nNoting that we want to assess distances between brands of beer, read in the data and do whatever you need to do to work out distances between the beers. Show your result.\n\nSolution\nThis is really a copy of last time. We need to transpose the data frame to get the beers in rows (dist works on distances between rows), then feed everything but the student IDs into dist:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/beer.txt\"\nbeer &lt;- read_table(my_url)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  student = col_character(),\n  AnchorS = col_double(),\n  Bass = col_double(),\n  Becks = col_double(),\n  Corona = col_double(),\n  GordonB = col_double(),\n  Guinness = col_double(),\n  Heineken = col_double(),\n  PetesW = col_double(),\n  SamAdams = col_double(),\n  SierraN = col_double()\n)\n\nbeer\n\n\n\n  \n\n\nd &lt;- beer %&gt;%\n  dplyr::select(-student) %&gt;%\n  t() %&gt;%\n  dist()\n\nI did it the funny-looking way. The cluster analysis question offers an alternative.\n\\(\\blacksquare\\)\n\nObtain a non-metric multidimensional scaling of the beers. (Comment coming up in a moment.)\n\nSolution\n\nlibrary(MASS)\nbeer.1 &lt;- isoMDS(d)\n\ninitial  value 13.344792 \niter   5 value 10.855662\niter  10 value 10.391446\nfinal  value 10.321949 \nconverged\n\n\n\\(\\blacksquare\\)\n\nObtain the stress value of the map, and comment on it.\n\nSolution\n\nbeer.1$stress\n\n[1] 10.32195\n\n\nThe stress is around 10%, on the boundary between “good” and “fair”. It seems as if the map should be more or less worth using. (Insert your own hand-waving language here.)\n\\(\\blacksquare\\)\n\nObtain a map of the beers, labelled with the names of the beers.\n\nSolution\nThis is slightly different from class, where I plotted the languages actually at their locations. But here, the beer names are longer, so we should plot the points and label them. I’d make a data frame first, and probably pipe it into the plot, thus. Don’t forget we have to get the names of the 10 beers, and not the 32 students! The names of the columns of the data frame include an identifier column for the students, so skip the first one:\n\nbeer_names &lt;- beer %&gt;% dplyr::select(-student) %&gt;% names()\nbeer_names\n\n [1] \"AnchorS\"  \"Bass\"     \"Becks\"    \"Corona\"   \"GordonB\"  \"Guinness\"\n [7] \"Heineken\" \"PetesW\"   \"SamAdams\" \"SierraN\" \n\ndata.frame(beer.1$points, beer = beer_names) %&gt;%\n  ggplot(aes(x = X1, y = X2, label = beer)) +\n  geom_point() + geom_text_repel()\n\n\n\n\n\\(\\blacksquare\\)\n\nFind a pair of beers close together on your map. Are they similar in terms of student ratings? Explain briefly.\n\nSolution\nI think Sam Adams and Gordon Biersch, right in the middle of the map. We can pull them out by name:\n\nbeer %&gt;% dplyr::select(SamAdams, GordonB)\n\n\n\n  \n\n\n\nThese are, with a few exceptions (the most glaring being the 18th student), within a couple of points of each other. So I would say they are similar. Another way to show this is to make a scatterplot of them, and draw on it the line where the ratings are the same. Since the ratings are whole numbers, they are likely to be duplicated, so I “jitter” them as I plot them, to prevent overplotting:\n\nggplot(beer, aes(x = SamAdams, y = GordonB)) + geom_jitter() +\n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\nAre they close to the line, or not? Dunno. Maybe I should plot the ratings of some far-apart beers, and see whether it looks any different, for example Becks and SierraN:\n\nggplot(beer, aes(x = Becks, y = SierraN)) + geom_jitter() +\n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\nNot much. I was expecting something a lot more impressive. Another way is to summarize the rating differences for the pairs of beers:\n\ndiff &lt;- with(beer, SamAdams - GordonB)\nsummary(diff)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-3.0000 -1.0000  0.0000  0.1562  1.0000  6.0000 \n\n\nand\n\ndiff &lt;- with(beer, Becks - SierraN)\nsummary(diff)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -8.00   -4.00   -1.50   -1.75    0.00    3.00 \n\n\nNow we see something: two beers that are similar ought to have a median rating difference close to 0 most of the time. That is the case for the first pair; the median is zero, and the IQR is only 2. But for the second pair, Becks is typically rated lower than Sierra Nevada (negative median) and the IQR is larger (4 rather than 2).\n\\(\\blacksquare\\)\n\nIn our cluster analysis, we found that Anchor Steam, Pete’s Wicked Ale, Guinness and Sierra Nevada were all in the same cluster. Would you expect them to be close together on your map? Are they? Explain briefly.\n\nSolution\nIf they are in the same cluster, we would expect them to “cluster” together on the map. Except that they don’t, really. These are the four beers over on the right of our map. They are kind of in the same general neighbourhood, but not really what you would call close together. (This is a judgement call, again.) In fact, none of the beers, with the exception of Sam Adams and Gordon Biersch in the centre, are really very close to any of the others.\nThat may be the story, in the end. With the cluster analysis, we were forcing the clustering to happen, whether it was really there or not. We haven’t seen a test for the “significance” of clusters, so the clusters we found may not mean very much.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mds.html#feeling-similar-again-1",
    "href": "mds.html#feeling-similar-again-1",
    "title": "36  Multidimensional Scaling",
    "section": "36.10 Feeling similar, again",
    "text": "36.10 Feeling similar, again\nPreviously, we looked at an experiment about which objects feel similar to one another and which ones feel different.\n30 subjects were each given 17 different objects to feel, for example “inner surface of pine bark”, “rubber eraser” and “nylon scouring pad”. The subjects had to group the objects into a maximum of 8 groups such that the objects within a group felt similar, and the ones in different groups felt different. A dissimilarity matrix was formed by counting how many subjects put each pair of objects into different groups, so that the dissimilarity ranged from 0 (the objects were always put together in the same group) to 30 (the objects were not put together into the same group by any of the subjects).\nThe data are in link. These have no column names.\nThis time we are going to try non-metric multidimensional scaling, to see whether that produces a more reasonable map. The reading in of the data is the same as before (so I have reduced the marks given for it).\n\nObtain and display the distance matrix that you used last time for these data. (I don’t care whether you run all the code again, or whether you just produce the matrix from where you had it before on R Studio. Correct is all I care about.)\n\nSolution\nCopied and pasted from last time:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/stimuli.txt\"\nstimuli &lt;- read_delim(my_url, \" \", col_names = F)\nobjects &lt;- stimuli$X1\nnames(stimuli) &lt;- c(\"object\", objects)\nstimuli\n\n\n\n  \n\n\n\nThat gets a data frame with the right column names. Then:\n\nd &lt;- stimuli %&gt;% dplyr::select(-1) %&gt;% as.dist()\n\nand just to check:\n\nw &lt;- getOption(\"width\")\noptions(width = 132)\nd\n\n                       innersurfaceofpinebark brick cardboard cork rubbereraser felt leatherwallet rigidplasticsheet\nbrick                                      22                                                                       \ncardboard                                  23    27                                                                 \ncork                                       24    27        18                                                       \nrubbereraser                               26    27        19   15                                                  \nfelt                                       27    29        28   28           28                                     \nleatherwallet                              26    29        23   25           20   24                                \nrigidplasticsheet                          23    28        24   26           27   28            22                  \nveryfinesandpaper                          24    16        24   28           24   29            28                27\nnylonscouringpad                           23    18        29   28           27   26            28                29\ncellulosekitchensponge                     23    28        27   20           24   26            27                29\nwovenstraw                                 18    25        28   27           25   29            26                27\nblockofstyrofoam                           23    24        21   10           19   28            25                25\nunglazedceramictile                        21    10        26   26           24   29            29                25\nvelvet                                     28    29        28   28           29    4            24                29\nwaxpaper                                   24    28        24   28           24   28            21                12\nglossypaintedwood                          22    27        23   29           28   29            20                13\n                       veryfinesandpaper nylonscouringpad cellulosekitchensponge wovenstraw blockofstyrofoam unglazedceramictile\nbrick                                                                                                                           \ncardboard                                                                                                                       \ncork                                                                                                                            \nrubbereraser                                                                                                                    \nfelt                                                                                                                            \nleatherwallet                                                                                                                   \nrigidplasticsheet                                                                                                               \nveryfinesandpaper                                                                                                               \nnylonscouringpad                      21                                                                                        \ncellulosekitchensponge                24               22                                                                       \nwovenstraw                            26               16                     19                                                \nblockofstyrofoam                      25               25                     21         26                                     \nunglazedceramictile                   12               24                     26         26               25                    \nvelvet                                29               27                     27         28               29                  29\nwaxpaper                              29               29                     29         27               26                  28\nglossypaintedwood                     27               28                     27         25               29                  26\n                       velvet waxpaper\nbrick                                 \ncardboard                             \ncork                                  \nrubbereraser                          \nfelt                                  \nleatherwallet                         \nrigidplasticsheet                     \nveryfinesandpaper                     \nnylonscouringpad                      \ncellulosekitchensponge                \nwovenstraw                            \nblockofstyrofoam                      \nunglazedceramictile                   \nvelvet                                \nwaxpaper                   27         \nglossypaintedwood          26       12\n\noptions(width = w)\n\n\\(\\blacksquare\\)\n\nObtain a non-metric multidimensional scaling map of the objects. (No plotting or comments yet.)\n\nSolution\n\nstimuli.1 &lt;- isoMDS(d)\n\ninitial  value 25.537169 \niter   5 value 20.521473\nfinal  value 20.216103 \nconverged\n\n\nI’m going to remind myself of what this has in it:\n\nnames(stimuli.1)\n\n[1] \"points\" \"stress\"\n\n\nThis is rather like running cmdscale with eig=T: a thing called points with coordinates to plot, and a measure of fit, here called stress, with, as you’d guess, a smaller stress being better.\n\\(\\blacksquare\\)\n\n* Obtain a number that says how well the map reproduces the distances in the data, and comment on that number.\n\nSolution\nWhat you need here is the “stress”:\n\nstimuli.1$stress\n\n[1] 20.2161\n\n\nThis is just over 20%, which is described in the notes as “poor”. We should thus be skeptical about the map that this produces.\n\\(\\blacksquare\\)\n\nPlot the non-metric multidimensional scaling map. Label each point with its (full) object name, sized suitably.\n\nSolution\nMake a data frame first of things to plot, the points and the names. I’m going to plot the names smaller). I am not naming this data frame d, in an attempt to avoid overwriting things I want to keep:\n\nstimuli.1.d &lt;- data.frame(stimuli.1$points, names = objects)\nstimuli.1.d\n\n\n\n  \n\n\n\nThe repeated stimuli down the left are row names, but since they’re row names, we won’t be able to use them in ggplot.11\nRemember that we are turning a matrix and a column into a data frame, so we need either the more forgiving data.frame, or to turn points into a data frame first, which would go like this:\n\nas_tibble(stimuli.1$points) %&gt;%\n  mutate(name = objects)\n\n\n\n  \n\n\n\nThis time, the columns are called V1 and V2, since that’s what as_tibble does. Also note the slightly different look: fewer decimals, since displaying a tibble rounds numerical things to three significant digits. Back to my data frame stimuli.1.d. The points have acquired names X1 and X2, as usual, so we have all we need:\n\nggplot(stimuli.1.d, aes(x = X1, y = X2, label = names)) +\n  geom_point() +\n  geom_text_repel(size = 2) +\n  coord_fixed()\n\n\n\n\nI think the full names are good with the text this small.\nActually, this came out a lot like the metric multidimensional scaling that we did earlier.\nI was expecting a greater difference. See whether you can find out what moved, if anything.\n\\(\\blacksquare\\)\n\n* Find all the distances in your distance matrix that are 10 or less (there should be three of them). Find these pairs of objects on your map. Describe where they are on your map. Do they appear to be the three closest pairs of objects?\n\nSolution\nThese are:\n\nvelvet and felt (distance 4). On the map, close together at the top.\nblock of styrofoam and cork (distance 10). On the map, close together at the bottom (though they appear to be slightly farther apart than rubbereraser and cardboard).\nunglazed ceramic tile and brick (distance 10). On the map, on the left but not especially close together (for example, very fine sandpaper is in between them and thus closer to both).\n\nI would say that they are not anything like the three closest pairs of objects on the map. Consider, for example, and wax paper, or and unglazed ceramic tile, or and cardboard, all of which are closer together on the map than the three pairs of objects with distances 10 or less. Three points for saying something about the three pairs of objects of distance 10 or less, and one point for making some relevant comment about whether these are the three closest pairs on the map, eg. by finding one pair (or more than one) of objects on the map that are closer.\n\\(\\blacksquare\\)\n\nConsider again your conclusions in parts (here) and (here). Explain briefly how they are consistent.\n\nSolution\nThe stress is rather high, which means that the map distances and actual distances may not correspond very well. This is also exactly what we found in the last part, by finding one (or two) pairs of objects that were really close in actuality, but not so much on the map. The point here is that a high stress means that distances in real life and on the map won’t correspond very well. That’s what I want you to say. Extra: another way to assess this is with a Shepard12 diagram. That would go like this:\n\nstimuli.sh &lt;- Shepard(d, stimuli.1$points)\nnames(stimuli.sh)\n\n[1] \"x\"  \"y\"  \"yf\"\n\n\nIn here, we plot the actual distances x against the map distances y, making a data frame first:\n\nwith(stimuli.sh, data.frame(x = x, y = y)) %&gt;%\n  ggplot(aes(x = x, y = y)) + geom_point()\n\n\n\n\nThe actual distances are on the \\(x\\)-axis and the map distances are on the \\(y\\) axis, as far as I can tell (the help file is not clear on this). But I think I am right, since a lot of the actual distances were 28 or 29. Notice how the upward trend, such as it is, is very fuzzy: when the actual distance is very large, the map distance could be almost anything.\n\\(\\blacksquare\\)\n\nWould a three-dimensional map work better than a two-dimensional one, bearing in mind that a three-dimensional map will need something like rgl to interpret? Calculate something that will help you decide, and explain what you think.\n\nSolution\nThe calculation part is to figure out the stress value for a three-dimensional map, and compare it with the 20% that we had before. k=3 gets the three dimensions:\n\nisoMDS(d, k = 3)$stress\n\ninitial  value 14.934110 \niter   5 value 11.280474\niter  10 value 10.094796\niter  15 value 9.938456\nfinal  value 9.859657 \nconverged\n\n\n[1] 9.859657\n\n\nWhat this does is to obtain the map and throw all of it away except for the stress value.13\nI think this stress value, just under 10%, or on the “fair” end of “good”, is a worthwhile improvement over the just-over-20% that we had in two dimensions. So I think this is worth the trouble of having to use a 3-dimensional plotting tool like rgl to interpret it with. You might agree with me, or you might not: for example, you might be looking for a bigger improvement. Either is good, as far as I’m concerned, as long as your answer does something to balance the smaller stress value14 with the difficulty of interpreting it.\nThe kind of rgl code you’ll need is something like this (not tested):\n\nlibrary(rgl)\nstimuli.2 &lt;- isoMDS(d, k = 3)$points\nplot3d(stimuli.2)\ntext3d(stimuli.2, text = object.abb)\n\nWhat you should find, looking at this (try it!) is that objects close together in this 3-dimensional place are more nearly close together in actuality as well, because of the in my opinion notably smaller stress value.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "mds.html#footnotes",
    "href": "mds.html#footnotes",
    "title": "36  Multidimensional Scaling",
    "section": "",
    "text": "Probably students in a psychology course. You know how it goes.↩︎\nThe maximum number of groups was to ensure that each subject actually did group some objects together, rather than saying that they all feel different.↩︎\nAs it ought to be, since there is a real answer here: the cities do exist as locations in two dimensions, if you ignore the curvature of the earth. The goodness of fit isn’t 100% because the roads bend a bit.↩︎\nProbably students in a psychology course. You know how it goes.↩︎\nThe maximum number of groups was to ensure that each subject actually did group some objects together, rather than saying that they all feel different.↩︎\nI lie. Last year, I got cork at the top, and a lot of other people got cork at the bottom as you see here.↩︎\nThe first association that unglazed made for me was donut!↩︎\nOxford semicolon, for the grammar mavens among you.↩︎\nThe way that a multicoloured function like print works is that when you ask to show something, like d, R first looks to see what kind of thing you want to show (by calling class), and determines that it is a dist object. Then it looks to see if there is a function called print.dist (there is), and if there is, it calls that (to produce that nice display). If there isn’t, it calls print.default, which just displays its input without doing anything special. This is why printing the output from an lm looks very different from printing a data frame: the first calls print.lm and the second calls print.data.frame, or print.tbl_df for a tibble.↩︎\nSet of four. Match-play golf has a game called foursomes where the two players on each team take it in turns to hit the ball, as opposed to the game called fourballs, where each of the two players plays their own ball, and the team’s score on a hole is the better of the two players’ scores.↩︎\nIn the future, I’ll make everyone turn matrices into data frames first so that we won’t deal with old-fashioned data frames, row names and the like.↩︎\nThis always makes me think of the number 985 bus, but that’s the wrong spelling. Spelled like this, this is also the name of the illustrator of the original pre-Disney Winnie-the-Pooh stories. See here.↩︎\nWhich seems like a very SAS way of doing things.↩︎\nUsing more dimensions will certainly decrease the stress, in the same way that adding an \\(x\\)-variable to a regression will increase R-squared; it’s the same issue, of whether the change is big enough to be worth having.↩︎"
  },
  {
    "objectID": "pc.html#the-weather-somewhere",
    "href": "pc.html#the-weather-somewhere",
    "title": "37  Principal Components",
    "section": "37.1 The weather, somewhere",
    "text": "37.1 The weather, somewhere\nThe data in link is of the weather in a certain location: daily weather records for 2014. The variables are:\n\nday of the year (1 through 365)\nday of the month\nnumber of the month\nseason\nlow temperature (for the day)\nhigh temperature\naverage temperature\ntime of the low temperature\ntime of the high temperature\nrainfall (mm)\naverage wind speed\nwind gust (highest wind speed)\ntime of the wind gust\nwind direction\n\n\nRead in the data, and create a data frame containing only the temperature variables, the rainfall and the wind speed variables (the ones that are actual numbers, not times or text). Display the first few lines of your data frame.\nFind five-number summaries for each column by running quantile on all the columns of the data frame (at once, if you can).\nRun a principal components analysis (on the correlation matrix).\nObtain a summary of your principal components analysis. How many components do you think are worth investigating?\nMake a scree plot. Does this support your conclusion from the previous part?\nObtain the component loadings. How do the first three components depend on the original variables? (That is, what kind of values for the original variables would make the component scores large or small?)\nObtain the principal component scores, for as many components as you think are reasonable, and display the first 20 of them for each component alongside the other variables in your data frame.\nFind a day that scores low on component 1, and explain briefly why it came out that way (by looking at the measured variables).\nFind a day that scores high on component 2, and explain briefly why it came out that way.\nFind a day that scores high on component 3, and explain briefly why it came out high.\nMake a biplot of these data, labelling the days by the day count (from 1 to 365). You may have to get the day count from the original data frame that you read in from the file. You can shrink the day numbers to make them overwrite each other (a bit) less.\nLooking at your biplot, what do you think was remarkable about the weather on day 37? Day 211? Confirm your guesses by looking at the appropriate rows of your data frame (and comparing with your summary from earlier).\n\nMy solutions follow:"
  },
  {
    "objectID": "pc.html#the-weather-somewhere-1",
    "href": "pc.html#the-weather-somewhere-1",
    "title": "37  Principal Components",
    "section": "37.2 The weather, somewhere",
    "text": "37.2 The weather, somewhere\nThe data in link is of the weather in a certain location: daily weather records for 2014. The variables are:\n\nday of the year (1 through 365)\nday of the month\nnumber of the month\nseason\nlow temperature (for the day)\nhigh temperature\naverage temperature\ntime of the low temperature\ntime of the high temperature\nrainfall (mm)\naverage wind speed\nwind gust (highest wind speed)\ntime of the wind gust\nwind direction\n\n\nRead in the data, and create a data frame containing only the temperature variables, the rainfall and the wind speed variables (the ones that are actual numbers, not times or text). Display the first few lines of your data frame.\n\nSolution\nRead into a temporary data frame, and then process:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/weather_2014.csv\"\nweather.0 &lt;- read_csv(my_url)\nweather.0\n\n\n\n  \n\n\n\nThere are lots of columns, of which we only want a few:\n\n(weather.0 %&gt;% select(l.temp:ave.temp, rain:gust.wind) -&gt; weather)\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nFind five-number summaries for each column by running quantile on all the columns of the data frame (at once, if you can).\n\nSolution\nI think this is the easiest way:\n\nweather %&gt;% \n  summarize(across(everything(), \\(x) quantile(x)))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n\n\n  \n\n\n\nThis loses the actual percents of the percentiles of the five-number summary (because they are “names” of the numerical result, and the tidyverse doesn’t like names.) I think you can see which percentile is which, though.\nAnother way to do it is to make a column of column names, using pivot_longer, and then use nest and list-columns to find the quantiles for each variable:\n\nweather %&gt;%\n  pivot_longer(everything(), names_to=\"xname\", values_to=\"x\") %&gt;%\n  nest_by(xname) %&gt;%\n  mutate(q = list(enframe(quantile(data$x)))) %&gt;%\n  unnest(q) %&gt;%\n  pivot_wider(names_from=name, values_from=value) %&gt;%\n  select(-data)\n\n\n\n  \n\n\n\nThat was a lot of work, but it depends on how you see it when you’re coding it. You should investigate this one line at a time, but the steps are:\n\ncreate a “long” data frame with one column of variable names and a second with the values for that variable\nmake mini-data-frames data containing everything but xname: that is, one column x with the values for that variable.\nfor each mini-data-frame, work out the quantiles of its x. The enframe saves the labels for what percentiles they are.\nUnnest this to make a long data frame with one row for each quantile for each variable.\nput the variable names in rows and the percentiles in columns.\n\n\\(\\blacksquare\\)\n\nRun a principal components analysis (on the correlation matrix).\n\nSolution\n\nweather.1 &lt;- princomp(weather, cor = T)\n\n\\(\\blacksquare\\)\n\nObtain a summary of your principal components analysis. How many components do you think are worth investigating?\n\nSolution\n\nsummary(weather.1)\n\nImportance of components:\n                          Comp.1    Comp.2     Comp.3     Comp.4     Comp.5\nStandard deviation     1.7830875 1.4138296 0.74407069 0.38584917 0.33552998\nProportion of Variance 0.5299001 0.3331524 0.09227353 0.02481326 0.01876339\nCumulative Proportion  0.5299001 0.8630525 0.95532604 0.98013930 0.99890270\n                            Comp.6\nStandard deviation     0.081140732\nProportion of Variance 0.001097303\nCumulative Proportion  1.000000000\n\n\nThe issue is to see where the standard deviations are getting small (after the second component, or perhaps the third one) and to see where the cumulative proportion of variance explained is acceptably high (again, after the second one, 86%, or the third, 95%).\n\\(\\blacksquare\\)\n\nMake a scree plot. Does this support your conclusion from the previous part?\n\nSolution\nggscreeplot from ggbiplot:\n\nggscreeplot(weather.1)\n\n\n\n\nI see elbows at 3 and at 4. Remember you want to be on the mountain for these, not on the scree, so this suggests 2 or 3 components, which is exactly what we got from looking at the standard deviations and cumulative variance explained.\nThe eigenvalue-greater-than-1 thing (that is, the “standard deviation” in the summary being greater than 1) says 2 components, rather than 3.\n\\(\\blacksquare\\)\n\nObtain the component loadings. How do the first three components depend on the original variables? (That is, what kind of values for the original variables would make the component scores large or small?)\n\nSolution\n\nweather.1$loadings\n\n\nLoadings:\n          Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6\nl.temp     0.465  0.348         0.542  0.470  0.379\nh.temp     0.510  0.231        -0.576 -0.381  0.458\nave.temp   0.502  0.311                      -0.804\nrain      -0.296  0.397  0.853        -0.163       \nave.wind  -0.253  0.560 -0.463  0.357 -0.529       \ngust.wind -0.347  0.507 -0.230 -0.492  0.572       \n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6\nSS loadings     1.000  1.000  1.000  1.000  1.000  1.000\nProportion Var  0.167  0.167  0.167  0.167  0.167  0.167\nCumulative Var  0.167  0.333  0.500  0.667  0.833  1.000\n\n\n\n1: This component loads mainly (and positively) on the temperature variables, so when temperature is high, component 1 is high. You could also say that it loads negatively on the other variables, in which case component 1 is high if the temperature variables are low and the rain and wind variables are high.\n2: This one loads most heavily, positively, on wind: when wind is high, component 2 is high. Again, you can make the judgement call that the other variables also feature in component 2, so that when everything is large, component 2 is large and small with small.\n3: This one is a bit clearer. The blank loadings are close to 0, and can be ignored. The main thing in component 3 is rain: when rainfall is large, component 3 is large. Or, if you like, it is large (positive) when rainfall is large and wind is small.\n\nThe interpretation here is kind of muffled, because each component has bits of everything. One of the advantages of factor analysis that we see in class later is that you can do a “rotation” so that each variable (to a greater extent) is either in a factor or out of it. Such a varimax rotation is the default for factanal, which I presume I now have to show you (so this is looking ahead):\n\nweather.2 &lt;- factanal(weather, 3, scores = \"r\")\nweather.2$loadings\n\n\nLoadings:\n          Factor1 Factor2 Factor3\nl.temp     0.964          -0.230 \nh.temp     0.939  -0.203   0.267 \nave.temp   0.992  -0.101         \nrain      -0.147   0.604         \nave.wind           0.864         \ngust.wind -0.144   0.984         \n\n               Factor1 Factor2 Factor3\nSS loadings      2.839   2.131   0.140\nProportion Var   0.473   0.355   0.023\nCumulative Var   0.473   0.828   0.852\n\n\nThese are a lot less ambiguous: factor 1 is temperature, factor 2 is rain and wind, and factor 3 is large (positive) if the high temperature is high or the low temperature is low: that is, if the high temperature was especially high relative to the low temperature (or, said differently, if the temperature range was high).\nThese factors are rather pleasantly interpretable.\nggbiplot mysteriously doesn’t handle factor analyses, so we have to go back to the base-graphics version, which goes a bit like this:\n\nbiplot(weather.2$scores, weather.2$loadings)\n\n\n\n\nNow you see that the factors are aligned with the axes, and it’s very clear what the factors “represent”. (You don’t see much else, in all honesty, but you see at least this much.)\n\\(\\blacksquare\\)\n\nObtain the principal component scores, for as many components as you think are reasonable, and display the first 20 of them for each component alongside the other variables in your data frame.\n\nSolution\nSomething like this. I begin by turning the component scores (which are a matrix) into a data frame, and selecting the ones I want (the first three):\n\nas_tibble(weather.1$scores) %&gt;%\n  select(1:3) %&gt;%\n  bind_cols(weather) %&gt;%\n  mutate(day = row_number()) -&gt; d\nd \n\n\n\n  \n\n\n\nI just did the first three scores. I made a column day so that I can see which day of the year I am looking at (later).\n\\(\\blacksquare\\)\n\nFind a day that scores low on component 1, and explain briefly why it came out that way (by looking at the measured variables).\n\nSolution\nWe can do this one and the ones following by running arrange appropriately:\n\nd %&gt;% arrange(Comp.1)\n\n\n\n  \n\n\n\nDay 40 has the lowest component 1 score. This is one of the cooler days. Also, there is a largish amount of rain and wind. So low temperature, high rain and wind. Some of the other days on my list were cooler than day 4, but they had less rain and less wind.\n\\(\\blacksquare\\)\n\nFind a day that scores high on component 2, and explain briefly why it came out that way.\n\nSolution\n\nd %&gt;% arrange(desc(Comp.2))\n\n\n\n  \n\n\n\nDay 37. These are days when the wind speed (average or gust) is on the high side.\n\\(\\blacksquare\\)\n\nFind a day that scores high on component 3, and explain briefly why it came out high.\n\nSolution\n\nd %&gt;% arrange(desc(Comp.3))\n\n\n\n  \n\n\n\nDay 307. Component 3 was mainly rain, so it is not surprising that the rainfall is the highest on this day.\n\\(\\blacksquare\\)\n\nMake a biplot of these data, labelling the days by the day count (from 1 to 365). You may have to get the day count from the original data frame that you read in from the file. You can shrink the day numbers to make them overwrite each other (a bit) less.\n\nSolution\nggbiplot. I did some digging in the help file to figure out how to label the points by a variable and how to control the size of the labels, and I also went digging in the data frame that I read in from the file to get the count of the day in the year, which was called day.count:\n\nggbiplot(weather.1, labels = weather.0$day.count, labels.size = 2)\n\n\n\n\nI think the label text is small enough, though you could make it smaller. I’ll be asking you to look at some extreme points in a moment, so those are the only ones you’ll need to be able to disentangle.\nThe variables divide into two groups: the temperature ones, that point to about 2 o’clock, and the wind and rain ones, that point to about 11 o’clock. These are not straight up or down or across, so they all feature in both components: component 1 is mostly temperature, but has a bit of wind/rain in it, while component 2 is mostly wind/rain with a bit of temperature in it. You might be wondering whether things couldn’t be rotated so that, say, the temperature variables go across and the rain/wind ones go down, which means you’d have a temperature component and a rain/wind component. This is what factor analysis does, and I think I did that earlier (and this is what we found).\n\\(\\blacksquare\\)\n\nLooking at your biplot, what do you think was remarkable about the weather on day 37? Day 211? Confirm your guesses by looking at the appropriate rows of your data frame (and comparing with your summary from earlier).\n\nSolution\nDay 37 is at the top left of the plot, at the pointy end of the arrows for rain, wind gust and average wind. This suggests a rainy, windy day:\n\nweather %&gt;% slice(37)\n\n\n\n  \n\n\n\nThose are high numbers for both rain and wind (the highest for average wind and above the third quartile otherwise), but the temperatures are unremarkable.\nDay 211 is towards the pointy end of the arrows for temperature, so this is a hot day:\n\nweather %&gt;% slice(211)\n\n\n\n  \n\n\n\nThis is actually the hottest day of the entire year: day 211 is highest on all three temperatures, while the wind speeds are right around average (and no rain is not completely surprising).\nI can do a couple more. Points away from the pointy end of the arrows are low on the variables in question, for example day 265:\n\nweather %&gt;% slice(265)\n\n\n\n  \n\n\n\nThis is not really low rain, but it is definitely low wind. What about day 47?\n\nweather %&gt;% slice(47)\n\n\n\n  \n\n\n\nThis is predominantly low on temperature. In fact, it is kind of low on wind and rain too.1 This makes sense, because not only is it at the “wrong” end of the temperature arrows, it is kind of at the wrong end of the wind/rain arrows as well.\nHaving done these by percentile ranks in one of the other questions, let’s see if we can do that here as well:\n\nweather %&gt;% mutate(across(everything(), \\(x) percent_rank(x))) %&gt;% \n    slice(c(37, 211, 265, 47))\n\n\n\n  \n\n\n\nThe idea here is that we want to replace all the data values by the percent-rank version of themselves, rather than summarizing them as we have done before. That’s what using an across inside a mutate will do.2\nThese are:\n\nDay 37: highly rainy and windy (and below average, but not remarkably so, on temperature).\nDay 211: the highest or near-highest temperature, no rain but unremarkable for wind.\nDay 265: Lowest for wind (and above Q3 for low temperature and rain).\nDay 47: Lowest or near-lowest temperature.\n\nThe advantage to doing it this way is that you don’t need a separate five-number summary for each variable; the percentile ranks give you a comparison with quartiles (or any other percentile of interest to you). In case you are wondering where this is: I was doing a presentation using these data to some Environmental Science grad students, and I had them guess where it was. The temperatures for the whole year are warm-temperate, with a smallish range, and sometimes a lot of rain. This suggests a maritime climate. I gave the additional clues of “western Europe” and “this place’s soccer team plays in blue and white striped shirts”. The temperatures have about the right range low-to-high for Britain, but are too warm. Which suggests going south: perhaps Brittany in France, but actually the west coast of the Iberian peninsula: Porto, in northern Portugal, with the weather blowing in off the Atlantic.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "pc.html#footnotes",
    "href": "pc.html#footnotes",
    "title": "37  Principal Components",
    "section": "",
    "text": "If you ignore the wind gust, anyway.↩︎\nThere are also options to keep the original variables, and to give the new ones new names.↩︎"
  },
  {
    "objectID": "fa.html#the-interpersonal-circumplex",
    "href": "fa.html#the-interpersonal-circumplex",
    "title": "38  Factor Analysis",
    "section": "38.1 The Interpersonal Circumplex",
    "text": "38.1 The Interpersonal Circumplex\nThe “IPIP Interpersonal Circumplex” (see link) is a personal behaviour survey, where respondents have to rate how accurately a number of statements about behaviour apply to them, on a scale from 1 (“very inaccurate”) to 5 (“very accurate”). A survey was done on 459 people, using a 44-item variant of the above questionnaire, where the statements were as follows. Put an “I” or an “I am” in front of each one:\n\ntalkative\nfind fault\ndo a thorough job\ndepressed\noriginal\nreserved\nhelpful\ncareless\nrelaxed\ncurious\nfull of energy\nstart quarrels\nreliable\ntense\ningenious\ngenerate enthusiasm in others\nforgiving\ndisorganized\nworry\nimaginative\nquiet\ntrusting\nlazy\nemotionally stable\ninventive\nassertive\ncold and aloof\npersevere\nmoody\nvalue artistic experiences\nshy\nconsiderate\nefficient\ncalm in tense situations\nprefer routine work\noutgoing\nsometimes rude\nstick to plans\nnervous\nreflective\nhave few artistic interests\nco-operative\ndistractible\nsophisticated in art and music\n\nI don’t know what a “circumplex” is, but I know it’s not one of those “hat” accents that they have in French. The data are in link. The columns PERS01 through PERS44 represent the above traits.\n\nRead in the data and check that you have the right number of rows and columns.\nThere are some missing values among these responses. Eliminate all the individuals with any missing values (since princomp can’t handle them).\nCarry out a principal components analysis and obtain a scree plot.\nHow many components/factors should you use? Explain briefly.\n* Using your preferred number of factors, run a factor analysis. Obtain “r”-type factor scores, as in class. You don’t need to look at any output yet.\nObtain the factor loadings. How much of the variability does your chosen number of factors explain?\nInterpret each of your chosen number of factors. That is, for each factor, identify the items that load heavily on it (you can be fairly crude about this, eg. use a cutoff like 0.4 in absolute value), and translate these items into the statements given in each item. Then, if you can, name what the items loading heavily on each factor have in common. Interpret a negative loading as “not” whatever the item says.\nFind a person who is extreme on each of your first three factors (one at a time). For each of these people, what kind of data should they have for the relevant ones of variables PERS01 through PERS44? Do they? Explain reasonably briefly.\nCheck the uniquenesses. Which one(s) seem unusually high? Check these against the factor loadings. Are these what you would expect?"
  },
  {
    "objectID": "fa.html#a-correlation-matrix",
    "href": "fa.html#a-correlation-matrix",
    "title": "38  Factor Analysis",
    "section": "38.2 A correlation matrix",
    "text": "38.2 A correlation matrix\nHere is a correlation matrix between five variables. This correlation matrix was based on \\(n=50\\) observations. Save the data into a file.\n\n1.00 0.90 -0.40 0.28 -0.05\n0.90 1.00 -0.60 0.43 -0.20\n-0.40 -0.60 1.00 -0.80 0.40\n0.28 0.43 -0.80 1.00 -0.70\n-0.05 -0.20 0.40 -0.70 1.00\n\n\nRead in the data, using col_names=F (why?). Check that you have five variables with names invented by R.\nRun a principal components analysis from this correlation matrix.\n* Obtain a scree plot. Can you justify the use of two components (later, factors), bearing in mind that we have only five variables?\nTake a look at the first two component loadings. Which variables appear to feature in which component? Do they have a positive or negative effect?\nCreate a “covariance list” (for the purposes of performing a factor analysis on the correlation matrix).\nCarry out a factor analysis with two factors. We’ll investigate the bits of it in a moment.\n* Look at the factor loadings. Describe how the factors are related to the original variables. Is the interpretation clearer than for the principal components analysis?\nLook at the uniquenesses. Are there any that are unusually high? Does that surprise you, given your answer to (here)? (You will probably have to make a judgement call here.)"
  },
  {
    "objectID": "fa.html#air-pollution",
    "href": "fa.html#air-pollution",
    "title": "38  Factor Analysis",
    "section": "38.3 Air pollution",
    "text": "38.3 Air pollution\nThe data in link are measurements of air-pollution variables recorded at 12 noon on 42 different days at a location in Los Angeles. The file is in .csv format, since it came from a spreadsheet. Specifically, the variables (in suitable units), in the same order as in the data file, are:\n\nwind speed\nsolar radiation\ncarbon monoxide\nNitric oxide (also known as nitrogen monoxide)\nNitrogen dioxide\nOzone\nHydrocarbons\n\nThe aim is to describe pollution using fewer than these seven variables.\n\nRead in the data and demonstrate that you have the right number of rows and columns in your data frame.\n* Obtain a five-number summary for each variable. You can do this in one go for all seven variables.\nObtain a principal components analysis. Do it on the correlation matrix, since the variables are measured on different scales. You don’t need to look at the results yet.\nObtain a scree plot. How many principal components might be worth looking at? Explain briefly. (There might be more than one possibility. If so, discuss them all.)\nLook at the summary of the principal components object. What light does this shed on the choice of number of components? Explain briefly.\n* How do each of your preferred number of components depend on the variables that were measured? Explain briefly.\nMake a data frame that contains (i) the original data, (ii) a column of row numbers, (iii) the principal component scores. Display some of it.\nDisplay the row of your new data frame for the observation with the smallest (most negative) score on component 1. Which row is this? What makes this observation have the most negative score on component 1?\nWhich observation has the lowest (most negative) value on component 2? Which variables ought to be high or low for this observation? Are they? Explain briefly.\nObtain a biplot, with the row numbers labelled, and explain briefly how your conclusions from the previous two parts are consistent with it.\nRun a factor analysis on the same data, obtaining two factors. Look at the factor loadings. Is it clearer which variables belong to which factor, compared to the principal components analysis? Explain briefly.\n\nMy solutions follow:"
  },
  {
    "objectID": "fa.html#the-interpersonal-circumplex-1",
    "href": "fa.html#the-interpersonal-circumplex-1",
    "title": "38  Factor Analysis",
    "section": "38.4 The Interpersonal Circumplex",
    "text": "38.4 The Interpersonal Circumplex\nThe “IPIP Interpersonal Circumplex” (see link) is a personal behaviour survey, where respondents have to rate how accurately a number of statements about behaviour apply to them, on a scale from 1 (“very inaccurate”) to 5 (“very accurate”). A survey was done on 459 people, using a 44-item variant of the above questionnaire, where the statements were as follows. Put an “I” or an “I am” in front of each one:\n\ntalkative\nfind fault\ndo a thorough job\ndepressed\noriginal\nreserved\nhelpful\ncareless\nrelaxed\ncurious\nfull of energy\nstart quarrels\nreliable\ntense\ningenious\ngenerate enthusiasm in others\nforgiving\ndisorganized\nworry\nimaginative\nquiet\ntrusting\nlazy\nemotionally stable\ninventive\nassertive\ncold and aloof\npersevere\nmoody\nvalue artistic experiences\nshy\nconsiderate\nefficient\ncalm in tense situations\nprefer routine work\noutgoing\nsometimes rude\nstick to plans\nnervous\nreflective\nhave few artistic interests\nco-operative\ndistractible\nsophisticated in art and music\n\nI don’t know what a “circumplex” is, but I know it’s not one of those “hat” accents that they have in French. The data are in link. The columns PERS01 through PERS44 represent the above traits.\n\nRead in the data and check that you have the right number of rows and columns.\n\nSolution\nSeparated by single spaces.\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/personality.txt\"\npers &lt;- read_delim(my_url, \" \")\n\nRows: 459 Columns: 45\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (45): id, PERS01, PERS02, PERS03, PERS04, PERS05, PERS06, PERS07, PERS08...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npers\n\n\n\n  \n\n\n\nYep, 459 people (in rows), and 44 items (in columns), plus one column of ids for the people who took the survey.\nIn case you were wondering about the “I” vs. “I am” thing, the story seems to be that each behaviour needs to have a verb. If the behaviour has a verb, “I” is all you need, but if it doesn’t, you have to add one, ie. “I am”.\nAnother thing you might be concerned about is whether these data are “tidy” or not. To some extent, it depends on what you are going to do with it. You could say that the PERS columns are all survey-responses, just to different questions, and you might think of doing something like this:\n\npers %&gt;% pivot_longer(-id, names_to=\"item\", values_to=\"response\")\n\n\n\n  \n\n\n\nto get a really long and skinny data frame. It all depends on what you are doing with it. Long-and-skinny is ideal if you are going to summarize the responses by survey item, or draw something like bar charts of responses facetted by item:\n\npers %&gt;%\n  pivot_longer(-id, names_to=\"item\", values_to=\"response\") %&gt;%\n  ggplot(aes(x = response)) + geom_bar() + facet_wrap(~item)\n\nWarning: Removed 371 rows containing non-finite values (`stat_count()`).\n\n\n\n\n\nThe first time I did this, item PERS36 appeared out of order at the end, and I was wondering what happened, until I realized it was actually misspelled as PES36! I corrected it in the data file, and it should be good now (though I wonder how many years that error persisted for).\nFor us, in this problem, though, we need the wide format.\n\\(\\blacksquare\\)\n\nThere are some missing values among these responses. Eliminate all the individuals with any missing values (since princomp can’t handle them).\n\nSolution\nThis is actually much easier than it was in the past. A way of asking “are there any missing values anywhere?” is:\n\nany(is.na(pers))\n\n[1] TRUE\n\n\nThere are. To remove them, just this:\n\npers %&gt;% drop_na() -&gt; pers.ok\n\nAre there any missings left?\n\nany(is.na(pers.ok))\n\n[1] FALSE\n\n\nNope. Extra: you might also have thought of the “tidy, remove, untidy” strategy here. The trouble with that here is that you want to remove all the observations for a subject who has any missing ones. This is unlike the multidimensional scaling one where we wanted to remove all the distances for two cities that we knew ahead of time.\nThat gives me an idea, though.\n\npers %&gt;%\n  pivot_longer(-id, names_to=\"item\", values_to=\"rating\")\n\n\n\n  \n\n\n\nTo find out which subjects have any missing values, we can do a group_by and summarize on subjects (that means, the id column; the PERS in the column I called item means “personality”, not “person”!). What do we summarize? Any one of the standard things like mean will return NA if the thing whose mean you are finding has any NA values in it anywhere, and a number if it’s “complete”, so this kind of thing, adding to my pipeline:\n\npers %&gt;%\n  pivot_longer(-id, names_to=\"item\", values_to=\"rating\") %&gt;% \n  group_by(id) %&gt;%\n  summarize(m = mean(rating)) %&gt;%\n  filter(is.na(m))\n\n\n\n  \n\n\n\nThis is different from drop_na, which would remove any rows (of the long data frame) that have a missing response. This, though, is exactly what we don’t want, since we are trying to keep track of the subjects that have missing values.\nMost of the subjects had an actual numerical mean here, whose value we don’t care about; all we care about here is whether the mean is missing, which implies that one (or more) of the responses was missing.\nSo now we define a column has_missing that is true if the subject has any missing values and false otherwise:\n\npers %&gt;%\n  pivot_longer(-id, names_to=\"item\", values_to=\"rating\") %&gt;% \n  group_by(id) %&gt;%\n  summarize(m = mean(rating)) %&gt;%\n  mutate(has_missing = is.na(m)) -&gt; pers.hm\npers.hm \n\n\n\n  \n\n\n\nThis data frame pers.hm has the same number of rows as the original data frame pers, one per subject, so we can just glue it onto the end of that:\n\npers %&gt;% bind_cols(pers.hm)\n\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...46`\n\n\n\n\n  \n\n\n\nand then filter out the rows for which has_missing is true. What we did here is really a way of mimicking complete.cases, which is the way we used to have to do it, before drop_na came on the scene.\n\\(\\blacksquare\\)\n\nCarry out a principal components analysis and obtain a scree plot.\n\nSolution\nThis ought to be straightforward, but we’ve got to remember to use only the columns with actual data in them: that is, PERS01 through PERS44:\n\npers.1 &lt;- pers.ok %&gt;%\n  select(starts_with(\"PERS\")) %&gt;%\n  princomp(cor = T)\nggscreeplot(pers.1)\n\n\n\n\n\\(\\blacksquare\\)\n\nHow many components/factors should you use? Explain briefly.\n\nSolution\nI think the clearest elbow is at 7, so we should use 6 components/factors. You could make a case that the elbow at 6 is also part of the scree, and therefore you should use 5 components/factors. Another one of those judgement calls. Ignore the “backwards elbow” at 5: this is definitely part of the mountain rather than the scree. Backwards elbows, as you’ll recall, don’t count as elbows anyway. When I drew this in R Studio, the elbow at 6 was clearer than the one at 7, so I went with 5 components/factors below. The other way to go is to take the number of eigenvalues bigger than 1:\n\nsummary(pers.1)\n\nImportance of components:\n                          Comp.1     Comp.2     Comp.3     Comp.4     Comp.5\nStandard deviation     2.6981084 2.04738207 1.74372011 1.59610543 1.50114586\nProportion of Variance 0.1654497 0.09526758 0.06910363 0.05789892 0.05121452\nCumulative Proportion  0.1654497 0.26071732 0.32982096 0.38771988 0.43893440\n                          Comp.6     Comp.7     Comp.8     Comp.9    Comp.10\nStandard deviation     1.2627066 1.14816136 1.10615404 1.07405521 1.02180353\nProportion of Variance 0.0362370 0.02996078 0.02780856 0.02621806 0.02372915\nCumulative Proportion  0.4751714 0.50513218 0.53294074 0.55915880 0.58288795\n                          Comp.11    Comp.12    Comp.13    Comp.14    Comp.15\nStandard deviation     0.98309198 0.97514006 0.94861102 0.90832065 0.90680594\nProportion of Variance 0.02196522 0.02161132 0.02045143 0.01875105 0.01868857\nCumulative Proportion  0.60485317 0.62646449 0.64691592 0.66566698 0.68435554\n                          Comp.16    Comp.17    Comp.18    Comp.19   Comp.20\nStandard deviation     0.86798188 0.85762608 0.84515849 0.82819534 0.8123579\nProportion of Variance 0.01712256 0.01671642 0.01623393 0.01558881 0.0149983\nCumulative Proportion  0.70147810 0.71819452 0.73442845 0.75001726 0.7650156\n                          Comp.21    Comp.22    Comp.23    Comp.24    Comp.25\nStandard deviation     0.80910333 0.80435744 0.76594963 0.75946741 0.75434835\nProportion of Variance 0.01487837 0.01470434 0.01333361 0.01310888 0.01293276\nCumulative Proportion  0.77989393 0.79459827 0.80793188 0.82104076 0.83397352\n                          Comp.26    Comp.27   Comp.28    Comp.29    Comp.30\nStandard deviation     0.74494825 0.73105470 0.6956473 0.68327155 0.67765233\nProportion of Variance 0.01261245 0.01214639 0.0109983 0.01061045 0.01043665\nCumulative Proportion  0.84658597 0.85873236 0.8697307 0.88034111 0.89077776\n                          Comp.31     Comp.32     Comp.33     Comp.34\nStandard deviation     0.66847179 0.660473737 0.651473777 0.629487724\nProportion of Variance 0.01015578 0.009914217 0.009645865 0.009005791\nCumulative Proportion  0.90093355 0.910847763 0.920493629 0.929499420\n                           Comp.35     Comp.36     Comp.37     Comp.38\nStandard deviation     0.618765271 0.605892700 0.594231727 0.581419871\nProportion of Variance 0.008701601 0.008343317 0.008025258 0.007682933\nCumulative Proportion  0.938201021 0.946544338 0.954569596 0.962252530\n                           Comp.39     Comp.40     Comp.41     Comp.42\nStandard deviation     0.568951666 0.560084703 0.547059522 0.524949694\nProportion of Variance 0.007356955 0.007129429 0.006801685 0.006263004\nCumulative Proportion  0.969609484 0.976738913 0.983540598 0.989803602\n                           Comp.43     Comp.44\nStandard deviation     0.490608152 0.456010047\nProportion of Variance 0.005470372 0.004726026\nCumulative Proportion  0.995273974 1.000000000\n\n\nThere are actually 10 of these. But if you look at the scree plot, there really seems to be no reason to take 10 factors rather than, say, 11 or 12. There are a lot of eigenvalues (standard deviations) close to (but just below) 1, and no obvious “bargains” in terms of variance explained: the “cumulative proportion” just keeps going gradually up.\n\\(\\blacksquare\\)\n\n* Using your preferred number of factors, run a factor analysis. Obtain “r”-type factor scores, as in class. You don’t need to look at any output yet.\n\nSolution\nI’m going to do the 5 factors that I preferred the first time I looked at this. Don’t forget to grab only the appropriate columns from pers.ok:\n\npers.ok.1 &lt;- pers.ok %&gt;%\n  select(starts_with(\"PERS\")) %&gt;%\n  factanal(5, scores = \"r\")\n\nIf you think 6 is better, you should feel free to use that here.\n\\(\\blacksquare\\)\n\nObtain the factor loadings. How much of the variability does your chosen number of factors explain?\n\nSolution\n\npers.ok.1$loadings\n\n\nLoadings:\n       Factor1 Factor2 Factor3 Factor4 Factor5\nPERS01  0.130   0.752           0.279         \nPERS02          0.199   0.202  -0.545         \nPERS03  0.677                   0.160   0.126 \nPERS04 -0.143  -0.239   0.528  -0.195         \nPERS05  0.191   0.258  -0.180   0.159   0.520 \nPERS06  0.104  -0.658   0.185  -0.143         \nPERS07  0.313   0.113           0.533   0.130 \nPERS08 -0.558           0.168  -0.173         \nPERS09                 -0.641           0.136 \nPERS10  0.149   0.169                   0.445 \nPERS11  0.106   0.282  -0.224  -0.107   0.271 \nPERS12 -0.157                  -0.404         \nPERS13  0.577                   0.331   0.126 \nPERS14                  0.685  -0.135         \nPERS15  0.133                           0.480 \nPERS16  0.106   0.481                   0.335 \nPERS17                          0.277   0.102 \nPERS18 -0.641                                 \nPERS19         -0.159   0.596   0.109         \nPERS20          0.215   0.103           0.498 \nPERS21         -0.805   0.121                 \nPERS22  0.153   0.175           0.533         \nPERS23 -0.622           0.121  -0.259         \nPERS24          0.135  -0.582           0.153 \nPERS25          0.176  -0.124           0.517 \nPERS26  0.144   0.515  -0.215  -0.260   0.244 \nPERS27         -0.205   0.136  -0.438         \nPERS28  0.623                   0.245   0.132 \nPERS29                  0.477  -0.231         \nPERS30                                  0.500 \nPERS31         -0.619   0.191                 \nPERS32  0.322                   0.553   0.134 \nPERS33  0.622                                 \nPERS34  0.105          -0.566                 \nPERS35         -0.180                  -0.161 \nPERS36          0.675  -0.146   0.118   0.107 \nPERS37 -0.300   0.134          -0.495         \nPERS38  0.521                           0.169 \nPERS39         -0.333   0.530                 \nPERS40                          0.162   0.585 \nPERS41                         -0.171  -0.326 \nPERS42  0.271   0.179           0.552   0.139 \nPERS43 -0.465           0.236  -0.174         \nPERS44                                  0.449 \n\n               Factor1 Factor2 Factor3 Factor4 Factor5\nSS loadings      3.810   3.740   3.174   2.939   2.627\nProportion Var   0.087   0.085   0.072   0.067   0.060\nCumulative Var   0.087   0.172   0.244   0.311   0.370\n\n\nThe Cumulative Var line at the bottom says that our five factors together have explained 37% of the variability. This is not great, but is the kind of thing we have to live with in this kind of analysis (like the personality one in class).\n\\(\\blacksquare\\)\n\nInterpret each of your chosen number of factors. That is, for each factor, identify the items that load heavily on it (you can be fairly crude about this, eg. use a cutoff like 0.4 in absolute value), and translate these items into the statements given in each item. Then, if you can, name what the items loading heavily on each factor have in common. Interpret a negative loading as “not” whatever the item says.\n\nSolution\nThis is a lot of work, but I figure you should go through it at least once! If you have some number of factors other than 5, your results will be different from mine. Keep going as long as you reasonably can. Factor 1: 3, 8 (negatively), 13, 18 (negative), 23 (negative), 28, 33, 38 and maybe 43 (negatively). These are: do a thorough job, not-careless, reliable, not-disorganized, not-lazy, persevere, efficient, stick to plans, not-distractible. These have the common theme of paying attention to detail and getting the job done properly.\nFactor 2: 1, not-6, 16, not-21, 26, not-31, 36. Talkative, not-reserved, generates enthusiasm, not-quiet, assertive, not-shy, outgoing. “Extravert” seems to capture all of those. Factor 3: 4, not-9, 14, 19, not-24, 29, not-34, 39. Depressed, not-relaxed, tense, worried, not emotionally stable, moody, not-calm-when-tense, nervous. “Not happy” or something like that.\nNotice how these seem to be jumping in steps of 5? The psychology scoring of assessments like this is that a person’s score on some dimension is found by adding up their scores on certain of the questions and subtracting their scores on others (“reverse-coded questions”). I’m guessing that these guys have 5 dimensions they are testing for, corresponding presumably to my five factors. The questionnaire at link is different, but you see the same idea there. (The jumps there seem to be in steps of 8, since they have 8 dimensions.)\nFactor 4: not-2, 7, not-12 (just), 22, not-27, 32, not-37, 42. Doesn’t find fault, helpful, doesn’t start quarrels, trusting, not-cold-and-aloof, considerate, not-sometimes-rude, co-operative. “Helps without judgement” or similar.\nFactor 5: 5, 10, 15, 20, 25, 30, 40, 44. Original, curious, ingenious, imaginative, inventive, values artistic experiences, reflective, sophisticated in art and music. Creative.\nI remembered that psychologists like to talk about the “big 5” personality traits. These are extraversion (factor 2 here), agreeableness (factor 4), openness (factor 5?), conscientiousness (factor 1), and neuroticism (factor 3). The correspondence appears to be pretty good. (I wrote my answers above last year without thinking about “big 5” at all.)\nI wonder whether 6 factors is different?\n\npers.ok.2 &lt;- pers.ok %&gt;%\n  select(starts_with(\"PERS\")) %&gt;%\n  factanal(6, scores = \"r\")\npers.ok.2$loadings\n\n\nLoadings:\n       Factor1 Factor2 Factor3 Factor4 Factor5 Factor6\nPERS01  0.114  -0.668           0.343           0.290 \nPERS02                  0.204  -0.451           0.354 \nPERS03  0.658                   0.200           0.131 \nPERS04 -0.141   0.215   0.538  -0.196                 \nPERS05  0.212  -0.271  -0.168   0.115   0.571         \nPERS06          0.694   0.171  -0.102                 \nPERS07  0.315  -0.114           0.530   0.138         \nPERS08 -0.602           0.145                   0.180 \nPERS09                 -0.669   0.118           0.106 \nPERS10  0.125                           0.398   0.264 \nPERS11         -0.131  -0.257           0.173   0.445 \nPERS12 -0.178                  -0.348           0.188 \nPERS13  0.572                   0.332   0.129         \nPERS14          0.142   0.675                   0.121 \nPERS15  0.141                           0.495         \nPERS16         -0.322  -0.130   0.114   0.248   0.491 \nPERS17                 -0.111   0.346           0.139 \nPERS18 -0.655                                         \nPERS19          0.139   0.598   0.101   0.104         \nPERS20         -0.165                   0.489   0.166 \nPERS21          0.843                          -0.110 \nPERS22  0.123  -0.105           0.613           0.110 \nPERS23 -0.631           0.115  -0.232                 \nPERS24                 -0.603           0.113   0.143 \nPERS25         -0.135  -0.119           0.513   0.166 \nPERS26  0.113  -0.385  -0.225  -0.163   0.176   0.454 \nPERS27          0.275   0.128  -0.365           0.177 \nPERS28  0.617                   0.253   0.129         \nPERS29                  0.466  -0.161           0.164 \nPERS30                                  0.497         \nPERS31          0.659   0.166                         \nPERS32  0.297                   0.630           0.102 \nPERS33  0.597                                   0.239 \nPERS34                 -0.584                   0.153 \nPERS35          0.221                  -0.210         \nPERS36         -0.562  -0.171   0.214           0.385 \nPERS37 -0.320                  -0.439           0.220 \nPERS38  0.501                   0.123   0.130   0.182 \nPERS39 -0.123   0.410   0.512                   0.118 \nPERS40                          0.146   0.598         \nPERS41                         -0.106  -0.378   0.117 \nPERS42  0.259  -0.139           0.587   0.118         \nPERS43 -0.515           0.210                   0.253 \nPERS44                                  0.454         \n\n               Factor1 Factor2 Factor3 Factor4 Factor5 Factor6\nSS loadings      3.830   3.284   3.213   2.865   2.527   1.659\nProportion Var   0.087   0.075   0.073   0.065   0.057   0.038\nCumulative Var   0.087   0.162   0.235   0.300   0.357   0.395\n\n\nMuch of the same kind of thing seems to be happening, though it’s a bit fuzzier. I suspect the devisers of this survey were adherents to the “big 5” theory. The factor 6 here is items 11, 16 and 26, which would be expected to belong to factor 2 here, given what we found before. I think these particular items are about generating enthusiasm in others, rather than (necessarily) about being extraverted oneself.\n\\(\\blacksquare\\)\n\nFind a person who is extreme on each of your first three factors (one at a time). For each of these people, what kind of data should they have for the relevant ones of variables PERS01 through PERS44? Do they? Explain reasonably briefly.\n\nSolution\nFor this, we need the factor scores obtained in part (here).1 I’m thinking that I will create a data frame with the original data (with the missing values removed) and the factor scores together, and then look in there. This will have a lot of columns, but we’ll only need to display some each time. This is based on my 5-factor solution. I’m adding a column id so that I know which of the individuals (with no missing data) we are looking at:\n\nscores.1 &lt;- as_tibble(pers.ok.1$scores) %&gt;%\n  bind_cols(pers.ok) %&gt;%\n  mutate(id = row_number())\nscores.1\n\n\n\n  \n\n\n\nI did it this way, rather than using data.frame, so that I would end up with a tibble that would display nicely rather than running off the page. This meant turning the matrix of factor scores into a tibble first and then gluing everything onto it. (There’s no problem in using data.frame here if you prefer. You could even begin with data.frame and pipe the final result into as_tibble to end up with a tibble.) Let’s start with factor 1. There are several ways to find the person who scores highest and/or lowest on that factor:\n\nscores.1 %&gt;% filter(Factor1 == max(Factor1))\n\n\n\n  \n\n\n\nto display the maximum, or\n\nscores.1 %&gt;% arrange(Factor1) %&gt;% slice(1:5)\n\n\n\n  \n\n\n\nto display the minimum (and in this case the five smallest ones), or\n\nscores.1 %&gt;% filter(abs(Factor1) == max(abs(Factor1)))\n\n\n\n  \n\n\n\nto display the largest one in size, whether positive or negative. The code is a bit obfuscated because I have to take absolute values twice. Maybe it would be clearer to create a column with the absolute values in it and look at that:\n\nscores.1 %&gt;%\n  mutate(abso = abs(Factor1)) %&gt;%\n  filter(abso == max(abso))\n\n\n\n  \n\n\n\nDoes this work too?\n\nscores.1 %&gt;% arrange(desc(abs(Factor1))) %&gt;% slice(1:5)\n\n\n\n  \n\n\n\nIt looks as if it does: “sort the Factor 1 scores in descending order by absolute value, and display the first few”. The most extreme scores on Factor 1 are all negative: the most positive one (found above) was only about 1.70.\nFor you, you don’t have to be this sophisticated. It’s enough to eyeball the factor scores on factor 1 and find one that’s reasonably far away from zero. Then you note its row and slice that row, later.\nI think I like the idea of creating a new column with the absolute values in it and finding the largest of that. Before we pursue that, though, remember that we don’t need to look at all the PERS columns, because only some of them load highly on each factor. These are the ones I defined into f1 first; the first ones have positive loadings and the last three have negative loadings:\n\nf1 &lt;- c(3, 13, 28, 33, 38, 8, 18, 23, 43)\nscores.1 %&gt;%\n  mutate(abso = abs(Factor1)) %&gt;%\n  filter(abso == max(abso)) %&gt;%\n  select(id, Factor1, num_range(\"PERS\", f1, width = 2)) \n\n\n\n  \n\n\n\nI don’t think I’ve used num_range before, like, ever. It is one of the select-helpers like starts_with. It is used when you have column names that are some constant text followed by variable numbers, which is exactly what we have here: we want to select the PERS columns with numbers that we specify. num_range requires (at least) two things: the text prefix, followed by a vector of numbers that are to be glued onto the prefix. I defined that first so as not to clutter up the select line. The third thing here is width: all the PERS columns have a name that ends with two digits, so PERS03 rather than PERS3, and using width makes sure that the zero gets inserted.\nIndividual 340 is a low scorer on factor 1, so they should have low scores on the first five items (the ones with positive loading on factor 1) and high scores on the last four. This is indeed what happened: 1s, 2s and 3s on the first five items and 4s and 5s on the last four. Having struggled through that, factors 2 and 3 are repeats of this. The high loaders on factor 2 are the ones shown in f2 below, with the first five loading positively and the last three negatively.2\n\nf2 &lt;- c(1, 11, 16, 26, 36, 6, 21, 31)\nscores.1 %&gt;%\n  mutate(abso = abs(Factor2)) %&gt;%\n  filter(abso == max(abso)) %&gt;%\n  select(id, Factor2, num_range(\"PERS\", f2, width = 2))\n\n\n\n  \n\n\n\nWhat kind of idiot, I was thinking, named the data frame of scores scores.1 when there are going to be three factors to assess?\nThe first five scores are lowish, but the last three are definitely high (three 5s). This idea of a low score on the positive-loading items and a high score on the negatively-loading ones is entirely consistent with a negative factor score.\nFinally, factor 3, which loads highly on items 4, 9 (neg), 14, 19, 24 (neg), 29, 34 (neg), 39. (Item 44, which you’d expect to be part of this factor, is actually in factor 5.) First we see which individual this is:\n\nf3 &lt;- c(4, 14, 19, 29, 39, 9, 24, 34)\nscores.1 %&gt;%\n  mutate(abso = abs(Factor3)) %&gt;%\n  filter(abso == max(abso)) %&gt;%\n  select(id, Factor3, num_range(\"PERS\", f3, width = 2)) \n\n\n\n  \n\n\n\nThe only mysterious one there is item 19, which ought to be low, because it has a positive loading and the factor score is unusually negative. But it is 4 on a 5-point scale. The others that are supposed to be low are 1 and the ones that are supposed to be high are 4 or 5, so those all match up.\n\\(\\blacksquare\\)\n\nCheck the uniquenesses. Which one(s) seem unusually high? Check these against the factor loadings. Are these what you would expect?\n\nSolution\nMine are\n\npers.ok.1$uniquenesses\n\n   PERS01    PERS02    PERS03    PERS04    PERS05    PERS06    PERS07    PERS08 \n0.3276244 0.6155884 0.4955364 0.6035655 0.5689691 0.4980334 0.5884146 0.6299781 \n   PERS09    PERS10    PERS11    PERS12    PERS13    PERS14    PERS15    PERS16 \n0.5546981 0.7460655 0.7740590 0.8016644 0.5336047 0.5035412 0.7381636 0.6352166 \n   PERS17    PERS18    PERS19    PERS20    PERS21    PERS22    PERS23    PERS24 \n0.8978624 0.5881834 0.5949740 0.6900378 0.3274366 0.6564542 0.5279346 0.6107080 \n   PERS25    PERS26    PERS27    PERS28    PERS29    PERS30    PERS31    PERS32 \n0.6795545 0.5412962 0.7438329 0.5289192 0.7114735 0.7386601 0.5762901 0.5592906 \n   PERS33    PERS34    PERS35    PERS36    PERS37    PERS38    PERS39    PERS40 \n0.6029914 0.6573411 0.9306766 0.4966071 0.6396371 0.6804821 0.5933423 0.6204610 \n   PERS41    PERS42    PERS43    PERS44 \n0.8560531 0.5684220 0.6898732 0.7872295 \n\n\nYours will be different if you used a different number of factors. But the procedure you follow will be the same as mine.\nI think the highest of these is 0.9307, for item 35. Also high is item 17, 0.8979. If you look back at the table of loadings, item 35 has low loadings on all the factors: the largest in size is only 0.180. The largest loading for item 17 is 0.277, on factor 4. This is not high either.\nLooking down the loadings table, also item 41 has only a loading of \\(-0.326\\) on factor 5, so its uniqueness ought to be pretty high as well. At 0.8561, it is.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "fa.html#a-correlation-matrix-1",
    "href": "fa.html#a-correlation-matrix-1",
    "title": "38  Factor Analysis",
    "section": "38.5 A correlation matrix",
    "text": "38.5 A correlation matrix\nHere is a correlation matrix between five variables. This correlation matrix was based on \\(n=50\\) observations. Save the data into a file.\n\n1.00 0.90 -0.40 0.28 -0.05\n0.90 1.00 -0.60 0.43 -0.20\n-0.40 -0.60 1.00 -0.80 0.40\n0.28 0.43 -0.80 1.00 -0.70\n-0.05 -0.20 0.40 -0.70 1.00\n\n\nRead in the data, using col_names=F (why?). Check that you have five variables with names invented by R.\n\nSolution\nI saved my data into cov5.txt,3 delimited by single spaces, so:\n\ncorr &lt;- read_delim(\"cov5.txt\", \" \", col_names = F)\n\nRows: 5 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (5): X1, X2, X3, X4, X5\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncorr\n\n\n\n  \n\n\n\nI needed to say that I have no variable names and I want R to provide some. As you see, it did: X1 through X5. You can also supply your own names in this fashion:\n\nmy_names &lt;- c(\"first\", \"second\", \"third\", \"fourth\", \"fifth\")\ncorr2 &lt;- read_delim(\"cov5.txt\", \" \", col_names = my_names)\n\nRows: 5 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (5): first, second, third, fourth, fifth\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncorr2\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nRun a principal components analysis from this correlation matrix.\n\nSolution\nTwo lines, these:\n\ncorr.mat &lt;- as.matrix(corr)\ncorr.1 &lt;- princomp(covmat = corr.mat)\n\nOr do it in one step as\n\ncorr.1a &lt;- princomp(as.matrix(corr))\n\nif you like, but I think it’s less clear what’s going on.\n\\(\\blacksquare\\)\n\n* Obtain a scree plot. Can you justify the use of two components (later, factors), bearing in mind that we have only five variables?\n\nSolution\n\nggscreeplot(corr.1)\n\n\n\n\nThere is kind of an elbow at 3, which would suggest two components/factors.4\nYou can also use the eigenvalue-bigger-than-1 thing:\n\nsummary(corr.1)\n\nImportance of components:\n                          Comp.1    Comp.2     Comp.3     Comp.4    Comp.5\nStandard deviation     1.7185460 1.1686447 0.70207741 0.36584870 0.2326177\nProportion of Variance 0.5906801 0.2731461 0.09858254 0.02676905 0.0108222\nCumulative Proportion  0.5906801 0.8638262 0.96240875 0.98917780 1.0000000\n\n\nOnly the first two eigenvalues are bigger than 1, and the third is quite a bit smaller. So this would suggest two factors also. The third eigenvalue is in that kind of nebulous zone between between being big and being small, and the percent of variance explained is also ambiguous: is 86% good enough or should I go for 96%? These questions rarely have good answers. But an issue is that you want to summarize your variables with a (much) smaller number of factors; with 5 variables, having two factors rather than more than two seems to be a way of gaining some insight.\n\\(\\blacksquare\\)\n\nTake a look at the first two component loadings. Which variables appear to feature in which component? Do they have a positive or negative effect?\n\nSolution\n\ncorr.1$loadings\n\n\nLoadings:\n   Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nX1  0.404  0.571  0.287  0.363  0.545\nX2  0.482  0.439  0.144 -0.363 -0.650\nX3 -0.501  0.122  0.652  0.412 -0.375\nX4  0.490 -0.390 -0.171  0.689 -0.323\nX5 -0.338  0.561 -0.665  0.304 -0.189\n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nSS loadings       1.0    1.0    1.0    1.0    1.0\nProportion Var    0.2    0.2    0.2    0.2    0.2\nCumulative Var    0.2    0.4    0.6    0.8    1.0\n\n\nThis is messy.\nIn the first component, the loadings are all about the same in size, but the ones for X3 and X5 are negative and the rest are positive. Thus, component 1 is contrasting X3 and X5 with the others.\nIn the second component, the emphasis is on X1, X2 and X5, all with negative loadings, and possibly X4, with a positive loading.\nNote that the first component is basically “size”, since the component loadings are all almost equal in absolute value. This often happens in principal components; for example, it also happened with the running records in class.\nI hope the factor analysis, with its rotation, will straighten things out some.\n\\(\\blacksquare\\)\n\nCreate a “covariance list” (for the purposes of performing a factor analysis on the correlation matrix).\n\nSolution\nThis is about the most direct way:\n\ncorr.list &lt;- list(cov = corr.mat, n.obs = 50)\n\nrecalling that there were 50 observations. The idea is that we feed this into factanal instead of the correlation matrix, so that the factor analysis knows how many individuals there were (for testing and such).\nNote that you need the correlation matrix as a matrix, not as a data frame. If you ran the princomp all in one step, you’ll have to create the correlation matrix again, for example like this:\n\ncorr.list2 &lt;- list(cov = as.matrix(corr), n.obs = 50)\n\nThe actual list looks like this:\n\ncorr.list\n\n$cov\n        X1    X2   X3    X4    X5\n[1,]  1.00  0.90 -0.4  0.28 -0.05\n[2,]  0.90  1.00 -0.6  0.43 -0.20\n[3,] -0.40 -0.60  1.0 -0.80  0.40\n[4,]  0.28  0.43 -0.8  1.00 -0.70\n[5,] -0.05 -0.20  0.4 -0.70  1.00\n\n$n.obs\n[1] 50\n\n\nAn R list is a collection of things not all of the same type, here a matrix and a number, and is a handy way of keeping a bunch of connected things together. You use the same dollar-sign notation as for a data frame to access the things in a list:\n\ncorr.list$n.obs\n\n[1] 50\n\n\nand logically this is because, to R, a data frame is a special kind of a list, so anything that works for a list also works for a data frame, plus some extra things besides.5\nThe same idea applies to extracting things from the output of a regression (with lm) or something like a t.test: the output from those is also a list. But for those, I like tidy from broom better.\n\\(\\blacksquare\\)\n\nCarry out a factor analysis with two factors. We’ll investigate the bits of it in a moment.\n\nSolution\n\ncorr.2 &lt;- factanal(factors = 2, covmat = corr.list)\n\n\\(\\blacksquare\\)\n\n* Look at the factor loadings. Describe how the factors are related to the original variables. Is the interpretation clearer than for the principal components analysis?\n\nSolution\n\ncorr.2$loadings\n\n\nLoadings:\n     Factor1 Factor2\n[1,]          0.905 \n[2,] -0.241   0.968 \n[3,]  0.728  -0.437 \n[4,] -0.977   0.201 \n[5,]  0.709         \n\n               Factor1 Factor2\nSS loadings      2.056   1.987\nProportion Var   0.411   0.397\nCumulative Var   0.411   0.809\n\n\nOh yes, this is a lot clearer. Factor 1 is variables 3 and 5 contrasted with variable 4; factor 2 is variables 1 and 2. No hand-waving required.\nPerhaps now is a good time to look back at the correlation matrix and see why the factor analysis came out this way:\n\ncorr\n\n\n\n  \n\n\n\nVariable X1 is highly correlated with X2 but not really any of the others. Likewise variables X3, X4, X5 are more or less highly correlated among themselves but not with the others (X2 and X3 being an exception, but the big picture is as I described). So variables that appear in the same factor should be highly correlated with each other and not with variables that are in different factors. But it took the factor analysis to really show this up.\n\\(\\blacksquare\\)\n\nLook at the uniquenesses. Are there any that are unusually high? Does that surprise you, given your answer to (here)? (You will probably have to make a judgement call here.)\n\nSolution\n\ncorr.2$uniquenesses\n\n       X1        X2        X3        X4        X5 \n0.1715682 0.0050000 0.2789445 0.0050000 0.4961028 \n\n\nThe ones for X3 and X5 are higher than the rest; this is because their loadings on factor 1 are lower than the rest. Since those loadings are still high, I wouldn’t be worried about the uniquenesses.\nThe point here is that an (excessively) high uniqueness indicates a variable that doesn’t appear in any factor. The easy link to make is “all the variables appear in a factor, so there shouldn’t be any very high uniquenesses”. If, say, X3 doesn’t have a high loading on any factor, X3 would have a high uniqueness (like 0.9, and none of these values approach that).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "fa.html#air-pollution-1",
    "href": "fa.html#air-pollution-1",
    "title": "38  Factor Analysis",
    "section": "38.6 Air pollution",
    "text": "38.6 Air pollution\nThe data in link are measurements of air-pollution variables recorded at 12 noon on 42 different days at a location in Los Angeles. The file is in .csv format, since it came from a spreadsheet. Specifically, the variables (in suitable units), in the same order as in the data file, are:\n\nwind speed\nsolar radiation\ncarbon monoxide\nNitric oxide (also known as nitrogen monoxide)\nNitrogen dioxide\nOzone\nHydrocarbons\n\nThe aim is to describe pollution using fewer than these seven variables.\n\nRead in the data and demonstrate that you have the right number of rows and columns in your data frame.\n\nSolution\nThis is a .csv file, so:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/airpollution.csv\"\nair &lt;- read_csv(my_url)\n\nRows: 42 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): wind, solar.radiation, CO, NO, NO2, O3, HC\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nair\n\n\n\n  \n\n\n\nThere should be 42 rows (for the 42 days), and 7 columns (for the 7 variables), and there are.\n\\(\\blacksquare\\)\n\n* Obtain a five-number summary for each variable. You can do this in one go for all seven variables.\n\nSolution\nLike this (the cleanest):\n\nair %&gt;% \n  summarize(across(everything(), \\(x) quantile(x)))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n\n\n  \n\n\n\nI have to figure out how to identify which number from the five number summary each of these is, but in this case you can easily figure it out since the min is the smallest and the max has to be the biggest in each column.\nOr, with some more work, this:\n\nair %&gt;%\n  pivot_longer(everything(), names_to=\"xname\", values_to=\"x\") %&gt;% \n  nest_by(xname) %&gt;%\n  rowwise() %&gt;% \n  mutate(q = list(enframe(quantile(data$x)))) %&gt;%\n  unnest(q) %&gt;%\n  pivot_wider(names_from=name, values_from=value) %&gt;% \n  select(-data)\n\n\n\n  \n\n\n\nThere’s a lot here. Run it one line at a time to see what it does:\n\nput the names of the variables in one column and the values in a second. This is the same trick as when we want to make plots of all the variables facetted.\nthe nest_by says: for each variable (whose names are now in xname), make a dataframe called data of the observations (in x) for that variable\nthe rest of the way, work one row at a time\nwork out the five-number summary for each variable, using the values x in the data frame data of each row of the list-column, one at a time. This is the base R quantile, working on a vector (the column x of the data frame data), so it gives you back a named vector. If you are not familiar with that, try running quantile(1:10) and see how the output has both the percentiles and, above them, the percents that they go with. The tidyverse doesn’t like names, so my favourite way of keeping them with a named vector is to run it through enframe. This makes a two-column dataframe, with a column called name that is in this case the percents, and a column called value that is the percentiles. This is a dataframe rather than a single number, so it needs a list on the front as well (to make another list-column). There are rather a lot of brackets to close here; if you are not sure you have enough, type another close bracket, pause, and see what it matches (R Studio will show you). If it matches nothing, you have too many close brackets.\nshow the values of the five-number summary for each variable (in long format, but with the percentages attached)\nfor human consumption, put the percentiles in columns, one row for each variable\nfinally, get rid of the dataframes of original values (that we don’t need any more now that we have summarized them).\n\nExtra: say you wanted to make facetted histograms of each variable. You would begin the same way, with the pivot_longer, and at the end, facet_wrap with scales = \"free\" (since the variables are measured on different scales):\n\nair %&gt;%\n  pivot_longer(everything(), names_to=\"xname\", values_to=\"x\") %&gt;% \n  ggplot(aes(x=x)) + geom_histogram(bins = 6) +\n  facet_wrap(~xname, scales = \"free\")\n\n\n\n\nExtra extra: I originally put a pipe symbol on the end of the line with the geom_histogram on it, and got an impenetrable error. However, googling the error message (often a good plan) gave me a first hit that told me exactly what I had done.\n\\(\\blacksquare\\)\n\nObtain a principal components analysis. Do it on the correlation matrix, since the variables are measured on different scales. You don’t need to look at the results yet.\n\nSolution\nThis too is all rather like the previous question:\n\nair.1 &lt;- princomp(air, cor = T)\n\n\\(\\blacksquare\\)\n\nObtain a scree plot. How many principal components might be worth looking at? Explain briefly. (There might be more than one possibility. If so, discuss them all.)\n\nSolution\nggscreeplot the thing you just obtained, having loaded package ggbiplot:\n\nggscreeplot(air.1)\n\n\n\n\nThere is a technicality here, which is that ggbiplot, the package, loads plyr, which contains a lot of the same things as dplyr (the latter is a cut-down version of the former). If you load dplyr and then plyr (that is to say, if you load the tidyverse first and then ggbiplot), you will end up with trouble, and probably the wrong version of a lot of functions. To avoid this, load ggbiplot first, and then you’ll be OK.\nNow, finally, we might diverge from the previous question. There are actually two elbows on this plot, at 2 and at 4, which means that we should entertain the idea of either 1 or 3 components. I would be inclined to say that the elbow at 2 is still “too high up” the mountain — there is still some more mountain below it.\nThe points at 3 and 6 components look like elbows too, but they are pointing the wrong way. What you are looking for when you search for elbows are points that are the end of the mountain and the start of the scree. The elbows at 2 (maybe) and 4 (definitely) are this kind of thing, but the elbows at 3 and at 6 are not.\n\\(\\blacksquare\\)\n\nLook at the summary of the principal components object. What light does this shed on the choice of number of components? Explain briefly.\n\nSolution\n\nsummary(air.1)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3    Comp.4     Comp.5\nStandard deviation     1.5286539 1.1772853 1.0972994 0.8526937 0.80837896\nProportion of Variance 0.3338261 0.1980001 0.1720094 0.1038695 0.09335379\nCumulative Proportion  0.3338261 0.5318262 0.7038356 0.8077051 0.90105889\n                           Comp.6     Comp.7\nStandard deviation     0.73259047 0.39484041\nProportion of Variance 0.07666983 0.02227128\nCumulative Proportion  0.97772872 1.00000000\n\n\nThe first component only explains 33% of the variability, not very much, but the first three components together explain 70%, which is much more satisfactory. So I would go with 3 components.\nThere are two things here: finding an elbow, and explaining a sensible fraction of the variability. You could explain more of the variability by taking more components, but if you are not careful you end up explaining seven variables with, um, seven variables.\nIf you go back and look at the scree plot, you’ll see that the first elbow is really rather high up the mountain, and it’s really the second elbow that is the start of the scree.\nIf this part doesn’t persuade you that three components is better than one, you need to pick a number of components to use for the rest of the question, and stick to it all the way through.\n\\(\\blacksquare\\)\n\n* How do each of your preferred number of components depend on the variables that were measured? Explain briefly.\n\nSolution\nWhen this was a hand-in question, there were three marks for it, which was a bit of a giveaway! Off we go:\n\nair.1$loadings\n\n\nLoadings:\n                Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7\nwind             0.237  0.278  0.643  0.173  0.561  0.224  0.241\nsolar.radiation -0.206 -0.527  0.224  0.778 -0.156              \nCO              -0.551        -0.114         0.573  0.110 -0.585\nNO              -0.378  0.435 -0.407  0.291         0.450  0.461\nNO2             -0.498  0.200  0.197               -0.745  0.338\nO3              -0.325 -0.567  0.160 -0.508         0.331  0.417\nHC              -0.319  0.308  0.541 -0.143 -0.566  0.266 -0.314\n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7\nSS loadings     1.000  1.000  1.000  1.000  1.000  1.000  1.000\nProportion Var  0.143  0.143  0.143  0.143  0.143  0.143  0.143\nCumulative Var  0.143  0.286  0.429  0.571  0.714  0.857  1.000\n\n\nYou’ll have to decide where to draw the line between “zero” and “nonzero”. It doesn’t matter so much where you put the line, so your answers can differ from mine and still be correct.\nWe need to pick the loadings that are “nonzero”, however we define that, for example:\n\ncomponent 1 depends (negatively) on carbon monoxide and nitrogen dioxide.\ncomponent 2 depends (negatively) on solar radiation and ozone and possibly positively on nitric oxide.\ncomponent 3 depends (positively) on wind and hydrocarbons.\n\nIt is a good idea to translate the variable names (which are abbreviated) back into the long forms.\n\\(\\blacksquare\\)\n\nMake a data frame that contains (i) the original data, (ii) a column of row numbers, (iii) the principal component scores. Display some of it.\n\nSolution\nAll the columns contain numbers, so cbind will do it. (The component scores are seven columns, so bind_cols won’t do it unless you are careful.):\n\ncbind(air, air.1$scores) %&gt;%  \n  mutate(row = row_number()) -&gt; d\nhead(d)\n\n\n\n  \n\n\n\nThis is probably the easiest way, but you see that there is a mixture of base R and Tidyverse. The result is actually a base R data.frame, so displaying it will display all of it, hence my use of head. If you want to do it the all-Tidyverse way6 then you need to bear in mind that bind_cols only accepts vectors or data frames, not matrices, so a bit of care is needed first:\n\nair.1$scores %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(air) %&gt;%\n  mutate(row = row_number()) -&gt; dd\ndd\n\n\n\n  \n\n\n\nI think the best way to think about this is to start with what is farthest from being a data frame or a vector (the matrix of principal component scores, here), bash that into shape first, and then glue the rest of the things to it.\nNote that we used all Tidyverse stuff here, so the result is a tibble, and displaying it for me displays the first ten rows as you’d expect. (This may be different in an R Notebook, since I think there you get the first ten rows anyway.)\n\\(\\blacksquare\\)\n\nDisplay the row of your new data frame for the observation with the smallest (most negative) score on component 1. Which row is this? What makes this observation have the most negative score on component 1?\n\nSolution\nI think the best strategy is to sort by component 1 score (in the default ascending order), and then display the first row:\n\nd %&gt;% arrange(Comp.1) %&gt;% slice(1)\n\n\n\n  \n\n\n\nIt’s row 8.\nWe said earlier that component 1 depends negatively on carbon monoxide and nitrogen dioxide, so that an observation that is low on component 1 should be high on these things.7\nSo are these values high or low? That was the reason for having you make the five-number summary here. For observation 8, CO is 6 and NO2 is 21; looking back at the five-number summary, the value of CO is above Q3, and the value of NO2 is the highest of all. So this is entirely what we’d expect.\n\\(\\blacksquare\\)\n\nWhich observation has the lowest (most negative) value on component 2? Which variables ought to be high or low for this observation? Are they? Explain briefly.\n\nSolution\nThis is a repeat of the ideas we just saw:\n\nd %&gt;% arrange(Comp.2) %&gt;% slice(1)\n\n\n\n  \n\n\n\nand for convenience, we’ll grab the quantiles again:\n\nair %&gt;% \n  summarize(across(everything(), \\(x) quantile(x)))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n\n\n  \n\n\n\nDay 34 (at the end of the line). We said that component 2 depends (negatively) on solar radiation and ozone and possibly positively on nitric oxide. This means that day 34 ought to be high on the first two and low on the last one (since it’s at the low end of component 2). Solar radiation is, surprisingly, close to the median (75), but ozone, 24, is very near the highest, and nitric oxide, 1, is one of a large number of values equal to the lowest. So day 34 is pointing the right way, even if its variable values are not quite what you’d expect. This business about figuring out whether values on variables are high or low is kind of fiddly, since you have to refer back to the five-number summary to see where the values for a particular observation come. Another way to approach this is to calculate percentile ranks for everything. Let’s go back to our original data frame and replace everything with its percent rank:\n\nair %&gt;% mutate(across(everything(), \\(x) percent_rank(x))) -&gt; pct_rank\npct_rank\n\n\n\n  \n\n\n\nObservation 34 is row 34 of this:\n\npct_rank %&gt;% slice(34)\n\n\n\n  \n\n\n\nVery high on ozone, (joint) lowest on nitric oxide, but middling on solar radiation. The one we looked at before, observation 8, is this:\n\npct_rank %&gt;% slice(8)\n\n\n\n  \n\n\n\nHigh on carbon monoxide, the highest on nitrogen dioxide.\n\\(\\blacksquare\\)\n\nObtain a biplot, with the row numbers labelled, and explain briefly how your conclusions from the previous two parts are consistent with it.\n\nSolution\n\nggbiplot(air.1, labels = d$row)\n\n\n\n\nDay 8 is way over on the left. The things that point in the direction of observation 8 (NO2, CO and to a lesser extent NO and HC) are the things that observation 8 is high on. On the other hand, observation 8 is around the middle of the arrows for wind, solar.radiation and O3, so that day is not especially remarkable for those.\nObservation 34 is nearest the bottom, so we’d expect it to be high on ozone (yes), high on solar radiation (no), low on nitric oxide (since that points the most upward, yes) and also maybe low on wind, since observation 34 is at the “back end” of that arrow. Wind is 6, which is at the first quartile, low indeed.\nThe other thing that you see from the biplot is that there are four variables pointing more or less up and to the left, and at right angles to them, three other variables pointing up-and-right or down-and-left. You could imagine rotating those arrows so that the group of 4 point upwards, and the other three point left and right. This is what factor analysis does, so you might imagine that this technique might give a clearer picture of which variables belong in which factor than principal components does. Hence what follows.\n\\(\\blacksquare\\)\n\nRun a factor analysis on the same data, obtaining two factors. Look at the factor loadings. Is it clearer which variables belong to which factor, compared to the principal components analysis? Explain briefly.\n\nSolution\n\nair.2 &lt;- factanal(air, 2, scores = \"r\")\nair.2$loadings\n\n\nLoadings:\n                Factor1 Factor2\nwind            -0.176  -0.249 \nsolar.radiation          0.319 \nCO               0.797   0.391 \nNO               0.692  -0.152 \nNO2              0.602   0.152 \nO3                       0.997 \nHC               0.251   0.147 \n\n               Factor1 Factor2\nSS loadings      1.573   1.379\nProportion Var   0.225   0.197\nCumulative Var   0.225   0.422\n\n\nI got the factor scores since I’m going to look at a biplot shortly. If you aren’t, you don’t need them.\nFactor 1 is rather more clearly carbon monoxide, nitric oxide and nitrogen dioxide. Factor 2 is mostly ozone, with a bit of solar radiation and carbon monoxide. I’d say this is clearer than before.\nA biplot would tell us whether the variables are better aligned with the axes now:\n\nbiplot(air.2$scores, air.2$loadings)\n\n\n\n\nAt least somewhat. Ozone points straight up, since it is the dominant part of factor 2 and not part of factor 1 at all. Carbon monoxide and the two oxides of nitrogen point to the right.\nExtra: wind, solar.radiation and HC don’t appear in either of our factors, which also shows up here:\n\nair.2$uniquenesses\n\n           wind solar.radiation              CO              NO             NO2 \n      0.9070224       0.8953343       0.2126417       0.4983564       0.6144170 \n             O3              HC \n      0.0050000       0.9152467 \n\n\nThose variables all have high uniquenesses.\nWhat with the high uniquenesses, and the fact that two factors explain only 42% of the variability, we really ought to look at 3 factors, the same way that we said we should look at 3 components:\n\nair.3 &lt;- factanal(air, 3)\nair.3$loadings\n\n\nLoadings:\n                Factor1 Factor2 Factor3\nwind                    -0.210  -0.334 \nsolar.radiation          0.318         \nCO               0.487   0.318   0.507 \nNO               0.238  -0.269   0.931 \nNO2              0.989                 \nO3                       0.987   0.124 \nHC               0.427   0.103   0.172 \n\n               Factor1 Factor2 Factor3\nSS loadings      1.472   1.312   1.288\nProportion Var   0.210   0.187   0.184\nCumulative Var   0.210   0.398   0.582\n\n\nIn case you are wondering, factanal automatically uses the correlation matrix, and so takes care of variables measured on different scales without our having to worry about that.\nThe rotation has only helped somewhat here. Factor 1 is mainly NO2 with some influence of CO and HC; factor 2 is mainly ozone (with a bit of solar radiation and carbon monoxide), and factor 3 is mainly NO with a bit of CO.\nI think I mentioned most of the variables in there, so the uniquenesses should not be too bad:\n\nair.3$uniquenesses\n\n           wind solar.radiation              CO              NO             NO2 \n      0.8404417       0.8905074       0.4046425       0.0050000       0.0050000 \n             O3              HC \n      0.0050000       0.7776557 \n\n\nWell, not great: wind and solar.radiation still have high uniquenesses because they are not strongly part of any factors.\nIf you wanted to, you could obtain the factor scores for the 3-factor solution, and plot them on a three-dimensional plot using rgl, rotating them to see the structure. A three dimensional “biplot”8 would also be a cool thing to look at.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "fa.html#footnotes",
    "href": "fa.html#footnotes",
    "title": "38  Factor Analysis",
    "section": "",
    "text": "There are two types of scores here: a person’s scores on the psychological test, 1 through 5, and their factor scores, which are decimal numbers centred at zero. Try not to get these confused.↩︎\nI think the last four items in the entire survey are different; otherwise the total number of items would be a multiple of 5.↩︎\nNot to be confused with covfefe, which was current news when I wrote this question.↩︎\nThere is also kind of an elbow at 4, which would suggest three factors, but that’s really too many with only 5 variables. That wouldn’t be much of a reduction in the number of variables, which is what principal components is trying to achieve.↩︎\nIn computer science terms, a data frame is said to inherit from a list: it is a list plus extra stuff.↩︎\nThere really ought to be a radio station CTDY: All Tidyverse, All The Time.↩︎\nYou might have said that component 1 depended on other things as well, in which case you ought to consider whether observation 8 is, as appropriate, high or low on these as well.↩︎\nA three-dimensional biplot ought to be called a triplot.↩︎"
  },
  {
    "objectID": "loglin.html#college-plans",
    "href": "loglin.html#college-plans",
    "title": "39  Frequency table analysis",
    "section": "39.1 College plans",
    "text": "39.1 College plans\n5199 male high school seniors in Wisconsin1 were classified by socio-economic status (low, lower-middle, upper-middle, high), by the degree that their parents encouraged them in their education (low or high), and whether or not they had plans to go to college (yes or no). How, if at all, are these categorical variables related? The data can be found at link.\n\nRead in the data and check that you have a column for each variable and a column of frequencies.\nFit a log-linear model containing all possible interactions. You don’t need to examine it yet.\nFind out which terms (interactions) could be removed. Do you think removing any of them is a good idea?\nRemove anything you can, and fit again. Hint: update.\nContinue to examine what can be removed, and if reasonable, remove it, until you need to stop. Which terms are left in your final model?\nMake two-way tables of any remaining two-way interactions, and describe any relationships that you see."
  },
  {
    "objectID": "loglin.html#predicting-voting",
    "href": "loglin.html#predicting-voting",
    "title": "39  Frequency table analysis",
    "section": "39.2 Predicting voting",
    "text": "39.2 Predicting voting\n1257 British voters were classified according to their social class, age (categorized), sex and the political party they voted for (Labour or Conservative). Which, if any, of these factors influences the party that someone votes for? The data are in link, one voter per line.\n\nRead in the data and display (some of) the data frame.\nThere is no frequency column here, because each row of the data frame only represents one voter. Count up the frequencies for each combo of the categorical variables, and save it (this is the data frame that we will use for the analysis). Display the first few rows of the result. Do you now have something that you need?\nFit a log-linear model with the appropriate interaction (as a starting point).\nRefine your model by taking out suitable non-significant terms, in multiple steps. What model do you finish with?\nIf we think of the party someone votes for as the final outcome (that depends on all the other things), what does our final model say that someone’s vote depends on?\nObtain sub-tables that explain how vote depends on any of the things it’s related to."
  },
  {
    "objectID": "loglin.html#brand-m-laundry-detergent",
    "href": "loglin.html#brand-m-laundry-detergent",
    "title": "39  Frequency table analysis",
    "section": "39.3 Brand M laundry detergent",
    "text": "39.3 Brand M laundry detergent\nA survey was carried out comparing respondents’ preferences of a laundry detergent M compared to a mystery brand X. For each respondent, the researchers recorded the temperature of the laundry load (low or high), whether or not they previously used brand M (yes or no), and the softness of the water used for the laundry load(hard, medium or soft). The aim of the survey was to find out what was associated with the respondents preferring brand M. The data are in http://ritsokiguess.site/datafiles/brand_m.csv.\n\nRead in and display (some of) the data. Explain briefly how the data is laid out appropriately to fit a log-linear model.\nUsing backward elimination, build a suitable log-linear model for the associations between the variables. (Do not use step; do the elimination yourself).\nWhat is associated with the brand a respondent prefers? By obtaining suitable frequency tables, describe the nature of these associations.\n\nMy solutions follow:"
  },
  {
    "objectID": "loglin.html#college-plans-1",
    "href": "loglin.html#college-plans-1",
    "title": "39  Frequency table analysis",
    "section": "39.4 College plans",
    "text": "39.4 College plans\n5199 male high school seniors in Wisconsin2 were classified by socio-economic status (low, lower-middle, upper-middle, high), by the degree that their parents encouraged them in their education (low or high), and whether or not they had plans to go to college (yes or no). How, if at all, are these categorical variables related? The data can be found at link.\n\nRead in the data and check that you have a column for each variable and a column of frequencies.\n\nSolution\nDelimited by one space:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/college-plans.txt\"\nwisc &lt;- read_delim(my_url, \" \")\n\nRows: 16 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (3): social.stratum, encouragement, college.plans\ndbl (1): frequency\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwisc\n\n\n\n  \n\n\n\nAs promised. We only have 16 observations, because we have all possible combinations of categorical variable combinations, 4 social strata, times 2 levels of encouragement, times 2 levels of college plans.\nEach line of the data file summarizes a number of students, not just one. For example, the first line says that 749 students were in the lower social stratum, received low encouragement and have no college plans. If we sum up the frequencies, we should get 5199 because there were that many students altogether:\n\nwisc %&gt;% summarize(tot = sum(frequency))\n\n\n\n  \n\n\n\n\\(\\blacksquare\\)\n\nFit a log-linear model containing all possible interactions. You don’t need to examine it yet.\n\nSolution\n\nwisc.1 &lt;- glm(frequency ~ social.stratum * encouragement * college.plans,\n  data = wisc, family = \"poisson\"\n)\n\n\\(\\blacksquare\\)\n\nFind out which terms (interactions) could be removed. Do you think removing any of them is a good idea?\n\nSolution\nThis is drop1. If you forget the test=, you won’t get any P-values:\n\ndrop1(wisc.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nThis P-value is not small, so the three-way interaction can be removed.\n\\(\\blacksquare\\)\n\nRemove anything you can, and fit again. Hint: update.\n\nSolution\nIn this kind of modelling, it’s easier to describe what changes should be made to get from one model to another, rather than writing out the whole thing from scratch again. Anyway, the three-way interaction can come out:\n\nwisc.2 &lt;- update(wisc.1, . ~ . - social.stratum:encouragement:college.plans)\n\n\\(\\blacksquare\\)\n\nContinue to examine what can be removed, and if reasonable, remove it, until you need to stop. Which terms are left in your final model?\n\nSolution\nStart with drop1:\n\ndrop1(wisc.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nThese are all strongly significant, so they have to stay. There is nothing else we can remove. All the two-way interactions have to stay in the model.\n\\(\\blacksquare\\)\n\nMake two-way tables of any remaining two-way interactions, and describe any relationships that you see.\n\nSolution\nWe have three two-way tables to make.\nMy first one is social stratum by parental encouragement. Neither of these is really a response, but I thought that social stratum would influence parental encouragement rather than the other way around, hence:\n\nxtabs(frequency ~ social.stratum + encouragement, data = wisc) %&gt;%\n  prop.table(margin = 1)\n\n              encouragement\nsocial.stratum      high       low\n   higher      0.8562249 0.1437751\n   lower       0.3182609 0.6817391\n   lowermiddle 0.2542373 0.7457627\n   uppermiddle 0.5584329 0.4415671\n\n\nThis says that there tends to be more parental encouragement, the higher the social stratum. Next, this:\n\nxtabs(frequency ~ social.stratum + college.plans, data = wisc) %&gt;%\n  prop.table(margin = 1)\n\n              college.plans\nsocial.stratum         no        yes\n   higher      0.33654618 0.66345382\n   lower       0.85391304 0.14608696\n   lowermiddle 0.97072419 0.02927581\n   uppermiddle 0.66467463 0.33532537\n\n\nIn this one (and the next), college.plans is the response, in columns, so we want to have the rows adding up to 1.\nThe higher the social stratum, the more likely is a male high school senior to have plans to go to college. (The social stratum is not in order, so you’ll have to jump from the second row to the third to the fourth to the first to assess this. Lower and lower middle are not in order, but the others are.)\nFinally, this:\n\nxtabs(frequency ~ encouragement + college.plans, data = wisc) %&gt;%\n  prop.table(margin = 1)\n\n             college.plans\nencouragement        no       yes\n         high 0.4621590 0.5378410\n         low  0.9472265 0.0527735\n\n\nAnd here you see an enormous effect of parental encouragement on college plans: if it is low, the high-school senior is very unlikely to be considering college.\nNothing, in all honesty, that is very surprising here. But the two-way interactions are easier to interpret than a three-way one would have been.\nHere, we think of college plans as being a response, and this analysis has shown that whether or not a student has plans to go to college depends separately on the socio-economic status and the level of parental encouragement (rather than on the combination of both, as would have been the case had the three-way interaction been significant).\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "loglin.html#predicting-voting-1",
    "href": "loglin.html#predicting-voting-1",
    "title": "39  Frequency table analysis",
    "section": "39.5 Predicting voting",
    "text": "39.5 Predicting voting\n1257 British voters were classified according to their social class, age (categorized), sex and the political party they voted for (Labour or Conservative). Which, if any, of these factors influences the party that someone votes for? The data are in link, one voter per line.\n\nRead in the data and display (some of) the data frame.\n\nSolution\nSpace-delimited:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/voting.txt\"\nvote0 &lt;- read_delim(my_url, \" \")\n\nRows: 1257 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (4): class, age, sex, vote\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvote0\n\n\n\n  \n\n\n\nI gave it a “disposable” name, since we make the “real” data set shortly.\n\\(\\blacksquare\\)\n\nThere is no frequency column here, because each row of the data frame only represents one voter. Count up the frequencies for each combo of the categorical variables, and save it (this is the data frame that we will use for the analysis). Display the first few rows of the result. Do you now have something that you need?\n\nSolution\nI changed my mind about how to do this from last year. Using count is alarmingly more direct than the method I had before:\n\nvotes &lt;- vote0 %&gt;% count(class, age, sex, vote)\nvotes\n\n\n\n  \n\n\n\nExactly the right thing now: note the new column n with frequencies in it. (Without a column of frequencies we can’t fit a log-linear model.) There are now only 58 combinations of the four categorical variables, as opposed to 1247 rows in the original data set (with, inevitably, a lot of repeats).\n\\(\\blacksquare\\)\n\nFit a log-linear model with the appropriate interaction (as a starting point).\n\nSolution\n\nvote.1 &lt;- glm(n ~ class * age * sex * vote, data = votes, family = \"poisson\")\n\n\\(\\blacksquare\\)\n\nRefine your model by taking out suitable non-significant terms, in multiple steps. What model do you finish with?\n\nSolution\nAlternating drop1 and update until everything remaining is significant:\n\ndrop1(vote.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nNot anywhere near significant, so out it comes:\n\nvote.2 &lt;- update(vote.1, . ~ . - class:age:sex:vote)\ndrop1(vote.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nTake out the first one, since it has the highest P-value:\n\nvote.3 &lt;- update(vote.2, . ~ . - class:age:sex)\ndrop1(vote.3, test = \"Chisq\")\n\n\n\n  \n\n\n\nclass:sex:vote:\n\nvote.4 &lt;- update(vote.3, . ~ . - class:sex:vote)\ndrop1(vote.4, test = \"Chisq\")\n\n\n\n  \n\n\n\nclass:sex:\n\nvote.5 &lt;- update(vote.4, . ~ . - class:sex)\ndrop1(vote.5, test = \"Chisq\")\n\n\n\n  \n\n\n\nI don’t like having three-way interactions, so I’m going to yank age:sex:vote now, even though its P-value is smallish:\n\nvote.6 &lt;- update(vote.5, . ~ . - age:sex:vote)\ndrop1(vote.6, test = \"Chisq\")\n\n\n\n  \n\n\n\nThe age-sex interaction can go, but we must be near the end now:\n\nvote.7 &lt;- update(vote.6, . ~ . - age:sex)\ndrop1(vote.7, test = \"Chisq\")\n\n\n\n  \n\n\n\nAnd that’s it. The age and sex main effects are not included in the list of droppable things because both variables are part of higher-order interactions that are still in the model.\nIf you want to, you can look at the summary of your final model:\n\nsummary(vote.7)\n\n\nCall:\nglm(formula = n ~ class + age + sex + vote + class:age + class:vote + \n    age:vote + sex:vote + class:age:vote, family = \"poisson\", \n    data = votes)\n\nCoefficients: (1 not defined because of singularities)\n                                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                            2.50737    0.21613  11.601  &lt; 2e-16 ***\nclassupper middle                     -0.45199    0.34188  -1.322 0.186151    \nclassworking                           0.37469    0.27696   1.353 0.176088    \nage&gt;75                                -0.25783    0.32292  -0.798 0.424622    \nage26-40                               0.34294    0.27877   1.230 0.218619    \nage41-50                               0.93431    0.25162   3.713 0.000205 ***\nage51-75                               0.89794    0.25293   3.550 0.000385 ***\nsexmale                               -0.23242    0.08016  -2.900 0.003737 ** \nvotelabour                            -0.50081    0.33324  -1.503 0.132882    \nclassupper middle:age&gt;75               0.25783    0.49713   0.519 0.604013    \nclassworking:age&gt;75                    0.01097    0.41896   0.026 0.979113    \nclassupper middle:age26-40             0.82466    0.41396   1.992 0.046358 *  \nclassworking:age26-40                  0.41083    0.35167   1.168 0.242713    \nclassupper middle:age41-50             0.37788    0.39239   0.963 0.335542    \nclassworking:age41-50                 -0.28917    0.33310  -0.868 0.385327    \nclassupper middle:age51-75             0.43329    0.39277   1.103 0.269954    \nclassworking:age51-75                  0.10223    0.32668   0.313 0.754325    \nclassupper middle:votelabour          -0.12338    0.53898  -0.229 0.818936    \nclassworking:votelabour                1.05741    0.39259   2.693 0.007073 ** \nage&gt;75:votelabour                     -0.72300    0.57745  -1.252 0.210547    \nage26-40:votelabour                    0.21667    0.41944   0.517 0.605452    \nage41-50:votelabour                   -0.93431    0.43395  -2.153 0.031315 *  \nage51-75:votelabour                   -0.62601    0.41724  -1.500 0.133526    \nsexmale:votelabour                     0.37323    0.11334   3.293 0.000992 ***\nclassupper middle:age&gt;75:votelabour         NA         NA      NA       NA    \nclassworking:age&gt;75:votelabour        -0.29039    0.68720  -0.423 0.672607    \nclassupper middle:age26-40:votelabour -0.53698    0.65445  -0.821 0.411931    \nclassworking:age26-40:votelabour      -0.28479    0.49429  -0.576 0.564516    \nclassupper middle:age41-50:votelabour -0.01015    0.68338  -0.015 0.988147    \nclassworking:age41-50:votelabour       1.06121    0.50772   2.090 0.036603 *  \nclassupper middle:age51-75:votelabour -0.06924    0.65903  -0.105 0.916328    \nclassworking:age51-75:votelabour       0.16608    0.49036   0.339 0.734853    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 797.594  on 57  degrees of freedom\nResidual deviance:  22.918  on 27  degrees of freedom\nAIC: 350.41\n\nNumber of Fisher Scoring iterations: 4\n\n\nThese tend to be rather unwieldy, and we’ll see a better way of understanding the results below, but you can look for the very significant results, bearing in mind that the first category is the baseline, for example, more of the males in the survey voted Labour (than Conservative).\n\\(\\blacksquare\\)\n\nIf we think of the party someone votes for as the final outcome (that depends on all the other things), what does our final model say that someone’s vote depends on?\n\nSolution\nFind out which of the surviving terms are interactions with vote. Here, there are two things, that vote depends on separately:\n\nsex\nThe age-class interaction.\n\n\\(\\blacksquare\\)\n\nObtain sub-tables that explain how vote depends on any of the things it’s related to.\n\nSolution\nThis is xtabs again. The 3-way interaction is a bit tricky, so we’ll do the simple one first:\n\nxtabs(n ~ vote + sex, data = votes) %&gt;%\n  prop.table(margin = 2)\n\n              sex\nvote              female      male\n  conservative 0.5474339 0.4543974\n  labour       0.4525661 0.5456026\n\n\nThe female voters slightly preferred to vote Conservative and the male voters slightly preferred to vote Labour. This is a small effect, but I guess the large number of voters made it big enough to be significant.\nI took it this way around because vote is the outcome, and therefore I want to address things like “if a voter is female, how likely are they to vote Labour”, rather than conditioning the other way around (which would be “if a voter voted Labour, how likely are they to be female”, which doesn’t make nearly so much sense).\nThen the tricky one:\n\nxt &lt;- xtabs(n ~ vote + age + class, data = votes)\nxt\n\n, , class = lower middle\n\n              age\nvote           &lt;26 &gt;75 26-40 41-50 51-75\n  conservative  22  17    31    56    54\n  labour        16   6    28    16    21\n\n, , class = upper middle\n\n              age\nvote           &lt;26 &gt;75 26-40 41-50 51-75\n  conservative  14  14    45    52    53\n  labour         9   0    21    13    17\n\n, , class = working\n\n              age\nvote           &lt;26 &gt;75 26-40 41-50 51-75\n  conservative  32  25    68    61    87\n  labour        67  19   133   145   115\n\n\nDoing it this way has produced different subtables for each class. This is actually OK, because we can say “if a voter was of lower middle class” and then talk about the relationship between age and vote, as if we were looking at a simple effect:\n\nIf a voter was of lower-middle-class, they strongly favour voting Conservative in all age groups except for &lt;26 and 26–40.\nIf a voter was of upper-middle-class, they even more strongly favour voting Conservative in all age groups except for “under 26” and maybe 26–40.\nIf a voter was of Working class, they strongly favour voting Labour, except in the “over 75” age group (and maybe 51–75 as well).\n\nIf the anomalous age group(s) had been the same one every time, there would no longer have been an interaction between age and class in their effect on vote. But the anomalous age groups were different for each class (“different pattern”), and that explains why there was a vote:age:class interaction: ” the way someone votes depends on the combination of age and social class”.\nFor prop.table in three dimensions, as we have here, we have to be a little more careful about what to make add up to 1. For example, to make the social classes each add up to 1, which is the third dimension:\n\nprop.table(xt, 3)\n\n, , class = lower middle\n\n              age\nvote                  &lt;26        &gt;75      26-40      41-50      51-75\n  conservative 0.08239700 0.06367041 0.11610487 0.20973783 0.20224719\n  labour       0.05992509 0.02247191 0.10486891 0.05992509 0.07865169\n\n, , class = upper middle\n\n              age\nvote                  &lt;26        &gt;75      26-40      41-50      51-75\n  conservative 0.05882353 0.05882353 0.18907563 0.21848739 0.22268908\n  labour       0.03781513 0.00000000 0.08823529 0.05462185 0.07142857\n\n, , class = working\n\n              age\nvote                  &lt;26        &gt;75      26-40      41-50      51-75\n  conservative 0.04255319 0.03324468 0.09042553 0.08111702 0.11569149\n  labour       0.08909574 0.02526596 0.17686170 0.19281915 0.15292553\n\n\nWhat happened here is that each of the three subtables adds up to 1, so that we have a “joint distribution” in each table. We can put two variables into prop.table, and see what happens then:\n\nprop.table(xt, c(2, 3))\n\n, , class = lower middle\n\n              age\nvote                 &lt;26       &gt;75     26-40     41-50     51-75\n  conservative 0.5789474 0.7391304 0.5254237 0.7777778 0.7200000\n  labour       0.4210526 0.2608696 0.4745763 0.2222222 0.2800000\n\n, , class = upper middle\n\n              age\nvote                 &lt;26       &gt;75     26-40     41-50     51-75\n  conservative 0.6086957 1.0000000 0.6818182 0.8000000 0.7571429\n  labour       0.3913043 0.0000000 0.3181818 0.2000000 0.2428571\n\n, , class = working\n\n              age\nvote                 &lt;26       &gt;75     26-40     41-50     51-75\n  conservative 0.3232323 0.5681818 0.3383085 0.2961165 0.4306931\n  labour       0.6767677 0.4318182 0.6616915 0.7038835 0.5693069\n\n\nThis is making each class-age combination add up to 1, so that we can clearly see what fraction of voters voted for each party in each case.3 In the first two subtables, the two youngest subgroups are clearly different from the others, with a smaller proportion of people voting Conservative rather than Labour than for the older subgroups. If that same pattern persisted for the third subtable, with the two youngest age groups being different from the three older ones, then we would have an age by vote interaction rather than the age by class by vote interaction that we actually have. So the third class group should be different. It is: it seems that the first three age groups are different from the other two, with ages 41–50 being more inclined to vote Labour, like the younger groups. That’s where the interaction came from.\nThe Labour Party in the UK is like the NDP here, in that it has strong ties with “working people”, trades unions in particular. The Conservatives are like the Conservatives here (indeed, the nickname “Tories” comes from the UK; the Conservatives there were officially known as the Tories many years ago). Many people are lifelong voters for their party, and would never think of voting for the “other side”, in the same way that many Americans vote either Democrat or Republican without thinking about it too much. Our parliamentary system comes from the UK system (vote for a candidate in a riding, the leader of the party with the most elected candidates becomes Prime Minister), and a “landslide” victory often comes from persuading enough of the voters open to persuasion to switch sides. In the UK, as here, the parties’ share of the popular vote doesn’t change all that much from election to election, even though the number of seats in Parliament might change quite a lot.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "loglin.html#brand-m-laundry-detergent-1",
    "href": "loglin.html#brand-m-laundry-detergent-1",
    "title": "39  Frequency table analysis",
    "section": "39.6 Brand M laundry detergent",
    "text": "39.6 Brand M laundry detergent\nA survey was carried out comparing respondents’ preferences of a laundry detergent M compared to a mystery brand X. For each respondent, the researchers recorded the temperature of the laundry load (low or high), whether or not they previously used brand M (yes or no), and the softness of the water used for the laundry load(hard, medium or soft). The aim of the survey was to find out what was associated with the respondents preferring brand M. The data are in http://ritsokiguess.site/datafiles/brand_m.csv.\n\nRead in and display (some of) the data. Explain briefly how the data is laid out appropriately to fit a log-linear model.\n\nSolution\nThe reading-in is entirely familiar:\n\nmy_url &lt;- \"http://ritsokiguess.site/datafiles/brand_m.csv\"\nprefs &lt;- read_csv(my_url)\n\nRows: 24 Columns: 5\n── Column specification ──────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): softness, m_user, temperature, prefer\ndbl (1): frequency\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprefs\n\n\n\n  \n\n\n\nThis is good because we have each “observation” (frequency, here) in one row, or, said differently, we have a column of frequencies and each of the factors in a column of its own. (See the Extra for the kind of layout we might have had to deal with.)\nExtra: as you might expect, this is very much not how the data came to me. It was originally in a textbook, laid out like this:\n\nThis is a common layout for frequency data, because it saves a lot of space. Multiple header rows are hard to deal with, though, so I combined the three column variables into one with a layout like this (aligned in columns):\nsoftness no_low_x no_low_m no_high_x no_high_m yes_low_x yes_low_m yes_high_x yes_high_m\nhard        68       42        42       30         37        52        24        43\nmedium      66       50        33       23         47        55        23        47\nsoft        63       53        29       27         57        49        19        29\nLet’s read this in and then think about what to do with it:\n\nprefs0 &lt;- read_table(\"brand_m.txt\")\n\n\n── Column specification ──────────────────────────────────────────────────────────────────\ncols(\n  softness = col_character(),\n  no_low_x = col_double(),\n  no_low_m = col_double(),\n  no_high_x = col_double(),\n  no_high_m = col_double(),\n  yes_low_x = col_double(),\n  yes_low_m = col_double(),\n  yes_high_x = col_double(),\n  yes_high_m = col_double()\n)\n\nprefs0\n\n\n\n  \n\n\n\nWe want to get all those frequencies into one column, which suggest some kind of pivot_longer.There are two ways to go about this. One is to try a regular pivot_longerand see what happens. I had to think for a moment about what to call the column that ended up as combo:\n\nprefs0 %&gt;% pivot_longer(-softness, names_to = \"combo\", values_to = \"frequency\")\n\n\n\n  \n\n\n\nThis is the right kind of shape, but those things in the column combo are three variables all smooshed together: respectively, previous user (of brand M), temperature, preference (you can tell by the values, which are all different). These can be split up with separate, thus:\n\nprefs0 %&gt;% pivot_longer(-softness, names_to = \"combo\", values_to = \"frequency\") %&gt;% \nseparate(combo, into = c(\"prev_user\", \"temperature\", \"preference\"), sep = \"_\")\n\n\n\n  \n\n\n\nThat works, but the combination of pivot_longer and separate is a common one, and so there is an “advanced” version of pivot_longer that does it all at once. The idea is that you enter three columns into names_to and then use names_sep to say what they’re separated by:\n\nprefs0 %&gt;% \npivot_longer(-softness, \nnames_to = c(\"m_user\", \"temperature\", \"prefer\"),\nnames_sep = \"_\", values_to = \"frequency\") \n\n\n\n  \n\n\n\nThis data frame is what I saved for you.\n\\(\\blacksquare\\)\n\nUsing backward elimination, build a suitable log-linear model for the associations between the variables. (Do not use step; do the elimination yourself).\n\nSolution\nThe first step is to fit a model containing all the interactions between the factors, using frequency as the response, and then to use drop1 with test=\"Chisq\" to see what can come out. Don’t forget the family = \"poisson\", since that’s what drives the modelling. I tnink it’s easiest to number these models, since there might be a lot of them:\n\nprefs.1 &lt;- glm(frequency~softness*m_user*temperature*prefer, family = \"poisson\", data=prefs)\ndrop1(prefs.1, test = \"Chisq\")\n\n\n\n  \n\n\n\nTo our relief, the four-way interaction is not significant and can be removed. (I was not looking forward to the prospect of interpreting that!)\nNow write an update line that removes that four-way interaction from your model, as shown below, and copy-paste your drop1 line from above, changing the number of your model to the one coming out of update. Copy-paste the complicated interaction from the drop1 output:\n\nprefs.2 &lt;- update(prefs.1, .~.-softness:m_user:temperature:prefer)\ndrop1(prefs.2, test = \"Chisq\")\n\n\n\n  \n\n\n\nThere are now four three-way interactions that could be removed. You might suspect that they are all going to go eventually, but as in regression, we take them one at a time, starting with the one that has the highest P-value (just in case, for example, the P-value of the second one goes under 0.05 when we remove the others). The easiest way to do the coding is a vigorous amount of copying and pasting. Copy-paste your last code chunk:\n\nprefs.2 &lt;- update(prefs.1, .~.-softness:m_user:temperature:prefer)\ndrop1(prefs.2, test = \"Chisq\")\n\nChange the interaction in the update to the one you want to remove (from the drop1 table), which is softness:temperature:prefer (you can copy-paste that too), and then increase all three of the model numbers by 1:\n\nprefs.3 &lt;- update(prefs.2, .~.-softness:temperature:prefer)\ndrop1(prefs.3, test = \"Chisq\")\n\n\n\n  \n\n\n\nThen, as they say, rinse and repeat. This one takes a while, but each step is just like the others.\ndrop softness:m_user:temperature\n\nprefs.4 &lt;- update(prefs.3, .~.-softness:m_user:temperature)\ndrop1(prefs.4, test = \"Chisq\")\n\n\n\n  \n\n\n\ndrop m_user:temperature:prefer\n\nprefs.5 &lt;- update(prefs.4, .~.-m_user:temperature:prefer)\ndrop1(prefs.5, test = \"Chisq\")\n\n\n\n  \n\n\n\ndrop m_user:temperature\n\nprefs.6 &lt;- update(prefs.5, .~.-m_user:temperature)\ndrop1(prefs.6, test = \"Chisq\")\n\n\n\n  \n\n\n\ndrop softness:m_user:prefer\n\nprefs.7 &lt;- update(prefs.6, .~.-softness:m_user:prefer)\ndrop1(prefs.7, test = \"Chisq\")\n\n\n\n  \n\n\n\nMore two-ways.\ndrop softness:prefer\n\nprefs.8 &lt;- update(prefs.7, .~.-softness:prefer)\ndrop1(prefs.8, test = \"Chisq\")\n\n\n\n  \n\n\n\nOne more?\ndrop softness:m_user\n\nprefs.9 &lt;- update(prefs.8, .~.-softness:m_user)\ndrop1(prefs.9, test = \"Chisq\")\n\n\n\n  \n\n\n\nAnd finally we are done! There are three two-way interactions left, which shouldn’t be too hard to interpret. That’s in the next part.\n\\(\\blacksquare\\)\n\nWhat is associated with the brand a respondent prefers? By obtaining suitable frequency tables, describe the nature of these associations.\n\nSolution\nTo see what is associated with brand preference, look for significant associations with prefer. There are two of them, one with m_user, and, separately, one with temperature. This means that a respondent’s brand preference depends on whether or not they previously used brand M, and also on what temperature the laundry was washed at.\nTo investigate these, use xtabs to get a frequency table (easiest), and if necessary use prop.table to get row or column proportions as appropriate. The first thing in the model formula in xtabs is the frequency column; the first thing after the squiggle is rows and the second thing is columns. (If there happened to be a third thing, it would be “layers” in the table.) You might find that the table of frequencies is enough to interpret what is going on, but if not, use prop.table to get a table of proportions. I talk about that in a moment.\n\nxt &lt;- xtabs(frequency ~ m_user + prefer, data = prefs)\nxt\n\n      prefer\nm_user   m   x\n   no  225 301\n   yes 275 207\n\n\nprefer is the response (outcome), which I put in the columns, so I look along the rows to see what is going on. Out of the people who were previous users of Brand M (second row), slightly more of them preferred Brand M; out of the people who were not Brand M users (first row), somewhat more of them preferred Brand X.\nThis was not hard to see, because the opposite frequencies were bigger each time. But you might find it easier to compute percentages and compare those. In my table, the response was rows, so the right percentages to compute are row ones. That is done like this. 1 is rows, 2 is columns:\n\nprop.table(xt, margin = 1)\n\n      prefer\nm_user         m         x\n   no  0.4277567 0.5722433\n   yes 0.5705394 0.4294606\n\n\nOut of the people who were previous Brand M users, 57% preferred Brand M; out of the people who were not previous Brand M users, only 43% of them preferred Brand M.\nAdvertisers use terms like “brand familiarity” to capture ideas like this: more people prefer Brand M in the survey if they have used it before. Not altogether surprising.\nOn to the effects of temperature on preference:\n\nxt &lt;- xtabs(frequency ~ temperature + prefer, data = prefs)\nxt\n\n           prefer\ntemperature   m   x\n       high 199 170\n       low  301 338\n\nprop.table(xt, margin = 1)\n\n           prefer\ntemperature         m         x\n       high 0.5392954 0.4607046\n       low  0.4710485 0.5289515\n\n\nOut of the people who used a high-temperature wash, 54% of them preferred brand M, but out of the people who used a low-temperature wash, only 47% of them preferred brand M.\nI’m making it seem like this is a big difference, and of course it’s a very small one, but the size of the survey makes even this tiny difference significant.\nThose are really the two effects of interest, since they are the ones associated with brand preference. But there was one more association that was significant: between temperature and softness. The softness in this case was of the water used to do the laundry (and not, for example, the softness of the clothes after they come out of the dryer). That goes like this:\n\nxt &lt;- xtabs(frequency ~ temperature + softness, data = prefs)\nxt\n\n           softness\ntemperature hard medium soft\n       high  139    126  104\n       low   199    218  222\n\nprop.table(xt, margin = 2)\n\n           softness\ntemperature      hard    medium      soft\n       high 0.4112426 0.3662791 0.3190184\n       low  0.5887574 0.6337209 0.6809816\n\n\nI decided to condition on the softness of the water, since that cannot be controlled by the person doing the laundry (the water just is hard or soft, depending on where you live and where your water comes from).\nIn each case, a majority of the washes were done at low temperature, but the softer the water, the bigger that majority was. Once again, the effect is not all that big, because the association was only just significant.\n\\(\\blacksquare\\)"
  },
  {
    "objectID": "loglin.html#footnotes",
    "href": "loglin.html#footnotes",
    "title": "39  Frequency table analysis",
    "section": "",
    "text": "I don’t know why Wisconsin again, but that’s what it is.↩︎\nI don’t know why Wisconsin again, but that’s what it is.↩︎\nThe reason I thought of doing this is that these two are all the variables except response.↩︎"
  }
]