{
  "hash": "301c27ab529fc9f7b29975f5462ae3b9",
  "result": {
    "engine": "knitr",
    "markdown": "# The sign test\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.5.1     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(smmr)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Running a maze\n\n\n A researcher is trying to design a maze that can be run by\nrats in about 60 seconds. One particular maze was run by a sample of\n21 rats, with the times shown in\n[link](http://ritsokiguess.site/datafiles/maze.txt). \n\n\n\n(a) Read the data into R. What (if anything) are the data values\ndelimited by?\n\n\n(b) Run a sign test, doing it yourself as we did in class:\ncount the number of values above and below 60, take the *smaller*\nof those, and find the probability of a value of that or smaller still\non a binomial distribution with $n=21$ and $p=0.5$ (we have 21 data\npoints), doubling the answer because the test is two-sided.\n\n\n\n\n(c) Install my package `smmr`, if you haven't already. To do\nthis, you first need to install the package `devtools` (if you\nhaven't already),\nby going to the console and typing\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"devtools\")\n```\n:::\n\n \n\nWhen that's all done, install `smmr` thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(devtools)\ninstall_github(\"nxskok/smmr\")\n```\n:::\n\n \n\nThat all needs to be done only once. Then, each R Studio session where\nyou want to use `smmr` needs this:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\n```\n:::\n\n \n\nAs usual, only the `library` thing only needs to be done every\ntime. \n\nWhen you have `smmr` installed, use `sign_test` from\nthat package to re-run your sign test. Do you get the same P-value?\n\n\n\n\n(d) Package `smmr` also has a function\n`pval_sign`, which has the same input as\n`sign_test`, but with the null median *first*.\nRun it on your data and see what it gives.\n\n\n\n\n\n(e) Obtain a 95\\% confidence interval for the median based on these\ndata. Do this two ways. First, use the trial and error way from class\n(either the try-lots-of-values way or the bisection way; either is good).\nSecond, use `ci_median` from `smmr`. The latter takes\nas input a data frame, a column name (unquoted) and optionally a\n`conf.level` that defaults to 0.95.\n\n\n\n\n\n\n\n## Chocolate chips\n\n\n\n A famous cookie manufacturer claims that\ntheir bags of chocolate chip cookies contain \"more than 1100 chocolate chips on average\". A diligent group of students buys 16\nbags of these cookies and counts the number of chocolate chips in each\nbag. The results are in [http://ritsokiguess.site/datafiles/chips.txt](http://ritsokiguess.site/datafiles/chips.txt).\n\n\n\n(a) Read in and display (some of) the data.\n\n\n\n(b) Build your own sign test in R for testing that the median is\n1100 chocolate chips, against the alternative that it is greater.\n(Do this as in class: count the appropriate thing,\ncompare it with an appropriate binomial distribution, and obtain a\nP-value.\n\n\n\n(c) Use my R package `smmr` to reproduce your sign test\nabove, and verify that you get consistent results. (See the\nmaze-design question for instructions on installing this, if you\nhaven't yet.)\n\n\n\n(d) Use `smmr` to obtain a 95\\% confidence interval for the\nmedian number of chocolate chips per bag of cookies.\n\n\n\n\n\n\n\n##  The power of the sign test\n\n\n I've mentioned several times that the sign test has less\npower than the $t$-test. Let's investigate this with a specific example.\n\nLet's suppose we are testing $H_0: \\mu=40$ against $H_a: \\mu \\ne 40$,\nwhere $\\mu$ is the population mean (and median, as we shall see). Our\npopulation actually has a normal distribution with mean 50 and SD 15,\nso that the null hypothesis is *wrong* and we want to reject it\nmost of the time. On the other hand, the population actually *is*\nnormally-distributed and so the $t$-test is the right one to use.\n\n(This is an old question, so I tackle the simulated power differently\nthan I did it in class this time. But see if you can follow what I do\nhere.)\n\n\n\n(a) Use `power.t.test` to find the probability that a\n$t$-test correctly rejects the null hypothesis using a sample size\nof $n=10$.\n\n\n\n(b) What code in R would draw a random sample of size 10 from the\n*true* population distribution and save the sample in a variable?\n\n\n\n\n(c) What code would count how many of the sampled values are less\nthan 40 and how many are greater (or equal)? \n\n\n\n\n(d) It turns out the sign test would reject $H_0: M=40$ against\n$H_a: M \\ne 40$ at $\\alpha=0.05$ if the smaller of the numbers in the\nlast part is 1 or less. ($M$ is the population median.) \nAdd to your pipeline to obtain `TRUE`\nif you should reject the null for your\ndata and `FALSE` otherwise. \n\n\n\n\n(e) Simulate the above process 1000 times:\ndraw a random sample from a normal distribution of size 10 with mean 50 and SD\n15, count the number of values below 40, reject if the\nminimum of those is 0, 1, 9, or 10, then count the number of rejections\nout of 1000.\n\n\n\n\n(f) Which is more powerful in this case, the sign test or the\n$t$-test? How do you know?\n\n\n\n\n\n\n\n\n## Ben Roethlisberger\n\n Ben Roethlisberger plays (American) football for the Pittsburgh  Steelers. He plays as a quarterback, which means that his job is to throw (pass) the ball so that one of his teammates can catch it. Each time he makes a pass that is caught, this is called a \"completion\", and the team coaches are interested in his average number of completions per game (this average could be the mean or the median).\n\nIn 2010, Roethlisberger was suspended for the first four games of the season, and there was concern that this might affect his performance (in terms of the number of passes completed in the games after he returned). The Pittsburgh Steelers did not play in week 5 of the 2010 season; the season is 17 weeks long (one game per week) and each team has one week in which they do not play.\n\nThe data are [here](http://ritsokiguess.site/datafiles/roethlisberger.csv). There are four columns: the year (always 2010), the week number of the season that the game was played in, the name of the opposing team, and the number of completed passes by Roethlisberger in the game.\n\n\n\n(a) Read in and display (some of) the data. Do you have what you were expecting?\n\n\n\n(b) Make a suitable graph of the number of completed passes, and explain briefly why you would have some doubts about using $t$-procedures in this situation.\n\n\n\n(c) Run a sign test to compare Roethlisberger's performance in 2010 with his previous average of 22.5 completions per game. What do you conclude?\n\n\n\n(d) Why might you have expected your sign test to come out significant, even without looking at the P-value? Explain briefly. \n\n\n\n(e) Obtain a 90% confidence interval for the median number of completed passes (over \"all possible games played by 2010 Ben Roethlisberger\"). \n\n\n\n(f) Find a 90% confidence interval for the *mean* number of passes completed, and explain briefly why it differs from the one for the median in the way that it does.\n\n\n\n\n\n\n\n## Six ounces of protein\n\n A company produces prepackaged  diet meals. These meals are advertised as containing \"6 ounces of protein per package\". A consumer organization is concerned that this is not accurate. The organization takes a random sample of 20 of these meals, and measures the protein content of each one. The data are in [http://ritsokiguess.site/datafiles/protein.txt](http://ritsokiguess.site/datafiles/protein.txt)\nas one column. \n\n\n\n(a) Read in and display (some of) the data.\n\n\n\n(b) Make a suitable graph of your data.\n\n\n\n(c) Why might a sign test be better than a $t$-test for assessing the average amount of protein per package? Explain briefly. (\"Average\" here means any measure of centre.)\n\n\n\n(d) Run a suitable sign test for these data. What do you conclude?\n\n\n\n(e) In your sign test, how could you have deduced that the P-value was going to be small even without looking at any of the P-values themselves? Explain briefly.\n\n\n\n(f) Obtain a 90% confidence interval for the population median protein content. What does this tell you about the reason for the rejection or non-rejection of the null hypothesis above?\n\n\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Running a maze\n\n\n A researcher is trying to design a maze that can be run by\nrats in about 60 seconds. One particular maze was run by a sample of\n21 rats, with the times shown in\n[link](http://ritsokiguess.site/datafiles/maze.txt). \n\n\n\n(a) Read the data into R. What (if anything) are the data values\ndelimited by?\n\nSolution\n\n\nTake a look at the data file first. There is only one column of\ndata, so you can treat it as being delimited by anything you like:\na space, or a comma (the file can also be treated as a\n`.csv`), etc.:\n\n::: {.cell}\n\n```{.r .cell-code}\nmyurl <- \"http://ritsokiguess.site/datafiles/maze.txt\"\ntimes <- read_delim(myurl, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 21 Columns: 1\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\ndbl (1): time\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ntimes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 x 1\n    time\n   <dbl>\n 1  38.4\n 2  46.2\n 3  62.5\n 4  38  \n 5  62.8\n 6  33.9\n 7  50.4\n 8  35  \n 9  52.8\n10  60.1\n# i 11 more rows\n```\n\n\n:::\n:::\n\n \n\n$\\blacksquare$\n\n(b) Run a sign test, doing it yourself as we did in class:\ncount the number of values above and below 60, take the *smaller*\nof those, and find the probability of a value of that or smaller still\non a binomial distribution with $n=21$ and $p=0.5$ (we have 21 data\npoints), doubling the answer because the test is two-sided.\n\n\n\nSolution\n\n\n\nCount how many values are above and below 60:\n\n::: {.cell}\n\n```{.r .cell-code}\ntimes %>% count(time > 60)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  `time > 60`     n\n  <lgl>       <int>\n1 FALSE          16\n2 TRUE            5\n```\n\n\n:::\n:::\n\n \n5 above and 16 below. Then find out how likely it is that a binomial\nwith $n=21, p=0.5$ would produce 5 or fewer successes:\n\n::: {.cell}\n\n```{.r .cell-code}\np <- sum(dbinom(0:5, 21, 0.5))\np\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01330185\n```\n\n\n:::\n:::\n\n \n\nor if you prefer count upwards from 16:\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(dbinom(16:21, 21, 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01330185\n```\n\n\n:::\n:::\n\n \n\nand double it to get a two-sided P-value:\n\n::: {.cell}\n\n```{.r .cell-code}\n2 * p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0266037\n```\n\n\n:::\n:::\n\n \n\nWe'll compare this with `smmr` in a moment.\n\n\n$\\blacksquare$\n\n(c) Install my package `smmr`, if you haven't already. To do\nthis, you first need to install the package `devtools` (if you\nhaven't already),\nby going to the console and typing\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"devtools\")\n```\n:::\n\n \n\nWhen that's all done, install `smmr` thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(devtools)\ninstall_github(\"nxskok/smmr\")\n```\n:::\n\n \n\nThat all needs to be done only once. Then, each R Studio session where\nyou want to use `smmr` needs this:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\n```\n:::\n\n \n\nAs usual, only the `library` thing only needs to be done every\ntime. \n\nWhen you have `smmr` installed, use `sign_test` from\nthat package to re-run your sign test. Do you get the same P-value?\n\n\n\nSolution\n\n\nThe sign test function takes a data frame, an (unquoted) column\nname from that data frame of data to test the median of, and a\nnull median (which defaults to 0 if you omit it):\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\nsign_test(times, time, 60)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$above_below\nbelow above \n   16     5 \n\n$p_values\n  alternative    p_value\n1       lower 0.01330185\n2       upper 0.99640131\n3   two-sided 0.02660370\n```\n\n\n:::\n:::\n\n     \n\nThis shows you two things: a count of the values below and above the\nnull median, and then the P-values according to the various\nalternative hypotheses you might have. \n\nIn our case, we see again the 16 maze-running times below 60 seconds\nand 5 above (one of which was a long way above, but we don't care\nabout that here). We were testing whether the median was different\nfrom 60, so we look at the two-sided P-value of 0.0266, which is\nexactly what we had before.\n\nIf `sign_test` doesn't work for you (perhaps because it needs\na function `enquo` that you don't have), there is an\nalternative function `sign_test0` that doesn't use it. It\nrequires as input a *column* of values (extracted from the data\nframe) and a null median, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(times, sign_test0(time, 60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$above_below\nbelow above \n   16     5 \n\n$p_values\n  alternative    p_value\n1       lower 0.01330185\n2       upper 0.99640131\n3   two-sided 0.02660370\n```\n\n\n:::\n:::\n\n \n\nThe output should be, and here is, identical.\n\n\n$\\blacksquare$\n\n(d) Package `smmr` also has a function\n`pval_sign`, which has the same input as\n`sign_test`, but with the null median *first*.\nRun it on your data and see what it gives.\n\n\n\nSolution\n\n\nTry it and see:\n\n::: {.cell}\n\n```{.r .cell-code}\npval_sign(60, times, time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0266037\n```\n\n\n:::\n:::\n\n     \n\nThe two-sided P-value, and that is all. We'll be using this in a minute.\n\nAlternatively, there is also this, which needs a null median and a\n*column* as input:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(times, pval_sign0(60, time))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0266037\n```\n\n\n:::\n:::\n\n \n\n$\\blacksquare$\n\n\n(e) Obtain a 95\\% confidence interval for the median based on these\ndata. Do this two ways. First, use the trial and error way from class\n(either the try-lots-of-values way or the bisection way; either is good).\nSecond, use `ci_median` from `smmr`. The latter takes\nas input a data frame, a column name (unquoted) and optionally a\n`conf.level` that defaults to 0.95.\n\n\n\nSolution\n\n\nThe reason for showing you `pval_sign` in the previous\npart is that this is a building block for the confidence interval.\nWhat we do is to try various null medians\nand find out which ones give P-values less than 0.05 (outside the\ninterval) and which ones bigger (inside). \nWe know that the value 60 is\noutside the 95\\% CI, and the sample median is close to 50 (which we\nexpect to be inside), so sensible values to try for the upper end of\nthe interval would be between 50 and 60:\n\n::: {.cell}\n\n```{.r .cell-code}\npval_sign(58, times, time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0266037\n```\n\n\n:::\n\n```{.r .cell-code}\npval_sign(55, times, time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6636238\n```\n\n\n:::\n:::\n\n \n\nSo, 55 is inside the interval and 58 is outside. I could investigate\nfurther in similar fashion, but I thought I would try a whole bunch of null\nmedians all at once. That goes like this, `rowwise` because `pval_sign` expects *one* null-hypothesis median, not several all at once:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(meds = seq(55, 58, 0.25)) %>% \n  rowwise() %>% \n  mutate(pvals = pval_sign(meds, times, time))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 x 2\n# Rowwise: \n    meds  pvals\n   <dbl>  <dbl>\n 1  55   0.664 \n 2  55.2 0.383 \n 3  55.5 0.263 \n 4  55.8 0.189 \n 5  56   0.189 \n 6  56.2 0.189 \n 7  56.5 0.0784\n 8  56.8 0.0784\n 9  57   0.0784\n10  57.2 0.0784\n11  57.5 0.0784\n12  57.8 0.0266\n13  58   0.0266\n```\n\n\n:::\n:::\n\n \n\nSo values for the median all the way up to and including 57.5 are in\nthe confidence interval.\n\n\nNow for the other end of the interval. I'm going to do this a\ndifferent way: more efficient, but less transparent. The first thing I\nneed is a pair of values for the median: one inside the interval and\none outside. Let's try 40 and 50:\n\n::: {.cell}\n\n```{.r .cell-code}\npval_sign(40, times, time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.00719738\n```\n\n\n:::\n\n```{.r .cell-code}\npval_sign(50, times, time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n \n\nOK, so 40 is outside and 50 is inside. So what do I guess for the next\nvalue to try? I could do something clever like assuming that the\nrelationship between hypothesized median and P-value is *linear*,\nand then guessing where that line crosses 0.05. But I'm going to\nassume *nothing* about the relationship except that it goes\nuphill, and therefore crosses 0.05 somewhere. So my next guess is\nhalfway between the two values I tried before:\n\n::: {.cell}\n\n```{.r .cell-code}\npval_sign(45, times, time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07835388\n```\n\n\n:::\n:::\n\n \n\nSo, 45 is inside the interval, and my (slightly) improved guess at the\nbottom end of the interval is that it's between 40 and 45. So next, I\ntry halfway between *those*:\n\n::: {.cell}\n\n```{.r .cell-code}\npval_sign(42.5, times, time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0266037\n```\n\n\n:::\n:::\n\n \n\n42.5 is outside, so the bottom end of the interval is between 42.5 and 45.\n\nWhat we are doing is narrowing down where the interval's bottom end\nis. We started by knowing it to within 10, and now we know it to\nwithin 2.5. So if we keep going, we'll know it as accurately as we wish.\n\nThis is called a \"bisection\" method, because at each step, we're\ndividing our interval by 2.\n\nThere is one piece of decision-making at each step: if the P-value for\nthe median you try is greater than 0.05, that becomes the top end of\nyour interval (as when we tried 45); if it is less, it becomes the\nbottom end (when we tried 42.5).\n\nThis all begs to be automated into a loop. It's not a\n`for`-type loop, because we don't know how many times we'll be\ngoing around. It's a `while` loop: keep going while something\nis true. Here's how it goes:\n\n::: {.cell}\n\n```{.r .cell-code}\nlo <- 40\nhi <- 50\nwhile (abs(hi - lo) > 0.1) {\n  try <- (hi + lo) / 2\n  ptry <- pval_sign(try, times, time)\n  print(c(try, ptry))\n  if (ptry < 0.05) {\n    lo <- try\n  } else {\n    hi <- try\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 45.00000000  0.07835388\n[1] 42.5000000  0.0266037\n[1] 43.7500000  0.0266037\n[1] 44.37500000  0.07835388\n[1] 44.0625000  0.0266037\n[1] 44.2187500  0.0266037\n[1] 44.2968750  0.0266037\n```\n\n\n:::\n\n```{.r .cell-code}\nlo\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 44.29688\n```\n\n\n:::\n\n```{.r .cell-code}\npval_sign(lo, times, time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0266037\n```\n\n\n:::\n\n```{.r .cell-code}\nhi\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 44.375\n```\n\n\n:::\n\n```{.r .cell-code}\npval_sign(hi, times, time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07835388\n```\n\n\n:::\n:::\n\n \n\nThe loop stopped because 44.297 and 44.375 are less than 0.1\napart. The first of those is outside the interval and the second is\ninside. So the bottom end of our interval is 44.375, to this\naccuracy. If you want it more accurately, change 0.1 in the\n`while` line to something smaller (but then you'll be waiting\nlonger for the answer). \n\nI put the `print` statement in the loop so that you could see\nwhat values were being tried, and what P-values they were\nproducing. What happens with these is that the P-value jumps at each\ndata value, so you won't get a P-value exactly 0.05; you'll get one\nabove and one below.\n\nLikewise, you can use the function with a zero on its name and feed it\na column rather than a data frame and a column name:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(meds =  seq(55, 58, 0.25)) %>% \n  rowwise() %>% \n  mutate(pvals =  with(times, pval_sign0(meds, time)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 x 2\n# Rowwise: \n    meds  pvals\n   <dbl>  <dbl>\n 1  55   0.664 \n 2  55.2 0.383 \n 3  55.5 0.263 \n 4  55.8 0.189 \n 5  56   0.189 \n 6  56.2 0.189 \n 7  56.5 0.0784\n 8  56.8 0.0784\n 9  57   0.0784\n10  57.2 0.0784\n11  57.5 0.0784\n12  57.8 0.0266\n13  58   0.0266\n```\n\n\n:::\n:::\n\n \n\nOr adapt the idea I had above for bisection.\nAll that was a lot of work, but I wanted you to see it all once, so that you\nknow where the confidence interval is coming from. `smmr` also\nhas a function `ci_median` that does all of the above without\nyou having to do it. As I first wrote it, it was using the trial and\nerror thing with `rowwise`, but I chose to rewrite it with the\nbisection idea, because I thought that would be more accurate.\n\n::: {.cell}\n\n```{.r .cell-code}\nci_median(times, time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 44.30747 57.59766\n```\n\n\n:::\n:::\n\n\n\n\nThis is a more accurate interval than we got above. (The\n`while` loop for the bisection keeps going until the two\nguesses at the appropriate end of the interval are less than 0.01\napart, by default.)^[You can change this by adding something like *tol=1e-4* to the end of your *ci-median*.] \n\nIf you want some other confidence level, you add `conf.level`\non the end, as you would for `t.test`:\n\n::: {.cell}\n\n```{.r .cell-code}\nci_median(times, time, conf.level = 0.75)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 46.20444 55.49473\n```\n\n\n:::\n:::\n\n \n\nA 75\\% CI, just for fun. This is a shorter interval than the 95\\% one,\nas it should be.\n\nLikewise there is a `ci_median0` that takes a column and an\noptional confidence level:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(times, ci_median0(time))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 44.30747 57.59766\n```\n\n\n:::\n\n```{.r .cell-code}\nwith(times, ci_median0(time, conf.level = 0.75))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 46.20444 55.49473\n```\n\n\n:::\n:::\n\n \n\nwith the same results. Try `ci_median` first, and if it\ndoesn't work, try `ci_median0`.\n\n\n$\\blacksquare$\n\n\n\n\n## Chocolate chips\n\n\n\n A famous cookie manufacturer claims that\ntheir bags of chocolate chip cookies contain \"more than 1100 chocolate chips on average\". A diligent group of students buys 16\nbags of these cookies and counts the number of chocolate chips in each\nbag. The results are in [http://ritsokiguess.site/datafiles/chips.txt](http://ritsokiguess.site/datafiles/chips.txt).\n\n\n\n(a) Read in and display (some of) the data.\n\n\nSolution\n\n\nI'll pretend it's a\n`.csv` this time, just for fun.  Give the data frame a\nname different from `chips`, so that you don't get\nconfused:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/chips.txt\"\nbags <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 16 Columns: 1\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (1): chips\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nbags\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 16 x 1\n   chips\n   <dbl>\n 1  1219\n 2  1214\n 3  1087\n 4  1200\n 5  1419\n 6  1121\n 7  1325\n 8  1345\n 9  1244\n10  1258\n11  1356\n12  1132\n13  1191\n14  1270\n15  1295\n16  1135\n```\n\n\n:::\n:::\n\nThat looks sensible.       \n\n\n$\\blacksquare$\n\n(b) Build your own sign test in R for testing that the median is\n1100 chocolate chips, against the alternative that it is greater.\n(Do this as in class: count the appropriate thing,\ncompare it with an appropriate binomial distribution, and obtain a\nP-value.\n\n\nSolution\n\n\nThe null median is 1100, so we count the number\nof values above and below:\n\n::: {.cell}\n\n```{.r .cell-code}\nbags %>% count(chips<1100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  `chips < 1100`     n\n  <lgl>          <int>\n1 FALSE             15\n2 TRUE               1\n```\n\n\n:::\n:::\n\nThe un-standard thing there is that we can put a logical condition\ndirectly into the `count`. If you don't think of that, you can\nalso do this, which creates a new variable `less` that is\n`TRUE` or `FALSE` for each bag appropriately:\n\n::: {.cell}\n\n```{.r .cell-code}\nbags %>% mutate(less=(chips<1100)) %>% count(less)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  less      n\n  <lgl> <int>\n1 FALSE    15\n2 TRUE      1\n```\n\n\n:::\n:::\n\nor the more verbose\n\n::: {.cell}\n\n```{.r .cell-code}\nbags %>% mutate(less=(chips<1100)) %>%\ngroup_by(less) %>% summarize(howmany=n())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  less  howmany\n  <lgl>   <int>\n1 FALSE      15\n2 TRUE        1\n```\n\n\n:::\n:::\n\nJust one value below, with all the rest above. \nGetting the right P-value, properly, requires some careful thought\n(but you will probably get the right answer anyway). If the\nalternative hypothesis is true, and the median is actually bigger than\n1100 (say, 1200), you would expect half the data values to be bigger\nthan 1200 and half smaller. So *more* than half the data values\nwould be bigger than *1100*, and fewer than half of them would be\nless than 1100. So, if we are going to reject the null (as it looks as\nif we will), that small number of values below 1100 is what we want.\n\nThe P-value is the probability of a value 1 or less in a binomial\ndistribution with $n=16, p=0.5$:\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(dbinom(0:1,16,0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002593994\n```\n\n\n:::\n:::\n\nOr, equivalently, count *up* from 15:\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(dbinom(15:16,16,0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002593994\n```\n\n\n:::\n:::\n\nThis is correctly one-sided, so we don't have to do anything with\nit. \n\n\n$\\blacksquare$\n\n(c) Use my R package `smmr` to reproduce your sign test\nabove, and verify that you get consistent results. (See the\nmaze-design question for instructions on installing this, if you\nhaven't yet.)\n\n\nSolution\n\n\nThis will mean reading the output carefully:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\nsign_test(bags,chips,1100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$above_below\nbelow above \n    1    15 \n\n$p_values\n  alternative      p_value\n1       lower 0.9999847412\n2       upper 0.0002593994\n3   two-sided 0.0005187988\n```\n\n\n:::\n:::\n\nThis time, we're doing a one-sided test, specifically an\n*upper-tail* test, since we are looking for evidence that the\nmedian is *greater than* 1100. The results are exactly what we\ngot \"by hand\": 15 values above and one below, and a P-value (look\nalong the `upper` line) of 0.00026.\n\nAlternatively, you can do this:\n\n::: {.cell}\n\n```{.r .cell-code}\nsign_test0(bags$chips,1100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$above_below\nbelow above \n    1    15 \n\n$p_values\n  alternative      p_value\n1       lower 0.9999847412\n2       upper 0.0002593994\n3   two-sided 0.0005187988\n```\n\n\n:::\n:::\n\nwith the same result (but only go this way if you need to).\n\n\n$\\blacksquare$\n\n(d) Use `smmr` to obtain a 95\\% confidence interval for the\nmedian number of chocolate chips per bag of cookies.\n\n\nSolution\n\n\nOnce everything is in place, this is simplicity itself:\n\n::: {.cell}\n\n```{.r .cell-code}\nci_median(bags,chips)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1135.003 1324.996\n```\n\n\n:::\n:::\n\n1135 to 1325. I would round these off to whole numbers, since the data\nvalues are all whole numbers. These values are all above 1100, which\nsupports the conclusion we got above that the median is above\n1100. This is as it should be, because the CI is \"all those medians that would *not* be rejected by the sign test\". \n\nOr, \n\n::: {.cell}\n\n```{.r .cell-code}\nci_median0(bags$chips)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1135.003 1324.996\n```\n\n\n:::\n:::\n\n$\\blacksquare$\n\n\n\n\n\n##  The power of the sign test\n\n\n I've mentioned several times that the sign test has less\npower than the $t$-test. Let's investigate this with a specific example.\n\nLet's suppose we are testing $H_0: \\mu=40$ against $H_a: \\mu \\ne 40$,\nwhere $\\mu$ is the population mean (and median, as we shall see). Our\npopulation actually has a normal distribution with mean 50 and SD 15,\nso that the null hypothesis is *wrong* and we want to reject it\nmost of the time. On the other hand, the population actually *is*\nnormally-distributed and so the $t$-test is the right one to use.\n\n(This is an old question, so I tackle the simulated power differently\nthan I did it in class this time. But see if you can follow what I do\nhere.)\n\n\n\n(a) Use `power.t.test` to find the probability that a\n$t$-test correctly rejects the null hypothesis using a sample size\nof $n=10$.\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(delta=50-40,n=10,sd=15,type=\"one.sample\",alternative=\"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 10\n          delta = 10\n             sd = 15\n      sig.level = 0.05\n          power = 0.4691805\n    alternative = two.sided\n```\n\n\n:::\n:::\n\nThe power is 0.469. Not great, but we'll see how this stacks up\nagainst the sign test.\n\n\n$\\blacksquare$\n\n(b) What code in R would draw a random sample of size 10 from the\n*true* population distribution and save the sample in a variable?\n\n\n\nSolution\n\n\nThe data actually have a normal distribution with mean 50 and\nSD 15, so we use `rnorm` with this mean and SD, obtaining\n10 values:\n\n::: {.cell}\n\n```{.r .cell-code}\nx=rnorm(10,50,15)  \nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 45.96255 28.61982 52.08105 68.96621 70.39565 12.70975 52.34722 66.26185\n [9] 51.58285 63.31829\n```\n\n\n:::\n:::\n\n\n$\\blacksquare$\n\n(c) What code would count how many of the sampled values are less\nthan 40 and how many are greater (or equal)? \n\n\n\nSolution\n\n\nThe way we know this is to put `x` into a data frame first:\n \n::: {.cell}\n\n```{.r .cell-code}\ntibble(x) %>% count(x<40)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  `x < 40`     n\n  <lgl>    <int>\n1 FALSE        8\n2 TRUE         2\n```\n\n\n:::\n:::\n\n2 values less (and 8 greater-or-equal).\n\n\n$\\blacksquare$\n\n(d) It turns out the sign test would reject $H_0: M=40$ against\n$H_a: M \\ne 40$ at $\\alpha=0.05$ if the smaller of the numbers in the\nlast part is 1 or less. ($M$ is the population median.) \nAdd to your pipeline to obtain `TRUE`\nif you should reject the null for your\ndata and `FALSE` otherwise. \n\n\n\nSolution\n\n\nThis is actually easier than you might think. The output from\n`count` is a data frame with a column called `n`,\nwhose minimum value you want. I add to my pipeline:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(x) %>% count(x<40) %>%\nsummarize(the_min=min(n)) %>%\nmutate(is_rejected=(the_min<=1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 2\n  the_min is_rejected\n    <int> <lgl>      \n1       2 FALSE      \n```\n\n\n:::\n:::\n\nThis will fail sometimes. If all 10 of your sample values are greater\nthan 40, which they might turn out to be, you'll get a table with only\none line, `FALSE` and 10; the minimum of the `n` values\nis 10 (since there is only one), and it will falsely say that you\nshould not reject. The fix is\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(x) %>% count(x<40) %>%\nsummarize(the_min=min(n)) %>%\nmutate(is_rejected=(the_min<=1 | the_min==10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 2\n  the_min is_rejected\n    <int> <lgl>      \n1       2 FALSE      \n```\n\n\n:::\n:::\n\nThe above is almost the right thing, but not quite: we only want that value\nthat I called `is_rejected`, rather than the whole data frame,\nso a `pull` will grab it:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(x) %>% count(x<40) %>%\nsummarize(the_min=min(n)) %>%\nmutate(is_rejected=(the_min<=1 | the_min==10)) %>%\npull(is_rejected)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\nYou might be wondering where the \"1 or less\" came from. Getting a\nP-value for the sign test involves the binomial distribution: if the\nnull is correct, each data value is independently either above or\nbelow 40, with probability 0.5 of each, so the number of values below\n40 (say) is binomial with $n=10$ and $p=0.5$. The P-value for 1\nobserved value below 40 and the rest above is\n\n::: {.cell}\n\n```{.r .cell-code}\n2*pbinom(1,10,0.5)  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02148438\n```\n\n\n:::\n:::\n\nwhich is less than 0.05; the P-value for 2 values below 40 and the\nrest above is \n\n::: {.cell}\n\n```{.r .cell-code}\n2*pbinom(2,10,0.5)    \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.109375\n```\n\n\n:::\n:::\n\nwhich is bigger than 0.05. \n\nYou might have encountered the term \"critical region\" for a\ntest. This is the values of your test statistic that you would reject\nthe null hypothesis for. In this case, the critical region is 1 and 0\nobservations below 40, along with 1 and 0 observations above 40.\n\nWhen you're thinking about power, I think it's easiest to think in\nterms of the critical region (rather than directly in terms of\nP-values) since you have a certain $\\alpha$ in mind all the way\nthrough, 0.05 in the power examples that I've done. The steps are then:\n\n\n\n* Work out the critical region for your test, the values of the\ntest statistic (or sample mean or sample count) that would lead to\nrejecting the null hypothesis.\n\n\n* Under your particular alternative hypothesis, find the\nprobability of falling into your critical region.\n\n\nWhen I say \"work out\", I mean either calculating (along the lines of\nSTAB57), or simulating, as we have done here.\n\n$\\blacksquare$\n\n(e) Simulate the above process 1000 times:\ndraw a random sample from a normal distribution of size 10 with mean 50 and SD\n15, count the number of values below 40, reject if the\nminimum of those is 0, 1, 9, or 10, then count the number of rejections\nout of 1000.\n\n\n\nSolution\n\nSet up a dataframe with a column (called, maybe, `sim`) that counts the number of simulations you are doing, and then use `rowwise` to take a random sample in each row and extract what you need from it.\n\nI start with setting the random number seed, so it comes out the\nsame each time. That way, if I rerun the code, my answers are the same (and I don't have to change my discussion of them.)\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(457299)\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(10, 50, 15)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,000 x 2\n# Rowwise: \n     sim sample    \n   <int> <list>    \n 1     1 <dbl [10]>\n 2     2 <dbl [10]>\n 3     3 <dbl [10]>\n 4     4 <dbl [10]>\n 5     5 <dbl [10]>\n 6     6 <dbl [10]>\n 7     7 <dbl [10]>\n 8     8 <dbl [10]>\n 9     9 <dbl [10]>\n10    10 <dbl [10]>\n# i 990 more rows\n```\n\n\n:::\n:::\n\nEach sample has 10 values in it, not just one, so you need the `list` around the `rnorm`. Note that `sample` is labelled as a list-column.\n\nNow we have to count how many of the sample values are less than 40:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(457299)\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(10, 50, 15))) %>% \n  mutate(less = list(sample<40)) %>% \n  mutate(counted = sum(less)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,000 x 4\n# Rowwise: \n     sim sample     less       counted\n   <int> <list>     <list>       <int>\n 1     1 <dbl [10]> <lgl [10]>       3\n 2     2 <dbl [10]> <lgl [10]>       4\n 3     3 <dbl [10]> <lgl [10]>       2\n 4     4 <dbl [10]> <lgl [10]>       1\n 5     5 <dbl [10]> <lgl [10]>       4\n 6     6 <dbl [10]> <lgl [10]>       2\n 7     7 <dbl [10]> <lgl [10]>       1\n 8     8 <dbl [10]> <lgl [10]>       1\n 9     9 <dbl [10]> <lgl [10]>       2\n10    10 <dbl [10]> <lgl [10]>       5\n# i 990 more rows\n```\n\n\n:::\n:::\n\nThis is a bit of a programmer's trick. In R, `less` contains a vector of 10 TRUE or FALSE values, according to whether the corresponding value in `sample` is less than 40 or not. In R (and many other programming languages), the numeric value of TRUE is 1 and of FALSE is 0, so you count how many TRUE values there are by adding them up. To verify that this worked, we should `unnest` `sample` and `less`:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(457299)\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(10, 50, 15))) %>% \n  mutate(less = list(sample<40)) %>% \n  mutate(counted = sum(less)) %>% \n  unnest(c(sample, less))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10,000 x 4\n     sim sample less  counted\n   <int>  <dbl> <lgl>   <int>\n 1     1   74.3 FALSE       3\n 2     1   38.8 TRUE        3\n 3     1   46.0 FALSE       3\n 4     1   39.5 TRUE        3\n 5     1   53.2 FALSE       3\n 6     1   60.6 FALSE       3\n 7     1   33.8 TRUE        3\n 8     1   61.9 FALSE       3\n 9     1   50.1 FALSE       3\n10     1   66.4 FALSE       3\n# i 9,990 more rows\n```\n\n\n:::\n:::\n\nIn the first sample, 38.8, 39.5, and 33.8 are less than 40, correctly identified so in `less`, and the `counted` column shows that the first sample did indeed have 3 values less than 40. You can check a few of the others as well, enough to convince yourself that this is working.\n\nNext, the sign test will reject if there are 0, 1, 9 or 10 values less than 40 (you might be guessing that the last two will be pretty unlikely), so make a column called `reject` that encapsulates that, and then count how many times you rejected in your simulations. I don't need my `unnest` any more; that was just to check that everything was working so far:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(457299)\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(10, 50, 15))) %>% \n  mutate(less = list(sample<40)) %>% \n  mutate(counted = sum(less)) %>% \n  mutate(reject = (counted<=1 | counted >= 9)) %>% \n  count(reject)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Rowwise: \n  reject     n\n  <lgl>  <int>\n1 FALSE    757\n2 TRUE     243\n```\n\n\n:::\n:::\n\nMy simulated power is 0.243\n\nThis is all liable to go wrong the first few times, so make sure that\neach line works before you go on to the next, as I did.\nWhile you're debugging, try it with a\nsmall number of random samples like 5. (It is smart to have a variable called `nsim` which you set to a small number like 5 when you are testing, and than to 1000 when you run the real thing, so that the first line of the pipeline is then `tibble(sim = 1:nsim)`.)\n\nIf you were handing something like this in, I would only want to see your code for the final pipeline that does everything, though you could and should have some words that describe what you did.\n\n\n\nI'm now thinking a better way to do this is to write a function that\ntakes a sample (in a vector) and returns a TRUE or FALSE according to\nwhether or not a median of 40 would be rejected for that sample:\n\n::: {.cell}\n\n```{.r .cell-code}\nis_reject=function(x) {\n  tibble(x=x) %>%\n    mutate(counted = (x < 40)) %>% \n    summarize(below = sum(counted)) %>% \n    summarize(is_rejected = (below<=1 | below>=9)) %>% \n    pull(is_rejected)\n}\nis_reject(c(35, 45, 55))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nis_reject(c(35, 38, 45, 55))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\nNow, we have to use that:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(457299)\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(sample = list(rnorm(10, 50, 15))) %>% \n  mutate(reject = is_reject(sample)) %>% \n  count(reject)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Rowwise: \n  reject     n\n  <lgl>  <int>\n1 FALSE    757\n2 TRUE     243\n```\n\n\n:::\n:::\n\nThis is a bit cleaner because the process of deciding whether each sample leads to a rejection of the median being 40 has been \"outsourced\" to the function, and the pipeline with the `rowwise` is a lot cleaner: take a sample, decide whether that sample leads to rejection, and count up the rejections.\n\n\n$\\blacksquare$\n\n(f) Which is more powerful in this case, the sign test or the\n$t$-test? How do you know?\n\n\n\nSolution\n\n\nThe power of the sign test is estimated as 0.243, which is quite a bit less\nthan the power of the $t$-test, which we found back in (a) to be\n0.469. So the $t$-test, in this situation where it is valid, is\nthe right test to use: it is (i) valid and (ii) more powerful.\nSo the $t$-test is more powerful. One way to think about how\n*much* more powerful is to ask \"how much smaller of a sample    size would be needed for the $t$-test to have the same power as    this sign test?\" \nThe power of my sign test was 0.243, so in\n`power.t.test` we set\n`power` equal to that and\nomit the sample size `n`:\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(delta=50-40,power=0.243,sd=15,type=\"one.sample\",alternative=\"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 5.599293\n          delta = 10\n             sd = 15\n      sig.level = 0.05\n          power = 0.243\n    alternative = two.sided\n```\n\n\n:::\n:::\n\nA sample of size 6 gives the same power for the $t$-test that a\nsample of size 10 does for the sign test. The ratio of these two\nsample sizes is called the *relative efficiency* of the two\ntests: in this case, the $t$-test is $10/6=1.67$ times more\nefficient. The data that you have are being used \"more    efficiently\" \nby the $t$-test.\nIt is possible to derive^[Meaning, I forget how to do it.      But it has something to do with looking at alternatives that are      very close to the null.]  \nthe limiting relative efficiency of\nthe $t$ test relative to the sign test when the data are actually\nnormal, as the sample size gets larger. This turns out not to\ndepend on how far wrong the null is (as long as it is the same for\nboth the $t$-test and the sign test). This \"asymptotic relative    efficiency\" is $\\pi/2=1.57$. \nOur relative efficiency for power\n0.243, namely 1.67, was pretty close to this, even though our\nsample sizes 10 and 6 are not especially close to infinity.\nThis says that, if your data are actually from a normal\ndistribution, you do a lot better to use the $t$-test than the\nsign test, because the sign test is wasteful of data (it only uses\nabove/below rather than the actual values). \n\nExtra: if your data are *not* from a normal distribution, then the\nstory can be very different. \nOf course you knew I would investigate this. There is a\ndistribution called the \"Laplace\" or \"double exponential\"\ndistribution, that has very long tails.^[If you've ever run    into the exponential distribution, you'll recall that this is    right skewed with a very long tail. The Laplace distribution looks    like two of these glued back to back.] \nThe distribution is not in\nbase R, but there is a package called `smoothmest` that\ncontains a function `rdoublex` to generate random values from\nthis distribution. So we're going to do a simulation investigation\nof the power of the sign test for Laplace data, by the same\nsimulation technique that we did above. Like the normal, the Laplace\ndistribution is symmetric, so its mean and median are the same\n(which makes our life easier).^[This is about the *only*  way in which the normal and Laplace distributions are alike.]\n\nLet's test the hypothesis that the median is zero. We'll suppose that\nthe true median is 0.5 (this is called `mu` in\n`rdoublex`). The first problem we run into is that we can't use\n`power.t.test` because they assume normal data, which we are\nfar from having. So we have to do two simulations: one to simulate the\npower of the $t$ test, and one to simulate the power of the sign test.\n\nTo simulate the $t$ test, we first have to generate some Laplace data\nwith the true mean of 0.5. We'll use a sample size of 50 throughout\nthese simulations.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smoothmest)\nrl <- rdoublex(50,mu=0.5)\nrl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] -0.33323285  0.70569291 -1.22513053  0.68517708  0.87221482  0.49250051\n [7]  0.26700527  1.90236874  0.53288312  1.37374732  0.72743434  0.46634071\n[13]  0.43581431 -0.01545866  0.18594908 -0.40403202 -0.13540289  0.83862694\n[19] -0.23360644 -0.74050354  2.92089551 -2.72173880  0.51571185  1.23636045\n[25]  0.82921382  1.72456334  0.07903058  0.74789589  0.90487190  2.52310082\n[31]  3.13629814  0.81851434  0.74615575 -0.26068744  2.70683355  1.46981530\n[37]  1.45646489  1.20232517  6.65249860 -0.51575026 -0.07606399  2.11338640\n[43] -1.20427995  1.70986104 -1.66466321  0.55346854  0.33908531  0.72100677\n[49]  0.92025176  0.98922656\n```\n\n\n:::\n:::\n\nThis seems to have some unusual values, far away from zero:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(rl) %>%\nggplot(aes(sample=rl))+\nstat_qq()+stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/wanners-1.pdf){fig-pos='H'}\n:::\n:::\n\nYou see the long tails compared to the normal.\n\nNow, we feed these values into `t.test` and see whether we\nreject a null median of zero (at $\\alpha=0.05$):\n\n::: {.cell}\n\n```{.r .cell-code}\ntt <- t.test(rl)  \ntt\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  rl\nt = 3.72, df = 49, p-value = 0.0005131\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.3399906 1.1388911\nsample estimates:\nmean of x \n0.7394408 \n```\n\n\n:::\n:::\n\nOr we can just pull out the P-value and even compare it to 0.05:\n\n::: {.cell}\n\n```{.r .cell-code}\npval <- tt$p.value  \npval\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0005130841\n```\n\n\n:::\n\n```{.r .cell-code}\nis.reject <- (pval<=0.05)\nis.reject\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nThis one has a small P-value and so the null median of 0 should be\n(correctly) rejected.\n\nWe'll use these ideas to simulate the power of the $t$-test for these\ndata, testing a mean of 0. This uses the same ideas as for any power\nsimulation; the difference here is the true distribution:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(sample = list(rdoublex(50, mu = 0.5))) %>% \n  mutate(t_test = list(t.test(sample, mu = 0))) %>% \n  mutate(t_pval = t_test$p.value) %>% \n  count(t_pval <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `t_pval <= 0.05`     n\n  <lgl>            <int>\n1 FALSE              298\n2 TRUE               702\n```\n\n\n:::\n:::\n\nAnd now we simulate the sign test. Since what we want is a P-value\nfrom a vector, the easiest way to do this is to use\n`pval_sign0` from `smmr`, which returns exactly the\ntwo-sided P-value that we want, so that the procedure is a step simpler:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(sample = list(rdoublex(50, mu = 0.5))) %>% \n  mutate(sign_pval = pval_sign0(0, sample)) %>% \n  count(sign_pval <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `sign_pval <= 0.05`     n\n  <lgl>               <int>\n1 FALSE                 228\n2 TRUE                  772\n```\n\n\n:::\n:::\n\n\nFor data from this Laplace\ndistribution, the power of this $t$-test is 0.696, but the power of\nthe sign test on the same data is 0.761, *bigger*.  For\nLaplace-distributed data, the sign test is *more* powerful than\nthe $t$-test.\n\nThis is not to say that you will ever run into data that comes from\nthe Laplace distribution. But the moral of the story is that the sign\ntest *can* be more powerful than the $t$-test, under the right\ncircumstances (and the above simulation is the \"proof\" of that\nstatement). So a blanket statement like \"the sign test is not very powerful\" \nneeds to be qualified a bit: when your data come from a\nsufficiently long-tailed distribution, the sign test can be more\npowerful relative to the $t$-test than you would think.\n\n$\\blacksquare$\n\n\n\n\n\n## Ben Roethlisberger\n\n Ben Roethlisberger plays (American) football for the Pittsburgh  Steelers. He plays as a quarterback, which means that his job is to throw (pass) the ball so that one of his teammates can catch it. Each time he makes a pass that is caught, this is called a \"completion\", and the team coaches are interested in his average number of completions per game (this average could be the mean or the median).\n\nIn 2010, Roethlisberger was suspended for the first four games of the season, and there was concern that this might affect his performance (in terms of the number of passes completed in the games after he returned). The Pittsburgh Steelers did not play in week 5 of the 2010 season; the season is 17 weeks long (one game per week) and each team has one week in which they do not play.\n\nThe data are [here](http://ritsokiguess.site/datafiles/roethlisberger.csv). There are four columns: the year (always 2010), the week number of the season that the game was played in, the name of the opposing team, and the number of completed passes by Roethlisberger in the game.\n\n\n\n(a) Read in and display (some of) the data. Do you have what you were expecting?\n\nSolution\n\n\nReading in is the usual, noting that this is a `.csv`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/roethlisberger.csv\"\nben <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 12 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): opponent\ndbl (3): season, week, completed\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nben\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 x 4\n   season  week opponent      completed\n    <dbl> <dbl> <chr>             <dbl>\n 1   2010     6 Cleveland            16\n 2   2010     7 Miami                19\n 3   2010     8 New Orleans          17\n 4   2010     9 Cincinnati           17\n 5   2010    10 New England          30\n 6   2010    11 Oakland              18\n 7   2010    12 Buffalo              20\n 8   2010    13 Baltimore            22\n 9   2010    14 Cincinnati           21\n10   2010    15 New York Jets        23\n11   2010    16 Carolina             22\n12   2010    17 Cleveland            15\n```\n\n\n:::\n:::\n\nSince \"Roethlisberger\" is a lot to type every time, I called the dataframe by his first name.\n\nI am showing all 12 rows here; you are probably seeing only 10, and will have to scroll down to see the last two.\n\nI have the four variables promised, and I also have a sensible number of rows. In particular, there is no data for weeks 1--4 (the suspension) and for week 5 (in which the team did not play), but there is a number of passes completed for all the other weeks of the season up to week 17. (If Roethlisberger had not played in any other games, you can expect that I would have told you about it.)\n\nExtra: I did some processing to get the data to this point. I wanted to ask you about the 2010 season, and that meant having the 2009 data to compare it with. So I went [here](https://www.pro-football-reference.com/teams/pit/2009.htm), scrolled down to Schedule and Game Results, and clicked on each of the Boxscores to get the player stats by game. Then I made a note of the opponent and the number of passes completed, and did the same for 2010. I put them in a file I called `r1.txt`, in aligned columns, and read that in. (An alternative would have been to make a spreadsheet and save that as a `.csv`, but I already had R Studio open.) Thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nr0 <- read_table(\"r1.txt\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n-- Column specification --------------------------------------------------------\ncols(\n  season = col_double(),\n  week = col_double(),\n  opponent = col_character(),\n  completed = col_double()\n)\n```\n\n\n:::\n\n```{.r .cell-code}\nr0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 27 x 4\n   season  week opponent    completed\n    <dbl> <dbl> <chr>           <dbl>\n 1   2009     1 Tennessee          33\n 2   2009     2 Chicago            23\n 3   2009     3 Cincinnati         22\n 4   2009     4 San_Diego          26\n 5   2009     5 Detroit            23\n 6   2009     6 Cleveland          23\n 7   2009     7 Minnesota          14\n 8   2009     9 Denver             21\n 9   2009    10 Cincinnati         20\n10   2009    11 Kansas_City        32\n# i 17 more rows\n```\n\n\n:::\n:::\n\nI was curious about the season medians (for reasons you see later), thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nr0 %>% group_by(season) %>% summarise(med = median(completed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  season   med\n   <dbl> <dbl>\n1   2009  22  \n2   2010  19.5\n```\n\n\n:::\n:::\n\nYou will realize that my asserted average for \"previous seasons\" is close to the median for 2009. Here is where I have to admit that I cheated. It actually *is* the median for 2009, except that there are some games in 2010 where Roethlisberger had 22 completed passes and I didn't want to mess the sign test up (I talk more about this later). So I made it 22.5, which is a possible value for the median of an even number of whole-number values.\n\nAnyway, the last thing to do is to grab only the rows for 2010 and save them for you. This uses `filter` to select only the rows for which something is true:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\nr0 %>% filter(season==2010) -> r1\nwrite_csv(r1, \"roethlisberger.csv\")\n```\n:::\n\n\n$\\blacksquare$\n\n\n(b) Make a suitable graph of the number of completed passes, and explain briefly why you would have some doubts about using $t$-procedures in this situation.\n\nSolution\n\n\nDon't be tempted to think *too* hard about the choice of graph (though I talk more about this below). One quantitative variable, so a histogram again. There are only 12 observations, so 5 bins is about as high as you should go:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ben, aes(x=completed)) + geom_histogram(bins=5)\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/roethlisberger-5-1.pdf){fig-pos='H'}\n:::\n:::\n\nThis one shows an outlier: there is one number of completed passes that is noticeably higher than the rest. A normal distribution doesn't have outliers, and so this, coupled with a small sample in which normality is important, means that we should not be using a $t$-test or confidence interval.\n\nIf you chose a different number of bins, you might get a different look. Here's 4 bins:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ben, aes(x=completed)) + geom_histogram(bins=4)\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/roethlisberger-6-1.pdf){fig-pos='H'}\n:::\n:::\n\nThat looks more like right-skewness, but the conclusion is the same.\n\nExtra: if you have read, for example, [Problem 6.1 in PASIAS](http://ritsokiguess.site/datafiles/pasias/one-sample-inference.html#hunter-gatherers-in-australia), you'll have seen that another possibility is a one-group boxplot. This might have been the context in which you first saw the boxplot, maybe at about the time you first saw the five-number summary, but I don't talk about that so much in this course because `ggplot` boxplots have both an `x` and a `y`, and it makes more sense to think about using boxplots to *compare* groups. But, you can certainly get R to make you a one-sample boxplot. What you do is to set the grouping variable to a \"dummy\" thing like the number 1:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ben, aes(x=1, y=completed)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/roethlisberger-7-1.pdf){fig-pos='H'}\n:::\n:::\nand then you ignore the $x$-axis.\n\nThis really shows off the outlier; it is actually *much* bigger than the other observations. It didn't show up so much on the histograms because of where the bin boundaries happened to come. On the four-bin histogram, the highest value 30 was in the 27.5--32.5 bin, and the second-highest value 23 was at the bottom of the 22.5--27.5 bin. So the highest and second-highest values looked closer together than they actually were.\n\nIf you have been reading ahead, you might also be thinking about a normal quantile plot. That is for specifically assessing normality, and here this is something that interests us, because a $t$-test will be doubtful if the normality fails:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ben, aes(sample=completed)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/roethlisberger-8-1.pdf){fig-pos='H'}\n:::\n:::\nThis again shows off the outlier at the high end. It is a reasonable choice of plot *here* because normality is of specific interest to us.\n\nA note: you are absolutely not required to read ahead to future lectures. Each assignment can be done using the material in the indicated lectures only. If you want to use something from future lectures, go ahead, but make sure you are using it appropriately.\n\nDon't be tempted to plot the number of completed passes against something like week number:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ben, aes(x=week, y=completed)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/roethlisberger-9-1.pdf){fig-pos='H'}\n:::\n:::\nThat is quite interesting (a mostly increasing trend over weeks, with the outlier performance in week 10), but it doesn't tell us what we want to know *here*: namely, is a $t$-test any good?\n\n\n$\\blacksquare$\n\n\n(c) Run a sign test to compare Roethlisberger's performance in 2010 with his previous average of 22.5 completions per game. What do you conclude?\n\nSolution\n\n\nUse `smmr`, dataframe, column, null median:\n\n::: {.cell}\n\n```{.r .cell-code}\nsign_test(ben, completed, 22.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$above_below\nbelow above \n   10     2 \n\n$p_values\n  alternative    p_value\n1       lower 0.01928711\n2       upper 0.99682617\n3   two-sided 0.03857422\n```\n\n\n:::\n:::\n\nI am looking for *any* change, so for me, a two-sided test is appropriate. If you think this is one-sided, make a case for your side, and then go ahead.\n\nMy P-value is 0.039, so I can reject the null hypothesis (that the median number of passes completed is 22.5) and conclude that it has changed in 2010. \n\n(You might hypothesize that this is the result of a decrease in confidence, that he is either throwing fewer passes, or the ones that he is throwing are harder to catch. If you know about football, you might suspect that Roethlisberger was actually passing *too much*, including in situations where he should have handing off to the running back, instead of reading the game appropriately.)\n\nExtra: I said above that I cheated and made the null median 22.5 instead of 22. What happens if we make the null median 22?\n\n::: {.cell}\n\n```{.r .cell-code}\nsign_test(ben, completed, 22)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$above_below\nbelow above \n    8     2 \n\n$p_values\n  alternative   p_value\n1       lower 0.0546875\n2       upper 0.9892578\n3   two-sided 0.1093750\n```\n\n\n:::\n:::\n\nFor one thing, the result is no longer significant. But looking at the table of values above and below reveals something odd: there are only ten values. What happened to the other two? What happened is that two of the data values were exactly equal to 22, so they are neither above nor below. In the sign test, they are thrown away, so that we are left with 8 values below 22 and 2 above. \n\nI didn't want to make you wonder what happened, so I made the null median 22.5.\n\n\n$\\blacksquare$\n\n\n(d) Why might you have expected your sign test to come out significant, even without looking at the P-value? Explain briefly. \n\nSolution\n\n\nThe other ingredient to the sign test is how many data values are above and below the null median. You can look at the output from `sign_test` (the first part), or count them yourself:\n\n::: {.cell}\n\n```{.r .cell-code}\nben %>% count(completed<22.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  `completed < 22.5`     n\n  <lgl>              <int>\n1 FALSE                  2\n2 TRUE                  10\n```\n\n\n:::\n:::\n\nYou can put a logical condition (something that can be true or false) into `count`, or you can create a new column using `ifelse` (which I think I showed you somewhere):\n\n::: {.cell}\n\n```{.r .cell-code}\nben %>% mutate(side = ifelse(completed<22.5, \"below\", \"above\")) %>% \ncount(side)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  side      n\n  <chr> <int>\n1 above     2\n2 below    10\n```\n\n\n:::\n:::\n\nWhichever way you do it, there seem to be a lot more values below than above, very different from a 50--50 split. Even with only 12 observations, this turns out to be enough to be significant. (If you tossed a fair coin 12 times, would you be surprised to get only 2 heads or 2 tails?)\n\n\n$\\blacksquare$\n\n\n(e) Obtain a 90% confidence interval for the median number of completed passes (over \"all possible games played by 2010 Ben Roethlisberger\"). \n\nSolution\n\n\nThis is `ci_median`, but with `conf.level` since you are not using the default level of 95%:\n\n::: {.cell}\n\n```{.r .cell-code}\nci_median(ben, completed, conf.level = 0.90)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 17.00244 21.99878\n```\n\n\n:::\n:::\n\n17 to 22 completed passes.\n\nExtra: the P-value of the sign test only changes (as the null median changes) when you get to a data point; otherwise, the number of values above and below will stay the same, and the P-value will stay the same. The data values here were all whole numbers, so the limits of the confidence interval are also whole numbers (to the accuracy of the bisection), so the interval really should be rounded off.\n\n\n\n$\\blacksquare$\n\n\n(f) Find a 90% confidence interval for the *mean* number of passes completed, and explain briefly why it differs from the one for the median in the way that it does.\n\nSolution\n\n\nAll right, get the interval for the mean first:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(ben, t.test(completed, conf.level = 0.90))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  completed\nt = 17.033, df = 11, p-value = 2.971e-09\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 17.89124 22.10876\nsample estimates:\nmean of x \n       20 \n```\n\n\n:::\n:::\n\nThe 95% confidence interval for the mean goes from 17.9 to 22.1 (completions per game). \n\nThis is higher at both ends than the interval for the median, though possibly not as much as I expected. This is because the mean is made higher by the outlier (compared to the median), and so the CI procedure comes to the conclusion that the mean is higher.\n\nExtra: this is one of those cases where the bootstrap might shed some light on the sampling distribution of the sample mean:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(ben$completed, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) %>% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 10)\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/roethlisberger-16-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThis is noticeably skewed to the right (it goes further up from the peak than down), which is why the CI for the mean went up a bit higher than the one for the median.\n\nFinally, bootstrapping the median is not something you'd want to do, since the sign test doesn't depend on anything being normally-distributed. This is a good thing, since bootstrapping the sample median is weird:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(ben$completed, replace = TRUE))) %>% \n  mutate(my_median = median(my_sample)) %>% \n  ggplot(aes(x = my_median)) + geom_histogram(bins = 30)\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/roethlisberger-17-1.pdf){fig-pos='H'}\n:::\n:::\n\nThe \"holes\" in the distribution comes about because there are not all that many different possible sample medians when you sample with replacement. For one thing, the values are all whole numbers, so the median can only be something or something and a half. Even then, the bar heights look kind of irregular.\n\nI used a large number of bins to emphasize this, but even a more reasonable number looks strange:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(ben$completed, replace = TRUE))) %>% \n  mutate(my_median = median(my_sample)) %>% \n  ggplot(aes(x = my_median)) + geom_histogram(bins = 10)\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/roethlisberger-18-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nA sample median of 19 or 20 is more likely than one of 19.5.\n\n\n$\\blacksquare$\n\n\n\n\n\n\n## Six ounces of protein\n\n A company produces prepackaged  diet meals. These meals are advertised as containing \"6 ounces of protein per package\". A consumer organization is concerned that this is not accurate. The organization takes a random sample of 20 of these meals, and measures the protein content of each one. The data are in [http://ritsokiguess.site/datafiles/protein.txt](http://ritsokiguess.site/datafiles/protein.txt)\nas one column. \n\n\n\n(a) Read in and display (some of) the data.\n\nSolution\n\n\nThe usual. This is one column only, so you can pretend the columns are separated by anything at all and it will still work:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/protein.txt\"\nmeals <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n-- Column specification --------------------------------------------------------\ncols(\n  protein = col_double()\n)\n```\n\n\n:::\n\n```{.r .cell-code}\nmeals %>% arrange(protein)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 x 1\n   protein\n     <dbl>\n 1     3.6\n 2     4.2\n 3     4.3\n 4     4.4\n 5     4.7\n 6     4.9\n 7     5.1\n 8     5.1\n 9     5.2\n10     5.5\n11     5.5\n12     5.6\n13     5.7\n14     5.8\n15     5.8\n16     6.1\n17     6.1\n18     6.1\n19     6.1\n20     6.1\n```\n\n\n:::\n:::\nGet it to work via one of the methods you've seen in this class (ie., *not* `read.table`); I don't mind how you manage it.\n\n\n$\\blacksquare$\n\n\n(b) Make a suitable graph of your data.\n\nSolution\n\n\nOne quantitative variable, so a histogram with a sufficiently small number of bins:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(meals, aes(x=protein)) + geom_histogram(bins = 5)\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/protein-2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(c) Why might a sign test be better than a $t$-test for assessing the average amount of protein per package? Explain briefly. (\"Average\" here means any measure of centre.)\n\nSolution\n\n\nThe shape of the above distribution is skewed to the left, and not symmetric like a normal distribution. (If you say \"the histogram is not normal\", make sure you also say how you know.) This means that the median would be a better measure of \"average\" (that is, centre) than the mean is, because the mean would be pulled downwards by the long tail, and the median would not. To complete the story, the sign test is a test of the median, so the sign test would be better than the $t$-test, which is a test of the mean.\n\nThe other thing you might consider is the sample size, 20, which *might* be large enough to overcome this amount of skewness, but then again it might not be. So you could say that we should be cautious and run the sign test here instead.\n\n\n$\\blacksquare$\n\n\n(d) Run a suitable sign test for these data. What do you conclude?\n\nSolution\n\n\nFirst, if you have not already done so, install `smmr` *following the instructions in the lecture notes*. (This one is not just `install.packages`.) Then, make sure you have a `library(smmr)` somewhere above where you are going to use something from it. Once that is in place, remember what we were interested in: was the median protein content 6 ounces, or is there evidence that it is something different? (The \"not accurate\" in the question says that the median could be higher or lower, either of which would be a problem, and so we need a two-sided alternative.) Thus, the null median is 6 and we need a two-sided test, which goes this way:\n\n::: {.cell}\n\n```{.r .cell-code}\nsign_test(meals, protein, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$above_below\nbelow above \n   15     5 \n\n$p_values\n  alternative    p_value\n1       lower 0.02069473\n2       upper 0.99409103\n3   two-sided 0.04138947\n```\n\n\n:::\n:::\n\nThe P-value, 0.0414, is less than 0.05, so we reject the null hypothesis and conclude that the median is different from 6 ounces. The advertisement by the company is not accurate. \n\nMake sure you give the actual P-value you are comparing with 0.05, since otherwise your answer is incomplete. That is, you need to say more than just \"the P-value is less than 0.05\"; there are three P-values here, and only one of them is the right one.\n\nExtra: we already decided that a $t$-test is not the best here, but I am curious as to how different its P-value is:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(meals, t.test(protein, mu=6))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  protein\nt = -4.2312, df = 19, p-value = 0.000452\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 4.946263 5.643737\nsample estimates:\nmean of x \n    5.295 \n```\n\n\n:::\n:::\n\nThe conclusion is the same, but the P-value is a lot smaller. I don't think it should really be this small; this is probably because the mean is pulled down by the left skew and so really ought not to look so far below 6. I am inclined to think that if the $t$-test were correct, its P-value ought to be between this and the one from the sign test, because the $t$-test uses the actual data values, and the sign test uses the data less efficiently (only considering whether each one is above or below the null median).\n\n$\\blacksquare$\n\n\n(e) In your sign test, how could you have deduced that the P-value was going to be small even without looking at any of the P-values themselves? Explain briefly.\n\nSolution\n\n\nLook at the other part of the output, the count of values above and below the null median. (You might have to click on \"R Console\" to see it.) If the null hypothesis was correct and the median was really 6, you'd expect to see about half the data values above 6 and about half below. But that is not what happened: there were 15 values below and only 5 above. Such an uneven split is rather unlikely if the null hypothesis was correct. So we would guess that our P-value would be small, as indeed it is.\n\n\n$\\blacksquare$\n\n\n(f) Obtain a 90% confidence interval for the population median protein content. What does this tell you about the reason for the rejection or non-rejection of the null hypothesis above?\n\nSolution\n\n\nThis is `ci_median`, but you need to be paying attention: it's not the default 95% confidence level, so you have to specify that as well:\n\n::: {.cell}\n\n```{.r .cell-code}\nci_median(meals, protein, conf.level = 0.90)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.905273 5.793750\n```\n\n\n:::\n:::\nThe interval goes from 4.91 to 5.79. (The data values have one decimal place, so you could justify two decimals in the CI for the median, but anything beyond that is noise and you shouldn't give it in your answer.^[There is actually slightly more to it here: the ends of this confidence interval for the median are always data values, because of the way it is constructed, so the actual end points really ought to be given to the *same* number of decimals as the data, here 4.9 to 5.8. The output given is not exactly 4.9 and 5.8 because of inaccuracy in the bisection.])\n\nThis interval is entirely below 6 (the null median), so evidently the reason that we rejected 6 as the population median is that the actual population median is *less than* 6. \n\nExtra: the CI for the median is not that different from the one for the mean, which suggests that maybe the $t$-test was not so bad after all. If you want to investigate further, you can try finding a bootstrapped sampling distribution of the sample mean, and see how non-normal it looks:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(meals$protein, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) -> d\nggplot(d, aes(x = my_mean)) + geom_histogram(bins = 10)\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/protein-6-1.pdf){fig-pos='H'}\n:::\n:::\n\nThat is pretty close to normal. So the $t$-test would in actual fact have been fine. To confirm, a normal quantile plot of the bootstrapped sampling distribution:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](sign_files/figure-pdf/protein-7-1.pdf){fig-pos='H'}\n:::\n:::\nA *tiny bit* skewed to the left.\n\nBut I didn't ask you to do this, because I wanted to give you a chance to do a sign test for what seemed like a good reason. \n\nExtra 2: I mentioned in an note that the endpoints of the CI for the median are actually data points, only we didn't see it because of the accuracy to which `ci_median` was working. You can control this accuracy by an extra input `tol`. Let's do something silly here:\n\n::: {.cell}\n\n```{.r .cell-code}\nci_median(meals, protein, conf.level = 0.90, tol = 0.00000001)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.900001 5.799999\n```\n\n\n:::\n:::\n\nThis takes a bit longer to run, since it has to get the answer more accurately, but now you can see how the interval goes from \"just over 4.9\" to \"just under 5.8\", and it actually makes the most sense to give the interval as \"4.9 to 5.8\" without giving any more decimals.\n\nExtra 3: the reason for the confidence interval endpoints to be data values is that the interval comes from inverting the test: that is, finding the values of the population median that would not be rejected by a sign test run on our data. Recall how the sign test works: it is based on a count of how many data values are above and below the hypothesized population median. These counts are only going to change as the hypothesized median changes if you hit a data point, since that's the only way you can change how many values are above and below.^[There is a technicality about what happens when the null median is exactly equal to a data value; see PASIAS for more discussion on this.] Thus, the only places where changing the null median changes whether or not a value for it is inside or outside the confidence interval are at data values, and thus the ends of the interval must be at (or, perhaps more strictly, just above or below) data values.\n\nThis is a peculiarity of using the sign test to get a CI for the median. If, say, you were to invert the $t$-test to get a confidence interval for the mean, you wouldn't see that. (This is in fact exactly what you do to get a confidence interval for the mean, but this is not the way it is usually introduced.) The reason that the CI for the mean (based on the $t$-test) is different from the one for the median (based on the sign test) is that if you change the null hypothesis in the $t$-test, however slightly, you change the P-value (maybe only slightly, but you change it). So the CI for the mean, based on the $t$-test, is not required to have data points at its ends, and indeed usually does not.  The difference is in the kind of distribution the test statistic has; the $t$-distribution is continuous, while the sign test statistic (a count of the number of values above or below something) is discrete. It's the discreteness that causes the problems.\n\n\n$\\blacksquare$\n\n\n\n",
    "supporting": [
      "sign_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}