{
  "hash": "31ee0ebf0078db3ce071b9752ed528a7",
  "result": {
    "markdown": "# Tidying data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Baseball and softball spaghetti\n\n\n On a previous assignment, we found that students could throw\na baseball further than they could throw a softball. In this question,\nwe will make a graph called a \"spaghetti plot\" to illustrate this\ngraphically. (The issue previously was that the data were matched\npairs: the same students threw both balls.)\n\nThis seems to work most naturally by building a pipe, a line or two at\na time. See if you can do it that way. (If you can't make it work, use lots of\ntemporary data frames, one to hold the result of each part.)\n\n\n\n(a) Read in the data again from\n[link](http://ritsokiguess.site/datafiles/throw.txt). The\nvariables had no names, so supply some, as you did before.\n\n\n\n\n(b) Create a new column that is the students turned into a `factor`,\nadding it to your data frame. The reason for this will become clear\nlater.\n\n\n\n(c) Collect together all the throwing distances into one column,\nmaking a second column that says which ball was thrown.\n\n\n\n(d) Using your new data frame, make a \"scatterplot\" of throwing\ndistance against type of ball.\n\n\n\n(e) Add two things to your plot: something that will distinguish\nthe students by colour (this works best if the thing distinguished\nby colour is a factor),^[You can try it without. See below.]\nand something that will join the two points for the same student by\na line.\n\n\n\n(f) The legend is not very informative. Remove it from the plot,\nusing `guides`.\n\n\n\n(g) What do you see on the final spaghetti plot? What does that tell you\nabout the relative distances a student can throw a baseball vs. a\nsoftball? Explain briefly, blah blah blah.\n\n\n\n\n\n\n\n##  Ethanol and sleep time in rats\n\n\n A biologist wished to study the effects of ethanol on sleep\ntime in rats. A sample of 20 rats (all the same age) was selected, and\neach rat was given an injection having a particular concentration (0,\n1, 2 or 4 grams per kilogram of body weight) of ethanol. These are\nlabelled `e0, e1, e2, e4`. The \"0\"\ntreatment was a control group. The rapid eye movement (REM) sleep time\nwas then recorded for each rat. The data are in\n[link](http://ritsokiguess.site/datafiles/ratsleep.txt). \n\n\n\n(a) Read the data in from the file. Check that you have four rows\nof observations  and five columns of sleep times.\n\n\n\n(b) Unfortunately, the data are in the wrong format. All the sleep\ntimes for each treatment group are on one row, and we should have\n*one* column containing *all* the sleep times, and the\ncorresponding row should show which treatment group that sleep time\ncame from. Transform this data frame into one that you could use for modelling or making graphs.\n\n\n\n\n\n\n(c) Using your new data frame, make side-by-side boxplots of sleep\ntime by treatment group. \n\n\n\n\n\n(d) In your boxplots, how does the median sleep time appear to\ndepend on treatment group?\n\n\n\n\n(e) There is an assumption about spread that the analysis of\nvariance needs in order to be reliable. Do your boxplots indicate that\nthis assumption is satisfied for these data, bearing in mind that you\nhave only five observations per group?\n\n\n\n\n(f) Run an analysis of variance to see whether sleep time differs\nsignificantly among treatment groups. What do you conclude?\n\n\n\n\n(g) Would it be a good idea to run Tukey's method here? Explain\nbriefly why or why not, and if you think it would be a good idea, run\nit.\n\n\n\n\n(h) What do you conclude from Tukey's method? (This is liable to be\na bit complicated.) Is there a treatment that is clearly best, in\nterms of the sleep time being largest?\n\n\n\n\n\n\n\n##  Growth of tomatoes\n\n\n A biology graduate student exposed each of 32\ntomato plants to one of four different colours of light (8 plants to\neach colour). The growth rate of each plant, in millimetres per week,\nwas recorded. The data are in\n[link](http://ritsokiguess.site/datafiles/tomatoes.txt). \n\n\n\n(a) Read the data into R and confirm that you have 8 rows and 5\ncolumns of data.\n\n\n\n(b) Re-arrange the data so that you have *one* column\ncontaining all the growth rates, and another column saying which\ncolour light each plant was exposed to. (The aim here is to produce\nsomething suitable for feeding into `aov` later.) \n\n\n\n(c) Save the data in the new format to a text file. This is\nmost easily done using `write_csv`, which is the opposite\nof `read_csv`. It requires two things: a data frame, and\nthe name of a file to save in, which should have a `.csv`\nextension.  \n\n\n\n(d) Make a suitable boxplot, and use it to assess the assumptions\nfor ANOVA. What do you conclude? Explain briefly. \n\n\n\n(e) Run (regular) ANOVA on these data. What do you conclude?\n(Optional extra: if you think that some other variant of ANOVA would\nbe better, run that as well and compare the results.)\n\n\n\n(f) If warranted, run a suitable follow-up. (If not warranted, explain briefly\nwhy not.)\n\n\n\n\n\n\n\n##  Pain relief in migraine headaches (again)\n\n\n The data in\n[link](http://ritsokiguess.site/datafiles/migraine.txt) are from a\nstudy of pain relief in migraine headaches. Specifically, 27 subjects\nwere randomly assigned to receive *one* of three pain relieving\ndrugs, labelled A, B and C. Each subject reported the number of hours\nof pain relief they obtained (that is, the number of hours between\ntaking the drug and the migraine symptoms returning). A higher value\nis therefore better. Can we make some recommendation about which drug\nis best for the population of migraine sufferers?\n\n\n\n(a) Read in and display the data. Take a look at the data\nfile first, and see if you can say why `read_table` will\nwork and `read_delim` will not.\n\n\n\n(b) What is it about the experimental design that makes a one-way\nanalysis of variance plausible for data like this?\n\n\n\n(c) What is wrong with the current format of the data as far as\ndoing a one-way ANOVA analysis is concerned? (This is related to the\nidea of whether or not the data are \"tidy\".)\n\n\n\n(d) \"Tidy\" the data to produce a data frame suitable for your\nanalysis. \n\n\n\n(e) Go ahead and run your one-way ANOVA (and Tukey if\nnecessary). Assume for this that the pain relief hours in each group\nare sufficiently close to normally distributed with sufficiently\nequal spreads.\n\n\n\n(f) What recommendation would you make about the best drug or\ndrugs? Explain briefly.\n\n\n\n\n\n\n\n##  Location, species and disease in plants\n\n\n The table below is a \"contingency table\", showing\nfrequencies of diseased and undiseased plants of two different species\nin two different locations:\n\n\n```\n\nSpecies     Disease present         Disease absent\nLocation X Location Y  Location X Location Y\nA            44         12          38        10\nB            28         22          20        18\n\n```\n\n\nThe data were saved as\n[link](http://ritsokiguess.site/datafiles/disease.txt). In that\nfile, the columns are coded by two letters: a `p` or an\n`a` to denote presence or absence of disease, and an `x`\nor a `y` to denote location X or Y. The data are separated by\nmultiple spaces and aligned with the variable names. \n\n\n\n(a) Read in and display the data.\n\n\n\n(b) Explain briefly how these data are not \"tidy\".\n\n\n\n(c) Use a suitable `tidyr` tool to get all the things\nthat are the same into a single column. (You'll need to make up a\ntemporary name for the other new column that you create.) Show your\nresult. \n\n\n\n(d) Explain briefly how the data frame you just created is\nstill not \"tidy\" yet.\n\n\n\n(e) Use one more `tidyr` tool to make these data tidy,\nand show your result.\n\n\n\n(f) Let's see if we can re-construct the original contingency\ntable (or something equivalent to it). Use the function\n`xtabs`. This requires first a model formula with the frequency\nvariable on the left of the squiggle, and the other variables\nseparated by plus signs on the right. Second it requires a data\nframe, with `data=`. Feed\nyour data frame from the previous part into `xtabs`. Save the\nresult in a variable and display the result.\n\n\n\n(g) Take the output from the last part and feed it into the\nfunction `ftable`. How has the output been changed? Which do\nyou like better? Explain briefly.\n\n\n\n\n\n\n\n\n##  Mating songs in crickets\n\n\n Male tree crickets produce \"mating songs\" by rubbing their\nwings together to produce a chirping sound. It is hypothesized that\nfemale tree crickets identify males of the correct species by how fast\n(in chirps per second) the male's mating song is. This is called the\n\"pulse rate\".  Some data for two species of crickets are in\n[link](http://ritsokiguess.site/datafiles/crickets.txt). The\ncolumns, which are unlabelled, are temperature and pulse rate\n(respectively) for *Oecanthus exclamationis* (first two\ncolumns) and *Oecanthus niveus* (third and fourth columns). The\ncolumns are separated by tabs. There are some missing values in the\nfirst two columns because fewer *exclamationis* crickets than\n*niveus* crickets were measured.\nThe research question is whether males\nof the different species have different average pulse rates. It is\nalso of interest to see whether temperature has an effect, and if\nso, what.\nBefore we get to that, however, we have some data organization to do.\n\n\n(a) Read in the data, allowing for the fact that you have no\ncolumn names. You'll see that the\ncolumns have names `X1` through `X4`. This is\nOK.\n\n\n\n(b) Tidy these untidy data, going as directly as you can to something tidy. (Some later parts show you how it used to be done.) Begin by: (i) adding a column of row numbers, (ii) `rename`-ing the columns to species name, an underscore, and the variable contents (keeping `pulserate` as one word), and then use `pivot_longer`. Note that the column names encode *two* things.\n\n\n\n\n(c) If you found (b) a bit much to take in, the rest of the way we take a rather more leisurely approach towards the tidying. \n\nThese data are rather far from being tidy. There need to be\nthree variables, temperature, pulse rate and species, and there\nare $14+17=31$ observations altogether. This one is tricky in that\nthere are temperature and pulse rate for each of two levels of a\nfactor, so I'll suggest combining the temperature and chirp rate\ntogether into one thing for each species, then pivoting them longer (\"combining\"),\nthen pivoting them wider again (\"splitting\"). Create new columns, named for  each species,\nthat contain the temperature and pulse rate for that species in\nthat order, `unite`d together.\nFor the rest of this question, start from the data frame you read\nin, and build a pipe, one or two steps at a time, to save creating\na lot of temporary data frames.\n\n\n\n(d) The two columns `exclamationis` and `niveus`\nthat you just created are both temperature-pulse rate combos, but\nfor different species. Collect them together into one\ncolumn, labelled by species. (This is a straight `tidyr` `pivot_longer`, even though the columns contain something odd-looking.)\n\n\n\n(e) Now split up the temperature-pulse combos at the underscore, into\ntwo separate columns. This is `separate`. When specifying\nwhat to separate by, you can use a number (\"split after this many characters\") or a piece of text, in quotes (\"when you see this text, split at it\"). \n\n\n\n(f) Almost there.  Temperature and pulse rate are still text\n(because `unite` turned them into text), but they should be\nnumbers. Create new variables that are numerical versions of\ntemperature and pulse rate (using `as.numeric`). Check that\nyou have no extraneous variables (and, if necessary, get rid of\nthe ones you don't want). (Species is also text and really ought\nto be a factor, but having it as text doesn't seem to cause any\nproblems.)\nYou can, if you like, use `parse_number` instead of\n`as.numeric`. They should both work. The distinction I\nprefer to make is that `parse_number` is good for text\nwith a number in it (that we want to pull the number out of),\nwhile `as.numeric` is for turning something that looks like\na number but isn't one into a genuine number.^[You could      just as well make the point that the text 20.8 contains the      number 20.8 and nothing else, so that parsing it as text in search of a number      will pull out 20.8 as a number. If that logic works for you, go      with it.]\n\n\n\n\n\n\n\n\n## Number 1 songs\n\n\n\n The data file\n[link](http://stat405.had.co.nz/data/billboard.csv) contains a lot of\ninformation about songs popular in 2000. This dataset is untidy.  Our\nultimate aim is to answer \"which song occupied the \\#1 position for the largest number of weeks?\". To do that, we will build a pipe that\nstarts from the data frame read in from the URL above, and finishes\nwith an answer to the question. I will take you through this step by\nstep. Each part will involve adding something to the pipe you built\npreviously (possibly after removing a line or two that you used to\ndisplay the previous result).\n\n\n\n(a) Read the data and display what you have.\n\n\n\n(b) The columns `x1st.week` through\n`x76th.week` contain the rank of each song in the Billboard\nchart in that week, with week 1 being the first week that the song\nappeared in the chart.  Convert all these columns into two: an\nindication of week, called `week`, and of rank, called\n`rank`. Most songs appeared in the Billboard chart for a\nlot less than 76 weeks, so there are missing values, which you\nwant to remove.  (I say \"indication of week\" since this will\nprobably be text at the moment). Display your new data frame. Do\nyou have fewer columns?  Why do you have a lot more rows? Explain\nbriefly.\n\n\n\n(c) Both your `week` and `rank` columns are\n(probably) text. Create new columns that contain just the numeric\nvalues, and display just your new columns, again adding onto the\nend of your pipe. If it so happens that `rank` is already a number, leave it as it is.\n\n \n\n\n(e) The meaning of your week-number column is that it refers\nto the number of weeks *after* the song first appeared in the\nBillboard chart. That is, if a song's first appearance (in\n`date.entered)` is July 24, then week 1 is July 24, week 2\nis July 31, week 3 is August 7, and so on. Create a column\n`current` by adding the appropriate number of *days*,\nbased on your week number, to `date.entered`. Display\n`date.entered`, your week number, and `current` to\nshow that you have calculated the right thing. Note that you can\nadd a number of days onto a date and you will get another date.\n\n\n\n(f) Reaching the \\#1 rank on the Billboard chart is one of\nthe highest accolades in the popular music world. List all the\nsongs that reached `rank` 1. For these songs, list the\nartist (as given in the data set), the song title, and the date(s)\nfor which the song was ranked number 1. Arrange the songs in date\norder of being ranked \\#1. Display all the songs (I found 55 of them).\n\n\n\n(g) Use R to find out which song held the \\#1 rank for the\nlargest number of weeks. For this, you can assume that the song\ntitles are all unique (if it's the same song title, it's the same\nsong), but the artists might not be (for example, Madonna might\nhave had two different songs reach the \\#1 rank). The information\nyou need is in the output you obtained for the previous part, so\nit's a matter of adding some code to the end of that.\nThe last mark was for displaying *only* the song that was\nranked \\#1 for the largest number of weeks, or for otherwise\nmaking it easy to see which song it was.\n\n\n\n\n\n\n\n##  Bikes on College\n\n\n The City of Toronto collects all kinds of data on aspects of\nlife in the city. See\n[link](http://www1.toronto.ca/wps/portal/contentonly?vgnextoid=1a66e03bb8d1e310VgnVCM10000071d60f89RCRD). One\ncollection of data is records of the number of cyclists on certain\ndowntown streets. The data in\n[link](http://ritsokiguess.site/datafiles/bikes.csv) are a record\nof the cyclists on College Street on the block west from Huron to\nSpadina on September 24, 2010. In the spreadsheet, each row relates to\none cyclist. The first column is the time the cyclist was observed (to\nthe nearest 15 minutes). After that, there are four pairs of\ncolumns. The observer filled in (exactly) one X in each pair of\ncolumns, according to whether (i) the cyclist was male or female, (ii)\nwas or was not wearing a helmet, (iii) was or was not carrying a\npassenger on the bike, (iv) was or was not riding on the sidewalk. We\nwant to create a tidy data frame that has the time in each row, and\nhas columns containing appropriate values, often `TRUE` or\n`FALSE`, for each of the four variables measured.\n\nI will lead you through the process, which will involve developing a\n(long) pipe, one step at a time.\n\n\n\n(a) Take a look at the spreadsheet (using Excel or similar:\nthis may open when you click the link). Are there any obvious\nheader rows? Is there any extra material before the data start?\nExplain briefly.\n\n\n\n(b) Read the data into an R data\nframe. Read *without* headers, and instruct R how many lines\nto skip over using `skip=` and a suitable number.\nWhen this is working, display the first few lines of your data\nframe.  Note that your columns have names `X1` through\n`X9`.\n\n\n\n(c) What do you notice about the times in your first\ncolumn? What do you think those \"missing\" times should be?\n\n\n(d) Find something from the `tidyverse` that will\nfill^[Oh, what a giveaway.]\nin those missing values with the right thing.\nStart a pipe from the data frame you read in, that updates the\nappropriate column with the filled-in times.\n\n\n\n(e) R's `ifelse` function works like `=IF` in\nExcel. You use it to create values for a new variable, for\nexample in a `mutate`.  The first input to it is a\nlogical condition (something that is either true or false); the\nsecond is the value your new variable should take if the\ncondition is true, and the third is the value of your new\nvariable if the condition is false.  Create a new column\n`gender` in your data frame that is \"male\" or\n\"female\" depending on the value of your `X2` column,\nusing `mutate`. (You can assume that exactly one of the\nsecond and third columns has an `X` in it.) Add your code\nto the end of your pipe and display (the first 10 rows of) the\nresult.\n\n\n\n\n(f) Create variables `helmet`, `passenger` and\n`sidewalk` in your data frame that are `TRUE` if\nthe \"Yes\" column contains `X` and `FALSE`\notherwise. This will use `mutate` again, but you don't\nneed `ifelse`: just set the variable equal to the\nappropriate logical condition. As before, the best way to\ncreate these variables is to test the appropriate things for\nmissingness.  Note that you can create as many new variables\nas you like in one `mutate`. Show the first few lines\nof your new data frame. (Add your code onto the end of the\npipe you made above.)\n\n\n\n(g) Finally \n(for the data manipulation), get rid of\nall the original columns, keeping only the new ones that\nyou created. Save the results in a data frame and display\nits first few rows.\n \n\n\n(h) The next few parts are a quick-fire analysis of\nthe data set. They can all be solved using `count`.\nHow many male and how many female cyclists were observed\nin total?\n\n\n\n(i) How many male and female cyclists were not\nwearing helmets?\n\n\n\n(j) How many cyclists were riding on the sidewalk\n*and* carrying a passenger?\n\n\n\n(k) What was the busiest 15-minute period of the\nday, and how many cyclists were there then?\n\n\n\n\n\n\n\n\n##  Feeling the heat\n\n\n In summer, the city of Toronto issues Heat Alerts for \n\"high heat or humidity that is expected to last two or more days\". The\nprecise definitions are shown at\n[link](http://www1.toronto.ca/wps/portal/contentonly?vgnextoid=923b5ce6dfb31410VgnVCM10000071d60f89RCRD). During\na heat alert, the city opens Cooling Centres and may extend the hours\nof operation of city swimming pools, among other things. All the heat\nalert days from 2001 to 2016 are listed at\n[link](http://ritsokiguess.site/datafiles/heat.csv).\n\nThe word \"warning\" is sometimes used in place of \"alert\" in these\ndata. They mean the same thing.^[Unlike *thunderstorm watch*  and *thunderstorm warning*, which mean different things.]\n\n\n\n(a) Read the data into R, and display the data frame. Note that there are four columns:\n\n\n* a numerical `id` (numbered upwards from the first Heat\nAlert in 2001; some of the numbers are missing)\n\n* the `date` of the heat alert, in year-month-day\nformat with 4-digit years.\n\n* a text `code` for the type of heat alert\n\n* `text` describing the kind of heat alert. This can be quite long. \n\n\n\n\n(b) In your data frame, are the dates stored as genuine dates or as text? How can you tell?\n\n\n\n(c) Which different heat alert codes do you have, and how many of each?\n\n\n\n(d) Use the `text` in your dataset (or look back\nat the original data file) to describe briefly in your own\nwords what the various codes represent.\n\n\n\n(e) How many (regular and extended) heat alert events\nare there altogether? A heat alert event is a stretch of\nconsecutive days, on all of which there is a heat alert or\nextended heat alert. Hints: (i) you can answer this from\noutput you already have; (ii) how can you tell when a heat\nalert event *starts*?\n\n\n\n(f) We are going to investigate how many heat alert\ndays\nthere were in each year. To do that, we have\nto extract the year from each of our dates. \n\n\n\n(g) Count the number of heat alert days for each\nyear, by tabulating the year variable.\nLooking at this table, would you say that there\nhave been more heat alert days in recent years? Explain\n(very) briefly. \n\n\n \n \n\n\n\n\n## Isoflavones\n\n The plant called kudzu was imported to the US South from Japan. It is rich in isoflavones, which are believed to be beneficial for bones. In a study, rats were randomly assigned to one of three diets: one with a low dose of isoflavones from kudzu, one with a high dose, and a control diet with no extra isoflavones. At the end of the study, each rat's bone density was measured, in milligrams per square centimetre. The data as recorded are shown in [http://ritsokiguess.site/isoflavones.txt](http://ritsokiguess.site/isoflavones.txt).^[Evidently the units were chosen for ease of recording; had the values been in grams instead, the person recording the data would have had to put a 0 and a decimal point on the front of each value. This is the old meaning of the word \"coding\"; making the data values be whole numbers and/or small deviations from something makes them easier to record, and in pre-computer days easier to calculate with. You will also see the same word used for classifying survey responses into categories, which is similar but not quite the same thing.] There are 15 observations for each treatment, and hence 45 altogether.\n\nHere are some code ideas you might need to use later, all part of the `tidyverse`. You may need to find out how they work.\n\n\n- `col_names` (in the `read_` functions)\n- `convert` (in various `tidyverse` functions)\n- `fill`\n- `na_if`\n- `rename`\n- `separate_rows`\n- `skip` (in the `read_` functions)\n- `values_drop_na` (in the `pivot_` functions)\n\nIf you use any of these, *cite* the webpage(s) or other source(s) where you learned about them.\n\n\n\n\n(a) Take a look at the data file. Describe briefly what you see.\n\n\n\n(b) Read in the data, using `read_table`, and get it into a tidy form, suitable for making a graph. This means finishing with (at least) a column of treatments with a suitable name (the treatments will be text) and a column of bone density values (numbers), one for each rat. You can have other columns as well; there is no obligation to get rid of them. Describe your process clearly enough that someone new to this data set would be able to understand what you have done and reproduce it on another similar dataset. Before you begin, think about whether or not you want to keep the column headers that are in the data file or not. (It can be done either way, but one way is easier than the other.)\n\n\n\n(c) The statistician on this study is thinking about running an ordinary analysis of variance to compare the bone mineral density for the different treatments. Obtain a plot from your tidy dataframe that will help her decide whether that is a good idea.\n\n\n\n(d) Based on your graph, and any additional graphs you wish to draw, what analysis would you recommend for this dataset? Explain briefly. (Don't do the analysis.)\n\n\n\n\n\n\n\n\n\n## Jocko's Garage\n\n Insurance adjusters are concerned that Jocko's Garage is giving estimates for repairing car damage that are too high. To see whether this is indeed the case, ten cars that had been in collisions were taken to both Jocko's Garage and another garage, and the two estimates for repair were recorded. The data as recorded are [here](http://ritsokiguess.site/datafiles/jocko.txt).\n\n\n\n(a) Take a look at the data file (eg. by using your web browser). How are the data laid out? Do there appear to be column headers?\n\n\n\n(b) Read in and display the data file, bearing in mind what you just concluded about it. What names did the columns acquire?\n\n\n\n(c) Make this data set tidy. That is, you need to end up with columns containing the repair cost estimates at each of the two garages and also identifying the cars, with each observation on one row. Describe your thought process. (It needs to be possible for the reader to follow your description and understand why it works.)\nSave your tidy dataframe.\n\n\n\n(d) Make a suitable graph to assess the comparison of interest, and say briefly what your graph tells you.\n\n\n\n(e) Carry  out a test to make an appropriate comparison of the mean estimates. What do you conclude, in the context of the data?\n\n\n\n\n\n\n\n\n## Tidying electricity consumption\n\n How does the consumption of electricity depend on  temperature?  To find out, a short-term study was carried out by a utility company based in a certain area. For a period of two years, the average monthly temperature was recorded (in degrees Fahrenheit), the mean daily demand for electricity per household (in kilowatt hours), and the cost per kilowatt  hour of electricity for that year (8 cents for the first year and 10 cents for the second, which it will be easiest to treat as categorical). \n\nThe data were laid out in an odd way, as shown in [http://ritsokiguess.site/datafiles/utils.txt](http://ritsokiguess.site/datafiles/utils.txt), in aligned columns: the twelve months of temperature were laid out on *two* lines for the first year, then the twelve months of consumption for the first year on the next two lines, and then four more lines for the second year laid out the same way. Thus the temperature of 31 in the first line goes with the consumption of 55 in the *third* line, and the last measurements for that year are the 78 at the end of the second line (temperature) and 73 at the end of the fourth line (consumption).  Lines 5 through 8 of the data file are the same thing for the second year (when electricity was more expensive). \n\nThe data seem to have been laid out in order of temperature, rather than in order of months, which I would have thought would make more sense. But this is what we have.\n\n\n\n(a) Read in and display the data file, bearing in mind that it has *no column names*.\n\n\n\n(b) Arrange these data tidily, so that there is a column of price (per kilowatt hour), a column of temperatures, and a column of consumptions. Describe your process, including why you got list-columns (if you did) and what you did about them (if necessary).\n\n\n\n(c) Make a suitable graph of temperature, consumption and price in your tidy dataframe. Add smooth trends if appropriate. If you were unable to get the data tidy, use my tidy version [here](http://ritsokiguess.site/datafiles/utils_tidy.csv). (If you need the other file, right-click on \"here\" and Copy Link Address.)\n\n\n\n(d) What patterns or trends do you see on your graph? Do they make practical sense? There are two things I would like you to comment on.\n\n\n\n\n\n\n\n## Tidy blood pressure\n\n Going to the dentist is scary for a lot of people. One way in which this might show up is that people might have higher blood pressure on average before their dentist's appointment than an hour after the appointment is done. Ten randomly-chosen individuals have their (systolic^[A blood pressure is usually given as two numbers, like \"120 over 80\". The first number, which is the one shown in our data, is called the systolic blood pressure. It is the pressure in the arteries when the heart is pumping. The second is called the diastolic blood pressure, and it is the pressure in the arteries when the heart is resting.])  blood pressure measured while they are in a dentist's waiting room, and then again one hour after their appointment is finished.\n\nYou might have seen a tidy version of this data set before.\n\nThe data as I originally received it is in [http://ritsokiguess.site/datafiles/blood_pressure2.csv](http://ritsokiguess.site/datafiles/blood_pressure2.csv).\n\n\n\n(a) Read in and display the data as originally received.\n\n\n\n(b) Describe briefly how the data you read in is not tidy, bearing in mind how the data were collected and how they would be analysed.\n\n\n\n(c) Produce a tidy dataframe from the one you read in from the file. (How many rows should you have?)\n\n\n\n(d) What kind of test might you run on these data? Explain briefly.\n\n\n\n(e) Draw a suitable graph of these data.\n\n\n\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Baseball and softball spaghetti\n\n\n On a previous assignment, we found that students could throw\na baseball further than they could throw a softball. In this question,\nwe will make a graph called a \"spaghetti plot\" to illustrate this\ngraphically. (The issue previously was that the data were matched\npairs: the same students threw both balls.)\n\nThis seems to work most naturally by building a pipe, a line or two at\na time. See if you can do it that way. (If you can't make it work, use lots of\ntemporary data frames, one to hold the result of each part.)\n\n\n\n(a) Read in the data again from\n[link](http://ritsokiguess.site/datafiles/throw.txt). The\nvariables had no names, so supply some, as you did before.\n\n\nSolution\n\n\nLiteral copy and paste: \n\n::: {.cell}\n\n```{.r .cell-code}\nmyurl <- \"http://ritsokiguess.site/datafiles/throw.txt\"\nthrows <- read_delim(myurl, \" \", col_names = c(\"student\", \"baseball\", \"softball\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 24 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\ndbl (3): student, baseball, softball\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nthrows\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 x 3\n   student baseball softball\n     <dbl>    <dbl>    <dbl>\n 1       1       65       57\n 2       2       90       58\n 3       3       75       66\n 4       4       73       61\n 5       5       79       65\n 6       6       68       56\n 7       7       58       53\n 8       8       41       41\n 9       9       56       44\n10      10       70       65\n# i 14 more rows\n```\n:::\n:::\n\n       \n$\\blacksquare$\n\n\n(b) Create a new column that is the students turned into a `factor`,\nadding it to your data frame. The reason for this will become clear\nlater.\n\n\nSolution\n\n\nFeed `student` into `factor`, creating a new\ncolumn with `mutate`:\n\n::: {.cell}\n\n```{.r .cell-code}\nthrows %>% mutate(fs = factor(student))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 x 4\n   student baseball softball fs   \n     <dbl>    <dbl>    <dbl> <fct>\n 1       1       65       57 1    \n 2       2       90       58 2    \n 3       3       75       66 3    \n 4       4       73       61 4    \n 5       5       79       65 5    \n 6       6       68       56 6    \n 7       7       58       53 7    \n 8       8       41       41 8    \n 9       9       56       44 9    \n10      10       70       65 10   \n# i 14 more rows\n```\n:::\n:::\n\n       \n\nThis doesn't look any different from the original student numbers, but\nnote the variable type at the top of the column.\n\n$\\blacksquare$\n\n(c) Collect together all the throwing distances into one column,\nmaking a second column that says which ball was thrown.\n\n\nSolution\n\nUse `pivot_longer`. It goes like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nthrows %>%\n  mutate(fs = factor(student)) %>%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 48 x 4\n   student fs    ball     distance\n     <dbl> <fct> <chr>       <dbl>\n 1       1 1     baseball       65\n 2       1 1     softball       57\n 3       2 2     baseball       90\n 4       2 2     softball       58\n 5       3 3     baseball       75\n 6       3 3     softball       66\n 7       4 4     baseball       73\n 8       4 4     softball       61\n 9       5 5     baseball       79\n10       5 5     softball       65\n# i 38 more rows\n```\n:::\n:::\n\nThe `names_to` is the name of a new categorical column whose values will be what is currently column names, and the `values_to` names a new quantitative (usually) column that will hold the values in those columns that you are making longer.\n\n\nIf you want to show off a little, you can use a select-helper, noting\nthat the columns you want to make longer all end in \"ball\":\n\n::: {.cell}\n\n```{.r .cell-code}\nthrows %>%\n  mutate(fs = factor(student)) %>%\n  pivot_longer(ends_with(\"ball\"), names_to=\"ball\", values_to=\"distance\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 48 x 4\n   student fs    ball     distance\n     <dbl> <fct> <chr>       <dbl>\n 1       1 1     baseball       65\n 2       1 1     softball       57\n 3       2 2     baseball       90\n 4       2 2     softball       58\n 5       3 3     baseball       75\n 6       3 3     softball       66\n 7       4 4     baseball       73\n 8       4 4     softball       61\n 9       5 5     baseball       79\n10       5 5     softball       65\n# i 38 more rows\n```\n:::\n:::\n\n       \n\nThe same result. Use whichever you like.\n\n$\\blacksquare$\n\n(d) Using your new data frame, make a \"scatterplot\" of throwing\ndistance against type of ball.\n\n\nSolution\n\n\nThe obvious thing. No data frame in the `ggplot` because it's the data frame that came out of the previous part of the pipeline (that doesn't have a name):\n\n::: {.cell}\n\n```{.r .cell-code}\nthrows %>%\n  mutate(fs = factor(student)) %>%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %>% \n  ggplot(aes(x = ball, y = distance)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/throw-spaghetti-5-1.pdf){fig-pos='H'}\n:::\n:::\n\nThis is an odd type of scatterplot because the $x$-axis is actually a categorical variable. It's really what would be called something like a dotplot. We'll be using this as raw material for the plot we actually want.\n       \n\nWhat this plot is missing is an indication of which student threw\nwhich ball. As it stands now, it could be an inferior version of a\nboxplot of distances thrown for each ball (which would imply that they\nare two independent sets of students, something that is not true).\n\n$\\blacksquare$\n\n(e) Add two things to your plot: something that will distinguish\nthe students by colour (this works best if the thing distinguished\nby colour is a factor),^[You can try it without. See below.]\nand something that will join the two points for the same student by\na line.\n\n\nSolution\n\n\nA `colour` and a `group` in the `aes`, and\na `geom_line`:\n\n::: {.cell}\n\n```{.r .cell-code}\nthrows %>%\n  mutate(fs = factor(student)) %>%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %>% \n  ggplot(aes(x = ball, y = distance, group = fs, colour = fs)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/throw-spaghetti-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nYou can see what happens if you use the student as a number:\n\n::: {.cell}\n\n```{.r .cell-code}\nthrows %>%\n  mutate(fs = factor(student)) %>%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %>% \n  ggplot(aes(x = ball, y = distance, group = student, colour = student)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/throw-spaghetti-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nNow the student numbers are distinguished as a shade of blue (on an\nimplied continuous scale: even a nonsensical fractional student number\nlike 17.5 would be a shade of blue). This is not actually so bad here,\nbecause all we are trying to do is to distinguish the students\nsufficiently from each other so that we can see where the spaghetti\nstrands go. But I like the multi-coloured one better.\n\n$\\blacksquare$\n\n(f) The legend is not very informative. Remove it from the plot,\nusing `guides`.\n\n\nSolution\n\n\nYou may not have seen this before. Here's what to do: Find what's\nat the top of the legend that you want to remove. Here that is\n`fs`. Find where `fs` appears in your\n`aes`. It actually appears in two places: in\n`group` and `colour`. I think the legend we want\nto get rid of is actually the `colour` one, so we do this:\n\n::: {.cell}\n\n```{.r .cell-code}\nthrows %>%\n  mutate(fs = factor(student)) %>%\n  pivot_longer(baseball:softball, names_to=\"ball\", values_to=\"distance\") %>% \n  ggplot(aes(x = ball, y = distance, group = fs, colour = fs)) +\n  geom_point() + geom_line() +\n  guides(colour = \"none\")\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/throw-spaghetti-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n       \n\nThat seems to have done it.\n\n$\\blacksquare$\n\n(g) What do you see on the final spaghetti plot? What does that tell you\nabout the relative distances a student can throw a baseball vs. a\nsoftball? Explain briefly, blah blah blah.\n\n\nSolution\n\n\nMost of the spaghetti strands go downhill from baseball to\nsoftball, or at least very few of them go uphill. That tells us\nthat most students can throw a baseball further than a softball.\nThat was the same impression that the matched-pairs $t$-test\ngave us. But the spaghetti plot tells us something else. If you\nlook carefully, you see that most of the big drops are for\nstudents who could throw a baseball a long way. These students\nalso threw a softball further than the other students, but not\nby as much. Most of the spaghetti strands in the bottom half of\nthe plot go more or less straight across. This indicates that\nstudents who cannot throw a baseball very far will throw a\nsoftball about the same distance as they threw the baseball.\nThere is an argument you could make here that the difference\nbetween distances thrown is a *proportional* one, something\nlike \"a student typically throws a baseball 20\\% further than a softball\". \nThat could be assessed by comparing not the\ndistances themselves, but the logs of the distances: in other\nwords, making a log transformation of all the\ndistances. (Distances have a lower limit of zero, so you might\nexpect observed distances to be skewed to the right, which is\nanother argument for making some kind of transformation.)\n\n$\\blacksquare$\n\n\n\n\n\n##  Ethanol and sleep time in rats\n\n\n A biologist wished to study the effects of ethanol on sleep\ntime in rats. A sample of 20 rats (all the same age) was selected, and\neach rat was given an injection having a particular concentration (0,\n1, 2 or 4 grams per kilogram of body weight) of ethanol. These are\nlabelled `e0, e1, e2, e4`. The \"0\"\ntreatment was a control group. The rapid eye movement (REM) sleep time\nwas then recorded for each rat. The data are in\n[link](http://ritsokiguess.site/datafiles/ratsleep.txt). \n\n\n\n(a) Read the data in from the file. Check that you have four rows\nof observations  and five columns of sleep times.\n\n\nSolution\n\n\nSeparated by single spaces:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/ratsleep.txt\"\nsleep1 <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 4 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (1): treatment\ndbl (5): obs1, obs2, obs3, obs4, obs5\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nsleep1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 6\n  treatment  obs1  obs2  obs3  obs4  obs5\n  <chr>     <dbl> <dbl> <dbl> <dbl> <dbl>\n1 e0         88.6  73.2  91.4  68    75.2\n2 e1         63    53.9  69.2  50.1  71.5\n3 e2         44.9  59.5  40.2  56.3  38.7\n4 e4         31    39.6  45.3  25.2  22.7\n```\n:::\n:::\n\n \n\nThere are six columns, but one of them labels the groups, and there\nare correctly five columns of sleep times.\n\nI used a \"temporary\" name for my data frame, because I'm going to be\ndoing some processing on it in a minute, and I want to reserve the\nname `sleep` for my processed data frame.\n\n$\\blacksquare$\n\n(b) Unfortunately, the data are in the wrong format. All the sleep\ntimes for each treatment group are on one row, and we should have\n*one* column containing *all* the sleep times, and the\ncorresponding row should show which treatment group that sleep time\ncame from. Transform this data frame into one that you could use for modelling or making graphs.\n\n\n\n\nSolution\n\nWe will want *one* column of sleep times, with an additional categorical column saying what observation each sleep time was within its group (or, you might say, we don't really care about that much, but that's what we are going to get). \n\nThe columns `obs1` through `obs5` are\ndifferent in that they are different observation numbers\n(\"replicates\", in the jargon). I'll call that `rep`. What\nmakes them the same is that they are all sleep times. Columns\n`obs1` through `obs5` are the ones we want to\ncombine, thus. \nHere is where I use the name `sleep`: I save the result of\nthe `pivot_longer` into a data frame `sleep`. Note that I\nalso used the brackets-around-the-outside to display what I had,\nso that I didn't have to do a separate display. This is a handy\nway of saving *and* displaying in one shot:\n\n::: {.cell}\n\n```{.r .cell-code}\n(sleep1 %>% \n  pivot_longer(-treatment, names_to=\"rep\", values_to=\"sleeptime\") -> sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 3\n   treatment rep   sleeptime\n   <chr>     <chr>     <dbl>\n 1 e0        obs1       88.6\n 2 e0        obs2       73.2\n 3 e0        obs3       91.4\n 4 e0        obs4       68  \n 5 e0        obs5       75.2\n 6 e1        obs1       63  \n 7 e1        obs2       53.9\n 8 e1        obs3       69.2\n 9 e1        obs4       50.1\n10 e1        obs5       71.5\n11 e2        obs1       44.9\n12 e2        obs2       59.5\n13 e2        obs3       40.2\n14 e2        obs4       56.3\n15 e2        obs5       38.7\n16 e4        obs1       31  \n17 e4        obs2       39.6\n18 e4        obs3       45.3\n19 e4        obs4       25.2\n20 e4        obs5       22.7\n```\n:::\n:::\n\nTypically in this kind of work, you have a lot of columns that need to be made longer, and a much smaller number of columns that need to be repeated as necessary. You can either specify all the columns to make longer, or you can specify \"not\" the other columns. Above, my first input to `pivot_longer` was \"everything but treatment\", but you could also do it like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep1 %>% \n  pivot_longer(obs1:obs5, names_to=\"rep\", values_to=\"sleeptime\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 3\n   treatment rep   sleeptime\n   <chr>     <chr>     <dbl>\n 1 e0        obs1       88.6\n 2 e0        obs2       73.2\n 3 e0        obs3       91.4\n 4 e0        obs4       68  \n 5 e0        obs5       75.2\n 6 e1        obs1       63  \n 7 e1        obs2       53.9\n 8 e1        obs3       69.2\n 9 e1        obs4       50.1\n10 e1        obs5       71.5\n11 e2        obs1       44.9\n12 e2        obs2       59.5\n13 e2        obs3       40.2\n14 e2        obs4       56.3\n15 e2        obs5       38.7\n16 e4        obs1       31  \n17 e4        obs2       39.6\n18 e4        obs3       45.3\n19 e4        obs4       25.2\n20 e4        obs5       22.7\n```\n:::\n:::\n\nor like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep1 %>% \n  pivot_longer(starts_with(\"obs\"), names_to=\"rep\", values_to=\"sleeptime\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 3\n   treatment rep   sleeptime\n   <chr>     <chr>     <dbl>\n 1 e0        obs1       88.6\n 2 e0        obs2       73.2\n 3 e0        obs3       91.4\n 4 e0        obs4       68  \n 5 e0        obs5       75.2\n 6 e1        obs1       63  \n 7 e1        obs2       53.9\n 8 e1        obs3       69.2\n 9 e1        obs4       50.1\n10 e1        obs5       71.5\n11 e2        obs1       44.9\n12 e2        obs2       59.5\n13 e2        obs3       40.2\n14 e2        obs4       56.3\n15 e2        obs5       38.7\n16 e4        obs1       31  \n17 e4        obs2       39.6\n18 e4        obs3       45.3\n19 e4        obs4       25.2\n20 e4        obs5       22.7\n```\n:::\n:::\n\nThis one was a little unusual in that usually with these you have the *treatments* in the columns and the replicates in the rows. It doesn't matter, though: `pivot_longer` handles both cases.\n\n\n \nWe have 20 rows of 3 columns. I got all the rows, but you will\nprobably get an output with ten rows as usual, and will need to click\nNext to see the last ten rows. The initial display will say how many\nrows (20) and columns (3) you have.\n\nThe column `rep` is not very interesting: it just says which\nobservation each one was within its group.^[Sometimes the  column playing the role of `rep` *is* interesting to us, but  not here.] \nThe interesting things are `treatment` and\n`sleeptime`, which are the two variables we'll need for our\nanalysis of variance.\n\n$\\blacksquare$\n\n\n(c) Using your new data frame, make side-by-side boxplots of sleep\ntime by treatment group. \n\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleep, aes(x = treatment, y = sleeptime)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/ethanol-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\n$\\blacksquare$\n\n\n(d) In your boxplots, how does the median sleep time appear to\ndepend on treatment group?\n\n\n\nSolution\n\n\nIt appears to *decrease* as the dose of ethanol increases,\nand pretty substantially so (in that the differences ought to be\nsignificant, but that's coming up). \n\n\n$\\blacksquare$\n\n(e) There is an assumption about spread that the analysis of\nvariance needs in order to be reliable. Do your boxplots indicate that\nthis assumption is satisfied for these data, bearing in mind that you\nhave only five observations per group?\n\n\n\nSolution\n\n\nThe assumption is that the population SDs of each group are all\nequal. Now, the boxplots show IQRs, which are kind of a surrogate\nfor SD, and because we only have five observations per group to\nbase the IQRs on, the *sample* IQRs might vary a bit. So we\nshould look at the heights of the boxes on the boxplot, and see\nwhether they are grossly unequal. They appear to be to be of very\nsimilar heights, all things considered, so I am happy.\n\nIf you want the SDs themselves:\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep %>%\n  group_by(treatment) %>%\n  summarize(stddev = sd(sleeptime))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  treatment stddev\n  <chr>      <dbl>\n1 e0         10.2 \n2 e1          9.34\n3 e2          9.46\n4 e4          9.56\n```\n:::\n:::\n\n \n\nThose are *very* similar, given only 5 observations per group. No\nproblems here.\n\n\n$\\blacksquare$\n\n(f) Run an analysis of variance to see whether sleep time differs\nsignificantly among treatment groups. What do you conclude?\n\n\n\nSolution\n\n\nI use `aov` here, because I might be following up with\nTukey in a minute:\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep.1 <- aov(sleeptime ~ treatment, data = sleep)\nsummary(sleep.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ntreatment    3   5882    1961   21.09 8.32e-06 ***\nResiduals   16   1487      93                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nThis is a very small P-value, so my conclusion is that the mean sleep\ntimes are not all the same for the treatment groups. Further than that\nI am not entitled to say (yet).\n\nThe technique here is to save the output from `aov` in\nsomething, look at that (via `summary`), and then that same\nsomething gets fed into `TukeyHSD` later. \n\n\n$\\blacksquare$\n\n(g) Would it be a good idea to run Tukey's method here? Explain\nbriefly why or why not, and if you think it would be a good idea, run\nit.\n\n\n\nSolution\n\n\nTukey's method is useful when (i) we have run an analysis of\nvariance and got a significant result and (ii) when we want to know\nwhich groups differ significantly from which. Both (i) and (ii) are\ntrue here. So:\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(sleep.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = sleeptime ~ treatment, data = sleep)\n\n$treatment\n        diff       lwr         upr     p adj\ne1-e0 -17.74 -35.18636  -0.2936428 0.0455781\ne2-e0 -31.36 -48.80636 -13.9136428 0.0005142\ne4-e0 -46.52 -63.96636 -29.0736428 0.0000056\ne2-e1 -13.62 -31.06636   3.8263572 0.1563545\ne4-e1 -28.78 -46.22636 -11.3336428 0.0011925\ne4-e2 -15.16 -32.60636   2.2863572 0.1005398\n```\n:::\n:::\n\n \n\n\n$\\blacksquare$\n\n(h) What do you conclude from Tukey's method? (This is liable to be\na bit complicated.) Is there a treatment that is clearly best, in\nterms of the sleep time being largest?\n\n\n\nSolution\n\n\nAll the differences are significant except treatment `e2`\nvs.\\ `e1` and `e4`. All the differences involving\nthe control group `e0` are significant, and if you look\nback at the boxplots in (c), you'll see that the control group `e0`\nhad the *highest* mean sleep time. So the control group is\nbest (from this point of view), or another way of saying it is\nthat *any* dose of ethanol is significantly reducing mean\nsleep time.\nThe other comparisons are a bit confusing, because the 1-4\ndifference is significant, but neither of the differences\ninvolving 2 are. That is, 1 is better than 4, but 2 is not\nsignificantly worse than 1 nor better than 4. This seems like it\nshould be a logical impossibility, but the story is that we don't\nhave enough data to decide where 2 fits relative to 1 or 4.  If we\nhad 10 or 20 observations per group, we might be able to conclude\nthat 2 is in between 1 and 4 as the boxplots suggest.\n\n\nExtra: I didn't ask about normality here, but like the equal-spreads assumption I'd say there's nothing controversial about it with these data. With normality good and equal spreads good, `aov` plus Tukey is the analysis of choice. \n\n$\\blacksquare$\n\n\n\n\n##  Growth of tomatoes\n\n\nA biology graduate student exposed each of 32\ntomato plants to one of four different colours of light (8 plants to\neach colour). The growth rate of each plant, in millimetres per week,\nwas recorded. The data are in\n[link](http://ritsokiguess.site/datafiles/tomatoes.txt). \n\n\n\n(a) Read the data into R and confirm that you have 8 rows and 5\ncolumns of data.\n\n\nSolution\n\n\nThis kind of thing:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url=\"http://ritsokiguess.site/datafiles/tomatoes.txt\"\ntoms1=read_delim(my_url,\" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 8 Columns: 5\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\ndbl (5): plant, blue, red, yellow, green\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ntoms1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 5\n  plant  blue   red yellow green\n  <dbl> <dbl> <dbl>  <dbl> <dbl>\n1     1  5.34  13.7   4.61  2.72\n2     2  7.45  13.0   6.63  1.08\n3     3  7.15  10.2   5.29  3.97\n4     4  5.53  13.1   5.29  2.66\n5     5  6.34  11.1   4.76  3.69\n6     6  7.16  11.4   5.57  1.96\n7     7  7.77  14.0   6.57  3.38\n8     8  5.09  13.5   5.25  1.87\n```\n:::\n:::\n\n \n\nI do indeed have 8 rows and 5 columns.\n\nWith only 8 rows, listing the data like this is good. \n\n\n$\\blacksquare$\n\n(b) Re-arrange the data so that you have *one* column\ncontaining all the growth rates, and another column saying which\ncolour light each plant was exposed to. (The aim here is to produce\nsomething suitable for feeding into `aov` later.) \n\n\nSolution\n\n\nThis is a job for `pivot_longer`:\n\n::: {.cell}\n\n```{.r .cell-code}\ntoms1 %>% \n   pivot_longer(-plant, names_to=\"colour\", values_to=\"growthrate\") -> toms2\ntoms2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 3\n   plant colour growthrate\n   <dbl> <chr>       <dbl>\n 1     1 blue         5.34\n 2     1 red         13.7 \n 3     1 yellow       4.61\n 4     1 green        2.72\n 5     2 blue         7.45\n 6     2 red         13.0 \n 7     2 yellow       6.63\n 8     2 green        1.08\n 9     3 blue         7.15\n10     3 red         10.2 \n# i 22 more rows\n```\n:::\n:::\n\nI chose to specify \"everything but plant number\", since there are several colour columns with different names.\n\n\n\nSince the column `plant` was never mentioned, this gets\nrepeated as necessary, so now it denotes \"plant within colour group\", \nwhich in this case is not very useful. (Where you have\nmatched pairs, or repeated measures in general, you *do* want to\nkeep track of which individual is which. But this is not repeated\nmeasures because plant number 1 in the blue group and plant number 1\nin the red group  are *different* plants.)\n\nThere were 8 rows originally and 4 different colours, so there should\nbe, and are, $8 \\times 4=32$ rows in the made-longer data set.\n\n\n$\\blacksquare$\n\n(c) Save the data in the new format to a text file. This is\nmost easily done using `write_csv`, which is the opposite\nof `read_csv`. It requires two things: a data frame, and\nthe name of a file to save in, which should have a `.csv`\nextension.  \n\n\nSolution\n\n\nThe code is easy enough:\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(toms2,\"tomatoes2.csv\")\n```\n:::\n\n       \n\nIf no error, it worked. That's all you need.\n\nTo verify (for my satisfaction) that it was saved correctly:\n\n::: {.cell}\n\n```{.bash .cell-code}\ncat tomatoes2.csv \n```\n\n\n::: {.cell-output .cell-output-stdout}\n```\nplant,colour,growthrate\n1,blue,5.34\n1,red,13.67\n1,yellow,4.61\n1,green,2.72\n2,blue,7.45\n2,red,13.04\n2,yellow,6.63\n2,green,1.08\n3,blue,7.15\n3,red,10.16\n3,yellow,5.29\n3,green,3.97\n4,blue,5.53\n4,red,13.12\n4,yellow,5.29\n4,green,2.66\n5,blue,6.34\n5,red,11.06\n5,yellow,4.76\n5,green,3.69\n6,blue,7.16\n6,red,11.43\n6,yellow,5.57\n6,green,1.96\n7,blue,7.77\n7,red,13.98\n7,yellow,6.57\n7,green,3.38\n8,blue,5.09\n8,red,13.49\n8,yellow,5.25\n8,green,1.87\n```\n:::\n:::\n\n \n\nOn my system, that will list the contents of the file. Or you can just\nopen it in R Studio (if you saved it the way I did, it'll be in the\nsame folder, and you can find it in the Files pane.)\n\n\n$\\blacksquare$\n\n(d) Make a suitable boxplot, and use it to assess the assumptions\nfor ANOVA. What do you conclude? Explain briefly. \n\n\nSolution\n\n\nNothing terribly surprising here. My data frame is called\n`toms2`, for some reason:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(toms2,aes(x=colour, y=growthrate))+geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/vonneckerburg-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nThere are no outliers, but there is a little skewness (compare the\n*whiskers*, not the placement of the median within the box,\nbecause what matters with skewness is the *tails*, not the middle\nof the distribution; it's problems in the tails that make the mean\nunsuitable as a measure of centre). The Red group looks the most\nskewed. Also, the Yellow group has smaller spread than the others (we\nassume that the population variances within each group are equal). The\nthing to bear in mind here, though, is that there are only eight\nobservations per group, so the distributions could appear to have\nunequal variances or some non-normality by chance. \n\nMy take is that these data, all things considered, are\n*just\nabout* OK for ANOVA. Another option would be to do Welch's ANOVA as\nwell and compare with the regular ANOVA: if they give more or less the\nsame P-value, that's a sign that I didn't need to worry.\n\nExtra: some people like to run a formal test on the variances to test\nthem for equality. My favourite (for reasons explained elsewhere) is\nthe Levene test, if you insist on going this way. It lives in package\n`car`, and *does not* take a `data=`, so you need\nto do the `with` thing:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nwith(toms2,leveneTest(growthrate,colour))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in leveneTest.default(growthrate, colour): colour coerced to factor.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  3  0.9075 0.4499\n      28               \n```\n:::\n:::\n\n \n\nThe warning is because `colour` was actually text, but the test\ndid the right thing by turning it into a factor, so that's OK.\n\nThere is no way we can reject equal variances in the four groups. The\n$F$-statistic is less than 1, in fact, which says that if the four\ngroups have the same population variances, the sample variances will\nbe *more* different than the ones we observed on average, and so\nthere is no way that these sample variances indicate different\npopulation variances. (This is because of 8 observations only per\ngroup; if there had been 80 observations per group, it would have been\na different story.) Decide for yourself whether you're surprised by this.\n\nWith that in mind, I think the regular ANOVA will be perfectly good,\nand we would expect that and the Welch ANOVA to give very similar results.\n\n\n \n\n\n$\\blacksquare$\n\n(e) Run (regular) ANOVA on these data. What do you conclude?\n(Optional extra: if you think that some other variant of ANOVA would\nbe better, run that as well and compare the results.)\n\n\nSolution\n\n\n`aov`, bearing in mind that Tukey is likely to follow:\n\n::: {.cell}\n\n```{.r .cell-code}\ntoms.1=aov(growthrate~colour,data=toms2)\nsummary(toms.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncolour       3  410.5  136.82   118.2 5.28e-16 ***\nResiduals   28   32.4    1.16                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nThis is a tiny P-value, so the mean growth rate for the different\ncolours is definitely *not* the same for all colours. Or, if you\nlike, one or more of the colours has a different mean growth rate than\nthe others.\n\nThis, remember, is as far as we go right now.\n\nExtra: if you thought that normality was OK but not equal spreads,\nthen Welch ANOVA is the way to go:\n\n::: {.cell}\n\n```{.r .cell-code}\ntoms.2=oneway.test(growthrate~colour,data=toms2)\ntoms.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne-way analysis of means (not assuming equal variances)\n\ndata:  growthrate and colour\nF = 81.079, num df = 3.000, denom df = 15.227, p-value = 1.377e-09\n```\n:::\n:::\n\n \n\nThe P-value is not *quite* as small as for the regular ANOVA, but\nit is still very small, and the conclusion is the same.\n\nIf you had doubts about the normality (that were sufficiently great,\neven given the small sample sizes), then go with Mood's median test\nfor multiple groups:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\nmedian_test(toms2,growthrate,colour)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$table\n        above\ngroup    above below\n  blue       5     3\n  green      0     8\n  red        8     0\n  yellow     3     5\n\n$test\n       what        value\n1 statistic 1.700000e+01\n2        df 3.000000e+00\n3   P-value 7.067424e-04\n```\n:::\n:::\n\n \n\nThe P-value is again extremely small (though not quite as small as for\nthe other two tests, for the usual reason that Mood's median test\ndoesn't use the data very efficiently: it doesn't use how *far*\nabove or below the overall median the data values are.)\n\nThe story here, as ever, is consistency: whatever you thought was\nwrong, looking at the boxplots, needs to guide the test you do:\n\n\n* if you are not happy with normality, go with\n`median_test` from `smmr` (Mood's median test). \n\n* if you are happy with normality and equal variances, go with\n`aov`.\n\n* if you are happy with normality but not equal variances, go with\n`oneway.test` (Welch ANOVA).\n\n\nSo the first thing to think about is normality, and if you are OK with\nnormality, then think about equal spreads. Bear in mind that you need\nto be willing to tolerate a certain amount of non-normality and\ninequality in the spreads, given that your data are only samples from\ntheir populations. (Don't expect perfection, in short.)\n\n\n$\\blacksquare$\n\n(f) If warranted, run a suitable follow-up. (If not warranted, explain briefly\nwhy not.)\n\n\nSolution\n\n\nWhichever flavour of ANOVA you ran (regular ANOVA, Welch ANOVA,\nMood's median test), you got the same conclusion for these data:\nthat the average growth rates were not all the same for the four\ncolours. That, as you'll remember, is as far as you go. To find\nout which colours differ from which in terms of growth rate, you\nneed to run some kind of multiple-comparisons follow-up, the\nright one for the analysis you did. Looking at the boxplots suggests that red is clearly best\nand green clearly worst, and it is possible that all the colours\nare significantly different from each other.)\nIf you did regular ANOVA, Tukey is what you need:\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(toms.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = growthrate ~ colour, data = toms2)\n\n$colour\n                diff       lwr        upr     p adj\ngreen-blue   -3.8125 -5.281129 -2.3438706 0.0000006\nred-blue      6.0150  4.546371  7.4836294 0.0000000\nyellow-blue  -0.9825 -2.451129  0.4861294 0.2825002\nred-green     9.8275  8.358871 11.2961294 0.0000000\nyellow-green  2.8300  1.361371  4.2986294 0.0000766\nyellow-red   -6.9975 -8.466129 -5.5288706 0.0000000\n```\n:::\n:::\n\n       \n\nAll of the differences are (strongly) significant, except for yellow\nand blue, the two with middling growth rates on the boxplot. Thus we\nwould have no hesitation in saying that growth rate is biggest in red\nlight and smallest in green light.\n\nIf you did Welch ANOVA, you need Games-Howell, which you have to get\nfrom one of the packages that offers it:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(PMCMRplus)\ngamesHowellTest(growthrate~factor(colour),data=toms2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n\tPairwise comparisons using Games-Howell test\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ndata: growthrate by factor(colour)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n       blue    green   red    \ngreen  1.6e-05 -       -      \nred    1.5e-06 4.8e-09 -      \nyellow 0.18707 0.00011 5.8e-07\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nP value adjustment method: none\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nalternative hypothesis: two.sided\n```\n:::\n:::\n\n \n\nThe conclusions are the same as for the Tukey: all the means are\nsignificantly different except for yellow and blue.\nFinally, if you did Mood's median test, you need this one:\n\n::: {.cell}\n\n```{.r .cell-code}\npairwise_median_test(toms2, growthrate, colour)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n  g1    g2       p_value adj_p_value\n  <chr> <chr>      <dbl>       <dbl>\n1 blue  green  0.0000633    0.000380\n2 blue  red    0.0000633    0.000380\n3 blue  yellow 0.317        1       \n4 green red    0.0000633    0.000380\n5 green yellow 0.0000633    0.000380\n6 red   yellow 0.0000633    0.000380\n```\n:::\n:::\n\n \n\nSame conclusions again. This is what I would have guessed; the\nconclusions from Tukey were so clear-cut that it really didn't matter\nwhich way you went; you'd come to the same conclusion.\n\nThat said, what I am looking for from you is a sensible choice of\nanalysis of variance (ANOVA, Welch's ANOVA or Mood's median test) for\na good reason, followed by the *right* follow-up for the test you\ndid. Even though the conclusions are all the same no matter what you\ndo here, I want you to get\nused to following the right method, so that you will be able to do the\nright thing when it *does* matter.\n\n$\\blacksquare$\n\n\n\n\n\n##  Pain relief in migraine headaches (again)\n\n\n The data in\n[link](http://ritsokiguess.site/datafiles/migraine.txt) are from a\nstudy of pain relief in migraine headaches. Specifically, 27 subjects\nwere randomly assigned to receive *one* of three pain relieving\ndrugs, labelled A, B and C. Each subject reported the number of hours\nof pain relief they obtained (that is, the number of hours between\ntaking the drug and the migraine symptoms returning). A higher value\nis therefore better. Can we make some recommendation about which drug\nis best for the population of migraine sufferers?\n\n\n\n(a) Read in and display the data. Take a look at the data\nfile first, and see if you can say why `read_table` will\nwork and `read_delim` will not.\n\n\nSolution\n\n\nThe key is two things: the data values are *lined up in        columns*, and \n*there is more than one space between  values*. \nThe second thing is why `read_delim` will not\nwork. If you look carefully at the data file, you'll see that\nthe column names are above and aligned with the columns. `read_table` doesn't actually need things to be lined up in columns; all it actually needs is for there to be one or more spaces between columns.\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/migraine.txt\"\nmigraine <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n-- Column specification --------------------------------------------------------\ncols(\n  DrugA = col_double(),\n  DrugB = col_double(),\n  DrugC = col_double()\n)\n```\n:::\n\n```{.r .cell-code}\nmigraine\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 3\n  DrugA DrugB DrugC\n  <dbl> <dbl> <dbl>\n1     4     6     6\n2     5     8     7\n3     4     4     6\n4     3     5     6\n5     2     4     7\n6     4     6     5\n7     3     5     6\n8     4    11     5\n9     4    10     5\n```\n:::\n:::\n\n \n\nSuccess.\n\n\n$\\blacksquare$\n\n(b) What is it about the experimental design that makes a one-way\nanalysis of variance plausible for data like this?\n\n\nSolution\n\n\nEach experimental subject only tested *one* drug, so that\nwe have 27 independent observations, nine from each drug. This\nis exactly the setup that a one-way ANOVA requires. \nCompare that to, for example, a situation where you had only 9\nsubjects, but they each tested *all* the drugs (so that\neach subject produced three measurements). That is like a\nthree-measurement version of matched pairs, a so-called\n**repeated-measures design**, which requires its own kind\nof analysis.^[To allow for the fact that measurements on the same      subject are not independent but correlated.] \n\n\n$\\blacksquare$\n\n(c) What is wrong with the current format of the data as far as\ndoing a one-way ANOVA analysis is concerned? (This is related to the\nidea of whether or not the data are \"tidy\".)\n\n\nSolution\n\n\nFor our analysis, we need one column of pain relief time and one\ncolumn labelling the drug that the subject in question took. \nOr, if you prefer to think about what would make these data\n\"tidy\": there are 27 subjects, so there ought to be 27 rows,\nand all three columns are measurements of pain relief, so they\nought to be in one column.\n\n\n$\\blacksquare$\n\n(d) \"Tidy\" the data to produce a data frame suitable for your\nanalysis. \n\n\nSolution\n\nThis is `pivot_longer`. The column names are going to be stored in a column `drug`, and the corresponding values in a column called `painrelief` (use whatever names you like):\n\n::: {.cell}\n\n```{.r .cell-code}\nmigraine %>% \n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") -> migraine2\n```\n:::\n\nSince I was making all the columns longer, I used the select-helper `everything()` to do that. Using instead `DrugA:DrugC` or `starts_with(\"Drug\")` would also be good. Try them. `starts_with` is not case-sensitive, as far as I remember, so `starts_with(\"drug\")` will also work here.\n\n\nWe do indeed have a new data frame with 27 rows, one per observation,\nand 2 columns, one for each variable: the pain relief hours, plus a\ncolumn identifying which drug that pain relief time came from. Exactly\nwhat `aov` needs.\n\nYou can probably devise a better name for your new data frame.\n\n\n$\\blacksquare$\n\n(e) Go ahead and run your one-way ANOVA (and Tukey if\nnecessary). Assume for this that the pain relief hours in each group\nare sufficiently close to normally distributed with sufficiently\nequal spreads.\n\n\nSolution\n\n\nMy last sentence absolves us from doing the boxplots that we\nwould normally insist on doing. \n\n::: {.cell}\n\n```{.r .cell-code}\npainrelief.1 <- aov(painrelief ~ drug, data = migraine2)\nsummary(painrelief.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value  Pr(>F)   \ndrug         2  41.19   20.59   7.831 0.00241 **\nResiduals   24  63.11    2.63                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nThere are (strongly) significant differences among the drugs, so it is\ndefinitely worth firing up Tukey to figure out where the differences are:\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(painrelief.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = painrelief ~ drug, data = migraine2)\n\n$drug\n                  diff        lwr      upr     p adj\nDrugB-DrugA  2.8888889  0.9798731 4.797905 0.0025509\nDrugC-DrugA  2.2222222  0.3132065 4.131238 0.0203671\nDrugC-DrugB -0.6666667 -2.5756824 1.242349 0.6626647\n```\n:::\n:::\n\n \n\nBoth the differences involving drug A are significant, and because a\nhigh value of `painrelief` is better, in both cases drug A is\n*worse* than the other drugs. Drugs B and C are not significantly\ndifferent from each other.\n\nExtra: we can also use the \"pipe\" to do this all in one go:\n\n::: {.cell}\n\n```{.r .cell-code}\nmigraine %>%\n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") %>%\n  aov(painrelief ~ drug, data = .) %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value  Pr(>F)   \ndrug         2  41.19   20.59   7.831 0.00241 **\nResiduals   24  63.11    2.63                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nwith the same results as before. Notice that I never actually created\na second data frame by name; it was created by `pivot_longer` and\nthen immediately used as input to `aov`.^[And then thrown away.] \nI also used the\n`data=.` trick to use \"the data frame that came out of the previous step\" as my input to `aov`.\n\nRead the above like this: \"take `migraine`, and then make everything longer, creating new columns `drug` and `painrelief`, and then do an ANOVA of `painrelief` by `drug`, and then summarize the results.\"\n\nWhat is even more alarming is that I can feed the output from\n`aov` straight into `TukeyHSD`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmigraine %>%\n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") %>%\n  aov(painrelief ~ drug, data = .) %>%\n  TukeyHSD()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = painrelief ~ drug, data = .)\n\n$drug\n                  diff        lwr      upr     p adj\nDrugB-DrugA  2.8888889  0.9798731 4.797905 0.0025509\nDrugC-DrugA  2.2222222  0.3132065 4.131238 0.0203671\nDrugC-DrugB -0.6666667 -2.5756824 1.242349 0.6626647\n```\n:::\n:::\n\n \n\nI wasn't sure whether this would work, since the output from\n`aov` is an R `list` rather than a data frame, but the\noutput from `aov` is sent into `TukeyHSD` whatever\nkind of thing it is.\n\nWhat I am missing here is to display the result of `aov`\n*and* use it as input to `TukeyHSD`. Of course, I had to\ndiscover that this could be solved, and indeed it can:\n\n::: {.cell}\n\n```{.r .cell-code}\nmigraine %>%\n  pivot_longer(everything(), names_to=\"drug\", values_to=\"painrelief\") %>%\n  aov(painrelief ~ drug, data = .) %>%\n  {\n    print(summary(.))\n    .\n  } %>%\n  TukeyHSD()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value  Pr(>F)   \ndrug         2  41.19   20.59   7.831 0.00241 **\nResiduals   24  63.11    2.63                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = painrelief ~ drug, data = .)\n\n$drug\n                  diff        lwr      upr     p adj\nDrugB-DrugA  2.8888889  0.9798731 4.797905 0.0025509\nDrugC-DrugA  2.2222222  0.3132065 4.131238 0.0203671\nDrugC-DrugB -0.6666667 -2.5756824 1.242349 0.6626647\n```\n:::\n:::\n\n \n\nThe odd-looking second-last line of that again uses the `.` trick\nfor \"whatever came out of the previous step\". The thing inside the\ncurly brackets is two commands one after the other; the first is to\ndisplay the `summary` of that `aov`^[It needs `print` around it to display it, as you need `print`  to display something within a loop or a function.] \nand the second is to just pass whatever came out of the\nprevious line, the output from `aov`, on, unchanged, into\n`TukeyHSD`. \n\nIn the Unix/Linux world this is called `tee`,\nwhere you print something *and* pass it on to the next step. The\nname `tee` comes from a (real physical) pipe that plumbers would use to\nsplit water flow into two, which looks like a letter T.\n\n\n$\\blacksquare$\n\n(f) What recommendation would you make about the best drug or\ndrugs? Explain briefly.\n\n\nSolution\n\n\nDrug A is significantly the worst, so we eliminate that. But\nthere is no significant difference between drugs B and C, so we\nhave no reproducible reason for preferring one rather than the\nother. Thus, we recommend \"either B or C\". \nIf you weren't sure which way around the drugs actually came\nout, then you should work out the mean pain relief score by\ndrug:\n\n::: {.cell}\n\n```{.r .cell-code}\nmigraine2 %>%\n  group_by(drug) %>%\n  summarize(m = mean(painrelief))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 2\n  drug      m\n  <chr> <dbl>\n1 DrugA  3.67\n2 DrugB  6.56\n3 DrugC  5.89\n```\n:::\n:::\n\n \nThese confirm that A is worst, and there is nothing much to choose\nbetween B and C.\nYou should *not* recommend drug B over drug C on this evidence,\njust because its (sample) mean is higher. The point about significant\ndifferences is that they are supposed to stand up to replication: in\nanother experiment, or in real-life experiences with these drugs, the\nmean pain relief score for drug A is expected to be worst, but between\ndrugs B and C, sometimes the mean of B will come out higher and\nsometimes C's mean will be higher, because there is no significant\ndifference between them.^[This talks about *means* rather  than individual observations; in individual cases, sometimes even  drug *A* will come out best. But we're interested in  population means, since we want to do the greatest good for the  greatest number. \"Greatest good for the greatest number\"  is from Jeremy Bentham, 1748--1832, British    philosopher and advocate of utilitarianism.]\nAnother way is to draw a boxplot of pain-relief scores:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(migraine2, aes(x = drug, y = painrelief)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/migraine-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThe medians of drugs B and C are actually exactly the same. Because\nthe pain relief values are all whole numbers (and there are only 9 in\neach group), you get that thing where enough of them are equal that\nthe median and third quartiles are equal, actually for two of the three\ngroups. \n\nDespite the weird distributions, I'm willing to call these groups sufficiently\nsymmetric for the ANOVA to be OK, but I didn't ask you to draw the\nboxplot, because I didn't want to confuse the issue with this. The\npoint of this question was to get the data tidy enough to do an\nanalysis.\n\nAs I said, I didn't want you to have to get into this, but if you are\nworried, you know what the remedy is --- Mood's median test. Don't\nforget to use the right data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\nmedian_test(migraine2, painrelief, drug)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$table\n       above\ngroup   above below\n  DrugA     0     8\n  DrugB     5     2\n  DrugC     6     0\n\n$test\n       what        value\n1 statistic 1.527273e+01\n2        df 2.000000e+00\n3   P-value 4.825801e-04\n```\n:::\n:::\n\n \n\nBecause the pain relief scores are integers, there are probably a lot\nof them equal to the overall median. There were 27 observations\naltogether, but Mood's median test will discard any that are equal to\nthis value. There must have been 9 observations in each group to start\nwith, but if you look at each row of the table, there are only 8\nobservations listed for drug A, 7 for drug B and 6 for drug C, so\nthere must have been 1, 2 and 3 (totalling 6) observations equal to\nthe median that were discarded.\n\nThe P-value is a little bigger than came out of the $F$-test, but the\nconclusion is still that there are definitely differences among the\ndrugs in terms of pain relief. The table at the top of the output\nagain suggests that drug A is worse than the others, but to confirm\nthat you'd have to do Mood's median test on all three *pairs* of\ndrugs, and then use Bonferroni to allow for your having done three tests:\n\n::: {.cell}\n\n```{.r .cell-code}\npairwise_median_test(migraine2, painrelief, drug)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 4\n  g1    g2     p_value adj_p_value\n  <chr> <chr>    <dbl>       <dbl>\n1 DrugA DrugB 0.00721     0.0216  \n2 DrugA DrugC 0.000183    0.000548\n3 DrugB DrugC 0.921       1       \n```\n:::\n:::\n\n \n\nDrug A gives worse pain relief (fewer hours) than both drugs B and C,\nwhich are not significantly different from each hour. This is exactly\nwhat you would have guessed from the boxplot.\n\nI adjusted the P-values as per Bonferroni by multiplying them by 3 (so that I could still compare with 0.05), but it makes no sense to have a P-value, which is a probability, greater than 1, so an \"adjusted P-value\" that comes out greater than 1 is rounded back down to 1. You interpret this as being \"no evidence at all of a difference in medians\" between drugs B and C.\n\n$\\blacksquare$\n\n\n\n\n\n##  Location, species and disease in plants\n\n\n The table below is a \"contingency table\", showing\nfrequencies of diseased and undiseased plants of two different species\nin two different locations:\n\n\n```\n\nSpecies     Disease present         Disease absent\n          Location X Location Y  Location X Location Y\nA            44         12          38        10\nB            28         22          20        18\n\n```\n\n\nThe data were saved as\n[link](http://ritsokiguess.site/datafiles/disease.txt). In that\nfile, the columns are coded by two letters: a `p` or an\n`a` to denote presence or absence of disease, and an `x`\nor a `y` to denote location X or Y. The data are separated by\nmultiple spaces and aligned with the variable names. \n\n\n\n(a) Read in and display the data.\n\n\nSolution\n\n\n`read_table` again. You know this because, when you looked\nat the data file, which of course you did (didn't you?), you saw\nthat the data values were aligned by columns with multiple spaces\nbetween them:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/disease.txt\"\ntbl <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n-- Column specification --------------------------------------------------------\ncols(\n  Species = col_character(),\n  px = col_double(),\n  py = col_double(),\n  ax = col_double(),\n  ay = col_double()\n)\n```\n:::\n\n```{.r .cell-code}\ntbl\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 5\n  Species    px    py    ax    ay\n  <chr>   <dbl> <dbl> <dbl> <dbl>\n1 A          44    12    38    10\n2 B          28    22    20    18\n```\n:::\n:::\n\n \n\nI was thinking ahead, since I'll be wanting to have one of my columns\ncalled `disease`, so I'm *not* calling the data frame\n`disease`. \n\nYou'll also have noticed that I simplified the data frame that I had\nyou read in, because the original contingency table I showed you has\n*two* header rows, and we have to have *one* header row. So\nI mixed up the information in the two header rows into one.\n\n\n$\\blacksquare$\n\n(b) Explain briefly how these data are not \"tidy\".\n\n\nSolution\n\n\nThe simple answer is that there are 8 frequencies, that each ought\nto be in a row by themselves. Or, if you like, there are three\nvariables, Species, Disease status and Location, and each of\n*those* should be in a *column* of its own. \nEither one\nof these ideas, or something like it, is good. I need you to\ndemonstrate that you know something about \"tidy data\" in this context.\n\n\n$\\blacksquare$\n\n(c) Use a suitable `tidyr` tool to get all the things\nthat are the same into a single column. (You'll need to make up a\ntemporary name for the other new column that you create.) Show your\nresult. \n\n\nSolution\n\n\n`pivot_longer` is the tool. All the columns apart from\n`Species` contain frequencies. \nThey are frequencies in disease-location combinations, so\nI'll call the column of \"names\" `disloc`. Feel\nfree to call it `temp` for now if you prefer:\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl %>% pivot_longer(-Species, names_to=\"disloc\", values_to = \"frequency\") -> tbl.2\ntbl.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 3\n  Species disloc frequency\n  <chr>   <chr>      <dbl>\n1 A       px            44\n2 A       py            12\n3 A       ax            38\n4 A       ay            10\n5 B       px            28\n6 B       py            22\n7 B       ax            20\n8 B       ay            18\n```\n:::\n:::\n\n\n\n\n$\\blacksquare$\n\n(d) Explain briefly how the data frame you just created is\nstill not \"tidy\" yet.\n\n\nSolution\n\n\nThe column I called `disloc` actually contains *two*\nvariables, disease and location, which need to be split up. A\ncheck on this is that we \nhave two columns (not including the frequencies), but back in\n(b) we found *three* variables, so there\nought to be three non-frequency columns.\n\n\n$\\blacksquare$\n\n(e) Use one more `tidyr` tool to make these data tidy,\nand show your result.\n\n\nSolution\n\n\nThis means splitting up `disloc` into two separate columns,\nsplitting after the first character, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\n(tbl.2 %>% separate(disloc, c(\"disease\", \"location\"), 1) -> tbl.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 4\n  Species disease location frequency\n  <chr>   <chr>   <chr>        <dbl>\n1 A       p       x               44\n2 A       p       y               12\n3 A       a       x               38\n4 A       a       y               10\n5 B       p       x               28\n6 B       p       y               22\n7 B       a       x               20\n8 B       a       y               18\n```\n:::\n:::\n\n \n\nThis is now tidy: eight frequencies in rows, and three non-frequency\ncolumns. (Go back and look at your answer to part (b)\nand note that the issues you found there have all been resolved now.)\n\nExtra: my reading of one of the vignettes (the one called `pivot`) for `tidyr` suggests that `pivot_longer` can do both the making longer and the separating in one shot:\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl %>% pivot_longer(-Species, names_to=c(\"disease\", \"location\"), names_sep=1, values_to=\"frequency\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 4\n  Species disease location frequency\n  <chr>   <chr>   <chr>        <dbl>\n1 A       p       x               44\n2 A       p       y               12\n3 A       a       x               38\n4 A       a       y               10\n5 B       p       x               28\n6 B       p       y               22\n7 B       a       x               20\n8 B       a       y               18\n```\n:::\n:::\n\nAnd I (amazingly) got that right first time!\n\nThe idea is that you recognize that the column names are actually two things: a disease status and a location. To get `pivot_longer` to recognize that, you put two things in the `names_to`. Then you have to say how the two things in the columns are separated: this might be by an underscore or a dot, or, as here, \"after the first character\" (just as in `separate`). Using two names and some indication of what separates them then does a combined pivot-longer-and-separate, all in one shot.\n\nThe more I use `pivot_longer`, the more I marvel at the excellence of its design: it seems to be easy to guess how to make things work.\n\n\n$\\blacksquare$\n\n(f) Let's see if we can re-construct the original contingency\ntable (or something equivalent to it). Use the function\n`xtabs`. This requires first a model formula with the frequency\nvariable on the left of the squiggle, and the other variables\nseparated by plus signs on the right. Second it requires a data\nframe, with `data=`. Feed\nyour data frame from the previous part into `xtabs`. Save the\nresult in a variable and display the result.\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl.4 <- xtabs(frequency ~ Species + disease + location, data = tbl.3)\ntbl.4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n, , location = x\n\n       disease\nSpecies  a  p\n      A 38 44\n      B 20 28\n\n, , location = y\n\n       disease\nSpecies  a  p\n      A 10 12\n      B 18 22\n```\n:::\n:::\n\n \n\nThis shows a pair of contingency tables, one each for each of the two\nlocations (in general, the variable you put last on the right side of\nthe model formula). You can check that everything corresponds with the\noriginal data layout at the beginning of the question, possibly with\nsome things rearranged (but with the same frequencies in the same\nplaces). \n\n\n$\\blacksquare$\n\n(g) Take the output from the last part and feed it into the\nfunction `ftable`. How has the output been changed? Which do\nyou like better? Explain briefly.\n\n\nSolution\n\n\nThis:\n\n::: {.cell}\n\n```{.r .cell-code}\nftable(tbl.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                location  x  y\nSpecies disease               \nA       a                38 10\n        p                44 12\nB       a                20 18\n        p                28 22\n```\n:::\n:::\n\n \n\nThis is the same output, but shown more compactly. (Rather like a\nvertical version of the original data, in fact.) I like\n`ftable` better because it displays the data in the smallest\namount of space, though I'm fine if you prefer the `xtabs`\noutput because it spreads things out more. This is a matter of\ntaste. Pick one and tell me why you prefer it, and I'm good.\n\nThat's the end of what you had to do, but I thought I would do some\nmodelling and try to find out what's associated with disease. The\nappropriate modelling with frequencies is called \"log-linear modelling\", \nand it assumes that the log of the frequencies has a\nlinear relationship with the effects of the other variables. This is\nnot quite as simple as the log transformations we had before, because\nbigger frequencies are going to be more variable, so we fit a\ngeneralized linear model with a Poisson-distributed response and log\nlink. (It's better if you know what that means, but you ought to be\nable to follow the logic if you don't. [Chapter 29](http://ritsokiguess.site/pasias/frequency-table-analysis.html#frequency-table-analysis) has more on this.)\n\nFirst, fit a model predicting frequency from everything, including all\nthe interactions. (The reason for doing it this way will become clear later):\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.1 <- glm(frequency ~ Species * location * disease, data = tbl.3, family = \"poisson\")\ndrop1(model.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nfrequency ~ Species * location * disease\n                         Df Deviance    AIC      LRT Pr(>Chi)\n<none>                      0.000000 55.291                  \nSpecies:location:disease  1 0.070257 53.362 0.070257    0.791\n```\n:::\n:::\n\n \n\nThe residuals are all zero because this model fits perfectly. The\nproblem is that it is very complicated, so it offers no insight. So\nwhat we do is to look at the highest-order interaction\n`Species:location:disease` and see whether it is\nsignificant. It is not, so we can remove it. This is reminiscent of\nvariable selection in regression, where we pull the least significant\nthing out of the model in turn until we can go no further. But here,\nwe have additional things to think about: we have to get rid of all\nthe three-way interactions before we can tackle the two-way ones, and\nall the two-way ones before we can tackle the main effects. There is a\nso-called \"nested\" structure happening here that says you don't look\nat, say, `Species`, until you have removed *all* the\nhigher-order interactions involving `Species`. Not clear yet?\nDon't fret. `drop1` allows you to assess what is currently up\nfor grabs (here, only the three-way interaction, which is not\nsignificant, so out it comes).\n\nLet's get rid of that three-way interaction. This is another use for\n`update` that you might have seen in connection with multiple regression\n(to make small changes to a big model):\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.2 <- update(model.1, . ~ . - Species:location:disease)\ndrop1(model.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nfrequency ~ Species + location + disease + Species:location + \n    Species:disease + location:disease\n                 Df Deviance    AIC     LRT  Pr(>Chi)    \n<none>                0.0703 53.362                      \nSpecies:location  1  13.0627 64.354 12.9924 0.0003128 ***\nSpecies:disease   1   0.2696 51.561  0.1993 0.6552865    \nlocation:disease  1   0.1043 51.396  0.0340 0.8536877    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nNotice how `update` saved us having to write the whole model\nout again.\n\nNow the three two-way interactions are up for grabs:\n`Species:location`, `Species:disease` and\n`location:disease`. The last of these is the least significant,\nso out it comes. I did some copying and pasting, but I had to remember\nwhich model I was working with and what I was removing:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.3 <- update(model.2, . ~ . - location:disease)\ndrop1(model.3, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nfrequency ~ Species + location + disease + Species:location + \n    Species:disease\n                 Df Deviance    AIC     LRT  Pr(>Chi)    \n<none>                0.1043 51.396                      \nSpecies:location  1  13.0678 62.359 12.9635 0.0003176 ***\nSpecies:disease   1   0.2746 49.566  0.1703 0.6798021    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\n`Species:disease` comes out, but it looks as if\n`Species:location` will have to stay:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.4 <- update(model.3, . ~ . - Species:disease)\ndrop1(model.4, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nfrequency ~ Species + location + disease + Species:location\n                 Df Deviance    AIC     LRT  Pr(>Chi)    \n<none>                0.2746 49.566                      \ndisease           1   2.3617 49.653  2.0871 0.1485461    \nSpecies:location  1  13.2381 60.530 12.9635 0.0003176 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\n`Species:location` indeed stays. That means that anything\n\"contained in\" it also has to stay, regardless of its main\neffect. So the only candidate for removal now is `disease`: not\nsignificant, out it comes:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.5 <- update(model.4, . ~ . - disease)\ndrop1(model.5, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nfrequency ~ Species + location + Species:location\n                 Df Deviance    AIC    LRT  Pr(>Chi)    \n<none>                2.3617 49.653                     \nSpecies:location  1  15.3252 60.617 12.963 0.0003176 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nAnd now we have to stop.\n\nWhat does this final model mean? Well, frequency depends significantly\non the `Species:location` combination, but not on anything\nelse. To see how, we make a contingency table of species by location\n(totalling up over disease status, since that is not significant):\n\n::: {.cell}\n\n```{.r .cell-code}\nxtabs(frequency ~ Species + location, data = tbl.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       location\nSpecies  x  y\n      A 82 22\n      B 48 40\n```\n:::\n:::\n\n \n\nMost of the species A's are at location X, but the species B's are\nabout evenly divided between the two locations. Or, if you prefer\n(equally good): location X has mostly species A, while location Y has\nmostly species B. You can condition on either variable and compare the\nconditional distribution of the other one.\n\nNow, this is rather interesting, because this began as a study of\ndisease, but disease has completely disappeared from our final model!\nThat means that nothing in our final model has any relationship with\ndisease. Indeed, if you check the original table, you'll find that\ndisease is present slightly more than it's absent, for all\ncombinations of species and location. That is, neither species nor\nlocation has any particular association with (effect on) disease,\nsince disease prevalence doesn't change appreciably if you change\nlocation, species or the combination of them.\n\nThe way an association with disease would show up is if a\n`disease:`something interaction had been significant and had\nstayed in the model, that something would have been associated with\ndisease. For example, if the `disease:Species` table had looked\nlike this:\n\n::: {.cell}\n\n```{.r .cell-code}\ndisease <- c(\"a\", \"a\", \"p\", \"p\")\nSpecies <- c(\"A\", \"B\", \"A\", \"B\")\nfrequency <- c(10, 50, 30, 30)\nxx <- tibble(disease, Species, frequency)\nxtabs(frequency ~ disease + Species, data=xx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Species\ndisease  A  B\n      a 10 50\n      p 30 30\n```\n:::\n:::\n\n \n\nFor species A, disease is present 75\\% of the time, but for species B\nit's present less than 40\\% of the time. So in this one there ought to be a\nsignificant association between disease and species:\n\n::: {.cell}\n\n```{.r .cell-code}\nxx.1 <- glm(frequency ~ disease * Species, data = xx, family = \"poisson\")\ndrop1(xx.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nfrequency ~ disease * Species\n                Df Deviance    AIC    LRT  Pr(>Chi)    \n<none>                0.000 28.400                     \ndisease:Species  1   15.518 41.918 15.518 8.171e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nAnd so there is. Nothing can come out of the model. (This is the same\nkind of test as a chi-squared test for association.  \nThe log-linear model is a multi-variable generalization of that.)\n\n$\\blacksquare$\n\n\n\n\n\n\n##  Mating songs in crickets\n\n\n Male tree crickets produce \"mating songs\" by rubbing their\nwings together to produce a chirping sound. It is hypothesized that\nfemale tree crickets identify males of the correct species by how fast\n(in chirps per second) the male's mating song is. This is called the\n\"pulse rate\".  Some data for two species of crickets are in\n[link](http://ritsokiguess.site/datafiles/crickets.txt). The\ncolumns, which are unlabelled, are temperature and pulse rate\n(respectively) for *Oecanthus exclamationis* (first two\ncolumns) and *Oecanthus niveus* (third and fourth columns). The\ncolumns are separated by tabs. There are some missing values in the\nfirst two columns because fewer *exclamationis* crickets than\n*niveus* crickets were measured.\nThe research question is whether males\nof the different species have different average pulse rates. It is\nalso of interest to see whether temperature has an effect, and if\nso, what.\nBefore we get to that, however, we have some data organization to do.\n\n\n(a) Read in the data, allowing for the fact that you have no\ncolumn names. You'll see that the\ncolumns have names `X1` through `X4`. This is\nOK.\n\n\nSolution\n\n\nTab-separated, so `read_tsv`; no column names, so `col_names=FALSE`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/crickets.txt\"\ncrickets <- read_tsv(my_url, col_names = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 17 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\ndbl (4): X1, X2, X3, X4\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ncrickets\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 17 x 4\n      X1    X2    X3    X4\n   <dbl> <dbl> <dbl> <dbl>\n 1  20.8  67.9  17.2  44.3\n 2  20.8  65.1  18.3  47.2\n 3  24    77.3  18.3  47.6\n 4  24    78.7  18.3  49.6\n 5  24    79.4  18.9  50.3\n 6  24    80.4  18.9  51.8\n 7  26.2  85.8  20.4  60  \n 8  26.2  86.6  21    58.5\n 9  26.2  87.5  21    58.9\n10  26.2  89.1  22.1  60.7\n11  28.4  98.6  23.5  69.8\n12  29   101.   24.2  70.9\n13  30.4  99.3  25.9  76.2\n14  30.4 102.   26.5  76.1\n15  NA    NA    26.5  77  \n16  NA    NA    26.5  77.7\n17  NA    NA    28.6  84.7\n```\n:::\n:::\n\n \n\nAs promised.\n\nIf you didn't catch the tab-separated part, this probably happened to you:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- read_delim(my_url, \" \", col_names = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 17 Columns: 1\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (1): X1\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n \n\nThis doesn't look good:\n\n::: {.cell}\n\n```{.r .cell-code}\nproblems(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 5\n    row   col expected  actual    file \n  <int> <int> <chr>     <chr>     <chr>\n1    15     2 1 columns 2 columns \"\"   \n2    16     2 1 columns 2 columns \"\"   \n3    17     2 1 columns 2 columns \"\"   \n```\n:::\n:::\n\n \n\nThe \"expected columns\" being 1 should bother you, since we know\nthere are supposed to be 4 columns. At this point, we take a look at\nwhat got read in:\n\n::: {.cell}\n\n```{.r .cell-code}\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 17 x 1\n   X1                       \n   <chr>                    \n 1 \"20.8\\t67.9\\t17.2\\t44.3\" \n 2 \"20.8\\t65.1\\t18.3\\t47.2\" \n 3 \"24.0\\t77.3\\t18.3\\t47.6\" \n 4 \"24.0\\t78.7\\t18.3\\t49.6\" \n 5 \"24.0\\t79.4\\t18.9\\t50.3\" \n 6 \"24.0\\t80.4\\t18.9\\t51.8\" \n 7 \"26.2\\t85.8\\t20.4\\t60.0\" \n 8 \"26.2\\t86.6\\t21.0\\t58.5\" \n 9 \"26.2\\t87.5\\t21.0\\t58.9\" \n10 \"26.2\\t89.1\\t22.1\\t60.7\" \n11 \"28.4\\t98.6\\t23.5\\t69.8\" \n12 \"29.0\\t100.8\\t24.2\\t70.9\"\n13 \"30.4\\t99.3\\t25.9\\t76.2\" \n14 \"30.4\\t101.7\\t26.5\\t76.1\"\n15 \"NA\\tNA \\t26.5\\t77.0\"    \n16 \"NA\\tNA \\t26.5\\t77.7\"    \n17 \"NA\\tNA \\t28.6\\t84.7\"    \n```\n:::\n:::\n\n\n\nand there you see the `\\t` or \"tab\" characters separating the\nvalues, instead of spaces. (This is what I tried first, and once I\nlooked at this, I realized that `read_tsv` was what I needed.)\n\n$\\blacksquare$\n\n(b) Tidy these untidy data, going as directly as you can to something tidy. (Some later parts show you how it used to be done.) Begin by: (i) adding a column of row numbers, (ii) `rename`-ing the columns to species name, an underscore, and the variable contents (keeping `pulserate` as one word), and then use `pivot_longer`. Note that the column names encode *two* things.\n\n\n\nSolution\n\n\n\nTake this one piece of the pipeline at a time: that is, first check that you got the renaming right and looking at what you have, before proceeding to the `pivot_longer`. The syntax of `rename` is new name equals old name, and I like to split this over several lines to make it easier to read:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrickets %>% \n  mutate(row=row_number()) %>% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 17 x 5\n   exclamationis_temperature exclamationis_pulserate niveus_temperature\n                       <dbl>                   <dbl>              <dbl>\n 1                      20.8                    67.9               17.2\n 2                      20.8                    65.1               18.3\n 3                      24                      77.3               18.3\n 4                      24                      78.7               18.3\n 5                      24                      79.4               18.9\n 6                      24                      80.4               18.9\n 7                      26.2                    85.8               20.4\n 8                      26.2                    86.6               21  \n 9                      26.2                    87.5               21  \n10                      26.2                    89.1               22.1\n11                      28.4                    98.6               23.5\n12                      29                     101.                24.2\n13                      30.4                    99.3               25.9\n14                      30.4                   102.                26.5\n15                      NA                      NA                 26.5\n16                      NA                      NA                 26.5\n17                      NA                      NA                 28.6\n# i 2 more variables: niveus_pulserate <dbl>, row <int>\n```\n:::\n:::\n\nThe first part of each column name is the species and the second part is what was measured each time, separated by an underscore. To handle that in `pivot_longer`, you give *two* names of new columns to create (in `names_to`), and say what they're separated by:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrickets %>% \n  mutate(row=row_number()) %>% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) %>% \n  pivot_longer(-row, names_to=c(\"species\", \"measurement\"), names_sep=\"_\", values_to = \"obs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 68 x 4\n     row species       measurement   obs\n   <int> <chr>         <chr>       <dbl>\n 1     1 exclamationis temperature  20.8\n 2     1 exclamationis pulserate    67.9\n 3     1 niveus        temperature  17.2\n 4     1 niveus        pulserate    44.3\n 5     2 exclamationis temperature  20.8\n 6     2 exclamationis pulserate    65.1\n 7     2 niveus        temperature  18.3\n 8     2 niveus        pulserate    47.2\n 9     3 exclamationis temperature  24  \n10     3 exclamationis pulserate    77.3\n# i 58 more rows\n```\n:::\n:::\n\nThis is tidy now, but we went a step too far: that column `measurement` should be *two* columns, called `temperature` and `pulserate`, which means it should be made wider. The obvious way is this:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrickets %>% \n  mutate(row=row_number()) %>% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) %>% \n  pivot_longer(-row, names_to=c(\"species\", \"measurement\"), names_sep=\"_\", values_to = \"obs\") %>% \n  pivot_wider(names_from=measurement, values_from=obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 34 x 4\n     row species       temperature pulserate\n   <int> <chr>               <dbl>     <dbl>\n 1     1 exclamationis        20.8      67.9\n 2     1 niveus               17.2      44.3\n 3     2 exclamationis        20.8      65.1\n 4     2 niveus               18.3      47.2\n 5     3 exclamationis        24        77.3\n 6     3 niveus               18.3      47.6\n 7     4 exclamationis        24        78.7\n 8     4 niveus               18.3      49.6\n 9     5 exclamationis        24        79.4\n10     5 niveus               18.9      50.3\n# i 24 more rows\n```\n:::\n:::\n\n\nThe row numbers are cricket-within-species, which isn't very meaningful, but we needed something for the `pivot_wider` to key on, to recognize what needed to go in which row. The way it works is it uses anything not mentioned in `names_from` or `values_from` as a \"key\": each unique combination belongs in a row. Here that would be the combination of row and species, which is a good key because each species appears once with each row number. \n\nThis works, but a better way is to recognize that one of the variants of `pivot_longer` will do this all at once (something to think about when you have a longer followed by a wider). The key is that temperature and pulse rate need to be column names, so the second thing in `names_to` has to be that special thing `.value`, and you remove the `values_to` since it is now clear where the values are coming from:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrickets %>% \n  mutate(row=row_number()) %>% \n  rename(\n    exclamationis_temperature = X1,\n    exclamationis_pulserate = X2,\n    niveus_temperature = X3,\n    niveus_pulserate = X4\n  ) %>% \n  pivot_longer(-row, names_to=c(\"species\", \".value\"), names_sep=\"_\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 34 x 4\n     row species       temperature pulserate\n   <int> <chr>               <dbl>     <dbl>\n 1     1 exclamationis        20.8      67.9\n 2     1 niveus               17.2      44.3\n 3     2 exclamationis        20.8      65.1\n 4     2 niveus               18.3      47.2\n 5     3 exclamationis        24        77.3\n 6     3 niveus               18.3      47.6\n 7     4 exclamationis        24        78.7\n 8     4 niveus               18.3      49.6\n 9     5 exclamationis        24        79.4\n10     5 niveus               18.9      50.3\n# i 24 more rows\n```\n:::\n:::\n\n\n$\\blacksquare$\n\n(c) If you found (b) a bit much to take in, the rest of the way we take a rather more leisurely approach towards the tidying. \n\nThese data are rather far from being tidy. There need to be\nthree variables, temperature, pulse rate and species, and there\nare $14+17=31$ observations altogether. This one is tricky in that\nthere are temperature and pulse rate for each of two levels of a\nfactor, so I'll suggest combining the temperature and chirp rate\ntogether into one thing for each species, then pivoting them longer (\"combining\"),\nthen pivoting them wider again (\"splitting\"). Create new columns, named for  each species,\nthat contain the temperature and pulse rate for that species in\nthat order, `unite`d together.\nFor the rest of this question, start from the data frame you read\nin, and build a pipe, one or two steps at a time, to save creating\na lot of temporary data frames.\n\n\nSolution\n\n\nBreathe, and then begin. `unite` creates new columns by\njoining together old ones:^[As `str_c` or `paste` do, actually, but the advantage of `unite` is that it gets rid of the other columns, which you probably no longer need.]\n\n::: {.cell}\n\n```{.r .cell-code}\ncrickets %>%\n  unite(exclamationis, X1:X2) %>%\n  unite(niveus, X3:X4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 17 x 2\n   exclamationis niveus   \n   <chr>         <chr>    \n 1 20.8_67.9     17.2_44.3\n 2 20.8_65.1     18.3_47.2\n 3 24_77.3       18.3_47.6\n 4 24_78.7       18.3_49.6\n 5 24_79.4       18.9_50.3\n 6 24_80.4       18.9_51.8\n 7 26.2_85.8     20.4_60  \n 8 26.2_86.6     21_58.5  \n 9 26.2_87.5     21_58.9  \n10 26.2_89.1     22.1_60.7\n11 28.4_98.6     23.5_69.8\n12 29_100.8      24.2_70.9\n13 30.4_99.3     25.9_76.2\n14 30.4_101.7    26.5_76.1\n15 NA_NA         26.5_77  \n16 NA_NA         26.5_77.7\n17 NA_NA         28.6_84.7\n```\n:::\n:::\n\n \n\nNote that the original columns `X1:X4` are *gone*, which\nis fine, because the information we needed from them is contained in\nthe two new columns. `unite` by default uses an underscore to\nseparate the joined-together values, which is generally safe since you\nwon't often find those in data.\n\nDigression: `unite`-ing with a space could cause problems if\nthe data values have spaces in them already. Consider this list of names:\n\n::: {.cell}\n\n```{.r .cell-code}\nnames <- c(\"Cameron McDonald\", \"Durwin Yang\", \"Ole Gunnar Solskjaer\", \"Mahmudullah\")\n```\n:::\n\n \n\nTwo very former students of mine, a Norwegian soccer player, and a\nBangladeshi cricketer. Only one of these has played for Manchester United:\n\n::: {.cell}\n\n```{.r .cell-code}\nmanu <- c(F, F, T, F)\n```\n:::\n\n \n\nand let's make a data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tibble(name = names, manu = manu)\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  name                 manu \n  <chr>                <lgl>\n1 Cameron McDonald     FALSE\n2 Durwin Yang          FALSE\n3 Ole Gunnar Solskjaer TRUE \n4 Mahmudullah          FALSE\n```\n:::\n:::\n\n\n\nNow, what happens if we `unite` those columns, separating them\nby a space?\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% unite(joined, name:manu, sep = \" \")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 1\n  joined                   \n  <chr>                    \n1 Cameron McDonald FALSE   \n2 Durwin Yang FALSE        \n3 Ole Gunnar Solskjaer TRUE\n4 Mahmudullah FALSE        \n```\n:::\n:::\n\n \n\nIf we then try to separate them again, what happens?\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  unite(joined, name:manu, sep = \" \") %>%\n  separate(joined, c(\"one\", \"two\"), \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Expected 2 pieces. Additional pieces discarded in 3 rows [1, 2, 3].\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  one         two     \n  <chr>       <chr>   \n1 Cameron     McDonald\n2 Durwin      Yang    \n3 Ole         Gunnar  \n4 Mahmudullah FALSE   \n```\n:::\n:::\n\n \n\nThings have gotten lost: most of the original values of `manu`\nand some of the names. If we use a different separator character,\neither choosing one deliberately or going with the default underscore,\neverything works swimmingly:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  unite(joined, name:manu, sep = \":\") %>%\n  separate(joined, c(\"one\", \"two\"), \":\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  one                  two  \n  <chr>                <chr>\n1 Cameron McDonald     FALSE\n2 Durwin Yang          FALSE\n3 Ole Gunnar Solskjaer TRUE \n4 Mahmudullah          FALSE\n```\n:::\n:::\n\n \n\nand we are back to where we started.\n\nIf you run just the `unite` line (move the pipe symbol to the\nnext line so that the `unite` line is complete as it stands),\nyou'll see what happened.\n      \n$\\blacksquare$\n\n(d) The two columns `exclamationis` and `niveus`\nthat you just created are both temperature-pulse rate combos, but\nfor different species. Collect them together into one\ncolumn, labelled by species. (This is a straight `tidyr` `pivot_longer`, even though the columns contain something odd-looking.)\n\n\nSolution\n\n\nThus, this, naming the new column `temp_pulse` since it\ncontains both of those things. Add to the end of the pipe you\nstarted building in the previous part:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrickets %>%\n  unite(exclamationis, X1:X2) %>%\n  unite(niveus, X3:X4) %>%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", values_to = \"temp_pulse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 34 x 2\n   species       temp_pulse\n   <chr>         <chr>     \n 1 exclamationis 20.8_67.9 \n 2 niveus        17.2_44.3 \n 3 exclamationis 20.8_65.1 \n 4 niveus        18.3_47.2 \n 5 exclamationis 24_77.3   \n 6 niveus        18.3_47.6 \n 7 exclamationis 24_78.7   \n 8 niveus        18.3_49.6 \n 9 exclamationis 24_79.4   \n10 niveus        18.9_50.3 \n# i 24 more rows\n```\n:::\n:::\n\n \n\nYep. You'll see both species of\ncrickets, and you'll see some missing values at the bottom, labelled,\nat the moment, `NA_NA`. \n\nThis is going to get rather long, but don't fret: we debugged the two\n`unite` lines before, so if you get any errors, they must\nhave come from the `pivot_longer`. So that would be the place to check.\n\n$\\blacksquare$\n\n(e) Now split up the temperature-pulse combos at the underscore, into\ntwo separate columns. This is `separate`. When specifying\nwhat to separate by, you can use a number (\"split after this many characters\") or a piece of text, in quotes (\"when you see this text, split at it\"). \n\n\nSolution\n\n\nThe text to split by is an underscore (in quotes), since\n`unite` by default puts an underscore in between the\nvalues it pastes together. Glue the `separate` onto the\nend. We are creating two new variables `temperature` and\n`pulse_rate`:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrickets %>%\n  unite(exclamationis, X1:X2) %>%\n  unite(niveus, X3:X4) %>%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", values_to = \"temp_pulse\") %>% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 34 x 3\n   species       temperature pulse_rate\n   <chr>         <chr>       <chr>     \n 1 exclamationis 20.8        67.9      \n 2 niveus        17.2        44.3      \n 3 exclamationis 20.8        65.1      \n 4 niveus        18.3        47.2      \n 5 exclamationis 24          77.3      \n 6 niveus        18.3        47.6      \n 7 exclamationis 24          78.7      \n 8 niveus        18.3        49.6      \n 9 exclamationis 24          79.4      \n10 niveus        18.9        50.3      \n# i 24 more rows\n```\n:::\n:::\n\n \n\nYou'll note that `unite` and `separate` are opposites (\"inverses\") of each other, but we haven't just done something and then undone it, because we have a `pivot_longer` in between; in fact, arranging it this way has done precisely the tidying we wanted.\n      \n$\\blacksquare$\n\n(f) Almost there.  Temperature and pulse rate are still text\n(because `unite` turned them into text), but they should be\nnumbers. Create new variables that are numerical versions of\ntemperature and pulse rate (using `as.numeric`). Check that\nyou have no extraneous variables (and, if necessary, get rid of\nthe ones you don't want). (Species is also text and really ought\nto be a factor, but having it as text doesn't seem to cause any\nproblems.)\nYou can, if you like, use `parse_number` instead of\n`as.numeric`. They should both work. The distinction I\nprefer to make is that `parse_number` is good for text\nwith a number in it (that we want to pull the number out of),\nwhile `as.numeric` is for turning something that looks like\na number but isn't one into a genuine number.^[You could      just as well make the point that the text 20.8 contains the      number 20.8 and nothing else, so that parsing it as text in search of a number      will pull out 20.8 as a number. If that logic works for you, go      with it.]\n\n\nSolution\n\n\n`mutate`-ing into a column that already exists overwrites\nthe variable that's already there (which saves us some effort\nhere). \n\n::: {.cell}\n\n```{.r .cell-code}\ncrickets %>%\n  unite(exclamationis, X1:X2) %>%\n  unite(niveus, X3:X4) %>%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", values_to = \"temp_pulse\") %>% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\") %>%\n  mutate(temperature = as.numeric(temperature)) %>%\n  mutate(pulse_rate = as.numeric(pulse_rate)) -> crickets.1\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There was 1 warning in `mutate()`.\ni In argument: `temperature = as.numeric(temperature)`.\nCaused by warning:\n! NAs introduced by coercion\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There was 1 warning in `mutate()`.\ni In argument: `pulse_rate = as.numeric(pulse_rate)`.\nCaused by warning:\n! NAs introduced by coercion\n```\n:::\n\n```{.r .cell-code}\ncrickets.1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 34 x 3\n   species       temperature pulse_rate\n   <chr>               <dbl>      <dbl>\n 1 exclamationis        20.8       67.9\n 2 niveus               17.2       44.3\n 3 exclamationis        20.8       65.1\n 4 niveus               18.3       47.2\n 5 exclamationis        24         77.3\n 6 niveus               18.3       47.6\n 7 exclamationis        24         78.7\n 8 niveus               18.3       49.6\n 9 exclamationis        24         79.4\n10 niveus               18.9       50.3\n# i 24 more rows\n```\n:::\n:::\n\n \n\nI saved the data frame this time, since this is the one we will use\nfor our analysis.\n\nThe warning message tells us that we got genuine missing-value NAs\nback, which is probably what we want. Specifically, they got turned\nfrom missing *text* to missing *numbers*!^[You might think that  missing is just missing, but R distinguishes between types of missing.]\nThe R word\n\"coercion\" means values being changed from one type of thing to\nanother type of thing.  (We'll ignore the missings and see if they\ncause us any trouble. The same warning messages will show up on graphs\nlater.)  So I have 34 rows (including three rows of missings) instead\nof the 31 rows I would have liked. Otherwise, success.\n\nThere is (inevitably) another way to do this. We are doing the\n`as.numeric` twice, exactly the same on two different columns,\nand when you are doing the same thing on a number of columns, here a\n`mutate` with the same function, you have the option of using\n`across`. This is the same idea \nthat we used way\nback to compute numerical summaries of a bunch of columns:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrickets %>%\n  unite(exclamationis, X1:X2) %>%\n  unite(niveus, X3:X4) %>%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", \n               values_to = \"temp_pulse\") %>% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\") %>%\n  mutate(across(c(temperature, pulse_rate), \\(x) as.numeric(x)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\ni In argument: `across(c(temperature, pulse_rate), function(x) as.numeric(x))`.\nCaused by warning:\n! NAs introduced by coercion\ni Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 34 x 3\n   species       temperature pulse_rate\n   <chr>               <dbl>      <dbl>\n 1 exclamationis        20.8       67.9\n 2 niveus               17.2       44.3\n 3 exclamationis        20.8       65.1\n 4 niveus               18.3       47.2\n 5 exclamationis        24         77.3\n 6 niveus               18.3       47.6\n 7 exclamationis        24         78.7\n 8 niveus               18.3       49.6\n 9 exclamationis        24         79.4\n10 niveus               18.9       50.3\n# i 24 more rows\n```\n:::\n:::\n\n \n\nCan't I just say that these are columns 2 and 3?\n\n::: {.cell}\n\n```{.r .cell-code}\ncrickets %>%\n  unite(exclamationis, X1:X2) %>%\n  unite(niveus, X3:X4) %>%\n  pivot_longer(exclamationis:niveus, names_to = \"species\", \n               values_to = \"temp_pulse\") %>% \n  separate(temp_pulse, c(\"temperature\", \"pulse_rate\"), \"_\") %>%\n  mutate(across(2:3, \\(x) as.numeric(x)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\ni In argument: `across(2:3, function(x) as.numeric(x))`.\nCaused by warning:\n! NAs introduced by coercion\ni Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 34 x 3\n   species       temperature pulse_rate\n   <chr>               <dbl>      <dbl>\n 1 exclamationis        20.8       67.9\n 2 niveus               17.2       44.3\n 3 exclamationis        20.8       65.1\n 4 niveus               18.3       47.2\n 5 exclamationis        24         77.3\n 6 niveus               18.3       47.6\n 7 exclamationis        24         78.7\n 8 niveus               18.3       49.6\n 9 exclamationis        24         79.4\n10 niveus               18.9       50.3\n# i 24 more rows\n```\n:::\n:::\n\n \n\nYes. Equally good. What goes into the `across`\nis the same as can go into a `select`: column numbers, names,\nor any of those \"select helpers\" like `starts_with`.\n\nYou might think of using `across` here on the quantitative columns, but remember the reason for doing this: all the columns are text, before you convert temperature and pulse rate to numbers, and so there's no way to pick out just the two columns you want that way. \n\nCheck that the temperature and pulse rate columns are now labelled\n`dbl`, which means they actually *are* decimal numbers\n(and don't just look like decimal numbers).\n\nEither way, using `unite` and then `separate` means that\nall the columns we created we want to keep (or, all the ones we would\nhave wanted to get rid of have already been gotten rid of).\n\nNow we could actually do some statistics. That we do elsewhere.\n      \n$\\blacksquare$\n\n\n\n\n\n\n## Number 1 songs\n\n\n\n The data file\n[link](http://stat405.had.co.nz/data/billboard.csv) contains a lot of\ninformation about songs popular in 2000. This dataset is untidy.  Our\nultimate aim is to answer \"which song occupied the \\#1 position for the largest number of weeks?\". To do that, we will build a pipe that\nstarts from the data frame read in from the URL above, and finishes\nwith an answer to the question. I will take you through this step by\nstep. Each part will involve adding something to the pipe you built\npreviously (possibly after removing a line or two that you used to\ndisplay the previous result).\n\n\n\n(a) Read the data and display what you have.\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbillboard <- read_csv(\"http://stat405.had.co.nz/data/billboard.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 317 Columns: 83\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr   (3): artist.inverted, track, genre\ndbl  (66): year, x1st.week, x2nd.week, x3rd.week, x4th.week, x5th.week, x6th...\nlgl  (11): x66th.week, x67th.week, x68th.week, x69th.week, x70th.week, x71st...\ndate  (2): date.entered, date.peaked\ntime  (1): time\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n       \n\nThere are a *lot* of columns. What does this look like?\n\n::: {.cell}\n\n```{.r .cell-code}\nbillboard\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 317 x 83\n    year artist.inverted    track time  genre date.entered date.peaked x1st.week\n   <dbl> <chr>              <chr> <tim> <chr> <date>       <date>          <dbl>\n 1  2000 Destiny's Child    Inde~ 03:38 Rock  2000-09-23   2000-11-18         78\n 2  2000 Santana            Mari~ 04:18 Rock  2000-02-12   2000-04-08         15\n 3  2000 Savage Garden      I Kn~ 04:07 Rock  1999-10-23   2000-01-29         71\n 4  2000 Madonna            Music 03:45 Rock  2000-08-12   2000-09-16         41\n 5  2000 Aguilera, Christi~ Come~ 03:38 Rock  2000-08-05   2000-10-14         57\n 6  2000 Janet              Does~ 04:17 Rock  2000-06-17   2000-08-26         59\n 7  2000 Destiny's Child    Say ~ 04:31 Rock  1999-12-25   2000-03-18         83\n 8  2000 Iglesias, Enrique  Be W~ 03:36 Latin 2000-04-01   2000-06-24         63\n 9  2000 Sisqo              Inco~ 03:52 Rock  2000-06-24   2000-08-12         77\n10  2000 Lonestar           Amaz~ 04:25 Coun~ 1999-06-05   2000-03-04         81\n# i 307 more rows\n# i 75 more variables: x2nd.week <dbl>, x3rd.week <dbl>, x4th.week <dbl>,\n#   x5th.week <dbl>, x6th.week <dbl>, x7th.week <dbl>, x8th.week <dbl>,\n#   x9th.week <dbl>, x10th.week <dbl>, x11th.week <dbl>, x12th.week <dbl>,\n#   x13th.week <dbl>, x14th.week <dbl>, x15th.week <dbl>, x16th.week <dbl>,\n#   x17th.week <dbl>, x18th.week <dbl>, x19th.week <dbl>, x20th.week <dbl>,\n#   x21st.week <dbl>, x22nd.week <dbl>, x23rd.week <dbl>, x24th.week <dbl>, ...\n```\n:::\n:::\n\n \n\nOn yours, you will definitely see a little arrow top right saying\n\"there are more columns\", and you will have to click on it several\ntimes to see them all. A lot of the ones on the right will be missing.\n\n \n$\\blacksquare$\n\n(b) The columns `x1st.week` through\n`x76th.week` contain the rank of each song in the Billboard\nchart in that week, with week 1 being the first week that the song\nappeared in the chart.  Convert all these columns into two: an\nindication of week, called `week`, and of rank, called\n`rank`. Most songs appeared in the Billboard chart for a\nlot less than 76 weeks, so there are missing values, which you\nwant to remove.  (I say \"indication of week\" since this will\nprobably be text at the moment). Display your new data frame. Do\nyou have fewer columns?  Why do you have a lot more rows? Explain\nbriefly.\n\n\nSolution\n\n\n\nAs is often the case, the first step is `pivot_longer`, to reduce all those columns to something easier to deal with. The columns we want to make longer are the ones ending in \"week\":\n\n::: {.cell}\n\n```{.r .cell-code}\nbillboard %>% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5,307 x 9\n    year artist.inverted track  time  genre date.entered date.peaked week   rank\n   <dbl> <chr>           <chr>  <tim> <chr> <date>       <date>      <chr> <dbl>\n 1  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x1st~    78\n 2  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x2nd~    63\n 3  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x3rd~    49\n 4  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x4th~    33\n 5  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x5th~    23\n 6  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x6th~    15\n 7  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x7th~     7\n 8  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x8th~     5\n 9  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x9th~     1\n10  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x10t~     1\n# i 5,297 more rows\n```\n:::\n:::\n\nThe \"values\" (ranks) have missings in them, which we wanted to get rid of.\n\n\n\n \n\nThere are now only 9 columns, a lot fewer than we started with. This\nis (I didn't need you to say) because we have collected together all\nthose `week` columns into one (a column called `rank`\nwith an indication of which `week` it came from). The logic of\nthe `pivot_longer` is that all those columns contain ranks (which is\nwhat make them the same), but they are ranks from different weeks\n(which is what makes them different).\n\nWhat has actually happened is that we have turned \"wide\" format into\n\"long\" format. This is not very insightful, so I would like you to\ngo a bit further in your explanation. The original data frame encodes\nthe rank of each song in each week, and what the `pivot_longer` has\ndone is to make that explicit: in the new data frame, each song's rank\nin each week appears in *one* row, so that there are as many rows\nas there are song-week combinations. The original data frame had 317\nsongs over 76 weeks, so this many:\n\n::: {.cell}\n\n```{.r .cell-code}\n317 * 76\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 24092\n```\n:::\n:::\n\n \n\nsong-week combinations.\n\nNot every song appeared in the Billboard chart for 76 weeks, so our tidy\ndata frame has a lot fewer rows than this.\n\nYou need to say that the original data frame had each song appearing\nonce (on one line), but now each song appears on multiple rows, one\nfor each week that the song was in the chart. Or something equivalent\nto that.\n\n \n\n\n\n$\\blacksquare$\n\n(c) Both your `week` and `rank` columns are\n(probably) text. Create new columns that contain just the numeric\nvalues, and display just your new columns, again adding onto the\nend of your pipe. If it so happens that `rank` is already a number, leave it as it is.\n\n \n\nSolution\n\n\nMy `rank` is already a number, so I could leave it; for later, I make a copy of it called `rakn_numbe`. The `week` has a number *in* it, which I can extract using `parse_number`:\n\n::: {.cell}\n\n```{.r .cell-code}\nbillboard %>% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %>% \n  mutate(week_number=parse_number(week),\n         rank_number=rank)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5,307 x 11\n    year artist.inverted track  time  genre date.entered date.peaked week   rank\n   <dbl> <chr>           <chr>  <tim> <chr> <date>       <date>      <chr> <dbl>\n 1  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x1st~    78\n 2  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x2nd~    63\n 3  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x3rd~    49\n 4  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x4th~    33\n 5  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x5th~    23\n 6  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x6th~    15\n 7  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x7th~     7\n 8  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x8th~     5\n 9  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x9th~     1\n10  2000 Destiny's Child Indep~ 03:38 Rock  2000-09-23   2000-11-18  x10t~     1\n# i 5,297 more rows\n# i 2 more variables: week_number <dbl>, rank_number <dbl>\n```\n:::\n:::\n\n\n\n\n\n \n$\\blacksquare$\n\n(e) The meaning of your week-number column is that it refers\nto the number of weeks *after* the song first appeared in the\nBillboard chart. That is, if a song's first appearance (in\n`date.entered)` is July 24, then week 1 is July 24, week 2\nis July 31, week 3 is August 7, and so on. Create a column\n`current` by adding the appropriate number of *days*,\nbased on your week number, to `date.entered`. Display\n`date.entered`, your week number, and `current` to\nshow that you have calculated the right thing. Note that you can\nadd a number of days onto a date and you will get another date.\n\n\nSolution\n\n\nThere is a (small) gotcha here: if you read carefully, you'll\nsee that \"week 1\" is actually \"week 0\"  in terms of the\nnumber of days to add on to `date.entered`. So you have\nto subtract one from the number of weeks before you multiply\nit by seven to get a number of days.\nAfter that thinking, this:\n\n::: {.cell}\n\n```{.r .cell-code}\nbillboard %>% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %>% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %>%\n  mutate(current = date.entered + (week_number - 1) * 7) %>%\n  select(date.entered, week_number, current)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5,307 x 3\n   date.entered week_number current   \n   <date>             <dbl> <date>    \n 1 2000-09-23             1 2000-09-23\n 2 2000-09-23             2 2000-09-30\n 3 2000-09-23             3 2000-10-07\n 4 2000-09-23             4 2000-10-14\n 5 2000-09-23             5 2000-10-21\n 6 2000-09-23             6 2000-10-28\n 7 2000-09-23             7 2000-11-04\n 8 2000-09-23             8 2000-11-11\n 9 2000-09-23             9 2000-11-18\n10 2000-09-23            10 2000-11-25\n# i 5,297 more rows\n```\n:::\n:::\n         \n\nDon't forget to use your `week`-turned-into-number, or else it\nwon't work! (This bit me too, so you don't need to feel bad.)\n\nYou can also combine the three column-definition statements into one\nmutate. It doesn't matter; as soon as you have defined a column, you\ncan use it in defining another column, even within the same\n`mutate`. \n\nAnyway, the rows displayed are all `week_number` 1, so the\n`current` date should be the same as `date.entered`, and\nis. (These are all the first week that a song is in the Billboard\nchart). \n\nYou might be thinking that this is not much of a check, and you would\nbe right. A handy trick is to display a random sample of 10 (say) out\nof the 5,000-odd rows of the data frame. To do that, add the line\n`sample_n(10)` on the end, like this:\n\n::: {.cell}\n\n:::\n\n \n\n::: {.cell}\n\n```{.r .cell-code}\nbillboard %>%\n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %>% \n  mutate(\n    week_number = parse_number(week),\n    rank_number = rank\n  ) %>%\n  mutate(current = date.entered + (week_number - 1) * 7) %>%\n  select(date.entered, week_number, current) %>%\n  sample_n(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 3\n   date.entered week_number current   \n   <date>             <dbl> <date>    \n 1 2000-09-09             7 2000-10-21\n 2 2000-11-25            18 2001-03-24\n 3 1999-11-13            18 2000-03-11\n 4 1999-12-25             1 1999-12-25\n 5 2000-07-08            20 2000-11-18\n 6 2000-08-19            13 2000-11-11\n 7 2000-08-12            15 2000-11-18\n 8 2000-10-21            10 2000-12-23\n 9 2000-08-12             2 2000-08-19\n10 2000-08-12            19 2000-12-16\n```\n:::\n:::\n\n \n\nThis gives a variety of rows to check. The first `current`\nshould be $7-1=6$ weeks, or about a month and a half, after the date the song\nentered the chart, and so it is; the second and third ones should be $18-1=17$\nweeks after entry, which is very close to a third of a year ($17 \\times 3 = 51$), or four months. November to March is indeed four months. The fourth one is the first week on the charts, so the current date and the date entered should be (and are) the same.\nAnd so on.\n\n\nYour random selection of rows is likely to be different from mine, but\nthe same kind of thinking will enable you to check whether it makes\nsense. \n\n \n$\\blacksquare$\n\n(f) Reaching the \\#1 rank on the Billboard chart is one of\nthe highest accolades in the popular music world. List all the\nsongs that reached `rank` 1. For these songs, list the\nartist (as given in the data set), the song title, and the date(s)\nfor which the song was ranked number 1. Arrange the songs in date\norder of being ranked \\#1. Display all the songs (I found 55 of them).\n\n\nSolution\n\n\nTo the previous pipe, add the last lines below. You can use\neither `rank` (text) or what I called\n`rank_number` (a number). It doesn't matter here,\nsince we are only checking for equal-to, not something like\n\"less than\":\n\n::: {.cell}\n\n```{.r .cell-code}\nbillboard %>% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %>% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %>%\n  mutate(current = date.entered + (week_number - 1) * 7) %>%\n  filter(rank == 1) %>%\n  arrange(current) %>%\n  select(artist.inverted, track, current)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 55 x 3\n   artist.inverted     track                 current   \n   <chr>               <chr>                 <date>    \n 1 Aguilera, Christina What A Girl Wants     2000-01-15\n 2 Aguilera, Christina What A Girl Wants     2000-01-22\n 3 Savage Garden       I Knew I Loved You    2000-01-29\n 4 Savage Garden       I Knew I Loved You    2000-02-05\n 5 Savage Garden       I Knew I Loved You    2000-02-12\n 6 Carey, Mariah       Thank God I Found You 2000-02-19\n 7 Savage Garden       I Knew I Loved You    2000-02-26\n 8 Lonestar            Amazed                2000-03-04\n 9 Lonestar            Amazed                2000-03-11\n10 Destiny's Child     Say My Name           2000-03-18\n# i 45 more rows\n```\n:::\n:::\n\n\n         \n\nYou'll see the first ten rows, as here, but with clickable buttons to\nsee the next 10 (and the previous 10 if you have moved beyond 1--10). \nThe \"artist\" column is called `artist.inverted` because, if\nthe artist is a single person rather than a group, their last name is\nlisted first. The song title appears in the column `track`. \n\nThe song by Destiny's Child spills into 2001 because it entered the\nchart in 2000, and the data set keeps a record of all such songs until\nthey drop out of the chart. I'm not sure what happened to the song\nthat was \\#1 on January 8, 2000; maybe it entered the chart in\n1999^[Which was the title of a song by Prince.] and so is not\nlisted here.\n\n \n$\\blacksquare$\n\n(g) Use R to find out which song held the \\#1 rank for the\nlargest number of weeks. For this, you can assume that the song\ntitles are all unique (if it's the same song title, it's the same\nsong), but the artists might not be (for example, Madonna might\nhave had two different songs reach the \\#1 rank). The information\nyou need is in the output you obtained for the previous part, so\nit's a matter of adding some code to the end of that.\nThe last mark was for displaying *only* the song that was\nranked \\#1 for the largest number of weeks, or for otherwise\nmaking it easy to see which song it was.\n\n\nSolution\n\n\nThis is a question of using `count`, but on the\n`track` title:\n\n::: {.cell}\n\n```{.r .cell-code}\nbillboard %>% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %>% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %>%\n  mutate(current = date.entered + (week_number - 1) * 7) %>%\n  filter(rank == 1) %>%\n  arrange(current) %>%\n  select(artist.inverted, track, current) %>%\n  count(track)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 17 x 2\n   track                                     n\n   <chr>                                 <int>\n 1 Amazed                                    2\n 2 Be With You                               3\n 3 Bent                                      1\n 4 Come On Over Baby (All I Want Is You)     4\n 5 Doesn't Really Matter                     3\n 6 Everything You Want                       1\n 7 I Knew I Loved You                        4\n 8 Incomplete                                2\n 9 Independent Women Part I                 11\n10 It's Gonna Be Me                          2\n11 Maria, Maria                             10\n12 Music                                     4\n13 Say My Name                               3\n14 Thank God I Found You                     1\n15 Try Again                                 1\n16 What A Girl Wants                         2\n17 With Arms Wide Open                       1\n```\n:::\n:::\n\n\n        \nThen you can scan down the `n` column, find that the\nbiggest number is 11, and say: it's the song \"Independent Women Part I\" by Destiny's Child. This is 3 points (out of 4, when the question\nwas to be handed in).\n\nBut, this is a data frame, so anything we can do to a data frame we\ncan do to this, like listing out only the row(s) where `n` is\nequal to its maximum value:\n\n::: {.cell}\n\n```{.r .cell-code}\nbillboard %>% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %>% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %>%\n  mutate(current = date.entered + (week_number - 1) * 7) %>%\n  filter(rank == 1) %>%\n  arrange(current) %>%\n  select(artist.inverted, track, current) %>%\n  count(track) %>% \n  filter(n == max(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n  track                        n\n  <chr>                    <int>\n1 Independent Women Part I    11\n```\n:::\n:::\n\n \n\nor arranging them in (most logically, descending) order by `n`\nto make it easier to pick out the top one:\n\n::: {.cell}\n\n```{.r .cell-code}\nbillboard %>% \n  pivot_longer(ends_with(\"week\"), names_to = \"week\", values_to=\"rank\", values_drop_na = T) %>% \n  mutate(week_number=parse_number(week),\n         rank_number=rank) %>%\n  mutate(current = date.entered + (week_number - 1) * 7) %>%\n  filter(rank == 1) %>%\n  arrange(current) %>%\n  select(artist.inverted, track, current) %>%\n  count(track) %>% \n  arrange(desc(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 17 x 2\n   track                                     n\n   <chr>                                 <int>\n 1 Independent Women Part I                 11\n 2 Maria, Maria                             10\n 3 Come On Over Baby (All I Want Is You)     4\n 4 I Knew I Loved You                        4\n 5 Music                                     4\n 6 Be With You                               3\n 7 Doesn't Really Matter                     3\n 8 Say My Name                               3\n 9 Amazed                                    2\n10 Incomplete                                2\n11 It's Gonna Be Me                          2\n12 What A Girl Wants                         2\n13 Bent                                      1\n14 Everything You Want                       1\n15 Thank God I Found You                     1\n16 Try Again                                 1\n17 With Arms Wide Open                       1\n```\n:::\n:::\n\n \n\nEither of those would have netted you the 4th point.\n\nIf you want to be a little bit more careful, you can make an\nartist-track combination as below. This would catch occasions where\nthe same song by two different artists made it to \\#1, or two\ndifferent songs that happened to have the same title did. It's not\nvery likely that the same artist would record two *different*\nsongs with the same title, though it is possible that the same song by\nthe same artist could appear in the Billboard chart on two different\noccasions.^[As, for example, when Prince died.]\n\nI think I want to create an artist-song combo fairly early in my pipe,\nand then display *that* later, something like this. This means\nreplacing `track` by my `combo` later in the pipe,\nwherever it appears:\n\n::: {.cell}\n\n```{.r .cell-code}\nbillboard %>%\n  pivot_longer(x1st.week:x76th.week, names_to = \"week\", values_to = \"rank\", values_drop_na = T) %>%\n  mutate(\n    week_number = parse_number(week),\n    rank_number = rank\n  ) %>%\n  mutate(combo = paste(track, artist.inverted, sep = \" by \")) %>%\n  mutate(current = date.entered + (week_number - 1) * 7) %>%\n  filter(rank == 1) %>%\n  arrange(current) %>%\n  select(combo, current) %>%\n  count(combo) %>%\n  arrange(desc(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 17 x 2\n   combo                                                            n\n   <chr>                                                        <int>\n 1 Independent Women Part I by Destiny's Child                     11\n 2 Maria, Maria by Santana                                         10\n 3 Come On Over Baby (All I Want Is You) by Aguilera, Christina     4\n 4 I Knew I Loved You by Savage Garden                              4\n 5 Music by Madonna                                                 4\n 6 Be With You by Iglesias, Enrique                                 3\n 7 Doesn't Really Matter by Janet                                   3\n 8 Say My Name by Destiny's Child                                   3\n 9 Amazed by Lonestar                                               2\n10 Incomplete by Sisqo                                              2\n11 It's Gonna Be Me by N'Sync                                       2\n12 What A Girl Wants by Aguilera, Christina                         2\n13 Bent by matchbox twenty                                          1\n14 Everything You Want by Vertical Horizon                          1\n15 Thank God I Found You by Carey, Mariah                           1\n16 Try Again by Aaliyah                                             1\n17 With Arms Wide Open by Creed                                     1\n```\n:::\n:::\n\n \n\nI don't think it makes any difference here, but it might in other\nyears, or if you look over several years where you might get cover\nversions of the same song performed by different artists.\n\nZero-point bonus: how many of these artists have you heard of? How\nmany have your parents heard of? (I followed popular music quite\nclosely much earlier than this, in the early 1980s in the UK. I\nremember both Madonna and U2 when they *first* became\nfamous. U2's first single was called \"Fire\" and it just scraped into\nthe UK top 40. Things changed after that.)\n\n \n$\\blacksquare$\n\n\n\n\n\n##  Bikes on College\n\n\n The City of Toronto collects all kinds of data on aspects of\nlife in the city. See\n[link](http://www1.toronto.ca/wps/portal/contentonly?vgnextoid=1a66e03bb8d1e310VgnVCM10000071d60f89RCRD). One\ncollection of data is records of the number of cyclists on certain\ndowntown streets. The data in\n[link](http://ritsokiguess.site/datafiles/bikes.csv) are a record\nof the cyclists on College Street on the block west from Huron to\nSpadina on September 24, 2010. In the spreadsheet, each row relates to\none cyclist. The first column is the time the cyclist was observed (to\nthe nearest 15 minutes). After that, there are four pairs of\ncolumns. The observer filled in (exactly) one X in each pair of\ncolumns, according to whether (i) the cyclist was male or female, (ii)\nwas or was not wearing a helmet, (iii) was or was not carrying a\npassenger on the bike, (iv) was or was not riding on the sidewalk. We\nwant to create a tidy data frame that has the time in each row, and\nhas columns containing appropriate values, often `TRUE` or\n`FALSE`, for each of the four variables measured.\n\nI will lead you through the process, which will involve developing a\n(long) pipe, one step at a time.\n\n\n\n(a) Take a look at the spreadsheet (using Excel or similar:\nthis may open when you click the link). Are there any obvious\nheader rows? Is there any extra material before the data start?\nExplain briefly.\n\n\nSolution\n\n\nThis is what I see (you should see something that looks like this):\n\n![](bikes-ss.png)\n\nThere are really *two* rows of headers (the rows\nhighlighted in yellow). The actual information that says what\nthe column pair is about is in the first of those two rows, and\nthe second row indicates which category of the information above\nthis column refers to.\nThis is not the usual way that the column headers encode what\nthe columns are about: we are used to having *one* column\n`gender` that would take the values `female` or\n`male`, or a column `helmet` containing the values\n`yes` or `no`. (You might be sensing\n`pivot_longer` here, which may be one way of tackling this, but\nI lead you into another idea below.)\nThere are also six lines above the highlighted ones that contain\nbackground information about this study. (This is where I got\nthe information about the date of the study and which block of\nwhich street it is about.)\nI am looking for two things: the apparent header line is\nactually two lines (the ones in yellow), and there are extra\nlines above that which are not data.\n\n$\\blacksquare$\n\n(b) Read the data into an R data\nframe. Read *without* headers, and instruct R how many lines\nto skip over using `skip=` and a suitable number.\nWhen this is working, display the first few lines of your data\nframe.  Note that your columns have names `X1` through\n`X9`.\n\n\nSolution\n\n\nThe actual data start on line 9, so we need to skip 8\nlines. `col_names=F` is the way to say that we have no\ncolumn names (not ones that we want to use, anyway). Just\ntyping the name of the data frame will display \"a few\" (that\nis, 10) lines of it, so that you can check it for\nplausibleness: \n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/bikes.csv\"\nbikes <- read_csv(my_url, skip = 8, col_names = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 1958 Columns: 9\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (8): X2, X3, X4, X5, X6, X7, X8, X9\ntime (1): X1\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nbikes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 9\n   X1     X2    X3    X4    X5    X6    X7    X8    X9   \n   <time> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n 1 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 2    NA  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 3    NA  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 4    NA  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 5    NA  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 6 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X    \n 7    NA  <NA>  X     X     <NA>  <NA>  X     <NA>  X    \n 8    NA  X     <NA>  X     <NA>  <NA>  X     <NA>  X    \n 9    NA  <NA>  X     X     <NA>  <NA>  X     X     <NA> \n10    NA  X     <NA>  X     <NA>  <NA>  X     <NA>  X    \n# i 1,948 more rows\n```\n:::\n:::\n\n         \n\nThis seems to have worked: a column with times in it, and four pairs\nof columns, with exactly one of each pair having an X in it. The\nvariable names `X1` through `X9` were generated by\n`read_csv`, as it does when you read in data with\n`col_names=FALSE`. The times are correctly `time`s, and the\nother columns are all text. The blank cells in the spreadsheet have\nappeared in our data frame as \"missing\" (`NA`). The notation\n`<NA>` means \"missing text\" (as opposed to a missing number,\nsay). \n\nThe first line in our data frame contains the first 7:00 (am) cyclist,\nso it looks as if we skipped the right number of lines.\n\n$\\blacksquare$\n\n(c) What do you notice about the times in your first\ncolumn? What do you think those \"missing\" times should be?\n\nSolution\n\n\nThere are some times and some missing values. It seems a\nreasonable guess that the person recording the data only\nrecorded a time when a new period of 15 minutes had begun, so\nthat the missing times should be the same as the previous\nnon-missing one: For\nexample, the first five rows are cyclists observed at 7:00 am\n(or, at least, between 7:00 and 7:15). So they should be\nrecorded as 7:00, and the ones in rows 7--10 should be\nrecorded as 7:15, and so on.\n\n$\\blacksquare$\n\n(d) Find something from the `tidyverse` that will\nfill^[Oh, what a giveaway.]\nin those missing values with the right thing.\nStart a pipe from the data frame you read in, that updates the\nappropriate column with the filled-in times.\n\n\nSolution\n\n\n`fill` from `tidyr` fills\nin the missing times with the previous non-missing\nvalue. (This will mean finding the help for `fill` in R\nStudio or online.)\nI told you it was a giveaway.\nIf you look in the help for `fill` via `?fill`\n(or if you Google `tidyr::fill`, which is the full\nname for \"the `fill` that lives in          `tidyr`\"), \nyou'll see that it requires up to two\nthings (not including the data frame): a column to fill, and\na direction to fill it (the default of \"down\" is exactly\nwhat we want). Thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>% fill(X1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 9\n   X1     X2    X3    X4    X5    X6    X7    X8    X9   \n   <time> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n 1 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 2 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 3 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 4 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 5 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 6 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X    \n 7 07:15  <NA>  X     X     <NA>  <NA>  X     <NA>  X    \n 8 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X    \n 9 07:15  <NA>  X     X     <NA>  <NA>  X     X     <NA> \n10 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X    \n# i 1,948 more rows\n```\n:::\n:::\n\n           \n\nSuccess!\n\nWe will probably want to rename `X1` to something like\n`time`, so let's do that now before we forget. There is a\n`rename` that does about what you'd expect:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>% fill(X1) %>% rename(Time = X1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 9\n   Time   X2    X3    X4    X5    X6    X7    X8    X9   \n   <time> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n 1 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 2 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 3 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 4 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 5 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X    \n 6 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X    \n 7 07:15  <NA>  X     X     <NA>  <NA>  X     <NA>  X    \n 8 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X    \n 9 07:15  <NA>  X     X     <NA>  <NA>  X     X     <NA> \n10 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X    \n# i 1,948 more rows\n```\n:::\n:::\n\n \n\nThe only thing I keep forgetting is that the syntax of `rename`\nis \"new name equals old name\". Sometimes I think it's the other way\naround, and then I wonder why it doesn't work.\n\nI gave it a capital T so as not to confuse it with other things in R\ncalled `time`.\n\n$\\blacksquare$\n\n(e) R's `ifelse` function works like `=IF` in\nExcel. You use it to create values for a new variable, for\nexample in a `mutate`.  The first input to it is a\nlogical condition (something that is either true or false); the\nsecond is the value your new variable should take if the\ncondition is true, and the third is the value of your new\nvariable if the condition is false.  Create a new column\n`gender` in your data frame that is \"male\" or\n\"female\" depending on the value of your `X2` column,\nusing `mutate`. (You can assume that exactly one of the\nsecond and third columns has an `X` in it.) Add your code\nto the end of your pipe and display (the first 10 rows of) the\nresult.\n\n\nSolution\n\n\nUnder the assumption we are making, we only have to look\nat column `X2` and we ignore `X3` totally:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  mutate(gender = ifelse(X2 == \"X\", \"male\", \"female\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 10\n   Time   X2    X3    X4    X5    X6    X7    X8    X9    gender\n   <time> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> \n 1 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male  \n 2 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male  \n 3 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male  \n 4 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male  \n 5 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male  \n 6 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     male  \n 7 07:15  <NA>  X     X     <NA>  <NA>  X     <NA>  X     <NA>  \n 8 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     male  \n 9 07:15  <NA>  X     X     <NA>  <NA>  X     X     <NA>  <NA>  \n10 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     male  \n# i 1,948 more rows\n```\n:::\n:::\n\n            \n\nOh, that didn't work. The gender column is either `male` or\nmissing; the two missing ones here should say `female`. What\nhappened? Let's just look at our logical condition this time:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  mutate(isX = (X2 == \"X\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 10\n   Time   X2    X3    X4    X5    X6    X7    X8    X9    isX  \n   <time> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <lgl>\n 1 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     TRUE \n 2 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     TRUE \n 3 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     TRUE \n 4 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     TRUE \n 5 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     TRUE \n 6 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     TRUE \n 7 07:15  <NA>  X     X     <NA>  <NA>  X     <NA>  X     NA   \n 8 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     TRUE \n 9 07:15  <NA>  X     X     <NA>  <NA>  X     X     <NA>  NA   \n10 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     TRUE \n# i 1,948 more rows\n```\n:::\n:::\n\n \n\nThis is not true and false, it is true and missing. The idea is that\nif `X2` is missing, we don't (in general) know what its value\nis: it might even be `X`! So if `X2` is missing, any\ncomparison of it with another value ought to be missing as well.\n\nThat's in general. Here, we know where those missing values came from:\nthey were blank cells in the spreadsheet, so we actually have more\ninformation. \n\nPerhaps a better way to go is to test whether `X2` is missing (in which\ncase, it's a female cyclist). R has a function `is.na` which is\n`TRUE` if the thing inside it is missing and `FALSE` if\nthe thing inside it has some non-missing value. In our case, it\ngoes like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 10\n   Time   X2    X3    X4    X5    X6    X7    X8    X9    gender\n   <time> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> \n 1 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male  \n 2 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male  \n 3 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male  \n 4 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male  \n 5 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male  \n 6 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     male  \n 7 07:15  <NA>  X     X     <NA>  <NA>  X     <NA>  X     female\n 8 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     male  \n 9 07:15  <NA>  X     X     <NA>  <NA>  X     X     <NA>  female\n10 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     male  \n# i 1,948 more rows\n```\n:::\n:::\n\n            \n\nOr you can test `X3` for missingness: if missing, it's male,\notherwise it's female. That also works.\n\nThis made an assumption that the person recording the X's actually\n*did* mark an X in exactly one of the columns. For example, the\ncolumns could *both* be missing, or *both* have an X in\nthem. This gives us more things to check, at least\nthree. `ifelse` is good for something with only two\nalternatives, but when you have more, `case_when` is much\nbetter.^[In some languages it is called `switch`. Python  appears not to have it. What you do there instead is to use a Python  dictionary to pick out the value you want.] \nHere's how that goes. Our strategy is to\ncheck for three things: (i) `X2` has an `X` and\n`X3` is missing; (ii) `X2` is missing and `X3`\nhas an `X`; (iii) anything else, which is an error:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  mutate(gender = case_when(\n    X2 == \"X\" & is.na(X3) ~ \"Male\",\n    is.na(X2) & X3 == \"X\" ~ \"Female\",\n    TRUE                  ~ \"Error!\"\n  ))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 10\n   Time   X2    X3    X4    X5    X6    X7    X8    X9    gender\n   <time> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> \n 1 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     Male  \n 2 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     Male  \n 3 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     Male  \n 4 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     Male  \n 5 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     Male  \n 6 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     Male  \n 7 07:15  <NA>  X     X     <NA>  <NA>  X     <NA>  X     Female\n 8 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     Male  \n 9 07:15  <NA>  X     X     <NA>  <NA>  X     X     <NA>  Female\n10 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     Male  \n# i 1,948 more rows\n```\n:::\n:::\n\n            \n\nIt seems nicest to format it with the squiggles lining up,\nso you can see what possible values `gender` might take.\n\nThe structure of the `case_when` is that the thing you're\nchecking for goes on the left of the squiggle, and the value you want\nyour new variable to take goes on the right. What it does is to go\ndown the list of conditions that you are checking for, and as soon as\nit finds one that is true, it grabs the value on the right of the\nsquiggle and moves on to the next row. The usual way to write these is\nto have a catch-all condition at the end that is always true, serving\nto make sure that your new variable always gets *some*\nvalue. `TRUE` is, um, always true. If you want an English word\nfor the last condition of your `case_when`, \"otherwise\" is a\nnice one.\n\nI wanted to check that the observer did check exactly one of\n`V2` and `V3` as I asserted, which can be done by\ngluing this onto the end:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  mutate(gender = case_when(\n    X2 == \"X\" & is.na(X3) ~ \"Male\",\n    is.na(X2) & X3 == \"X\" ~ \"Female\",\n    TRUE ~ \"Error!\"\n  )) %>%\n  count(gender)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n  gender     n\n  <chr>  <int>\n1 Female   861\n2 Male    1097\n```\n:::\n:::\n\n            \n\nThere are only Males and Females, so the observer really did mark\nexactly one X. (As a bonus, you see that there were slightly more male\ncyclists than female ones.)\n\nExtra: I was wondering how `pivot_longer` would play out here. The\nway to do it seems to be to rename the columns we want first, and get rid of the others:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  rename(male = X2, female = X3) %>% \n  select(-starts_with(\"X\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 3\n   Time   male  female\n   <time> <chr> <chr> \n 1 07:00  X     <NA>  \n 2 07:00  X     <NA>  \n 3 07:00  X     <NA>  \n 4 07:00  X     <NA>  \n 5 07:00  X     <NA>  \n 6 07:15  X     <NA>  \n 7 07:15  <NA>  X     \n 8 07:15  X     <NA>  \n 9 07:15  <NA>  X     \n10 07:15  X     <NA>  \n# i 1,948 more rows\n```\n:::\n:::\n\nEach row should have one X and one missing in it, so we may as well drop the missings as we pivot-longer:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  rename(male = X2, female = X3) %>% \n  select(-starts_with(\"X\")) %>% \n  pivot_longer(-Time, names_to=\"gender\", values_to=\"observed\", \n               values_drop_na = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 3\n   Time   gender observed\n   <time> <chr>  <chr>   \n 1 07:00  male   X       \n 2 07:00  male   X       \n 3 07:00  male   X       \n 4 07:00  male   X       \n 5 07:00  male   X       \n 6 07:15  male   X       \n 7 07:15  female X       \n 8 07:15  male   X       \n 9 07:15  female X       \n10 07:15  male   X       \n# i 1,948 more rows\n```\n:::\n:::\n\nThe `observed` column is kind of pointless, since its value is always X. But we do have a check: the previous data frame had 1958 rows, with an X in either the male or the female column. This data frame has the gender of each observed cyclist in the `gender` column, and it also has 1958 rows. So, either way, that's how many cyclists were observed in total.\n\n\n\n \n$\\blacksquare$\n\n\n(f) Create variables `helmet`, `passenger` and\n`sidewalk` in your data frame that are `TRUE` if\nthe \"Yes\" column contains `X` and `FALSE`\notherwise. This will use `mutate` again, but you don't\nneed `ifelse`: just set the variable equal to the\nappropriate logical condition. As before, the best way to\ncreate these variables is to test the appropriate things for\nmissingness.  Note that you can create as many new variables\nas you like in one `mutate`. Show the first few lines\nof your new data frame. (Add your code onto the end of the\npipe you made above.)\n\n\nSolution\n\n\nOn the face of it, the way to do this is to go looking\nfor `X`'s:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %>%\n  mutate(\n    helmet = (X4 == \"X\"),\n    passenger = (X6 == \"X\"),\n    sidewalk = (X8 == \"X\")\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 13\n   Time  X2    X3    X4    X5    X6    X7    X8    X9    gender helmet passenger\n   <tim> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>  <lgl>  <lgl>    \n 1 07:00 X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   NA     NA       \n 2 07:00 X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   NA     NA       \n 3 07:00 X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   NA     NA       \n 4 07:00 X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   NA     NA       \n 5 07:00 X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   NA     NA       \n 6 07:15 X     <NA>  X     <NA>  <NA>  X     <NA>  X     male   TRUE   NA       \n 7 07:15 <NA>  X     X     <NA>  <NA>  X     <NA>  X     female TRUE   NA       \n 8 07:15 X     <NA>  X     <NA>  <NA>  X     <NA>  X     male   TRUE   NA       \n 9 07:15 <NA>  X     X     <NA>  <NA>  X     X     <NA>  female TRUE   NA       \n10 07:15 X     <NA>  X     <NA>  <NA>  X     <NA>  X     male   TRUE   NA       \n# i 1,948 more rows\n# i 1 more variable: sidewalk <lgl>\n```\n:::\n:::\n\n    \n\nBut, we run into the same problem that we did with `gender`:\nthe new variables are either `TRUE` or missing, never `FALSE`.\n\nThe solution is the same: look for the things that are *missing*\nif the cyclist is wearing a helmet, carrying a passenger or riding on\nthe sidewalk. These are `X5, X7, X9` respectively:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %>%\n  mutate(\n    helmet = is.na(X5),\n    passenger = is.na(X7),\n    sidewalk = is.na(X9)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 13\n   Time  X2    X3    X4    X5    X6    X7    X8    X9    gender helmet passenger\n   <tim> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>  <lgl>  <lgl>    \n 1 07:00 X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   FALSE  FALSE    \n 2 07:00 X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   FALSE  FALSE    \n 3 07:00 X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   FALSE  FALSE    \n 4 07:00 X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   FALSE  FALSE    \n 5 07:00 X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   FALSE  FALSE    \n 6 07:15 X     <NA>  X     <NA>  <NA>  X     <NA>  X     male   TRUE   FALSE    \n 7 07:15 <NA>  X     X     <NA>  <NA>  X     <NA>  X     female TRUE   FALSE    \n 8 07:15 X     <NA>  X     <NA>  <NA>  X     <NA>  X     male   TRUE   FALSE    \n 9 07:15 <NA>  X     X     <NA>  <NA>  X     X     <NA>  female TRUE   FALSE    \n10 07:15 X     <NA>  X     <NA>  <NA>  X     <NA>  X     male   TRUE   FALSE    \n# i 1,948 more rows\n# i 1 more variable: sidewalk <lgl>\n```\n:::\n:::\n\n    \nAgain, you can do the `mutate` all on one line if you want to,\nor all four variable assignments in one `mutate`,\nbut I used newlines and indentation to make the structure\nclear. \n\nIt is less elegant, though equally good for the purposes of the\nassignment, to use `ifelse` for these as well, which would go\nlike this, for example:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  mutate(gender = ifelse(X2 == \"X\", \"male\", \"female\")) %>%\n  mutate(helmet = ifelse(is.na(X5), TRUE, FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 11\n   Time   X2    X3    X4    X5    X6    X7    X8    X9    gender helmet\n   <time> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>  <lgl> \n 1 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   FALSE \n 2 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   FALSE \n 3 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   FALSE \n 4 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   FALSE \n 5 07:00  X     <NA>  <NA>  X     <NA>  X     <NA>  X     male   FALSE \n 6 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     male   TRUE  \n 7 07:15  <NA>  X     X     <NA>  <NA>  X     <NA>  X     <NA>   TRUE  \n 8 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     male   TRUE  \n 9 07:15  <NA>  X     X     <NA>  <NA>  X     X     <NA>  <NA>   TRUE  \n10 07:15  X     <NA>  X     <NA>  <NA>  X     <NA>  X     male   TRUE  \n# i 1,948 more rows\n```\n:::\n:::\n\n \n\nand the same for `passenger` and `sidewalk`. The warning\nis, whenever you see a `TRUE` and a `FALSE` in an\n`ifelse`, that you could probably get rid of the\n`ifelse` and use the logical condition directly.^[If I  was helping you, and you were struggling with *ifelse* but  finally mastered it, it seemed easier to suggest that you used it  again for the others.]\n\nFor `gender`, though, you need the\n`ifelse` (or a `case_when`) because the values you want\nit to take are `male` and `female`, something other than\n`TRUE` and `FALSE`.\n\nI like to put brackets around logical conditions when I am assigning\nthem to a variable or defining new columns containing them.\nIf I don't, I get something like\n\n::: {.cell}\n\n```{.r .cell-code}\nhelmet <- V4 == \"X\"\n```\n:::\n\n \n\nwhich actually works, but is hard to read. Well, I *think* it\nworks. Let's check:\n\n::: {.cell}\n\n```{.r .cell-code}\nexes <- c(\"X\", \"\", \"X\", \"\", \"X\")\ny <- exes == \"X\"\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  TRUE FALSE  TRUE FALSE  TRUE\n```\n:::\n:::\n\n\n\nYes it does. But I would not recommend writing it this way, because\nunless you are paying attention, you won't notice that `==` is\ntesting for \"logically equal\" rather than putting something in a column.\n\nIt works because of a thing called \"operator precedence\": the\nlogical-equals is evaluated first, and the result of that is saved in\nthe variable. But unless you or your readers remember that, it's\nbetter to write\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- (exes == \"X\")\n```\n:::\n\n \n\nto draw attention to the order of calculation. This is the same reason that\n\n::: {.cell}\n\n```{.r .cell-code}\n4 + 5 * 6\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 34\n```\n:::\n:::\n\n \n\nevaluates this way rather than doing the addition first and getting\n54. BODMAS and all that.\n\nThe `pivot_longer` approach works for these too. Rename the columns\nas `yes` and `no`, and then give the `names_to` column a name like `helmet`.\nGive the `values_to` column a name like\n`what2`, to make it easier to remove later. And then do the\nsame with the others, one `pivot_longer` at a time. (Keep all the columns, and then discard them at the end if you want. That way you don't risk deleting something you might need later.)\n\n$\\blacksquare$\n\n(g) Finally \n(for the data manipulation), get rid of\nall the original columns, keeping only the new ones that\nyou created. Save the results in a data frame and display\nits first few rows.\n \n\nSolution\n\n\nThis is a breath of fresh air after all the thinking\nneeded above: this is just `select`, added to\nthe end:\n\n::: {.cell}\n\n```{.r .cell-code}\nmybikes <- bikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %>%\n  mutate(\n    helmet = is.na(X5),\n    passenger = is.na(X7),\n    sidewalk = is.na(X9)\n  ) %>%\n  select(-(X2:X9))\nmybikes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 5\n   Time   gender helmet passenger sidewalk\n   <time> <chr>  <lgl>  <lgl>     <lgl>   \n 1 07:00  male   FALSE  FALSE     FALSE   \n 2 07:00  male   FALSE  FALSE     FALSE   \n 3 07:00  male   FALSE  FALSE     FALSE   \n 4 07:00  male   FALSE  FALSE     FALSE   \n 5 07:00  male   FALSE  FALSE     FALSE   \n 6 07:15  male   TRUE   FALSE     FALSE   \n 7 07:15  female TRUE   FALSE     FALSE   \n 8 07:15  male   TRUE   FALSE     FALSE   \n 9 07:15  female TRUE   FALSE     TRUE    \n10 07:15  male   TRUE   FALSE     FALSE   \n# i 1,948 more rows\n```\n:::\n:::\n\n           \n\nYou might not have renamed your `X1`, in which case, you still\nhave it, but need to keep it (because it holds the times).\n\nAnother way to do this is to use a \"select-helper\", thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nbikes %>%\n  fill(X1) %>%\n  rename(Time = X1) %>%\n  mutate(gender = ifelse(is.na(X2), \"female\", \"male\")) %>%\n  mutate(\n    helmet = is.na(X5),\n    passenger = is.na(X7),\n    sidewalk = is.na(X9)\n  ) %>%\n  select(-num_range(\"X\", 2:9))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,958 x 5\n   Time   gender helmet passenger sidewalk\n   <time> <chr>  <lgl>  <lgl>     <lgl>   \n 1 07:00  male   FALSE  FALSE     FALSE   \n 2 07:00  male   FALSE  FALSE     FALSE   \n 3 07:00  male   FALSE  FALSE     FALSE   \n 4 07:00  male   FALSE  FALSE     FALSE   \n 5 07:00  male   FALSE  FALSE     FALSE   \n 6 07:15  male   TRUE   FALSE     FALSE   \n 7 07:15  female TRUE   FALSE     FALSE   \n 8 07:15  male   TRUE   FALSE     FALSE   \n 9 07:15  female TRUE   FALSE     TRUE    \n10 07:15  male   TRUE   FALSE     FALSE   \n# i 1,948 more rows\n```\n:::\n:::\n\n           \n\nThis means \"get rid of all the columns whose names are *X* followed by a number 2 through 9\". \n\nThe pipe looks long and forbidding, but you built it (and tested it) a\nlittle at a time. Which is how you do it.\n\n$\\blacksquare$\n\n(h) The next few parts are a quick-fire analysis of\nthe data set. They can all be solved using `count`.\nHow many male and how many female cyclists were observed\nin total?\n\n\nSolution\n\n\nI already got this one when I was checking for\nobserver-notation errors earlier:\n\n::: {.cell}\n\n```{.r .cell-code}\nmybikes %>% count(gender)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n  gender     n\n  <chr>  <int>\n1 female   861\n2 male    1097\n```\n:::\n:::\n\n                   \n\n861 females and 1097 males.\n\n$\\blacksquare$\n\n(i) How many male and female cyclists were not\nwearing helmets?\n\n\nSolution\n\n\nYou can count two variables at once, in which case\nyou get counts of all combinations of them:\n\n::: {.cell}\n\n```{.r .cell-code}\nmybikes %>% count(gender, helmet)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 3\n  gender helmet     n\n  <chr>  <lgl>  <int>\n1 female FALSE    403\n2 female TRUE     458\n3 male   FALSE    604\n4 male   TRUE     493\n```\n:::\n:::\n\n                   \n\n403 females and 604 males were not wearing helmets, picking out what\nwe need.\n\nThe real question of interest here is \"what *proportion* of male and female cyclists were not wearing helmets?\"^[But I didn't want to complicate this question any farther.] \nThis has a rather elegant\nsolution that I will have to explain. First, let's go back to the\n`group_by` and `summarize` version of the\n`count` here:\n\n::: {.cell}\n\n```{.r .cell-code}\nmybikes %>%\n  group_by(gender, helmet) %>%\n  summarize(the_count = n())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 3\n# Groups:   gender [2]\n  gender helmet the_count\n  <chr>  <lgl>      <int>\n1 female FALSE        403\n2 female TRUE         458\n3 male   FALSE        604\n4 male   TRUE         493\n```\n:::\n:::\n\n \n\nThat's the same table we got just now. Now, let's calculate a\nproportion and see what happens:\n\n::: {.cell}\n\n```{.r .cell-code}\nmybikes %>%\n  group_by(gender, helmet) %>%\n  summarize(the_count = n()) %>%\n  mutate(prop = the_count / sum(the_count))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 4\n# Groups:   gender [2]\n  gender helmet the_count  prop\n  <chr>  <lgl>      <int> <dbl>\n1 female FALSE        403 0.468\n2 female TRUE         458 0.532\n3 male   FALSE        604 0.551\n4 male   TRUE         493 0.449\n```\n:::\n:::\n\n \n\nWe seem to have the proportions of males and females who were and were\nnot wearing a helmet, and you can check that this is indeed the case,\nfor example:\n\n::: {.cell}\n\n```{.r .cell-code}\n403 / (403 + 458)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4680604\n```\n:::\n:::\n\n \n47\\% of females were not wearing helmets, while 55\\% of males were\nhelmetless. (You can tell from the original frequencies that a small\nmajority of females wore helmets and a small majority of males did not.)\n\nNow, we have to ask ourselves: how on earth did that work?\n\nWhen you calculate a summary (like our `sum(count)` above), it\nfigures that you can't want the sum by gender-helmet combination,\nsince you already have those in `count`. You must want the sum\n*over* something. What? What happens is that it goes back to the\n`group_by` and \"peels off\" the last thing there, which in\nthis case is `helmet`, leaving only `gender`. It then\nsums the counts for each gender, giving us what we wanted.\n\nIt just blows my mind that someone (ie., Hadley Wickham) could (i) think\nthat this would be a nice syntax to have (instead of just being an\nerror), (ii) find a way to implement it and (iii) find a nice logical\nexplanation (\"peeling off\") to explain how it worked.\n\nWhat happens if we switch the order of the things in the `group_by`?\n\n::: {.cell}\n\n```{.r .cell-code}\nmybikes %>%\n  group_by(helmet, gender) %>%\n  summarize(the_count = n()) %>%\n  mutate(prop = the_count / sum(the_count))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'helmet'. You can override using the\n`.groups` argument.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 4\n# Groups:   helmet [2]\n  helmet gender the_count  prop\n  <lgl>  <chr>      <int> <dbl>\n1 FALSE  female       403 0.400\n2 FALSE  male         604 0.600\n3 TRUE   female       458 0.482\n4 TRUE   male         493 0.518\n```\n:::\n:::\n\n \n\nNow we get the proportion of helmeted riders of each gender, which is\nnot the same as what we had before. Before, we had \"out of males\"\nand \"out of females\"; now we have \"out of helmeted riders\" and\n\"out of helmetless riders\". (The riders with helmets are almost\n50--50 males and females, but the riders without helmets are about\n60\\% male.)\n\nThis is row and column proportions in a contingency table, B22 style.\n\nNow, I have to see whether the `count` variant of this works:\n\n::: {.cell}\n\n```{.r .cell-code}\nmybikes %>%\n  count(gender, helmet) %>%\n  mutate(prop = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 4\n  gender helmet     n  prop\n  <chr>  <lgl>  <int> <dbl>\n1 female FALSE    403 0.206\n2 female TRUE     458 0.234\n3 male   FALSE    604 0.308\n4 male   TRUE     493 0.252\n```\n:::\n:::\n\n \n\nIt doesn't. Well, it kind of does, but it divided by the sum of all of\nthem rather than \"peeling off\", so these are overall proportions\nrather than row or column proportions.\n\nSo I think you have to do this the `group_by` and\n`summarize` way.\n\n$\\blacksquare$\n\n(j) How many cyclists were riding on the sidewalk\n*and* carrying a passenger?\n\n\nSolution\n\n\nNot too many, I'd hope. Again:\n\n::: {.cell}\n\n```{.r .cell-code}\nmybikes %>% count(passenger, sidewalk)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 3\n  passenger sidewalk     n\n  <lgl>     <lgl>    <int>\n1 FALSE     FALSE     1880\n2 FALSE     TRUE        73\n3 TRUE      FALSE        5\n```\n:::\n:::\n\n                   \n\nWe're looking for the \"true\", \"true\" entry of that table, which\nseems to have vanished. That means the count is *zero*:\nnone at all. \n(There were\nonly 5 passenger-carrying riders, and they were all on the road.)\n\n$\\blacksquare$\n\n(k) What was the busiest 15-minute period of the\nday, and how many cyclists were there then?\n\n\nSolution\n\n\nThe obvious way is to list every 15-minute period\nand eyeball the largest frequency. There are quite a\nfew 15-minute periods, so be prepared to hit Next a\nfew times (or use `View`):\n\n::: {.cell}\n\n```{.r .cell-code}\nmybikes %>% count(Time) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 48 x 2\n   Time       n\n   <time> <int>\n 1 07:00      5\n 2 07:15      8\n 3 07:30      9\n 4 07:45      5\n 5 08:00     18\n 6 08:15     14\n 7 08:30     12\n 8 08:45     22\n 9 09:00     17\n10 09:15     15\n# i 38 more rows\n```\n:::\n:::\n\n                   \n\n17:15, or 5:15 pm, with 128 cyclists. \n\nBut, computers are meant to save us that kind of effort. How?  Note\nthat the output from `count` is itself a data frame, so\nanything you can do to a data frame, you can do to *it*: for\nexample, display only the rows where the frequency equals the maximum\nfrequency:\n\n::: {.cell}\n\n```{.r .cell-code}\nmybikes %>%\n  count(Time) %>%\n  filter(n == max(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n  Time       n\n  <time> <int>\n1 17:15    128\n```\n:::\n:::\n\n                   \n\nThat will actually display *all* the times where the cyclist\ncount equals the maximum, of which there might be more than one.\n\n$\\blacksquare$\n\n\n\n\n\n\n##  Feeling the heat\n\n\n In summer, the city of Toronto issues Heat Alerts for \n\"high heat or humidity that is expected to last two or more days\". The\nprecise definitions are shown at\n[link](http://www1.toronto.ca/wps/portal/contentonly?vgnextoid=923b5ce6dfb31410VgnVCM10000071d60f89RCRD). During\na heat alert, the city opens Cooling Centres and may extend the hours\nof operation of city swimming pools, among other things. All the heat\nalert days from 2001 to 2016 are listed at\n[link](http://ritsokiguess.site/datafiles/heat.csv).\n\nThe word \"warning\" is sometimes used in place of \"alert\" in these\ndata. They mean the same thing.^[Unlike *thunderstorm watch*  and *thunderstorm warning*, which mean different things.]\n\n\n\n(a) Read the data into R, and display the data frame. Note that there are four columns:\n\n\n* a numerical `id` (numbered upwards from the first Heat\nAlert in 2001; some of the numbers are missing)\n\n* the `date` of the heat alert, in year-month-day\nformat with 4-digit years.\n\n* a text `code` for the type of heat alert\n\n* `text` describing the kind of heat alert. This can be quite long. \n\n\n\nSolution\n\n\nA `.csv`, so:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/heat.csv\"\nheat <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 200 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (2): code, text\ndbl  (1): id\ndate (1): date\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nheat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 x 4\n      id date       code  text                                                  \n   <dbl> <date>     <chr> <chr>                                                 \n 1   232 2016-09-08 HAU   Toronto's Medical Officer of Health has upgraded the ~\n 2   231 2016-09-07 HAE   Toronto's Medical Officer of Health has continued the~\n 3   230 2016-09-06 HA    Toronto's Medical Officer of Health has issued a Heat~\n 4   228 2016-08-13 EHAE  Toronto's Medical Officer of Health has continued the~\n 5   227 2016-08-12 EHAE  Toronto's Medical Officer of Health has continued the~\n 6   226 2016-08-11 HAU   Toronto's Medical Officer of Health has upgraded the ~\n 7   225 2016-08-10 HAE   Toronto's Medical Officer of Health has continued the~\n 8   224 2016-08-09 HA    Toronto's Medical Officer of Health has issued a Heat~\n 9   222 2016-08-05 HAE   Toronto's Medical Officer of Health has continued the~\n10   221 2016-08-04 HA    Toronto's Medical Officer of Health has issued a Heat~\n# i 190 more rows\n```\n:::\n:::\n\n       \n\nYou might get a truncated `text` as I did, or you might have to\nclick to see more of it. In any case, we won't be using the text, so\nyou can just forget about it from here on.\n\n$\\blacksquare$\n\n(b) In your data frame, are the dates stored as genuine dates or as text? How can you tell?\n\n\nSolution\n\n\nLook at the top of the column on your display of the data frame. Under the date column it says `date` rather than `chr` (which means \"text\"), so these are genuine dates. \nThis happened because the data file contained the dates in\nyear-month-day order, so `read_csv` read them in as\ndates. (If they had been in some other order, they would have\nbeen read in as text and we would need to use\n`lubridate` to make them into dates.)\n\n$\\blacksquare$\n\n(c) Which different heat alert codes do you have, and how many of each?\n\n\nSolution\n\n\n`count`, most easily:\n\n::: {.cell}\n\n```{.r .cell-code}\nheat %>% count(code)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 2\n  code      n\n  <chr> <int>\n1 EHA      59\n2 EHAD      1\n3 EHAE     18\n4 HA       93\n5 HAE      16\n6 HAU      13\n```\n:::\n:::\n\n          \n\nAlternatively, `group_by` and `summarize`:\n\n::: {.cell}\n\n```{.r .cell-code}\nheat %>% group_by(code) %>% summarize(count = n())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 2\n  code  count\n  <chr> <int>\n1 EHA      59\n2 EHAD      1\n3 EHAE     18\n4 HA       93\n5 HAE      16\n6 HAU      13\n```\n:::\n:::\n\n \n\n(note that `n()` gives the number of rows, in each group if you have groups.)\n\nThere are six different codes, but EHAD only appears once.\n\n$\\blacksquare$\n\n(d) Use the `text` in your dataset (or look back\nat the original data file) to describe briefly in your own\nwords what the various codes represent.\n\n\nSolution\n\n\nYou can check that each time a certain code appears, the\ntext next to it is identical.\nThe six codes and my brief descriptions are:\n\n- EHA: (Start of) Extended Heat Alert\n- EHAD: Extreme Heat Alert downgraded to Heat Alert\n- EHAE: Extended Heat Alert continues\n- HA: (Start of) Heat Alert\n- HAE: Heat Alert continues\n- HAU: Heat Alert upgraded to Extended Heat Alert\n\nI thought there was such a thing as an Extreme Heat Alert,\nbut here the word is (usually) Extended, meaning a heat\nalert that extends over several days, long in duration\nrather than extremely hot. The only place Extreme occurs\nis in EHAD, which only occurs once.\nI want your answer to say or suggest something about\nwhether a code applies *only* to continuing heat\nalerts (ie., that EHAD, EHAE, HAE and HAU are different\nfrom the others).\n\n$\\blacksquare$\n\n(e) How many (regular and extended) heat alert events\nare there altogether? A heat alert event is a stretch of\nconsecutive days, on all of which there is a heat alert or\nextended heat alert. Hints: (i) you can answer this from\noutput you already have; (ii) how can you tell when a heat\nalert event *starts*?\n\n\nSolution\n\n\nThis turned out to be more messed-up than I\nthought. There is a detailed discussion below.\nThe codes EHAD, EHAE, HAE, HAU all indicate that there\nwas a heat alert on the day before. Only the codes HA\nand EHA can indicate the start of a heat alert\n(event). The problem is that HA and EHA sometimes\nindicate the start of a heat alert event and sometimes\none that is continuing.  You can check by looking at the\ndata that HA and EHA days can (though they don't always:\nsee below) have a non-heat-alert day before (below) them\nin the data file: for example, August 4, 2012 is an HA\nday, but August 3 of that year was not part of any kind\nof heat alert.\nI had intended the answer to be this:\n\n> So we get the total number of heat alert events by\n> totalling up the number of HA and EHA days:\n> $59+93=152$.                 \n\n\nThis is not right because there are some\nconsecutive EHA days, eg. 5--8 July 2010, so that EHA\nsometimes indicates the continuation of an extended heat\nalert and sometimes the start of one. I was expecting\nEHA to be used only for the start, and one of the other\ncodes to indicate a continuation. The same is\n(sometimes) true of HA.\nSo reasonable answers to the question as set include:\n\n\n* 93, the number of HAs\n\n* 59, the number of EHAs\n\n* 152, the number of HAs and EHAs combined\n\n* \"152 or less\", \"between 93 and 152\", ``between\n59 and 152'' to reflect that not all of these mark the\nstart of a heat alert event.\n\nAny of these, or something similar *with an               explanation of how you got your answer*, are\nacceptable. In your career as a data scientist, you will\noften run into this kind of thing, and it will be your\njob to do something with the data and *explain*\nwhat you did so that somebody else can decide whether\nthey believe you or not. A good explanation, even if it\nis not correct, will help you get at the truth because\nit will inspire someone to say \"in fact, it goes *this* way\",\nand then the two of you can jointly\nfigure out what's actually going on.\nDetailed discussion follows. If you have *any*\nambitions of working with data, you should try to follow\nthe paragraphs below, because they indicate how you\nwould get an *actual* answer to the question.\n\nI think the key is the number of days between one heat\nalert day and the next one. `dplyr` has a\nfunction `diff` that works out exactly this. Building\na pipeline, just because:\n\n::: {.cell}\n\n```{.r .cell-code}\nheat %>%\n  select(-text) %>%\n  mutate(daycount = as.numeric(date)) %>%\n  mutate(daydiff = abs(c(diff(daycount), 0)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 x 5\n      id date       code  daycount daydiff\n   <dbl> <date>     <chr>    <dbl>   <dbl>\n 1   232 2016-09-08 HAU      17052       1\n 2   231 2016-09-07 HAE      17051       1\n 3   230 2016-09-06 HA       17050      24\n 4   228 2016-08-13 EHAE     17026       1\n 5   227 2016-08-12 EHAE     17025       1\n 6   226 2016-08-11 HAU      17024       1\n 7   225 2016-08-10 HAE      17023       1\n 8   224 2016-08-09 HA       17022       4\n 9   222 2016-08-05 HAE      17018       1\n10   221 2016-08-04 HA       17017      11\n# i 190 more rows\n```\n:::\n:::\n\n             \n\nOof. I have some things to keep track of here:\n\n\n\n* Get rid of the `text`, since it serves no purpose here.\n\n* The `date` column is a proper `Date` (we checked).\n\n* Then I want the date as number of days; since it\nis a number of days internally, I just make it a number with\n`as.numeric`. \n\n* Then I use `diff` to get the difference\nbetween each date and the previous one, remembering to glue a 0 onto\nthe end so that I have the right number of differences.\n\n* Since the dates are most recent first, I take the absolute value\nso that the `daydiff` values are positive (except for the one\nthat is 0 on the end).\n\n\nStill with me? All right. You can check that the `daydiff`\nvalues are the number of days between the date on that line and the\nline below it. For example, there were 24 days between August 13 and\nSeptember 6.\n\nNow, when `daydiff` is 1, there was also a heat alert on the\nprevious day (the line below in the file), but when `daydiff` is\n*not* 1, that day must have been the *start* of a heat\nalert event. So if I count the non-1's, that will count the  number of heat\nalert events there were. (That includes the difference of 0 on the\nfirst day, the one at the end of the file.)\n\nThus my pipeline continues like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nheat %>%\n  select(-text) %>%\n  mutate(daycount = as.numeric(date)) %>%\n  mutate(daydiff = abs(c(diff(daycount), 0))) %>%\n  count(daydiff != 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n  `daydiff != 1`     n\n  <lgl>          <int>\n1 FALSE            121\n2 TRUE              79\n```\n:::\n:::\n\n             \n\nAnd that's how many actual heat alert events there were: 79, less even\nthan the number of HAs. So that tells me that a lot of my HAs and EHAs\nwere actually continuations of heat alert events rather than the start\nof them. I think I need to have a word with the City of Toronto about\ntheir data collection processes.\n\n`count` will count anything that is, or can be made into, a\ncategorical variable. It doesn't have to be one of the columns of your\ndata frame; here it is something that is either `TRUE` or\n`FALSE` about every row of the data frame.\n\nOne step further: what *is* the connection between the codes and\nthe start of heat alert events? We can figure that out now:\n\n::: {.cell}\n\n```{.r .cell-code}\nheat %>%\n  select(-text) %>%\n  mutate(daycount = as.numeric(date)) %>%\n  mutate(daydiff = abs(c(diff(daycount), 0))) %>%\n  mutate(start = (daydiff != 1)) %>%\n  count(code, start)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 3\n  code  start     n\n  <chr> <lgl> <int>\n1 EHA   FALSE    53\n2 EHA   TRUE      6\n3 EHAD  FALSE     1\n4 EHAE  FALSE    18\n5 HA    FALSE    20\n6 HA    TRUE     73\n7 HAE   FALSE    16\n8 HAU   FALSE    13\n```\n:::\n:::\n\n             \n\nI made a column `start` that is `TRUE` at the start of a\nheat alert event and `FALSE` otherwise, by comparing the days\nfrom the previous heat alert day with 1. Then I can make a\n`table`, or, as here, the `dplyr` equivalent with\n`count`.^[I did not know until just now that you could put  two variables in a count and you get counts of all the  combinations of them. Just goes to show the value of \"try it and  see\".] \nOr `group_by` and `summarize`. What this\nshows is that EHAD, EHAE, HAE and HAU *never* go with the start\nof a heat alert event (as they shouldn't). But look at the HAs and\nEHAs. For the HAs, 73 of them go with the start of an event, but 20 do\nnot. For the EHAs, just 6 of them go with the start, and 53 do\nnot. (Thus, counting just the HAs was very much a reasonable thing to\ndo.)\n\nThe 79 heat alert events that we found above had 73 of them starting\nwith an HA, and just 6 starting with an EHA.\nI wasn't quite sure how this would come out, but I knew it had\nsomething to do with the number of days between one heat alert day and\nthe next, so I calculated those first and then figured out what to do\nwith them.\n\n$\\blacksquare$\n\n(f) We are going to investigate how many heat alert\ndays\nthere were in each year. To do that, we have\nto extract the year from each of our dates. \n\n\nSolution\n\n\nThis will need the `lubridate` package, but you don't need to load it specifically because it is now loaded with the `tidyverse`:\n\n::: {.cell}\n\n```{.r .cell-code}\nheat %>% select(-text) %>% mutate(year = year(date)) %>% sample_n(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 4\n      id date       code   year\n   <dbl> <date>     <chr> <dbl>\n 1   219 2016-07-24 EHAE   2016\n 2    47 2005-07-10 HA     2005\n 3    67 2006-07-03 EHA    2006\n 4   176 2013-06-26 EHAD   2013\n 5   193 2015-07-28 HAE    2015\n 6   141 2012-06-19 HA     2012\n 7    55 2005-07-18 EHA    2005\n 8   119 2010-09-02 EHA    2010\n 9   102 2009-06-25 EHA    2009\n10     9 2001-08-09 EHA    2001\n```\n:::\n:::\n\n                 \nThat seems to have worked. I listed a random sample of rows to\nget back to previous years.\nHaving convinced myself that it worked, let me save it:\n\n::: {.cell}\n\n```{.r .cell-code}\nheat %>% select(-text) %>% mutate(year = year(date)) -> heat\n```\n:::\n\n                 \n\n$\\blacksquare$\n\n(g) Count the number of heat alert days for each\nyear, by tabulating the year variable.\nLooking at this table, would you say that there\nhave been more heat alert days in recent years? Explain\n(very) briefly. \n\n\nSolution\n\n\nCount them again:\n\n::: {.cell}\n\n```{.r .cell-code}\nheat %>% count(year)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 16 x 2\n    year     n\n   <dbl> <int>\n 1  2001     9\n 2  2002    16\n 3  2003     6\n 4  2004     2\n 5  2005    26\n 6  2006    17\n 7  2007    15\n 8  2008     9\n 9  2009     3\n10  2010    16\n11  2011    12\n12  2012    21\n13  2013    13\n14  2014     1\n15  2015    12\n16  2016    22\n```\n:::\n:::\n\n                 \nThere are various things you could say, most of which are likely to be\ngood. My immediate reaction is that most of the years with a lot of\nheat-alert days are in the last few years, and most of the years with\nnot many are near the start, so there is something of an upward\ntrend. Having said that, 2014 is unusually low (that was a cool\nsummer), and 2005 was unusually high. (Was that the\nsummer of the big power outage? I forget.^[I looked it up. It  was 2003, my first summer in Ontario. I realize as I write this that you may not be old enough to remember these years. Sometimes I forget how old I am.]\n\nYou could also reasonably say that there isn't much pattern: the\nnumber of heat-alert days goes up and down. In fact, anything that's\nnot obviously nonsense will do.\n\nI was thinking about making a graph of these frequencies against year,\nand sticking some kind of smooth trend on it. This uses the output we just got, which is itself a data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\nheat %>%\n  count(year) %>%\n  ggplot(aes(x = year, y = n)) + geom_point() + geom_smooth(se = F)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/heat-r-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n \nThe pattern is very scattered, as is commonly the case with\nenvironmental-science-type data, but there is a very small upward\ntrend. So it seems that either answer is justified, either \"there is no trend\" or\n\"there is something of an upward trend\".\n\nThe other thing I notice on this plot is that if there are a lot of\nheat-alert days one year, there will probably also be a lot in the next\nyear (and correspondingly if the number of heat-alert days is below\naverage: it tends to be below average again in the next year). This\npattern is known to time-series people as \"autocorrelation\" and\nindicates that the number of heat-alert days in one year and the next\nis not independent: if you know one year, you can predict the next\nyear. (Assessment of trend and autocorrelation are hard to untangle properly.)\n\nExtra 1: I learn from Environmental Science grad students (of whom we have a\nnumber at UTSC) that the approved measure of association is called the\nMann-Kendall correlation, which is the Kendall correlation of the data\nvalues with time. In the same way that we use the sign test when we\ndoubt normality, and it uses the data more crudely but safely, the\nregular (so-called Pearson) correlation assumes normality (of the\nerrors in the regression of one variable on the other), and when you\ndoubt that (as you typically do with this kind of data) you compute a\ndifferent kind of correlation with time. What the Kendall correlation\ndoes is to take each pair of observations and ask whether the trend\nwith time is uphill or downhill. For example, there were 3 heat-alert\ndays in 2009, 16 in 2010 and 12 in 2011. Between 2009 and 2010, the\ntrend is uphill (increasing with time), and also between 2009 and 2011\n(there were more heat-alert days in the later year), but between 2010\nand 2011 the trend is downhill. The idea of the Kendall correlation is\nyou take *all* the pairs of points, of which there are typically\nrather a lot, count up how many pairs are uphill and how many\ndownhill, and apply a formula to get a correlation between $-1$ and\n1. (If there are about an equal number of uphills and downhills, the\ncorrelation comes out near 0; if they are mostly uphill, the\ncorrelation is near 1, and if they are mostly downhill, the\ncorrelation is near $-1$.) It doesn't matter *how* uphill or\ndownhill the trends are, only the number of each, in the same way that\nthe sign test only counts the *number* of values above or below\nthe hypothesized median, not how far above or below they are.\n\nThis can be calculated, and even tested:\n\n::: {.cell}\n\n```{.r .cell-code}\nheat %>%\n  count(year) %>%\n  with(., cor.test(year, n, method = \"kendall\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in cor.test.default(year, n, method = \"kendall\"): Cannot compute exact\np-value with ties\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tKendall's rank correlation tau\n\ndata:  year and n\nz = 0.31612, p-value = 0.7519\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n0.05907646 \n```\n:::\n:::\n\n \n\nThe Mann-Kendall correlation is a thoroughly unremarkable 0.06, and\nwith only 16 data points, a null hypothesis that the correlation is\nzero is far from being rejected, P-value 0.7519 as shown. So this is\nno evidence of a time trend at all.\n\nExtra 2: I'd like to say a word about how I got these data. They came from\n[link](http://app.toronto.ca/opendata/heat_alerts/heat_alerts_list.json). If\nyou take a look there, there are no obvious rows and columns. This\nformat is called JSON. Look a bit more carefully and you'll see stuff\nlike this, repeated:\n\n\n```\n\n{\"id\":\"232\",\"date\":\"2016-09-08\",\"code\":\"HAU\",\n\"text\":\"Toronto's Medical Officer of Health has upgraded the Heat Warning to an Extended Heat Warning\"}\n\n```\n\n\none for each heat alert day. These are \"keys\" (on the left side of\nthe `:`) and \"values\" (on the right side).^[This is  the same kind of thing as a \"dictionary\" in Python.]  The keys\nare column headers (if the data were in a data frame) and the values\nare the data values that would be in that column. In JSON generally,\nthere's no need for the keys to be the same in every row, but if they\nare, as they are here, the data can be arranged in a data frame. How?\nRead on.\n\nI did this in R, using a package called `jsonlite`, with this code:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(jsonlite)\nurl <- \"http://app.toronto.ca/opendata/heat_alerts/heat_alerts_list.json\"\nheat <- fromJSON(url, simplifyDataFrame = T)\nhead(heat)\nwrite_csv(heat, \"heat.csv\")\n```\n:::\n\n \n\nAfter loading the package, I create a variable `url` that\ncontains the URL for the JSON file. The `fromJSON` line takes\nsomething that is JSON (which could be in text, a file or a URL) and\nconverts it to and saves it in a data frame. Finally, I save the data\nframe in a `.csv` file.  That's the `.csv` file you used. If you run that code, you'll get a `.csv` file of heat\nalerts right up to the present, and you can update my analysis.\n\nWhy `.csv`? If I had used `write_delim`, the values\nwould have been separated by spaces.  *But*, the `text` is\na sentence of several words, which are themselves separated by\nspaces. I could have had you read in everything else and not the\ntext, and then separated-by-spaces would have been fine, but I wanted\nyou to see the text so that you could understand the `code`\nvalues. So `.csv` is what it was.\n\n$\\blacksquare$\n \n \n\n\n\n\n## Isoflavones\n\n The plant called kudzu was imported to the US South from Japan. It is rich in isoflavones, which are believed to be beneficial for bones. In a study, rats were randomly assigned to one of three diets: one with a low dose of isoflavones from kudzu, one with a high dose, and a control diet with no extra isoflavones. At the end of the study, each rat's bone density was measured, in milligrams per square centimetre. The data as recorded are shown in [http://ritsokiguess.site/isoflavones.txt](http://ritsokiguess.site/isoflavones.txt).^[Evidently the units were chosen for ease of recording; had the values been in grams instead, the person recording the data would have had to put a 0 and a decimal point on the front of each value. This is the old meaning of the word coding; making the data values be whole numbers and/or small deviations from something makes them easier to record, and in pre-computer days easier to calculate with. You will also see the same word used for classifying survey responses into categories, which is similar but not quite the same thing.] There are 15 observations for each treatment, and hence 45 altogether.\n\nHere are some code ideas you might need to use later, all part of the `tidyverse`. You may need to find out how they work.\n\n\n- `col_names` (in the `read_` functions)\n- `convert` (in various `tidyverse` functions)\n- `fill`\n- `na_if`\n- `rename`\n- `separate_rows`\n- `skip` (in the `read_` functions)\n- `values_drop_na` (in the `pivot_` functions)\n\nIf you use any of these, *cite* the webpage(s) or other source(s) where you learned about them.\n\n\n\n\n(a) Take a look at the data file. Describe briefly what you see.\n\nSolution\n\n\nThe data values are (at least kind of) aligned in columns, suggesting `read_table`. There are up to six bone density values in each row, with a header that spans all of them (by the looks of it). The treatment column looks all right except that some of the rows are blank.\nThe blank treatments are the same as the ones in the row(s) above them, you can infer, because there are 15 observations for each treatment, six, six, and then three. (This is how a spreadsheet is often laid out: blank means the same as the previous line.)^[It shouldn't be, but it often is.]\n\nThis, you might observe, will need some tidying. \n\n\n$\\blacksquare$\n\n\n(b) Read in the data, using `read_table`, and get it into a tidy form, suitable for making a graph. This means finishing with (at least) a column of treatments with a suitable name (the treatments will be text) and a column of bone density values (numbers), one for each rat. You can have other columns as well; there is no obligation to get rid of them. Describe your process clearly enough that someone new to this data set would be able to understand what you have done and reproduce it on another similar dataset. Before you begin, think about whether or not you want to keep the column headers that are in the data file or not. (It can be done either way, but one way is easier than the other.)\n\nSolution\n\n\nThe tidying part is a fair bit easier to see if you *do not* read the column headers. A clue to this is that `bone_mineral_density` is not aligned with the values (of bone mineral density) below it. The next question is how to do that. You might remember `col_names=FALSE` from when the data file has no column headers at all, but here it *does* have headers; we just want to skip over them. Keep reading in the documentation for `read_table`, and you'll find an option `skip` that does exactly that, leading to:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/isoflavones.txt\"\nbmd0a <- read_table(my_url, col_names = FALSE, skip = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n-- Column specification --------------------------------------------------------\ncols(\n  X1 = col_character(),\n  X2 = col_double(),\n  X3 = col_double(),\n  X4 = col_double(),\n  X5 = col_double(),\n  X6 = col_double(),\n  X7 = col_double()\n)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 6 parsing failures.\nrow col  expected    actual                                                 file\n  2  -- 7 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  3  -- 7 columns 3 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  5  -- 7 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  6  -- 7 columns 3 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  8  -- 7 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n... ... ......... ......... ....................................................\nSee problems(...) for more details.\n```\n:::\n\n```{.r .cell-code}\nbmd0a\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 7\n  X1           X2    X3    X4    X5    X6    X7\n  <chr>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 control     228   207   234   220   217   228\n2 209         221   204   220   203   219    NA\n3 218         245   210    NA    NA    NA    NA\n4 low_dose    211   220   211   233   219   233\n5 226         228   216   225   200   208    NA\n6 198         208   203    NA    NA    NA    NA\n7 high_dose   250   237   217   206   247   228\n8 245         232   267   261   221   219    NA\n9 232         209   255    NA    NA    NA    NA\n```\n:::\n:::\n\nIf you miss the `skip`, the first row of \"data\" will be those column headers that were in the data file, and you really don't want that. [This link](https://readr.tidyverse.org/reference/read_delim.html) talks about both `col_names` and `skip`.\n\nThis, however, is looking very promising. A pivot_longer will get those columns of numbers into one column, which we can call something like `bmd`, and \\ldots but, not so fast. What about those blank treatments in `X1`? The first two blank ones are `control`, the next two are `low_dose` and the last two are `high_dose`. How do we fill them in? The word \"fill\" might inspire you to read up on `fill`. Except that this doesn't quite work, because it replaces *missings* with the non-missing value above them, and we have blanks, not missings.\n\nAll right, can we replace the blanks with missings, and then `fill` those? This might inspire you to go back to the list of ideas in the question, and find out what `na_if` does: namely, exactly this! Hence:\n\n::: {.cell}\n\n```{.r .cell-code}\nbmd0a %>% mutate(X1=na_if(X1, \"\")) %>% \nfill(X1) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 7\n  X1           X2    X3    X4    X5    X6    X7\n  <chr>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 control     228   207   234   220   217   228\n2 209         221   204   220   203   219    NA\n3 218         245   210    NA    NA    NA    NA\n4 low_dose    211   220   211   233   219   233\n5 226         228   216   225   200   208    NA\n6 198         208   203    NA    NA    NA    NA\n7 high_dose   250   237   217   206   247   228\n8 245         232   267   261   221   219    NA\n9 232         209   255    NA    NA    NA    NA\n```\n:::\n:::\n\nRun this one line at a time to see how it works. `fill` takes a column with missing values to replace, namely `X1`, and `na_if` takes two things: a column containing some values to make `NA`, and the values that should be made `NA`, namely the blank ones.\n\nSo that straightens out the treatment column. It needs renaming; you can do that now, or wait until later. I'm going to wait on that. \n\nYou need to organize the treatment column first, before you do the `pivot_longer`, or else that won't work.^[Data tidying has a lot of this kind of thing: try something, see that it doesn't work, figure out what went wrong, fix that, repeat. The work you hand in, or show to your boss, won't necessarily look very much like your actual process.]\n\nNow, we need to get one column of bone mass densities, instead of six. This you'll recognize as a standard `pivot_longer`, with one tweak: those missing values in `X5` through `X7`, which we want to get rid of. You might remember that this is what `values_drop_na` does:\n\n::: {.cell}\n\n```{.r .cell-code}\nbmd0a %>% mutate(X1=na_if(X1, \"\")) %>% \nfill(X1) %>% \npivot_longer(X2:X7, names_to=\"old\", values_to=\"bmd\", values_drop_na=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 39 x 3\n   X1      old     bmd\n   <chr>   <chr> <dbl>\n 1 control X2      228\n 2 control X3      207\n 3 control X4      234\n 4 control X5      220\n 5 control X6      217\n 6 control X7      228\n 7 209     X2      221\n 8 209     X3      204\n 9 209     X4      220\n10 209     X5      203\n# i 29 more rows\n```\n:::\n:::\n\nIf you didn't think of `values_drop_na`, do the pivot without, and then check that you have too many rows because the missings are still there (there are 45 rats but you have 54 rows), so add a `drop_na()` to the end of your pipe. The only missing values are in the column I called `bmd`.\n\nThis is almost there. We have a numeric column of bone mass densities, a column called `old` that we can ignore, and a treatment column with a stupid name that we can fix. I find `rename` backwards: the syntax is new name equals old name, so you start with the name that doesn't exist yet and finish with the one you want to get rid of:\n\n::: {.cell}\n\n```{.r .cell-code}\nbmd0a %>% mutate(X1=na_if(X1, \"\")) %>% \nfill(X1) %>% \npivot_longer(X2:X7, names_to=\"old\", values_to=\"bmd\", values_drop_na=TRUE) %>% \nrename(treatment=X1) -> bmd1b\nbmd1b\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 39 x 3\n   treatment old     bmd\n   <chr>     <chr> <dbl>\n 1 control   X2      228\n 2 control   X3      207\n 3 control   X4      234\n 4 control   X5      220\n 5 control   X6      217\n 6 control   X7      228\n 7 209       X2      221\n 8 209       X3      204\n 9 209       X4      220\n10 209       X5      203\n# i 29 more rows\n```\n:::\n:::\n\nDone!\n\nThe best way to describe this kind of work is to run your pipeline up to a point that needs explanation, describe what comes next, and then run the *whole pipeline* again up to the next point needing explanation, rinse and repeat. (This avoids creating unnecessary temporary dataframes, since the purpose of the pipe is to avoid those.)\n\nThe guideline for description is that if *you* don't know what's going to happen next, your reader won't know either. For me, that was these steps:\n\n- read the data file without row names and see how it looks\n- fix up the treatment column (convincing myself and the reader that we were now ready to pivot-longer)\n- do the `pivot_longer` and make sure it worked\n- rename the treatment column\n\nSo, I said there was another way. This happens to have a simple but clever solution. It starts from wondering \"what happens if I read the data file *with* column headers, the normal way? Do it and find out:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/isoflavones.txt\"\nbmd0b <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n-- Column specification --------------------------------------------------------\ncols(\n  treatment = col_character(),\n  bone_mineral_density = col_double()\n)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 9 parsing failures.\nrow col  expected    actual                                                 file\n  1  -- 2 columns 7 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  2  -- 2 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  3  -- 2 columns 3 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  4  -- 2 columns 7 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n  5  -- 2 columns 6 columns 'http://ritsokiguess.site/datafiles/isoflavones.txt'\n... ... ......... ......... ....................................................\nSee problems(...) for more details.\n```\n:::\n\n```{.r .cell-code}\nbmd0b\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 2\n  treatment bone_mineral_density\n  <chr>                    <dbl>\n1 control                    228\n2 209                        221\n3 218                        245\n4 low_dose                   211\n5 226                        228\n6 198                        208\n7 high_dose                  250\n8 245                        232\n9 232                        209\n```\n:::\n:::\n\nThis looks ... strange. There are two column headers, and so there are two columns. It so happened that this worked because the text `bone_mineral_density` is long enough to span all the columns of numbers. That second column is actually *text*: six or three numbers as text with spaces between them. \n\nThe first thing is, as before, to fill in the missing treatments, which is as above, but changing some names:\n\n::: {.cell}\n\n```{.r .cell-code}\nbmd0b %>% mutate(treatment=na_if(treatment, \"\")) %>% \nfill(treatment) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 2\n  treatment bone_mineral_density\n  <chr>                    <dbl>\n1 control                    228\n2 209                        221\n3 218                        245\n4 low_dose                   211\n5 226                        228\n6 198                        208\n7 high_dose                  250\n8 245                        232\n9 232                        209\n```\n:::\n:::\n\n\nThe way we learned in class for dealing with this kind of thing is `separate`. It is rather unwieldy here since we have to split `bone_mineral_density` into six (temporary) things:\n\n::: {.cell}\n\n```{.r .cell-code}\nbmd0b %>% mutate(treatment=na_if(treatment, \"\")) %>% \nfill(treatment) %>% \nseparate(bone_mineral_density, into = c(\"z1\", \"z2\", \"z3\", \"z4\", \"z5\", \"z6\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Expected 6 pieces. Missing pieces filled with `NA` in 9 rows [1, 2, 3, 4, 5, 6,\n7, 8, 9].\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 7\n  treatment z1    z2    z3    z4    z5    z6   \n  <chr>     <chr> <chr> <chr> <chr> <chr> <chr>\n1 control   228   <NA>  <NA>  <NA>  <NA>  <NA> \n2 209       221   <NA>  <NA>  <NA>  <NA>  <NA> \n3 218       245   <NA>  <NA>  <NA>  <NA>  <NA> \n4 low_dose  211   <NA>  <NA>  <NA>  <NA>  <NA> \n5 226       228   <NA>  <NA>  <NA>  <NA>  <NA> \n6 198       208   <NA>  <NA>  <NA>  <NA>  <NA> \n7 high_dose 250   <NA>  <NA>  <NA>  <NA>  <NA> \n8 245       232   <NA>  <NA>  <NA>  <NA>  <NA> \n9 232       209   <NA>  <NA>  <NA>  <NA>  <NA> \n```\n:::\n:::\n\nThis works, though if you check, there's a warning that some of the rows don't have six values. However, these have been replaced by missings, which is just fine. From here, we do exactly what we did before: pivot-longer all the columns I called `z`-something, and get rid of the missings.\n\nHaving thought of `separate`, maybe you're now wondering what `separate_rows` does. It turns out that it bypasses the business of creating extra columns and then pivoting them longer, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nbmd0b %>% mutate(treatment=na_if(treatment, \"\")) %>% \nfill(treatment)  %>% \nseparate_rows(bone_mineral_density, convert = TRUE) -> bmd1a\nbmd1a\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 2\n  treatment bone_mineral_density\n  <chr>                    <int>\n1 control                    228\n2 209                        221\n3 218                        245\n4 low_dose                   211\n5 226                        228\n6 198                        208\n7 high_dose                  250\n8 245                        232\n9 232                        209\n```\n:::\n:::\n\nBoom! This takes all the things in that mess in `bone_mineral_density`, splits them up into individual data values, and puts them one per row back into the same column. The `convert` is needed because otherwise the values in the second column would be text and you wouldn't be able to plot them. (If you don't see that, use a mutate to convert the column into the numerical version of itself.)\n\n\n$\\blacksquare$\n\n\n(c) The statistician on this study is thinking about running an ordinary analysis of variance to compare the bone mineral density for the different treatments. Obtain a plot from your tidy dataframe that will help her decide whether that is a good idea.\n\nSolution\n\n\nThe key issues here are whether the values within each treatment group are close enough to normally distributed, and, if they are, whether the spreads are close enough to equal. The best plot is therefore a normal quantile plot of each of the three groups, in facets. You can do this *without* `scales=\"free\"`:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(bmd1b, aes(sample=bmd)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~treatment)\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/isoflavones-9-1.pdf){fig-pos='H'}\n:::\n:::\n\nThe value of doing it this way is that you also get a sense of variability, from the slopes of the lines, or from how much of each box is filled vertically. (Here, the high-dose values are more spread-out than the other two groups, which are similar in spread.)\n\nYou could also do it *with* `scales = \"free\"`:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(bmd1b, aes(sample=bmd)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~treatment, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/isoflavones-10-1.pdf){fig-pos='H'}\n:::\n:::\n\nThe value of doing it *this* way is that you fill the facets (what I called \"not wasting real estate\" on an earlier assignment), and so you get a better assessment of normality, but the downside is that you will need another plot, for example a boxplot (see below) to assess equality of spreads if you are happy with the normality.\n\nI'm happy with either way of making the normal quantile plots, as long as you have a *reason* for your choice, coming from what you will be using the normal quantile plot for. You might not think of saying that here as you do it, but when you do the next part, you may realize that you need to assess equality of spreads, and in that case you should come back here and add a reason for using or not using `scales = \"free\"`. \n\nThe next-best graph here is boxplots:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(bmd1b, aes(x=treatment, y=bmd)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/isoflavones-11-1.pdf){fig-pos='H'}\n:::\n:::\n\nThis is not so good because it doesn't address normality as directly (just giving you a general sense of shape). On the other hand, you can assess spread directly with a boxplot; see discussion above.\n\nThe grader is now probably thoroughly confused, so let me summarize possible answers in order of quality:\n\n1. A normal quantile plot of all three groups, using `scales = \"free\"` or not, with a good reason. (If with `scales = \"free\"`, and there needs to be a comparison of spread, there needs to be a boxplot or similar below as well. That's what I meant by \"any additional graphs\" in the next part.)\n1. A normal quantile plot of all three groups, using `scales = \"free\"` or not, *without* a good reason.\n1. A side-by-side boxplot. Saying in addition that normality doesn't matter so much because we have moderate-sized samples of 15 and therefore that boxplots are good enough moves this answer up a place.\n\nNote that getting the graph is (relatively) easy once you have the tidy data, but is impossible if you don't! This is the way the world of applied statistics works; without being able to get your data into the right form, you won't be able to do *anything* else. This question is consistent with that fact; I'm not going to give you a tidy version of the data so that you can make some graphs. The point of this question is to see whether you can get the data tidy enough, and if you can, you get the bonus of being able to do something straightforward with it. \n\n\n$\\blacksquare$\n\n\n(d) Based on your graph, and any additional graphs you wish to draw, what analysis would you recommend for this dataset? Explain briefly. (Don't do the analysis.)\n\n\nSolution\n\n\nMake a decision about normality first. You need *all three* groups to be sufficiently normal. I don't think there's any doubt about the high-dose and low-dose groups; these are if anything *short*-tailed, which is not a problem for the ANOVA. You might find that the control group is OK too; make a call. Or you might find it skewed to the right, something suggested rather more by the boxplot. My take, from looking at the normal quantile plot, is that the highest value in the control group is a little too high, but with a sample size of 15, the Central Limit Theorem will take care of that. For yourself, you can find a bootstrapped sampling distribution of the sample mean for the control group and see how normal it looks.\n\nIf you are not happy with the normality, recommend Mood's median test.\n\nIf you are OK with the normality, you need to assess equal spreads. You can do this from a boxplot, where the high-dose group clearly has bigger spread. Or, if you drew normal quantile plots *without* `scales = \"free\"`, compare the slopes of the lines. This means that you need to recommend a Welch ANOVA.\n\nIf your normal quantile plots looked like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(bmd1b, aes(sample=bmd)) + stat_qq() + stat_qq_line() +\nfacet_wrap(~treatment, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/isoflavones-12-1.pdf){fig-pos='H'}\n:::\n:::\n\nthe only way to assess spread is to make another plot, and for this job, the boxplot is best.\n\nExtra 1: the bootstrapped sampling distribution of the sample mean for the control group goes this way:\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmd1b %>% \nfilter(treatment == \"control\") -> d\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(d$bmd, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) %>% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/isoflavones-14-1.pdf){fig-pos='H'}\n:::\n:::\n\nNo problems there. The Welch ANOVA is fine.\n\nExtra 2: You might be curious how the analysis comes out. Here is Welch:\n\n::: {.cell}\n\n```{.r .cell-code}\noneway.test(bmd~treatment, data=bmd1b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne-way analysis of means (not assuming equal variances)\n\ndata:  bmd and treatment\nF = 2.5385, num df = 8.000, denom df = 7.718, p-value = 0.1082\n```\n:::\n:::\n\nNot all the same means, so use Games-Howell to explore:\n\n::: {.cell}\n\n```{.r .cell-code}\ngamesHowellTest(bmd~factor(treatment), data = bmd1b)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n\nWarning in ptukey(abs(qval), nmeans = k, df = df, lower.tail = FALSE): NaNs\nproduced\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n\tPairwise comparisons using Games-Howell test\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ndata: bmd by factor(treatment)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n          198  209  218 226  232 245  control high_dose\n209       0.75 -    -   -    -   -    -       -        \n218       -    -    -   -    -   -    -       -        \n226       0.73 1.00 -   -    -   -    -       -        \n232       -    -    -   -    -   -    -       -        \n245       0.20 0.41 -   0.51 -   -    -       -        \ncontrol   0.13 0.80 -   0.97 -   0.77 -       -        \nhigh_dose 0.15 0.51 -   0.70 -   1.00 0.97    -        \nlow_dose  0.18 0.89 -   0.99 -   0.72 1.00    0.94     \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nP value adjustment method: none\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nalternative hypothesis: two.sided\n```\n:::\n:::\n\nHigh dose is significantly different from both the other two, which are not significantly different from each other.\n\nMood's median test, for comparison:\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian_test(bmd1b, bmd, treatment)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$table\n           above\ngroup       above below\n  198           0     2\n  209           1     3\n  218           1     1\n  226           2     3\n  232           1     1\n  245           4     1\n  control       3     2\n  high_dose     4     2\n  low_dose      2     3\n\n$test\n       what     value\n1 statistic 6.0666667\n2        df 8.0000000\n3   P-value 0.6397643\n```\n:::\n:::\n\nNot *any* significant differences, although it is a close thing.\n\nThe table of aboves and belows suggests the same thing as the Welch test: the high-dose values are mainly high, and the others are mostly low. But with these sample sizes it is not strong enough evidence. My guess is that the median test is lacking power compared to the Welch test; having seen that the Welch test is actually fine, it is better to use that here.^[This is the opposite way to the usual: when two tests disagree, it is usually the one with fewer assumptions that is preferred, but in this case, the Welch ANOVA is fine, and the median test fails to give significance because it is not using the data as efficiently.]\n\n\n\n$\\blacksquare$\n\n\n\n\n\n\n\n## Jocko's Garage\n\n Insurance adjusters are concerned that Jocko's Garage is giving estimates for repairing car damage that are too high. To see whether this is indeed the case, ten cars that had been in collisions were taken to both Jocko's Garage and another garage, and the two estimates for repair were recorded. The data as recorded are [here](http://ritsokiguess.site/datafiles/jocko.txt).\n\n\n\n(a) Take a look at the data file (eg. by using your web browser). How are the data laid out? Do there appear to be column headers?\n\nSolution\n\n\nThe data are laid out in aligned columns, so that we will need to use `read_table` to read it in. There are no column headers, since there is no line at the top of the file saying what each column represents. (The fact that I was asking about column headers is kind of a clue that something non-standard is happening there.)\n\n\n$\\blacksquare$\n\n\n(b) Read in and display the data file, bearing in mind what you just concluded about it. What names did the columns acquire?\n\nSolution\n\n\nAs mentioned above, you'll need `read_table`, plus `col_names=FALSE` to *not* read the first row as column names:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/jocko.txt\"\ncars0 <- read_table(my_url, col_names = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n-- Column specification --------------------------------------------------------\ncols(\n  X1 = col_character(),\n  X2 = col_character(),\n  X3 = col_double(),\n  X4 = col_double(),\n  X5 = col_double(),\n  X6 = col_double(),\n  X7 = col_double()\n)\n```\n:::\n\n```{.r .cell-code}\ncars0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 7\n  X1    X2       X3    X4    X5    X6    X7\n  <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 a     Car       1     2     3     4     5\n2 a     Jocko  1375  1550  1250  1300   900\n3 a     Other  1250  1300  1250  1200   950\n4 b     Car       6     7     8     9    10\n5 b     Jocko  1500  1750  3600  2250  2800\n6 b     Other  1575  1600  3300  2125  2600\n```\n:::\n:::\n\nThe column names have become `X1` through `X7`. You'll need to work with these in a minute, so it is good to be aware of that now.\n\nI used a \"temporary\" name for my dataframe since we are going to be tidying it before we do anything with it, and I'm saving the \"good\" name `cars` for the tidy one.\n\n\n$\\blacksquare$\n\n\n(c) Make this data set tidy. That is, you need to end up with columns containing the repair cost estimates at each of the two garages and also identifying the cars, with each observation on one row. Describe your thought process. (It needs to be possible for the reader to follow your description and understand why it works.)\nSave your tidy dataframe.\n\nSolution\n\n\nThis looks very far from tidy right now. The things in `X2` look like they will need to be variable names eventually, but there are two copies of them, and there are also five columns of data values that need eventually to become three. Having all the data values in *one* column might be a useful place to start:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars0 %>% pivot_longer(X3:X7, names_to=\"old_cols\", values_to=\"values\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 30 x 4\n   X1    X2    old_cols values\n   <chr> <chr> <chr>     <dbl>\n 1 a     Car   X3            1\n 2 a     Car   X4            2\n 3 a     Car   X5            3\n 4 a     Car   X6            4\n 5 a     Car   X7            5\n 6 a     Jocko X3         1375\n 7 a     Jocko X4         1550\n 8 a     Jocko X5         1250\n 9 a     Jocko X6         1300\n10 a     Jocko X7          900\n# i 20 more rows\n```\n:::\n:::\n\nThis is tidier, but it's now *too long*: this has 30 rows but there are only 10 cars, or, depending on your point of view, there are 20 observations on 10 individual cars, so you could justify (in some way) having 20 rows, but not 30.\n\nNow, therefore, we need to pivot *wider*. But to get to this point, we had to try pivoting longer to see what it did, and then go from there. I don't think it's  at all obvious that this is what will  happen, so I think you need to do a pivot-longer first, *talk about it*, and then move on.\n\nFrom here, we want to make columns whose names are the things in `X2`, and whose values are the things in `values`. This is exactly what `pivot_wider` does, so add that to our pipe:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars0 %>% pivot_longer(X3:X7, names_to=\"names\", values_to=\"values\") %>% \npivot_wider(names_from = X2, values_from = values) -> cars\ncars\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 5\n   X1    names   Car Jocko Other\n   <chr> <chr> <dbl> <dbl> <dbl>\n 1 a     X3        1  1375  1250\n 2 a     X4        2  1550  1300\n 3 a     X5        3  1250  1250\n 4 a     X6        4  1300  1200\n 5 a     X7        5   900   950\n 6 b     X3        6  1500  1575\n 7 b     X4        7  1750  1600\n 8 b     X5        8  3600  3300\n 9 b     X6        9  2250  2125\n10 b     X7       10  2800  2600\n```\n:::\n:::\n\nThis is now tidy: one row for each of the 10 cars, one column containing the repair estimates for each car at each of the two garages, and a column identifying the cars. I think this is best because this is a matched-pairs study, and so you want the two measurements for each individual car in columns next to each other (for `t.test` with `paired=TRUE`). \n\nI think it is best to show the whole pipeline here, even though you are making R work a little harder, rather than having to make up a temporary variable name for the output from `pivot_longer` (that you are never going to look at again after this).\n\n\n\n\n\nIf you thought there were 20 observations, you have a bit more work to do (that you will have to undo later to get the right graph), namely:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars %>% pivot_longer(c(Jocko, Other), names_to=\"garage\", values_to=\"estimate\") -> cars1\ncars1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 5\n   X1    names   Car garage estimate\n   <chr> <chr> <dbl> <chr>     <dbl>\n 1 a     X3        1 Jocko      1375\n 2 a     X3        1 Other      1250\n 3 a     X4        2 Jocko      1550\n 4 a     X4        2 Other      1300\n 5 a     X5        3 Jocko      1250\n 6 a     X5        3 Other      1250\n 7 a     X6        4 Jocko      1300\n 8 a     X6        4 Other      1200\n 9 a     X7        5 Jocko       900\n10 a     X7        5 Other       950\n11 b     X3        6 Jocko      1500\n12 b     X3        6 Other      1575\n13 b     X4        7 Jocko      1750\n14 b     X4        7 Other      1600\n15 b     X5        8 Jocko      3600\n16 b     X5        8 Other      3300\n17 b     X6        9 Jocko      2250\n18 b     X6        9 Other      2125\n19 b     X7       10 Jocko      2800\n20 b     X7       10 Other      2600\n```\n:::\n:::\n\nThis would be the right thing to do if you had independent observations (that is, *20 different* cars, and you randomly choose a garage to send each one to). But you can have a car assessed for repair without actually repairing it, so it makes more sense to send each car to both garages, and compare like with like. Compare the kids learning to read; once a child has learned to read, you can't teach them to read again, so that study had to be done with two independent samples.\n\nExtra: I thought about starting by making the dataframe even wider:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars0 %>% pivot_wider(names_from = X2, values_from = X3:X7)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 16\n  X1    X3_Car X3_Jocko X3_Other X4_Car X4_Jocko X4_Other X5_Car X5_Jocko\n  <chr>  <dbl>    <dbl>    <dbl>  <dbl>    <dbl>    <dbl>  <dbl>    <dbl>\n1 a          1     1375     1250      2     1550     1300      3     1250\n2 b          6     1500     1575      7     1750     1600      8     3600\n# i 7 more variables: X5_Other <dbl>, X6_Car <dbl>, X6_Jocko <dbl>,\n#   X6_Other <dbl>, X7_Car <dbl>, X7_Jocko <dbl>, X7_Other <dbl>\n```\n:::\n:::\n\nThis is sort of the right thing, but there are repeat columns now, depending on where the data values came from in `cars0`. What we want to do now is *some* kind of `pivot_longer`, creating three columns called `Car`, `Jocko`, and `Other`. If we only had *one* kind of thing to make longer, this would be a standard `pivot_longer`. But we have three. There are two \"extras\" to `pivot_longer` that will get you to the right place. The first one is to give multiple inputs to `names_to`, because the column names encode two things: where in the original data frame the value came from (which is now junk to us), and what the value actually represents, which we definitely *do* want to keep. I don't have a good name for it, though, so I'll call it `z` for now. Note that we need a `names_sep` that says what the two things in the column names are separated by, the underscore that the `pivot_wider` put in there: \n\n::: {.cell}\n\n```{.r .cell-code}\ncars0 %>% pivot_wider(names_from = X2, values_from = X3:X7) %>% \npivot_longer(-X1, names_to = c(\"junk\", \"z\"), names_sep=\"_\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 30 x 4\n   X1    junk  z     value\n   <chr> <chr> <chr> <dbl>\n 1 a     X3    Car       1\n 2 a     X3    Jocko  1375\n 3 a     X3    Other  1250\n 4 a     X4    Car       2\n 5 a     X4    Jocko  1550\n 6 a     X4    Other  1300\n 7 a     X5    Car       3\n 8 a     X5    Jocko  1250\n 9 a     X5    Other  1250\n10 a     X6    Car       4\n# i 20 more rows\n```\n:::\n:::\n\nThis is now exactly what I got by *starting* with `pivot_longer`, and so the same `pivot_wider` that I finished with before will tidy this up:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars0 %>% pivot_wider(names_from = X2, values_from = X3:X7) %>% \npivot_longer(-X1, names_to = c(\"junk\", \"z\"), names_sep=\"_\") %>% \npivot_wider(names_from = z, values_from = value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 5\n   X1    junk    Car Jocko Other\n   <chr> <chr> <dbl> <dbl> <dbl>\n 1 a     X3        1  1375  1250\n 2 a     X4        2  1550  1300\n 3 a     X5        3  1250  1250\n 4 a     X6        4  1300  1200\n 5 a     X7        5   900   950\n 6 b     X3        6  1500  1575\n 7 b     X4        7  1750  1600\n 8 b     X5        8  3600  3300\n 9 b     X6        9  2250  2125\n10 b     X7       10  2800  2600\n```\n:::\n:::\n\nThis is now tidy, so you have achieved what you set out to do, but you have not done it the best way, so you should expect to lose a little something.\n\nThis kind of longer-then-wider happens often enough that there is an option in `pivot_longer` to do it in one step. Let's remind ourselves of where we were:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncars0 %>% pivot_wider(names_from = X2, values_from = X3:X7) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 16\n  X1    X3_Car X3_Jocko X3_Other X4_Car X4_Jocko X4_Other X5_Car X5_Jocko\n  <chr>  <dbl>    <dbl>    <dbl>  <dbl>    <dbl>    <dbl>  <dbl>    <dbl>\n1 a          1     1375     1250      2     1550     1300      3     1250\n2 b          6     1500     1575      7     1750     1600      8     3600\n# i 7 more variables: X5_Other <dbl>, X6_Car <dbl>, X6_Jocko <dbl>,\n#   X6_Other <dbl>, X7_Car <dbl>, X7_Jocko <dbl>, X7_Other <dbl>\n```\n:::\n:::\n\nThe second part of those funky column names needs to become *the names of our new columns*. To make that happen in one step, you put the special indicator `.value` in where we had `z` before:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars0 %>% pivot_wider(names_from = X2, values_from = X3:X7) %>% \npivot_longer(-X1, names_to = c(\"junk\", \".value\"), names_sep=\"_\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 5\n   X1    junk    Car Jocko Other\n   <chr> <chr> <dbl> <dbl> <dbl>\n 1 a     X3        1  1375  1250\n 2 a     X4        2  1550  1300\n 3 a     X5        3  1250  1250\n 4 a     X6        4  1300  1200\n 5 a     X7        5   900   950\n 6 b     X3        6  1500  1575\n 7 b     X4        7  1750  1600\n 8 b     X5        8  3600  3300\n 9 b     X6        9  2250  2125\n10 b     X7       10  2800  2600\n```\n:::\n:::\n\nand as if by magic, we have tidiness. It's best to discover this and do it in two steps, though by starting with `pivot_wider` you have made it more difficult for yourself. By starting with `pivot_longer`, it is a very standard longer-then-wider, and there is nothing extra you have to learn. (The column `X1` I added to the data so that `pivot_wider` would work smoothly. See what happens if you remove it with `select(-X1)` before you start pivoting.)\n\nThere is usually a relatively simple way to do these, and if your way is complicated, that is an invitation to try it again a different way. I don't think there's a way to do it in *one* step, though, because those things in `X2` have to get to column names somehow, and they can only do so by being attached to which original column the values came from. \n\nAll of these ideas are [here](https://tidyr.tidyverse.org/articles/pivot.html), which is a dense read, but worth working through to see what is possible. This problem is of the type in \"Longer, then wider\".\n\n\n$\\blacksquare$\n\n\n(d) Make a suitable graph to assess the comparison of interest, and say briefly what your graph tells you.\n\nSolution\n\n\nYou might be tempted to look at `cars`, see two quantitative variables, and think \"scatterplot\":\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(cars, aes(x=Jocko, y=Other)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/jocko-10-1.pdf){fig-pos='H'}\n:::\n:::\nThis says that a repair that is more expensive at one garage is more expensive at the other as well, which is true, but it's an answer to the *wrong question*. We care about whether Jocko's Garage is more expensive than the other one *on the same car*. To rescue the scatterplot, you can add the line $y=x$ to the graph and see which side of the line the points are, which you might have to find out how to do:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(cars, aes(x=Jocko, y=Other)) + geom_point() + geom_abline(slope = 1, intercept = 0)\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/jocko-11-1.pdf){fig-pos='H'}\n:::\n:::\n\nMore of the points are below and to the right of the line, indicating that Jocko's Garage is typically more expensive (in the cases where the other garage is more expensive, there is not much in it). \n\nThere is a more direct approach here, based on the idea that a matched pairs test looks at the differences between the two estimates for each car: work out the differences, and make a *one*-sample plot of them:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars %>% mutate(diff=Jocko-Other) %>% \nggplot(aes(x=diff)) + geom_histogram(bins = 4)\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/jocko-12-1.pdf){fig-pos='H'}\n:::\n:::\nMost of the differences, this way around, are positive, so the indication is that Jocko's Garage is indeed more expensive. Don't have too many bins.\n\nA one-sample boxplot of the differences would also work:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars %>% mutate(diff=Jocko-Other) %>% \nggplot(aes(x=1, y=diff)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/jocko-13-1.pdf){fig-pos='H'}\n:::\n:::\nThis tells you that at least 75% of the differences are positive.\n\nIf you ended up with my `cars1`:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 5\n   X1    names   Car garage estimate\n   <chr> <chr> <dbl> <chr>     <dbl>\n 1 a     X3        1 Jocko      1375\n 2 a     X3        1 Other      1250\n 3 a     X4        2 Jocko      1550\n 4 a     X4        2 Other      1300\n 5 a     X5        3 Jocko      1250\n 6 a     X5        3 Other      1250\n 7 a     X6        4 Jocko      1300\n 8 a     X6        4 Other      1200\n 9 a     X7        5 Jocko       900\n10 a     X7        5 Other       950\n11 b     X3        6 Jocko      1500\n12 b     X3        6 Other      1575\n13 b     X4        7 Jocko      1750\n14 b     X4        7 Other      1600\n15 b     X5        8 Jocko      3600\n16 b     X5        8 Other      3300\n17 b     X6        9 Jocko      2250\n18 b     X6        9 Other      2125\n19 b     X7       10 Jocko      2800\n20 b     X7       10 Other      2600\n```\n:::\n:::\n\nthis is \"obviously\" a boxplot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(cars1, aes(x=garage, y=estimate)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/jocko-15-1.pdf){fig-pos='H'}\n:::\n:::\nexcept that you have not used the fact that each group is measurements on the *same* 10 cars. Here is a way to rescue that:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(cars1, aes(x=garage, y=estimate, group=Car)) + geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/jocko-16-1.pdf){fig-pos='H'}\n:::\n:::\n\nThe majority of the lines go downhill, so Jocko's Garage is more expensive most of the time. (The lines are really another way to look at the differences.) This last graph I would be good with, since it shows which pairs of measurements are related because of being on the same cars.\n\n\n$\\blacksquare$\n\n\n(e) Carry  out a test to make an appropriate comparison of the mean estimates. What do you conclude, in the context of the data?\n\nSolution\n\n\nComparing means requires the right flavour of $t$-test, in this case a matched-pairs one, with a one-sided alternative, since we were concerned that the Jocko estimates were bigger. In a matched pairs test, `alternative` says how the first column you name compares with the other one. If your columns are the opposite way to mine, your `alternative` needs to be \"less\":\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(cars, t.test(Jocko, Other, paired = TRUE, alternative = \"greater\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  Jocko and Other\nt = 2.8749, df = 9, p-value = 0.009164\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 40.76811      Inf\nsample estimates:\nmean difference \n          112.5 \n```\n:::\n:::\n\nRemember that this flavour of $t$-test doesn't take a `data=`, so you need to use `with` or dollar signs.\n\nThe P-value is actually just less than 0.01, so we can definitely conclude that the Jocko estimates are bigger on average.\n\nIf you calculated the differences earlier, feel free to use them here:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars %>% mutate(diff=Jocko-Other) %>% \nwith(., t.test(diff, mu=0, alternative = \"greater\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  diff\nt = 2.8749, df = 9, p-value = 0.009164\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 40.76811      Inf\nsample estimates:\nmean of x \n    112.5 \n```\n:::\n:::\nSaving the data frame with the differences in it is probably smart.\n\nAgain, if you got to `cars1`, you might think to do this:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(estimate~garage, data=cars1, alternative=\"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  estimate by garage\nt = 0.32056, df = 17.798, p-value = 0.3761\nalternative hypothesis: true difference in means between group Jocko and group Other is greater than 0\n95 percent confidence interval:\n -496.4343       Inf\nsample estimates:\nmean in group Jocko mean in group Other \n             1827.5              1715.0 \n```\n:::\n:::\n\nbut you would be *wrong*, because the two groups are not independent (they're the same cars at each garage). You have also lost the significant result, because some of the repairs are more expensive than others (at both garages), and this introduces extra variability that this test does not account for.\n\nI said to compare the means, so I don't want a sign test here. If you think we should be doing one, you'll need to make the case for it properly: first, calculate and plot the differences and make the case that they're not normal enough. I see left-skewness in the histogram of differences, but personally I don't find this bad enough to worry about. If you do, make that case (but, a sample of size 10 even from a normal distribution might look this skewed) and then run the right test:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars %>% mutate(diff=Jocko-Other) %>% \nsign_test(diff, 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$above_below\nbelow above \n    2     7 \n\n$p_values\n  alternative    p_value\n1       lower 0.98046875\n2       upper 0.08984375\n3   two-sided 0.17968750\n```\n:::\n:::\n\nThe upper-tail P-value is the one you want (explain why), and this is not quite significant. This is different from the correct $t$-test for a couple of reasons: there is probably not much power with this small sample, and the two estimates that are higher at the Other garage are not much higher, which the $t$-test accounts for but the sign test does not.\n\n\n$\\blacksquare$\n\n\n\n\n\n\n\n## Tidying electricity consumption\n\n How does the consumption of electricity depend on  temperature?  To find out, a short-term study was carried out by a utility company based in a certain area. For a period of two years, the average monthly temperature was recorded (in degrees Fahrenheit), the mean daily demand for electricity per household (in kilowatt hours), and the cost per kilowatt  hour of electricity for that year (8 cents for the first year and 10 cents for the second, which it will be easiest to treat as categorical). \n\nThe data were laid out in an odd way, as shown in [http://ritsokiguess.site/datafiles/utils.txt](http://ritsokiguess.site/datafiles/utils.txt), in aligned columns: the twelve months of temperature were laid out on *two* lines for the first year, then the twelve months of consumption for the first year on the next two lines, and then four more lines for the second year laid out the same way. Thus the temperature of 31 in the first line goes with the consumption of 55 in the *third* line, and the last measurements for that year are the 78 at the end of the second line (temperature) and 73 at the end of the fourth line (consumption).  Lines 5 through 8 of the data file are the same thing for the second year (when electricity was more expensive). \n\nThe data seem to have been laid out in order of temperature, rather than in order of months, which I would have thought would make more sense. But this is what we have.\n\n\n\n(a) Read in and display the data file, bearing in mind that it has *no column names*.\n\nSolution\n\n\nThat means `col_names = FALSE` when reading in. I gave this a \"disposable\" name, saving the good name `utils` for the tidy version:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/utils.txt\"\nutils0 <- read_table(my_url, col_names = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n-- Column specification --------------------------------------------------------\ncols(\n  X1 = col_character(),\n  X2 = col_character(),\n  X3 = col_double(),\n  X4 = col_double(),\n  X5 = col_double(),\n  X6 = col_double(),\n  X7 = col_double(),\n  X8 = col_double()\n)\n```\n:::\n\n```{.r .cell-code}\nutils0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 8\n  X1      X2             X3    X4    X5    X6    X7    X8\n  <chr>   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 8cents  temperature    31    34    39    42    47    56\n2 8cents  temperature    62    66    68    71    75    78\n3 8cents  consumption    55    49    46    47    40    43\n4 8cents  consumption    41    46    44    51    62    73\n5 10cents temperature    32    36    39    42    48    56\n6 10cents temperature    62    66    68    72    75    79\n7 10cents consumption    50    44    42    42    36    40\n8 10cents consumption    39    44    40    44    50    55\n```\n:::\n:::\n\nThe columns have acquired names `X1` through `X8`. It doesn't really matter what these names *are*, but as we will see shortly, it matters that they *have* names.\n\n\n$\\blacksquare$\n\n\n(b) Arrange these data tidily, so that there is a column of price (per kilowatt hour), a column of temperatures, and a column of consumptions. Describe your process, including why you got list-columns (if you did) and what you did about them (if necessary).\n\nSolution\n\n\nThis question is asking about your process as well as your answer, so I think it's best to build a pipeline one step at a time (which corresponds in any case to how you would figure out what to do). The first step seems to be to make longer, for example getting all those numbers in one column. I'm not quite sure what to call the new columns, so I'll make up some names and figure things out later:\n\n::: {.cell}\n\n```{.r .cell-code}\nutils0 %>% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 48 x 4\n   X1     X2          col   value\n   <chr>  <chr>       <chr> <dbl>\n 1 8cents temperature X3       31\n 2 8cents temperature X4       34\n 3 8cents temperature X5       39\n 4 8cents temperature X6       42\n 5 8cents temperature X7       47\n 6 8cents temperature X8       56\n 7 8cents temperature X3       62\n 8 8cents temperature X4       66\n 9 8cents temperature X5       68\n10 8cents temperature X6       71\n# i 38 more rows\n```\n:::\n:::\n\nIf you scroll down, `X2` has consumptions as well as temperatures, so we need to get that straightened out.\n\nThis, so far, is actually a lot like the weather one in lecture (where we had a max and a min temperature), and the solution is the same: follow up with a `pivot_wider` to get the temperatures and consumptions in their own columns:\n\n::: {.cell}\n\n```{.r .cell-code}\nutils0 %>% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %>% \npivot_wider(names_from = X2, values_from = value) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n  dplyr::group_by(X1, col, X2) %>%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n  dplyr::filter(n > 1L)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 12 x 4\n   X1      col   temperature consumption\n   <chr>   <chr> <list>      <list>     \n 1 8cents  X3    <dbl [2]>   <dbl [2]>  \n 2 8cents  X4    <dbl [2]>   <dbl [2]>  \n 3 8cents  X5    <dbl [2]>   <dbl [2]>  \n 4 8cents  X6    <dbl [2]>   <dbl [2]>  \n 5 8cents  X7    <dbl [2]>   <dbl [2]>  \n 6 8cents  X8    <dbl [2]>   <dbl [2]>  \n 7 10cents X3    <dbl [2]>   <dbl [2]>  \n 8 10cents X4    <dbl [2]>   <dbl [2]>  \n 9 10cents X5    <dbl [2]>   <dbl [2]>  \n10 10cents X6    <dbl [2]>   <dbl [2]>  \n11 10cents X7    <dbl [2]>   <dbl [2]>  \n12 10cents X8    <dbl [2]>   <dbl [2]>  \n```\n:::\n:::\n\nExcept that it didn't appear to work. Although it actually did. These are list-columns. I actually recorded lecture 14a to help you with this. (See also the discussion in Extra 3 of the last part of the writers question, and the word \"list\" at the top of `temperature` and `consumption`). Each cell holds *two* numbers instead of the one you were probably expecting. \n\nWhy did that happen? The warning above the output is a clue. Something is going to be \"not uniquely identified\". Think about how `pivot_wider` works. It has to decide which *row* and *column* of the wider dataframe to put each value in. The column comes from the `names_from`: temperature or consumption. So that's not a problem. The row comes from the combination of the other columns not named in the `pivot_wider`: that means the ones called `X1` and `col`. (Another way to see that is the columns in the result from the `pivot_wider` that *do not* have values in them: not `temperature` or `consumption`, the other two.)\n\nIf you look back at the things in `col`, they go from `X3` to `X8`, so there are six of them. There are two values in `X1`, so there are $2 \\times 6 = 12$ combinations of the two, and so 12 rows in the wider dataframe. This has two columns, and thus $12 \\times 2 = 24$ cells altogether. But there were 48 values in the longer dataframe (go back and look: it has 48 rows), so there isn't enough room for all of them here.\n\nIf you go back and look at the longer dataframe, you'll see, for example, that there are two `temperature` values that go with an `X1` of 8 cents and a col of `X3`, so that they will both have to be jammed into one cell of the wider dataframe.\n\nThe resolution of the list-columns here is the same as in the one about the writers: `unnest` them,  and then you can ignore the warning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nutils0 %>% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %>% \npivot_wider(names_from = X2, values_from = value) %>% \nunnest(c(temperature, consumption)) -> utils\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n  dplyr::group_by(X1, col, X2) %>%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n  dplyr::filter(n > 1L)\n```\n:::\n\n```{.r .cell-code}\nutils\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 x 4\n   X1     col   temperature consumption\n   <chr>  <chr>       <dbl>       <dbl>\n 1 8cents X3             31          55\n 2 8cents X3             62          41\n 3 8cents X4             34          49\n 4 8cents X4             66          46\n 5 8cents X5             39          46\n 6 8cents X5             68          44\n 7 8cents X6             42          47\n 8 8cents X6             71          51\n 9 8cents X7             47          40\n10 8cents X7             75          62\n# i 14 more rows\n```\n:::\n:::\n\nThere were 24 months of data, and a temperature and consumption for each, so this is now tidy and I can give it a proper name.\n\n\n\nExtra: if you got to here and got scared:\n\n::: {.cell}\n\n```{.r .cell-code}\nutils0 %>% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %>% \npivot_wider(names_from = X2, values_from = value) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n  dplyr::group_by(X1, col, X2) %>%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n  dplyr::filter(n > 1L)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 12 x 4\n   X1      col   temperature consumption\n   <chr>   <chr> <list>      <list>     \n 1 8cents  X3    <dbl [2]>   <dbl [2]>  \n 2 8cents  X4    <dbl [2]>   <dbl [2]>  \n 3 8cents  X5    <dbl [2]>   <dbl [2]>  \n 4 8cents  X6    <dbl [2]>   <dbl [2]>  \n 5 8cents  X7    <dbl [2]>   <dbl [2]>  \n 6 8cents  X8    <dbl [2]>   <dbl [2]>  \n 7 10cents X3    <dbl [2]>   <dbl [2]>  \n 8 10cents X4    <dbl [2]>   <dbl [2]>  \n 9 10cents X5    <dbl [2]>   <dbl [2]>  \n10 10cents X6    <dbl [2]>   <dbl [2]>  \n11 10cents X7    <dbl [2]>   <dbl [2]>  \n12 10cents X8    <dbl [2]>   <dbl [2]>  \n```\n:::\n:::\n\nwhich is an entirely reasonable reaction, you might have asked yourself how you could have prevented this from happening. The problem, as discussed earlier, is with the rows, and that the `X1`-`col` combinations repeat. Let's go back to \"longer\":\n\n::: {.cell}\n\n```{.r .cell-code}\nutils0 %>% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 48 x 4\n   X1     X2          col   value\n   <chr>  <chr>       <chr> <dbl>\n 1 8cents temperature X3       31\n 2 8cents temperature X4       34\n 3 8cents temperature X5       39\n 4 8cents temperature X6       42\n 5 8cents temperature X7       47\n 6 8cents temperature X8       56\n 7 8cents temperature X3       62\n 8 8cents temperature X4       66\n 9 8cents temperature X5       68\n10 8cents temperature X6       71\n# i 38 more rows\n```\n:::\n:::\n\nRows 1 and 7, 2 and 8, etc, are \"replicates\" in that they have the same `X1` and `col` values but different temperatures. This is because they come from the same column in the original layout of the data (the 31 and the 62 are underneath each other). This means that the first six rows are \"replicate 1\" and the next six are \"replicate 2\". Scrolling down, we then get to 8 cents and consumption, and we need to do the same again. So if we make a column that has 1s and 2s in the right places (six 1s, six 2s, repeat), we should then have unique rows for the `pivot_wider`. \n\n::: {.cell}\n\n```{.r .cell-code}\nutils0 %>% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %>% \nmutate(replicate = rep(1:2, each = 6, length.out = 48))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 48 x 5\n   X1     X2          col   value replicate\n   <chr>  <chr>       <chr> <dbl>     <int>\n 1 8cents temperature X3       31         1\n 2 8cents temperature X4       34         1\n 3 8cents temperature X5       39         1\n 4 8cents temperature X6       42         1\n 5 8cents temperature X7       47         1\n 6 8cents temperature X8       56         1\n 7 8cents temperature X3       62         2\n 8 8cents temperature X4       66         2\n 9 8cents temperature X5       68         2\n10 8cents temperature X6       71         2\n# i 38 more rows\n```\n:::\n:::\n\n`rep` does repeats like this: something to repeat (the numbers 1 through 2), how many times to repeat each one (six times), and how long the final thing has to be (48, since there were 48 rows in the longer dataframe).\n\nThen, this time, if we do the `pivot_wider`, it should give us something tidy:\n\n::: {.cell}\n\n```{.r .cell-code}\nutils0 %>% pivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %>% \nmutate(replicate = rep(1:2, each = 6, length.out = 48)) %>% \npivot_wider(names_from = X2, values_from = value) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 x 5\n   X1     col   replicate temperature consumption\n   <chr>  <chr>     <int>       <dbl>       <dbl>\n 1 8cents X3            1          31          55\n 2 8cents X4            1          34          49\n 3 8cents X5            1          39          46\n 4 8cents X6            1          42          47\n 5 8cents X7            1          47          40\n 6 8cents X8            1          56          43\n 7 8cents X3            2          62          41\n 8 8cents X4            2          66          46\n 9 8cents X5            2          68          44\n10 8cents X6            2          71          51\n# i 14 more rows\n```\n:::\n:::\n\nand so it does, with 24 rows for the 24 months.\n\nAnother, perhaps easier, way to think about this (you might find it easier, anyway) is to go back to the original dataframe and make the `replicate` there:\n\n::: {.cell}\n\n```{.r .cell-code}\nutils0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 8\n  X1      X2             X3    X4    X5    X6    X7    X8\n  <chr>   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 8cents  temperature    31    34    39    42    47    56\n2 8cents  temperature    62    66    68    71    75    78\n3 8cents  consumption    55    49    46    47    40    43\n4 8cents  consumption    41    46    44    51    62    73\n5 10cents temperature    32    36    39    42    48    56\n6 10cents temperature    62    66    68    72    75    79\n7 10cents consumption    50    44    42    42    36    40\n8 10cents consumption    39    44    40    44    50    55\n```\n:::\n:::\n\nThe first two rows are replicates (both 8 cents and temperature), then the third and fourth, and so on. So setting a `replicate` column as 1, 2, 1, 2 etc should do it, and this is short enough to type directly. Do this *first*, then the `pivot_longer`, then the `pivot_wider` as we did before, and we should end up with something tidy:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nutils0 %>% mutate(replicate = c(1,2,1,2,1,2,1,2)) %>% \npivot_longer(X3:X8, names_to = \"col\", values_to = \"value\") %>% \npivot_wider(names_from = X2, values_from = value) %>% \nunnest(c(temperature, consumption)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 x 5\n   X1     replicate col   temperature consumption\n   <chr>      <dbl> <chr>       <dbl>       <dbl>\n 1 8cents         1 X3             31          55\n 2 8cents         1 X4             34          49\n 3 8cents         1 X5             39          46\n 4 8cents         1 X6             42          47\n 5 8cents         1 X7             47          40\n 6 8cents         1 X8             56          43\n 7 8cents         2 X3             62          41\n 8 8cents         2 X4             66          46\n 9 8cents         2 X5             68          44\n10 8cents         2 X6             71          51\n# i 14 more rows\n```\n:::\n:::\n\nIf you check this, you'll see that `replicate` gets turned into the same thing in the longer dataframe that we had earlier, so you can do it either way.\n\nThe moral of the story is that when you are planning to do a pivot-wider, you ought to devote some attention to which *rows* things are going into. Sometimes you can get away with just doing it and it works, but thinking about rows is how to diagnose it when it doesn't. (The ideas below also appear in Lecture 14a.) Here's another mini-example where the observations are matched pairs but they come to us long, like two-sample data:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tribble(\n~obs, ~y, ~time,\n1, 10, \"pre\",\n2, 13, \"post\",\n3, 12, \"pre\",\n4, 14, \"post\",\n5, 13, \"pre\",\n6, 15, \"post\"\n)\nd %>% pivot_wider(names_from = time, values_from = y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n    obs   pre  post\n  <dbl> <dbl> <dbl>\n1     1    10    NA\n2     2    NA    13\n3     3    12    NA\n4     4    NA    14\n5     5    13    NA\n6     6    NA    15\n```\n:::\n:::\n\nOh. The columns are all right, but the rows certainly are not.\n\nThe problem is that the only thing left after `y` and `time` have been used in the `pivot_wider` is the column `obs`, and there are six values there, so there are six rows. This is, in a way, the opposite of the problem we had before; now, there is *not enough* data to fill the twelve cells of the wider dataframe. For example, there is no `pre` measurement in the row where `obs` is 2, so this cell of the wider dataframe is empty: it has a missing value in it.\n\nThe problem is that the `obs` column numbered the six observations 1 through 6, but really they are three groups of two observations on three people, so instead of `obs` we need a column called `person` that shows which observations are the matched pairs, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tribble(\n~person, ~y, ~time,\n1, 10, \"pre\",\n1, 13, \"post\",\n2, 12, \"pre\",\n2, 14, \"post\",\n3, 13, \"pre\",\n3, 15, \"post\"\n)\n```\n:::\n\nNow there are going to be three rows with a pre and a post in each:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% pivot_wider(names_from = time, values_from = y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 3\n  person   pre  post\n   <dbl> <dbl> <dbl>\n1      1    10    13\n2      2    12    14\n3      3    13    15\n```\n:::\n:::\n\n`pivot_wider` requires more thinking than `pivot_longer`, and when it does something mysterious, that's when you need to have some understanding of how it works, so that you can fix things up.\n\n\n\n\n\n\n$\\blacksquare$\n\n\n(c) Make a suitable graph of temperature, consumption and price in your tidy dataframe. Add smooth trends if appropriate. If you were unable to get the data tidy, use my tidy version [here](http://ritsokiguess.site/datafiles/utils_tidy.csv). (If you need the other file, right-click on \"here\" and Copy Link Address.)\n\nSolution\n\n\nI said earlier to treat price (rather badly labelled as `X1`) as categorical, so we have two quantitative variables and one categorical. This suggests a scatterplot with the two prices distinguished by colours. (We don't have a mechanism for making three-dimensional plots, and in any case if you have a quantitative variable with not that many distinct different values, you can often treat that as categorical, such as price here.)\n\nBefore we make a graph, though, we should rename `X1`. The way you might think of is to create a new column with the same values as `X1`, but a new name.^[This actually creates a copy of the original column, so if you look you now have two columns with the same thing in them, one with a bad name and one with a good one.] Like this. Consumption is the outcome, so it goes on the $y$-axis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nutils %>% \nmutate(price = X1) %>% \nggplot(aes(x = temperature, y = consumption, colour = price)) + \ngeom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/util-temp-14-1.pdf){fig-pos='H'}\n:::\n:::\n\nI said smooth trends rather than lines, because you don't know until you draw the graph whether the trends *are* lines. If they're not, there's not much point in drawing lines through them. These ones are rather clearly curves, which we take up in the next part.\n\nIf you fail to rename `X1`, that's what will appear on the legend, and the first thing your reader would ask is \"what is `X1`?\" When writing, you need to think of your reader, since they are (in the real world) paying you for your work.\n\n\n\nExtra: there is an official `rename` also. I haven't used that in class, so if you discover this, make sure to say where you found out about it from:\n\n::: {.cell}\n\n```{.r .cell-code}\nutils %>% \nrename(price = X1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 x 4\n   price  col   temperature consumption\n   <chr>  <chr>       <dbl>       <dbl>\n 1 8cents X3             31          55\n 2 8cents X3             62          41\n 3 8cents X4             34          49\n 4 8cents X4             66          46\n 5 8cents X5             39          46\n 6 8cents X5             68          44\n 7 8cents X6             42          47\n 8 8cents X6             71          51\n 9 8cents X7             47          40\n10 8cents X7             75          62\n# i 14 more rows\n```\n:::\n:::\n\nThe syntax is \"new name equals old one\". I used to think it was something like \"take the column called `X1` and rename it to `price`\", but as you see, that's exactly backwards. The English-language version is \"create a new column called `price` from the column previously called `X1`\".\n\n\n$\\blacksquare$\n\n\n(d) What patterns or trends do you see on your graph? Do they make practical sense? There are two things I would like you to comment on.\n\nSolution\n\n\nThe two things are:\n\n- the relationships are both curves, going down and then up again.\n- the blue curve is above the red one.\n\nIf the temperature is low (30 degrees F is just below freezing), people will need to heat their houses, and the electricity consumption to do this is reflected in the curves being higher at the left. (Not all people have electric heating, but at least some people do, and electric heating uses a lot of electricity.) When the temperature is high, people will turn on the air-conditioning (which is usually electric), and that explains the sharp increase in consumption at high temperatures. In between is the zone where the house stays a good temperature without heating or cooling.\n\nSo why is the blue curve above the red one? This is saying that when electricity is cheaper, people will use more of it. (This seems to be particularly true when the temperature is high; people might \"crank\" the air-conditioning if it doesn't cost too much to run.) Conversely, if electricity is more expensive, people might be more thoughtful about what temperature to turn on the heating or AC. (For example, in summer you might keep the drapes closed so that it stays cooler inside without needing to use the AC so much.)\n\n\n$\\blacksquare$\n\n\n\n\n\n\n## Tidy blood pressure\n\n Going to the dentist is scary for a lot of people. One way in which this might show up is that people might have higher blood pressure on average before their dentist's appointment than an hour after the appointment is done. Ten randomly-chosen individuals have their (systolic^[A blood pressure is usually given as two numbers, like \"120 over 80\". The first number, which is the one shown in our data, is called the systolic blood pressure. It is the pressure in the arteries when the heart is pumping. The second is called the diastolic blood pressure, and it is the pressure in the arteries when the heart is resting.])  blood pressure measured while they are in a dentist's waiting room, and then again one hour after their appointment is finished.\n\nYou might have seen a tidy version of this data set before.\n\nThe data as I originally received it is in [http://ritsokiguess.site/datafiles/blood_pressure2.csv](http://ritsokiguess.site/datafiles/blood_pressure2.csv).\n\n\n\n(a) Read in and display the data as originally received.\n\nSolution\n\n\nYou ought to be suspicious that something is going to be up with the layout. With that in mind, I'm using a \"disposable\" name for this dataframe:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/blood_pressure2.csv\"\nbp0 <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 2 Columns: 11\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (1): time\ndbl (10): p1, p2, p3, p4, p5, p6, p7, p8, p9, p10\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nbp0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 11\n  time      p1    p2    p3    p4    p5    p6    p7    p8    p9   p10\n  <chr>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 before   132   135   149   133   119   121   128   132   119   110\n2 after    118   137   140   139   107   116   122   124   115   103\n```\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(b) Describe briefly how the data you read in is not tidy, bearing in mind how the data were collected and how they would be analysed.\n\nSolution\n\n\nIn short, the things that are rows should be columns, and the things that are columns should be rows. Or, the individuals (people, here) should be in rows but they are in columns. Or, the variables (time points) should be in columns but they are in rows. One of those. \n\nAnother way to go at it is to note that the numbers are all blood pressure measurements, and so should be all in one column, labelled by which time and individual they belong to. This is, however, not quite right, for reasons of how the data were collected. They are pairs of measurements on the same individual, and so there should be (for something like a matched pairs $t$-test) a column of  before measurements and a column of after measurements. This will mean some extra work in the next part to get it tidy.\n\n\n$\\blacksquare$\n\n\n(c) Produce a tidy dataframe from the one you read in from the file. (How many rows should you have?)\n\nSolution\n\n\nThe usual starting point for these is to get all the measurements into one column and see what to do after that.\nThis is `pivot_longer`:\n\n::: {.cell}\n\n```{.r .cell-code}\nbp0 %>% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 3\n   time   person    bp\n   <chr>  <chr>  <dbl>\n 1 before p1       132\n 2 before p2       135\n 3 before p3       149\n 4 before p4       133\n 5 before p5       119\n 6 before p6       121\n 7 before p7       128\n 8 before p8       132\n 9 before p9       119\n10 before p10      110\n11 after  p1       118\n12 after  p2       137\n13 after  p3       140\n14 after  p4       139\n15 after  p5       107\n16 after  p6       116\n17 after  p7       122\n18 after  p8       124\n19 after  p9       115\n20 after  p10      103\n```\n:::\n:::\n\nThis would be tidy if we had 20 independent observations from 20 different people. But we don't. We only have 10 people, with two measurements on each, so we should only have 10 rows. Having made things longer, they are now *too* long, and we have to make it wider again.\n\nWe want to have a column of before and a column of after, so the names of the new columns are coming from what I called `time`. The values are coming from what I called `bp`, so, gluing the `pivot_wider` on the end of the pipe:\n\n::: {.cell}\n\n```{.r .cell-code}\nbp0 %>% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %>% \n  pivot_wider(names_from = time, values_from = bp) -> blood_pressure\nblood_pressure\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 3\n   person before after\n   <chr>   <dbl> <dbl>\n 1 p1        132   118\n 2 p2        135   137\n 3 p3        149   140\n 4 p4        133   139\n 5 p5        119   107\n 6 p6        121   116\n 7 p7        128   122\n 8 p8        132   124\n 9 p9        119   115\n10 p10       110   103\n```\n:::\n:::\n\nThis is now tidy, so I gave it a \"permanent\" name.\n\nI laid out the steps of my thinking, so you could follow my logic. I'm expecting your *thinking* to be about the same as mine, but the work you hand in can certainly be the finished pipe I had just above, as if you thought of it right away.\n\nExtra: `pivot_wider` is smarter than you think, but it can be helpful to know what it does, in order to help diagnose when things go wrong. Let's go back and look at the too-long dataframe again:\n\n::: {.cell}\n\n```{.r .cell-code}\nbp0 %>% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 3\n   time   person    bp\n   <chr>  <chr>  <dbl>\n 1 before p1       132\n 2 before p2       135\n 3 before p3       149\n 4 before p4       133\n 5 before p5       119\n 6 before p6       121\n 7 before p7       128\n 8 before p8       132\n 9 before p9       119\n10 before p10      110\n11 after  p1       118\n12 after  p2       137\n13 after  p3       140\n14 after  p4       139\n15 after  p5       107\n16 after  p6       116\n17 after  p7       122\n18 after  p8       124\n19 after  p9       115\n20 after  p10      103\n```\n:::\n:::\n\nEach one of those values in `bp` has to go somewhere in the wider dataframe. In particular, it needs to go in a particular row and column. The column is pretty obvious: the column whose name is in the `time` column. But the row is much less obvious. How does `pivot_wider` figure it out? Well, it looks for all combinations of values in the *other* columns, the ones not mentioned in the `pivot_wider`, and makes a row for each of those. In this case, the only other column is `person`, so it makes one row for each person. Since there is one before and one after measurement for each person, everything works smoothly. \n\nThis enables us to try a couple of what-ifs to see what can go wrong. \n\nFirst, what if there's no person column at all, so there is nothing to say what row an observation should go in?\n\n::: {.cell}\n\n```{.r .cell-code}\nbp0 %>% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %>% \nselect(-person) %>% \npivot_wider(names_from = time, values_from = bp)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Values from `bp` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n  dplyr::group_by(time) %>%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n  dplyr::filter(n > 1L)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n  before     after     \n  <list>     <list>    \n1 <dbl [10]> <dbl [10]>\n```\n:::\n:::\n\nIt kinda works, but with a warning. The warning says \"values are not uniquely identified\", which is a posh way to say that it doesn't know where to put them (because there is no longer a way to say which row each observation should go in). \n\nHere's another one, similar:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tribble(\n~g, ~id, ~y,\n\"a\", \"p1\", 10,\n\"a\", \"p2\", 11,\n\"b\", \"p1\", 12,\n\"b\", \"p2\", 13,\n\"a\", \"p3\", 14,\n\"a\", \"p1\", 15\n)\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n  g     id        y\n  <chr> <chr> <dbl>\n1 a     p1       10\n2 a     p2       11\n3 b     p1       12\n4 b     p2       13\n5 a     p3       14\n6 a     p1       15\n```\n:::\n:::\n\nWhen we do this:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% pivot_wider(names_from = g, values_from = y)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Values from `y` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n  dplyr::group_by(id, g) %>%\n  dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n  dplyr::filter(n > 1L)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 3\n  id    a         b        \n  <chr> <list>    <list>   \n1 p1    <dbl [2]> <dbl [1]>\n2 p2    <dbl [1]> <dbl [1]>\n3 p3    <dbl [1]> <NULL>   \n```\n:::\n:::\n\nwe get list-columns again (and the same warning). What this output is telling you is that mostly there is one number per id-group combination (the `dbl[1]`) but there are two observations labelled id `p1` and group `a`, and no observations at all labelled id `p3` and group `b`. It turns out^[I know because I made these data up. that the last row of the tribble contains errors. Fix them, and all is good:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tribble(\n~g, ~id, ~y,\n\"a\", \"p1\", 10,\n\"a\", \"p2\", 11,\n\"b\", \"p1\", 12,\n\"b\", \"p2\", 13,\n\"a\", \"p3\", 14,\n\"b\", \"p3\", 15\n)\nd %>% pivot_wider(names_from = g, values_from = y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 3\n  id        a     b\n  <chr> <dbl> <dbl>\n1 p1       10    12\n2 p2       11    13\n3 p3       14    15\n```\n:::\n:::\n\nOne last one:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tribble(\n~id, ~g, ~y,\n1, \"a\", 10,\n2, \"a\", 11,\n3, \"a\", 12,\n4, \"b\", 13,\n5, \"b\", 14,\n6, \"b\", 15\n)\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n     id g         y\n  <dbl> <chr> <dbl>\n1     1 a        10\n2     2 a        11\n3     3 a        12\n4     4 b        13\n5     5 b        14\n6     6 b        15\n```\n:::\n:::\n\nand then\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% pivot_wider(names_from = g, values_from = y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n     id     a     b\n  <dbl> <dbl> <dbl>\n1     1    10    NA\n2     2    11    NA\n3     3    12    NA\n4     4    NA    13\n5     5    NA    14\n6     6    NA    15\n```\n:::\n:::\n\nWhere did those missing values come from? If you go back and look at this `d`, you'll see that each person has only *one* measurement, *either* an `a` or a `b`, not both. There is, for example, nothing to go in the `a` column for person number 4, because their only measurement was in group `b`. This kind of thing happens with two independent samples, and is a warning that you don't *need* to pivot wider; it's already tidy:\n\n::: {.cell}\n\n```{.r .cell-code}\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n     id g         y\n  <dbl> <chr> <dbl>\n1     1 a        10\n2     2 a        11\n3     3 a        12\n4     4 b        13\n5     5 b        14\n6     6 b        15\n```\n:::\n:::\nThink about the kind of layout you need for a two-sample $t$-test.\n\n\n$\\blacksquare$\n\n\n(d) What kind of test might you run on these data? Explain briefly.\n\nSolution\n\n\nThis is a matched-pairs experiment, so it needs a matched-pairs analysis. This could be a matched-pairs $t$-test, or a sign test on the differences (testing that the population median difference is zero). You can suggest either, since we haven't drawn any graphs yet, but \"sign test\" is not enough; you need to say something about what kind of sign test. (It's actually quicker to answer \"matched-pairs $t$-test\" since you don't need any more detail than that.)\n\n\n$\\blacksquare$\n\n\n(e) Draw a suitable graph of these data.\n\nSolution\n\n\nGiven that we are going to do a matched-pairs analysis of some kind, the best graph looks at the *differences* between the two measurements. So calculate them first, and then make a one-sample plot of them, such as a histogram:\n\n::: {.cell}\n\n```{.r .cell-code}\nblood_pressure %>% mutate(diff = before - after) %>% \nggplot(aes(x=diff)) + geom_histogram(bins=5)\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/blood-pressure-tidy-12-1.pdf){fig-pos='H'}\n:::\n:::\nYou will need a suitably small number of bins, since we only have ten observations. You can take the differences the other way around if you prefer; they will then be mostly negative.\n\nYou might have looked at the two quantitative columns and thought \"scatterplot\":\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(blood_pressure, aes(x=before, y=after)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/blood-pressure-tidy-13-1.pdf){fig-pos='H'}\n:::\n:::\nThis says that if the blood pressure before was large, the blood pressure afterwards is as well. This is fair enough, but it is the answer to a question *we don't care about*. What we *do* care about is whether the after measurement is bigger than the before one *for the same person*, which this graph does not show. So this is not the best.\n\nTo rescue this graph, you can add the line $y=x$ to it. The value of this is that a point above this line has the after measurement bigger than the corresponding before one, and a point below the line has the after measurement smaller. You will need to find out how to add a line with a given intercept and slope to the plot, since I haven't shown you how to do it. It's called `geom_abline`, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(blood_pressure, aes(x=before, y=after)) + geom_point() +\ngeom_abline(intercept = 0, slope = 1)\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/blood-pressure-tidy-14-1.pdf){fig-pos='H'}\n:::\n:::\nThis is insightful, because most of the points are below the line, so that most of the before measurements were bigger than the corresponding after ones.\n\nNote that if you put a *regression line* on your plot, you will need to offer a convincing explanation of why that offers insight, which I think you will find difficult.\n\nFinally, if you thought the long data frame was tidy, this one:\n\n::: {.cell}\n\n```{.r .cell-code}\nbp0 %>% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 3\n   time   person    bp\n   <chr>  <chr>  <dbl>\n 1 before p1       132\n 2 before p2       135\n 3 before p3       149\n 4 before p4       133\n 5 before p5       119\n 6 before p6       121\n 7 before p7       128\n 8 before p8       132\n 9 before p9       119\n10 before p10      110\n11 after  p1       118\n12 after  p2       137\n13 after  p3       140\n14 after  p4       139\n15 after  p5       107\n16 after  p6       116\n17 after  p7       122\n18 after  p8       124\n19 after  p9       115\n20 after  p10      103\n```\n:::\n:::\n\nthen you can rescue some points here by making a suitable plot of that. A boxplot is not enough:\n\n::: {.cell}\n\n```{.r .cell-code}\nbp0 %>% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %>% \nggplot(aes(x=time, y=bp)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/blood-pressure-tidy-16-1.pdf){fig-pos='H'}\n:::\n:::\n\nbecause you have *lost* the connection between the two measurements for each person. To keep that connection, start with the same plot but as points rather than boxes:\n\n::: {.cell}\n\n```{.r .cell-code}\nbp0 %>% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %>% \nggplot(aes(x=time, y=bp)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/blood-pressure-tidy-17-1.pdf){fig-pos='H'}\n:::\n:::\nand then join the two points that belong to the same person. This is done with `geom_line` as usual, only you have to say which points are going to be joined, namely the two for each person, and to do that, you specify `person` in `group`:\n\n::: {.cell}\n\n```{.r .cell-code}\nbp0 %>% pivot_longer(-time, names_to=\"person\", values_to=\"bp\") %>% \nggplot(aes(x=time, y=bp, group=person)) + geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](tidying-data_files/figure-pdf/blood-pressure-tidy-18-1.pdf){fig-pos='H'}\n:::\n:::\nMost of the lines are going uphill, so most of the `after` measurements are less than the corresponding `before` ones.^[This is known in the trade as a *spaghetti plot* because the lines resemble strands of spaghetti.]\n\n\n$\\blacksquare$\n\n\n\n\n",
    "supporting": [
      "tidying-data_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}