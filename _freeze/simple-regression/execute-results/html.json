{
  "hash": "c695f436dcc01bc7fbc1b28c6879f302",
  "result": {
    "markdown": "# Simple regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Rainfall in California\n\n\n The data in\n[link](http://ritsokiguess.site/datafiles/calirain.txt) are\nrainfall and other measurements for 30 weather stations in\nCalifornia. Our aim is to understand how well the annual rainfall at\nthese stations (measured in inches) can be predicted from the other\nmeasurements, which are the altitude (in feet above sea level), the\nlatitude (degrees north of the equator) and the distance from the\ncoast (in miles).\n\n\n\n(a) Read the data into R. You'll have to be careful here, since the\nvalues are space-delimited, but sometimes by more than one space, to\nmake the columns line up.  `read_table2`, with filename or\nurl, will read it in.\nOne of the variables\nis called `rainfall`, so as long as you *do not* call\nthe data frame that, you should be safe.\n\n\n\n(b) Make a boxplot of the rainfall figures, and explain why the\nvalues are reasonable. (A rainfall cannot be negative, and it is\nunusual for a annual rainfall to exceed 60 inches.) A\n`ggplot` boxplot needs *something* on the $x$-axis: the\nnumber 1 will do.\n\n\n\n(c) Plot `rainfall` against each of the other quantitative\nvariables (that is, not `station`). \n\n\n\n(d) Look at the relationship of each other variable with\n`rainfall`. \nJustify the assertion that `latitude` seems most strongly\nrelated with `rainfall`. Is that relationship positive or negative? linear? Explain\nbriefly. \n\n\n\n(e) Fit a regression with `rainfall` as the response\nvariable, and `latitude` as your explanatory variable. What\nare the intercept, slope and R-squared values? Is there a \n*significant* relationship between `rainfall` and your\nexplanatory variable? What does that mean?\n\n\n\n(f) Fit a multiple regression predicting `rainfall` from\nall three of the other (quantitative) variables. Display the\nresults. Comment is coming up later.\n\n\n\n(g) What is the R-squared for the regression of the last part?\nHow does that compare with the R-squared of your regression in part (e)?\n\n\n\n(h) What do you conclude about the importance of the variables\nthat you did *not* include in your model in\n(e)? Explain briefly.\n\n\n\n(i) Make a suitable hypothesis test that the variables\n`altitude` and `fromcoast` significantly improve the\nprediction of `rainfall` over the use of `latitude`\nalone. What do you conclude?\n\n\n\n\n\n\n\n\n##  Carbon monoxide in cigarettes\n\n\n The (US) Federal Trade Commission assesses cigarettes\naccording to their tar, nicotine and carbon monoxide contents. In a\nparticular year, 25 brands were assessed. For each brand, the tar,\nnicotine and carbon monoxide (all in milligrams) were measured, along\nwith the weight in grams. Our aim is to predict carbon monoxide from\nany or all of the other variables. The data are in\n[link](http://ritsokiguess.site/datafiles/ftccigar.txt). These are\naligned by column (except for the variable names), with more than one\nspace between each column of data.\n\n\n(a) Read the data into R, and check that you have 25 observations\nand 4 variables.\n\n\n\n(b) Run a regression to predict carbon monoxide from the other\nvariables, and obtain a summary of the output.\n\n\n\n(c) Which one of your explanatory variables would you\nremove from this regression? Explain (very) briefly. Go ahead and\nfit the regression  without it, and describe how the change in\nR-squared from the regression in (b) was entirely predictable.\n\n\n\n(d) Fit a regression predicting carbon monoxide from\n`nicotine` *only*, and display the summary.\n\n\n\n(e) `nicotine` was far from being significant in the model\nof (c), and yet in the model of\n(d), it was *strongly* significant, and the\nR-squared value of (d) was almost as high as that\nof (c). What does this say about the importance of\n`nicotine` as an explanatory variable? Explain, as briefly as\nyou can manage.\n\n\n\n(f) Make a \"pairs plot\": that is, scatter plots between all\npairs of variables. This can be done by feeding the whole data frame\ninto `plot`.^[This is a base graphics graph rather than a `ggplot` one, but it will do for our purposes.]\nDo you see any strong relationships that do\n*not* include `co`? Does that shed any light on the last\npart? Explain briefly (or \"at length\" if that's how it comes\nout). \n\n\n\n\n\n\n##  Maximal oxygen uptake in young boys\n\n\n A physiologist wanted to understand the relationship between\nphysical characteristics of pre-adolescent boys and their maximal\noxygen uptake (millilitres of oxygen per kilogram of body weight). The\ndata are in\n[link](http://ritsokiguess.site/datafiles/youngboys.txt) for a\nrandom sample of 10 pre-adolescent boys. The variables are (with\nunits):\n\n\n\n* `uptake`: Oxygen uptake (millitres of oxygen per kilogram\nof body weight)\n\n* `age`: boy's age (years)\n\n* `height`: boy's height (cm)\n\n* `weight`: boy's weight (kg)\n\n* `chest`: chest depth (cm).\n\n\n\n\n(a) Read the data into R and confirm that you do indeed have\n10 observations.\n\n\n\n(b) Fit a regression predicting oxygen uptake from all the\nother variables, and display the results.\n\n\n\n(c) (A one-mark question.) Would you say, on the evidence so\nfar, that the regression fits well or badly?  Explain (very)\nbriefly.\n\n\n\n(d) It seems reasonable that an older boy should have a\ngreater oxygen uptake, all else being equal. Is this supported\nby your output?  Explain briefly.\n\n\n\n(e) It seems reasonable that a boy with larger weight\nshould have larger lungs and thus a *statistically\nsignificantly* larger oxygen uptake. Is that what happens\nhere? Explain briefly.\n\n\n\n(f) Fit a model that contains only the significant\nexplanatory variables from your first regression. How do\nthe R-squared values from the two regressions compare?\n(The last sentence asks for more or less the same thing as\nthe next part. Answer it either here or there. Either\nplace is good.)\n\n\n\n(g) How has R-squared changed between your two\nregressions? Describe what you see in a few words.\n\n\n\n(h) Carry out a test comparing the fit of your\ntwo regression models. What do you conclude, and\ntherefore what recommendation would you make about the\nregression that would be preferred?\n\n\n\n(i) Obtain a table of correlations between all\nthe variables in the data frame. Do this by feeding\nthe whole data frame into `cor`. \nWe found that a regression predicting oxygen uptake\nfrom just `height` was acceptably good. What\ndoes your table of correlations say about why that\nis? (Hint: look for all the correlations that are\n*large*.) \n\n\n\n \n\n\n\n## Facebook friends and grey matter\n\n\n\n Is there a relationship between the number\nof Facebook friends a person has, and the density of grey matter in\nthe areas of the brain associated with social perception and\nassociative memory? To find out, a 2012 study measured both of these\nvariables for a sample of 40 students at City University in London\n(England). The data are at\n[link](http://ritsokiguess.site/datafiles/facebook.txt). The grey\nmatter density is on a $z$-score standardized scale. The values are\nseparated by *tabs*.\n\nThe aim of this question is to produce a Quarto report that\ncontains your answers to the questions below. \n\nYou should aim to make your report flow smoothly, so that it would be\npleasant for a grader to read, and can stand on its own as an analysis\n(rather than just being the answer to a question that I set you).\nSome suggestions: give your report a title and arrange it into\nsections with an Introduction; add a small amount of additional text\nhere and there explaining what you are doing and why. I don't expect\nyou to spend a large amount of time on this, but I *do* hope\nyou will make some effort. (My report came out to 4 Word pages; HTML of course doesn't have page numbers, but that's a good kind of length to aim for.)\n\n\n(a) Read in the data and make a scatterplot for predicting the\nnumber of Facebook friends from the grey matter density. On your\nscatterplot, add a smooth trend.\n\n\n\n\n(b) Describe what you see on your scatterplot: is there a\ntrend, and if so, what kind of trend is it? (Don't get too taken in\nby the exact shape of your smooth trend.) Think \"form, direction,  strength\". \n\n\n\n(c) Fit a regression predicting the number of Facebook friends\nfrom the grey matter density, and display the output.\n\n\n\n(d) Is the slope of your regression line significantly\ndifferent from zero? What does that mean, in the context of the\ndata?\n\n\n\n\n(e) Are you surprised by the results of\nparts (b) and (d)? Explain briefly.\n\n\n\n(f) Obtain a scatterplot with the regression line on it.\n\n\n\n(g) Obtain a plot of the residuals from the regression against\nthe fitted values, and comment briefly on it.\n\n\n\n\n\n\n\n##  Endogenous nitrogen excretion in carp\n\n\n A paper in Fisheries Science reported on variables that\naffect \"endogenous nitrogen excretion\" or ENE in carp raised in\nJapan. A number of carp were divided into groups based on body weight,\nand each group was placed in a different tank. The mean body weight of\nthe carp placed in each tank was recorded. The carp were then fed a\nprotein-free diet three times daily for a period of 20 days. At the\nend of the experiment, the amount of ENE in each tank was measured, in\nmilligrams of total fish body weight per day. (Thus it should not\nmatter that some of the tanks had more fish than others, because the\nscaling is done properly.)\n\nFor this question, write a report in a Quarto document that answers the\nquestions below and contains some narrative that describes your\nanalysis. Create an HTML document from your Quarto document.\n\n\n\n(a) Read the data in from\n[link](http://ritsokiguess.site/datafiles/carp.txt). There are 10\ntanks. \n\n\n\n\n(b) Create a scatterplot of ENE (response) against bodyweight\n(explanatory). Add a smooth trend to your plot.\n\n\n\n(c) Is there an upward or downward trend (or neither)? Is the\nrelationship a line or a curve? Explain briefly.\n\n\n\n(d) Fit a straight line to the data, and obtain the R-squared\nfor the regression.\n\n\n\n(e) Obtain a residual plot (residuals against fitted values)\nfor this regression. Do you see any problems? If so, what does that\ntell you about the relationship in the data?\n\n\n\n(f) Fit a parabola to the data (that is, including an\n$x$-squared term). Compare the R-squared values for the models in\nthis part and part (d). Does that suggest that the parabola\nmodel is an improvement here over the linear model?\n\n\n\n(g) Is the test for the slope coefficient for the squared term\nsignificant? What does this mean?\n\n\n\n(h) Make the scatterplot of part (b), but add\nthe fitted curve. Describe any way in which the curve fails to fit well.\n\n\n\n(i) Obtain a residual plot for the parabola model. Do you see\nany problems with it? (If you do, I'm not asking you to do anything\nabout them in this question, but I will.)\n\n\n\n\n##  Salaries of social workers\n\n\n Another salary-prediction question: does the number of years\nof work experience that a social worker has help to predict their \nsalary? Data for 50 social workers are in\n[link](http://ritsokiguess.site/datafiles/socwork.txt). \n\n\n\n(a) Read the data into R. Check that you have 50 observations on\ntwo variables. Also do something to check that the years of\nexperience and annual salary figures look reasonable overall.\n\n\n\n(b) Make a scatterplot showing how salary depends on\nexperience. Does the nature of the trend make sense?\n\n\n\n(c) Fit a regression predicting salary from experience, and\ndisplay the results. Is the slope positive or negative? Does that\nmake sense?\n\n\n\n(d) Obtain and plot the residuals against the fitted values. What\nproblem do you see?\n\n\n\n(e) The problem you unearthed in the previous part is often helped\nby a transformation. Run Box-Cox on your data to find a suitable\ntransformation. What transformation is suggested?\n\n\n\n(f) Calculate a new variable as suggested by your\ntransformation. Use your transformed response in a regression,\nshowing the summary.\n\n\n\n(g) Obtain and plot the residuals against the fitted values for\nthis regression. Do you seem to have solved the problem with the\nprevious residual plot?\n\n\n\n\n\n\n##  Predicting volume of wood in pine trees\n\n\n In forestry, the financial value of a tree\nis the volume of wood that it contains. This is difficult to estimate\nwhile the tree is still standing, but the diameter is easy to measure\nwith a tape measure (to measure the circumference) and a calculation\ninvolving $\\pi$, assuming that the cross-section of the tree is at\nleast approximately circular.  The standard measurement is\n\"diameter at breast height\" \n(that is, at the height of a human breast or\nchest), defined as being 4.5 feet above the ground.\n\nSeveral pine trees had their diameter measured shortly before being\ncut down, and for each tree, the volume of wood was recorded. The data\nare in\n[link](http://ritsokiguess.site/datafiles/pinetrees.txt). The\ndiameter is in inches and the volume is in cubic inches.  Is it\npossible to predict the volume of wood from the diameter?\n\n\n\n(a) Read the data into R and display the values (there are not\nvery many).\n\n\n\n(b) Make a suitable plot.\n\n\n\n(c) Describe what you learn from your plot about the\nrelationship between diameter and volume, if anything.\n\n\n\n(d) Fit a (linear) regression, predicting volume from diameter,\nand obtain the `summary`. How would you describe the R-squared?\n\n\n\n(e) Draw a graph that will help you decide whether you trust\nthe linearity of this regression. What do you conclude? Explain briefly.\n\n\n\n(f) What would you guess would be the volume of a tree of\ndiameter zero? Is that what the regression predicts? Explain briefly.\n\n\n\n(g) A simple way of modelling a tree's shape is to pretend it is a\ncone, like this, but probably taller and skinnier:\n\n\n![](conebnw.jpg)\n        \n\nwith its base on the ground. What is the relationship between the\n*diameter* (at the base) and volume of a cone? (If you don't\nremember, look it up. You'll probably get a formula in terms of the\nradius, which you'll have to convert.\nCite the website you used.)\n\n\n\n(h) Fit a regression model that predicts volume from diameter\naccording to the formula you obtained in the previous part. You can\nassume that the trees in this data set are of similar heights, so\nthat the height can be treated as a constant.  \nDisplay the\nresults.\n\n\n\n\n \n\n\n\n\n##  Tortoise shells and eggs\n\n\n A biologist measured the length of the carapace (shell) of\nfemale tortoises, and then x-rayed the tortoises to count how many\neggs they were carrying. The length is measured in millimetres. The\ndata are in\n[link](http://ritsokiguess.site/datafiles/tortoise-eggs.txt). The\nbiologist is wondering what kind of relationship, if any, there is\nbetween the carapace length (as an explanatory variable) and the\nnumber of eggs (as a response variable).\n\n\n\n(a) Read in the data, and check that your values look\nreasonable. \n \n\n(b) Obtain a scatterplot, with a smooth trend, of the data.\n\n \n\n(c) The biologist expected that a larger tortoise would be able\nto carry more eggs. Is that what the scatterplot is suggesting?\nExplain briefly why or why not.\n\n \n\n(d) Fit a straight-line relationship and display the summary.\n\n \n\n(e) Add a squared term to your regression, fit that and display\nthe  summary.\n\n \n\n(f) Is a curve better than a line for these data? Justify your\nanswer in two ways: by comparing a measure of fit, and  by doing a\nsuitable test of significance.\n\n \n\n(g) Make a residual plot for the straight line model: that is, plot\nthe residuals against the fitted values.\n Does this echo\nyour conclusions of the previous part? In what way? Explain briefly.\n\n \n\n\n\n\n\n\n\n##  Roller coasters\n\n\n A poll on the Discovery Channel asked people to nominate the\nbest roller-coasters in the United States. We will examine the 10\nroller-coasters that received the most votes. Two features of a\nroller-coaster that are of interest are the distance it drops from\nstart to finish, measured here in feet^[Roller-coasters work by   gravity, so there must be some drop.] and the duration of the ride,\nmeasured in seconds. Is it true that roller-coasters with a bigger\ndrop also tend to have a longer ride? The data are at\n[link](http://ritsokiguess.site/datafiles/coasters.csv).^[These are not to be confused with what your mom insists that you place between your coffee mug and the table.]\n\n\n\n(a) Read the data into R and verify that you have a sensible\nnumber of rows and columns.\n\n\n\n(b) Make a scatterplot of duration (response) against drop\n(explanatory), labelling each roller-coaster with its name in such a\nway that the labels do not overlap. Add a regression line to your plot.\n\n\n\n(c) Would you say that roller-coasters with a larger drop tend\nto have a longer ride? Explain briefly.\n\n\n\n(d) Find a roller-coaster that is unusual compared to the\nothers. What about its combination of `drop` and\n`duration` is unusual?\n\n\n\n\n\n\n\n##  Running and blood sugar\n\n\n A diabetic wants to know how aerobic exercise affects his\nblood sugar. When his blood sugar reaches 170 (mg/dl), he goes out for\na run at a pace of 10 minutes per mile. He runs different distances on\ndifferent days. Each time he runs, he measures his blood sugar after\nthe run. (The preferred blood sugar level is between 80 and 120 on\nthis scale.) The data are in the file\n[link](http://ritsokiguess.site/datafiles/runner.txt).  Our aim is\nto predict blood sugar from distance.\n\n\n\n(a) Read in the data and display the data frame that you read\nin.\n \n\n(b) Make a scatterplot and add a smooth trend to it.\n \n\n(c) Would you say that the relationship between blood sugar and\nrunning distance is approximately linear, or not? It is therefore\nreasonable to use a regression of blood sugar on distance? Explain briefly.\n \n\n(d) Fit a suitable regression, and obtain the regression output.\n \n\n(e) How would you *interpret* the slope? That is, what is\nthe slope, and what does that mean about blood sugar and running distance?\n \n\n(f) Is there a (statistically) significant relationship between\nrunning distance and blood sugar? How do you know? Do you find this\nsurprising, given what you have seen so far? Explain briefly.\n \n\n(g) This diabetic is planning to go for a 3-mile run tomorrow\nand a 5-mile run the day after. Obtain suitable 95\\% intervals that\nsay what his blood sugar might be after each of these runs. \n \n\n(h) Which of your two intervals is longer? Does this make\nsense? Explain briefly.\n \n\n\n\n\n\n##  Calories and fat in pizza\n\n\n \nThe file at\n[link](http://ritsokiguess.site/datafiles/Pizza.csv) \ncame from a spreadsheet of information about 24 brands\nof pizza: specifically, per 5-ounce serving, the number of calories,\nthe grams of fat, and the cost (in US dollars). The names of the pizza\nbrands are quite long. This file may open in a spreadsheet when you\nbrowse to the link, depending on your computer's setup.\n\n\n\n(a) Read in the data and display at least some of the data\nframe. Are the variables of the right types? (In particular, why is\nthe number of calories labelled one way and the cost labelled a\ndifferent way?)\n\n\n(b) Make a scatterplot for predicting calories from the number\nof grams of fat. Add a smooth trend. What kind of relationship do\nyou see, if any?\n\n\n(c) Fit a straight-line relationship, and display the intercept,\nslope, R-squared, etc. Is there a real relationship between the two\nvariables, or is any apparent trend just chance?\n\n\n(d) Obtain a plot of the residuals against the fitted values\nfor this regression. Does this indicate that there are any problems\nwith this regression, or not? Explain briefly.\n\n\n(e) The research assistant in this study returns with two\nnew brands of pizza (ones that were not in the original data). The\nfat content of a 5-ounce serving was 12 grams for the first brand\nand 20 grams for the second brand. For each of these brands of\npizza, obtain a suitable 95\\% interval for the number of calories\ncontained in a 5-ounce serving.\n\n\n\n\n\n\n##  Where should the fire stations be?\n\n\n In city planning, one major issue is where to locate fire\nstations. If a city has too many fire stations, it will spend too much\non running them, but if it has too few, there may be unnecessary fire\ndamage because the fire trucks take too long to get to the fire.\n\nThe first part of a study of this kind of issue is to understand the\nrelationship between the distance from the fire station (measured in\nmiles in our data set) and the amount of fire damage caused (measured\nin thousands of dollars). A city recorded the fire damage and distance\nfrom fire station for 15 residential fires (which you can take as a\nsample of \"all possible residential fires in that city\"). The data\nare in [link](http://ritsokiguess.site/datafiles/fire_damage.txt). \n\n\n\n(a) Read in and display the data, verifying that you have the\nright number of rows and the right columns.\n\n\n\n\n(b) <a name=\"part:ttest\">*</a> Obtain a 95\\% confidence interval for the\nmean fire damage. (There is nothing here from STAD29, and your\nanswer should have nothing to do with distance.)\n\n\n\n(c) Draw a scatterplot for predicting the amount of fire damage\nfrom the distance from the fire station. Add a smooth trend to your\nplot. \n\n\n\n(d) <a name=\"part:howgood\">*</a> Is there a relationship between distance from fire station\nand fire damage? Is it linear or definitely curved? How strong is\nit? Explain briefly.\n\n\n\n(e) Fit a regression predicting fire damage from distance. How\nis the R-squared consistent (or inconsistent) with your answer from\npart~(<a href=\"#part:howgood\">here</a>)?\n\n\n\n(f) <a name=\"part:cim\">*</a> Obtain a 95% confidence interval for the mean fire damage\n*for a residence that is 4 miles from the nearest fire station*. \n(Note the contrast with part~(<a href=\"#part:ttest\">here</a>).)\n\n\n\n(g) Compare the confidence intervals of parts\n(<a href=\"#part:ttest\">here</a>) and (<a href=\"#part:cim\">here</a>). Specifically, compare their\ncentres and their lengths, and explain briefly why the results\nmake sense.\n\n\n\n\n\n\n\n\n\n\n\n## Making it stop\n\n If you are driving, and you hit the brakes, how far do you travel before coming to a complete stop? \nPresumably this depends on how fast you are going. \nKnowing this relationship is important in setting speed limits on roads. For example, on a very bendy road, the speed limit needs to be low, because you cannot see very far ahead, and there could be something just\nout of sight that you need to stop for. \n\nData were collected for a typical car and driver, as shown in [http://ritsokiguess.site/datafiles/stopping.csv](http://ritsokiguess.site/datafiles/stopping.csv). These are American data, so the speeds are miles per hour and the stopping distances are in feet.\n\n\n\n(a) Read in and display (probably all of) the data.\n\n\n\n(b) Make a suitable plot of the data.\n\n\n\n(c) Describe any trend you see in your graph.\n\n\n\n(d) Fit a linear regression predicting stopping distance from speed. (You might have some misgivings about doing this, but do it anyway.)\n\n\n\n(e) Plot the residuals against the fitted values for this regression.\n\n\n\n(f) What do you learn from the residual plot? Does that surprise you? Explain briefly. \n\n\n\n(g) What is the actual relationship between stopping distance and speed, according to the physics? See if you can find out. Cite any books or websites that you use: that is, include a link to a website, or give enough information about a book that the grader could find it.\n\n\n\n(h) Fit the relationship that your research indicated (in the previous part) and display the results. Comment briefly on the R-squared value.\n\n\n\n\n\n(i) Somebody says to you \"if you have a regression with a high R-squared, like 95%, there is no need to look for a better model.\" How do you respond to this? Explain briefly.\n\n\n\n\n\n\n\n\n\n## Predicting height from foot length\n\n Is it possible to estimate the height of a person from the length of their foot? To find out, 33 (male) students had their height and foot length measured. The data are in \n[http://ritsokiguess.site/datafiles/heightfoot.csv](http://ritsokiguess.site/datafiles/heightfoot.csv).\n\n\n\n(a) Read in and display (some of) the data. (If you are having trouble, make sure you have *exactly* the right URL. The correct URL has no spaces or other strange characters in it.)\n\n\n\n(b) Make a suitable plot of the two variables in the data frame.\n\n\n\n(c) Are there any observations not on the trend of the other points? What is unusual about those observations? \n\n\n\n(d) Fit a regression predicting height from foot length, *including* any observations that you identified in the previous part. For that regression, plot the residuals against the fitted values and make a normal quantile plot of the residuals.\n\n\n\n\n(e) Earlier, you identified one or more observations that were off the trend. How does this point or points show up on each of the plots you drew in the previous part?\n\n\n\n(f) Any data points that concerned you earlier were actually errors.\nCreate and save a new data frame that does not contain any of those data points. \n\n\n\n(g) Run a regression predicting height from foot length for your data set without errors. Obtain a plot of the residuals against fitted values and a normal quantile plot of the residuals for this regression.\n\n\n\n\n(h) Do you see any problems on the plots you drew in the previous part? Explain briefly. \n\n\n\n(i) Find a way to plot the data and *both* regression lines on the same plot, in such a way that you can see which regression line is which. If you get help from anything outside the course materials, cite your source(s).\n\n\n\n(j) Discuss briefly how removing the observation(s) that were errors has changed where the regression line goes, and whether that is what you expected.\n\n\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Rainfall in California\n\n\n The data in\n[link](http://ritsokiguess.site/datafiles/calirain.txt) are\nrainfall and other measurements for 30 weather stations in\nCalifornia. Our aim is to understand how well the annual rainfall at\nthese stations (measured in inches) can be predicted from the other\nmeasurements, which are the altitude (in feet above sea level), the\nlatitude (degrees north of the equator) and the distance from the\ncoast (in miles).\n\n\n\n(a) Read the data into R. You'll have to be careful here, since the\nvalues are space-delimited, but sometimes by more than one space, to\nmake the columns line up.  `read_table`, with filename or\nurl, will read it in.\nOne of the variables\nis called `rainfall`, so as long as you *do not* call\nthe data frame that, you should be safe.\n\n\nSolution\n\n\nI used `rains` as the name of my data frame: \n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url=\"http://ritsokiguess.site/datafiles/calirain.txt\"\nrains=read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  station = col_character(),\n  rainfall = col_double(),\n  altitude = col_double(),\n  latitude = col_double(),\n  fromcoast = col_double()\n)\n```\n:::\n:::\n\n \nI have the right number of rows and columns.\n\nI don't need you to investigate the data yet (that happens in the next\npart), but this is interesting (to me):\n\n::: {.cell}\n\n```{.r .cell-code}\nrains\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"station\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"rainfall\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"altitude\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"latitude\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fromcoast\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Eureka\",\"2\":\"39.57\",\"3\":\"43\",\"4\":\"40.8\",\"5\":\"1\"},{\"1\":\"RedBluff\",\"2\":\"23.27\",\"3\":\"341\",\"4\":\"40.2\",\"5\":\"97\"},{\"1\":\"Thermal\",\"2\":\"18.20\",\"3\":\"4152\",\"4\":\"33.8\",\"5\":\"70\"},{\"1\":\"FortBragg\",\"2\":\"37.48\",\"3\":\"74\",\"4\":\"39.4\",\"5\":\"1\"},{\"1\":\"SodaSprings\",\"2\":\"49.26\",\"3\":\"6752\",\"4\":\"39.3\",\"5\":\"150\"},{\"1\":\"SanFrancisco\",\"2\":\"21.82\",\"3\":\"52\",\"4\":\"37.8\",\"5\":\"5\"},{\"1\":\"Sacramento\",\"2\":\"18.07\",\"3\":\"25\",\"4\":\"38.5\",\"5\":\"80\"},{\"1\":\"SanJose\",\"2\":\"14.17\",\"3\":\"95\",\"4\":\"37.4\",\"5\":\"28\"},{\"1\":\"GiantForest\",\"2\":\"42.63\",\"3\":\"6360\",\"4\":\"36.6\",\"5\":\"145\"},{\"1\":\"Salinas\",\"2\":\"13.85\",\"3\":\"74\",\"4\":\"36.7\",\"5\":\"12\"},{\"1\":\"Fresno\",\"2\":\"9.44\",\"3\":\"331\",\"4\":\"36.7\",\"5\":\"114\"},{\"1\":\"PtPiedras\",\"2\":\"19.33\",\"3\":\"57\",\"4\":\"35.7\",\"5\":\"1\"},{\"1\":\"PasaRobles\",\"2\":\"15.67\",\"3\":\"740\",\"4\":\"35.7\",\"5\":\"31\"},{\"1\":\"Bakersfield\",\"2\":\"6.00\",\"3\":\"489\",\"4\":\"35.4\",\"5\":\"75\"},{\"1\":\"Bishop\",\"2\":\"5.73\",\"3\":\"4108\",\"4\":\"37.3\",\"5\":\"198\"},{\"1\":\"Mineral\",\"2\":\"47.82\",\"3\":\"4850\",\"4\":\"40.4\",\"5\":\"142\"},{\"1\":\"SantaBarbara\",\"2\":\"17.95\",\"3\":\"120\",\"4\":\"34.4\",\"5\":\"1\"},{\"1\":\"Susanville\",\"2\":\"18.20\",\"3\":\"4152\",\"4\":\"40.3\",\"5\":\"198\"},{\"1\":\"TuleLake\",\"2\":\"10.03\",\"3\":\"4036\",\"4\":\"41.9\",\"5\":\"140\"},{\"1\":\"Needles\",\"2\":\"4.63\",\"3\":\"913\",\"4\":\"34.8\",\"5\":\"192\"},{\"1\":\"Burbank\",\"2\":\"14.74\",\"3\":\"699\",\"4\":\"34.2\",\"5\":\"47\"},{\"1\":\"LosAngeles\",\"2\":\"15.02\",\"3\":\"312\",\"4\":\"34.1\",\"5\":\"16\"},{\"1\":\"LongBeach\",\"2\":\"12.36\",\"3\":\"50\",\"4\":\"33.8\",\"5\":\"12\"},{\"1\":\"LosBanos\",\"2\":\"8.26\",\"3\":\"125\",\"4\":\"37.8\",\"5\":\"74\"},{\"1\":\"Blythe\",\"2\":\"4.05\",\"3\":\"268\",\"4\":\"33.6\",\"5\":\"155\"},{\"1\":\"SanDiego\",\"2\":\"9.94\",\"3\":\"19\",\"4\":\"32.7\",\"5\":\"5\"},{\"1\":\"Daggett\",\"2\":\"4.25\",\"3\":\"2105\",\"4\":\"34.1\",\"5\":\"85\"},{\"1\":\"DeathValley\",\"2\":\"1.66\",\"3\":\"-178\",\"4\":\"36.5\",\"5\":\"194\"},{\"1\":\"CrescentCity\",\"2\":\"74.87\",\"3\":\"35\",\"4\":\"41.7\",\"5\":\"1\"},{\"1\":\"Colusa\",\"2\":\"15.95\",\"3\":\"60\",\"4\":\"39.2\",\"5\":\"91\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nSome of the station names are two words, but they have been smooshed\ninto one word, so that `read_table` will recognize them as a\nsingle thing. Someone had already done that for us, so I didn't even\nhave to do it myself. \n\nIf the station names had been two genuine words, a `.csv` would\nprobably have been the best choice (the actual data values being\nseparated by commas then, and not spaces).\n\n$\\blacksquare$\n\n(b) Make a boxplot of the rainfall figures, and explain why the\nvalues are reasonable. (A rainfall cannot be negative, and it is\nunusual for a annual rainfall to exceed 60 inches.) A\n`ggplot` boxplot needs *something* on the $x$-axis: the\nnumber 1 will do.\n\n\nSolution\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(rains,aes(y=rainfall,x=1))+geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/calirain-3-1.png){width=672}\n:::\n:::\n\n     \nThere is only one rainfall over 60 inches, and the smallest one is\nclose to zero but positive, so that is good.\n\nAnother possible plot here is a histogram, since there is only one quantitative variable:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(rains, aes(x=rainfall))+geom_histogram(bins=7)\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/calirain-4-1.png){width=672}\n:::\n:::\n\n\n\nThis clearly shows the rainfall value above 60 inches, but some other things are less clear: are those two rainfall values around 50 inches above or below 50, and are those six rainfall values near zero actually above zero? \nExtra: What stations have those extreme values? Should you wish to find out:\n\n::: {.cell}\n\n```{.r .cell-code}\nrains %>% filter(rainfall>60)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"station\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"rainfall\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"altitude\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"latitude\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fromcoast\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"CrescentCity\",\"2\":\"74.87\",\"3\":\"35\",\"4\":\"41.7\",\"5\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \nThis is a place right on the Pacific coast, almost up into Oregon (it's almost\nthe northernmost of all the stations). So it makes sense that it would\nhave a high rainfall, if anywhere does. (If you know anything about\nrainy places, you'll probably think of Vancouver and Seattle, in the\nPacific Northwest.) Here it is:\n[link](https://www.google.ca/maps/place/Crescent+City,+CA,+USA/@41.7552589,-123.9652917,8.42z/data=!4m5!3m4!1s0x54d066375c6288db:0x76e89ab07375e62e!8m2!3d41.7557501!4d-124.2025913). \nWhich station has less than 2 inches of annual rainfall?\n\n::: {.cell}\n\n```{.r .cell-code}\nrains %>% filter(rainfall<2)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"station\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"rainfall\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"altitude\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"latitude\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fromcoast\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"DeathValley\",\"2\":\"1.66\",\"3\":\"-178\",\"4\":\"36.5\",\"5\":\"194\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \nThe name of the station is a clue: this one is in the desert. So you'd\nexpect very little rain. Its altitude is *negative*, so it's\nactually *below* sea level. This is correct. Here is where it is:\n[link](https://www.google.ca/maps/place/Death+Valley,+CA,+USA/@36.6341288,-118.2252974,7.75z/data=!4m5!3m4!1s0x80c739a21e8fffb1:0x1c897383d723dd25!8m2!3d36.5322649!4d-116.9325408).\n\n$\\blacksquare$\n\n(c) Plot `rainfall` against each of the other quantitative\nvariables (that is, not `station`). \n\n\nSolution\n\n\nThat is, `altitude`, `latitude` and\n`fromcoast`. The obvious way to do this (perfectly\nacceptable) is one plot at a time:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(rains,aes(y=rainfall,x=altitude))+geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/calirain-7-1.png){width=672}\n:::\n:::\n\n     \n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(rains,aes(y=rainfall,x=latitude))+geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/calirain-8-1.png){width=672}\n:::\n:::\n\n \n\nand finally \n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(rains,aes(y=rainfall,x=fromcoast))+geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/calirain-9-1.png){width=672}\n:::\n:::\n\n \n\nYou can add a smooth trend to these if you want. Up to you. Just the\npoints is fine with me.\n\nHere is a funky way to get all three plots in one shot:\n\n::: {.cell}\n\n```{.r .cell-code}\nrains %>% \n  pivot_longer(altitude:fromcoast, names_to=\"xname\",values_to=\"x\") %>%\n  ggplot(aes(x=x,y=rainfall))+geom_point()+\n  facet_wrap(~xname,scales=\"free\")\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/calirain-10-1.png){width=672}\n:::\n:::\n\n \n\nThis always seems extraordinarily strange if you haven't run into it\nbefore. The strategy is to put *all* the $x$-variables you want\nto plot into *one* column and then plot your $y$ against the\n$x$-column.  Thus: make a column of all the $x$'s glued\ntogether, labelled by which $x$ they are, then plot $y$ against $x$\nbut make a different sub-plot or \"facet\" for each different\n$x$-name. The last thing is that each $x$ is measured on a different\nscale, and unless we take steps, all the sub-plots will have the\n*same* scale on each axis, which we don't want.\n\nI'm not sure I like how it came out, with three very tall\nplots. `facet_wrap` can also take an `nrow` or an\n`ncol`, which tells it how many rows or columns to use for the\ndisplay. Here, for example, two columns because I thought three was\ntoo many:\n\n::: {.cell}\n\n```{.r .cell-code}\nrains %>% \n  pivot_longer(altitude:fromcoast, names_to=\"xname\",values_to=\"x\") %>%\n  ggplot(aes(x=x,y=rainfall))+geom_point()+\n  facet_wrap(~xname,scales=\"free\",ncol=2)\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/calirain-11-1.png){width=672}\n:::\n:::\n\n \n\nNow, the three plots have come out about square, or at least \"landscape\", which I like a lot better.\n\n$\\blacksquare$\n\n(d) Look at the relationship of each other variable with\n`rainfall`. \nJustify the assertion that `latitude` seems most strongly\nrelated with `rainfall`. Is that relationship positive or negative? linear? Explain\nbriefly. \n\n\nSolution\n\n\nLet's look at the three variables in turn:\n\n\n* `altitude`: not much of anything. The stations near\nsea level have rainfall all over the place, though the three\nhighest-altitude stations have the three highest rainfalls apart\nfrom Crescent City.\n\n* `latitude`: there is a definite upward trend here, in\nthat stations further north (higher latitude) are likely to have\na higher rainfall. I'd call this trend linear (or, not obviously\ncurved), though the two most northerly stations have one higher\nand one much lower rainfall than you'd expect.\n\n* `fromcoast`: this is a weak downward trend, though\nthe trend is spoiled by those three stations about 150 miles\nfrom the coast that have more than 40 inches of rainfall.\n\nOut of those, only `latitude` seems to have any meaningful\nrelationship with `rainfall`.\n\n$\\blacksquare$\n\n(e) Fit a regression with `rainfall` as the response\nvariable, and `latitude` as your explanatory variable. What\nare the intercept, slope and R-squared values? Is there a \n*significant* relationship between `rainfall` and your\nexplanatory variable? What does that mean?\n\n\nSolution\n\n\nSave your `lm` into a\nvariable, since it will get used again later:\n\n::: {.cell}\n\n```{.r .cell-code}\nrainfall.1=lm(rainfall~latitude,data=rains)\nsummary(rainfall.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = rainfall ~ latitude, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.297  -7.956  -2.103   6.082  38.262 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -113.3028    35.7210  -3.172  0.00366 ** \nlatitude       3.5950     0.9623   3.736  0.00085 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.82 on 28 degrees of freedom\nMultiple R-squared:  0.3326,\tAdjusted R-squared:  0.3088 \nF-statistic: 13.96 on 1 and 28 DF,  p-value: 0.0008495\n```\n:::\n:::\n\n \n\nMy intercept is $-113.3$, slope is $3.6$ and R-squared is $0.33$ or\n33\\%. (I want you to pull these numbers out of the output and round\nthem off to something sensible.) The slope is significantly nonzero,\nits P-value being 0.00085: rainfall really does depend on latitude, although\nnot strongly so.\n\nExtra: Of course, I can easily do the others as well, though you don't have to:\n\n::: {.cell}\n\n```{.r .cell-code}\nrainfall.2=lm(rainfall~fromcoast,data=rains)\nsummary(rainfall.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = rainfall ~ fromcoast, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.240  -9.431  -6.603   2.871  51.147 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.77306    4.61296   5.154 1.82e-05 ***\nfromcoast   -0.05039    0.04431  -1.137    0.265    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.54 on 28 degrees of freedom\nMultiple R-squared:  0.04414,\tAdjusted R-squared:   0.01 \nF-statistic: 1.293 on 1 and 28 DF,  p-value: 0.2651\n```\n:::\n:::\n\n \n\nHere, the intercept is 23.8, the slope is $-0.05$ and R-squared is a\ndismal 0.04 (4\\%).  This is a way of seeing that this relationship is\nreally weak, and it doesn't even have a curve to the trend or anything\nthat would compensate for this. I looked at the scatterplot again and\nsaw that if it were not for the point bottom right which is furthest\nfrom the coast and has almost no rainfall, there would be almost no\ntrend at all. The slope here is not significantly different from zero,\nwith a P-value of 0.265.\n\nFinally:\n\n::: {.cell}\n\n```{.r .cell-code}\nrainfall.3=lm(rainfall~altitude,data=rains)\nsummary(rainfall.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = rainfall ~ altitude, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.620  -8.479  -2.729   4.555  58.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 16.514799   3.539141   4.666  6.9e-05 ***\naltitude     0.002394   0.001428   1.676    0.105    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.13 on 28 degrees of freedom\nMultiple R-squared:  0.09121,\tAdjusted R-squared:  0.05875 \nF-statistic:  2.81 on 1 and 28 DF,  p-value: 0.1048\n```\n:::\n:::\n\n \n\nThe intercept is 16.5, the slope is 0.002 and the R-squared is 0.09 or\n9\\%, also terrible. The P-value is 0.105, which is not small enough to\nbe significant. \n\nSo it looks as if it's only `latitude` that\nhas any impact at all. This is the only explanatory variable with a\nsignificantly nonzero slope. On its own, at least.\n\n$\\blacksquare$\n\n(f) Fit a multiple regression predicting `rainfall` from\nall three of the other (quantitative) variables. Display the\nresults. Comment is coming up later.\n\n\nSolution\n\n\nThis, then:\n\n::: {.cell}\n\n```{.r .cell-code}\nrainfall.4=lm(rainfall~latitude+altitude+fromcoast,data=rains)\nsummary(rainfall.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = rainfall ~ latitude + altitude + fromcoast, data = rains)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.722  -5.603  -0.531   3.510  33.317 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.024e+02  2.921e+01  -3.505 0.001676 ** \nlatitude     3.451e+00  7.949e-01   4.342 0.000191 ***\naltitude     4.091e-03  1.218e-03   3.358 0.002431 ** \nfromcoast   -1.429e-01  3.634e-02  -3.931 0.000559 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.1 on 26 degrees of freedom\nMultiple R-squared:  0.6003,\tAdjusted R-squared:  0.5542 \nF-statistic: 13.02 on 3 and 26 DF,  p-value: 2.205e-05\n```\n:::\n:::\n\n \n\n$\\blacksquare$\n\n(g) What is the R-squared for the regression of the last part?\nHow does that compare with the R-squared of your regression in part (e)?\n\n\nSolution\n\n\nThe R-squared is 0.60 (60\\%), which is quite a bit bigger than the\nR-squared of 0.33 (33\\%) we got back in (e). \n\n$\\blacksquare$\n\n(h) What do you conclude about the importance of the variables\nthat you did *not* include in your model in\n(e)? Explain briefly.\n\n\nSolution\n\n\nBoth variables `altitude` and `fromcoast` are\nsignificant in this regression, so they have *something to add* over and above `latitude` when it comes to\npredicting rainfall, even though (and this seems odd) they have no\napparent relationship with `rainfall` on their own. \nAnother way to say this is that the three variables work together\nas a team to predict rainfall, and together they do much better\nthan any one of them can do by themselves.  \nThis also goes to show that the scatterplots we began\nwith don't get to the heart of multi-variable relationships,\nbecause they are only looking at the variables two at a time.\n\n$\\blacksquare$\n\n(i) Make a suitable hypothesis test that the variables\n`altitude` and `fromcoast` significantly improve the\nprediction of `rainfall` over the use of `latitude`\nalone. What do you conclude?\n\n\nSolution\n\n\nThis calls for `anova`. Feed this two fitted models,\nsmaller (fewer explanatory variables) first. The null hypothesis\nis that the two models are equally good (so we should go with the\nsmaller); the alternative is that the larger model is better, so\nthat the extra complication is worth it:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(rainfall.1,rainfall.4)  \n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Res.Df\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"RSS\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Df\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sum of Sq\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Pr(>F)\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"28\",\"2\":\"5346.766\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"_rn_\":\"1\"},{\"1\":\"26\",\"2\":\"3202.298\",\"3\":\"2\",\"4\":\"2144.469\",\"5\":\"8.705653\",\"6\":\"0.0012759\",\"_rn_\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nThe P-value is small, so we reject the null in favour of the\nalternative: the regression with all three explanatory variables fits\nbetter than the one with just `latitude`, so the bigger model\nis the one we should go with.\n\nIf you have studied these things: this one is a \n\"multiple-partial $F$-test\", for testing the combined significance of more than one $x$\nbut less than all the $x$'s.^[If you had just one $x$, you'd  use a $t$-test for its slope, and if you were testing all the $x$'s, you'd use the global $F$-test that appears in the regression output.]\n\n$\\blacksquare$\n\n\n\n\n\n\n##  Carbon monoxide in cigarettes\n\n\n The (US) Federal Trade Commission assesses cigarettes\naccording to their tar, nicotine and carbon monoxide contents. In a\nparticular year, 25 brands were assessed. For each brand, the tar,\nnicotine and carbon monoxide (all in milligrams) were measured, along\nwith the weight in grams. Our aim is to predict carbon monoxide from\nany or all of the other variables. The data are in\n[link](http://ritsokiguess.site/datafiles/ftccigar.txt). These are\naligned by column (except for the variable names), with more than one\nspace between each column of data.\n\n\n(a) Read the data into R, and check that you have 25 observations\nand 4 variables.\n\n\nSolution\n\n\nThis specification calls for `read_table`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/ftccigar.txt\"\ncigs <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  tar = col_double(),\n  nicotine = col_double(),\n  weight = col_double(),\n  co = col_double()\n)\n```\n:::\n\n```{.r .cell-code}\ncigs\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"tar\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"nicotine\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"weight\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"co\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"14.1\",\"2\":\"0.86\",\"3\":\"0.9853\",\"4\":\"13.6\"},{\"1\":\"16.0\",\"2\":\"1.06\",\"3\":\"1.0938\",\"4\":\"16.6\"},{\"1\":\"29.8\",\"2\":\"2.03\",\"3\":\"1.1650\",\"4\":\"23.5\"},{\"1\":\"8.0\",\"2\":\"0.67\",\"3\":\"0.9280\",\"4\":\"10.2\"},{\"1\":\"4.1\",\"2\":\"0.40\",\"3\":\"0.9462\",\"4\":\"5.4\"},{\"1\":\"15.0\",\"2\":\"1.04\",\"3\":\"0.8885\",\"4\":\"15.0\"},{\"1\":\"8.8\",\"2\":\"0.76\",\"3\":\"1.0267\",\"4\":\"9.0\"},{\"1\":\"12.4\",\"2\":\"0.95\",\"3\":\"0.9225\",\"4\":\"12.3\"},{\"1\":\"16.6\",\"2\":\"1.12\",\"3\":\"0.9372\",\"4\":\"16.3\"},{\"1\":\"14.9\",\"2\":\"1.02\",\"3\":\"0.8858\",\"4\":\"15.4\"},{\"1\":\"13.7\",\"2\":\"1.01\",\"3\":\"0.9643\",\"4\":\"13.0\"},{\"1\":\"15.1\",\"2\":\"0.90\",\"3\":\"0.9316\",\"4\":\"14.4\"},{\"1\":\"7.8\",\"2\":\"0.57\",\"3\":\"0.9705\",\"4\":\"10.0\"},{\"1\":\"11.4\",\"2\":\"0.78\",\"3\":\"1.1240\",\"4\":\"10.2\"},{\"1\":\"9.0\",\"2\":\"0.74\",\"3\":\"0.8517\",\"4\":\"9.5\"},{\"1\":\"1.0\",\"2\":\"0.13\",\"3\":\"0.7851\",\"4\":\"1.5\"},{\"1\":\"17.0\",\"2\":\"1.26\",\"3\":\"0.9186\",\"4\":\"18.5\"},{\"1\":\"12.8\",\"2\":\"1.08\",\"3\":\"1.0395\",\"4\":\"12.6\"},{\"1\":\"15.8\",\"2\":\"0.96\",\"3\":\"0.9573\",\"4\":\"17.5\"},{\"1\":\"4.5\",\"2\":\"0.42\",\"3\":\"0.9106\",\"4\":\"4.9\"},{\"1\":\"14.5\",\"2\":\"1.01\",\"3\":\"1.0070\",\"4\":\"15.9\"},{\"1\":\"7.3\",\"2\":\"0.61\",\"3\":\"0.9806\",\"4\":\"8.5\"},{\"1\":\"8.6\",\"2\":\"0.69\",\"3\":\"0.9693\",\"4\":\"10.6\"},{\"1\":\"15.2\",\"2\":\"1.02\",\"3\":\"0.9496\",\"4\":\"13.9\"},{\"1\":\"12.0\",\"2\":\"0.82\",\"3\":\"1.1184\",\"4\":\"14.9\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \nYes, I have 25 observations on 4 variables indeed.\n\n`read_delim` won't work (try it and see what happens), because\nthat would require the values to be separated by *exactly one* space.\n\n\n$\\blacksquare$\n\n(b) Run a regression to predict carbon monoxide from the other\nvariables, and obtain a summary of the output.\n\n\nSolution\n\n\nThe word \"summary\" is meant to be a big clue that\n`summary` is what you need:\n\n::: {.cell}\n\n```{.r .cell-code}\ncigs.1 <- lm(co ~ tar + nicotine + weight, data = cigs)\nsummary(cigs.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = co ~ tar + nicotine + weight, data = cigs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89261 -0.78269  0.00428  0.92891  2.45082 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.2022     3.4618   0.925 0.365464    \ntar           0.9626     0.2422   3.974 0.000692 ***\nnicotine     -2.6317     3.9006  -0.675 0.507234    \nweight       -0.1305     3.8853  -0.034 0.973527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.446 on 21 degrees of freedom\nMultiple R-squared:  0.9186,\tAdjusted R-squared:  0.907 \nF-statistic: 78.98 on 3 and 21 DF,  p-value: 1.329e-11\n```\n:::\n:::\n\n \n\n$\\blacksquare$\n\n(c) Which one of your explanatory variables would you\nremove from this regression? Explain (very) briefly. Go ahead and\nfit the regression  without it, and describe how the change in\nR-squared from the regression in (b) was entirely predictable.\n\n\nSolution\n\n\nFirst, the $x$-variable to remove. The obvious candidate is\n`weight`, since it has easily the highest, and clearly\nnon-significant, P-value. So, out it comes:\n\n::: {.cell}\n\n```{.r .cell-code}\ncigs.2 <- lm(co ~ tar + nicotine, data = cigs)\nsummary(cigs.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = co ~ tar + nicotine, data = cigs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89941 -0.78470 -0.00144  0.91585  2.43064 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.0896     0.8438   3.662 0.001371 ** \ntar           0.9625     0.2367   4.067 0.000512 ***\nnicotine     -2.6463     3.7872  -0.699 0.492035    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.413 on 22 degrees of freedom\nMultiple R-squared:  0.9186,\tAdjusted R-squared:  0.9112 \nF-statistic: 124.1 on 2 and 22 DF,  p-value: 1.042e-12\n```\n:::\n:::\n\n \n\nR-squared has dropped from 0.9186 to \\ldots 0.9186! That is, taking\nout `weight` has not just had a minimal effect on R-squared;\nit's not changed R-squared at all. This is because `weight` was\nso far from being significant: it literally had *nothing* to add.\n\nAnother way of achieving the same thing is via the function\n`update`, which takes a fitted model object and describes the\n*change* that you want to make:\n\n::: {.cell}\n\n```{.r .cell-code}\ncigs.2a <- update(cigs.1, . ~ . - weight)\nsummary(cigs.2a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = co ~ tar + nicotine, data = cigs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89941 -0.78470 -0.00144  0.91585  2.43064 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.0896     0.8438   3.662 0.001371 ** \ntar           0.9625     0.2367   4.067 0.000512 ***\nnicotine     -2.6463     3.7872  -0.699 0.492035    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.413 on 22 degrees of freedom\nMultiple R-squared:  0.9186,\tAdjusted R-squared:  0.9112 \nF-statistic: 124.1 on 2 and 22 DF,  p-value: 1.042e-12\n```\n:::\n:::\n\n \n\nThis can be shorter than describing the whole model again, as you do\nwith the `cigs.2` version of `lm`. The syntax is that\nyou first specify a \"base\"  fitted model object that you're going to\nupdate. Because the model `cigs.1` contains all the information\nabout the kind of model it is, and which data frame the data come\nfrom, R already knows that this is a linear \nmultiple regression and which $x$'s it contains. The second thing to describe is the change from\nthe \"base\". In this case, we want to use the same response variable\nand all the same explanatory variables that we had before, except for\n`weight`. This is specified by a special kind of model formula\nwhere `.` means \"whatever was there before\": in English,\n\"same response and same explanatories except take out `weight`\". \n\n$\\blacksquare$\n\n(d) Fit a regression predicting carbon monoxide from\n`nicotine` *only*, and display the summary.\n\n\nSolution\n\n\nAs you would guess:\n\n::: {.cell}\n\n```{.r .cell-code}\ncigs.3 <- lm(co ~ nicotine, data = cigs)\nsummary(cigs.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = co ~ nicotine, data = cigs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3273 -1.2228  0.2304  1.2700  3.9357 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.6647     0.9936   1.675    0.107    \nnicotine     12.3954     1.0542  11.759 3.31e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.828 on 23 degrees of freedom\nMultiple R-squared:  0.8574,\tAdjusted R-squared:  0.8512 \nF-statistic: 138.3 on 1 and 23 DF,  p-value: 3.312e-11\n```\n:::\n:::\n\n \n\n$\\blacksquare$\n\n(e) `nicotine` was far from being significant in the model\nof (c), and yet in the model of\n(d), it was *strongly* significant, and the\nR-squared value of (d) was almost as high as that\nof (c). What does this say about the importance of\n`nicotine` as an explanatory variable? Explain, as briefly as\nyou can manage.\n\n\nSolution\n\n\nWhat this says is that you *cannot* say anything about the\n\"importance\" of `nicotine` without also describing the\ncontext that you're talking about.  *By itself*,\n`nicotine` is important, but *when you have\n`tar` in the model*, `nicotine` is not\nimportant: precisely, it now has nothing to add over and above\nthe predictive value that `tar` has. You might guess that\nthis is because `tar` and `nicotine` are \n\"saying  the same thing\" in some fashion. \nWe'll explore that in a moment.\n\n$\\blacksquare$\n\n(f) Make a \"pairs plot\": that is, scatter plots between all\npairs of variables. This can be done by feeding the whole data frame\ninto `plot`.^[This is a base graphics graph rather than a `ggplot` one, but it will do for our purposes.]\nDo you see any strong relationships that do\n*not* include `co`? Does that shed any light on the last\npart? Explain briefly (or \"at length\" if that's how it comes\nout). \n\n\nSolution\n\n\nPlot the entire data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(cigs)\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/ftccigar-6-1.png){width=672}\n:::\n:::\n\n \n\nWe're supposed to ignore `co`, but I comment that strong\nrelationships between `co` and *both* of `tar` and\n`nicotine` show up here, along with `weight` being\nat most weakly related to anything else.\n\nThat leaves the relationship of `tar` and `nicotine`\nwith each other. That also looks like a strong linear trend. When you\nhave correlations between explanatory variables, it is called\n\"multicollinearity\". \n\nHaving correlated $x$'s is\ntrouble. Here is where we find out why. The problem is that when\n`co` is large, `nicotine` is large, and a large value of\n`tar` will come along with it. So we don't know whether a large\nvalue of `co` is caused by a large value of `tar` or a\nlarge value of `nicotine`: there is no way to separate out\ntheir effects because in effect they are \"glued together\". \n\nYou might know of this effect (in an experimental design context) as\n\"confounding\": the effect of `tar` on `co` is\nconfounded with the effect of `nicotine` on `co`, and\nyou can't tell which one deserves the credit for predicting `co`.\n\nIf you were able to design an experiment here, you could (in\nprinciple) manufacture a bunch of cigarettes with high tar; some of\nthem would have high  nicotine and some would have low. Likewise for\nlow tar. Then the\ncorrelation between `nicotine` and `tar` would go away,\ntheir effects on `co` would no longer be confounded, and you\ncould see unambiguously which one of the variables deserves credit for\npredicting `co`. Or maybe it depends on both, genuinely, but at\nleast then you'd know.\n\nWe, however, have an observational study, so we have to make do with\nthe data we have. Confounding is one of the risks we take when we work\nwith observational data.\n\nThis was a \"base graphics\" plot. There is a way of doing a\n`ggplot`-style \"pairs plot\", as this is called, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GGally)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n```{.r .cell-code}\ncigs %>% ggpairs(progress = FALSE)\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/tober-1.png){width=672}\n:::\n:::\n\n \n\nAs ever, `install.packages` first, in the likely event that you\ndon't have this package installed yet. Once you do, though, I think\nthis is a nicer way to get a pairs plot.\n\nThis plot is a bit more sophisticated: instead of just having the\nscatterplots of the pairs of variables in the row and column, it uses\nthe diagonal to show a \"kernel density\" (a smoothed-out histogram),\nand upper-right it shows the correlation between each pair of\nvariables. The three correlations between `co`, `tar`\nand `nicotine` are clearly the highest.\n\nIf you want only some of the columns to appear in your pairs plot,\n`select` them first, and then pass that data frame into\n`ggpairs`. Here, we found that `weight` was not\ncorrelated with anything much, so we can take it out and then make a\npairs plot of the other variables:\n\n::: {.cell}\n\n```{.r .cell-code}\ncigs %>% select(-weight) %>% ggpairs(progress = FALSE)\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/ftccigar-7-1.png){width=672}\n:::\n:::\n\n \n\nThe three correlations that remain are all very high, which is\nentirely consistent with the strong linear relationships that you see\nbottom left.\n\n$\\blacksquare$\n\n\n\n\n##  Maximal oxygen uptake in young boys\n\n\n A physiologist wanted to understand the relationship between\nphysical characteristics of pre-adolescent boys and their maximal\noxygen uptake (millilitres of oxygen per kilogram of body weight). The\ndata are in\n[link](http://ritsokiguess.site/datafiles/youngboys.txt) for a\nrandom sample of 10 pre-adolescent boys. The variables are (with\nunits):\n\n\n\n* `uptake`: Oxygen uptake (millitres of oxygen per kilogram\nof body weight)\n\n* `age`: boy's age (years)\n\n* `height`: boy's height (cm)\n\n* `weight`: boy's weight (kg)\n\n* `chest`: chest depth (cm).\n\n\n\n\n(a) Read the data into R and confirm that you do indeed have\n10 observations.\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/youngboys.txt\"\nboys <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 10 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (5): uptake, age, height, weight, chest\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nboys\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"uptake\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"age\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"height\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"weight\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"chest\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1.54\",\"2\":\"8.4\",\"3\":\"132.0\",\"4\":\"29.1\",\"5\":\"14.4\"},{\"1\":\"1.74\",\"2\":\"8.7\",\"3\":\"135.5\",\"4\":\"29.7\",\"5\":\"14.5\"},{\"1\":\"1.32\",\"2\":\"8.9\",\"3\":\"127.7\",\"4\":\"28.4\",\"5\":\"14.0\"},{\"1\":\"1.50\",\"2\":\"9.9\",\"3\":\"131.1\",\"4\":\"28.8\",\"5\":\"14.2\"},{\"1\":\"1.46\",\"2\":\"9.0\",\"3\":\"130.0\",\"4\":\"25.9\",\"5\":\"13.6\"},{\"1\":\"1.35\",\"2\":\"7.7\",\"3\":\"127.6\",\"4\":\"27.6\",\"5\":\"13.9\"},{\"1\":\"1.53\",\"2\":\"7.3\",\"3\":\"129.9\",\"4\":\"29.0\",\"5\":\"14.0\"},{\"1\":\"1.71\",\"2\":\"9.9\",\"3\":\"138.1\",\"4\":\"33.6\",\"5\":\"14.6\"},{\"1\":\"1.27\",\"2\":\"9.3\",\"3\":\"126.6\",\"4\":\"27.7\",\"5\":\"13.9\"},{\"1\":\"1.50\",\"2\":\"8.1\",\"3\":\"131.8\",\"4\":\"30.8\",\"5\":\"14.5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n         \n\n10 boys (rows) indeed.\n\n$\\blacksquare$\n\n(b) Fit a regression predicting oxygen uptake from all the\nother variables, and display the results.\n\n\nSolution\n\n\nFitting four explanatory variables with only ten observations is likely to be pretty shaky, but we \npress ahead regardless:\n\n::: {.cell}\n\n```{.r .cell-code}\nboys.1 <- lm(uptake ~ age + height + weight + chest, data = boys)\nsummary(boys.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = uptake ~ age + height + weight + chest, data = boys)\n\nResiduals:\n        1         2         3         4         5         6         7         8 \n-0.020697  0.019741 -0.003649  0.038470 -0.023639 -0.026026  0.050459 -0.014380 \n        9        10 \n 0.004294 -0.024573 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.774739   0.862818  -5.534 0.002643 ** \nage         -0.035214   0.015386  -2.289 0.070769 .  \nheight       0.051637   0.006215   8.308 0.000413 ***\nweight      -0.023417   0.013428  -1.744 0.141640    \nchest        0.034489   0.085239   0.405 0.702490    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03721 on 5 degrees of freedom\nMultiple R-squared:  0.9675,\tAdjusted R-squared:  0.9415 \nF-statistic:  37.2 on 4 and 5 DF,  p-value: 0.0006513\n```\n:::\n:::\n\n         \n\n$\\blacksquare$\n\n(c) (A one-mark question.) Would you say, on the evidence so\nfar, that the regression fits well or badly?  Explain (very)\nbriefly.\n\n\nSolution\n\n\nR-squared of 0.97 (97\\%) is very high, so I'd say this\nregression fits very well. That's all.\nI said \"on the evidence so far\" to dissuade you from\noverthinking this, or thinking that you needed to produce\nsome more evidence. That, plus the fact that this was only\none mark.\n\n$\\blacksquare$\n\n(d) It seems reasonable that an older boy should have a\ngreater oxygen uptake, all else being equal. Is this supported\nby your output?  Explain briefly.\n\n\nSolution\n\n\nIf an older boy has greater oxygen uptake (the \"all else   equal\" was a hint), \nthe slope of `age` should be\npositive. It is not: it is $-0.035$, so it is suggesting\n(all else equal) that a greater age goes with a\n*smaller* oxygen uptake.\nThe reason why this happens (which you didn't need, but\nyou can include it if you like) is that `age` has a\nnon-small P-value of 0.07, so that the `age` slope\nis not significantly different from zero. With all the\nother variables, `age` has nothing to *add*\nover and above them, and we could therefore remove it.\n\n$\\blacksquare$\n\n(e) It seems reasonable that a boy with larger weight\nshould have larger lungs and thus a *statistically\nsignificantly* larger oxygen uptake. Is that what happens\nhere? Explain briefly.\n\n\nSolution\n\n\nLook at the P-value for `weight`. This is 0.14,\nnot small, and so a boy with larger weight does not have\na significantly larger oxygen uptake, all else\nequal. (The slope for `weight` is not\nsignificantly different from zero either.)\nI emphasized \"statistically significant\" to remind you\nthat this means to do a test and get a P-value.\n\n$\\blacksquare$\n\n(f) Fit a model that contains only the significant\nexplanatory variables from your first regression. How do\nthe R-squared values from the two regressions compare?\n(The last sentence asks for more or less the same thing as\nthe next part. Answer it either here or there. Either\nplace is good.)\n\n\nSolution\n\n\nOnly `height` is significant, so that's the\nonly explanatory variable we need to keep. I would\njust do the regression straight rather than using\n`update` here:\n\n::: {.cell}\n\n```{.r .cell-code}\nboys.2 <- lm(uptake ~ height, data = boys)\nsummary(boys.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = uptake ~ height, data = boys)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.069879 -0.033144  0.001407  0.009581  0.084012 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.843326   0.609198  -6.309 0.000231 ***\nheight       0.040718   0.004648   8.761 2.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05013 on 8 degrees of freedom\nMultiple R-squared:  0.9056,\tAdjusted R-squared:  0.8938 \nF-statistic: 76.75 on 1 and 8 DF,  p-value: 2.258e-05\n```\n:::\n:::\n\n        \n\nIf you want, you can use `update` here, which looks like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nboys.2a <- update(boys.1, . ~ . - age - weight - chest)\nsummary(boys.2a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = uptake ~ height, data = boys)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.069879 -0.033144  0.001407  0.009581  0.084012 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.843326   0.609198  -6.309 0.000231 ***\nheight       0.040718   0.004648   8.761 2.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05013 on 8 degrees of freedom\nMultiple R-squared:  0.9056,\tAdjusted R-squared:  0.8938 \nF-statistic: 76.75 on 1 and 8 DF,  p-value: 2.258e-05\n```\n:::\n:::\n\n \n\nThis doesn't go quite so smoothly here because there are three\nvariables being removed, and it's a bit of work to type them all. \n\n$\\blacksquare$\n\n(g) How has R-squared changed between your two\nregressions? Describe what you see in a few words.\n\n\nSolution\n\n\nR-squared has dropped by a bit, from 97\\% to 91\\%. (Make your own\ncall: pull out the two R-squared numbers, and say a word or two about\nhow they compare. I don't much mind what you say: \n\"R-squared has decreased (noticeably)\", \"R-squared has hardly changed\". But say\nsomething.)\n\n$\\blacksquare$ \n\n(h) Carry out a test comparing the fit of your\ntwo regression models. What do you conclude, and\ntherefore what recommendation would you make about the\nregression that would be preferred?\n\n\nSolution\n\n\nThe word \"test\" again implies something that  produces a P-value with a\nnull hypothesis that you might reject. In this case, the test that\ncompares two models differing by more than one $x$ uses\n`anova`, testing the null hypothesis that the two regressions\nare equally good, against the alternative that the bigger (first) one\nis better. Feed `anova` two fitted model objects, smaller first:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(boys.2, boys.1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Res.Df\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"RSS\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Df\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sum of Sq\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Pr(>F)\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"8\",\"2\":\"0.020101574\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"_rn_\":\"1\"},{\"1\":\"5\",\"2\":\"0.006922613\",\"3\":\"3\",\"4\":\"0.01317896\",\"5\":\"3.172926\",\"6\":\"0.1229759\",\"_rn_\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nThis P-value of 0.123 is not small, so we do not reject the null\nhypothesis. There is not a significant difference in fit between the\ntwo models. Therefore, we should go with the smaller model\n`boys.2` because it is simpler. \n\nThat drop in R-squared from 97\\% to 91\\% was, it turns out, *not*\nsignificant: the three extra variables\ncould have produced a change in R-squared like that, \n*even if  they were worthless*.^[Recall that adding $x$-variables to a regression will always make R-squared go up, even if they are just random noise.]\n\nIf you have learned about \"adjusted R-squared\", you might recall\nthat this is supposed to go down *only* if the variables you took\nout should not have been taken out. But adjusted R-squared goes down\nhere as well, from 94\\% to 89\\% (not quite as much, therefore). What\nhappens is that adjusted R-squared is rather more relaxed about\nkeeping variables than the `anova` $F$-test is; if we had used\nan $\\alpha$ of something like 0.10, the decision between the two\nmodels would have been a lot closer, and this is reflected in the\nadjusted R-squared values.\n\n$\\blacksquare$\n\n(i) Obtain a table of correlations between all\nthe variables in the data frame. Do this by feeding\nthe whole data frame into `cor`. \nWe found that a regression predicting oxygen uptake\nfrom just `height` was acceptably good. What\ndoes your table of correlations say about why that\nis? (Hint: look for all the correlations that are\n*large*.) \n\n\nSolution\n\n\nCorrelations first:\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(boys)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          uptake       age    height    weight     chest\nuptake 1.0000000 0.1361907 0.9516347 0.6576883 0.7182659\nage    0.1361907 1.0000000 0.3274830 0.2307403 0.1657523\nheight 0.9516347 0.3274830 1.0000000 0.7898252 0.7909452\nweight 0.6576883 0.2307403 0.7898252 1.0000000 0.8809605\nchest  0.7182659 0.1657523 0.7909452 0.8809605 1.0000000\n```\n:::\n:::\n\n \nThe correlations with `age` are all on the low side, but all\nthe other correlations are high, not just between `uptake` and the\nother variables, but between the explanatory variables as well.\n\nWhy is this helpful in understanding what's going on? Well, imagine a\nboy with large height (a tall one). The regression `boys.2`\nsays that this alone is enough to predict that such a boy's oxygen\nuptake is likely to be large, since the slope is positive. But the\ncorrelations tell you more: a boy with large height is also (somewhat)\nlikely to be older (have large age), heavier (large weight) and to have\nlarger `chest` cavity. So oxygen uptake does depend on those other\nvariables as well, but once you know `height` you can make a\ngood guess at their values; you don't need to know them.\n\nFurther remarks: `age` has a low correlation with\n`uptake`, so its non-significance earlier appears to be\n\"real\": it really does have nothing extra to say, because the other\nvariables have a stronger link with `uptake` than\n`age`. Height, however, seems to be the best way of relating\noxygen uptake to any of the other variables. I think the suppositions\nfrom earlier about relating oxygen uptake to \"bigness\"^[This  is not, I don't think, a real word, but I mean size emphasizing  how big a boy is generally, rather than how small.] in some sense\nare actually sound, but age and weight and `chest` capture\n\"bigness\" worse than height does. Later, when you learn about\nPrincipal Components, you will see that the first principal component,\nthe one that best captures how the variables vary together, is often\n\"bigness\" in some sense.\n\nAnother way to think about these things is via pairwise\nscatterplots. The nicest way to produce these is via `ggpairs`\nfrom package `GGally`:\n\n::: {.cell}\n\n```{.r .cell-code}\nboys %>% ggpairs(progress = FALSE)\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/youngboys-7-1.png){width=672}\n:::\n:::\n\n \n\nA final remark: with five variables, we really ought to have more than\nten observations (something like 50 would be better). But with more\nobservations and the same correlation structure, the same issues would\ncome up again, so the question would not be materially changed.\n\n$\\blacksquare$\n\n \n\n\n\n## Facebook friends and grey matter\n\n\n\n Is there a relationship between the number\nof Facebook friends a person has, and the density of grey matter in\nthe areas of the brain associated with social perception and\nassociative memory? To find out, a 2012 study measured both of these\nvariables for a sample of 40 students at City University in London\n(England). The data are at\n[link](http://ritsokiguess.site/datafiles/facebook.txt). The grey\nmatter density is on a $z$-score standardized scale. The values are\nseparated by *tabs*.\n\nThe aim of this question is to produce a Quarto report that\ncontains your answers to the questions below. \n\nYou should aim to make your report flow smoothly, so that it would be\npleasant for a grader to read, and can stand on its own as an analysis\n(rather than just being the answer to a question that I set you).\nSome suggestions: give your report a title and arrange it into\nsections with an Introduction; add a small amount of additional text\nhere and there explaining what you are doing and why. I don't expect\nyou to spend a large amount of time on this, but I *do* hope\nyou will make some effort. (My report came out to 4 Word pages; HTML of course doesn't have page numbers, but that's a good kind of length to aim for.)\n\n\n(a) Read in the data and make a scatterplot for predicting the\nnumber of Facebook friends from the grey matter density. On your\nscatterplot, add a smooth trend.\n\n\nSolution\n\n\nBegin your document with a code chunk containing\n`library(tidyverse)`. The data values are\nseparated by tabs, which you will need to take into account:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/facebook.txt\"\nfb <- read_tsv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 40 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (2): GMdensity, FBfriends\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nfb\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"GMdensity\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"FBfriends\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-1.8\",\"2\":\"23\"},{\"1\":\"0.1\",\"2\":\"35\"},{\"1\":\"-1.2\",\"2\":\"80\"},{\"1\":\"-0.4\",\"2\":\"110\"},{\"1\":\"-0.9\",\"2\":\"120\"},{\"1\":\"-2.1\",\"2\":\"140\"},{\"1\":\"-1.5\",\"2\":\"168\"},{\"1\":\"0.5\",\"2\":\"132\"},{\"1\":\"0.6\",\"2\":\"154\"},{\"1\":\"-0.5\",\"2\":\"241\"},{\"1\":\"-1.1\",\"2\":\"255\"},{\"1\":\"-1.0\",\"2\":\"274\"},{\"1\":\"0.1\",\"2\":\"238\"},{\"1\":\"0.1\",\"2\":\"260\"},{\"1\":\"0.0\",\"2\":\"275\"},{\"1\":\"0.3\",\"2\":\"283\"},{\"1\":\"-0.8\",\"2\":\"320\"},{\"1\":\"-0.9\",\"2\":\"341\"},{\"1\":\"-1.0\",\"2\":\"358\"},{\"1\":\"1.8\",\"2\":\"384\"},{\"1\":\"1.2\",\"2\":\"365\"},{\"1\":\"0.9\",\"2\":\"377\"},{\"1\":\"0.1\",\"2\":\"398\"},{\"1\":\"0.2\",\"2\":\"372\"},{\"1\":\"1.8\",\"2\":\"515\"},{\"1\":\"1.3\",\"2\":\"504\"},{\"1\":\"1.3\",\"2\":\"550\"},{\"1\":\"0.9\",\"2\":\"506\"},{\"1\":\"0.4\",\"2\":\"447\"},{\"1\":\"0.7\",\"2\":\"728\"},{\"1\":\"0.0\",\"2\":\"624\"},{\"1\":\"-1.2\",\"2\":\"428\"},{\"1\":\"1.1\",\"2\":\"617\"},{\"1\":\"-0.9\",\"2\":\"472\"},{\"1\":\"-0.8\",\"2\":\"526\"},{\"1\":\"-0.6\",\"2\":\"541\"},{\"1\":\"-0.5\",\"2\":\"575\"},{\"1\":\"-0.3\",\"2\":\"531\"},{\"1\":\"-0.2\",\"2\":\"430\"},{\"1\":\"0.3\",\"2\":\"639\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nggplot(fb, aes(x = GMdensity, y = FBfriends)) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/fbfriends-1-1.png){width=672}\n:::\n:::\n\n       \n\n$\\blacksquare$\n\n\n(b) Describe what you see on your scatterplot: is there a\ntrend, and if so, what kind of trend is it? (Don't get too taken in\nby the exact shape of your smooth trend.) Think \"form, direction,  strength\". \n\n\nSolution\n\n\nI'd say there seems to be a weak, upward, apparently linear\ntrend. The points are not especially close to the trend, so I\ndon't think there's any justification for calling this other\nthan \"weak\". (If you think the trend is, let's say,\n\"moderate\", you ought to say what makes you think that: for\nexample, that the people with a lot of Facebook friends also\ntend to have a higher grey matter density. I can live with a\nreasonably-justified \"moderate\".)\nThe reason I said not to get taken in by the shape of the smooth\ntrend is that this has a \"wiggle\" in it: it goes down again\nbriefly in the middle. But this is likely a quirk of the data,\nand the trend, if there is any, seems to be an upward one.\n\n\n$\\blacksquare$\n\n(c) Fit a regression predicting the number of Facebook friends\nfrom the grey matter density, and display the output.\n\n\nSolution\n\n\nThat looks like this. You can call the \"fitted model object\"\nwhatever you like, but you'll need to get the capitalization of\nthe  variable names correct:\n\n::: {.cell}\n\n```{.r .cell-code}\nfb.1 <- lm(FBfriends ~ GMdensity, data = fb)\nsummary(fb.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = FBfriends ~ GMdensity, data = fb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-339.89 -110.01   -5.12   99.80  303.64 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   366.64      26.35  13.916  < 2e-16 ***\nGMdensity      82.45      27.58   2.989  0.00488 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 165.7 on 38 degrees of freedom\nMultiple R-squared:  0.1904,\tAdjusted R-squared:  0.1691 \nF-statistic: 8.936 on 1 and 38 DF,  p-value: 0.004882\n```\n:::\n:::\n\n       \n\nI observe, though I didn't ask you to, that the R-squared is pretty\nawful, going with a correlation of \n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(0.1904)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4363485\n```\n:::\n:::\n\n \n\nwhich *would* look\nlike as weak of a trend as we saw.^[Correlations have to go up beyond 0.50 before they start looking at all interesting.]\n\n\n$\\blacksquare$\n\n(d) Is the slope of your regression line significantly\ndifferent from zero? What does that mean, in the context of the\ndata?\n\n\nSolution\n\n\nThe P-value of the slope is 0.005, which is less than\n0.05. Therefore the slope *is* significantly different from\nzero. That means that the number of Facebook friends really does\ndepend on the grey matter density, for the whole population of\ninterest and not just the 40 students observed here (that were a\nsample from that population). I don't mind so much what you\nthink the population is, but it needs to be clear that the\nrelationship applies to a population. \nAnother way to approach this is to say that you would expect\nthis relationship to show up again in another similar\nexperiment. That also works, because it gets at the idea of\nreproducibility. \n\n$\\blacksquare$\n\n\n(e) Are you surprised by the results of\nparts (b) and (d)? Explain briefly.\n\n\nSolution\n\n\nI *am* surprised, because I thought the trend on the\nscatterplot was so weak that there would not be a significant\nslope. I guess there was enough of an upward trend to be\nsignificant, and with $n=40$ observations we were able to get a\nsignificant slope out of that scatterplot. With this many\nobservations, even a weak correlation can be significantly\nnonzero. \nYou can be surprised or not, but you need to have some kind of\nconsideration of the strength of the trend on the scatterplot as\nagainst the significance of the slope. For example, if you\ndecided that the trend was \"moderate\" in strength, you would\nbe justified in being less surprised than I was. \nHere, there is the usual issue that we have proved that the\nslope is not zero (that the relationship is not flat), but we\nmay not have a very clear idea of what the slope actually\n*is*. There are a couple of ways to get a confidence\ninterval. The obvious one is to use R as a calculator and go up\nand down twice its standard error (to get a rough idea):\n\n::: {.cell}\n\n```{.r .cell-code}\n82.45 + 2 * 27.58 * c(-1, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  27.29 137.61\n```\n:::\n:::\n\n       \nThe `c()` thing is to get both confidence limits at once. The\nsmoother way is this:\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(fb.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                2.5 %   97.5 %\n(Intercept) 313.30872 419.9810\nGMdensity    26.61391 138.2836\n```\n:::\n:::\n\n \n\nFeed `confint` a \"fitted model object\" and it'll give you\nconfidence intervals (by default 95\\%) for all the parameters in it. \n\nThe confidence interval for the slope goes from about 27 to about\n138. That is to say, a one-unit increase in grey matter density goes\nwith an increase in Facebook friends of this much. This is not\nespecially insightful: it's bigger than zero (the test was\nsignificant), but other than that, it could be almost\nanything. *This* is where the weakness of the trend comes back to\nbite us. With this much scatter in our data, we need a *much*\nlarger sample size to estimate accurately how big an effect grey\nmatter density has.\n\n\n$\\blacksquare$\n\n(f) Obtain a scatterplot with the regression line on it.\n\n\nSolution\n\n\nJust a modification\nof (a):\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(fb, aes(x = GMdensity, y = FBfriends)) + geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/fbfriends-6-1.png){width=672}\n:::\n:::\n\n       \n\n\n$\\blacksquare$\n\n(g) Obtain a plot of the residuals from the regression against\nthe fitted values, and comment briefly on it.\n\n\nSolution\n\n\nThis is, to my mind, the easiest way:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(fb.1, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/fbfriends-7-1.png){width=672}\n:::\n:::\n\n       \n\nThere is some \"magic\" here, since the fitted model object is not\nactually a data frame, but it works this way.\nThat looks to me like a completely random scatter of\npoints. Thus, I am completely happy with the straight-line regression\nthat we fitted, and I see no need to improve it.\n\n(You should make two points here: one, describe what you see, and two,\nwhat it implies about whether or not your regression is satisfactory.)\n\nCompare that residual plot with this one:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(fb.1, aes(x = .fitted, y = .resid)) +\n  geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/fbfriends-8-1.png){width=672}\n:::\n:::\n\n       \nNow, why did I try adding a smooth trend, and why is it not\nnecessarily a good idea? The idea of a residual plot is that there\nshould be no trend, and so the smooth trend curve ought to go straight\nacross. The problem is that it will tend to wiggle, just by chance, as\nhere: it looks as if it goes up and down before flattening out. But if\nyou look at the points, *they* are all over the place, not close\nto the smooth trend at all. So the smooth trend is rather\ndeceiving. Or, to put it another way, to indicate a real problem, the\nsmooth trend would have to be a *lot* farther from flat than this\none is. I'd call this one basically flat.\n\n$\\blacksquare$\n\n\n\n\n\n##  Endogenous nitrogen excretion in carp\n\n\n A paper in Fisheries Science reported on variables that\naffect \"endogenous nitrogen excretion\" or ENE in carp raised in\nJapan. A number of carp were divided into groups based on body weight,\nand each group was placed in a different tank. The mean body weight of\nthe carp placed in each tank was recorded. The carp were then fed a\nprotein-free diet three times daily for a period of 20 days. At the\nend of the experiment, the amount of ENE in each tank was measured, in\nmilligrams of total fish body weight per day. (Thus it should not\nmatter that some of the tanks had more fish than others, because the\nscaling is done properly.)\n\nFor this question, write a report in a Quarto document that answers the\nquestions below and contains some narrative that describes your\nanalysis. Create an HTML document from your Quarto document, as usual for an assignment.\n\n\n\n(a) Read the data in from\n[link](http://ritsokiguess.site/datafiles/carp.txt). There are 10\ntanks. \n\n\nSolution\n\n\nJust this. Listing the data is up to you, but doing so and\ncommenting that the values appear to be correct will improve your report.\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/carp.txt\"\ncarp <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 10 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (3): tank, bodyweight, ENE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ncarp\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"tank\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"bodyweight\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ENE\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"11.7\",\"3\":\"15.3\"},{\"1\":\"2\",\"2\":\"25.3\",\"3\":\"9.3\"},{\"1\":\"3\",\"2\":\"90.2\",\"3\":\"6.5\"},{\"1\":\"4\",\"2\":\"213.0\",\"3\":\"6.0\"},{\"1\":\"5\",\"2\":\"10.2\",\"3\":\"15.7\"},{\"1\":\"6\",\"2\":\"17.6\",\"3\":\"10.0\"},{\"1\":\"7\",\"2\":\"32.6\",\"3\":\"8.6\"},{\"1\":\"8\",\"2\":\"81.3\",\"3\":\"6.4\"},{\"1\":\"9\",\"2\":\"141.5\",\"3\":\"5.6\"},{\"1\":\"10\",\"2\":\"285.7\",\"3\":\"6.0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n$\\blacksquare$\n\n\n(b) Create a scatterplot of ENE (response) against bodyweight\n(explanatory). Add a smooth trend to your plot.\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(carp, aes(x = bodyweight, y = ENE)) + geom_point() +\n  geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/marttis-1.png){width=672}\n:::\n:::\n\n \n\nThis part is just about getting the plot. Comments are coming in a\nminute. Note that `ENE` is capital letters, so that\n`ene` will not work.\n\n$\\blacksquare$\n\n(c) Is there an upward or downward trend (or neither)? Is the\nrelationship a line or a curve? Explain briefly.\n\n\nSolution\n\n\nThe trend is downward: as bodyweight increases, ENE\ndecreases. However, the decrease is rapid at first and then levels\noff, so the relationship is nonlinear. I want some kind of\nsupport for an assertion of non-linearity: anything that says that\nthe slope or rate of decrease is not constant is good.\n\n$\\blacksquare$\n\n(d) Fit a straight line to the data, and obtain the R-squared\nfor the regression.\n\n\nSolution\n\n\n`lm`. The first stage is to fit the straight line, saving\nthe result in a  variable, and the second stage is to look at the\n\"fitted model object\", here via `summary`:\n\n::: {.cell}\n\n```{.r .cell-code}\ncarp.1 <- lm(ENE ~ bodyweight, data = carp)\nsummary(carp.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ENE ~ bodyweight, data = carp)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.800 -1.957 -1.173  1.847  4.572 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.40393    1.31464   8.675 2.43e-05 ***\nbodyweight  -0.02710    0.01027  -2.640   0.0297 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.928 on 8 degrees of freedom\nMultiple R-squared:  0.4656,\tAdjusted R-squared:  0.3988 \nF-statistic: 6.971 on 1 and 8 DF,  p-value: 0.0297\n```\n:::\n:::\n\n \nFinally, you need to give me a (suitably rounded) value for\nR-squared: 46.6\\% or 47\\% or the equivalents as a decimal. I just\nneed the value at this point.\nThis kind of R-squared is actually pretty good for natural data, but\nthe issue is whether we can improve it by fitting a non-linear\nmodel.^[The suspicion being that we can, since the    scatterplot suggested serious non-linearity.]\n\n$\\blacksquare$\n\n(e) Obtain a residual plot (residuals against fitted values)\nfor this regression. Do you see any problems? If so, what does that\ntell you about the relationship in the data?\n\n\nSolution\n\n\n\nThis is the easiest way: feed the output of the regression\nstraight into `ggplot`:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(carp.1, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/goeslingerin-1.png){width=672}\n:::\n:::\n\n \n\n$\\blacksquare$\n\n(f) Fit a parabola to the data (that is, including an\n$x$-squared term). Compare the R-squared values for the models in\nthis part and part (d). Does that suggest that the parabola\nmodel is an improvement here over the linear model?\n\n\nSolution\n\n\nAdd bodyweight-squared to\nthe regression. Don't forget the `I()`:\n\n::: {.cell}\n\n```{.r .cell-code}\ncarp.2 <- lm(ENE ~ bodyweight + I(bodyweight^2), data = carp)\nsummary(carp.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ENE ~ bodyweight + I(bodyweight^2), data = carp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0834 -1.7388 -0.5464  1.3841  2.9976 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     13.7127373  1.3062494  10.498 1.55e-05 ***\nbodyweight      -0.1018390  0.0288109  -3.535  0.00954 ** \nI(bodyweight^2)  0.0002735  0.0001016   2.692  0.03101 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.194 on 7 degrees of freedom\nMultiple R-squared:  0.7374,\tAdjusted R-squared:  0.6624 \nF-statistic: 9.829 on 2 and 7 DF,  p-value: 0.009277\n```\n:::\n:::\n\n \n\nR-squared has gone up from 47\\% to 74\\%, a substantial\nimprovement. This suggests to me that the parabola model is a\nsubstantial improvement.^[Again, not a surprise, given our  initial scatterplot.]\n\nI try to avoid using the word \"significant\" in this context, since\nwe haven't actually done a test of significance.\n\nThe reason for the `I()` is that the up-arrow has a special\nmeaning in `lm`, relating to interactions between factors (as\nin ANOVA), that we don't want here. Putting `I()` around it\nmeans \"use as is\", that is, raise bodyweight to power 2, rather than\nusing the special meaning of the up-arrow in `lm`.\n\nBecause it's the up-arrow that is the problem, this applies whenever\nyou're raising an explanatory variable to a power (or taking a\nreciprocal or a square root, say).\n\n$\\blacksquare$\n\n(g) Is the test for the slope coefficient for the squared term\nsignificant? What does this mean?\n\n\nSolution\n\n\nLook along the `bodyweight`-squared line to get a P-value\nof 0.031. This is less than the default 0.05, so it *is*\nsignificant.\nThis means, in short, that the quadratic model is a significant\n*improvement* over the linear one.^[Now we can use that word   *significant*.] \nSaid longer: the null hypothesis being tested is that the slope\ncoefficient of the squared term is zero (that is, that the squared\nterm has nothing to add over the linear model). This is rejected,\nso the squared term has *something* to add in terms of\nquality of prediction.\n\n$\\blacksquare$\n\n(h) Make the scatterplot of part (b), but add\nthe fitted curve. Describe any way in which the curve fails to fit well.\n\n\nSolution\n\n\nThis is a bit slippery, because the points to plot and the\nfitted curve are from different data frames. What you do in this\ncase is to put a `data=` in one of the `geom`s,\nwhich says \"don't use the data frame that was in the  `ggplot`, but use this one instead\". \nI would think about\nstarting with the regression object `carp.2` as my base\ndata frame, since we want (or I want) to do two things with\nthat: plot the fitted values and join them with lines. Then I\nwant to add the original data, just the points:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(carp.2, aes(x = carp$bodyweight, y = .fitted), colour = \"blue\") +\n  geom_line(colour = \"blue\") +\n  geom_point(data = carp, aes(x = bodyweight, y = ENE))\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/bletz-1.png){width=672}\n:::\n:::\n\n       \n\nThis works, but is not very aesthetic, because the bodyweight that is\nplotted against the fitted values is in the wrong data frame, and so\nwe have to use the dollar-sign thing to get it from the right one.\n\nA better way around this is \"augment\" the data with output from the regression object. \nThis is done using `augment` from \npackage `broom`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\ncarp.2a <- augment(carp.2, carp)\ncarp.2a\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"tank\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"bodyweight\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ENE\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".fitted\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".resid\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".hat\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".sigma\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".cooksd\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".std.resid\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"11.7\",\"3\":\"15.3\",\"4\":\"12.558657\",\"5\":\"2.7413428\",\"6\":\"0.2392022\",\"7\":\"1.992810\",\"8\":\"0.21499558\",\"9\":\"1.4322784\"},{\"1\":\"2\",\"2\":\"25.3\",\"3\":\"9.3\",\"4\":\"11.311261\",\"5\":\"-2.0112607\",\"6\":\"0.1629840\",\"7\":\"2.193651\",\"8\":\"0.06514640\",\"9\":\"-1.0018443\"},{\"1\":\"3\",\"2\":\"90.2\",\"3\":\"6.5\",\"4\":\"6.751885\",\"5\":\"-0.2518851\",\"6\":\"0.2396677\",\"7\":\"2.367208\",\"8\":\"0.00182089\",\"9\":\"-0.1316435\"},{\"1\":\"4\",\"2\":\"213.0\",\"3\":\"6.0\",\"4\":\"4.428445\",\"5\":\"1.5715555\",\"6\":\"0.3246419\",\"7\":\"2.237874\",\"8\":\"0.12169479\",\"9\":\"0.8714880\"},{\"1\":\"5\",\"2\":\"10.2\",\"3\":\"15.7\",\"4\":\"12.702432\",\"5\":\"2.9975680\",\"6\":\"0.2511559\",\"7\":\"1.902036\",\"8\":\"0.27859605\",\"9\":\"1.5785999\"},{\"1\":\"6\",\"2\":\"17.6\",\"3\":\"10.0\",\"4\":\"12.005083\",\"5\":\"-2.0050832\",\"6\":\"0.1993691\",\"7\":\"2.186473\",\"8\":\"0.08656338\",\"9\":\"-1.0212098\"},{\"1\":\"7\",\"2\":\"32.6\",\"3\":\"8.6\",\"4\":\"10.683427\",\"5\":\"-2.0834268\",\"6\":\"0.1425478\",\"7\":\"2.184918\",\"8\":\"0.05826041\",\"9\":\"-1.0253497\"},{\"1\":\"8\",\"2\":\"81.3\",\"3\":\"6.4\",\"4\":\"7.240829\",\"5\":\"-0.8408294\",\"6\":\"0.2112076\",\"7\":\"2.338418\",\"8\":\"0.01661407\",\"9\":\"-0.4314448\"},{\"1\":\"9\",\"2\":\"141.5\",\"3\":\"5.6\",\"4\":\"4.778159\",\"5\":\"0.8218410\",\"6\":\"0.3546865\",\"7\":\"2.333053\",\"8\":\"0.03982493\",\"9\":\"0.4662310\"},{\"1\":\"10\",\"2\":\"285.7\",\"3\":\"6.0\",\"4\":\"6.939822\",\"5\":\"-0.9398221\",\"6\":\"0.8745372\",\"7\":\"2.108136\",\"8\":\"3.39715895\",\"9\":\"-1.2091687\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nso now you see what `carp.2a` has in it, and then:\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- ggplot(carp.2a, aes(x = bodyweight, y = .fitted)) +\n  geom_line(colour = \"blue\") +\n  geom_point(aes(y = ENE))\n```\n:::\n\n \n\nThis is easier coding: there are only two non-standard things. The\nfirst is that the fitted-value lines should be a distinct colour like\nblue so that you can tell them from the data points. The second thing\nis that for the second `geom_point`, the one that plots the data,\nthe $x$ coordinate `bodyweight` is correct so that we don't\nhave to change that; we only have to change the $y$-coordinate, which\nis `ENE`. The plot is this:\n\n::: {.cell}\n\n```{.r .cell-code}\ng\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/carp-4-1.png){width=672}\n:::\n:::\n\n \n\nConcerning interpretation, you have a number of possibilities\nhere. The simplest is that the points in the middle are above the\ncurve, and the points at the ends are below. (That is, negative\nresiduals at the ends, and positive ones in the middle, which gives\nyou a hint for the next part.) Another is that the parabola curve\nfails to capture the *shape* of the relationship; for example, I\nsee nothing much in the data suggesting that the relationship should go\nback up, and even given that, the fitted curve doesn't go especially\nnear any of the points.\n\nI was thinking that the data should be fit better by something like\nthe left half of an upward-opening parabola, but I guess the curvature\non the left half of the plot suggests that it needs most of the left\nhalf of the parabola just to cover the left half of the plot.\n\nThe moral of the story, as we see in the next part, is that the\nparabola is the wrong curve for the job.\n\n$\\blacksquare$\n\n(i) Obtain a residual plot for the parabola model. Do you see\nany problems with it? (If you do, I'm not asking you to do anything\nabout them in this question, but I will.)\n\n\n$\\blacksquare$\n\n\nThe same idea as before for the other residual plot.  Use the\nfitted model object `carp.2` as your data frame for the\n`ggplot`:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(carp.2, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/ohem-1.png){width=672}\n:::\n:::\n\n \n\nI think this is *still* a curve (or, it goes down and then\nsharply up at the end). Either way, there is still a pattern. \n\nThat was all I needed, but as to what this means: our parabola was a\ncurve all right, but it appears not to be the right *kind* of\ncurve. I think the original data looks more like a hyperbola (a curve\nlike $y=1/x$) than a parabola, in that it seems to decrease fast and\nthen gradually to a limit, and *that* suggests, as in the class\nexample, that we should try an asymptote model. Note how I specify it,\nwith the `I()` thing again, since `/` has a special meaning \nto `lm` in the same way that \n`^` does:\n\n::: {.cell}\n\n```{.r .cell-code}\ncarp.3 <- lm(ENE ~ I(1 / bodyweight), data = carp)\nsummary(carp.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ENE ~ I(1/bodyweight), data = carp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.29801 -0.12830  0.04029  0.26702  0.91707 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       5.1804     0.2823   18.35 8.01e-08 ***\nI(1/bodyweight) 107.6690     5.8860   18.29 8.21e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6121 on 8 degrees of freedom\nMultiple R-squared:  0.9766,\tAdjusted R-squared:  0.9737 \nF-statistic: 334.6 on 1 and 8 DF,  p-value: 8.205e-08\n```\n:::\n:::\n\n \n\nThat fits *extraordinarily* well, with an R-squared up near\n98\\%. The intercept is the asymptote, which suggests a (lower) limit\nof about 5.2 for `ENE` (in the limit for large bodyweight). We\nwould have to ask the fisheries scientist whether this kind of thing\nis a reasonable biological mechanism. It says that a carp always has\nsome ENE, no matter how big it gets, but a smaller carp will have a\nlot more.\n\nDoes the fitted value plot look reasonable now? This is `augment` again since the fitted values and observed data come from different data frames:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\naugment(carp.3, carp) %>%\n  ggplot(aes(x = bodyweight, y = .fitted)) +\n  geom_line(colour = \"blue\") +\n  geom_point(aes(y = ENE))\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/augment2-1.png){width=672}\n:::\n:::\n\n \n\nI'd say that does a really nice job of fitting the data. But it would\nbe nice to have a few more tanks with large-bodyweight fish, to\nconvince us that we have the shape of the trend right.\n\nAnd, as ever, the residual plot. That's a lot easier than the plot we\njust did:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(carp.3, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/sailer-1.png){width=672}\n:::\n:::\n\n \n\nAll in all, that looks pretty good (and certainly a vast improvement\nover the ones you got before).\n\nWhen you write up your report, you can make it flow better by writing\nit in a way that suggests that each thing was the obvious thing to do\nnext: that is, that *you* would have thought to do it next,\nrather than me telling you what to do.\n\nMy report (as a Quarto document) is at\n[link](http://ritsokiguess.site/datafiles/carp.qmd). Download it,\nrender it, play with it.\n\n$\\blacksquare$\n\n\n\n\n\n\n##  Salaries of social workers\n\n\n Another salary-prediction question: does the number of years\nof work experience that a social worker has help to predict their \nsalary? Data for 50 social workers are in\n[link](http://ritsokiguess.site/datafiles/socwork.txt). \n\n\n\n(a) Read the data into R. Check that you have 50 observations on\ntwo variables. Also do something to check that the years of\nexperience and annual salary figures look reasonable overall.\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/socwork.txt\"\nsoc <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): experience, salary\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nsoc\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"experience\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"salary\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"7\",\"2\":\"26075\"},{\"1\":\"28\",\"2\":\"79370\"},{\"1\":\"23\",\"2\":\"65726\"},{\"1\":\"18\",\"2\":\"41983\"},{\"1\":\"19\",\"2\":\"62308\"},{\"1\":\"15\",\"2\":\"41154\"},{\"1\":\"24\",\"2\":\"53610\"},{\"1\":\"13\",\"2\":\"33697\"},{\"1\":\"2\",\"2\":\"22444\"},{\"1\":\"8\",\"2\":\"32562\"},{\"1\":\"20\",\"2\":\"43076\"},{\"1\":\"21\",\"2\":\"56000\"},{\"1\":\"18\",\"2\":\"58667\"},{\"1\":\"7\",\"2\":\"22210\"},{\"1\":\"2\",\"2\":\"20521\"},{\"1\":\"18\",\"2\":\"49727\"},{\"1\":\"11\",\"2\":\"33233\"},{\"1\":\"21\",\"2\":\"43628\"},{\"1\":\"4\",\"2\":\"16105\"},{\"1\":\"24\",\"2\":\"65644\"},{\"1\":\"20\",\"2\":\"63022\"},{\"1\":\"20\",\"2\":\"47780\"},{\"1\":\"15\",\"2\":\"38853\"},{\"1\":\"25\",\"2\":\"66537\"},{\"1\":\"25\",\"2\":\"67477\"},{\"1\":\"28\",\"2\":\"64785\"},{\"1\":\"26\",\"2\":\"61581\"},{\"1\":\"27\",\"2\":\"70678\"},{\"1\":\"20\",\"2\":\"51301\"},{\"1\":\"18\",\"2\":\"39346\"},{\"1\":\"1\",\"2\":\"24833\"},{\"1\":\"26\",\"2\":\"65929\"},{\"1\":\"20\",\"2\":\"41721\"},{\"1\":\"26\",\"2\":\"82641\"},{\"1\":\"28\",\"2\":\"99139\"},{\"1\":\"23\",\"2\":\"52624\"},{\"1\":\"17\",\"2\":\"50594\"},{\"1\":\"25\",\"2\":\"53272\"},{\"1\":\"26\",\"2\":\"65343\"},{\"1\":\"19\",\"2\":\"46216\"},{\"1\":\"16\",\"2\":\"54288\"},{\"1\":\"3\",\"2\":\"20844\"},{\"1\":\"12\",\"2\":\"32586\"},{\"1\":\"23\",\"2\":\"71235\"},{\"1\":\"20\",\"2\":\"36530\"},{\"1\":\"19\",\"2\":\"52745\"},{\"1\":\"27\",\"2\":\"67282\"},{\"1\":\"25\",\"2\":\"80931\"},{\"1\":\"12\",\"2\":\"32303\"},{\"1\":\"11\",\"2\":\"38371\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nThat checks that we have the right *number* of observations; to\ncheck that we have sensible *values*, something like\n`summary` is called for:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(soc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   experience        salary     \n Min.   : 1.00   Min.   :16105  \n 1st Qu.:13.50   1st Qu.:36990  \n Median :20.00   Median :50948  \n Mean   :18.12   Mean   :50171  \n 3rd Qu.:24.75   3rd Qu.:65204  \n Max.   :28.00   Max.   :99139  \n```\n:::\n:::\n\n \n\nA person working in any field cannot have a negative number of years\nof experience, and cannot have more than about 40 years of experience\n(or else they would have retired). Our experience numbers fit\nthat. Salaries had better be five or six figures, and salaries for\nsocial workers are not generally all that high, so these figures look\nreasonable. \n\nA rather more `tidyverse` way is this:\n\n::: {.cell}\n\n```{.r .cell-code}\nsoc %>% \n  summarize(across(everything(), \n                   list(min = \\(x) min(x),  max = \\(x) max(x))))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"experience_min\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"experience_max\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"salary_min\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"salary_max\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"28\",\"3\":\"16105\",\"4\":\"99139\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nThis gets the minimum and maximum of all the variables. I would have\nliked them arranged in a nice rectangle (`min` and `max`\nas rows, the variables as columns), but that's not how this came out. We fix that shortly.\n\nThe code so far uses `across`. This means to do something across multiple columns. In this case, we want to do the calculation on *all* the columns, so we use the select-helper `everything`. You can use any of the other select-helpers like `starts_with`, or you could do something like `where(is.numeric)` to do your summaries only on the quantitative columns (which would also work here). The thing after the `everything()` means \"for each column selected, work out the  `min` and `max` of it\"; `x` is our name for \"the variable we are looking at at the moment\".\n\nWhat, you want a nice rectangle? This is a pivot-longer, but a fancy version because the column names encode two kinds of things, a variable and a statistic. I took the view that I wanted variables in columns (as usual), and the different summary statistics in rows. This means that the first part of the column names we created above (eg. the `salary` part of `salary_min`) should stay in columns, and the rest of it should be pivoted longer. That means using the special name `.value` for the things that should stay as columns:\n\n::: {.cell}\n\n```{.r .cell-code}\nsoc %>% \n  summarize(across(everything(), \n                   list(min = \\(x) min(x),  max = \\(x) max(x)))) %>% \n  pivot_longer(everything(), \n               names_to = c(\".value\", \"statistic\"), \n               names_sep = \"_\"\n               )\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"statistic\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"experience\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"salary\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"min\",\"2\":\"1\",\"3\":\"16105\"},{\"1\":\"max\",\"2\":\"28\",\"3\":\"99139\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nNote that we're using two simpler tools here, rather than one complicated one: first we get the summary statistics, and once we have that, we can do some tidying to get it arranged the way we want.\n\nYour first guess is likely to be to make it *too* long:\n\n::: {.cell}\n\n```{.r .cell-code}\nsoc %>% \n  summarize(across(everything(), \n                   list(min = \\(x) min(x),  max = \\(x) max(x)))) %>% \n  pivot_longer(everything(), \n               names_to = c(\"variable\", \"statistic\"), \n               names_sep = \"_\", \n               values_to = \"value\"\n               )\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"variable\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"statistic\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"value\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"experience\",\"2\":\"min\",\"3\":\"1\"},{\"1\":\"experience\",\"2\":\"max\",\"3\":\"28\"},{\"1\":\"salary\",\"2\":\"min\",\"3\":\"16105\"},{\"1\":\"salary\",\"2\":\"max\",\"3\":\"99139\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nand then you'll have to make it wider, or recall that you can do the thing with `.value`.  We are working \"columnwise\", doing something for each column, no matter how many there are. My go-to for this stuff is [here](https://dplyr.tidyverse.org/articles/colwise.html).\n\nAnother way to work is with the five-number summary. This gives a more nuanced picture of the data values we have.^[This might be overkill at this point, since we really only care about whether our data values are reasonable, and often just looking at the highest and lowest values will tell us that.] \n\nThe base-R five-number summary looks like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nqq <- quantile(soc$experience)\nqq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   0%   25%   50%   75%  100% \n 1.00 13.50 20.00 24.75 28.00 \n```\n:::\n:::\n\nThis is what's known as a \"named vector\". The numbers on the bottom are the summaries themselves, and the names above say which percentile you are looking at. Unfortunately, the `tidyverse` doesn't like names, so modelling after the above doesn't quite work:\n\n::: {.cell}\n\n```{.r .cell-code}\nsoc %>% \n  summarize(across(everything(), list(q = \\(x) quantile(x))))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n```\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"experience_q\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"salary_q\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1.00\",\"2\":\"16105.00\"},{\"1\":\"13.50\",\"2\":\"36990.25\"},{\"1\":\"20.00\",\"2\":\"50947.50\"},{\"1\":\"24.75\",\"2\":\"65203.50\"},{\"1\":\"28.00\",\"2\":\"99139.00\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nYou can guess which percentile is which (they have to be in order), but this is not completely satisfactory. It also gives a warning because the summary is five numbers long, rather than only one (like the mean, for example), and this is not the preferred way to handle this.\n\nThe warning mentions `reframe`, which is new (as in, less than a year old as I write this). Let's see how it goes here:\n\n::: {.cell}\n\n```{.r .cell-code}\nsoc %>% \n  reframe(q_exp = quantile(experience), q_sal = quantile(salary))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"q_exp\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"q_sal\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1.00\",\"2\":\"16105.00\"},{\"1\":\"13.50\",\"2\":\"36990.25\"},{\"1\":\"20.00\",\"2\":\"50947.50\"},{\"1\":\"24.75\",\"2\":\"65203.50\"},{\"1\":\"28.00\",\"2\":\"99139.00\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThe idea is that `reframe` is like `summarize`, but it is designed for when your summary function returns *more than one* number, not just one number per group like `mean` or `median` do.\n\nThis is not quite the best (I don't see the percentiles and I have to repeat myself), but at least I no longer get a warning. Here's how you do it with `across`:\n\n::: {.cell}\n\n```{.r .cell-code}\nsoc %>% \n  reframe(across(everything(), \\(x) enframe(quantile(x)), .unpack = TRUE))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"experience_name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"experience_value\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"salary_name\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"salary_value\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0%\",\"2\":\"1.00\",\"3\":\"0%\",\"4\":\"16105.00\"},{\"1\":\"25%\",\"2\":\"13.50\",\"3\":\"25%\",\"4\":\"36990.25\"},{\"1\":\"50%\",\"2\":\"20.00\",\"3\":\"50%\",\"4\":\"50947.50\"},{\"1\":\"75%\",\"2\":\"24.75\",\"3\":\"75%\",\"4\":\"65203.50\"},{\"1\":\"100%\",\"2\":\"28.00\",\"3\":\"100%\",\"4\":\"99139.00\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThe `enframe` turns a \"named vector\" (that is, a thing like my `qq` above) into a dataframe with two columns, one called `name` with the names (percentiles), and one called `value` with the values. By using `across`, you get those two columns for *each* variable, and you can see which of the five numbers is which percentile in each case.\n\n\n$\\blacksquare$\n\n(b) Make a scatterplot showing how salary depends on\nexperience. Does the nature of the trend make sense?\n\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(soc, aes(x = experience, y = salary)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/socwork-9-1.png){width=672}\n:::\n:::\n\n \n\nAs experience goes up, salary also goes up, as you would expect. Also,\nthe trend seems more or less straight.\n\n$\\blacksquare$\n\n(c) Fit a regression predicting salary from experience, and\ndisplay the results. Is the slope positive or negative? Does that\nmake sense?\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsoc.1 <- lm(salary ~ experience, data = soc)\nsummary(soc.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ experience, data = soc)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17666.3  -5498.2   -726.7   4667.7  27811.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11368.7     3160.3   3.597 0.000758 ***\nexperience    2141.4      160.8  13.314  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8642 on 48 degrees of freedom\nMultiple R-squared:  0.7869,\tAdjusted R-squared:  0.7825 \nF-statistic: 177.3 on 1 and 48 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n \n\nThe slope is (significantly) positive, which squares with our guess\n(more experience goes with greater salary), and also the upward trend\non the scatterplot. The value of the slope is about 2,000; this means\nthat one more year of experience goes with about a \\$2,000 increase in\nsalary. \n\n$\\blacksquare$\n\n(d) Obtain and plot the residuals against the fitted values. What\nproblem do you see?\n\n\nSolution\n\n\nThe easiest way to do this with `ggplot` is to plot the\n*regression object* (even though it is not actually a data\nframe), and plot the `.fitted` and `.resid`\ncolumns in it, not forgetting the initial dots:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(soc.1, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/socwork-11-1.png){width=672}\n:::\n:::\n\n       \nI see a \"fanning-out\": the residuals are getting bigger *in size* \n(further away from zero) as the fitted values get bigger. That\nis, when the (estimated) salary gets larger, it also gets more\nvariable. \n\nFanning-out is sometimes hard to see. What you can do if you suspect\nthat it might have happened is to plot the *absolute value* of\nthe residuals against the fitted values. The absolute value is the\nresidual without its plus or minus sign, so if the residuals are\ngetting bigger in size, their absolute values are getting bigger. That\nwould look like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(soc.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/socwork-12-1.png){width=672}\n:::\n:::\n\n \n\nI added a smooth trend to this to help us judge whether the\nabsolute-value-residuals are getting bigger as the fitted values get\nbigger. It looks to me as if the overall trend is an increasing one,\napart from those few small fitted values that have larger-sized\nresiduals. Don't get thrown off by the kinks in the smooth trend. Here\nis a smoother version:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(soc.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth(span = 2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/socwork-13-1.png){width=672}\n:::\n:::\n\n \n\nThe larger fitted values, according to this, have residuals larger in size.\n\nThe thing that controls the smoothness of the smooth trend is the\nvalue of `span` in `geom_smooth`. The default is\n0.75. The larger the value you use, the smoother the trend; the\nsmaller, the more wiggly. I'm inclined to think that the default value\nis a bit too small. Possibly this value is too big, but it shows you\nthe idea.\n\n$\\blacksquare$\n\n(e) The problem you unearthed in the previous part is often helped\nby a transformation. Run Box-Cox on your data to find a suitable\ntransformation. What transformation is suggested?\n\n\nSolution\n\n\nYou'll need to load (and install if necessary) the package\n`MASS` that contains `boxcox`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n```\n:::\n\n \nWhen you run this, you may see a warning containing the word \"masked\". I talk about that below.\n\n::: {.cell}\n\n```{.r .cell-code}\nboxcox(salary ~ experience, data = soc)\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/socwork-15-1.png){width=672}\n:::\n:::\n\n \n\nThat one looks like $\\lambda=0$ or log. You could probably also\njustify fourth root (power 0.25), but log is a very common\ntransformation, which people won't need much persuasion to accept.\n\nExtra: There's one annoyance with `MASS`: it has a `select`\n(which I have never used), and if you load `tidyverse` first\nand `MASS` second, as I have done here, when you mean to run\nthe column-selection `select`, it will actually run the\n`select` that comes from `MASS`, and give you an error\nthat you will have a terrible time debugging. That's what that\n\"masked\" message was when you loaded `MASS`. This is a great place to learn about the `conflicted` package. See [here](https://github.com/r-lib/conflicted) for how it works. (Scroll down to under the list of files.)\n\nIf you want to insist on something like \"the `select` that lives in `dplyr`\", \nyou can do that by saying\n`dplyr::select`. But this is kind of cumbersome if you don't\nneed to do it.\n\n$\\blacksquare$\n\n(f)  Use your transformed response in a regression,\nshowing the summary.\n\n\nSolution\n\n\nYou can do the transformation right in the `lm`, as I do below, or if you prefer, you can create a new column that is the log-salary and then use that in the `lm`. Either way is good:\n\n::: {.cell}\n\n```{.r .cell-code}\nsoc.3 <- lm(log(salary) ~ experience, data = soc)\nsummary(soc.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(salary) ~ experience, data = soc)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.35435 -0.09046 -0.01725  0.09739  0.26355 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 9.841315   0.056356  174.63   <2e-16 ***\nexperience  0.049979   0.002868   17.43   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1541 on 48 degrees of freedom\nMultiple R-squared:  0.8635,\tAdjusted R-squared:  0.8607 \nF-statistic: 303.7 on 1 and 48 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n$\\blacksquare$\n\n(g) Obtain and plot the residuals against the fitted values for\nthis regression. Do you seem to have solved the problem with the\nprevious residual plot?\n\n\nSolution\n\n\nAs we did before, treating the regression object as if it were a\ndata frame:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(soc.3, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/socwork-20-1.png){width=672}\n:::\n:::\n\n       \n\nThat, to my mind, is a horizontal band of points, so I would say yes,\nI have solved the fanning out.\n\nOne concern I have about the residuals is that there seem to be a\ncouple of very negative values: that is, are the residuals normally\ndistributed as they should be? Well, that's easy enough to check:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(soc.3, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/socwork-21-1.png){width=672}\n:::\n:::\n\n \n\nThe issues here are that those bottom two values are a bit too low,\nand the top few values are a bit bunched up (that curve at the top).\nIt is really not bad, though, so I am making the call that I don't\nthink I needed to worry.\nNote that the transformation we found here is the same as the\nlog-salary used by the management consultants in the\nbackward-elimination question, and with the same effect: an extra year\nof experience goes with a *percent* increase in salary.\n\nWhat increase? Well, the slope is about 0.05, so adding a year of\nexperience is predicted to increase log-salary by 0.05, or to\nmultiply actual salary by \n\n::: {.cell}\n\n```{.r .cell-code}\nexp(0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.051271\n```\n:::\n:::\n\n \n\nor to increase salary by about 5\\%.^[Mathematically,  $e^x$ is approximately $1+x$ for small $x$, which winds up meaning that the  slope in a model like this, if it is small, indicates about the  percent increase in the response associated with a 1-unit change in  the explanatory variable. Note that this only works with $e^x$ and  natural logs, not base 10 logs or anything like that.]\n\n\n$\\blacksquare$\n\n\n\n\n\n\n\n\n\n\n## Predicting volume of wood in pine trees\n\nIn forestry, the financial value of a tree is the volume of wood that it\ncontains. This is difficult to estimate while the tree is still\nstanding, but the diameter is easy to measure with a tape measure (to\nmeasure the circumference) and a calculation involving $\\pi$, assuming\nthat the cross-section of the tree is at least approximately circular.\nThe standard measurement is \"diameter at breast height\" (that is, at the\nheight of a human breast or chest), defined as being 4.5 feet above the\nground.\n\nSeveral pine trees had their diameter measured shortly before being cut\ndown, and for each tree, the volume of wood was recorded. The data are\nin [link](http://ritsokiguess.site/datafiles/pinetrees.txt). The\ndiameter is in inches and the volume is in cubic inches. Is it possible\nto predict the volume of wood from the diameter?\n\n(a) Read the data into R and display the values (there are not very\n    many).\n\nSolution\n\nObserve that the data values are separated by spaces, and therefore that\n`read_delim` will do it:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/pinetrees.txt\"\ntrees <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 10 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): diameter, volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ntrees\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"diameter\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"volume\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"32\",\"2\":\"185\"},{\"1\":\"29\",\"2\":\"109\"},{\"1\":\"24\",\"2\":\"95\"},{\"1\":\"45\",\"2\":\"300\"},{\"1\":\"20\",\"2\":\"30\"},{\"1\":\"30\",\"2\":\"125\"},{\"1\":\"26\",\"2\":\"55\"},{\"1\":\"40\",\"2\":\"246\"},{\"1\":\"24\",\"2\":\"60\"},{\"1\":\"18\",\"2\":\"15\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThat looks like the data file.\n\n$\\blacksquare$\n\n(b) Make a suitable plot.\n\nSolution\n\nNo clues this time. You need to recognize that you have two quantitative\nvariables, so that a scatterplot is called for. Also, the volume is the\nresponse, so that should go on the $y$-axis:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(trees, aes(x = diameter, y = volume)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/pinetrees-2-1.png){width=672}\n:::\n:::\n\nYou can put a smooth trend on it if you like, which would look like\nthis:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(trees, aes(x = diameter, y = volume)) +\n  geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/pinetrees-3-1.png){width=672}\n:::\n:::\n\nI'll take either of those for this part, though I think the smooth trend\nactually obscures the issue here (because there is not so much data).\n\n$\\blacksquare$\n\n(c) Describe what you learn from your plot about the relationship\n    between diameter and volume, if anything.\n\nSolution\n\nThe word \"relationship\" offers a clue that a scatterplot would have been\na good idea, if you hadn't realized by now. I am guided by \"form,\ndirection, strength\" in looking at a scatterplot:\n\n-   Form: it is an apparently linear relationship.\n\n-   Direction: it is an upward trend: that is, a tree with a larger\n    diameter also has a larger volume of wood. (This is not very\n    surprising.)\n\n-   Strength: I'd call this a strong (or moderate-to-strong)\n    relationship. (We'll see in a minute what the R-squared is.)\n\nYou don't need to be as formal as this, but you *do* need to get at the\nidea that it is an upward trend, apparently linear, and at least fairly\nstrong.[^_pinetrees-1]\n\n[^_pinetrees-1]: When this was graded, it was 3 marks, to clue you in\n    that there are three things to say.\n\n$\\blacksquare$\n\n(d) Fit a (linear) regression, predicting volume from diameter, and\n    obtain the `summary`. How would you describe the R-squared?\n\nSolution\n\nMy naming convention is (usually) to call the fitted model object by the\nname of the response variable and a number.[^_pinetrees-2]\n\n[^_pinetrees-2]: I have always used dots, but in the spirit of the\n    `tidyverse` I suppose I should use underscores.\n\n::: {.cell}\n\n```{.r .cell-code}\nvolume.1 <- lm(volume ~ diameter, data = trees)\nsummary(volume.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = volume ~ diameter, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.497  -9.982   1.751   8.959  28.139 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -191.749     23.954  -8.005 4.35e-05 ***\ndiameter      10.894      0.801  13.600 8.22e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.38 on 8 degrees of freedom\nMultiple R-squared:  0.9585,\tAdjusted R-squared:  0.9534 \nF-statistic:   185 on 1 and 8 DF,  p-value: 8.217e-07\n```\n:::\n:::\n\nR-squared is nearly 96%, so the relationship is definitely a strong one.\n\nI also wanted to mention the `broom` package, which was installed with\nthe `tidyverse` but which you need to load separately. It provides two\nhandy ways to summarize a fitted model (regression, analysis of variance\nor whatever):\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nglance(volume.1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"r.squared\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"adj.r.squared\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sigma\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"logLik\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"AIC\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BIC\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"deviance\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df.residual\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"nobs\"],\"name\":[12],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.9585402\",\"2\":\"0.9533577\",\"3\":\"20.38484\",\"4\":\"184.9578\",\"5\":\"8.21677e-07\",\"6\":\"1\",\"7\":\"-43.22158\",\"8\":\"92.44316\",\"9\":\"93.35092\",\"10\":\"3324.333\",\"11\":\"8\",\"12\":\"10\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThis gives a one-line summary of a model, including things like\nR-squared. This is handy if you're fitting more than one model, because\nyou can collect the one-line summaries together into a data frame and\neyeball them.\n\nThe other summary is this one:\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(volume.1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"-191.74923\",\"3\":\"23.9536222\",\"4\":\"-8.00502\",\"5\":\"4.34715e-05\"},{\"1\":\"diameter\",\"2\":\"10.89407\",\"3\":\"0.8010394\",\"4\":\"13.59992\",\"5\":\"8.21677e-07\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThis gives a table of intercepts, slopes and their P-values, but the\nvalue to this one is that it is a *data frame*, so if you want to pull\nanything out of it, you know how to do that:[^_pinetrees-3]\n\n[^_pinetrees-3]: The `summary` output is more designed for looking at\n    than for extracting things from.\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(volume.1) %>% filter(term == \"diameter\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"diameter\",\"2\":\"10.89407\",\"3\":\"0.8010394\",\"4\":\"13.59992\",\"5\":\"8.21677e-07\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThis gets the estimated slope and its P-value, without worrying about\nthe corresponding things for the intercept, which are usually of less\ninterest anyway.\n\n$\\blacksquare$\n\n(e) Draw a graph that will help you decide whether you trust the\n    linearity of this regression. What do you conclude? Explain briefly.\n\nSolution\n\nThe thing I'm fishing for is a residual plot (of the residuals against\nthe fitted values), and on it you are looking for a random mess of\nnothingness:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(volume.1, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/pinetrees-8-1.png){width=672}\n:::\n:::\n\nMake a call. You could say that there's no discernible pattern,\nespecially with such a small data set, and therefore that the regression\nis fine. Or you could say that there is fanning-in: the two points on\nthe right have residuals close to 0 while the points on the left have\nresiduals larger in size. Say something.\n\nI don't think you can justify a curve or a trend, because the residuals\non the left are both positive and negative.\n\nMy feeling is that the residuals on the right are close to 0 because\nthese points have noticeably larger diameter than the others, and they\nare *influential* points in the regression that will pull the line\ncloser to themselves. This is why their residuals are close to zero. But\nI am happy with either of the points made in the paragraph under the\nplot.\n\n$\\blacksquare$\n\n(f) What would you guess would be the volume of a tree of diameter zero?\n    Is that what the regression predicts? Explain briefly.\n\nSolution\n\nLogically, a tree that has diameter zero is a non-existent tree, so its\nvolume should be zero as well. In the regression, the quantity that says\nwhat volume is when diameter is zero is the *intercept*. Here the\nintercept is $-192$, which is definitely not zero. In fact, if you look\nat the P-value, the intercept is significantly *less* than zero. Thus,\nthe model makes no logical sense for trees of small diameter. The\nsmallest tree in the data set has diameter 18, which is not really\nsmall, I suppose, but it is a little disconcerting to have a model that\nmakes no logical sense.\n\n$\\blacksquare$\n\n(g) A simple way of modelling a tree's shape is to pretend it is a cone,\n    like this, but probably taller and skinnier:\n\n![](conebnw.jpg)\n\nwith its base on the ground. What is the relationship between the\n*diameter* (at the base) and volume of a cone? (If you don't remember,\nlook it up. You'll probably get a formula in terms of the radius, which\nyou'll have to convert. Cite the website you used.)\n\nSolution\n\nAccording to\n[link](http://www.web-formulas.com/Math_Formulas/Geometry_Volume_of_Cone.aspx),\nthe volume of a cone is $V=\\pi r^2h/3$, where $V$ is the volume, $r$ is\nthe radius (at the bottom of the cone) and $h$ is the height. The\ndiameter is twice the radius, so replace $r$ by $d/2$, $d$ being the\ndiameter. A little algebra gives $$ V = \\pi d^2 h / 12.$$\n\n$\\blacksquare$\n\n(h) Fit a regression model that predicts volume from diameter according\n    to the formula you obtained in the previous part. You can assume\n    that the trees in this data set are of similar heights, so that the\n    height can be treated as a constant.\\\n    Display the results.\n\nSolution\n\nAccording to my formula, the volume depends on the diameter squared,\nwhich I include in the model thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nvolume.2 <- lm(volume ~ I(diameter^2), data = trees)\nsummary(volume.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = volume ~ I(diameter^2), data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.708  -9.065  -5.722   3.032  40.816 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -30.82634   13.82243   -2.23   0.0563 .  \nI(diameter^2)   0.17091    0.01342   12.74 1.36e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.7 on 8 degrees of freedom\nMultiple R-squared:  0.953,\tAdjusted R-squared:  0.9471 \nF-statistic: 162.2 on 1 and 8 DF,  p-value: 1.359e-06\n```\n:::\n:::\n\nThis adds an intercept as well, which is fine (there are technical\ndifficulties around removing the intercept).\n\nThat's as far as I wanted you to go, but (of course) I have a few\ncomments.\n\nThe intercept here is still negative, but not significantly different\nfrom zero, which is a step forward. The R-squared for this regression is\nvery similar to that from our linear model (the one for which the\nintercept made no sense). So, from that point of view, either model\npredicts the data well. I should look at the residuals from this one:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(volume.2, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/pinetrees-10-1.png){width=672}\n:::\n:::\n\nI really don't think there are any problems there.\n\nNow, I said to assume that the trees are all of similar height. This\nseems entirely questionable, since the trees vary quite a bit in\ndiameter, and you would guess that trees with bigger diameter would also\nbe taller. It seems more plausible that the same kind of trees (pine\ntrees in this case) would have the same \"shape\", so that if you knew the\ndiameter you could *predict* the height, with larger-diameter trees\nbeing taller. Except that we don't have the heights here, so we can't\nbuild a model for that.\n\nSo I went looking in the literature. I found this paper:\n[link](https://pdfs.semanticscholar.org/5497/3d02d63428e3dfed6645acfdba874ad80822.pdf).\nThis gives several models for relationships between volume, diameter and\nheight. In the formulas below, there is an implied \"plus error\" on the\nright, and the $\\alpha_i$ are parameters to be estimated.\n\nFor predicting height from diameter (equation 1 in paper):\n\n$$  h = \\exp(\\alpha_1+\\alpha_2 d^{\\alpha_3}) $$\n\nFor predicting volume from height and diameter (equation 6):\n\n$$  V = \\alpha_1 d^{\\alpha_2} h^{\\alpha_3} $$\n\nThis is a take-off on our assumption that the trees were cone-shaped,\nwith cone-shaped trees having $\\alpha_1=\\pi/12$, $\\alpha_2=2$ and\n$\\alpha_3=1$. The paper uses different units, so $\\alpha_1$ is not\ncomparable, but $\\alpha_2$ and $\\alpha_3$ are (as estimated from the\ndata in the paper, which were for longleaf pine) quite close to 2 and 1.\n\nLast, the actual relationship that helps us: predicting volume from just\ndiameter (equation 5):\n\n$$  V = \\alpha_1 d^{\\alpha_2}$$\n\nThis is a power law type of relationship. For example, if you were\nwilling to pretend that a tree was a cone with height proportional to\ndiameter (one way of getting at the idea of a bigger tree typically\nbeing taller, instead of assuming constant height as we did), that would\nimply $\\alpha_2=3$ here.\n\nThis is non-linear as it stands, but we can bash it into shape by taking\nlogs:\n\n$$\n\\ln V = \\ln \\alpha_1 + \\alpha_2 \\ln d\n$$\n\nso that log-volume has a linear relationship with log-diameter and we\ncan go ahead and estimate it:\n\n::: {.cell}\n\n```{.r .cell-code}\nvolume.3 <- lm(log(volume) ~ log(diameter), data = trees)\nsummary(volume.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(volume) ~ log(diameter), data = trees)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40989 -0.22341  0.01504  0.10459  0.53596 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -5.9243     1.1759  -5.038    0.001 ** \nlog(diameter)   3.1284     0.3527   8.870 2.06e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3027 on 8 degrees of freedom\nMultiple R-squared:  0.9077,\tAdjusted R-squared:  0.8962 \nF-statistic: 78.68 on 1 and 8 DF,  p-value: 2.061e-05\n```\n:::\n:::\n\nThe parameter that I called $\\alpha_2$ above is the slope of this model,\n3.13. This is a bit different from the figure in the paper, which was\n2.19. I think these are comparable even though the other parameter is\nnot (again, measurements in different units, plus, this time we need to\ntake the log of it). I think the \"slopes\" are comparable because we\nhaven't estimated our slope all that accurately:\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(volume.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  2.5 %    97.5 %\n(Intercept)   -8.635791 -3.212752\nlog(diameter)  2.315115  3.941665\n```\n:::\n:::\n\nFrom 2.3 to 3.9. It is definitely not zero, but we are rather less sure\nabout what it is, and 2.19 is not completely implausible.\n\nThe R-squared here, though it is less than the other ones we got, is\nstill high. The residuals are these:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(volume.3, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/pinetrees-13-1.png){width=672}\n:::\n:::\n\nwhich again seem to show no problems. The residuals are smaller in size\nnow because of the log transformation: the actual and predicted\nlog-volumes are smaller numbers than the actual and predicted volumes,\nso the residuals are now closer to zero.\n\nDoes this model behave itself at zero? Well, roughly at least: if the\ndiameter is very small, its log is very negative, and the predicted\nlog-volume is also very negative (the slope is positive). So the\npredicted actual volume will be close to zero. If you want to make that\nmathematically rigorous, you can take limits, but that's the intuition.\nWe can also do some predictions: set up a data frame that has a column\ncalled `diameter` with some diameters to predict for:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tibble(diameter = c(1, 2, seq(5, 50, 5)))\nd\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"diameter\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\"},{\"1\":\"2\"},{\"1\":\"5\"},{\"1\":\"10\"},{\"1\":\"15\"},{\"1\":\"20\"},{\"1\":\"25\"},{\"1\":\"30\"},{\"1\":\"35\"},{\"1\":\"40\"},{\"1\":\"45\"},{\"1\":\"50\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nand then feed that into `predictions`from package `marginaleffects`:\n\n::: {.cell}\n\n```{.r .cell-code}\np <- cbind(predictions(volume.3, newdata = d)) \np %>% select(diameter, estimate, conf.low, conf.high) -> pp\npp\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"diameter\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"-5.9242712\",\"3\":\"-8.2288987\",\"4\":\"-3.6196437\"},{\"1\":\"2\",\"2\":\"-3.7558365\",\"3\":\"-5.5833436\",\"4\":\"-1.9283295\"},{\"1\":\"5\",\"2\":\"-0.8893218\",\"3\":\"-2.0885682\",\"4\":\"0.3099245\"},{\"1\":\"10\",\"2\":\"1.2791128\",\"3\":\"0.5492341\",\"4\":\"2.0089915\"},{\"1\":\"15\",\"2\":\"2.5475658\",\"3\":\"2.0829205\",\"4\":\"3.0122111\"},{\"1\":\"20\",\"2\":\"3.4475475\",\"3\":\"3.1536433\",\"4\":\"3.7414517\"},{\"1\":\"25\",\"2\":\"4.1456275\",\"3\":\"3.9446714\",\"4\":\"4.3465837\"},{\"1\":\"30\",\"2\":\"4.7160005\",\"3\":\"4.5207483\",\"4\":\"4.9112526\"},{\"1\":\"35\",\"2\":\"5.1982439\",\"3\":\"4.9512720\",\"4\":\"5.4452158\"},{\"1\":\"40\",\"2\":\"5.6159822\",\"3\":\"5.3010818\",\"4\":\"5.9308825\"},{\"1\":\"45\",\"2\":\"5.9844534\",\"3\":\"5.6010851\",\"4\":\"6.3678218\"},{\"1\":\"50\",\"2\":\"6.3140622\",\"3\":\"5.8657639\",\"4\":\"6.7623605\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThese are predicted log-volumes, so we'd better anti-log them. `log` in\nR is natural logs, so this is inverted using `exp`. The ends of the\nconfidence intervals can be exp-ed as well, which I do all at once:\n\n::: {.cell}\n\n```{.r .cell-code}\npp %>% mutate(across(-diameter, \\(x) exp(x)))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"diameter\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"2.673756e-03\",\"3\":\"2.668300e-04\",\"4\":\"0.02679222\"},{\"1\":\"2\",\"2\":\"2.338088e-02\",\"3\":\"3.759973e-03\",\"4\":\"0.14539088\"},{\"1\":\"5\",\"2\":\"4.109343e-01\",\"3\":\"1.238644e-01\",\"4\":\"1.36332224\"},{\"1\":\"10\",\"2\":\"3.593450e+00\",\"3\":\"1.731926e+00\",\"4\":\"7.45579458\"},{\"1\":\"15\",\"2\":\"1.277597e+01\",\"3\":\"8.027880e+00\",\"4\":\"20.33230738\"},{\"1\":\"20\",\"2\":\"3.142323e+01\",\"3\":\"2.342124e+01\",\"4\":\"42.15914904\"},{\"1\":\"25\",\"2\":\"6.315724e+01\",\"3\":\"5.165936e+01\",\"4\":\"77.21422468\"},{\"1\":\"30\",\"2\":\"1.117205e+02\",\"3\":\"9.190435e+01\",\"4\":\"135.80942573\"},{\"1\":\"35\",\"2\":\"1.809542e+02\",\"3\":\"1.413546e+02\",\"4\":\"231.64727491\"},{\"1\":\"40\",\"2\":\"2.747831e+02\",\"3\":\"2.005537e+02\",\"4\":\"376.48662098\"},{\"1\":\"45\",\"2\":\"3.972054e+02\",\"3\":\"2.707200e+02\",\"4\":\"582.78700332\"},{\"1\":\"50\",\"2\":\"5.522839e+02\",\"3\":\"3.527515e+02\",\"4\":\"864.68090044\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nFor a diameter near zero, the predicted volume appears to be near zero\nas well. If you don't like the scientific notation:\n\n::: {.cell}\n\n```{.r .cell-code}\npp %>% mutate(across(-diameter, \\(x) exp(x))) %>% \n  mutate(across(-diameter, \\(x) format(x, scientific = FALSE)))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"diameter\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"conf.low\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"conf.high\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"0.002673756\",\"3\":\"0.000266830\",\"4\":\"0.02679222\"},{\"1\":\"2\",\"2\":\"0.023380883\",\"3\":\"0.003759973\",\"4\":\"0.14539088\"},{\"1\":\"5\",\"2\":\"0.410934340\",\"3\":\"0.123864356\",\"4\":\"1.36332224\"},{\"1\":\"10\",\"2\":\"3.593450319\",\"3\":\"1.731926095\",\"4\":\"7.45579458\"},{\"1\":\"15\",\"2\":\"12.775966620\",\"3\":\"8.027879965\",\"4\":\"20.33230738\"},{\"1\":\"20\",\"2\":\"31.423232247\",\"3\":\"23.421239452\",\"4\":\"42.15914904\"},{\"1\":\"25\",\"2\":\"63.157243234\",\"3\":\"51.659359265\",\"4\":\"77.21422468\"},{\"1\":\"30\",\"2\":\"111.720527798\",\"3\":\"91.904345110\",\"4\":\"135.80942573\"},{\"1\":\"35\",\"2\":\"180.954189310\",\"3\":\"141.354646374\",\"4\":\"231.64727491\"},{\"1\":\"40\",\"2\":\"274.783129616\",\"3\":\"200.553656132\",\"4\":\"376.48662098\"},{\"1\":\"45\",\"2\":\"397.205361735\",\"3\":\"270.720003178\",\"4\":\"582.78700332\"},{\"1\":\"50\",\"2\":\"552.283890387\",\"3\":\"352.751512640\",\"4\":\"864.68090044\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nNote now that these, though they look like numbers, are actually *text*,\nso if you want to display numbers in non-scientific notation like this,\ndo it at the *very end*, after you have finished any calculations with\nthe numbers.\n\n<br>\n\nI mentioned `broom` earlier. We can make a data frame out of the\none-line summaries of our three models:\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_rows(glance(volume.1), glance(volume.2), glance(volume.3))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"r.squared\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"adj.r.squared\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sigma\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"logLik\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"AIC\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BIC\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"deviance\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df.residual\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"nobs\"],\"name\":[12],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.9585402\",\"2\":\"0.9533577\",\"3\":\"20.3848388\",\"4\":\"184.95779\",\"5\":\"8.216770e-07\",\"6\":\"1\",\"7\":\"-43.221582\",\"8\":\"92.443164\",\"9\":\"93.350919\",\"10\":\"3324.3332304\",\"11\":\"8\",\"12\":\"10\"},{\"1\":\"0.9530072\",\"2\":\"0.9471331\",\"3\":\"21.7024608\",\"4\":\"162.23895\",\"5\":\"1.359294e-06\",\"6\":\"1\",\"7\":\"-43.847924\",\"8\":\"93.695848\",\"9\":\"94.603604\",\"10\":\"3767.9744307\",\"11\":\"8\",\"12\":\"10\"},{\"1\":\"0.9077109\",\"2\":\"0.8961748\",\"3\":\"0.3027174\",\"4\":\"78.68414\",\"5\":\"2.061375e-05\",\"6\":\"1\",\"7\":\"-1.124113\",\"8\":\"8.248226\",\"9\":\"9.155981\",\"10\":\"0.7331027\",\"11\":\"8\",\"12\":\"10\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n(I mistakenly put `glimpse` instead of `glance` there the first time.\nThe former is for a quick look at a *data frame*, while the latter is\nfor a quick look at a *model*.)\n\nThe three R-squareds are all high, with the one from the third model\nbeing a bit lower as we saw before.\n\nMy code is rather repetitious. There has to be a way to streamline it. I\nwas determined to find out how. My solution involves putting the three\nmodels in a list-column, and then using `rowwise` to get the `glance`\noutput for each one.\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(i = 1:3, model = list(volume.1, volume.2, volume.3)) %>% \n  rowwise() %>% \n  mutate(glances = list(glance(model))) %>% \n  unnest(glances)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"i\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"model\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"r.squared\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"adj.r.squared\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sigma\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"logLik\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"AIC\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BIC\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"deviance\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"df.residual\"],\"name\":[13],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"nobs\"],\"name\":[14],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"<S3: lm>\",\"3\":\"0.9585402\",\"4\":\"0.9533577\",\"5\":\"20.3848388\",\"6\":\"184.95779\",\"7\":\"8.216770e-07\",\"8\":\"1\",\"9\":\"-43.221582\",\"10\":\"92.443164\",\"11\":\"93.350919\",\"12\":\"3324.3332304\",\"13\":\"8\",\"14\":\"10\"},{\"1\":\"2\",\"2\":\"<S3: lm>\",\"3\":\"0.9530072\",\"4\":\"0.9471331\",\"5\":\"21.7024608\",\"6\":\"162.23895\",\"7\":\"1.359294e-06\",\"8\":\"1\",\"9\":\"-43.847924\",\"10\":\"93.695848\",\"11\":\"94.603604\",\"12\":\"3767.9744307\",\"13\":\"8\",\"14\":\"10\"},{\"1\":\"3\",\"2\":\"<S3: lm>\",\"3\":\"0.9077109\",\"4\":\"0.8961748\",\"5\":\"0.3027174\",\"6\":\"78.68414\",\"7\":\"2.061375e-05\",\"8\":\"1\",\"9\":\"-1.124113\",\"10\":\"8.248226\",\"11\":\"9.155981\",\"12\":\"0.7331027\",\"13\":\"8\",\"14\":\"10\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nI almost got caught by forgetting the `list` on the definition of\n`glances`. I certainly need it, because the output from `glance` is a\n(one-row) dataframe, not a single number.\n\nIt works. You see the three R-squared values in the first column of\nnumbers. The third model is otherwise a lot different from the others\nbecause it has a different response variable.\n\nOther thoughts:\n\nHow might you measure or estimate the height of a tree (other than by\nclimbing it and dropping a tape measure down)? One way, that works if\nthe tree is fairly isolated, is to walk away from its base.\nPeriodically, you point at the top of the tree, and when the angle\nbetween your arm and the ground reaches 45 degrees, you stop walking.\n(If it's greater than 45 degrees, you walk further away, and if it's\nless, you walk back towards the tree.) The distance between you and the\nbase of the tree is then equal to the height of the tree, and if you\nhave a long enough tape measure you can measure it.\n\nThe above works because the tangent of 45 degrees is 1. If you have a\ndevice that will measure the actual angle,[^_pinetrees-4] you can be any\ndistance away from the tree, point the device at the top, record the\nangle, and do some trigonometry to estimate the height of the tree (to\nwhich you add the height of your eyes).\n\n[^_pinetrees-4]: These days, there are apps that will let you do this\n    with your phone. I found one called Clinometer. See also\n    [link](https://gabrielhemery.com/how-to-calculate-tree-height-using-a-smartphone/).\n\n$\\blacksquare$\n\n\n##  Tortoise shells and eggs\n\n\n A biologist measured the length of the carapace (shell) of\nfemale tortoises, and then x-rayed the tortoises to count how many\neggs they were carrying. The length is measured in millimetres. The\ndata are in\n[link](http://ritsokiguess.site/datafiles/tortoise-eggs.txt). The\nbiologist is wondering what kind of relationship, if any, there is\nbetween the carapace length (as an explanatory variable) and the\nnumber of eggs (as a response variable).\n\n\n\n(a) Read in the data, and check that your values look\nreasonable. \n \nSolution\n\n\nLook at the data first. The columns are aligned and separated by\nmore than one space, so it's `read_table`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/tortoise-eggs.txt\"\ntortoises <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  length = col_double(),\n  eggs = col_double()\n)\n```\n:::\n\n```{.r .cell-code}\ntortoises\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"length\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"eggs\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"284\",\"2\":\"3\"},{\"1\":\"290\",\"2\":\"2\"},{\"1\":\"290\",\"2\":\"7\"},{\"1\":\"290\",\"2\":\"7\"},{\"1\":\"298\",\"2\":\"11\"},{\"1\":\"299\",\"2\":\"12\"},{\"1\":\"302\",\"2\":\"10\"},{\"1\":\"306\",\"2\":\"8\"},{\"1\":\"306\",\"2\":\"8\"},{\"1\":\"309\",\"2\":\"9\"},{\"1\":\"310\",\"2\":\"10\"},{\"1\":\"311\",\"2\":\"13\"},{\"1\":\"317\",\"2\":\"7\"},{\"1\":\"317\",\"2\":\"9\"},{\"1\":\"320\",\"2\":\"6\"},{\"1\":\"323\",\"2\":\"13\"},{\"1\":\"334\",\"2\":\"2\"},{\"1\":\"334\",\"2\":\"8\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n     \n\nThose look the same as the values in the data file. (*Some*\ncomment is needed here. I don't much mind what, but something that\nsuggests that you have eyeballed the data and there are no obvious\nproblems: that is what I am looking for.)\n \n\n$\\blacksquare$\n\n(b) Obtain a scatterplot, with a smooth trend, of the data.\n\n \nSolution\n\n\nSomething like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tortoises, aes(x = length, y = eggs)) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/looe-1.png){width=672}\n:::\n:::\n\n   \n \n\n$\\blacksquare$\n\n(c) The biologist expected that a larger tortoise would be able\nto carry more eggs. Is that what the scatterplot is suggesting?\nExplain briefly why or why not.\n\n \nSolution\n\n\nThe biologist's expectation is of an upward trend. But it looks as\nif the trend on the scatterplot is up, then down, ie.\\ a curve\nrather than a straight line. So this is not what the biologist was\nexpecting. \n \n\n$\\blacksquare$\n\n(d) Fit a straight-line relationship and display the summary.\n\n \nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntortoises.1 <- lm(eggs ~ length, data = tortoises)\nsummary(tortoises.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = eggs ~ length, data = tortoises)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7790 -1.1772 -0.0065  2.0487  4.8556 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.43532   17.34992  -0.025    0.980\nlength       0.02759    0.05631   0.490    0.631\n\nResidual standard error: 3.411 on 16 degrees of freedom\nMultiple R-squared:  0.01478,\tAdjusted R-squared:  -0.0468 \nF-statistic:  0.24 on 1 and 16 DF,  p-value: 0.6308\n```\n:::\n:::\n\n   \n\nI didn't ask for a comment, but feel free to observe that this\nregression is truly awful, with an R-squared of less than 2\\% and a\nnon-significant effect of `length`.\n \n\n$\\blacksquare$\n\n(e) Add a squared term to your regression, fit that and display\nthe  summary.\n\n \nSolution\n\n\nThe `I()` is needed because the raise-to-a-power symbol has\na special meaning in a model formula, and we want to *not*\nuse that special meaning:\n\n::: {.cell}\n\n```{.r .cell-code}\ntortoises.2 <- lm(eggs ~ length + I(length^2), data = tortoises)\nsummary(tortoises.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = eggs ~ length + I(length^2), data = tortoises)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0091 -1.8480 -0.1896  2.0989  4.3605 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -8.999e+02  2.703e+02  -3.329  0.00457 **\nlength       5.857e+00  1.750e+00   3.347  0.00441 **\nI(length^2) -9.425e-03  2.829e-03  -3.332  0.00455 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.671 on 15 degrees of freedom\nMultiple R-squared:  0.4338,\tAdjusted R-squared:  0.3583 \nF-statistic: 5.747 on 2 and 15 DF,  p-value: 0.01403\n```\n:::\n:::\n\n \n\nAnother way is to use `update`:\n\n::: {.cell}\n\n```{.r .cell-code}\ntortoises.2a <- update(tortoises.1, . ~ . + I(length^2))\nsummary(tortoises.2a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = eggs ~ length + I(length^2), data = tortoises)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0091 -1.8480 -0.1896  2.0989  4.3605 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -8.999e+02  2.703e+02  -3.329  0.00457 **\nlength       5.857e+00  1.750e+00   3.347  0.00441 **\nI(length^2) -9.425e-03  2.829e-03  -3.332  0.00455 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.671 on 15 degrees of freedom\nMultiple R-squared:  0.4338,\tAdjusted R-squared:  0.3583 \nF-statistic: 5.747 on 2 and 15 DF,  p-value: 0.01403\n```\n:::\n:::\n\n \n \n\n$\\blacksquare$\n\n(f) Is a curve better than a line for these data? Justify your\nanswer in two ways: by comparing a measure of fit, and  by doing a\nsuitable test of significance.\n\n \nSolution\n\n\nAn appropriate measure of fit is R-squared. For the straight line,\nthis is about 0.01, and for the regression with the squared term it\nis about 0.43. This tells us that  a straight line fits appallingly\nbadly, and that a curve fits a *lot* better. \nThis doesn't do a test, though. For that, look at the slope of the\nlength-squared term in the second regression; in particular,\nlook at its P-value. This is 0.0045, which is small: the squared\nterm is necessary, and taking it out would be a mistake. The\nrelationship really is curved, and trying to describe it with a\nstraight line would be a big mistake.\n \n\n$\\blacksquare$\n\n(g) Make a residual plot for the straight line model: that is, plot\nthe residuals against the fitted values.\n Does this echo\nyour conclusions of the previous part? In what way? Explain briefly.\n\n \nSolution\n\n\nPlot the things called `.fitted` and `.resid` from the\nregression object, which is not a data frame but you can treat it as\nif it is for this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tortoises.1, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/tortoise-5-1.png){width=672}\n:::\n:::\n\n \n\nUp to you whether you put a smooth trend on it or not:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tortoises.1, aes(x = .fitted, y = .resid)) + geom_point() +\n  geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/tortoise-6-1.png){width=672}\n:::\n:::\n\n \nLooking at the plot, you see a curve, up and down. The most\nnegative residuals go with small or large fitted values; when the\nfitted value is in the middle, the residual is usually positive. A\ncurve on the residual plot indicates a curve in the actual\nrelationship. We just found above that a curve does fit a lot\nbetter, so this is all consistent.\n\nAside: the grey \"envelope\" is wide, so there is a lot of scatter on the\nresidual plot. The grey envelope almost contains zero all the way\nacross, so the evidence for a curve (or any other kind of trend) is\nnot all that strong, based on this plot. This is in great contrast to\nthe regression with length-squared, where the length-squared term is\n*definitely* necessary. \n\nThat was all I wanted, but you can certainly look at other\nplots. Normal quantile plot of the residuals:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tortoises.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/tortoise-7-1.png){width=672}\n:::\n:::\n\n \n\nThis is not the best: the low values are a bit too low, so that the\nwhole picture is (a little) skewed to the left.^[The very   negative residuals are at the left and right of the residual plot;  they are there because the relationship is a curve. If you were to  look at the residuals from the model with length-squared, you  probably wouldn't see this.]\n\nAnother plot you can make is to assess fan-out: you plot the\n*absolute value*^[The value, but throw away the minus sign if it has one.] of the residuals against the fitted values. The idea\nis that if there is fan-out, the absolute value of the residuals will\nget bigger:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tortoises.1, aes(x = .fitted, y = abs(.resid))) + geom_point() +\n  geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/tortoise-8-1.png){width=672}\n:::\n:::\n\n \n\nI put the smooth curve on as a kind of warning: it looks as if the\nsize of the residuals goes down and then up again as the fitted values\nincrease. But the width of the grey \"envelope\" and the general\nscatter of the points suggests that there is really not much happening\nhere at all. On a plot of residuals, the grey envelope is really more\ninformative than the blue smooth trend. On this one, there is no\nevidence of any fan-out (or fan-in). \n\n$\\blacksquare$\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Roller coasters\n\nA poll on the Discovery Channel asked people to nominate the best\nroller-coasters in the United States. We will examine the 10\nroller-coasters that received the most votes. Two features of a\nroller-coaster that are of interest are the distance it drops from start\nto finish, measured here in feet[^_coasters-1] and the duration of the\nride, measured in seconds. Is it true that roller-coasters with a bigger\ndrop also tend to have a longer ride? The data are at\n[link](http://ritsokiguess.site/datafiles/coasters.csv).[^_coasters-2]\n\n[^_coasters-1]: Roller-coasters work by gravity, so there must be some\n    drop.\n\n[^_coasters-2]: These are not to be confused with what your mom insists\n    that you place between your coffee mug and the table.\n\n(a) Read the data into R and verify that you have a sensible number of\n    rows and columns.\n\nSolution\n\nA `.csv`, so the usual for that:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/coasters.csv\"\ncoasters <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 10 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): coaster_name, state\ndbl (2): drop, duration\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ncoasters\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"coaster_name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"state\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"drop\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"duration\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Incredible Hulk\",\"2\":\"Florida\",\"3\":\"105\",\"4\":\"135\"},{\"1\":\"Millennium Force\",\"2\":\"Ohio\",\"3\":\"300\",\"4\":\"105\"},{\"1\":\"Goliath\",\"2\":\"California\",\"3\":\"255\",\"4\":\"180\"},{\"1\":\"Nitro\",\"2\":\"New Jersey\",\"3\":\"215\",\"4\":\"240\"},{\"1\":\"Magnum XL-2000\",\"2\":\"Ohio\",\"3\":\"195\",\"4\":\"120\"},{\"1\":\"The Beast\",\"2\":\"Ohio\",\"3\":\"141\",\"4\":\"65\"},{\"1\":\"Son of Beast\",\"2\":\"Ohio\",\"3\":\"214\",\"4\":\"140\"},{\"1\":\"Thunderbolt\",\"2\":\"Pennsylvania\",\"3\":\"95\",\"4\":\"90\"},{\"1\":\"Ghost Rider\",\"2\":\"California\",\"3\":\"108\",\"4\":\"160\"},{\"1\":\"Raven\",\"2\":\"Indiana\",\"3\":\"86\",\"4\":\"90\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThe number of marks for this kind of thing has been decreasing through\nthe course, since by now you ought to have figured out how to do it\nwithout looking it up.\n\nThere are 10 rows for the promised 10 roller-coasters, and there are\nseveral columns: the drop for each roller-coaster and the duration of\nits ride, as promised, as well as the name of each roller-coaster and\nthe state that it is in. (A lot of them seem to be in Ohio, for some\nreason that I don't know.) So this all looks good.\n\n$\\blacksquare$\n\n(b) Make a scatterplot of duration (response) against drop\n    (explanatory), labelling each roller-coaster with its name in such a\n    way that the labels do not overlap. Add a regression line to your\n    plot.\n\nSolution\n\nThe last part, about the labels not overlapping, is an invitation to use\n`ggrepel`, which is the way I'd recommend doing this. (If not, you have\nto do potentially lots of work organizing where the labels sit relative\nto the points, which is time you probably don't want to spend.) Thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggrepel)\nggplot(coasters, aes(x = drop, y = duration, label = coaster_name)) +\n  geom_point() + geom_text_repel() + \n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The following aesthetics were dropped during statistical transformation: label\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/coasters-2-1.png){width=672}\n:::\n:::\n\nThe `se=FALSE` at the end is optional; if you omit it, you get that\n\"envelope\" around the line, which is fine here.\n\nNote that with the labelling done this way, you can easily identify\nwhich roller-coaster is which.\n\nThe warning seems to be `ggplot` being over-zealous; the `geom_point` and the `geom_smooth` don't need a `label`, but `geom_text_repel` certainly does. If it bothers you, move the `label` into the `geom_text_repel`:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(coasters, aes(x = drop, y = duration)) +\n  geom_point() + geom_text_repel(aes(label = coaster_name)) + \n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n$\\blacksquare$\n\n(c) Would you say that roller-coasters with a larger drop tend to have a\n    longer ride? Explain briefly.\n\nSolution\n\nI think there are two good answers here: \"yes\" and \"kind of\". Supporting\n\"yes\" is the fact that the regression line does go uphill, so that\noverall, or on average, roller-coasters with a larger drop do tend to\nhave a longer duration of ride as well. Supporting \"kind of\" is the fact\nthat, though the regression line goes uphill, there are a lot of\nroller-coasters that are some way off the trend, far from the regression\nline. I am happy to go with either of those. I could also go with \"not\nreally\" and the same discussion that I attached to \"kind of\".\n\n$\\blacksquare$\n\n(d) Find a roller-coaster that is unusual compared to the others. What\n    about its combination of `drop` and `duration` is unusual?\n\nSolution\n\nThis is an invitation to find a point that is a long way off the line. I\nthink the obvious choice is my first one below, but I would take either\nof the others as well:\n\n-   \"Nitro\" is a long way above the line. That means it has a long\n    duration, relative to its drop. There are two other roller-coasters\n    that have a larger drop but not as long a duration. In other words,\n    this roller-coaster drops slowly, presumably by doing a lot of\n    twisting, loop-the-loop and so on.\n\n-   \"The Beast\" is a long way below the line, so it has a short duration\n    relative to its drop. It is actually the shortest ride of all, but\n    is only a bit below average in terms of drop. This suggests that The\n    Beast is one of those rides that drops a long way quickly.\n\n-   \"Millennium Force\" has the biggest drop of all, but a\n    shorter-than-average duration. This looks like another ride with a\n    big drop in it.\n\nA roller-coaster that is \"unusual\" will have a residual that is large in\nsize (either positive, like Nitro, or negative, like the other two). I\ndidn't ask you to find the residuals, but if you want to, `augment` from\n`broom` is the smoothest way to go:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nduration.1 <- lm(duration ~ drop, data = coasters)\naugment(duration.1, coasters) %>%\n  select(coaster_name, duration, drop, .resid) %>%\n  arrange(desc(abs(.resid)))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"coaster_name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"duration\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"drop\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".resid\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Nitro\",\"2\":\"240\",\"3\":\"215\",\"4\":\"96.95170\"},{\"1\":\"The Beast\",\"2\":\"65\",\"3\":\"141\",\"4\":\"-60.14522\"},{\"1\":\"Millennium Force\",\"2\":\"105\",\"3\":\"300\",\"4\":\"-58.61266\"},{\"1\":\"Ghost Rider\",\"2\":\"160\",\"3\":\"108\",\"4\":\"42.83859\"},{\"1\":\"Goliath\",\"2\":\"180\",\"3\":\"255\",\"4\":\"27.27435\"},{\"1\":\"Thunderbolt\",\"2\":\"90\",\"3\":\"95\",\"4\":\"-24.01628\"},{\"1\":\"Raven\",\"2\":\"90\",\"3\":\"86\",\"4\":\"-21.83887\"},{\"1\":\"Incredible Hulk\",\"2\":\"135\",\"3\":\"105\",\"4\":\"18.56439\"},{\"1\":\"Magnum XL-2000\",\"2\":\"120\",\"3\":\"195\",\"4\":\"-18.20963\"},{\"1\":\"Son of Beast\",\"2\":\"140\",\"3\":\"214\",\"4\":\"-2.80637\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n`augment` produces a data frame (of the original data frame with some\nnew columns that come from the regression), so I can feed it into a pipe\nto do things with it, like only displaying the columns I want, and\narranging them in order by absolute value of residual, so that the\nroller-coasters further from the line come out first. This identifies\nthe three that we found above. The fourth one, \"Ghost Rider\", is like\nNitro in that it takes a (relatively) long time to fall not very far.\nYou can also put `augment` in the *middle* of a pipe. What you may have\nto do then is supply the *original* data frame name to `augment` so that\nyou have everything:\n\n::: {.cell}\n\n```{.r .cell-code}\ncoasters %>%\n  lm(duration ~ drop, data = .) %>%\n  augment(coasters) %>%\n  arrange(desc(abs(.resid)))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"coaster_name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"state\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"drop\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"duration\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".fitted\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".resid\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".hat\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".sigma\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".cooksd\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".std.resid\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Nitro\",\"2\":\"New Jersey\",\"3\":\"215\",\"4\":\"240\",\"5\":\"143.0483\",\"6\":\"96.95170\",\"7\":\"0.1378057\",\"8\":\"37.54500\",\"9\":\"0.3355862058\",\"10\":\"2.04920817\"},{\"1\":\"The Beast\",\"2\":\"Ohio\",\"3\":\"141\",\"4\":\"65\",\"5\":\"125.1452\",\"6\":\"-60.14522\",\"7\":\"0.1183794\",\"8\":\"48.79432\",\"9\":\"0.1061087925\",\"10\":\"-1.25716851\"},{\"1\":\"Millennium Force\",\"2\":\"Ohio\",\"3\":\"300\",\"4\":\"105\",\"5\":\"163.6127\",\"6\":\"-58.61266\",\"7\":\"0.4289016\",\"8\":\"45.90966\",\"9\":\"0.8700715796\",\"10\":\"-1.52219103\"},{\"1\":\"Ghost Rider\",\"2\":\"California\",\"3\":\"108\",\"4\":\"160\",\"5\":\"117.1614\",\"6\":\"42.83859\",\"7\":\"0.1799397\",\"8\":\"51.45256\",\"9\":\"0.0945675641\",\"10\":\"0.92842216\"},{\"1\":\"Goliath\",\"2\":\"California\",\"3\":\"255\",\"4\":\"180\",\"5\":\"152.7256\",\"6\":\"27.27435\",\"7\":\"0.2389942\",\"8\":\"53.17339\",\"9\":\"0.0591230416\",\"10\":\"0.61361184\"},{\"1\":\"Thunderbolt\",\"2\":\"Pennsylvania\",\"3\":\"95\",\"4\":\"90\",\"5\":\"114.0163\",\"6\":\"-24.01628\",\"7\":\"0.2160836\",\"8\":\"53.49714\",\"9\":\"0.0390597997\",\"10\":\"-0.53235830\"},{\"1\":\"Raven\",\"2\":\"Indiana\",\"3\":\"86\",\"4\":\"90\",\"5\":\"111.8389\",\"6\":\"-21.83887\",\"7\":\"0.2450440\",\"8\":\"53.63586\",\"9\":\"0.0394909652\",\"10\":\"-0.49329040\"},{\"1\":\"Incredible Hulk\",\"2\":\"Florida\",\"3\":\"105\",\"4\":\"135\",\"5\":\"116.4356\",\"6\":\"18.56439\",\"7\":\"0.1876840\",\"8\":\"53.91145\",\"9\":\"0.0188788219\",\"10\":\"0.40425125\"},{\"1\":\"Magnum XL-2000\",\"2\":\"Ohio\",\"3\":\"195\",\"4\":\"120\",\"5\":\"138.2096\",\"6\":\"-18.20963\",\"7\":\"0.1110766\",\"8\":\"53.97930\",\"9\":\"0.0089770351\",\"10\":\"-0.37905502\"},{\"1\":\"Son of Beast\",\"2\":\"Ohio\",\"3\":\"214\",\"4\":\"140\",\"5\":\"142.8064\",\"6\":\"-2.80637\",\"7\":\"0.1360914\",\"8\":\"54.45872\",\"9\":\"0.0002765802\",\"10\":\"-0.05925762\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nI wanted to hang on to the roller-coaster names, so I added the data\nframe name to `augment`. If you don't (that is, you just put `augment()`\nin the middle of a pipe), then `augment` \"attempts to reconstruct the\ndata from the model\".[^_coasters-3] That means you wouldn't get\n*everything* from the original data frame; you would just get the things\nthat were in the regression. In this case, that means you would lose the\ncoaster names.\n\n[^_coasters-3]: A quote from the package vignette.\n\nA technicality (but one that you should probably care about): `augment`\ntakes up to *two* inputs: a fitted model object like my `duration.1`,\nand an optional data frame to include other things from, like the\ncoaster names. I had only one input to it in the pipe because the\nimplied first input was the output from the `lm`, which doesn't have a\nname; the input `coasters` in the pipe was what would normally be the\n*second* input to `augment`.\n\n$\\blacksquare$\n\n\n##  Running and blood sugar\n\n\n A diabetic wants to know how aerobic exercise affects his\nblood sugar. When his blood sugar reaches 170 (mg/dl), he goes out for\na run at a pace of 10 minutes per mile. He runs different distances on\ndifferent days. Each time he runs, he measures his blood sugar after\nthe run. (The preferred blood sugar level is between 80 and 120 on\nthis scale.) The data are in the file\n[link](http://ritsokiguess.site/datafiles/runner.txt).  Our aim is\nto predict blood sugar from distance.\n\n\n\n(a) Read in the data and display the data frame that you read\nin.\n \nSolution\n\n\nFrom the URL is easiest. These are delimited by one space, as\nyou can tell by looking at the file:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/runner.txt\"\nruns <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 12 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): distance, blood_sugar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nruns\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"distance\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"blood_sugar\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2.0\",\"2\":\"136\"},{\"1\":\"2.0\",\"2\":\"146\"},{\"1\":\"2.5\",\"2\":\"131\"},{\"1\":\"2.5\",\"2\":\"125\"},{\"1\":\"3.0\",\"2\":\"120\"},{\"1\":\"3.0\",\"2\":\"116\"},{\"1\":\"3.5\",\"2\":\"104\"},{\"1\":\"3.5\",\"2\":\"95\"},{\"1\":\"4.0\",\"2\":\"85\"},{\"1\":\"4.0\",\"2\":\"94\"},{\"1\":\"4.5\",\"2\":\"83\"},{\"1\":\"4.5\",\"2\":\"75\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n     \n\nThat looks like my data file.\n \n$\\blacksquare$\n\n(b) Make a scatterplot and add a smooth trend to it.\n \nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(runs, aes(x = distance, y = blood_sugar)) + geom_point() +\n  geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/plymouth-1.png){width=672}\n:::\n:::\n\n     \n\n`blood_sugar` should be on the vertical axis, since this is\nwhat we are trying to predict. Getting the `x` and the\n`y` right is easy on these, because they are the $x$ and $y$\nfor your plot.\n \n$\\blacksquare$\n\n(c) Would you say that the relationship between blood sugar and\nrunning distance is approximately linear, or not? It is therefore\nreasonable to use a regression of blood sugar on distance? Explain briefly.\n \nSolution\n\n\nI'd say that this is about as linear as you could ever wish\nfor. Neither the pattern of points nor the smooth trend have any\nkind of noticeable bend in them. (Observing a lack of curvature in\neither the points or the smooth trend is enough.) The trend\nis a linear one, so using a regression will be just fine. (If it\nweren't, the rest of the question would be kind of dumb.)\n \n$\\blacksquare$\n\n(d) Fit a suitable regression, and obtain the regression output.\n \nSolution\n\n\nTwo steps: `lm` and then `summary`:\n\n::: {.cell}\n\n```{.r .cell-code}\nruns.1 <- lm(blood_sugar ~ distance, data = runs)\nsummary(runs.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = blood_sugar ~ distance, data = runs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8238 -3.6167  0.8333  4.0190  5.5476 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  191.624      5.439   35.23 8.05e-12 ***\ndistance     -25.371      1.618  -15.68 2.29e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.788 on 10 degrees of freedom\nMultiple R-squared:  0.9609,\tAdjusted R-squared:  0.957 \nF-statistic: 245.7 on 1 and 10 DF,  p-value: 2.287e-08\n```\n:::\n:::\n\n     \n \n$\\blacksquare$\n\n(e) How would you *interpret* the slope? That is, what is\nthe slope, and what does that mean about blood sugar and running distance?\n \nSolution\n\n\nThe slope is $-25.37$. This means that for each additional mile run,\nthe runner's blood sugar will decrease on average by about 25 units.\n\nYou can check this from the scatterplot. For example, from 2 to 3\nmiles, average blood sugar decreases from about 140 to about 115, a\ndrop of 25.\n \n$\\blacksquare$\n\n(f) Is there a (statistically) significant relationship between\nrunning distance and blood sugar? How do you know? Do you find this\nsurprising, given what you have seen so far? Explain briefly.\n \nSolution\n\n\nLook at the P-value either on the `distance` line (for its\n$t$-test) or for the $F$-statistic on the bottom line. These are\nthe same: 0.000000023. (They will be the same any time there is\none $x$-variable.) This P-value is *way* smaller than 0.05,\nso there *is* a significant relationship between running distance\nand blood sugar. This does not surprise me in the slightest,\nbecause the trend on the scatterplot is *so* clear, there's\nno way it could have happened by chance if in fact there were no\nrelationship between running distance and blood sugar.\n \n$\\blacksquare$\n\n(g) This diabetic is planning to go for a 3-mile run tomorrow\nand a 5-mile run the day after. Obtain suitable 95\\% intervals that\nsay what his blood sugar might be after each of these runs. \n \nSolution\n\n\nThis is a prediction interval, in each case, since we are talking about\n*individual* runs of 3 miles and 5 miles (not the mean blood\nsugar after *all* runs of 3 miles, which is what a confidence\ninterval for the mean response would be).\nThe procedure is to set up a data frame with the two\n`distance` values in it, and then feed that and the\nregression object into `predict`, coming up in a moment.\n\n::: {.cell}\n\n```{.r .cell-code}\ndists <- c(3, 5)\nnew <- tibble(distance = dists)\nnew\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"distance\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3\"},{\"1\":\"5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n     \nThe important thing is that the name of the column of the new data\nframe must be *exactly* the same as the name of the explanatory\nvariable in the regression. If they don't match, `predict`\nwon't work. At least, it won't work properly.^[It won't give you an error, but it will go back to the *original* data frame to get distances to predict from, and you will get very confused. This is another example of (base) R trying to make life easier for you, but when it fails, it *fails*.]\n\nIf your first thought is `datagrid`, well, that will also work:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew2 <- datagrid(model = runs.1, distance = c(5, 10))\nnew2\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"blood_sugar\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"distance\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"109.1667\",\"2\":\"5\"},{\"1\":\"109.1667\",\"2\":\"10\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nUse whichever of these methods comes to your mind.\n\nThen, `predict`, because you want prediction intervals rather than confidence intervals for the mean response (which is what `marginaleffects` gives you):\n\n::: {.cell}\n\n```{.r .cell-code}\npp <- predict(runs.1, new, interval = \"p\")\npp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        fit       lwr       upr\n1 115.50952 104.37000 126.64905\n2  64.76667  51.99545  77.53788\n```\n:::\n:::\n\n \n\nand display this with the distances by the side:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(new, pp)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"distance\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fit\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lwr\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3\",\"2\":\"115.50952\",\"3\":\"104.37000\",\"4\":\"126.64905\",\"_rn_\":\"1\"},{\"1\":\"5\",\"2\":\"64.76667\",\"3\":\"51.99545\",\"4\":\"77.53788\",\"_rn_\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(new, pp)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"distance\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fit\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lwr\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3\",\"2\":\"115.50952\",\"3\":\"104.37000\",\"4\":\"126.64905\",\"_rn_\":\"1\"},{\"1\":\"5\",\"2\":\"64.76667\",\"3\":\"51.99545\",\"4\":\"77.53788\",\"_rn_\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \nBlood sugar after a 3-mile run is predicted to be between 104 and 127;\nafter a 5-mile run it is predicted to be between 52 and 77.5.\n\nExtra: both `cbind` and `data.frame` are \"base R\" ways of\ncombining a data frame with something else to make a new data\nframe. They are not from the `tidyverse`. The\n`tidyverse` way is via `tibble` or `bind_cols`,\nbut they are a bit more particular about what they will take:\n`tibble` takes vectors (single variables) and\n`bind_cols` takes vectors or data frames. The problem here is\nthat `pp` is not either of those:\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(pp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n \n\nso that we have to use `as_tibble` first to turn it into a\ndata frame, and thus:\n\n::: {.cell}\n\n```{.r .cell-code}\npp %>% as_tibble() %>% bind_cols(new)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"fit\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lwr\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upr\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"distance\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"115.50952\",\"2\":\"104.37000\",\"3\":\"126.64905\",\"4\":\"3\"},{\"1\":\"64.76667\",\"2\":\"51.99545\",\"3\":\"77.53788\",\"4\":\"5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nwhich puts things backwards, unless you do it like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew %>% bind_cols(as_tibble(pp))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"distance\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fit\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lwr\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3\",\"2\":\"115.50952\",\"3\":\"104.37000\",\"4\":\"126.64905\"},{\"1\":\"5\",\"2\":\"64.76667\",\"3\":\"51.99545\",\"4\":\"77.53788\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nwhich is a pretty result from very ugly code. \n\nI also remembered that if you finish with a `select`, you get the columns in the order they were in the `select`:\n\n::: {.cell}\n\n```{.r .cell-code}\npp %>%\n  as_tibble() %>%\n  bind_cols(new) %>%\n  select(c(distance, everything()))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"distance\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fit\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lwr\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3\",\"2\":\"115.50952\",\"3\":\"104.37000\",\"4\":\"126.64905\"},{\"1\":\"5\",\"2\":\"64.76667\",\"3\":\"51.99545\",\"4\":\"77.53788\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\n`everything` is a so-called \"select helper\". It means \n\"everything except any columns you already named\", so this whole thing has the effect of listing the columns \nwith `distance` first and all the other columns afterwards, in the order that they were in before.\n \n$\\blacksquare$\n\n(h) Which of your two intervals is longer? Does this make\nsense? Explain briefly.\n \nSolution\n\n\nThe intervals are about 22.25 and 25.5 units long. The one for a\n5-mile run is a bit longer. I think this makes sense because 3\nmiles is close to the average run distance, so there is a lot of\n\"nearby\" data. 5 miles is actually longer than any of the runs\nthat were actually done (and therefore we are actually\nextrapolating), but the important point for the prediction\ninterval is that there is less nearby data: those 2-mile runs\ndon't help so much in predicting blood sugar after a 5-mile\nrun. (They help *some*, because the trend is so linear. This\nis why the 5-mile interval is not *so* much longer. If the\ntrend were less clear, the 5-mile interval would be more\nnoticeably worse.)\n\n$\\blacksquare$\n\n\n\n\n\n##  Calories and fat in pizza\n\n\n \nThe file at\n[link](http://ritsokiguess.site/datafiles/Pizza.csv) \ncame from a spreadsheet of information about 24 brands\nof pizza: specifically, per 5-ounce serving, the number of calories,\nthe grams of fat, and the cost (in US dollars). The names of the pizza\nbrands are quite long. This file may open in a spreadsheet when you\nbrowse to the link, depending on your computer's setup.\n\n\n\n(a) Read in the data and display at least some of the data\nframe. Are the variables of the right types? (In particular, why is\nthe number of calories labelled one way and the cost labelled a\ndifferent way?)\n\nSolution\n\n\n`read_csv` is the thing this time:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/Pizza.csv\"\npizza <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 24 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Type\ndbl (3): Calories, Fat, Cost\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\npizza\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Type\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Calories\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Fat\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Cost\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Domino's Deep Dish with Pepperoni\",\"2\":\"385\",\"3\":\"19.5\",\"4\":\"1.87\"},{\"1\":\"Pizza Hut's Stuffed Crust with Pepperoni\",\"2\":\"370\",\"3\":\"15.0\",\"4\":\"1.83\"},{\"1\":\"Pizza Hut's Pan Pizza with Pepperoni\",\"2\":\"280\",\"3\":\"14.0\",\"4\":\"1.83\"},{\"1\":\"Domino's Hand-Tossed with Pepperoni\",\"2\":\"305\",\"3\":\"12.0\",\"4\":\"1.67\"},{\"1\":\"Pizza Hut's Hand-Tossed with Pepperoni\",\"2\":\"230\",\"3\":\"9.0\",\"4\":\"1.63\"},{\"1\":\"Little Caesars' Deep Dish with Pepperoni\",\"2\":\"350\",\"3\":\"14.2\",\"4\":\"1.06\"},{\"1\":\"Little Caesars' Original Round with Pepperoni\",\"2\":\"230\",\"3\":\"8.0\",\"4\":\"0.81\"},{\"1\":\"Freschetta Bakes & Rises  4-Cheese\",\"2\":\"364\",\"3\":\"15.0\",\"4\":\"0.98\"},{\"1\":\"Freschetta Bakes & Rises Sauce Stuffed Crust 4-Cheese\",\"2\":\"334\",\"3\":\"11.0\",\"4\":\"1.23\"},{\"1\":\"DiGiorno Rising Crust Four Cheese\",\"2\":\"332\",\"3\":\"12.0\",\"4\":\"0.94\"},{\"1\":\"Amy's Organic Crust & Tomatoes Cheese\",\"2\":\"341\",\"3\":\"14.0\",\"4\":\"1.92\"},{\"1\":\"Safeway Select Verdi Quattro Formaggio Self Rising Crust\",\"2\":\"307\",\"3\":\"9.0\",\"4\":\"0.84\"},{\"1\":\"Tony's Super Rise Crust Four-Cheese\",\"2\":\"335\",\"3\":\"12.0\",\"4\":\"0.96\"},{\"1\":\"Kroger Self Rising Crust Four Cheese\",\"2\":\"292\",\"3\":\"9.0\",\"4\":\"0.80\"},{\"1\":\"Tombstone Stuffed Crust Cheese\",\"2\":\"364\",\"3\":\"18.0\",\"4\":\"0.96\"},{\"1\":\"Red Baron Classic 4 Cheese\",\"2\":\"384\",\"3\":\"20.0\",\"4\":\"0.91\"},{\"1\":\"Boboli Original\",\"2\":\"333\",\"3\":\"12.0\",\"4\":\"0.89\"},{\"1\":\"Tombstone Original Extra Cheese\",\"2\":\"328\",\"3\":\"14.0\",\"4\":\"0.94\"},{\"1\":\"Reggio's Chicago Style Cheese\",\"2\":\"367\",\"3\":\"13.0\",\"4\":\"1.02\"},{\"1\":\"Jack's Original Cheese\",\"2\":\"325\",\"3\":\"13.0\",\"4\":\"0.92\"},{\"1\":\"Celeste Pizza for One Cheese\",\"2\":\"346\",\"3\":\"17.0\",\"4\":\"1.17\"},{\"1\":\"McCain Ellio's Cheese\",\"2\":\"299\",\"3\":\"9.0\",\"4\":\"0.54\"},{\"1\":\"Michelina's Zap 'ems That'za Pizza!\",\"2\":\"394\",\"3\":\"19.0\",\"4\":\"1.28\"},{\"1\":\"Totino's The Original Crisp Crust Party Cheese\",\"2\":\"322\",\"3\":\"14.0\",\"4\":\"0.67\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n    \n\nThe four variables are: the brand of pizza, which got read in as text,\nthe number of calories (an integer), and the fat and cost, which are\nboth decimal numbers so they get labelled `dbl`, which is short for \n\"double-precision floating point number\". \n\nAnyway, these are apparently the right thing. \n\nExtra: I wanted to mention something else that I discovered\nyesterday.^[R is like that: sometimes it seems as if it has  infinite depth.] \nThere is a package called `rio` that will\nread (and write) data in a whole bunch of different formats in a\nunified way.^[It does this by figuring what kind of thing you have, from the extension to its filename, and then calling an appropriate function to read in or write out the data. This is an excellent example of \"standing on the shoulders of giants\" to make our lives easier. The software does the hard work of figuring out what kind of thing you have and how to read it in; all we do is say `import`.] Anyway, the usual installation thing, done once:\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"rio\")\n```\n:::\n\n \n\nwhich takes a moment since it probably has to install some other\npackages too, and then you read in a file like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rio)\npizza3 <- import(my_url)\nhead(pizza3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Type\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Calories\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Fat\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Cost\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Domino's Deep Dish with Pepperoni\",\"2\":\"385\",\"3\":\"19.5\",\"4\":\"1.87\",\"_rn_\":\"1\"},{\"1\":\"Pizza Hut's Stuffed Crust with Pepperoni\",\"2\":\"370\",\"3\":\"15.0\",\"4\":\"1.83\",\"_rn_\":\"2\"},{\"1\":\"Pizza Hut's Pan Pizza with Pepperoni\",\"2\":\"280\",\"3\":\"14.0\",\"4\":\"1.83\",\"_rn_\":\"3\"},{\"1\":\"Domino's Hand-Tossed with Pepperoni\",\"2\":\"305\",\"3\":\"12.0\",\"4\":\"1.67\",\"_rn_\":\"4\"},{\"1\":\"Pizza Hut's Hand-Tossed with Pepperoni\",\"2\":\"230\",\"3\":\"9.0\",\"4\":\"1.63\",\"_rn_\":\"5\"},{\"1\":\"Little Caesars' Deep Dish with Pepperoni\",\"2\":\"350\",\"3\":\"14.2\",\"4\":\"1.06\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\n`import` figures that you have a `.csv` file, so it\ncalls up `read_csv` or similar.\n\nTechnical note: `rio` does not use the `read_`\nfunctions, so what it gives you is actually a `data.frame`\nrather than a `tibble`, so that when you display it, you get\nthe whole thing even if it is long. Hence the `head` here and\nbelow to display the first six lines.\n\nI originally had the data as an Excel spreadsheet, but `import`\nwill gobble up that pizza too:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_other_url <- \"http://ritsokiguess.site/datafiles/Pizza_E29.xls\"\npizza4 <- import(my_other_url)\nhead(pizza4)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Type\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Calories\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Fat (g)\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Cost ($)\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Domino's Deep Dish with Pepperoni\",\"2\":\"385\",\"3\":\"19.5\",\"4\":\"1.87\",\"_rn_\":\"1\"},{\"1\":\"Pizza Hut's Stuffed Crust with Pepperoni\",\"2\":\"370\",\"3\":\"15.0\",\"4\":\"1.83\",\"_rn_\":\"2\"},{\"1\":\"Pizza Hut's Pan Pizza with Pepperoni\",\"2\":\"280\",\"3\":\"14.0\",\"4\":\"1.83\",\"_rn_\":\"3\"},{\"1\":\"Domino's Hand-Tossed with Pepperoni\",\"2\":\"305\",\"3\":\"12.0\",\"4\":\"1.67\",\"_rn_\":\"4\"},{\"1\":\"Pizza Hut's Hand-Tossed with Pepperoni\",\"2\":\"230\",\"3\":\"9.0\",\"4\":\"1.63\",\"_rn_\":\"5\"},{\"1\":\"Little Caesars' Deep Dish with Pepperoni\",\"2\":\"350\",\"3\":\"14.2\",\"4\":\"1.06\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nThe corresponding function for writing a data frame to a file in the\nright format is, predictably enough, called `export`.\n\n$\\blacksquare$\n\n(b) Make a scatterplot for predicting calories from the number\nof grams of fat. Add a smooth trend. What kind of relationship do\nyou see, if any?\n\nSolution\n\n\nAll the variable names start with Capital Letters:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pizza, aes(x = Fat, y = Calories)) + geom_point() +\n  geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/alskhslafkhlksfhsasvvvv-1.png){width=672}\n:::\n:::\n\n       \n\nThere is definitely an upward trend: the more fat, the more\ncalories. The trend is more or less linear (or, a little bit curved:\nsay what you like, as long as it's not obviously crazy). *I*\nthink, with this much scatter, there's no real justification for\nfitting a curve.\n \n$\\blacksquare$\n\n(c) Fit a straight-line relationship, and display the intercept,\nslope, R-squared, etc. Is there a real relationship between the two\nvariables, or is any apparent trend just chance?\n\nSolution\n\n\n`lm`, with `summary`:\n\n::: {.cell}\n\n```{.r .cell-code}\npizza.1 <- lm(Calories ~ Fat, data = pizza)\nsummary(pizza.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Calories ~ Fat, data = pizza)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-55.44 -11.67   6.18  17.87  41.61 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  194.747     21.605   9.014 7.71e-09 ***\nFat           10.050      1.558   6.449 1.73e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.79 on 22 degrees of freedom\nMultiple R-squared:  0.654,\tAdjusted R-squared:  0.6383 \nF-statistic: 41.59 on 1 and 22 DF,  p-value: 1.731e-06\n```\n:::\n:::\n\n       \n\nTo assess whether this trend is real or just chance, look at the\nP-value on the end of the `Fat` line, or on the bottom line\nwhere the $F$-statistic is (they are the same value of $1.73\\times\n10^{-6}$ or 0.0000017, so you can pick either). This P-value is\nreally small, so the slope is definitely *not* zero, and\ntherefore there really is a relationship between the two variables.\n \n$\\blacksquare$\n\n(d) Obtain a plot of the residuals against the fitted values\nfor this regression. Does this indicate that there are any problems\nwith this regression, or not? Explain briefly.\n\nSolution\n\n\nUse the regression object `pizza.1`:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pizza.1, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/pizza-6-1.png){width=672}\n:::\n:::\n\n \n\nOn my residual plot, I see a slight curve in the smooth trend,\nbut I am not worried about that because the residuals on the plot are\nall over the place in a seemingly random pattern (the grey envelope is\nwide and that is pretty close to going straight across). So I think a\nstraight line model is satisfactory. \n\nThat's all you needed, but it is also worth looking at a normal\nquantile plot of the residuals:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pizza.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/pizza-7-1.png){width=672}\n:::\n:::\n\n \n\nA bit skewed to the left (the low ones are too low).\n\nAlso a plot of the absolute residuals, for assessing fan-out:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pizza.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/pizza-8-1.png){width=672}\n:::\n:::\n\n \n\nA tiny bit of fan-in (residuals getting *smaller* in size as the\nfitted value gets bigger), but nothing much, I think.\n\nAnother way of assessing curvedness is to fit a squared term anyway,\nand see whether it is significant:\n\n::: {.cell}\n\n```{.r .cell-code}\npizza.2 <- update(pizza.1, . ~ . + I(Fat^2))\nsummary(pizza.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Calories ~ Fat + I(Fat^2), data = pizza)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.103 -14.280   5.513  15.423  35.474 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  90.2544    77.8156   1.160   0.2591  \nFat          25.9717    11.5121   2.256   0.0349 *\nI(Fat^2)     -0.5702     0.4086  -1.395   0.1775  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.25 on 21 degrees of freedom\nMultiple R-squared:  0.6834,\tAdjusted R-squared:  0.6532 \nF-statistic: 22.66 on 2 and 21 DF,  p-value: 5.698e-06\n```\n:::\n:::\n\n \n\nThe fat-squared term is not significant, so that curve on the smooth trend\nin the (first) residual plot was indeed nothing to get excited about.\n \n$\\blacksquare$\n\n(e) The research assistant in this study returns with two\nnew brands of pizza (ones that were not in the original data). The\nfat content of a 5-ounce serving was 12 grams for the first brand\nand 20 grams for the second brand. For each of these brands of\npizza, obtain a suitable 95\\% interval for the number of calories\ncontained in a 5-ounce serving.\n\nSolution\n\n\nThe suitable interval here is a prediction interval, because we\nare interested in each case in the calorie content of the\n*particular* pizza brands that the research assistant\nreturned with (and not, for example, in the mean calorie content\nfor *all* brands of pizza that have 12 grams of fat per\nserving). Thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nnewfat <- c(12, 20)\nnew <- tibble(Fat = newfat)\nnew\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Fat\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"12\"},{\"1\":\"20\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\npreds <- predict(pizza.1, new, interval = \"p\")\ncbind(new, preds)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Fat\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fit\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lwr\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"12\",\"2\":\"315.3447\",\"3\":\"260.5524\",\"4\":\"370.1369\",\"_rn_\":\"1\"},{\"1\":\"20\",\"2\":\"395.7431\",\"3\":\"337.1850\",\"4\":\"454.3011\",\"_rn_\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nUse `datagrid` to make `new` if you like, but it is a very simple dataframe, so there is no obligation to do it that way.\n\nOr, if you like:\n\n::: {.cell}\n\n```{.r .cell-code}\nas_tibble(preds) %>% bind_cols(new) %>% select(Fat, everything())\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Fat\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fit\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lwr\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"12\",\"2\":\"315.3447\",\"3\":\"260.5524\",\"4\":\"370.1369\"},{\"1\":\"20\",\"2\":\"395.7431\",\"3\":\"337.1850\",\"4\":\"454.3011\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nFor the pizza with 12 grams of fat, the predicted calories are between\n261 and 370 with 95\\% confidence, and for the pizza with 20 grams of\nfat, the calories are predicted to be between 337 and 454. (You should\nwrite down what these intervals are, and not leave the reader to find\nthem in the output.)\n\n(Remember the steps: create a new data frame containing the values to\npredict for, and then feed that into `predict` along with the\nmodel that you want to use to predict with. The variable in the data\nframe has to be called *precisely* `Fat` with a capital F,\notherwise it won't work.)\n\nThese intervals are both pretty awful: you get a very weak picture of\nhow many calories per serving the pizza brands in question might\ncontain. This is for two reasons: (i) there was a fair bit of scatter\nin the original relationship, R-squared being around 65\\%, and (ii)\neven if we knew perfectly where the line went (which we don't),\nthere's no guarantee that individual brands of pizza would be on it\nanyway. (Prediction intervals are always hit by this double whammy, in\nthat individual observations suffer from variability in where the line\ngoes *and* variability around whatever the line is.)\n\nI was expecting, when I put together this question, that the\n20-grams-of-fat interval would  be noticeably worse, because 20 is\nfarther away from the mean fat content of all the brands. But there\nisn't much to choose. For the confidence intervals for the mean\ncalories of *all* brands with these fat contents, the picture is clearer:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_cap(pizza.1, condition = \"Fat\")\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\nA `fat` value of 12 is close to the middle of the data, so the interval is shorter, but a value of 20 is out near the extreme and the interval is noticeably longer.\n\n\nThis part was a fair bit of work for 3 points, so I'm not insisting that you explain\nyour choice of a prediction interval over a confidence interval, but I\nthink it is still a smart thing to do, even purely from a marks point\nof view, because if you get it wrong for a semi-plausible reason, you\nmight pick up some partial credit. Not pulling out your prediction\nintervals from your output is a sure way to lose a point, however.\n \n$\\blacksquare$\n\n\n\n\n\n##  Where should the fire stations be?\n\n\nIn city planning, one major issue is where to locate fire\nstations. If a city has too many fire stations, it will spend too much\non running them, but if it has too few, there may be unnecessary fire\ndamage because the fire trucks take too long to get to the fire.\n\nThe first part of a study of this kind of issue is to understand the\nrelationship between the distance from the fire station (measured in\nmiles in our data set) and the amount of fire damage caused (measured\nin thousands of dollars). A city recorded the fire damage and distance\nfrom fire station for 15 residential fires (which you can take as a\nsample of \"all possible residential fires in that city\"). The data\nare in [link](http://ritsokiguess.site/datafiles/fire_damage.txt). \n\n\n\n(a) Read in and display the data, verifying that you have the\nright number of rows and the right columns.\n\n\nSolution\n\n\nA quick check of the data reveals that the data values are\nseparated by exactly  one space, so:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/fire_damage.txt\"\nfire <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 15 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): distance, damage\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nfire\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"distance\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"damage\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3.4\",\"2\":\"26.2\"},{\"1\":\"1.8\",\"2\":\"17.8\"},{\"1\":\"4.6\",\"2\":\"31.3\"},{\"1\":\"2.3\",\"2\":\"23.1\"},{\"1\":\"3.1\",\"2\":\"27.5\"},{\"1\":\"5.5\",\"2\":\"36.0\"},{\"1\":\"0.7\",\"2\":\"14.1\"},{\"1\":\"3.0\",\"2\":\"22.3\"},{\"1\":\"2.6\",\"2\":\"19.6\"},{\"1\":\"4.3\",\"2\":\"31.3\"},{\"1\":\"2.1\",\"2\":\"24.0\"},{\"1\":\"1.1\",\"2\":\"17.3\"},{\"1\":\"6.1\",\"2\":\"43.2\"},{\"1\":\"4.8\",\"2\":\"36.4\"},{\"1\":\"3.8\",\"2\":\"26.1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n   \n\n15 observations (rows), and promised, and a column each of distances\nand amounts of fire damage, also as promised.\n\n$\\blacksquare$\n\n\n(b) <a name=\"part:ttest\">*</a> Obtain a 95\\% confidence interval for the\nmean fire damage. (There is nothing here from STAD29, and your\nanswer should have nothing to do with distance.)\n\n\nSolution\n\n\nI wanted to dissuade you  from thinking too hard here. It's just\nan ordinary one-sample $t$-test, extracting the interval from it:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(fire$damage)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  fire$damage\nt = 12.678, df = 14, p-value = 4.605e-09\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.94488 30.88178\nsample estimates:\nmean of x \n 26.41333 \n```\n:::\n:::\n\n     \n\nOr\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(fire, t.test(damage))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  damage\nt = 12.678, df = 14, p-value = 4.605e-09\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.94488 30.88178\nsample estimates:\nmean of x \n 26.41333 \n```\n:::\n:::\n\n \n\nIgnore the P-value (it's testing that the mean is the default\n*zero*, which makes no sense). The confidence interval either way\ngoes from 21.9 to 30.9 (thousand dollars).\n    \n$\\blacksquare$\n\n(c) Draw a scatterplot for predicting the amount of fire damage\nfrom the distance from the fire station. Add a smooth trend to your\nplot. \n\n\nSolution\n\n\nWe are predicting fire damage, so that goes on the $y$-axis:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(fire, aes(x = distance, y = damage)) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/riemer-1.png){width=672}\n:::\n:::\n\n     \n\n$\\blacksquare$\n\n(d) <a name=\"part:howgood\">*</a> Is there a relationship between distance from fire station\nand fire damage? Is it linear or definitely curved? How strong is\nit? Explain briefly.\n\n\nSolution\n\n\nWhen the distance is larger, the fire damage is definitely larger,\nso there is clearly a relationship. I would call this one\napproximately linear: it wiggles a bit, but it is not to my mind\nobviously curved. I would also call it a strong relationship,\nsince the points are close to the smooth trend.\n    \n$\\blacksquare$\n\n(e) Fit a regression predicting fire damage from distance. How\nis the R-squared consistent (or inconsistent) with your answer from\npart~(<a href=\"#part:howgood\">here</a>)?\n\n\nSolution\n\n\nThe regression is an ordinary `lm`:\n\n::: {.cell}\n\n```{.r .cell-code}\ndamage.1 <- lm(damage ~ distance, data = fire)\nsummary(damage.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = damage ~ distance, data = fire)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4682 -1.4705 -0.1311  1.7915  3.3915 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  10.2779     1.4203   7.237 6.59e-06 ***\ndistance      4.9193     0.3927  12.525 1.25e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.316 on 13 degrees of freedom\nMultiple R-squared:  0.9235,\tAdjusted R-squared:  0.9176 \nF-statistic: 156.9 on 1 and 13 DF,  p-value: 1.248e-08\n```\n:::\n:::\n\n     \n\nWe need to display the results, since we need to see the R-squared in\norder to say something about it.\n\nR-squared is about 92\\%, high, indicating a strong and linear\nrelationship. Back in part~(<a href=\"#part:howgood\">here</a>), I said that the\nrelationship is linear and strong, which is entirely consistent with\nsuch an R-squared. (If you said something different previously, say\nhow it does or doesn't square with this kind of R-squared value.)\n\nPoints: one for fitting the regression, one for displaying it, and two\n(at the grader's discretion) for saying what the R-squared is and how\nit's consistent (or not) with part~(<a href=\"#part:howgood\">here</a>).\n\nExtra: if you thought the trend was \"definitely curved\", you would\nfind that a parabola (or some other kind of curve) was definitely\nbetter than a straight line. Here's the parabola:\n\n::: {.cell}\n\n```{.r .cell-code}\ndamage.2 <- lm(damage ~ distance + I(distance^2), data = fire)\nsummary(damage.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = damage ~ distance + I(distance^2), data = fire)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8856 -1.6915 -0.0179  1.5490  3.6278 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    13.3395     2.5303   5.272 0.000197 ***\ndistance        2.6400     1.6302   1.619 0.131327    \nI(distance^2)   0.3376     0.2349   1.437 0.176215    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.227 on 12 degrees of freedom\nMultiple R-squared:  0.9347,\tAdjusted R-squared:  0.9238 \nF-statistic: 85.91 on 2 and 12 DF,  p-value: 7.742e-08\n```\n:::\n:::\n\n \n\nThere's no evidence here that a quadratic is better.\n\nOr you might even have thought from the wiggles that it was more like cubic:\n\n::: {.cell}\n\n```{.r .cell-code}\ndamage.3 <- update(damage.2, . ~ . + I(distance^3))\nsummary(damage.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = damage ~ distance + I(distance^2) + I(distance^3), \n    data = fire)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2325 -1.8377  0.0322  1.1512  3.1806 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    10.8466     4.3618   2.487   0.0302 *\ndistance        5.9555     4.9610   1.200   0.2552  \nI(distance^2)  -0.8141     1.6409  -0.496   0.6296  \nI(distance^3)   0.1141     0.1608   0.709   0.4928  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.274 on 11 degrees of freedom\nMultiple R-squared:  0.9376,\tAdjusted R-squared:  0.9205 \nF-statistic: 55.07 on 3 and 11 DF,  p-value: 6.507e-07\n```\n:::\n:::\n\n\n\nNo evidence that a cubic is better; that increase in R-squared up to\nabout 94\\% is just chance (bearing in mind that adding *any* $x$,\neven a useless one, will increase R-squared).\n\nHow bendy is the cubic?\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(fire, aes(x = distance, y = damage)) + geom_point() +\n  geom_smooth(method = \"lm\") +\n  geom_line(data = damage.3, aes(y = .fitted), colour = \"red\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/ganter-1.png){width=672}\n:::\n:::\n\n \n\nThe cubic, in red, does bend a little, but it doesn't do an obvious\njob of going through the points better than the straight line does. It\nseems to be mostly swayed by that one observation with damage over 40,\nand choosing a relationship by how well it fits one point is flimsy at\nthe best of times.  So, by Occam's Razor, we go with the line rather\nthan the cubic because it (i) fits equally well, (ii) is simpler.\n    \n$\\blacksquare$\n\n(f) <a name=\"part:cim\">*</a> Obtain a 95% confidence interval for the mean fire damage\n*for a residence that is 4 miles from the nearest fire station*. \n(Note the contrast with part~(<a href=\"#part:ttest\">here</a>).)\n\n\nSolution\n\n\nThis is a confidence interval for a mean response at a given value\nof the explanatory variable. This is as opposed to\npart~(<a href=\"#part:ttest\">here</a>), which is averaged over *all* distances.\nSo, follow the steps. Make a tiny data frame with this one value\nof `distance`:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = damage.1, distance = 4)\nnew\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"damage\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"distance\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"26.41333\",\"2\":\"4\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nand then\n\n::: {.cell}\n\n```{.r .cell-code}\npp <- cbind(predictions(damage.1, newdata = new))\npp\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"rowid\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"damage\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"distance\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"29.95525\",\"3\":\"0.6615595\",\"4\":\"45.27976\",\"5\":\"0\",\"6\":\"28.65862\",\"7\":\"31.25188\",\"8\":\"26.41333\",\"9\":\"4\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n28.5 to 31.4 (thousand dollars).\n\n(I saved this one because I want to refer to it again later.)\n\n$\\blacksquare$\n\n(g) Compare the confidence intervals of parts\n(<a href=\"#part:ttest\">here</a>) and (<a href=\"#part:cim\">here</a>). Specifically, compare their\ncentres and their lengths, and explain briefly why the results\nmake sense.\n\n\nSolution\n\n\nLet me just put them side by side for ease of comparison:\npart~(<a href=\"#part:ttest\">here</a>) is:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(fire$damage)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  fire$damage\nt = 12.678, df = 14, p-value = 4.605e-09\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.94488 30.88178\nsample estimates:\nmean of x \n 26.41333 \n```\n:::\n:::\n\n       \n\nand part~(<a href=\"#part:cim\">here</a>)'s is\n\n::: {.cell}\n\n```{.r .cell-code}\npp\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"rowid\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"damage\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"distance\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"29.95525\",\"3\":\"0.6615595\",\"4\":\"45.27976\",\"5\":\"0\",\"6\":\"28.65862\",\"7\":\"31.25188\",\"8\":\"26.41333\",\"9\":\"4\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThe centre of the interval is higher for the mean damage when the\ndistance is 4. This is because the mean distance is a bit less than 4:\n\n::: {.cell}\n\n```{.r .cell-code}\nfire %>% summarize(m = mean(distance))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"m\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3.28\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nWe know it's an upward trend, so our best guess at the mean damage is\nhigher if the mean distance is higher (in (<a href=\"#part:cim\">here</a>), the\ndistance is *always* 4: we're looking at the mean fire damage for\n*all* residences that are 4 miles from a fire station.)\n\nWhat about the lengths of the intervals? The one in (<a href=\"#part:ttest\">here</a>)\nis about $30.9-21.9=9$ (thousand dollars) long, but the one in\n(<a href=\"#part:cim\">here</a>) is only $31.4-28.5=2.9$ long, much shorter. This\nmakes sense because the relationship is a strong one: knowing the\ndistance from the fire station is very useful, because the bigger it\nis, the bigger the damage going to be, with near certainty. Said\ndifferently, if you know the distance, you can estimate the damage\naccurately.  If you don't know the distance (as is the case in\n(<a href=\"#part:ttest\">here</a>)), you're averaging over a lot of different\ndistances and thus there is a lot of uncertainty in the amount of fire\ndamage also.\n\nIf you have some reasonable discussion of the reason why the centres\nand lengths of the intervals differ, I'm happy. It doesn't have to be\nthe same as mine.\n      \n$\\blacksquare$\n\n\n\n\n\n\n\n\n\n## Making it stop\n\n If you are driving, and you hit the brakes, how far do you travel before coming to a complete stop? \nPresumably this depends on how fast you are going. \nKnowing this relationship is important in setting speed limits on roads. For example, on a very bendy road, the speed limit needs to be low, because you cannot see very far ahead, and there could be something just\nout of sight that you need to stop for. \n\nData were collected for a typical car and driver, as shown in [http://ritsokiguess.site/datafiles/stopping.csv](http://ritsokiguess.site/datafiles/stopping.csv). These are American data, so the speeds are miles per hour and the stopping distances are in feet.\n\n\n\n(a) Read in and display (probably all of) the data.\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/stopping.csv\"\nstopping <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 8 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): speed, distance\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nstopping\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"speed\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"distance\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0\",\"2\":\"0\"},{\"1\":\"10\",\"2\":\"20\"},{\"1\":\"20\",\"2\":\"50\"},{\"1\":\"30\",\"2\":\"95\"},{\"1\":\"40\",\"2\":\"150\"},{\"1\":\"50\",\"2\":\"220\"},{\"1\":\"60\",\"2\":\"300\"},{\"1\":\"70\",\"2\":\"400\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\nThere are only eight observations.\n\n\n$\\blacksquare$\n\n\n(b) Make a suitable plot of the data.\n\nSolution\n\n\nTwo quantitative variables means a scatterplot. Stopping distance is the outcome, so that goes on the $y$-axis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(stopping, aes(x=speed, y=distance)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/braking-2-1.png){width=672}\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(c) Describe any trend you see in your graph.\n\nSolution\n\n\nIt's an upward trend, but not linear: the stopping distance seems to increase faster at higher speeds.\n\n\n\n$\\blacksquare$\n\n\n(d) Fit a linear regression predicting stopping distance from speed. (You might have some misgivings about doing this, but do it anyway.)\n\nSolution\n\n\nHaving observed  a curved relationship, it seems odd to fit a straight line. But we are going to do it anyway and then critique what we have:\n\n::: {.cell}\n\n```{.r .cell-code}\nstopping.1 <- lm(distance~speed, data=stopping)\nsummary(stopping.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = distance ~ speed, data = stopping)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.738 -22.351  -7.738  16.622  47.083 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -44.1667    22.0821   -2.00   0.0924 .  \nspeed         5.6726     0.5279   10.75 3.84e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 34.21 on 6 degrees of freedom\nMultiple R-squared:  0.9506,\tAdjusted R-squared:  0.9424 \nF-statistic: 115.5 on 1 and 6 DF,  p-value: 3.837e-05\n```\n:::\n:::\n\nExtra: note that R-squared is actually really high. We come back to that later.\n\n\n$\\blacksquare$\n\n\n(e) Plot the residuals against the fitted values for this regression.\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(stopping.1, aes(x=.fitted, y=.resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/braking-4-1.png){width=672}\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(f) What do you learn from the residual plot? Does that surprise you? Explain briefly. \n\nSolution\n\n\nThe points on the residual plot form a (perfect) curve, so the original relationship was a curve. This is exactly what we saw on the scatterplot, so to me at least, this is no surprise.\n\n(Make sure you say *how you know* that the original relationship was a curve from looking at the residual plot. Joined-up thinking. \nThere are *two* ways we know that the relationship is a curve. Get them both.)\n\n\n$\\blacksquare$\n\n\n(g) What is the actual relationship between stopping distance and speed, according to the physics? See if you can find out. Cite any books or websites that you use: that is, include a link to a website, or give enough information about a book that the grader could find it.\n\nSolution\n\n\nI searched for \"braking distance and speed\" and found the first two things below, that seemed to be useful. Later, I was thinking about the fourth point (which came out of my head) and while searching for other things about that, I found the third thing:\n\n- a [British road safety website](https://www.brake.org.uk/info-and-resources/facts-advice-research/road-safety-facts/15-facts-a-resources/facts/1255-speed), that says \"The braking distance depends on how fast the vehicle was travelling before the brakes were applied, and is proportional to the square of the initial speed.\"\n- the [Wikipedia article on braking distance](https://en.wikipedia.org/wiki/Braking_distance), which gives the actual formula. This is the velocity squared, divided by a constant that depends on the coefficient of friction. (That is why your driving instructor tells you to leave a bigger gap behind the vehicle in front if it is raining, and an even bigger gap if it is icy.)\n- an [Australian math booklet](http://www.amsi.org.au/teacher_modules/pdfs/Maths_delivers/Braking5.pdf) that talks specifically about braking distance and derives the formula (and the other equations of motion).\n- also, if you have done physics, you might remember the equation of motion $v^2 = u^2 + 2as$, where $u$ is the initial velocity, $v$ is the final velocity, $a$ is the acceleration and $s$ is the distance covered. In this case, $v=0$ (the car is stationary at the end), and so $-u^2/2a = s$. The acceleration is negative (the car is slowing down), so the left side is, despite appearances, positive. There seems to be a standard assumption that deceleration due to braking is constant (the same for all speeds), at least if you are trying to stop a car in a hurry.\n\nThese are all saying that we should add a speed-squared term to our regression, and then we will have the relationship exactly right, according to the physics.\n\nExtra: Another way  to measure how far you are behind the vehicle in front is time.\nMany of the British \"motorways\" (think 400-series highways) were built when I was young, and I remember [a TV commercial](https://www.youtube.com/watch?v=mf5d2DP4Pp0) that said \"Only a Fool Breaks the Two Second Rule\".^[This is perhaps not a commercial so much as a public safety message.] In those days (the linked one is from the 1970s),^[There are some typical British cars of the era in the commercial.] a lot of British drivers were not used to going that fast, or on roads that straight, so this was a way to know how big a gap to leave, so that you had time to take evasive action if needed. The value of the two-second rule is that it works for any speed, and you don't have to remember a bunch of stopping distances. (When I did my (Canadian) driving theory test, I think I worked out and learned a formula for the stopping distances that I could calculate in my head. I didn't have to get very close since the test was multiple-choice.)\n\n\n\n$\\blacksquare$\n\n\n(h) Fit the relationship that your research indicated (in the previous part) and display the results. Comment briefly on the R-squared value.\n\nSolution\n\n\nAdd a squared term in `speed`:\n\n::: {.cell}\n\n```{.r .cell-code}\nstopping.2 <- lm(distance~speed+I(speed^2), data=stopping)\nsummary(stopping.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = distance ~ speed + I(speed^2), data = stopping)\n\nResiduals:\n       1        2        3        4        5        6        7        8 \n-1.04167  0.98214  0.08929  1.27976 -0.44643 -0.08929 -2.64881  1.87500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1.041667   1.429997   0.728    0.499    \nspeed       1.151786   0.095433  12.069 6.89e-05 ***\nI(speed^2)  0.064583   0.001311  49.267 6.51e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.699 on 5 degrees of freedom\nMultiple R-squared:  0.9999,\tAdjusted R-squared:  0.9999 \nF-statistic: 2.462e+04 on 2 and 5 DF,  p-value: 1.039e-10\n```\n:::\n:::\n\nThe R-squared now is basically 1, so that the model fits very close to perfectly.\n\nExtra: you probably found in your research that the distance should be just something times speed squared, with no constant or linear term. Here, though, we have a significant linear term as well.\nThat is probably just chance, since the distances in the data look as if they have been rounded off. With more accurate values, I think the linear term would have been closer to zero.\n\nIf you want to go literally for the something-times-speed-squared, you can do that. This doesn't quite work:\n\n::: {.cell}\n\n```{.r .cell-code}\nstopping.3x <- lm(distance~I(speed^2), data=stopping)\nsummary(stopping.3x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = distance ~ I(speed^2), data = stopping)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7327  -3.4670   0.6761   6.2323   8.4513 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.732704   4.362859   3.377   0.0149 *  \nI(speed^2)   0.079796   0.001805  44.218 8.96e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.514 on 6 degrees of freedom\nMultiple R-squared:  0.9969,\tAdjusted R-squared:  0.9964 \nF-statistic:  1955 on 1 and 6 DF,  p-value: 8.958e-09\n```\n:::\n:::\nbecause it still has an intercept in it. In R, the intercept is denoted by `1`. It is always included, unless you explicitly remove it. Some odd things start to happen if you remove the intercept, so it is not a good thing to do unless you know what you are doing. The answers [here](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model) have some good discussion. Having decided that you *are* going to remove the intercept,\nyou can remove it the same way as anything else (see `update` in the multiple regression lecture) with \"minus\". I haven't shown you this, so if you do it, you will need to cite your source: that is, say where you learned what to do:\n\n::: {.cell}\n\n```{.r .cell-code}\nstopping.3 <- lm(distance~I(speed^2)-1, data=stopping)\nsummary(stopping.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = distance ~ I(speed^2) - 1, data = stopping)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.6123  -0.7859  10.5314  15.5314  19.2141 \n\nCoefficients:\n           Estimate Std. Error t value Pr(>|t|)    \nI(speed^2) 0.084207   0.001963   42.89 9.77e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.42 on 7 degrees of freedom\nMultiple R-squared:  0.9962,\tAdjusted R-squared:  0.9957 \nF-statistic:  1840 on 1 and 7 DF,  p-value: 9.772e-10\n```\n:::\n:::\n\nThe R-squared is still extremely high, much higher than for the straight line. The coefficient value, as I said earlier (citing Wikipedia), depends on the coefficient of friction; the stopping distances you see typically are based on a dry road, so you have to allow extra distance (or time: see above) if the road is not dry.\n\n$\\blacksquare$\n\n\n\n\n(i) Somebody says to you \"if you have a regression with a high R-squared, like 95%, there is no need to look for a better model.\" How do you respond to this? Explain briefly.\n\nSolution\n\n\nAn example of a regression with an R-squared of 95% is the straight-line fit from earlier in this question. This is an example of a regression that fits well but is not appropriate because it doesn't capture the form of the relationship. \n\nIn general, we are saying that no matter how high R-squared is, we might still be able to improve on the model we have. The flip side is that we might not be able to do any better (with another data set) than an R-squared of, say, 30%, because there is a lot of variability that is, as best as we can assess it, random and not explainable by anything.\n\nUsing R-squared as a measure of absolute model quality is, thus, a mistake. Or, to say it perhaps more clearly, asking \"how high does R-squared have to be to indicate a good fit?\" is asking the wrong question.\nThe right thing to do is to concentrate on getting the form of the model right, and thereby get the R-squared as high as we can for that data set (which might be very high, as here, or not high at all).\n\n\n$\\blacksquare$\n\n\n\n\n\n\n\n\n## Predicting height from foot length\n\n Is it possible to estimate the height of a person from the length of their foot? To find out, 33 (male) students had their height and foot length measured. The data are in \n[http://ritsokiguess.site/datafiles/heightfoot.csv](http://ritsokiguess.site/datafiles/heightfoot.csv).\n\n\n\n(a) Read in and display (some of) the data. (If you are having trouble, make sure you have *exactly* the right URL. The correct URL has no spaces or other strange characters in it.)\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/heightfoot.csv\"\nhf <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 33 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): height, foot\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nhf\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"height\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"foot\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"66.5\",\"2\":\"27.0\"},{\"1\":\"73.5\",\"2\":\"29.0\"},{\"1\":\"70.0\",\"2\":\"25.5\"},{\"1\":\"71.0\",\"2\":\"27.9\"},{\"1\":\"73.0\",\"2\":\"27.0\"},{\"1\":\"71.0\",\"2\":\"26.0\"},{\"1\":\"71.0\",\"2\":\"29.0\"},{\"1\":\"69.5\",\"2\":\"27.0\"},{\"1\":\"73.0\",\"2\":\"29.0\"},{\"1\":\"71.0\",\"2\":\"27.0\"},{\"1\":\"69.0\",\"2\":\"29.0\"},{\"1\":\"69.0\",\"2\":\"27.2\"},{\"1\":\"73.0\",\"2\":\"29.0\"},{\"1\":\"75.0\",\"2\":\"29.0\"},{\"1\":\"73.0\",\"2\":\"27.2\"},{\"1\":\"72.0\",\"2\":\"27.5\"},{\"1\":\"69.0\",\"2\":\"25.0\"},{\"1\":\"68.0\",\"2\":\"25.0\"},{\"1\":\"72.5\",\"2\":\"28.0\"},{\"1\":\"78.0\",\"2\":\"31.5\"},{\"1\":\"79.0\",\"2\":\"30.0\"},{\"1\":\"71.0\",\"2\":\"28.0\"},{\"1\":\"74.0\",\"2\":\"29.0\"},{\"1\":\"66.0\",\"2\":\"25.5\"},{\"1\":\"71.0\",\"2\":\"26.7\"},{\"1\":\"71.0\",\"2\":\"29.0\"},{\"1\":\"71.0\",\"2\":\"28.0\"},{\"1\":\"84.0\",\"2\":\"27.0\"},{\"1\":\"77.0\",\"2\":\"29.0\"},{\"1\":\"72.0\",\"2\":\"28.0\"},{\"1\":\"70.0\",\"2\":\"26.0\"},{\"1\":\"76.0\",\"2\":\"30.0\"},{\"1\":\"68.0\",\"2\":\"27.0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nCall the data frame whatever you like, but keeping away from the names `height` and `foot` is probably wise, since those are the names of the columns.\n\nThere are indeed 33 rows as promised.\n\nExtra: my comment in the question was to help you if you copy-pasted the file URL into R Studio. Depending on your setup, this might have gotten pasted with a space in it, at the point where it is split over two lines. The *best* way to proceed, one that won't run into this problem, is to *right*-click on the URL and select Copy Link Address (or the equivalent on your system), and then it will put the whole URL on the clipboard in one piece, even if it is split over two lines in the original document, so that pasting it will work without problems.\n\n\n\n\n$\\blacksquare$\n\n\n(b) Make a suitable plot of the two variables in the data frame.\n\nSolution\n\n\nThey are both quantitative, so a scatter plot is called for:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(hf, aes(y=height, x=foot)) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-2-1.png){width=672}\n:::\n:::\n\nI added a smooth trend, or you could just plot the points. (This is better than plotting a regression line at this stage, because we haven't yet thought about whether the trend is straight.)\n\nNow that we've seen the scatterplot, the trend looks more or less straight (but you should take a look at the scatterplot first, with or without smooth trend, before you put a regression line on it). That point top left is  a concern, though, which brings us to...\n\n\n$\\blacksquare$\n\n\n(c) Are there any observations not on the trend of the other points? What is unusual about those observations? \n\nSolution\n\n\nThe observation with height greater than 80 at the top of the graph looks like an outlier and does not follow the trend of the rest of the points. Or, this individual is much taller than you would expect for someone with a foot length of 27 inches. Or, this person is over 7 feet tall, which makes little sense as a height. Say something about what makes this person be off the trend.\n\n\n$\\blacksquare$\n\n\n(d) Fit a regression predicting height from foot length, *including* any observations that you identified in the previous part. For that regression, plot the residuals against the fitted values and make a normal quantile plot of the residuals.\n\nSolution\n\n\nThese things. Displaying the `summary` of the regression is optional, but gives the grader an opportunity to check that your work is all right so far.\n\n::: {.cell}\n\n```{.r .cell-code}\nhf.1 <- lm(height~foot, data=hf)\nsummary(hf.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = height ~ foot, data = hf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7491 -1.3901 -0.0310  0.8918 12.9690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  34.3363     9.9541   3.449 0.001640 ** \nfoot          1.3591     0.3581   3.795 0.000643 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.102 on 31 degrees of freedom\nMultiple R-squared:  0.3173,\tAdjusted R-squared:  0.2952 \nF-statistic: 14.41 on 1 and 31 DF,  p-value: 0.0006428\n```\n:::\n\n```{.r .cell-code}\nggplot(hf.1, aes(x=.fitted, y=.resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(hf.1, aes(sample=.resid)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-3-2.png){width=672}\n:::\n:::\n\nNote that we did not exclude the off-trend point. Removing points *because they are outliers* is a **bad** idea. [This](https://statisticsbyjim.com/basics/remove-outliers/) is a good discussion of the issues.\n\n\n$\\blacksquare$\n\n\n\n(e) Earlier, you identified one or more observations that were off the trend. How does this point or points show up on each of the plots you drew in the previous part?\n\nSolution\n\n\nOn its own at the top in both cases; the large positive residual on the first plot, and the unusually large value at the top right of the normal quantile plot. (You need to say one thing about each graph, or say as I did that the same kind of thing happens on both graphs.)\n\nExtra: in the residuals vs. fitted values, the other residuals show a slight upward trend. This is because the regression line for these data, with the outlier, is pulled (slightly) closer to the outlier and thus slightly further away from the other points, particularly the ones on the left, compared to the same data but with the outlier removed (which you will be seeing shortly). If the unusual point had happened to have an extreme $x$ (foot length) as well, the effect would have been more pronounced.\n\nThis is the kind of thing I mean (made-up data):\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(x = 1:10) %>% \nmutate(y = rnorm(10, 10+2*x, 1)) %>% \nmutate(y = ifelse(x == 9, 40, y)) -> madeup\nmadeup\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"x\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"y\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"13.62187\"},{\"1\":\"2\",\"2\":\"13.25365\"},{\"1\":\"3\",\"2\":\"15.73107\"},{\"1\":\"4\",\"2\":\"17.30046\"},{\"1\":\"5\",\"2\":\"20.21324\"},{\"1\":\"6\",\"2\":\"22.70897\"},{\"1\":\"7\",\"2\":\"22.92167\"},{\"1\":\"8\",\"2\":\"26.79131\"},{\"1\":\"9\",\"2\":\"40.00000\"},{\"1\":\"10\",\"2\":\"31.09588\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(madeup, aes(x=x, y=y)) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-6-1.png){width=672}\n:::\n:::\n\nThe second-last point is off a clearly linear trend otherwise (the smooth gets \"distracted\" by the outlying off-trend point). Fitting a regression anyway and looking at the residual plot gives this:\n\n::: {.cell}\n\n```{.r .cell-code}\nmadeup.1 <- lm(y~x, data = madeup)\nggplot(madeup.1, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-7-1.png){width=672}\n:::\n:::\n\nThis time you see a rather more obvious downward trend in the other residuals. The problem is not with them, but with the one very positive residual, corresponding to the outlier that is way off the trend on the scatterplot.\n\nThe residuals in a regression have to add up to zero. If one of them is very positive (as in the one you did and the example I just showed you), at least some of the other residuals have to become more negative to compensate -- the ones on the right just above and the ones on the left in the one you did. If you have done STAC67, you will have some kind of sense of why that is: think about the two equations you have to solve to get the estimates of intercept and slope, and how they are related to the residuals. Slide 6 of [this](https://statweb.stanford.edu/~jtaylo/courses/stats203/notes/introduction.pdf) shows them; at the least squares estimates, these two partial derivatives both have to be zero, and the things inside the brackets are the residuals.\n\n\n$\\blacksquare$\n\n\n(f) Any data points that concerned you earlier were actually errors.\nCreate and save a new data frame that does not contain any of those data points. \n\nSolution\n\n\nFind a way to not pick that outlying point. For example, you can choose the observations with height less than 80:\n\n::: {.cell}\n\n```{.r .cell-code}\nhf %>% filter(height<80) -> hfx\nhfx\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"height\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"foot\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"66.5\",\"2\":\"27.0\"},{\"1\":\"73.5\",\"2\":\"29.0\"},{\"1\":\"70.0\",\"2\":\"25.5\"},{\"1\":\"71.0\",\"2\":\"27.9\"},{\"1\":\"73.0\",\"2\":\"27.0\"},{\"1\":\"71.0\",\"2\":\"26.0\"},{\"1\":\"71.0\",\"2\":\"29.0\"},{\"1\":\"69.5\",\"2\":\"27.0\"},{\"1\":\"73.0\",\"2\":\"29.0\"},{\"1\":\"71.0\",\"2\":\"27.0\"},{\"1\":\"69.0\",\"2\":\"29.0\"},{\"1\":\"69.0\",\"2\":\"27.2\"},{\"1\":\"73.0\",\"2\":\"29.0\"},{\"1\":\"75.0\",\"2\":\"29.0\"},{\"1\":\"73.0\",\"2\":\"27.2\"},{\"1\":\"72.0\",\"2\":\"27.5\"},{\"1\":\"69.0\",\"2\":\"25.0\"},{\"1\":\"68.0\",\"2\":\"25.0\"},{\"1\":\"72.5\",\"2\":\"28.0\"},{\"1\":\"78.0\",\"2\":\"31.5\"},{\"1\":\"79.0\",\"2\":\"30.0\"},{\"1\":\"71.0\",\"2\":\"28.0\"},{\"1\":\"74.0\",\"2\":\"29.0\"},{\"1\":\"66.0\",\"2\":\"25.5\"},{\"1\":\"71.0\",\"2\":\"26.7\"},{\"1\":\"71.0\",\"2\":\"29.0\"},{\"1\":\"71.0\",\"2\":\"28.0\"},{\"1\":\"77.0\",\"2\":\"29.0\"},{\"1\":\"72.0\",\"2\":\"28.0\"},{\"1\":\"70.0\",\"2\":\"26.0\"},{\"1\":\"76.0\",\"2\":\"30.0\"},{\"1\":\"68.0\",\"2\":\"27.0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nOnly 32 rows left.\n\nThere are many other possibilities. Find one.\n\n\n$\\blacksquare$\n\n\n(g) Run a regression predicting height from foot length for your data set without errors. Obtain a plot of the residuals against fitted values and a normal quantile plot of the residuals for this regression.\n\nSolution\n\n\nCode-wise, the same as before, but with the new data set:\n\n::: {.cell}\n\n```{.r .cell-code}\nhf.2 <- lm(height~foot, data=hfx)\nsummary(hf.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = height ~ foot, data = hfx)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5097 -1.0158  0.4757  1.1141  3.9951 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  30.1502     6.5411   4.609 7.00e-05 ***\nfoot          1.4952     0.2351   6.360 5.12e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.029 on 30 degrees of freedom\nMultiple R-squared:  0.5741,\tAdjusted R-squared:  0.5599 \nF-statistic: 40.45 on 1 and 30 DF,  p-value: 5.124e-07\n```\n:::\n\n```{.r .cell-code}\nggplot(hf.2, aes(x=.fitted, y=.resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(hf.2, aes(sample=.resid)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-9-2.png){width=672}\n:::\n:::\n\n\n$\\blacksquare$\n\n\n\n(h) Do you see any problems on the plots you drew in the previous part? Explain briefly. \n\nSolution\n\n\nFor myself, I see a random scatter of points on the first plot, and points close to the line on the second one. Thus I don't see any problems at all. I would declare myself happy with the second regression, after removing the outlier. (Remember that we removed the outlier *because it was an error*, not just because it was an outlier. Outliers can be perfectly correct data points, and if they are, they have to be included in the modelling.)\n\nYou might have a different point of view, in which case you need to make the case for it. You might see a (very mild) case of fanning out in the first plot, or the two most negative residuals might be a bit too low. These are not really outliers, though, not at least in comparison to what we had before. \n\nExtra: a standard diagnostic for fanning-out is to plot the *absolute values* of the residuals against the fitted values, with a smooth trend. If this looks like an increasing trend, there is fanning-out; a decreasing trend shows fanning-in. The idea is that we want to see whether the residuals are changing in *size* (for example, getting more positive *and* more negative both):\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(hf.2, aes(x=.fitted, y=abs(.resid))) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-10-1.png){width=672}\n:::\n:::\n\nNo evidence of fanning-out at all. In fact, the residuals seem to be smallest in size *in the middle*.\n\nAnother thing you might think of is to try Box-Cox:\n\n::: {.cell}\n\n```{.r .cell-code}\nboxcox(height~foot, data=hfx)\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-11-1.png){width=672}\n:::\n:::\n\nIt looks as if the best $\\lambda$ is $-1$, and we should predict one over height from foot length. But this plot is deceiving, since it doesn't even show the whole confidence interval for $\\lambda$!\nWe should zoom out (a lot) more:\n\n::: {.cell}\n\n```{.r .cell-code}\nboxcox(height~foot, data=hfx, lambda = seq(-8, 8, 0.1))\n```\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-12-1.png){width=672}\n:::\n:::\n\nThis shows that the confidence interval for $\\lambda$ goes from about $-7$ to almost 5: that is, *any* value of $\\lambda$ in that interval is supported by the data! This very definitely includes the do-nothing $\\lambda=1$, so there is really no support for any kind of transformation.\n\n\n$\\blacksquare$\n\n\n(i) Find a way to plot the data and *both* regression lines on the same plot, in such a way that you can see which regression line is which. If you get help from anything outside the course materials, cite your source(s).\n\nSolution\n\n\nThis is the same idea as with [the windmill data](http://ritsokiguess.site/STAC33/windmill_slides.pdf), page 22, though this one is a bit easier since everything is linear (no curves).\n\nThe easiest way is to use `geom_smooth` twice, once with the original data set, and then on the one with the outlier removed:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(hf, aes(y=height, x=foot)) + geom_point() + geom_smooth(method = \"lm\", se=F) +\ngeom_smooth(data=hfx, method=\"lm\", colour=\"red\", se=F)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-13-1.png){width=672}\n:::\n:::\n\nI would use the original data set as the \"base\", since we want to plot its points (including the outlier) as well as its line. \nThen we want to plot just the line for the second data set. This entails using a `data=` in the second `geom_smooth`, to say that we want to get *this* regression line from a different data set, and also\nentails drawing this line in a different colour (or in some way distinguishing it from the first one). Putting the `colour` *outside* an `aes` is a way to make the *whole line* red. (Compare how you make points different colours according to their value on a third variable.)\n\nThis is, I think, the best way to do it. You can mimic the idea that I used for the windmill data:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(hf, aes(y=height, x=foot)) + geom_point() + geom_smooth(method = \"lm\", se=F) +\ngeom_line(data=hf.2, aes(y = .fitted))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](simple-regression_files/figure-html/heightfoot-14-1.png){width=672}\n:::\n:::\n\nbut this is not as good, because you don't need to use the trickery with `geom_line`: the second trend is another regression line not a curve, and we know how to draw regression lines with `geom_smooth` without having to actually fit them. (Doing it this way reveals that you are copying without thinking, instead of wondering whether there is a better way to do it.)\n\nThe idea of using a different data set in different \"layers\" of a plot is quite well-known. For example, the idea is the one in [here](https://bookdown.org/yih_huynh/Guide-to-R-Book/graphing-with-different-datasets.html), though used for a different purpose there (plotting different sets of points instead of different lines).\n\n\n$\\blacksquare$\n\n\n(j) Discuss briefly how removing the observation(s) that were errors has changed where the regression line goes, and whether that is what you expected.\n\nSolution\n\n\nThe regression line for the original data (my blue line) is pulled up compared to the one with outliers removed (red).\n\nThis is not very surprising, because we know that regression lines tend to get pulled towards outliers. What was surprising to me was that the difference wasn't very big. Even at the low end, where the lines differ the most, the difference in predicted height is only about one inch. \nSince regression lines are based on means, I would have expected a big outlier to have moved the line a lot more.\n\nSay something about what you expected, and say something insightful about whether that was what you saw.\n\nExtra: the regression lines are very similar, but their R-squared values are not: 32% and 57% respectively. Having a point far from the line has a big (downward) impact on R-squared.\n\n\n$\\blacksquare$\n\n\n\n",
    "supporting": [
      "simple-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}