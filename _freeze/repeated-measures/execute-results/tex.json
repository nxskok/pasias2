{
  "hash": "e44bacd7d1a810e535ebb090368f163e",
  "result": {
<<<<<<< HEAD
    "engine": "knitr",
    "markdown": "# Repeated measures\n\nPackages for this chapter:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nlibrary(lme4)\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Effect of drug on rat weight\n\n\n Box (1950) gives data on the weights of three groups of\nrats. One group was given thyroxin in their drinking water, one group\nthiouracil, and the third group was a control. (This description comes\nfrom Christensen (2001).)^[References: Box, George EP, Problems in the analysis of growth and wear curves, Biometrics vol. 6, 362-369, 1950; Christensen R, Advanced Linear Modeling, 2nd edition, Springer, 2001. Yes, that's the Box-Cox Box.]\nWeights are measured in\ngrams at weekly intervals (over a 4-week period, so that each rat is\nmeasured 5 times). The data are in\n[link](http://ritsokiguess.site/datafiles/ratweight.csv) as a\n`.csv` file.\n\n\n\n(a) Read in the data and check that you have a column of\n`drug` and five columns of rat weights at different times.\n \n\n(b) Why would it be *wrong* to use something like\n`pivot_longer` to create one column of weights, and separate\ncolumns of drug and time, and then to run a two-way ANOVA? Explain briefly.\n \n\n(c) Create a suitable response variable and fit a suitable\n`lm` as the first step of the repeated-measures analysis.\n \n\n(d) Load the package `car` and run a suitable\n`Manova`. To do this, you will need to set up the right thing\nfor `idata` and `idesign`.\n \n\n(e) Take a look at the output from the MANOVA. Is there a\nsignificant interaction? What does its significance (or lack\nthereof) mean?\n \n\n(f) We are going to draw an interaction plot in a moment. To\nset that up, use `pivot_longer` as in the lecture notes to create\none column of weights and a second column of times. (You don't\nneed to do the `separate` thing that I did in class, though\nif you want to try it, go ahead.)\n \n\n(g) Obtain an interaction plot. Putting `time` as the\n`x` will put time along the horizontal axis, which is the\nway we're used to seeing such things. Begin by calculating the mean\n`weight` for each `time`-`drug` combination.\n \n\n(h) How does this plot show why the interaction was\nsignificant? Explain briefly.\n \n\n\n\n\n##  Social interaction among old people\n\n\n A graduate student wrote a thesis comparing different\ntreatments for increasing social interaction among geriatric\npatients. He recruited 21 patients at a state mental hospital and\nrandomly assigned them to treatments: Reality Orientation\n(`ro`), Behavior Therapy (`bt`) or no treatment\n(`ctrl`). Each subject was observed at four times, labelled\n`t1` through `t4` in the data file\n[link](http://ritsokiguess.site/datafiles/geriatrics.txt). The\nresponse variable was the percentage of time that the subject was\n\"engaging in the relevant social interaction\", so that a higher\nvalue is better.\n\nThe principal aim of the study was to see whether there were\ndifferences among the treatments (one would hope that the real\ntreatments were better than the control one), and whether there were\nany patterns over time.\n\n\n\n(a) Read in the data and display at least some of it.\n\n\n\n(b) Create a response variable and fit a suitable `lm`\nas the first stage of the repeated-measures analysis.\n\n\n\n(c) Run a suitable `Manova`. There is some setup\nfirst. Make sure you do that.\n\n\n\n(d) Display the results of your repeated-measures\nanalysis. What do you conclude? Explain briefly.\n\n\n\n(e) To understand the results that you got from the repeated\nmeasures analysis, you are going to draw a picture (or two). To do\n*that*, we are going to need the data in \"long\" format with\none response value per line (instead of four). Use `pivot_longer`\nsuitably to get the data in that format, and demonstrate that you\nhave done so.\n\n\n\n(f) Calculate and save the mean interaction percents for each\ntime-treatment combination.\n\n\n\n(g) Make an interaction plot. Arrange things so that time goes\nacross the page. Use your data frame of means that you just calculated.\n\n\n\n(h) Describe what you see on your interaction plot, and what it\nsays about why your repeated-measures analysis came out as it did.\n\n\n\n(i) Draw a spaghetti plot of these data. That is, use\n`ggplot` to plot the interaction percent against time for\neach subject,\njoining the points for the *same subject* by lines whose colour\nshows what treatment they were on. Use the \"long\" data frame for\nthis (not the data frame of means).\n\n\n\n\n\n\n##  Children's stress levels and airports\n\n\n If you did STAC32, you might remember this question, which\nwe can now do properly. Some of this question is a repeat from there.\n\nThe data in [link](http://ritsokiguess.site/datafiles/airport.txt)\nare based on a 1998 study of stress levels in children as a result of\nthe building of a new airport in Munich, Germany. A total of 200\nchildren had their epinephrine levels (a stress indicator) measured at\neach of four different times: before the airport was built, and 6, 18\nand 36 months after it was built. The four measurements are labelled\n`epi_1` through `epi_4`.  Out of the children, 100\nwere living near the new airport (location 1 in the data set), and\ncould be expected to suffer stress because of the new airport. The\nother 100 children lived in the same city, but outside the noise\nimpact zone. These children thus serve as a control group. The\nchildren are identified with numbers 1 through 200.\n\n\n\n(a) If we were testing for the effect of time, explain briefly\nwhat it is about the structure of the data that would make an\nanalysis of variance *inappropriate*.\n\n\n\n(b) Read the data into R and demonstrate that you have the right\nnumber of observations and variables.\n\n\n\n(c) Create and save a \"longer\" data frame with all the epinephrine\nvalues collected together into one column.\n\n\n\n(d) Make a \"spaghetti plot\" of these data: that is, a plot of\nepinephrine levels against time, with the locations identified by\ncolour, and the points for the same child joined by lines.  To do\nthis: (i) from the long data frame, create a new column containing\nonly the numeric values of time (1 through 4), (ii) plot epinephrine\nlevel against time with the points grouped by child and coloured by\nlocation (which you may have to turn from a number into a factor.)\n\n\n\n(e) What do you see on your spaghetti plot? We are looking ahead\nto possible effects of time, location and their interaction.\n\n\n\n(f) The spaghetti plot was hard to interpret because there are\nso many children. Calculate the mean epinephrine levels for each\nlocation-time combination, and make an interaction plot with time\non the $x$-axis and location as the second factor.\n\n\n\n(g) What do you conclude from your interaction plot? Is your\nconclusion clearer than from the spaghetti plot?\n\n\n\n(h) Run a repeated-measures analysis of variance and display the\nresults. Go back to your original data frame, the one you read in\nfrom the file, for this. You'll need to make sure your numeric\n`location` gets treated as a `factor`.\n\n\n\n(i) What do you conclude from the MANOVA? Is that consistent\nwith your graphs? Explain briefly.\n\n\n\n\n\n\n\n##  Body fat as repeated measures\n\n\n This one is also stolen from STAC32. Athletes are concerned\nwith measuring their body fat percentage. Two different methods are\navailable: one using ultrasound, and the other using X-ray\ntechnology. We are interested in whether there is a difference in the\nmean body fat percentage as measured by these two methods, and if so,\nhow big that difference is. Data on 16 athletes are at\n[link](http://ritsokiguess.site/datafiles/bodyfat.txt).\n\n\n\n(a) Read in the data and check that you have a sensible number of\nrows and columns.\n\n\n\n\n(b) Carry out a suitable (matched-pairs) $t$-test to determine\nwhether the means are the same or different. \n\n\n\n\n\n\n(c) What do you conclude from the test?\n\n\n\n\n(d) Run a repeated-measures analysis of variance, treating the two\nmethods of measuring body fat as the repeated measures (ie., playing\nthe role of \"time\" that you have seen in most of the other repeated\nmeasures analyses). There is no \"treatment\" here, so there is\nnothing to go on the right side of the squiggle. Insert a `1`\nthere to mean \"just an intercept\". Display the results.\n\n\n\n\n\n(e) Compare your repeated-measures analysis to your matched-pairs\none. Do you draw the same conclusions?\n\n\n\n\n\n\n\n\n\n\n##  Investigating motor activity in rats\n\n\n A researcher named King was investigating\nthe effect of the drug midazolam on motor activity in rats. Typically,\nthe first time the drug is injected, a rat's motor activity decreases\nsubstantially, but rats typically develop a \"tolerance\", so that\nfurther injections of the drug have less impact on the rat's motor\nactivity.\n\nThe data shown in\n[link](http://ritsokiguess.site/datafiles/king.csv) were all taken\nin one day, called the \"experiment day\" below. 24 different rats\nwere used. Each rat, on the experiment day, was injected with a fixed\namount of midazolam, and at each of six five-minute intervals after\nbeing injected, the rat's motor activity was measured (these are\nlabelled `i1` through `i6` in the data). The rats\ndiffered in how they had been treated before the experiment day. The\ncontrol group of rats had previously been injected repeatedly with a\nsaline solution (no active ingredient), so the experiment day was the\nfirst time this group of rats had received midazolam. The other two\ngroups of rats had both received midazolam repeatedly before the\nexperiment day: the \"same\" group was injected on experiment day in\nthe same environment that the previous injections had taken place (this\nis known in psychology as a \"conditioned tolerance\"), but the\n\"different\" group had the previous injections in a different\nenvironment than on experiment day.\n\nThe column `id` identifies the rat from which each sequence of\nvalues was obtained.\n\n\n\n(a) Explain briefly why we need to use a repeated measures\nanalysis for these data.\n\n\n\n(b) Read in the data and note that you have what was promised\nin the question.\n\n\n\n(c) We are going to do a repeated-measures analysis using the\n\"profile\" method shown in class. Create a suitable response\nvariable for this method.\n\n\n\n(d) Set up the \"within-subjects\" part of the analysis. That\nmeans getting hold of the names of the columns that hold the\ndifferent times, saving them, and also making a data frame out of them:\n\n\n\n(e) Fit the repeated-measures ANOVA. This will involve fitting\nan `lm` first, if you have not already done so.\n\n\n\n(f) What do you conclude from your repeated-measures ANOVA?\nExplain briefly, in the context of the data.\n\n\n\n(g) To understand the results of the previous part, we are going\nto make a spaghetti plot. In preparation for that, we need to save\nthe data in \"long format\" with one observation on *one* time\npoint in each row. Arrange that, and show by displaying (some of)\nthe data that you have done so.\n\n\n\n(h) Make a spaghetti plot: that is, plot motor activity against\nthe time points, joining the points for each *rat* by lines,\nand colouring the points and lines according to the *context*.\n\n\n\n(i) Looking at your spaghetti plot, why do you think your\nrepeated-measures ANOVA came out as it did? Explain briefly.\n\n\n\n\n\n\n\n\n\n##  Repeated measures with no background\n\n\n Nine people are randomly chosen to receive one of three\ntreatments, labelled A, B and C. Each person has their response\n`y` to the treatment measured at three times, labelled T1, T2\nand T3. The main aim of the study is to properly assess the effects of\nthe treatments. A higher value of `y` is better.\n\nThe data are in [link](http://ritsokiguess.site/datafiles/rm.txt).\n\n\n\n(a) There are $9 \\times 3=27$ observations  of `y` in\nthis study. Why would it be wrong to treat these as 27 independent\nobservations? Explain briefly.\n\n\n(b) Read in the data values. Are they tidy or untidy?  Explain\nbriefly. (The data values are separated by *tabs*, like the\nAustralian athlete data.)\n\n\n(c) Make a spaghetti plot: that is, a plot of `y`\nagainst time, with the observations for the same individual joined\nby lines which are coloured according to the treatment that\nindividual received.\n\n\n(d) On your spaghetti plot, how do the values of `y` for\nthe treatments compare over time?\n\n\n(e) Explain briefly how the data are in the wrong format for a\nrepeated-measures ANOVA (done using MANOVA, as in class), and use\n`pivot_wider` to get the data set into the right format.\n\n\n(f) Run a repeated-measures ANOVA the `Manova` way. What do you\nconclude from it?\n\n\n(g) How is your conclusion from the previous part consistent\nwith your spaghetti plot? Explain briefly.\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Effect of drug on rat weight\n\n\nBox (1950) gives data on the weights of three groups of\nrats.^[This is the same Box as Box-Cox and the M test.] One group was given thyroxin in their drinking water, one group\nthiouracil, and the third group was a control. (This description comes\nfrom Christensen (2001).)^[References: Box, George EP, Problems in the analysis of growth and wear curves, Biometrics vol. 6, 362-369, 1950; Christensen R, Advanced Linear Modeling, 2nd edition, Springer, 2001. Yes, that's the Box-Cox Box.]\nWeights are measured in\ngrams at weekly intervals (over a 4-week period, so that each rat is\nmeasured 5 times). The data are in\n[link](http://ritsokiguess.site/datafiles/ratweight.csv) as a\n`.csv` file.\n\n\n\n(a) Read in the data and check that you have a column of\n`drug` and five columns of rat weights at different times.\n \nSolution\n\n\nA `.csv` file, so `read_csv`. (I typed the data from\nChristensen (2001) into a spreadsheet.)\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/ratweight.csv\"\nweights <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 27 Columns: 7\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): drug\ndbl (6): rat, Time0, Time1, Time2, Time3, Time4\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nweights\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 27 x 7\n     rat drug       Time0 Time1 Time2 Time3 Time4\n   <dbl> <chr>      <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     1 thyroxin      59    85   121   156   191\n 2     2 thyroxin      54    71    90   110   138\n 3     3 thyroxin      56    75   108   151   189\n 4     4 thyroxin      59    85   116   148   177\n 5     5 thyroxin      57    72    97   120   144\n 6     6 thyroxin      52    73    97   116   140\n 7     7 thyroxin      52    70   105   138   171\n 8     8 thiouracil    61    86   109   120   129\n 9     9 thiouracil    59    80   101   111   122\n10    10 thiouracil    53    79   100   106   133\n# i 17 more rows\n```\n\n\n:::\n:::\n\n   \n\nThere are 27 rats altogether, each measured five times (labelled time\n0 through 4). The rats are also labelled `rat` (the first column), which will be useful later.\n \n$\\blacksquare$ \n\n(b) Why would it be *wrong* to use something like\n`pivot_longer` to create one column of weights, and separate\ncolumns of drug and time, and then to run a two-way ANOVA? Explain briefly.\n \nSolution\n\n\nSuch a solution would assume that we have measurements on\n*different* rats, one for each drug-time combination. But we\nhave sets of five measurements all on the *same* rat: that is\nto say, we have repeated measures, and the proper analysis will\ntake that into account.\n \n$\\blacksquare$\n\n(c) Create a suitable response variable and fit a suitable\n`lm` as the first step of the repeated-measures analysis.\n \nSolution\n\n\nThe response variable is the same idea as for any MANOVA: just\nglue the columns together:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(weights, cbind(Time0, Time1, Time2, Time3, Time4))\nweights.1 <- lm(response ~ drug, data = weights)\n```\n:::\n\n     \n::: {.cell}\n\n```{.r .cell-code}\nweights %>% select(starts_with(\"Time\")) %>% \n  as.matrix() -> y\n```\n:::\n\nNow, we *don't* look at `weights.1`, but we *do* use\nit as input to `Manova` in a moment.\n \n$\\blacksquare$\n\n(d) Load the package `car` and run a suitable\n`Manova`. To do this, you will need to set up the right thing\nfor `idata` and `idesign`.\n \nSolution\n\n\nSomething like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ntimes <- colnames(response)\ntimes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Time0\" \"Time1\" \"Time2\" \"Time3\" \"Time4\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntimes.df <- data.frame(times=factor(times))\ntimes.df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  times\n1 Time0\n2 Time1\n3 Time2\n4 Time3\n5 Time4\n```\n\n\n:::\n\n```{.r .cell-code}\nweights.2 <- Manova(weights.1, idata = times.df, idesign = ~times)\n```\n:::\n\n     \n\nThe thought process is that the columns of the response\n(`Time.0` through `Time.4`) are all times. This is the\n\"within-subject design\" part of it: within a rat, the different\nresponse values are at different times. That's the only part of it\nthat is within subjects. The different drugs are a\n\"between-subjects\" factor: each rat only gets one of the\ndrugs.^[Things would be a lot more complicated if each rat got a different drug at a different time! But the rats each got one drug *once*, at the beginning, and the issue was the effect of that drug on all the growth that followed.]\n \n$\\blacksquare$\n\n(e) Take a look at all the output from the MANOVA. Is there a\nsignificant interaction? What does its significance (or lack\nthereof) mean?\n \nSolution\n\nLook at the `summary`, which is rather long:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(weights.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nTime0           1\nTime1           1\nTime2           1\nTime3           1\nTime4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     6875579\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1   0.99257 3204.089      1     24 < 2.22e-16 ***\nWilks             1   0.00743 3204.089      1     24 < 2.22e-16 ***\nHotelling-Lawley  1 133.50372 3204.089      1     24 < 2.22e-16 ***\nRoy               1 133.50372 3204.089      1     24 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug \n\n Response transformation matrix:\n      (Intercept)\nTime0           1\nTime1           1\nTime2           1\nTime3           1\nTime4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    33193.27\n\nMultivariate Tests: drug\n                 Df test stat approx F num Df den Df    Pr(>F)   \nPillai            2 0.3919186 7.734199      2     24 0.0025559 **\nWilks             2 0.6080814 7.734199      2     24 0.0025559 **\nHotelling-Lawley  2 0.6445166 7.734199      2     24 0.0025559 **\nRoy               2 0.6445166 7.734199      2     24 0.0025559 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3 times4\nTime0      1      0      0      0\nTime1      0      1      0      0\nTime2      0      0      1      0\nTime3      0      0      0      1\nTime4     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1 times2    times3   times4\ntimes1 235200.00 178920 116106.67 62906.67\ntimes2 178920.00 136107  88324.00 47854.00\ntimes3 116106.67  88324  57316.15 31053.93\ntimes4  62906.67  47854  31053.93 16825.04\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1   0.98265 297.3643      4     21 < 2.22e-16 ***\nWilks             1   0.01735 297.3643      4     21 < 2.22e-16 ***\nHotelling-Lawley  1  56.64082 297.3643      4     21 < 2.22e-16 ***\nRoy               1  56.64082 297.3643      4     21 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug:times \n\n Response transformation matrix:\n      times1 times2 times3 times4\nTime0      1      0      0      0\nTime1      0      1      0      0\nTime2      0      0      1      0\nTime3      0      0      0      1\nTime4     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n         times1   times2   times3   times4\ntimes1 9192.071 8948.843 6864.676 3494.448\ntimes2 8948.843 8787.286 6740.286 3381.529\ntimes3 6864.676 6740.286 5170.138 2594.103\ntimes4 3494.448 3381.529 2594.103 1334.006\n\nMultivariate Tests: drug:times\n                 Df test stat  approx F num Df den Df     Pr(>F)    \nPillai            2 0.8779119  4.303151      8     44 0.00069308 ***\nWilks             2 0.2654858  4.939166      8     42 0.00023947 ***\nHotelling-Lawley  2 2.2265461  5.566365      8     40 9.3465e-05 ***\nRoy               2 1.9494810 10.722146      4     22 5.6277e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n             Sum Sq num Df Error SS den Df   F value    Pr(>F)    \n(Intercept) 1375116      1  10300.2     24 3204.0892 < 2.2e-16 ***\ndrug           6639      2  10300.2     24    7.7342  0.002556 ** \ntimes        146292      4   4940.7     96  710.6306 < 2.2e-16 ***\ndrug:times     6777      8   4940.7     96   16.4606 4.185e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n           Test statistic   p-value\ntimes           0.0072565 1.781e-19\ndrug:times      0.0072565 1.781e-19\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n            GG eps Pr(>F[GG])    \ntimes      0.33165  < 2.2e-16 ***\ndrug:times 0.33165  2.539e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n              HF eps   Pr(>F[HF])\ntimes      0.3436277 9.831975e-26\ndrug:times 0.3436277 1.757377e-06\n```\n\n\n:::\n:::\n\nStart near the bottom with Mauchly's test. This is strongly significant (for the interaction, which is our focus here) and means that sphericity fails and the P-value for the interaction in the univariate test is not to be trusted (it is much too small). Look instead at the Huynh-Feldt adjusted P-value at the very bottom, $1.76 \\times 10^{-6}$. This is strongly significant still, but it is a *billion* times bigger than the one in the univariate table! For comparison, the test for interaction in the multivariate analysis has a P-value of 0.0007 or less, depending on which of the four tests you look at (this time, they are not all the same). As usual, the multivariate tests have bigger P-values than the appropriately adjusted univariate tests, but the P-values are all pointing in the same direction.\n\nThe significant interaction means that the effect of time on growth is different for\nthe different drugs: that is, the effect of drug is over the whole\ntime profile, not just something like \n\"a rat on Thyroxin is on average 10 grams heavier than a control rat, over all times\".\n\nSince the interaction is significant, that's where we stop, as far as\ninterpretation is concerned.\n \n$\\blacksquare$\n\n(f) We are going to draw an interaction plot in a moment. To\nset that up, use `pivot_longer` as in the lecture notes to create\none column of weights and a second column of times. (You don't\nneed to do the `separate` thing that I did in class, though\nif you want to try it, go ahead.)\n \nSolution\n\nLike this:\n\n::: {.cell}\n\n```{.r .cell-code}\nweights %>% \n  pivot_longer(starts_with(\"Time\"), names_to=\"time\", values_to=\"weight\") -> weights.long\nweights.long\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 135 x 4\n     rat drug     time  weight\n   <dbl> <chr>    <chr>  <dbl>\n 1     1 thyroxin Time0     59\n 2     1 thyroxin Time1     85\n 3     1 thyroxin Time2    121\n 4     1 thyroxin Time3    156\n 5     1 thyroxin Time4    191\n 6     2 thyroxin Time0     54\n 7     2 thyroxin Time1     71\n 8     2 thyroxin Time2     90\n 9     2 thyroxin Time3    110\n10     2 thyroxin Time4    138\n# i 125 more rows\n```\n\n\n:::\n:::\n\n\n\n     \n\nMy data frame was called `weights`, so I was OK with having a\nvariable called `weight`. Watch out for that if you call the\ndata frame `weight`, though.\n\nSince the piece of the time we want is the number,\n`parse_number` (from `readr`, part of the\n`tidyverse`) should also work:\n\n::: {.cell}\n\n```{.r .cell-code}\nweights %>% \n  pivot_longer(starts_with(\"Time\"), names_to=\"timex\", values_to=\"weight\") %>% \n  mutate(time = parse_number(timex)) -> weights2.long\nweights2.long %>% sample_n(20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 x 5\n     rat drug       timex weight  time\n   <dbl> <chr>      <chr>  <dbl> <dbl>\n 1    15 thiouracil Time1     69     1\n 2    19 control    Time1     93     1\n 3     8 thiouracil Time3    120     3\n 4     3 thyroxin   Time0     56     0\n 5     9 thiouracil Time1     80     1\n 6    20 control    Time4    185     4\n 7    23 control    Time3    131     3\n 8    26 control    Time3    112     3\n 9    23 control    Time0     46     0\n10    22 control    Time3    121     3\n11    24 control    Time4    141     4\n12    17 thiouracil Time3    104     3\n13    21 control    Time2    100     2\n14     7 thyroxin   Time4    171     4\n15    16 thiouracil Time0     46     0\n16    21 control    Time0     49     0\n17    27 control    Time0     57     0\n18    20 control    Time3    144     3\n19    27 control    Time4    169     4\n20     9 thiouracil Time0     59     0\n```\n\n\n:::\n:::\n\n \n\nI decided to show you a random collection of rows, so that you can see\nthat `parse_number` worked for various different times. \n \n$\\blacksquare$\n\n(g) Obtain an interaction plot. Putting `time` as the\n`x` will put time along the horizontal axis, which is the\nway we're used to seeing such things. Begin by calculating the mean\n`weight` for each `time`-`drug` combination.\n \nSolution\n\n\n`group_by`, `summarize` and `ggplot`, the\nlatter using the data frame that came out of the\n`summarize`. The second factor `drug` goes as the\n`colour` and `group` both, since `time` has\ngrabbed the `x` spot:\n\n::: {.cell}\n\n```{.r .cell-code}\nweights.long %>%\n  group_by(time, drug) %>%\n  summarize(mean.weight = mean(weight)) %>%\n  ggplot(aes(x = time, y = mean.weight, colour = drug, group = drug)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'time'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/ratweight-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n       \n \n$\\blacksquare$\n\n(h) How does this plot show why the interaction was\nsignificant? Explain briefly.\n \nSolution\n\n\nAt the beginning, all the rats have the same average growth, but\nfrom time 2 (or maybe even 1) or so, the rats on thiouracil\ngrew more slowly. The idea is not just that thiouracil has a\n*constant* effect over all times, but that the *pattern*\nof growth is different for the different drugs: whether or not\nthiouracil inhibits growth, and, if so, by how much, depends on\nwhat time point you are looking at.\n\nRats on thyroxin or the control drug grew at pretty much the same\nrate over all times, so I wouldn't concern myself with any\ndifferences there.\n\nWhat I thought would be interesting is to plot the growth curves for\n*all* the rats individually, colour-coded by which drug the rat\nwas on. This is the repeated-measures version of the ANOVA interaction\nplot with the data on it, a so-called spaghetti plot. (We don't use the lines for the means, here,\ninstead using them for joining the measurements belonging to the same\nsubject.)\n\nWhen I first used this data set, it didn't have a column identifying which rat was which, which made this plot awkward, but now it does (the column `rat`). So we can start directly from the dataframe I created above called `weights.long`:\n\n::: {.cell}\n\n```{.r .cell-code}\nweights.long\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 135 x 4\n     rat drug     time  weight\n   <dbl> <chr>    <chr>  <dbl>\n 1     1 thyroxin Time0     59\n 2     1 thyroxin Time1     85\n 3     1 thyroxin Time2    121\n 4     1 thyroxin Time3    156\n 5     1 thyroxin Time4    191\n 6     2 thyroxin Time0     54\n 7     2 thyroxin Time1     71\n 8     2 thyroxin Time2     90\n 9     2 thyroxin Time3    110\n10     2 thyroxin Time4    138\n# i 125 more rows\n```\n\n\n:::\n:::\n\n  \nEach rat is identified by `rat``, which repeats 5 times,\nonce for each value of `time`:\n\n::: {.cell}\n\n```{.r .cell-code}\nweights.long %>% count(rat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 27 x 2\n     rat     n\n   <dbl> <int>\n 1     1     5\n 2     2     5\n 3     3     5\n 4     4     5\n 5     5     5\n 6     6     5\n 7     7     5\n 8     8     5\n 9     9     5\n10    10     5\n# i 17 more rows\n```\n\n\n:::\n:::\n\nIn the data frame `weights.long`, we plot\n`time` ($x$) against `weight` ($y$), grouping the points\naccording to `rat` and colouring them according to\n`drug`. \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(weights.long, aes(time, weight, group = rat, colour = drug)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/mantequilla-1.pdf){fig-pos='H'}\n:::\n:::\n\nAs you see, \"spaghetti plot\" is a rather apt name for this kind of thing.\n\nI like this plot because, unlike the interaction plot, which shows\nonly means, this gives a sense of variability as well. The blue and\nred lines (thyroxin and control) are all intermingled and they go\nstraight up. So there is nothing to choose between these. The green\nlines, though, start off mixed up with the red and blue ones but\nfinish up at the bottom: the *pattern* of growth of the\nthiouracil rats is different from the others, which is why we had a\nsignificant interaction between drug and time.\n\n`drug` is categorical, so `ggplot`\nuses a set of distinguishable colours to mark the levels. If our\ncolour had been a numerical variable, `ggplot` would have used\na range of colours like light blue to dark blue, with lighter being\nhigher, for example.\n\nWhat, you want to see that? All right. This one is kind of silly, but\nyou see the point:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weights.long, aes(time, weight, group = rat, colour = weight)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/alphington-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThe line segments get lighter as you go up the page.\n\nSince we went to the trouble of making the \"long\" data frame, we can also run a repeated measures analysis using the\nmixed-model idea (described more fully in the problem of the children\nnear the new airport):\n\n::: {.cell}\n\n```{.r .cell-code}\nwt.1 <- lmer(weight ~ drug * time + (1 | rat), data = weights.long)\ndrop1(wt.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nweight ~ drug * time + (1 | rat)\n          npar    AIC   LRT   Pr(Chi)    \n<none>          990.5                    \ndrug:time    8 1067.8 93.27 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nThe drug-by-time interaction is even more strongly significant than in\nthe profile analysis. (The output from `drop1` reminds us that\nthe only thing we should be thinking about now is that interaction.)\n \n$\\blacksquare$\n\n\n\n\n\n\n\n\n\n\n## Social interaction among old people\n\nA graduate student wrote a thesis comparing different treatments for\nincreasing social interaction among geriatric patients. He recruited 21\npatients at a state mental hospital and randomly assigned them to\ntreatments: Reality Orientation (`ro`), Behavior Therapy (`bt`) or no\ntreatment (`ctrl`). Each subject was observed at four times, labelled\n`t1` through `t4` in the data file\n[link](http://ritsokiguess.site/datafiles/geriatrics.txt). The response\nvariable was the percentage of time that the subject was \"engaging in\nthe relevant social interaction\", so that a higher value is better.\n\nThe principal aim of the study was to see whether there were differences\namong the treatments (one would hope that the real treatments were\nbetter than the control one), and whether there were any patterns over\ntime.\n\n(a) Read in the data and display at least some of it.\n\nSolution\n\nThe usual, separated by a single space:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/geriatrics.txt\"\ngeriatrics <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 21 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (1): treatment\ndbl (5): subject, t1, t2, t3, t4\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ngeriatrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 x 6\n   subject treatment    t1    t2    t3    t4\n     <dbl> <chr>     <dbl> <dbl> <dbl> <dbl>\n 1       1 bt          1.5   9     5     4  \n 2       2 bt          5    14     4.5   7  \n 3       3 bt          1     8     4.5   2.5\n 4       4 bt          5    14     8     5  \n 5       5 bt          3     8     4     4  \n 6       6 bt          0.5   3.5   1.3   1  \n 7       7 bt          0.5   3     1     0  \n 8       8 ro          2     5     5     1.5\n 9       9 ro          1.5   1.9   1.5   1  \n10      10 ro          3.5   7     8     4  \n# i 11 more rows\n```\n\n\n:::\n:::\n\nCorrectly 21 observations measured at 4 different times. We also have\nsubject numbers, which might be useful later.\n\n$\\blacksquare$\n\n(b) Create a response variable and fit a suitable `lm` as the first\n    stage of the repeated-measures analysis.\n\nSolution\n\nThis:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(geriatrics, cbind(t1, t2, t3, t4))\n#response\ngeriatrics.1 <- lm(response ~ treatment, data = geriatrics)\n```\n:::\n\nThere is no need to look at this, since we are going to feed it into\n`Manova` in a moment, but in case you're curious, you see (in `summary`)\na regression of each of the four columns in `response` on `treatment`,\none by one.\n\n$\\blacksquare$\n\n(c) Run a suitable `Manova`. There is some setup first. Make sure you do\n    that.\n\nSolution\n\nMake sure `car` is loaded, and do the `idata` and `idesign` thing:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       t1   t2  t3  t4\n [1,] 1.5  9.0 5.0 4.0\n [2,] 5.0 14.0 4.5 7.0\n [3,] 1.0  8.0 4.5 2.5\n [4,] 5.0 14.0 8.0 5.0\n [5,] 3.0  8.0 4.0 4.0\n [6,] 0.5  3.5 1.3 1.0\n [7,] 0.5  3.0 1.0 0.0\n [8,] 2.0  5.0 5.0 1.5\n [9,] 1.5  1.9 1.5 1.0\n[10,] 3.5  7.0 8.0 4.0\n[11,] 1.5  4.2 2.0 1.5\n[12,] 1.5  3.6 2.0 1.0\n[13,] 1.5  2.5 2.8 4.0\n[14,] 1.5  2.4 0.5 0.0\n[15,] 3.0  3.1 0.5 2.0\n[16,] 5.0  4.3 3.5 4.0\n[17,] 1.5  0.8 1.3 3.0\n[18,] 2.0  2.6 1.5 2.0\n[19,] 6.0  3.5 4.0 3.0\n[20,] 1.0  1.0 0.3 0.0\n[21,] 0.0  0.3 0.3 1.0\n```\n\n\n:::\n\n```{.r .cell-code}\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\ntimes.df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  times\n1    t1\n2    t2\n3    t3\n4    t4\n```\n\n\n:::\n\n```{.r .cell-code}\ngeriatrics.2 <- Manova(geriatrics.1, idata = times.df, idesign = ~times)\n```\n:::\n\nIn case you're curious, `response` is an R `matrix`:\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"matrix\" \"array\" \n```\n\n\n:::\n:::\n\nand not a data frame (because it was created by `cbind` which makes a\nmatrix out of vectors). So, to pull the names off the top, we really do\nneed `colnames` (applied to a matrix) rather than just `names` (which\napplies to a data frame).\n\n$\\blacksquare$\n\n(d) Display the results of your repeated-measures analysis. What do you\n    conclude? Explain briefly.\n\nSolution\n\nIts `summary` will get you what you want:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(geriatrics.2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in summary.Anova.mlm(geriatrics.2): HF eps > 1 treated as 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    3286.252\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.7458921 52.83606      1     18 9.3318e-07 ***\nWilks             1 0.2541079 52.83606      1     18 9.3318e-07 ***\nHotelling-Lawley  1 2.9353366 52.83606      1     18 9.3318e-07 ***\nRoy               1 2.9353366 52.83606      1     18 9.3318e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    360.6695\n\nMultivariate Tests: treatment\n                 Df test stat approx F num Df den Df   Pr(>F)  \nPillai            2 0.2436597 2.899406      2     18 0.080994 .\nWilks             2 0.7563403 2.899406      2     18 0.080994 .\nHotelling-Lawley  2 0.3221562 2.899406      2     18 0.080994 .\nRoy               2 0.3221562 2.899406      2     18 0.080994 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1     times2    times3\ntimes1  0.5833333  -8.366667 -1.666667\ntimes2 -8.3666667 120.001905 23.904762\ntimes3 -1.6666667  23.904762  4.761905\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df   Pr(>F)    \nPillai            1 0.7214276  13.8119      3     16 0.000105 ***\nWilks             1 0.2785724  13.8119      3     16 0.000105 ***\nHotelling-Lawley  1 2.5897315  13.8119      3     16 0.000105 ***\nRoy               1 2.5897315  13.8119      3     16 0.000105 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment:times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1    times2    times3\ntimes1   8.166667 -27.33333 -4.933333\ntimes2 -27.333333  91.61524 17.569524\ntimes3  -4.933333  17.56952 11.443810\n\nMultivariate Tests: treatment:times\n                 Df test stat  approx F num Df den Df     Pr(>F)    \nPillai            2 0.9258067  4.883886      6     34 0.00107288 ** \nWilks             2 0.2190296  6.062534      6     32 0.00025426 ***\nHotelling-Lawley  2 2.9043306  7.260827      6     30 7.4555e-05 ***\nRoy               2 2.6552949 15.046671      3     17 4.8948e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                Sum Sq num Df Error SS den Df F value    Pr(>F)    \n(Intercept)     821.56      1   279.89     18 52.8361 9.332e-07 ***\ntreatment        90.17      2   279.89     18  2.8994   0.08099 .  \ntimes            87.07      3    72.25     54 21.6933 2.378e-09 ***\ntreatment:times  90.77      6    72.25     54 11.3067 3.827e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                Test statistic p-value\ntimes                  0.85209 0.75008\ntreatment:times        0.85209 0.75008\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                 GG eps Pr(>F[GG])    \ntimes           0.90848  1.108e-08 ***\ntreatment:times 0.90848  1.434e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                  HF eps   Pr(>F[HF])\ntimes           1.086414 2.377839e-09\ntreatment:times 1.086414 3.826914e-08\n```\n\n\n:::\n:::\n\nAs is the way, start at the bottom and go up to Mauchly's test for\nsphericity. No problem here, so you can use the P-value for interaction\non the univariate test as is (\\$3.8 \\times 10\\^{-8}). By way of\ncomparison, the Huynh-Feldt adjusted P-value is exactly the same (not\nactually adjusted at all), which makes sense because there was no lack\nof sphericity. The multivariate tests for the interaction have P-values\nthat vary, but they are all (i) a bit bigger than the univariate one,\nand (ii) still significant.\n\nThus, the interaction is significant, so the effects of the treatments\nare different at different times. (It makes most sense to say it this\nway around, since treatment is something that was controlled and time\nwas not.)\n\nYou, I hope, know better than to look at the main effects when there is\na significant interaction!\n\n$\\blacksquare$\n\n(e) To understand the results that you got from the repeated measures\n    analysis, you are going to draw a picture (or two). To do *that*, we\n    are going to need the data in \"long\" format with one response value\n    per line (instead of four). Use `pivot_longer` suitably to get the\n    data in that format, and demonstrate that you have done so.\n\nSolution\n\nThe usual layout:\n\n::: {.cell}\n\n```{.r .cell-code}\ngeriatrics %>% \n  pivot_longer(t1:t4, names_to=\"time\", values_to = \"intpct\") -> geriatrics.long\ngeriatrics.long\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 84 x 4\n   subject treatment time  intpct\n     <dbl> <chr>     <chr>  <dbl>\n 1       1 bt        t1       1.5\n 2       1 bt        t2       9  \n 3       1 bt        t3       5  \n 4       1 bt        t4       4  \n 5       2 bt        t1       5  \n 6       2 bt        t2      14  \n 7       2 bt        t3       4.5\n 8       2 bt        t4       7  \n 9       3 bt        t1       1  \n10       3 bt        t2       8  \n# i 74 more rows\n```\n\n\n:::\n:::\n\nI have *one* column of interaction percents, and *one* column of times.\nIf you check the whole thing, you'll see that `pivot_longer` gives all\nthe measurements for subject 1, then subject 2, and so on.\n\nThe long data frame is, well, long.\n\nIt's not necessary to pull out the numeric time values, though you could\nif you wanted to, by using `parse_number`.\n\n$\\blacksquare$\n\n(f) Calculate and save the mean interaction percents for each\n    time-treatment combination.\n\nSolution\n\n`group_by` followed by `summarize`, as ever:\n\n::: {.cell}\n\n```{.r .cell-code}\ngeriatrics.long %>%\n  group_by(treatment, time) %>%\n  summarize(mean = mean(intpct)) -> means\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'treatment'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code}\nmeans\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 x 3\n# Groups:   treatment [3]\n   treatment time   mean\n   <chr>     <chr> <dbl>\n 1 bt        t1     2.36\n 2 bt        t2     8.5 \n 3 bt        t3     4.04\n 4 bt        t4     3.36\n 5 ctrl      t1     2.64\n 6 ctrl      t2     2.23\n 7 ctrl      t3     1.63\n 8 ctrl      t4     2.14\n 9 ro        t1     1.86\n10 ro        t2     3.8 \n11 ro        t3     3.11\n12 ro        t4     1.86\n```\n\n\n:::\n:::\n\n$\\blacksquare$\n\n(g) Make an interaction plot. Arrange things so that time goes across\n    the page. Use your data frame of means that you just calculated.\n\nSolution\n\nOnce you have the means, this is not too bad:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(means, aes(x = time, y = mean, group = treatment, colour = treatment)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/ger_int-1.pdf){fig-pos='H'}\n:::\n:::\n\nThe \"second factor\" `treatment` appears as both `group` and `colour`.\n\n$\\blacksquare$\n\n(h) Describe what you see on your interaction plot, and what it says\n    about why your repeated-measures analysis came out as it did.\n\nSolution\n\nThe two \"real\" treatments `bt` and `ro` both go up sharply between time\n1 and time 2, and then come back down so that by time 4 they are about\nwhere they started. The control group basically didn't change at all,\nand if anything went *down* between times 1 and 2, a completely\ndifferent pattern to the others. The two treatments `bt` and `ro` are\nnot exactly parallel, but they do at least have qualitatively the same\npattern.[^_geriatrics-1] It is, I think, the fact that the control group\nhas a *completely* different pattern over time that makes the\ninteraction come out significant.[^_geriatrics-2] I'm going to explore\nthat some more later, but first I want to get you to draw a spaghetti\nplot.\n\n[^_geriatrics-1]: That is to say, it's the same kind of shape.\n\n[^_geriatrics-2]: I am kind of wrong about that, as we see.\n\n$\\blacksquare$\n\n(i) Draw a spaghetti plot of these data. That is, use `ggplot` to plot\n    the interaction percent against time for each subject, joining the\n    points for the *same subject* by lines whose colour shows what\n    treatment they were on. Use the \"long\" data frame for this (not the\n    data frame of means).\n\nSolution\n\nThis is almost easier to do than it is to ask you to do:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(geriatrics.long, aes(x = time, y = intpct, colour = treatment, group = subject)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/ger_spag-1.pdf){fig-pos='H'}\n:::\n:::\n\nThe basic difficulty here is to get all the parts. We need both a\n`colour` and a `group`; the latter controls the joining of points by\nlines (if you have both). Fortunately we already had subject numbers in\nthe original data; if we had not had them, we would have had to create\nthem. `dplyr` has a function `row_number` that we could have used for\nthat; we'd apply the row numbers to the original wide data frame, before\nwe made it long, so that the correct subject numbers would get carried\nalong.\n\nWhether you add a `geom_point()` to plot the data points, or not, is up\nto you. Logically, it makes sense to include the actual data, but\naesthetically, it looks more like spaghetti if you leave the points out.\nEither way is good, as far as I'm concerned.\n\nI didn't ask you to comment on the spaghetti plot, because the story is\nmuch the same as the interaction plot. There is a lot of variability,\nbut the story within each group is basically what we already said: the\nred lines go sharply up and almost as sharply back down again, the blue\nlines do something similar, only not as sharply up and down, and the\ngreen lines do basically nothing.\n\nExtra: now that we have the long data, we can also do a mixed model\nanalysis:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\ngeriatrics.3 <- lmer(intpct ~ time * treatment + (1|subject), data = geriatrics.long)\ndrop1(geriatrics.3, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nintpct ~ time * treatment + (1 | subject)\n               npar    AIC    LRT   Pr(Chi)    \n<none>              329.40                     \ntime:treatment    6 368.66 51.265 2.621e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nThis is about the same P-value for the interaction that we had from the\n`Manova` analysis. We needed to do `drop1` because there are four time\npoints and three treatments, so that the `summary` output is rather\nunwieldy:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(geriatrics.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: intpct ~ time * treatment + (1 | subject)\n   Data: geriatrics.long\n\nREML criterion at convergence: 292.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.03739 -0.46212 -0.00484  0.42273  2.33651 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n subject  (Intercept) 3.553    1.885   \n Residual             1.338    1.157   \nNumber of obs: 84, groups:  subject, 21\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)            2.3571     0.8359   2.820\ntimet2                 6.1429     0.6183   9.935\ntimet3                 1.6857     0.6183   2.726\ntimet4                 1.0000     0.6183   1.617\ntreatmentctrl          0.2857     1.1821   0.242\ntreatmentro           -0.5000     1.1821  -0.423\ntimet2:treatmentctrl  -6.5571     0.8744  -7.499\ntimet3:treatmentctrl  -2.7000     0.8744  -3.088\ntimet4:treatmentctrl  -1.5000     0.8744  -1.715\ntimet2:treatmentro    -4.2000     0.8744  -4.803\ntimet3:treatmentro    -0.4286     0.8744  -0.490\ntimet4:treatmentro    -1.0000     0.8744  -1.144\n\nCorrelation of Fixed Effects:\n             (Intr) timet2 timet3 timet4 trtmntc trtmntr tmt2:trtmntc\ntimet2       -0.370                                                  \ntimet3       -0.370  0.500                                           \ntimet4       -0.370  0.500  0.500                                    \ntretmntctrl  -0.707  0.262  0.262  0.262                             \ntreatmentro  -0.707  0.262  0.262  0.262  0.500                      \ntmt2:trtmntc  0.262 -0.707 -0.354 -0.354 -0.370  -0.185              \ntmt3:trtmntc  0.262 -0.354 -0.707 -0.354 -0.370  -0.185   0.500      \ntmt4:trtmntc  0.262 -0.354 -0.354 -0.707 -0.370  -0.185   0.500      \ntmt2:trtmntr  0.262 -0.707 -0.354 -0.354 -0.185  -0.370   0.500      \ntmt3:trtmntr  0.262 -0.354 -0.707 -0.354 -0.185  -0.370   0.250      \ntmt4:trtmntr  0.262 -0.354 -0.354 -0.707 -0.185  -0.370   0.250      \n             tmt3:trtmntc tmt4:trtmntc tmt2:trtmntr tmt3:trtmntr\ntimet2                                                          \ntimet3                                                          \ntimet4                                                          \ntretmntctrl                                                     \ntreatmentro                                                     \ntmt2:trtmntc                                                    \ntmt3:trtmntc                                                    \ntmt4:trtmntc  0.500                                             \ntmt2:trtmntr  0.250        0.250                                \ntmt3:trtmntr  0.500        0.250        0.500                   \ntmt4:trtmntr  0.250        0.500        0.500        0.500      \n```\n\n\n:::\n:::\n\nIn the table of fixed effects, `timet1` and `treatmentbt` are baselines.\nIn the interaction plot, the baseline treatment `bt` was the one that\nwent sharply up at time `t2`, so that the `timet2` estimate is very\npositive; also, the `timet2:treatmentctrl` and `timet2:treatmentro`\nestimates are very negative, to reflect that the time `t2` measurements\nfor these two treatments are very much lower than for the baseline\ntreatment `bt`.\n\nExtra extra: I said that the control subjects' time pattern was\nnoticeably different from the others. Which made me think: what if we\nremove the control subjects? Would there still be an\ninteraction?[^_geriatrics-3]\n\n[^_geriatrics-3]: This is rather like removing time zero in the example\n    in lecture.\n\nAll right, we need to start with the original wide data frame, and from\n*that* select everything but `ctrl`:\n\n::: {.cell}\n\n```{.r .cell-code}\ngg <- geriatrics %>% filter(treatment != \"ctrl\")\ngg\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 14 x 6\n   subject treatment    t1    t2    t3    t4\n     <dbl> <chr>     <dbl> <dbl> <dbl> <dbl>\n 1       1 bt          1.5   9     5     4  \n 2       2 bt          5    14     4.5   7  \n 3       3 bt          1     8     4.5   2.5\n 4       4 bt          5    14     8     5  \n 5       5 bt          3     8     4     4  \n 6       6 bt          0.5   3.5   1.3   1  \n 7       7 bt          0.5   3     1     0  \n 8       8 ro          2     5     5     1.5\n 9       9 ro          1.5   1.9   1.5   1  \n10      10 ro          3.5   7     8     4  \n11      11 ro          1.5   4.2   2     1.5\n12      12 ro          1.5   3.6   2     1  \n13      13 ro          1.5   2.5   2.8   4  \n14      14 ro          1.5   2.4   0.5   0  \n```\n\n\n:::\n:::\n\nSo now there are two treatments left, seven people on\neach:[^_geriatrics-4]\n\n[^_geriatrics-4]: The factor `treatment` still has three levels, but\n    only two of them have any remaining data.\n\n::: {.cell}\n\n```{.r .cell-code}\ngg %>% count(treatment)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  treatment     n\n  <chr>     <int>\n1 bt            7\n2 ro            7\n```\n\n\n:::\n:::\n\nThen we do the same stuff over again: construct the response, run the\n`lm`, create the stuff for `idata` and `idesign`, and run the `Manova`.\nThere's really nothing new here:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(gg, cbind(t1, t2, t3, t4))\ngg.1 <- lm(response ~ treatment, data = gg)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\ngg.2 <- Manova(gg.1, idata = times.df, idesign = ~times)\nsummary(gg.2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in summary.Anova.mlm(gg.2): HF eps > 1 treated as 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    2920.346\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.765026 39.06941      1     12 4.2509e-05 ***\nWilks             1  0.234974 39.06941      1     12 4.2509e-05 ***\nHotelling-Lawley  1  3.255785 39.06941      1     12 4.2509e-05 ***\nRoy               1  3.255785 39.06941      1     12 4.2509e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    203.6829\n\nMultivariate Tests: treatment\n                 Df test stat approx F num Df den Df Pr(>F)\nPillai            1 0.1850562 2.724941      1     12 0.1247\nWilks             1 0.8149438 2.724941      1     12 0.1247\nHotelling-Lawley  1 0.2270784 2.724941      1     12 0.1247\nRoy               1 0.2270784 2.724941      1     12 0.1247\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1    times2   times3\ntimes1    3.5 -24.80000 -6.80000\ntimes2  -24.8 175.72571 48.18286\ntimes3   -6.8  48.18286 13.21143\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.817303 14.91185      3     10 0.00050726 ***\nWilks             1  0.182697 14.91185      3     10 0.00050726 ***\nHotelling-Lawley  1  4.473555 14.91185      3     10 0.00050726 ***\nRoy               1  4.473555 14.91185      3     10 0.00050726 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment:times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1 times2    times3\ntimes1    3.5 -11.20  2.000000\ntimes2  -11.2  35.84 -6.400000\ntimes3    2.0  -6.40  1.142857\n\nMultivariate Tests: treatment:times\n                 Df test stat approx F num Df den Df   Pr(>F)  \nPillai            1 0.5816827 4.635099      3     10 0.027959 *\nWilks             1 0.4183173 4.635099      3     10 0.027959 *\nHotelling-Lawley  1 1.3905298 4.635099      3     10 0.027959 *\nRoy               1 1.3905298 4.635099      3     10 0.027959 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                Sum Sq num Df Error SS den Df F value    Pr(>F)    \n(Intercept)     730.09      1  224.243     12 39.0694 4.251e-05 ***\ntreatment        50.92      1  224.243     12  2.7249 0.1247005    \ntimes           136.04      3   60.551     36 26.9595 2.560e-09 ***\ntreatment:times  38.16      3   60.551     36  7.5629 0.0004777 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                Test statistic p-value\ntimes                  0.66019 0.48791\ntreatment:times        0.66019 0.48791\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                 GG eps Pr(>F[GG])    \ntimes           0.82418  5.012e-08 ***\ntreatment:times 0.82418   0.001217 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                HF eps   Pr(>F[HF])\ntimes           1.0546 2.560037e-09\ntreatment:times 1.0546 4.777491e-04\n```\n\n\n:::\n:::\n\nThe procedure, as before: Mauchly's test is not significant, so you can\nlook at the univariate test for interaction. For comparison, the\nP-values for interaction in the multivariate test (all the same) are\nquite a bit bigger but still significant.\n\nThere is still an interaction, but it's not as significant as it was\nbefore. I think it is still significant because the shape of the two\ntime trends is not the same: the red `bt` group goes up further and down\nfurther. I was musing that the higher values are also more variable,\nwhich would suggest a transformation, but I haven't explored that.\n\nIf the interaction had turned out to be nonsignificant this way? You\nmight think about trying to remove it from the model, except that in\nthis kind of model, `treatment` is a \"between-subjects factor\" and\n`times` is a \"within-subjects factor\", so they are different kinds of\nthings. What you do in that case is to ignore the non-significant\ninteraction and interpret the main effects: there is no way to \"gain df\nfor error\" like we did in two-way ANOVA.\n\nSupposing, in this case, that we were using $\\alpha=0.01$, we would say\nthat the interaction is not significant. Then we look at the main\neffects: there is no effect of treatment, but there is an effect of\ntime. Or, to put it another way, once you allow for an effect of time,\nthere is no difference between the two remaining\ntreatments.[^_geriatrics-5]\n\n[^_geriatrics-5]: There is often an effect of time, which is why you\n    would be taking multiple time measurements, but the issue is when\n    you take that into account, you are giving yourself an improved\n    chance, in general, to find a treatment effect. This is exactly the\n    same idea as using a matched pairs design to give yourself an\n    improved chance of finding a treatment effect, even though the\n    subjects might be quite different from each other. In fact, repeated\n    measures *is* matched pairs with more than two measurements on each\n    person. Which makes me think, I should have you do a matched pairs\n    as repeated measures.\n\nThinking back to our spaghetti plot, we are now comparing the red and\nblue treatments. They both go up at time 2 and down afterwards, which is\nthe time effect, but even once you allow for this time trend, there is\ntoo much scatter to be able to infer a difference between the traes(x =\ntime, y = intpct, colour = treatment, group = subject)eatments.\n\nExtra (maybe I could branch off into another question sometime?) I was\nthinking that this is not terribly clear, so I thought I would fake up\nsome data where there is a treatment effect and a time effect (but no\ninteraction), and draw a spaghetti plot, so you can see the difference,\nidealized somewhat of course. Let's try to come up with something with\nthe same kind of time effect, up at time 2 and then down afterwards,\nthat is the same for two drugs A and B. Here's what I came up with:\n\n::: {.cell}\n\n```{.r .cell-code}\nfake <- read.csv(\"fake.csv\", header = T)\nfake\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   subject drug t1 t2 t3 t4\n1        1    a 10 15 13 11\n2        2    a 11 14 12  9\n3        3    a 12 16 13 11\n4        4    a 10 14 11 11\n5        5    a 11 13 10  9\n6        6    b  7 10  8  6\n7        7    b  8 12 11  9\n8        8    b  5  9  7  5\n9        9    b  7  8  6  5\n10      10    b  8 12 11  9\n```\n\n\n:::\n:::\n\nYou can kind of get the idea that the pattern over time is up and then\ndown, so that it finishes about where it starts, but the numbers for\ndrug A are usually bigger than the ones for drug B, consistently over\ntime. So there ought not to be an interaction, but there ought to be\nboth a time effect and a drug effect.\n\nLet's see whether we can demonstrate that. First, a spaghetti plot,\nwhich involves getting the data in long format first. I'm saving the\nlong format to use again later.\n\n::: {.cell}\n\n```{.r .cell-code}\nfake %>% \n  pivot_longer(t1:t4, names_to=\"times\", values_to=\"score\") -> fake.long\nfake.long %>%\n  ggplot(aes(x = times, y = score, colour = drug, group = subject)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/geriatrics-12-1.pdf){fig-pos='H'}\n:::\n:::\n\nThe reds are consistently higher than the blues (drug effect), the\npattern over time goes up and then down (time effect), but the time\neffect is basically the same for both drugs (no interaction).\n\nI got the plot wrong the first time, because I forgot whether I was\ndoing an interaction plot (where `group=` and `colour=` are the same) or\na spaghetti plot (where `group` has to be `subject` and the colour\nrepresents the treatment, usually).\n\nLet's do the repeated-measures ANOVA and see whether my guess above is\nright:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(fake, cbind(t1, t2, t3, t4))\nfake.1 <- lm(response ~ drug, data = fake)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\nfake.2 <- Manova(fake.1, idata = times.df, idesign = ~times)\n```\n:::\n\nAfter typing this kind of stuff out a few too many times, I hope you're\ngetting the idea \"function\". Also, the construction of the response is\nkind of annoying, where you have to list all the time columns. The\ntrouble is, `response` has to be a `matrix`, which it is:\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"matrix\" \"array\" \n```\n\n\n:::\n:::\n\nbut if you do the obvious thing of selecting the columns of the data\nframe that you want:\n\n::: {.cell}\n\n```{.r .cell-code}\nfake %>% select(t1:t4) -> r\nclass(r)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"data.frame\"\n```\n\n\n:::\n:::\n\nyou get a data frame instead. I think this would work:\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- fake %>% select(t1:t4) %>% as.matrix()\nclass(r)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"matrix\" \"array\" \n```\n\n\n:::\n:::\n\nThe idea is that you select the columns you want as a data frame first\n(with `select`), and then turn it into a `matrix` at the end.\n\nThis is the kind of thing you'd have to do in a function, I think, since\nyou'd have to have some way of telling the function which are the \"time\"\ncolumns. Anyway, hope you haven't forgotten what we were\ndoing:[^_geriatrics-6]\n\n[^_geriatrics-6]: I got sidetracked, surprise surprise.\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fake.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     15920.1\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1   0.98478 517.7268      1      8 1.4752e-08 ***\nWilks             1   0.01522 517.7268      1      8 1.4752e-08 ***\nHotelling-Lawley  1  64.71585 517.7268      1      8 1.4752e-08 ***\nRoy               1  64.71585 517.7268      1      8 1.4752e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)       532.9\n\nMultivariate Tests: drug\n                 Df test stat approx F num Df den Df    Pr(>F)   \nPillai            1   0.68417 17.33008      1      8 0.0031525 **\nWilks             1   0.31583 17.33008      1      8 0.0031525 **\nHotelling-Lawley  1   2.16626 17.33008      1      8 0.0031525 **\nRoy               1   2.16626 17.33008      1      8 0.0031525 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1 times2 times3\ntimes1    1.6   15.2    6.8\ntimes2   15.2  144.4   64.6\ntimes3    6.8   64.6   28.9\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1   0.98778 161.7086      3      6 3.9703e-06 ***\nWilks             1   0.01222 161.7086      3      6 3.9703e-06 ***\nHotelling-Lawley  1  80.85428 161.7086      3      6 3.9703e-06 ***\nRoy               1  80.85428 161.7086      3      6 3.9703e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug:times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1 times2 times3\ntimes1    0.4    0.8   -0.2\ntimes2    0.8    1.6   -0.4\ntimes3   -0.2   -0.4    0.1\n\nMultivariate Tests: drug:times\n                 Df test stat approx F num Df den Df   Pr(>F)  \nPillai            1 0.6490046  3.69808      3      6 0.081108 .\nWilks             1 0.3509954  3.69808      3      6 0.081108 .\nHotelling-Lawley  1 1.8490401  3.69808      3      6 0.081108 .\nRoy               1 1.8490401  3.69808      3      6 0.081108 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n            Sum Sq num Df Error SS den Df  F value    Pr(>F)    \n(Intercept) 3980.0      1     61.5      8 517.7268 1.475e-08 ***\ndrug         133.2      1     61.5      8  17.3301  0.003152 ** \ntimes         87.9      3     14.9     24  47.1812 3.233e-10 ***\ndrug:times     1.5      3     14.9     24   0.7919  0.510323    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n           Test statistic p-value\ntimes             0.18708 0.04852\ndrug:times        0.18708 0.04852\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n            GG eps Pr(>F[GG])    \ntimes      0.54943  1.886e-06 ***\ndrug:times 0.54943     0.4505    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n              HF eps   Pr(>F[HF])\ntimes      0.6735592 1.708486e-07\ndrug:times 0.6735592 4.709652e-01\n```\n\n\n:::\n:::\n\nThe usual procedure: check for sphericity first. Here, that is *just*\nrejected, but since the P-value on the sphericity test is only just less\nthan 0.05, you would expect the P-values on the univariate test for\ninteraction and the Huynh-Feldt adjustment to be similar, and they are\n(0.510 and 0.471 respectively). Scrolling up a bit further, the\nmultivariate test for interaction only just fails to be significant,\nwith a P-value of 0.081. It is a mild concern that this one differs so\nmuch from the others; normally the multivariate test(s) would tell a\nsimilar story to the others.\n\nThe drug-by-time interaction is not significant, so we go ahead and\ninterpret the main effects: there is a time effect (the increase at time\n2 that I put in on purpose), and, allowing for the time effect, there is\nalso a difference between the drugs (because the drug A scores are a bit\nhigher than the drug B scores). The procedure is to look at the\nHuynh-Feldt adjusted P-value for time ($1.71 \\times 10^{-7}$), expecting\nit to be a bit bigger than the one in the univariate table (it is) and\ncomparable to the one for time in the appropriate multivariate analysis\n($3.97 \\times 10^{-6}$; it is, but remember to scroll back enough). In\nthis kind of analysis, the effect of drug is averaged over\ntime,[^_geriatrics-7] so the test for the drug main effect is unaffected\nby sphericity. Its P-value, 0.0032, is identical in the univariate and\nmultivariate tables, and you see that the drug main effect is not part\nof the sphericity testing.\n\n[^_geriatrics-7]: See below.\n\nWhat if we ignored the time effect? You'd think we could do something\nlike this, treating the measurements at different times as replicates:\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(fake.long)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 4\n  subject drug  times score\n    <int> <chr> <chr> <int>\n1       1 a     t1       10\n2       1 a     t2       15\n3       1 a     t3       13\n4       1 a     t4       11\n5       2 a     t1       11\n6       2 a     t2       14\n```\n\n\n:::\n\n```{.r .cell-code}\nfake.3 <- aov(score ~ drug, data = fake.long)\nsummary(fake.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ndrug         1  133.2  133.22   30.54 2.54e-06 ***\nResiduals   38  165.8    4.36                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nbut this would be *wrong*, because we are acting as if we have 40\nindependent observations, which we don't (this is the point of doing\nrepeated measures in the first place). It looks as if we have achieved\nsomething by getting a lower P-value for `drug`, but we haven't really,\nbecause we have done so by cheating.\n\nWhat we could do instead is to average the scores for each subject over\nall the times,[^_geriatrics-8] for which we go back to the original data\nframe:\n\n[^_geriatrics-8]: This would be allowable, since we are averaging *over*\n    the time-dependence; we are creating 10 independent averages, from\n    the 10 subjects. People do this kind of thing, instead of having to\n    deal with the repeated measures.\n\n::: {.cell}\n\n```{.r .cell-code}\nfake\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   subject drug t1 t2 t3 t4\n1        1    a 10 15 13 11\n2        2    a 11 14 12  9\n3        3    a 12 16 13 11\n4        4    a 10 14 11 11\n5        5    a 11 13 10  9\n6        6    b  7 10  8  6\n7        7    b  8 12 11  9\n8        8    b  5  9  7  5\n9        9    b  7  8  6  5\n10      10    b  8 12 11  9\n```\n\n\n:::\n\n```{.r .cell-code}\nfake %>%\n  mutate(avg.score = (t1 + t2 + t3 + t4) / 4) %>%\n  aov(avg.score ~ drug, data = .) %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Df Sum Sq Mean Sq F value  Pr(>F)   \ndrug         1  33.31   33.31   17.33 0.00315 **\nResiduals    8  15.37    1.92                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nAh, now, this is very interesting. I was hoping that by throwing away\nthe time information (which is useful), we would have diminished the\nsignificance of the drug effect. By failing to include the\ntime-dependence in our model, we ought to have introduced some extra\nvariability, which ought to weaken our test. But this test gives\n*exactly the same* P-value as the ones in the MANOVA, and it looks like\nexactly the same test (the $F$-value is the same too). So it looks as if\nthis is what the MANOVA is doing, to assess the `drug` effect: it's\naveraging over the times. Since the same four (here) time points are\nbeing used to compute the average for each subject, we are comparing\nlike with like at least, and even if there is a large time effect, I\nsuppose it's going to have the same effect on each average. For example,\nif as here the scores at time 2 are typically highest, all the averages\nare going to be composed of one high score and three lower ones. So\nmaybe I have to go back and dilute my conclusions about the significance\nof treatments earlier: it's actually saying that there is a difference\nbetween the two remaining treatments *averaged over time* rather than\n*allowing for time* as I said earlier.\n\n$\\blacksquare$\n\n\n##  Children's stress levels and airports\n\n\n If you did STAC32, you might remember this question, which\nwe can now do properly. Some of this question is a repeat from there.\n\nThe data in [link](http://ritsokiguess.site/datafiles/airport.txt)\nare based on a 1998 study of stress levels in children as a result of\nthe building of a new airport in Munich, Germany. A total of 200\nchildren had their epinephrine levels (a stress indicator) measured at\neach of four different times: before the airport was built, and 6, 18\nand 36 months after it was built. The four measurements are labelled\n`epi_1` through `epi_4`.  Out of the children, 100\nwere living near the new airport (location 1 in the data set), and\ncould be expected to suffer stress because of the new airport. The\nother 100 children lived in the same city, but outside the noise\nimpact zone. These children thus serve as a control group. The\nchildren are identified with numbers 1 through 200.\n\n\n\n(a) If we were testing for the effect of time, explain briefly\nwhat it is about the structure of the data that would make an\nanalysis of variance *inappropriate*.\n\n\nSolution\n\n\nIt is the fact that each child was measured four times, rather\nthan each measurement being on a *different* child (with\nthus $4\\times 200=800$ observations altogether). It's\nthe same distinction as between matched pairs and a two-sample\n$t$ test.\n\n$\\blacksquare$\n\n(b) Read the data into R and demonstrate that you have the right\nnumber of observations and variables.\n\n\nSolution\n\n\nThe usual, data values separated by one space:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/airport.txt\"\nairport <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 200 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\ndbl (6): epi_1, epi_2, epi_3, epi_4, location, child\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nairport\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 200 x 6\n    epi_1  epi_2 epi_3 epi_4 location child\n    <dbl>  <dbl> <dbl> <dbl>    <dbl> <dbl>\n 1  89.6  253.   214.   209.        1     1\n 2 -55.5   -1.45  26.0  259.        1     2\n 3 201.   280.   265.   174.        1     3\n 4 448.   349.   386.   225.        1     4\n 5  -4.60 315.   331.   333.        1     5\n 6 231.   237.   488.   319.        1     6\n 7 227.   469.   382.   359.        1     7\n 8 336.   280.   362.   472.        1     8\n 9  16.8  190.    90.9  145.        1     9\n10  54.5  359.   454.   199.        1    10\n# i 190 more rows\n```\n\n\n:::\n:::\n\n \n\nThere are 200 rows (children), with four `epi` measurements, a\nlocation and a child identifier, so that looks good.\n\n(I am mildly concerned about the negative `epi` measurements,\nbut I don't know what the scale is, so presumably they are all\nright. Possibly epinephrine is measured on a log scale, so that a\nnegative value here is less than 1 on the original scale that we don't\nsee.)\n\n$\\blacksquare$\n\n(c) Create and save a \"longer\" data frame with all the epinephrine\nvalues collected together into one column.\n\n\nSolution\n\n`pivot_longer`:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>% pivot_longer(starts_with(\"epi\"), names_to=\"when\", values_to=\"epinephrine\") -> airport.long\nairport.long\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 800 x 4\n   location child when  epinephrine\n      <dbl> <dbl> <chr>       <dbl>\n 1        1     1 epi_1       89.6 \n 2        1     1 epi_2      253.  \n 3        1     1 epi_3      214.  \n 4        1     1 epi_4      209.  \n 5        1     2 epi_1      -55.5 \n 6        1     2 epi_2       -1.45\n 7        1     2 epi_3       26.0 \n 8        1     2 epi_4      259.  \n 9        1     3 epi_1      201.  \n10        1     3 epi_2      280.  \n# i 790 more rows\n```\n\n\n:::\n:::\n\n \n\nSuccess. I'm saving the name `time` for later, so I've called\nthe time points `when` for now. There were 4 measurements on\neach of 200 children, so the long data frame should (and does) have\n$200\\times 4 = 800$ rows.\n\n$\\blacksquare$\n\n(d) Make a \"spaghetti plot\" of these data: that is, a plot of\nepinephrine levels against time, with the locations identified by\ncolour, and the points for the same child joined by lines.  To do\nthis: (i) from the long data frame, create a new column containing\nonly the numeric values of time (1 through 4), (ii) plot epinephrine\nlevel against time with the points grouped by child and coloured by\nlocation (which you may have to turn from a number into a factor.)\n\n\nSolution\n\nNote the use of the different things for `colour` and `group`, as usual for a spaghetti plot. Also, note that the locations are identified by number, but the number is only a label, and we want to use different colours for the different locations, so we need to turn `location` into a factor for this. \n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>%\n  mutate(time = parse_number(when)) %>%\n  ggplot(aes(x = time, y = epinephrine, colour = factor(location), group = child)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/toofat-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThis^[The term \"accidental aRt\" is sometimes used for graphs that cross the boundary between being informative and looking like a piece of art, particularly if it was not done on purpose. This one is a bit like that.] is different from the plot we had in C32, where I had you use a\ndifferent colour for each *child*, and we ended up with a huge\nlegend of all the children (which we then removed). \n\nIf you forget to turn `location` into a factor, `ggplot`\nwill assume that you want `location` to be on a continuous\nscale, and you'll get two shades of blue. \n\nAnother problem with this plot is that there are so many children, you\ncan't see the ones underneath because the ones on top are overwriting\nthem. The solution to that is to make the lines (partly) transparent,\nwhich is controlled by a parameter `alpha`:^[This is  different from the 0.05 $\\alpha$.]\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>%\n  mutate(time = parse_number(when)) %>%\n  ggplot(aes(x = time, y = epinephrine, colour = factor(location), group = child)) +\n  geom_point() + geom_line(alpha = 0.2)\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/skinnier-1.pdf){fig-pos='H'}\n:::\n:::\n\n   \nIt seems to make the lines skinnier, so they look more like\nthreads. Even given the lesser thickness, they seem to be a little bit\nsee-through as well. You can experiment with adding transparency to\nthe points in addition. \n\n$\\blacksquare$\n\n(e) What do you see on your spaghetti plot? We are looking ahead\nto possible effects of time, location and their interaction.\n\n\nSolution\n\n\nThis is not clear, so it's very much your call.\nI see the red spaghetti strands as going up further\n(especially) and maybe down further than the blue ones. The\nepinephrine levels of the children near the new airport are\ndefinitely more spread out, and maybe have a higher mean, than\nthose of the control group of children not near the airport.\nThe red spaghetti strands show something of an increase over\ntime, at least up to time 3, after which they seem to drop\nagain. The blue strands, however, don't show any kind of trend\nover time. Since the time trend is different for the two\nlocations, I would expect to see a significant interaction.\n\n$\\blacksquare$\n\n(f) The spaghetti plot was hard to interpret because there are\nso many children. Calculate the mean epinephrine levels for each\nlocation-time combination, and make an interaction plot with time\non the $x$-axis and location as the second factor.\n\n\nSolution\n\n\nWe've done this before:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>%\n  mutate(time = parse_number(when)) %>%\n  mutate(floc = factor(location)) %>%\n  group_by(floc, time) %>%\n  summarize(mean.epi = mean(epinephrine)) %>%\n  ggplot(aes(x = time, y = mean.epi, group = floc, colour = floc)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'floc'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/airport-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n         \n\nI wanted the actual numerical times, so I made them again. Also, it\nseemed to be easier to create a factor version of the numeric location\nup front, and then use it several times later. I'm actually not sure\nthat you need it here, since `group_by` works with the\ndistinct values of a variable, whatever they are, and `group`\nin a boxplot may or may not insist on something other than a number. I\nshould try it:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>%\n  mutate(time = parse_number(when)) %>%\n  group_by(location, time) %>%\n  summarize(mean.epi = mean(epinephrine)) %>%\n  ggplot(aes(x = time, y = mean.epi, group = location, colour = location)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/airport-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n          \n\nIt seems that `colour` requires a non-number:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>%\n  mutate(time = parse_number(when)) %>%\n  group_by(location, time) %>%\n  summarize(mean.epi = mean(epinephrine)) %>%\n  ggplot(aes(x = time, y = mean.epi, group = location, colour = factor(location))) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/airport-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n         \nWith a long pipeline like this, none of us get it right the first time (I\ncertainly didn't), so be prepared to debug it one line at a time. The\nway I like to do this is to take the pipe symbol and move it down to\nthe next line (moving the cursor to just before it and hitting\nEnter). This ends the pipe at the end of *this* line and displays\nwhat it produces so far. When you are happy with that, go to the start\nof the next line (that currently has a pipe symbol by itself) and hit\nBackspace to move the pipe symbol back where it was. Then go to the\nend of the next line (where the next pipe symbol is), move *that*\nto a line by itself, and so on. Keep going until each line produces\nwhat you want, and when you are finished, the whole pipeline will do\nwhat you want.\n\n$\\blacksquare$\n\n(g) What do you conclude from your interaction plot? Is your\nconclusion clearer than from the spaghetti plot?\n\n\nSolution\n\n\nThe two \"traces\" are not parallel, so I would expect to see\nan interaction between location and time. The big difference\nseems to be between times 1 and 2; the traces are the same at\ntime 1, and more or less parallel after time 2. Between times\n1 and 2, the mean epinephrine level of the children near the\nnew airport increases sharply, whereas for the children in the\ncontrol group it increases much less.\nThis, to my mind, is very much easier to interpret than the\nspaghetti plot, even the second version with the thinner\nstrands, because there is a lot of variability there that\nobscures the overall pattern. The interaction plot is plain as\nday, but it might be an oversimplification because it doesn't\nshow variability.\n\n$\\blacksquare$\n\n(h) Run a repeated-measures analysis of variance and display the\nresults. Go back to your original data frame, the one you read in\nfrom the file, for this. You'll need to make sure your numeric\n`location` gets treated as a `factor`.\n\n\nSolution\n\n\nThe usual process. I'll try the other way I used of making the\n`response`: \n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>%\n  select(epi_1:epi_4) %>%\n  as.matrix() -> response\nairport.1 <- lm(response ~ factor(location), data = airport)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\nairport.2 <- Manova(airport.1, idata = times.df, idesign = ~times)\nsummary(airport.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   268516272\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.920129     2281      1    198 < 2.22e-16 ***\nWilks             1  0.079871     2281      1    198 < 2.22e-16 ***\nHotelling-Lawley  1 11.520204     2281      1    198 < 2.22e-16 ***\nRoy               1 11.520204     2281      1    198 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: factor(location) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     3519790\n\nMultivariate Tests: factor(location)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.1311980 29.90002      1    198 1.3611e-07 ***\nWilks             1 0.8688020 29.90002      1    198 1.3611e-07 ***\nHotelling-Lawley  1 0.1510102 29.90002      1    198 1.3611e-07 ***\nRoy               1 0.1510102 29.90002      1    198 1.3611e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1     times2     times3\ntimes1  497500.84 -113360.08 -56261.667\ntimes2 -113360.08   25830.12  12819.731\ntimes3  -56261.67   12819.73   6362.552\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.3274131 31.80405      3    196 < 2.22e-16 ***\nWilks             1 0.6725869 31.80405      3    196 < 2.22e-16 ***\nHotelling-Lawley  1 0.4867966 31.80405      3    196 < 2.22e-16 ***\nRoy               1 0.4867966 31.80405      3    196 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: factor(location):times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1    times2      times3\ntimes1 533081.68 206841.01 -14089.6126\ntimes2 206841.01  80256.38  -5466.9104\ntimes3 -14089.61  -5466.91    372.3954\n\nMultivariate Tests: factor(location):times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.2373704 20.33516      3    196 1.6258e-11 ***\nWilks             1 0.7626296 20.33516      3    196 1.6258e-11 ***\nHotelling-Lawley  1 0.3112525 20.33516      3    196 1.6258e-11 ***\nRoy               1 0.3112525 20.33516      3    196 1.6258e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                         Sum Sq num Df Error SS den Df  F value    Pr(>F)    \n(Intercept)            67129068      1  5827073    198 2281.000 < 2.2e-16 ***\nfactor(location)         879947      1  5827073    198   29.900 1.361e-07 ***\ntimes                    475671      3  3341041    594   28.190 < 2.2e-16 ***\nfactor(location):times   366641      3  3341041    594   21.728 2.306e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                       Test statistic  p-value\ntimes                          0.9488 0.066194\nfactor(location):times         0.9488 0.066194\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                        GG eps Pr(>F[GG])    \ntimes                  0.96685  < 2.2e-16 ***\nfactor(location):times 0.96685   5.35e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                          HF eps   Pr(>F[HF])\ntimes                  0.9827628 8.418534e-17\nfactor(location):times 0.9827628 3.571900e-13\n```\n\n\n:::\n:::\n\n         \n\n$\\blacksquare$\n\n(i) What do you conclude from the MANOVA? Is that consistent\nwith your graphs? Explain briefly.\n\n\nSolution\n\nStart with Mauchly's test near the bottom. This is not quite significant (for the interaction), so we are entitled to look at the univariate test for interaction, which is $2.3 \\times 10^{-13}$, extremely significant. If you want a comparison, look at the Huynh-Feldt adjustment for the interaction, which is almost exactly the same ($3.57 \\times 10^{-13}$), or the multivariate tests for the interaction (almost the same again).\n\n\nSo, we start and end with the significant interaction: there is an\neffect of location, but the nature of that effect depends on\ntime. This is the same as we saw in the interaction plot:\nfrom time 2 on, the mean epinephrine levels for the children near\nthe new airport were clearly higher. \n\nIf you stare at the spaghetti plot, you *might* come to\nthe same conclusion. Or you might not! I suppose those red\ndots at time 2 are mostly at the top, and generally so\nafterwards, whereas at time 1 they are all mixed up with the\nblue ones.\n\nInteractions of this sort in this kind of analysis  are very\ncommon. There is an \"intervention\" or \"treatment\", and the\ntime points are chosen so that the first one is before the\ntreatment happens, and the other time points are after. Then,\nthe results are very similar for the first time point, and\nvery different after that, rather than being (say) always\nhigher for the treatment group by about the same amount for\nall times (in which case there would be no interaction). \n\nSo, you have some choices in practice as to how you might\ngo. You might do the MANOVA, get a significant interaction,\nand draw an interaction plot to see why. You might stop there,\nor you might do something like what we did in class: having\nseen that the first time point is different from the others\nfor reasons that you can explain, do the analysis again, but\nomitting the first time point. For the MANOVA, that means\ntweaking your definition of your `response` to omit the\nfirst time point. The rest of it stays the same, though you\nmight want to change your model numbers rather than re-using\nthe old ones as I did:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>%\n  select(epi_2:epi_4) %>%\n  as.matrix() -> response\nairport.1 <- lm(response ~ factor(location), data = airport)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\nairport.2 <- Manova(airport.1, idata = times.df, idesign = ~times)\nsummary(airport.2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in summary.Anova.mlm(airport.2): HF eps > 1 treated as 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   165867956\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.918531 2232.383      1    198 < 2.22e-16 ***\nWilks             1  0.081469 2232.383      1    198 < 2.22e-16 ***\nHotelling-Lawley  1 11.274662 2232.383      1    198 < 2.22e-16 ***\nRoy               1 11.274662 2232.383      1    198 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: factor(location) \n\n Response transformation matrix:\n      (Intercept)\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     3567099\n\nMultivariate Tests: factor(location)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.1951509 48.00886      1    198 5.8219e-11 ***\nWilks             1 0.8048491 48.00886      1    198 5.8219e-11 ***\nHotelling-Lawley  1 0.2424690 48.00886      1    198 5.8219e-11 ***\nRoy               1 0.2424690 48.00886      1    198 5.8219e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2\nepi_2      1      0\nepi_3      0      1\nepi_4     -1     -1\n\nSum of squares and products for the hypothesis:\n         times1    times2\ntimes1 25830.12 12819.731\ntimes2 12819.73  6362.552\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df  Pr(>F)\nPillai            1 0.0123563 1.232325      2    197 0.29385\nWilks             1 0.9876437 1.232325      2    197 0.29385\nHotelling-Lawley  1 0.0125109 1.232325      2    197 0.29385\nRoy               1 0.0125109 1.232325      2    197 0.29385\n\n------------------------------------------\n \nTerm: factor(location):times \n\n Response transformation matrix:\n      times1 times2\nepi_2      1      0\nepi_3      0      1\nepi_4     -1     -1\n\nSum of squares and products for the hypothesis:\n         times1     times2\ntimes1 80256.38 -5466.9104\ntimes2 -5466.91   372.3954\n\nMultivariate Tests: factor(location):times\n                 Df test stat approx F num Df den Df    Pr(>F)   \nPillai            1 0.0508561 5.277736      2    197 0.0058507 **\nWilks             1 0.9491439 5.277736      2    197 0.0058507 **\nHotelling-Lawley  1 0.0535811 5.277736      2    197 0.0058507 **\nRoy               1 0.0535811 5.277736      2    197 0.0058507 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                         Sum Sq num Df Error SS den Df   F value    Pr(>F)    \n(Intercept)            55289319      1  4903856    198 2232.3830 < 2.2e-16 ***\nfactor(location)        1189033      1  4903856    198   48.0089 5.822e-11 ***\ntimes                     12915      2  2281728    396    1.1207  0.327070    \nfactor(location):times    57397      2  2281728    396    4.9807  0.007306 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                       Test statistic p-value\ntimes                         0.99068 0.39746\nfactor(location):times        0.99068 0.39746\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                        GG eps Pr(>F[GG])   \ntimes                  0.99076   0.326702   \nfactor(location):times 0.99076   0.007483 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                         HF eps  Pr(>F[HF])\ntimes                  1.000731 0.327069634\nfactor(location):times 1.000731 0.007305714\n```\n\n\n:::\n:::\n\n         \n\nThe interaction is still significant. The sphericity test is not significant, so you can use the 0.0073 in the univariate tests as your P-value (note that the Huynh-Feldt adjustment is actually not adjusted at all).\n\nSo there is still not a\nconsistent effect of being near the new airport on epinephrine levels:\nthat is to say, the effect of the new airport *still* varies over\ntime. That might be because (looking at the interaction plot) for the\nchildren near the new airport, the mean epinephrine level went up\nagain between times 2 and 3, whereas for the control children it (for\nsome reason) went dramatically down over the same time period.\n\nWe have lots of data here (200 children), so the significant\ninteraction effect also might not be very big.\n\nExperimental designs like this are kind of awkward, because you expect\nthere to be some kind of pattern over time for the treatment group,\nthat will vary over time, whereas for the control group, you expect\nthere to be no pattern over time. So a significant difference shows up\nas an *interaction*, which is messier to interpret than you would\nlike. \n\nExtra: the other way to analyze repeated measures data ^[This is something *I* want to  understand, so I will share my findings with you. You can read them  or not, as you choose.] is to treat them as \"mixed models\", which\nrequires a different kind of analysis using the `lme4`\npackage. I always forget how these go, and I have to look them up when\nI need them, but the idea is this: the treatments you observe, and the\ntime points at which you observe them, are\ntypically the only ones you care about (a so-called \"fixed effect\"),\nbut the individuals (children, here)\nwhich you happen to observe are\nsomething like a random sample of all the children you might have\nobserved (a so-called \"random effect\"). Models with random effects\nin them are called \"mixed models\" (or, I suppose, models with both\nfixed and random effects). This matters because you have repeated\nobservations on the *same* child. Some people like to think of\nthis in terms of \"sources of variability\": the epinephrine levels\nvary because of the location and time at which they were observed, but\nalso because of the particular child they happen to have been observed\nfor: each child has a \"random effect\" that they carry with them\nthrough all the time points at which they are observed.\n\nLet's see if we can make it fly for this example. We need the data in\n\"long\" format, the way we arranged it for graphing: the data frame\n`airport.long`. I'd like to convert things to factors first:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>% mutate(\n  fchild = factor(child),\n  flocation = factor(location)\n) -> fairport\n\nairport.3 <- lmer(epinephrine ~ flocation * when + (1 | fchild), data = fairport)\nanova(airport.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n               npar Sum Sq Mean Sq F value\nflocation         1 168177  168177  29.900\nwhen              3 475671  158557  28.190\nflocation:when    3 366641  122214  21.728\n```\n\n\n:::\n\n```{.r .cell-code}\ndrop1(airport.3, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nepinephrine ~ flocation * when + (1 | fchild)\n               npar    AIC    LRT   Pr(Chi)    \n<none>              9521.2                     \nflocation:when    3 9577.6 62.475 1.739e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nThe `anova` doesn't actually give us any tests, but what you\nsee in the ANOVA table are the fixed effects. These are testable. The\neasiest way to see what you can get rid of is `drop1`; the\nchi-squared test appears to be the right one (more on that\nbelow). This says that the interaction is strongly significant, and we\nshould not consider removing it, the same conclusion as our \n\"profile analysis\" before.\nThe other choice for\ntesting is to fit a model without what you\nare testing and use `anova` to compare the two models:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.4 <- update(airport.3, . ~ . - flocation:when)\nanova(airport.4, airport.3)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: fairport\nModels:\nairport.4: epinephrine ~ flocation + when + (1 | fchild)\nairport.3: epinephrine ~ flocation * when + (1 | fchild)\n          npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nairport.4    7 9577.6 9610.4 -4781.8   9563.6                         \nairport.3   10 9521.2 9568.0 -4750.6   9501.2 62.475  3  1.739e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nThere are technical considerations involved in comparing the fit of\ntwo models (which is the reason for the \n\"refitting models with...\"): there is one method for estimating and a different\nmethod for testing. The test is based on \"likelihood ratio\", which\nmeans that the right test for the `drop1` above is\n`Chisq`. \n\nIf you omit the test in `drop1`, it just gives\nyou AIC values, which you can use for an informal assessment. In this\ncase, `<none>` has a much smaller AIC than the interaction\n(smaller by over 50), so there's no way we should entertain taking out\nthe interaction. However, if it had not been significant, we would\njust take it out by fitting a model like `airport4`: there is\nno distinction here between \"within-subject\" and \n\"between-subject\"\nfactors that prevented us from taking the interaction out in profile\nanalysis. \n\nAs ever when an interaction is significant, we might think  about\nsimple effects: that is, look at the two locations separately. That\nmakes sense here because of the kind of experimental design it is: we\n*expect* a different kind of relationship with time for the\n\"treatment\" children (the ones living near the new airport) as\ncompared to the control children, the ones who live farther away. That\napproach would work with either the profile-analysis way using\n`Manova` or the mixed-modelling way using `lmer`. In\neither case, we'd expect to see a time effect at location 1 but not at\nlocation 2. (Having separated out locations, only the time effect\nwould be left for testing.) I guess I have to show you that, but I\nhave to get ready for class first.\n\nLater\\ldots\n\nThe nice thing about Wednesday evenings is that I am so tired from\nclass that I have energy for almost nothing except playing with these\nthings. So let's have a crack at it. \n\nLet's start with location 1, at which we expect there to be\nsomething happening. This is really a simple effect of time at\nlocation 1, but in repeated measures guise. The awkwardness is that\nthe profile analysis needs the wide-format data, while the mixed-model\nanalysis needs long format, so we'll have to repeat our process, once\nfor each format of data set:\n\n::: {.cell}\n\n```{.r .cell-code}\nloc1 <- airport %>% filter(location == 1)\nresponse <- loc1 %>% select(epi_1:epi_4) %>% as.matrix()\nloc1.1 <- lm(response ~ 1, data = loc1)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\nloc1.2 <- Manova(loc1.1, idata = times.df, idesign = ~times)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNote: model has only an intercept; equivalent type-III tests substituted.\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(loc1.2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in summary.Anova.mlm(loc1.2): HF eps > 1 treated as 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType III Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   166760848\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.897288 864.8584      1     99 < 2.22e-16 ***\nWilks             1  0.102712 864.8584      1     99 < 2.22e-16 ***\nHotelling-Lawley  1  8.735944 864.8584      1     99 < 2.22e-16 ***\nRoy               1  8.735944 864.8584      1     99 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1    times2     times3\ntimes1 1030275.32 87978.051 -71100.691\ntimes2   87978.05  7512.688  -6071.484\ntimes3  -71100.69 -6071.484   4906.755\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.4542272 26.90988      3     97 9.4963e-13 ***\nWilks             1 0.5457728 26.90988      3     97 9.4963e-13 ***\nHotelling-Lawley  1 0.8322643 26.90988      3     97 9.4963e-13 ***\nRoy               1 0.8322643 26.90988      3     97 9.4963e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n              Sum Sq num Df Error SS den Df F value    Pr(>F)    \n(Intercept) 41690212      1  4772262     99 864.858 < 2.2e-16 ***\ntimes         776618      3  2774062    297  27.716 7.869e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n      Test statistic p-value\ntimes        0.95865 0.53129\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n       GG eps Pr(>F[GG])    \ntimes 0.97423  1.748e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        HF eps   Pr(>F[HF])\ntimes 1.007064 7.869406e-16\n```\n\n\n:::\n:::\n\n \n\nThis is one of those times where a pipe doesn't quite do it. We need\nto grab the data for location 1, but then we need to do two things\nwith it: one, create a response variable, and two, feed the response\nvariable *and* the location 1 data into `lm`. Pipes are\nfor linear sequences of things to do, and this one diverges. So, save\nwhat you need to save, and then do it without a pipe.\n\nThis is one of those explanatory-variable-less repeated measures where\nyou *only* have things varying over time. Here, the treatment was\nlocation, and we're only looking at one of those locations. We still\nhave a within-subject design (the four times), but there is no\nbetween-subject design left.\n\nThe conclusion is that there is very strong evidence of a time effect\nfor location 1, as we would have guessed.\n\nNow, let's see whether the mixed-model analysis comes to the same\nconclusion. This time, we have to start from the *long* data set\n(the one I'm using is the one called `fairport` with things as\nfactors) and pull out the rows for location 1. The only remaining\nfixed effect is time:\n\n::: {.cell}\n\n```{.r .cell-code}\nfairport %>%\n  filter(location == 1) %>%\n  lmer(epinephrine ~ when + (1 | fchild), data = .) %>%\n  drop1(test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nepinephrine ~ when + (1 | fchild)\n       npar    AIC    LRT   Pr(Chi)    \n<none>      4964.1                     \nwhen      3 5032.1 74.048 5.796e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nThe effect of time is, once again, substantial. The P-value is not the\nsame, because the test is not the same, but it is very similar and\npoints to exactly the same conclusion.\n\nI should perhaps point out that you don't *have* to do these\nmodels in a pipe. I just did because it was easier for me. But there's\nnothing wrong with doing it like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp <- fairport %>% filter(location == 1)\ntmp.1 <- lmer(epinephrine ~ when + (1 | child), data = tmp)\ndrop1(tmp.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nepinephrine ~ when + (1 | child)\n       npar    AIC    LRT   Pr(Chi)    \n<none>      4964.1                     \nwhen      3 5032.1 74.048 5.796e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nand it gives exactly the same result.\n\nThis last model (in either of its versions) is a so-called random\nintercepts model. What it says is that epinephrine level for a child\ndepends on time, but the effect of being one child rather than another\nis to shunt the mean epinephrine level up or down by a fixed amount\nfor all times. It seems to me that this is a very reasonable way to\nmodel the child-to-child variation in this case, but in other cases,\nthings might be different. `lmer` allows more sophisticated\nthings, like for example the random child effect depending linearly on\ntime. To do that, you'd rewrite the above like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nfairport %>%\n  filter(location == 1) %>%\n  lmer(epinephrine ~ when + (1 + when | fchild), data = .) %>%\n  drop1(test = \"Chisq\")\n```\n:::\n\n \n\nThe change is on the `lmer` line: the bit in brackets has a\nlinear model in `when` for each child. I didn't run that, so\nI'm not certain it works,^[I'm suspicious about *when*  needing to be the numerical time inside. Not sure.] but that's the idea.\n\nAll right, let's get on to location 2. This was the control location,\nso we expect to see *no* dependence of epinephrine level on time,\neither in the profile analysis or in the mixed-model analysis. There\nis a large amount of copying and pasting coming up:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>% filter(location == 2) -> loc2\nloc2 %>% select(epi_1:epi_4) %>% as.matrix() -> response\nloc2.1 <- lm(response ~ 1, data = loc1)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\nloc2.2 <- Manova(loc2.1, idata = times.df, idesign = ~times)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNote: model has only an intercept; equivalent type-III tests substituted.\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(loc2.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType III Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   105275213\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.961466 2470.169      1     99 < 2.22e-16 ***\nWilks             1  0.038534 2470.169      1     99 < 2.22e-16 ***\nHotelling-Lawley  1 24.951203 2470.169      1     99 < 2.22e-16 ***\nRoy               1 24.951203 2470.169      1     99 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1    times2     times3\ntimes1  307.1984  5502.882   749.4117\ntimes2 5502.8823 98573.811 13424.3048\ntimes3  749.4117 13424.305  1828.1931\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.3023560 14.01313      3     97 1.1613e-07 ***\nWilks             1 0.6976440 14.01313      3     97 1.1613e-07 ***\nHotelling-Lawley  1 0.4333957 14.01313      3     97 1.1613e-07 ***\nRoy               1 0.4333957 14.01313      3     97 1.1613e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n              Sum Sq num Df Error SS den Df  F value    Pr(>F)    \n(Intercept) 26318803      1  1054811     99 2470.169 < 2.2e-16 ***\ntimes          65694      3   566979    297   11.471 3.884e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n      Test statistic    p-value\ntimes        0.77081 0.00011477\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n       GG eps Pr(>F[GG])    \ntimes 0.84304  2.433e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n         HF eps   Pr(>F[HF])\ntimes 0.8669702 1.838348e-06\n```\n\n\n:::\n:::\n\n \n\nI think I changed everything I needed to. \n\nThere actually *is* still an effect of time. This is true even though sphericity fails and so the Huynh-Feldt adjustment to the P-value is fairly substantial; even so, it is still clearly significant.\n\nWhat kind of effect?\nWe can look at that by finding epinephrine means at each time. That\nwould be easier if we had long format, but I think we can still do\nit. The magic word is `across`:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>%\n  filter(location == 2) %>%\n  summarize(across(starts_with(\"epi\"), \\(epi) mean(epi)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 4\n  epi_1 epi_2 epi_3 epi_4\n  <dbl> <dbl> <dbl> <dbl>\n1  249.  279.  251.  247.\n```\n\n\n:::\n:::\n\n(in words, \"for each column that starts with `epi`, find the mean of it.\")\n \n\nThe message here is that the mean epinephrine levels at time 2 were\nhigher than the others, so that's what seems to be driving the time\neffect. And remember, these were the control children, so they had no\nnew airport nearby.\n\nI want to try this:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>% nest_by(location)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Rowwise:  location\n  location               data\n     <dbl> <list<tibble[,5]>>\n1        1          [100 x 5]\n2        2          [100 x 5]\n```\n\n\n:::\n:::\n\n \n\nThis is like the idea in the Spilich problem: I want to write a\nfunction that calculates the means of the `epi_1` through\n`epi_4` columns of a data frame like `airport`, and\nthen apply it to each of those \"nested\" data frames.\n\n::: {.cell}\n\n```{.r .cell-code}\nepi.means <- function(x) {\n  x %>% summarize(across(starts_with(\"epi\"), \\(epi) mean(epi)))\n}\n\nepi.means(airport)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 4\n  epi_1 epi_2 epi_3 epi_4\n  <dbl> <dbl> <dbl> <dbl>\n1  248.  309.  304.  298.\n```\n\n\n:::\n:::\n\n \n\nOK, and then:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>%\n  nest_by(location) %>%\n  rowwise() %>% \n  mutate(means = list(epi.means(data))) %>%\n  unnest(means)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 6\n  location               data epi_1 epi_2 epi_3 epi_4\n     <dbl> <list<tibble[,5]>> <dbl> <dbl> <dbl> <dbl>\n1        1          [100 x 5]  247.  340.  356.  349.\n2        2          [100 x 5]  249.  279.  251.  247.\n```\n\n\n:::\n:::\n\n \n\nOoh, nice. There are the means for the four time points at both of the\ntwo locations. At location 1, epinephrine starts out low, jumps high\nafter the airport is built, and stays there, while at location 2, the\nmean is mysteriously higher at time 2 and then epinephrine levels go\nback down again.\n\nEnough of that. Back to the mixed model analysis, with more copying\nand pasting. Here is the \"simple time effect\" at location 2:\n\n::: {.cell}\n\n```{.r .cell-code}\nfairport %>%\n  filter(location == 2) %>%\n  lmer(epinephrine ~ when + (1 | fchild), data = .) %>%\n  drop1(test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nepinephrine ~ when + (1 | fchild)\n       npar    AIC    LRT   Pr(Chi)    \n<none>      4336.8                     \nwhen      3 4363.7 32.889 3.399e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nThis also finds a time effect at location 2, with a very similar P-value.\n\nSo which method is preferred? They are, I think, two different ways of\napproaching the same problem, and so there is no good reason to prefer\none way over the other. The profile-analysis way is easier to follow\nif you are coming from a MANOVA direction: set it up the same way as\nyou would a MANOVA, with your repeated measures as your multivariate\nresponse, and distinguish between the within-subject design (times,\nfor us) and the between-subject design (treatments, stuff like\nthat). As for mixed models: when you get your head around that crazy\nway of specifying the random effects, which are typically \"subjects\"\nin this kind of analysis, the actual model statement is brief:\nresponse depends on fixed effects plus random effects with the\nbrackets and vertical bar. I always find the `lmer` models look\nvery simple once you have figured them out (like `ggplot` in\nthat regard). I also like the fact that `lmer` uses \"tidy data\", \nso that you can make graphs and do this flavour of analysis\nwith the same data frame. Having gotten my head around getting my data\ntidy, it seems odd to me that profile analysis requires the data to be\nuntidy, although it does so for a good reason: if you were genuinely\ndoing a MANOVA, you would *want* your multiple response\nvariables each in their own column. Doing repeated measures this way\nis thinking of your responses at different times like different\nvariables. \n\nThe approach I've taken in this course is for historical reasons. That\nis to say, *my* history, not because the historical perspective\nis necessarily the best one.  It began with doing repeated measures in\nSAS (this was many years ago; I don't teach SAS any more), and the only way I could get my\nhead around that was the profile-analysis way. I'm not even sure there\nwas software for doing mixed models in those days, certainly not in\nSAS. Besides, random effects scared me back then. Anyway, when I\nmoved things over to R, I\nrealized I was using basically the same idea in R: make it into a\nMANOVA and tweak things as needed. So I developed things by showing\nyou MANOVA first as ANOVA when you have lots of $y$-variables instead\nof just one, and then using it for repeated measures by thinking of\nthe repeated measures as \"different-but-related\" $y$-variables. To\nmy mind, this is a reasonable way of doing it in a course like this:\nyou get to see MANOVA, which is worth knowing about anyway, and then\nyou get to apply it to a very common situation.\n\nMixed models are actually quite recent, as a way of doing this kind of\nmodelling. Back when I was learning this stuff, you learned to\ndistinguish between \"fixed effects\", whose levels were the only ones\nyou cared about (like the two locations here), and \"random effects\",\nwhere the ones you observed were only a sample of the ones you might\nhave seen (like \"children\" or in general \"subjects\"). Then you\nwould test the fixed effects by hypothesizing that their mean effect\nwas zero and seeing whether you could reject that (in the same way\nthat you do in regression or regular ANOVA: in the latter case, you\ntest that the effects of all the different groups are zero, which\ncomes down to saying that all the groups have the same mean). The way\nyou handled random effects was to estimate their *variance*, and\nthen you would test whether a random effect exists or not by testing\nwhether the variance of that random effect is zero (does not exist) or\nis greater than zero. Zero is the smallest a variance (or SD) can be,\nwhich means that testing for it has always been kind of flakey and the\nstandard theory doesn't work for it (the technical term is \n\"on the boundary of the parameter space\", and the theory applies when the\nnull-hypothesis value is strictly *inside* the set of possible\nvalues the parameter can take). \n\nBack when we did this stuff by\nhand,^[I understand psychology students *still* do this  kind of stuff by hand.] we had to figure out whether we were testing\na fixed or a random effect, and there were rules, involving things\ncalled \"expected mean squares\", that told you how to get the tests\nright. Anyway, that is all considered rather old-fashioned now, and\nmixed models are where it is at. In these, you typically *only*\ntest the fixed effects, while estimating the size of the random\neffects, taking it for granted that they exist. This is an active area\nof research; the things that `lmer` fits are called \n\"linear mixed models\", and there are also now \n\"generalized linear mixed models\", things like logistic regression with random effects.\n\nWe haven't had to worry about this up to now because in most of the\nexperimental designs we have used, each subject only contributes\n*one*  measurement. In that case, you cannot separate out a\nsubject-specific \"random effect\" from random error, and so we lump\nit all into random error. The one experimental design where a subject\ngives us more than one measurement is matched pairs, and the way we\nhave handled that so far is to turn the two measurements into one by\ntaking the difference between them; even in that case, each subject\ncontributes only one *difference* to the analysis. Repeated\nmeasures, though, is genuinely different: you can't wish away multiple\nmeasurements on the same subject by taking differences any more, so\nyou have to face up to the issue at last. Either we think of it as\nseveral different measurements that might be correlated (the\nprofile-analysis MANOVA way), or we face up to the idea that different\nsubjects bring their own random effect to the table that shows up in\nevery measurement that the subject provides (mixed models).\n\nI did analysis of covariance as a separate mini-section, thinking of\nit as a regression, but if the numerical covariate is the same thing\nas the response but measured at a different time (eg., before rather than\nafter), that would also respond to a repeated-measures approach, or to\nthe taking of differences as matched pairs does. There is almost\nalways more than one way to look at these things.\n\n$\\blacksquare$\n\n\n\n\n\n##  Body fat as repeated measures\n\n\n This one is also stolen from STAC32. Athletes are concerned\nwith measuring their body fat percentage. Two different methods are\navailable: one using ultrasound, and the other using X-ray\ntechnology. We are interested in whether there is a difference in the\nmean body fat percentage as measured by these two methods, and if so,\nhow big that difference is. Data on 16 athletes are at\n[link](http://ritsokiguess.site/datafiles/bodyfat.txt).\n\n\n\n(a) Read in the data and check that you have a sensible number of\nrows and columns.\n\n\n\nSolution\n\n\nThis kind of thing:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/bodyfat.txt\"\nbodyfat <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 16 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\ndbl (3): athlete, xray, ultrasound\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nbodyfat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 16 x 3\n   athlete  xray ultrasound\n     <dbl> <dbl>      <dbl>\n 1       1  5          4.75\n 2       2  7          3.75\n 3       3  9.25       9   \n 4       4 12         11.8 \n 5       5 17.2       17   \n 6       6 29.5       27.5 \n 7       7  5.5        6.5 \n 8       8  6          6.75\n 9       9  8          8.75\n10      10  8.5        9.5 \n11      11  9.25       9.5 \n12      12 11         12   \n13      13 12         12.2 \n14      14 14         15.5 \n15      15 17         18   \n16      16 18         18.2 \n```\n\n\n:::\n:::\n\n \n\n16 rows (athletes) and 3 columns, one for each measurement\nmethod and one labelling the athletes. All good.\n\n\n\n$\\blacksquare$\n\n(b) Carry out a suitable (matched-pairs) $t$-test to determine\nwhether the means are the same or different. \n\n\n\nSolution\n\n\nFeed the two columns into `t.test` along with\n`paired=T`. Remember that this is (in effect) a one-sample\n$t$-test, so that you can't use a `data=`. You therefore have\nto wrap everything in a `with`:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(bodyfat, t.test(xray, ultrasound, paired = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  xray and ultrasound\nt = -0.30801, df = 15, p-value = 0.7623\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.7425068  0.5550068\nsample estimates:\nmean difference \n       -0.09375 \n```\n\n\n:::\n:::\n\n \n\nThe test we want is two-sided, so we didn't have to take any special\nsteps to get that.\n\n$\\blacksquare$\n\n\n\n(c) What do you conclude from the test?\n\n\n\nSolution\n\n\nThe P-value of 0.7623 is not at all small, so there is no way we can\nreject the null hypothesis.^[My hat stays on my head.] There\nis no evidence of a difference in means; we can act as if the two\nmethods produce the same mean body fat percentage. \nThat is to say, on this evidence we can use either method, whichever\none is cheaper or more convenient.\n\n \n$\\blacksquare$\n\n(d) Run a repeated-measures analysis of variance, treating the two\nmethods of measuring body fat as the repeated measures (ie., playing\nthe role of \"time\" that you have seen in most of the other repeated\nmeasures analyses). There is no \"treatment\" here, so there is\nnothing to go on the right side of the squiggle. Insert a `1`\nthere to mean \"just an intercept\". Display the results.\n\n\n\nSolution\n\n\nConstruct the response variable, run `lm`, construct the\nwithin-subjects part of the design, run `Manova`:\n\n::: {.cell}\n\n```{.r .cell-code}\nbodyfat %>%\n  select(xray:ultrasound) %>%\n  as.matrix() -> response\nbodyfat.1 <- lm(response ~ 1)\nmethods <- colnames(response)\nmethods.df <- data.frame(methods=factor(methods))\nbodyfat.2 <- Manova(bodyfat.1, idata = methods.df, idesign = ~methods)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNote: model has only an intercept; equivalent type-III tests substituted.\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(bodyfat.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType III Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n           (Intercept)\nxray                 1\nultrasound           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)        9025\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.797341 59.01586      1     15 1.4135e-06 ***\nWilks             1  0.202659 59.01586      1     15 1.4135e-06 ***\nHotelling-Lawley  1  3.934390 59.01586      1     15 1.4135e-06 ***\nRoy               1  3.934390 59.01586      1     15 1.4135e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: methods \n\n Response transformation matrix:\n           methods1\nxray             -1\nultrasound        1\n\nSum of squares and products for the hypothesis:\n         methods1\nmethods1 0.140625\n\nMultivariate Tests: methods\n                 Df test stat   approx F num Df den Df  Pr(>F)\nPillai            1 0.0062849 0.09486999      1     15 0.76231\nWilks             1 0.9937151 0.09486999      1     15 0.76231\nHotelling-Lawley  1 0.0063247 0.09486999      1     15 0.76231\nRoy               1 0.0063247 0.09486999      1     15 0.76231\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n            Sum Sq num Df Error SS den Df F value    Pr(>F)    \n(Intercept) 4512.5      1  1146.94     15 59.0159 1.413e-06 ***\nmethods        0.1      1    11.12     15  0.0949    0.7623    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nThis is an unusual one in that the repeated measures variable is not time but method, and there is no actual\ntreatment. The consequence (with there being only two methods) is that sphericity is automatically good, and the univariate test for methods (at the bottom) has exactly the same P-value as all the multivariate tests, so that the conclusion is the same either way. (Also, see below about the matched pairs.)\n\n$\\blacksquare$\n\n\n(e) Compare your repeated-measures analysis to your matched-pairs\none. Do you draw the same conclusions?\n\n\n\nSolution\n\n\nThe P-value for `methods`, which is testing the same thing\nas the matched pairs, is 0.7623, which is actually\n*identical* to the matched pairs $t$-test, and so the\nconclusion is identical also. That is, there is no difference\nbetween the two methods for measuring body fat. \nThis goes to show that repeated measures gives the same answer as\na matched-pairs $t$-test in the situation where they both\napply. But repeated measures is, as we have seen, a lot more general.\n\nSince this really is repeated measures, we ought to be able to use\na mixed model here too. We need \"long\" or \"tidy\" format, which\nwe don't have yet. One pipe to save them all, to paraphrase Lord\nof the Rings^[The movies of Lord of the Rings were filmed      in New Zealand, which is also the country in which R was first      designed.] --- put all the fat measurements in one column with a\nlabel saying which `method` they were obtained with; create\na column which is the athlete number as a factor; fit the linear\nmixed model; see what we can drop from it:\n\n::: {.cell}\n\n```{.r .cell-code}\nbodyfat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 16 x 3\n   athlete  xray ultrasound\n     <dbl> <dbl>      <dbl>\n 1       1  5          4.75\n 2       2  7          3.75\n 3       3  9.25       9   \n 4       4 12         11.8 \n 5       5 17.2       17   \n 6       6 29.5       27.5 \n 7       7  5.5        6.5 \n 8       8  6          6.75\n 9       9  8          8.75\n10      10  8.5        9.5 \n11      11  9.25       9.5 \n12      12 11         12   \n13      13 12         12.2 \n14      14 14         15.5 \n15      15 17         18   \n16      16 18         18.2 \n```\n\n\n:::\n\n```{.r .cell-code}\nbodyfat %>%\n  pivot_longer(-athlete, names_to=\"method\", values_to=\"fat\") %>%\n  mutate(fathlete = factor(athlete)) %>%\n  lmer(fat ~ method + (1 | fathlete), data = .) %>%\n  drop1(test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfat ~ method + (1 | fathlete)\n       npar    AIC     LRT Pr(Chi)\n<none>      161.34                \nmethod    1 159.44 0.10088  0.7508\n```\n\n\n:::\n:::\n\n     \n\nOnce again, there is no difference between methods, and though the\nP-value is different from the matched pairs or profile analysis, it is\nvery close to those.\n\nIf you're not clear about the tidy data frame used for input to\n`lmer`, pull the top two lines off the pipeline and see what they produce:\n\n::: {.cell}\n\n```{.r .cell-code}\nbodyfat %>%\n  pivot_longer(-athlete, names_to=\"method\", values_to=\"fat\") %>%\n  mutate(fathlete = factor(athlete))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 32 x 4\n   athlete method       fat fathlete\n     <dbl> <chr>      <dbl> <fct>   \n 1       1 xray        5    1       \n 2       1 ultrasound  4.75 1       \n 3       2 xray        7    2       \n 4       2 ultrasound  3.75 2       \n 5       3 xray        9.25 3       \n 6       3 ultrasound  9    3       \n 7       4 xray       12    4       \n 8       4 ultrasound 11.8  4       \n 9       5 xray       17.2  5       \n10       5 ultrasound 17    5       \n# i 22 more rows\n```\n\n\n:::\n:::\n\n \n\nEach athlete now appears twice: once with their `fat` measured\nby `xray`, and again with it measured by\n`ultrasound`. The column `fathlete` is a factor.\n\nThe mixed model took me two goes to get right: I forgot that I needed the\n`data=.` in `lmer`, because it works like `lm`\nwith the model formula first, not the input data. If the pipeline is\ngoing too fast for you, create the tidy data frame and save it, then\nuse the saved data frame as input to `lmer`.\n\n$\\blacksquare$\n\n\n\n\n\n\n\n##  Investigating motor activity in rats\n\n\nA researcher named King was investigating\nthe effect of the drug midazolam on motor activity in rats. Typically,\nthe first time the drug is injected, a rat's motor activity decreases\nsubstantially, but rats typically develop a \"tolerance\", so that\nfurther injections of the drug have less impact on the rat's motor\nactivity.\n\nThe data shown in\n[link](http://ritsokiguess.site/datafiles/king.csv) were all taken\nin one day, called the \"experiment day\" below. 24 different rats\nwere used. Each rat, on the experiment day, was injected with a fixed\namount of midazolam, and at each of six five-minute intervals after\nbeing injected, the rat's motor activity was measured (these are\nlabelled `i1` through `i6` in the data). The rats\ndiffered in how they had been treated before the experiment day. The\ncontrol group of rats had previously been injected repeatedly with a\nsaline solution (no active ingredient), so the experiment day was the\nfirst time this group of rats had received midazolam. The other two\ngroups of rats had both received midazolam repeatedly before the\nexperiment day: the \"same\" group was injected on experiment day in\nthe same environment that the previous injections had taken place (this\nis known in psychology as a \"conditioned tolerance\"), but the\n\"different\" group had the previous injections in a different\nenvironment than on experiment day.\n\nThe column `id` identifies the rat from which each sequence of\nvalues was obtained.\n\n\n\n(a) Explain briefly why we need to use a repeated measures\nanalysis for these data.\n\n\nSolution\n\n\nEach rat is measured at six different times (`i1` through\n`i6`): that is to say, each row of the data set consists of\nrepeated measurements on the *same* rat. (If each row had\nused six *different* rats to obtain the six measurements, we\nwould have been back in familiar territory and could have used a\nregular analysis of variance.)\n    \n$\\blacksquare$\n\n(b) Read in the data and note that you have what was promised\nin the question.\n\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/king.csv\"\nking <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 24 Columns: 8\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): context\ndbl (7): id, i1, i2, i3, i4, i5, i6\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nking\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 24 x 8\n      id context    i1    i2    i3    i4    i5    i6\n   <dbl> <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1   101 control   150    44    71    59   132    74\n 2   102 control   335   270   156   160   118   230\n 3   103 control   149    52    91   115    43   154\n 4   104 control   159    31   127   212    71   224\n 5   105 control   292   125   184   246   225   170\n 6   106 control   297   187    66    96   209    74\n 7   107 control   170    37    42    66   114    81\n 8   108 control   159     0    35    75    71    34\n 9   109 same      346   175   177   192   239   140\n10   110 same      426   329   236    76   102   232\n# i 14 more rows\n```\n\n\n:::\n:::\n\n     \n\nThere are 24 rats (rows). The columns label the rat (`id`) and\nthe times at which motor activity was measured (`i1` through\n`i6`). The remaining column, `context`, describes how\nthe rats were treated before experiment day, with the levels being the\nsame ones given in the question.\n    \n$\\blacksquare$\n\n(c) We are going to do a repeated-measures analysis using the\n\"profile\" method shown in class. Create a suitable response\nvariable for this method.\n\n\nSolution\n\n\n`cbind` the appropriate columns together, to make a matrix:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(king, cbind(i1, i2, i3, i4, i5, i6))\n```\n:::\n\n     \n\nThis is the \"simple\" but tedious way, and produces a matrix because\nthe `i1` through `i6` are vectors (single columns):\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"matrix\" \"array\" \n```\n\n\n:::\n:::\n\n \n\n`i1:i6` does not work here, because we are outside of the\n`tidyverse`, and in that world, `:` only means\n\"through\" (as in \"this through that\") when the things on either\nside of it are or represent numbers.\n\nThe clever way to get the response is to `select` the columns\nand then turn them into a matrix. This *does* permit the colon\nbecause we are now in the `tidyverse`:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- king %>%\n  select(i1:i6) %>%\n  as.matrix()\nclass(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"matrix\" \"array\" \n```\n\n\n:::\n:::\n\n \nIt is indeed a matrix.\n\nI tried to be extra-clever and use `starts_with`, but I have\nanother column `id` that starts with `i` that I do\n*not* want to be part of the response. So I had to abandon that\nidea, but not before trying this:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- king %>%\n  select(matches(\"i[0-9]\")) %>%\n  as.matrix()\nhead(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      i1  i2  i3  i4  i5  i6\n[1,] 150  44  71  59 132  74\n[2,] 335 270 156 160 118 230\n[3,] 149  52  91 115  43 154\n[4,] 159  31 127 212  71 224\n[5,] 292 125 184 246 225 170\n[6,] 297 187  66  96 209  74\n```\n\n\n:::\n:::\n\n \n\n`head` displays the first six lines (of anything). We don't\nnormally need it because we are typically dealing with\n`tibble`-like data frames that display only ten rows of\nthemselves by default. But this worked. The `matches` part\ntakes a so-called \"regular expression\" which is a very flexible way\nof matching anything: in this case, a column whose name starts with\n`i` followed by exactly one digit (something between 0 and 9\ninclusive). \n    \n$\\blacksquare$\n\n(d) Set up the \"within-subjects\" part of the analysis. That\nmeans getting hold of the names of the columns that hold the\ndifferent times, saving them, and also making a data frame out of them:\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\n```\n:::\n\n   \n    \n$\\blacksquare$\n\n(e) Fit the repeated-measures ANOVA. This will involve fitting\nan `lm` first, if you have not already done so.\n\n\nSolution\n\n\nFit the `lm` first, and then pass that into `Manova`\nfrom `car`:\n\n::: {.cell}\n\n```{.r .cell-code}\nking.1 <- lm(response ~ context, data = king)\nking.2 <- Manova(king.1, idata = times.df, idesign = ~times)\nsummary(king.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\ni1           1\ni2           1\ni3           1\ni4           1\ni5           1\ni6           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    24399650\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df    Pr(>F)    \nPillai            1  0.913261 221.1069      1     21 1.273e-12 ***\nWilks             1  0.086739 221.1069      1     21 1.273e-12 ***\nHotelling-Lawley  1 10.528902 221.1069      1     21 1.273e-12 ***\nRoy               1 10.528902 221.1069      1     21 1.273e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: context \n\n Response transformation matrix:\n   (Intercept)\ni1           1\ni2           1\ni3           1\ni4           1\ni5           1\ni6           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     1611524\n\nMultivariate Tests: context\n                 Df test stat approx F num Df den Df    Pr(>F)   \nPillai            2 0.4101695 7.301725      2     21 0.0039141 **\nWilks             2 0.5898305 7.301725      2     21 0.0039141 **\nHotelling-Lawley  2 0.6954024 7.301725      2     21 0.0039141 **\nRoy               2 0.6954024 7.301725      2     21 0.0039141 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3 times4 times5\ni1      1      0      0      0      0\ni2      0      1      0      0      0\ni3      0      0      1      0      0\ni4      0      0      0      1      0\ni5      0      0      0      0      1\ni6     -1     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n            times1      times2       times3      times4      times5\ntimes1 463982.0417 18075.41667 -17241.16667 -47691.2917  973.291667\ntimes2  18075.4167   704.16667   -671.66667  -1857.9167   37.916667\ntimes3 -17241.1667  -671.66667    640.66667   1772.1667  -36.166667\ntimes4 -47691.2917 -1857.91667   1772.16667   4902.0417 -100.041667\ntimes5    973.2917    37.91667    -36.16667   -100.0417    2.041667\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.856997 20.37569      5     17 1.2326e-06 ***\nWilks             1  0.143003 20.37569      5     17 1.2326e-06 ***\nHotelling-Lawley  1  5.992849 20.37569      5     17 1.2326e-06 ***\nRoy               1  5.992849 20.37569      5     17 1.2326e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: context:times \n\n Response transformation matrix:\n   times1 times2 times3 times4 times5\ni1      1      0      0      0      0\ni2      0      1      0      0      0\ni3      0      0      1      0      0\ni4      0      0      0      1      0\ni5      0      0      0      0      1\ni6     -1     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1    times2    times3    times4    times5\ntimes1 40376.583 44326.333 28404.667 -6315.458  9666.458\ntimes2 44326.333 93153.083 56412.417  3080.292 24377.208\ntimes3 28404.667 56412.417 34289.083  1235.458 14606.042\ntimes4 -6315.458  3080.292  1235.458  3241.583  1586.167\ntimes5  9666.458 24377.208 14606.042  1586.167  6573.083\n\nMultivariate Tests: context:times\n                 Df test stat approx F num Df den Df    Pr(>F)   \nPillai            2 0.8033948 2.417022     10     36 0.0256284 * \nWilks             2 0.3347058 2.476886     10     34 0.0237616 * \nHotelling-Lawley  2 1.5750953 2.520152     10     32 0.0230293 * \nRoy               2 1.2432100 4.475556      5     18 0.0079604 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n               Sum Sq num Df Error SS den Df  F value    Pr(>F)    \n(Intercept)   4066608      1   386233     21 221.1069 1.273e-12 ***\ncontext        268587      2   386233     21   7.3017 0.0039141 ** \ntimes          407439      5   273592    105  31.2736 < 2.2e-16 ***\ncontext:times   88901     10   273592    105   3.4119 0.0006746 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n              Test statistic  p-value\ntimes                0.24793 0.022324\ncontext:times        0.24793 0.022324\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n               GG eps Pr(>F[GG])    \ntimes         0.70016  3.152e-14 ***\ncontext:times 0.70016   0.003269 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                 HF eps   Pr(>F[HF])\ntimes         0.8573896 6.269000e-17\ncontext:times 0.8573896 1.422381e-03\n```\n\n\n:::\n:::\n\n     \n    \n$\\blacksquare$\n\n(f) What do you conclude from your repeated-measures ANOVA?\nExplain briefly, in the context of the data.\n\n\nSolution\n\n\nThe interaction term is significant, with a P-value less than\n0.05. This is where we start and stop looking.\n\nTo be precise, the sphericity test fails (P-value 0.022), so that the P-value in the univariate test for interaction is too small, and we should trust the Huynh-Feldt one of 0.0014 below it. This is still significant, but not as significant as you might have thought. For comparison, the multivariate test P-values vary between 0.008 and 0.026, a bit bigger but still significant.\n\n\nThis means that the effect of time on motor activity (that is, the\nway the motor activity depends on time) is different for each\n`context`. That's all we can say now.\nGrading note: as long as the setup and MANOVA are done somewhere,\nI don't mind which part they are labelled with. But you need to do\nthe setup, initial `lm` and `Manova`\n*somewhere* so that everything comes out right in the end.\n    \n$\\blacksquare$\n\n(g) To understand the results of the previous part, we are going\nto make a spaghetti plot. In preparation for that, we need to save\nthe data in \"long format\" with one observation on *one* time\npoint in each row. Arrange that, and show by displaying (some of)\nthe data that you have done so.\n\n\nSolution\n\n\nThis is `pivot_longer` yet again: gather up columns `i1`\nthrough `i6` and call them something like `activity`:\n\n::: {.cell}\n\n```{.r .cell-code}\nking %>% \n  pivot_longer(i1:i6, names_to=\"time\", values_to=\"activity\") -> king.long\nking.long\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 144 x 4\n      id context time  activity\n   <dbl> <chr>   <chr>    <dbl>\n 1   101 control i1         150\n 2   101 control i2          44\n 3   101 control i3          71\n 4   101 control i4          59\n 5   101 control i5         132\n 6   101 control i6          74\n 7   102 control i1         335\n 8   102 control i2         270\n 9   102 control i3         156\n10   102 control i4         160\n# i 134 more rows\n```\n\n\n:::\n:::\n\n     \nDisplaying the top 10 rows of the resulting data frame is a good way to display \"some of\" it. \nYou can always look at more if you like. There are more rows\nand fewer columns than there were before, which is\nencouraging. `pivot_longer` collects up all the time and activity values for the first rat, then the second, and so on. \n\n    \n$\\blacksquare$\n\n(h) Make a spaghetti plot: that is, plot motor activity against\nthe time points, joining the points for each *rat* by lines,\nand colouring the points and lines according to the *context*.\n\n\nSolution\n\n\nThat means this, using `group` to indicate which points to\njoin by lines, since it's different from the `colour`: \n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(king.long, aes(x = time, y = activity, colour = context, group = id)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/king-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nI'd say the `geom_point` is optional, so that this is also\ngood, perhaps better even:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(king.long, aes(x = time, y = activity, colour = context, group = id)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/king-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n    \n$\\blacksquare$\n\n(i) Looking at your spaghetti plot, why do you think your\nrepeated-measures ANOVA came out as it did? Explain briefly.\n\n\n\nSolution\n\n\nWhat you're after is an explanation of how the *patterns* over\ntime are different for the three `context`s. If you can find\nsomething that says that, I'm good. For example, even though all of\nthe rats experienced a decrease in motor activity between times 1\nand 2, the rats in the `same` group didn't decrease as\nmuch. Or, the rats in the `same` group continued to decrease\nbeyond time 2, whereas the rats in the `control` and\n`different` groups tended to level off after time 2, not\ndecreasing so much after that.\nIf you like, you can draw an interaction plot by working out the\nmeans for each `context`-`time` group first:\n\n::: {.cell}\n\n```{.r .cell-code}\nking.long %>%\n  group_by(context, time) %>%\n  summarize(m = mean(activity)) %>%\n  ggplot(aes(x = time, y = m, colour = context, group = context)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'context'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/king-11-1.pdf){fig-pos='H'}\n:::\n:::\n\n   \n\nThis seems to illustrate the same things as I found on the spaghetti\nplot. It gains in clarity by only looking at means, but loses by not\nconsidering the variability. Your call.\n\nThis kind of thing also runs with `lmer` from package\n`lme4`. It uses the long data frame, thus, treating `id`\n(identifying the rats) as a random effect:\n\n::: {.cell}\n\n```{.r .cell-code}\nking.3 <- lmer(activity ~ context * time + (1 | id), data = king.long)\n```\n:::\n\n \n\nWhat can we drop? The only thing under consideration is the interaction:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(king.3, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nactivity ~ context * time + (1 | id)\n             npar    AIC    LRT   Pr(Chi)    \n<none>            1609.0                     \ncontext:time   10 1622.7 33.764 0.0002025 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nand we get the same conclusion as before, but with a much smaller P-value.\n\nWith this kind of modelling, there is no distinction between\n\"within\" and \"between\", so that even though `time` is a\nwithin-subjects factor and `context` is between subjects, if\nthe interaction had not been significant, we could have dropped it\nfrom the model, and then we would have had an effect of `time`\nand an effect of `context`, independent of each other. I was\nactually looking for an example with a non-significant interaction,\nbut I couldn't find one. \n  \n$\\blacksquare$\n\n\n\n\n\n\n##  Repeated measures with no background\n\n\n Nine people are randomly chosen to receive one of three\ntreatments, labelled A, B and C. Each person has their response\n`y` to the treatment measured at three times, labelled T1, T2\nand T3. The main aim of the study is to properly assess the effects of\nthe treatments. A higher value of `y` is better.\n\nThe data are in [link](http://ritsokiguess.site/datafiles/rm.txt).\n\n\n\n(a) There are $9 \\times 3=27$ observations  of `y` in\nthis study. Why would it be wrong to treat these as 27 independent\nobservations? Explain briefly.\n\nSolution\n\n\nThere are only 9 people with 3 observations on each person. The\nthree observations on the same person are likely to be correlated\nwith each other, and so treating them as independent would be a\nmistake.\nThis is repeated-measures data. If you say that, that's useful,\nbut you also need to demonstrate that you know what repeated\nmeasures *means* and why it needs to be handled differently\nfrom one-observation-per-individual data. Another way to look at\nit is that individuals will differ from each other, and so there\nought to be an \"individual\" effect included in the model, in the\nsame way that you would include a block effect in a randomized\nblock design: not because you care about differences among\nindividuals, but because you are pretty sure they'll be there and\nyou want to account for them.\n\n$\\blacksquare$\n\n(b) Read in the data values. Are they tidy or untidy?  Explain\nbriefly. (The data values are separated by *tabs*, like the\nAustralian athlete data.)\n\nSolution\n\n\nWe therefore need `read_tsv`. I'm not quite sure what to\ncall this one:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/rm.txt\"\ntreatments <- read_tsv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 27 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr (2): trt, time\ndbl (2): subject, y\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ntreatments\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 27 x 4\n   trt   time  subject     y\n   <chr> <chr>   <dbl> <dbl>\n 1 A     T1          1    10\n 2 A     T1          2    12\n 3 A     T1          3    13\n 4 A     T2          1    16\n 5 A     T2          2    19\n 6 A     T2          3    20\n 7 A     T3          1    25\n 8 A     T3          2    27\n 9 A     T3          3    28\n10 B     T1          4    12\n# i 17 more rows\n```\n\n\n:::\n:::\n\n     \nFind a way to display what you have, so you can decide whether it is\ntidy or not.\nEach observation of `y` is in a row by itself, so this is\ntidy, or long format. You might even call this extra-tidy, because\neach person is spread over three rows, one for each time point.\nLooking ahead, this is ideal for making a graph, or for doing the\nadvanced version of the analysis with `lme4`, but it is not\nso good for our MANOVA way of doing a repeated measures\nanalysis. That we will have to prepare for.\n\n$\\blacksquare$\n\n(c) Make a spaghetti plot: that is, a plot of `y`\nagainst time, with the observations for the same individual joined\nby lines which are coloured according to the treatment that\nindividual received.\n\nSolution\n\n\nThe individuals are labelled in `subject` and the\ntreatments are in `trt`, which means we need to do this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(treatments, aes(x = time, y = y, colour = trt, group = subject)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/bock-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nI'm going to be all smug and tell you that I got this right first\ntime. (I'm telling you this because it doesn't happen often.) If it didn't come out that way for you, no stress: figure out what went wrong, fix it, and no one is any the wiser. On a spaghetti plot, you want to join together the points that belong to the same *subject*, and you want to colour them by *treatment*, so `colour` needs to be `trt` and `group` needs to be `subject`, since otherwise *all* the observations on the same treatment will get connected by lines, which is not what you want. You can use `group` in other contexts too, but for us we would normally distinguish groups by colour, and here we have two grouping variables (subject and treatment), which we want to distinguish in different ways.\n\n$\\blacksquare$\n\n(d) On your spaghetti plot, how do the values of `y` for\nthe treatments compare over time?\n\nSolution\n\n\nThe most obvious thing is that the values of `y` *all*\ngo up over time, regardless of which treatment they were from.\nAt the initial time T1, the treatments are all about the same, but\nat the second and third time points, `y` is bigger for\ntreatment C than for the other two treatments (which are about the\nsame as each other). If you like, say that the gap between\ntreatment C and the others is increasing over time, or that the\nlines for treatment C are steeper than for the other\ntreatments. Any of those ways of saying it comes to the same\nconclusion. \nExtra: if you look at the lines of the same colour (treatment),\nthey don't seem to cross over very much. That suggests that an\nindividual who starts with a larger value of `y`\n(relatively, compared to the other individuals on the same\ntreatment) tends to stay larger than the other individuals on the\nsame treatment all the way through. This would be another thing\nyou'd see if the measurements for the individuals are correlated,\nor if there is an \"individual effect\" to go along with a\ntreatment effect (and a time effect).\nIf you think of this like an individual-level version of an\ninteraction plot (which would be obtained by plotting the\n*means* for each treatment at each time), there is a\nsuggestion here of an interaction between treatment and time, as\nwell as a treatment effect (the latter because treatment C appears\nbetter than the rest). \n\n$\\blacksquare$\n\n(e) Explain briefly how the data are in the wrong format for a\nrepeated-measures ANOVA (done using MANOVA, as in class), and use\n`pivot_wider` to get the data set into the right format.\n\nSolution\n\n\nFor MANOVA, we want the three responses (here, the values of\n`y` at the three different times) in three separate\ncolumns, with *all* the measurements for one subject in one\nrow (rather than on three separate rows, as here).\n`pivot_wider` is the flip-side of `pivot_longer`: instead of\nmaking different columns that all measure the same thing into one\ncolumn, we split one column that contains things that are (slightly)\ndifferent from each other (here, `y` at different\ntimes). It needs two inputs: `names_from`, the current single column that\ncontains the column names you are going to make, and `values_from`, the values to\ncarry along with them:\n\n::: {.cell}\n\n```{.r .cell-code}\ntreatments %>% pivot_wider(names_from = time, values_from = y) -> tr2\ntr2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 x 5\n  trt   subject    T1    T2    T3\n  <chr>   <dbl> <dbl> <dbl> <dbl>\n1 A           1    10    16    25\n2 A           2    12    19    27\n3 A           3    13    20    28\n4 B           4    12    18    25\n5 B           5    11    20    26\n6 B           6    10    22    27\n7 C           7    10    22    31\n8 C           8    12    23    34\n9 C           9    13    22    33\n```\n\n\n:::\n:::\n\n  \n\n(I got this right the first time too. I must be having a good day!)\n\nTechnical detail: in `pivot_longer`, the column names are in quotes because they don't exist yet, but in `pivot_wider`, they are *not* in quotes because they are columns that already exist.\n\nNote that the `time` and `y` columns have\n*disappeared*: the columns labelled with the time points are\nwhere those values of `y` have gone. The nine subjects make up\nthe nine rows of the new \"wide\" data set, which is in the format we\nwant.\n\n$\\blacksquare$\n\n(f) Run a repeated-measures ANOVA the `Manova` way. What do you\nconclude from it?\n\nSolution\n\n\nCreate the response variable first, and use it in an `lm`:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(tr2, cbind(T1, T2, T3))\ntreatment.1 <- lm(response ~ trt, data = tr2)\n```\n:::\n\n     \n\nNow we have to construct the within-subject stuff, for which we need\nto get the different times we have. You can type them in again (fine\nhere), or get them from the `response` you just made:\n\n::: {.cell}\n\n```{.r .cell-code}\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\n```\n:::\n\n \n\nThis is where the possible time effect is accounted for. Because time\nis within-subjects (each subject is measured at several different\ntimes) but treatment is between subjects (each subject only gets one\ntreatment), the two things have to be treated separately, in this\napproach at least. \n\nThen, uppercase-M `Manova`:\n\n::: {.cell}\n\n```{.r .cell-code}\ntreatment.2 <- Manova(treatment.1, idata = times.df, idesign = ~times)\nsummary(treatment.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nT1           1\nT2           1\nT3           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    32520.11\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1    0.9975 2399.025      1      6 4.8568e-09 ***\nWilks             1    0.0025 2399.025      1      6 4.8568e-09 ***\nHotelling-Lawley  1  399.8374 2399.025      1      6 4.8568e-09 ***\nRoy               1  399.8374 2399.025      1      6 4.8568e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: trt \n\n Response transformation matrix:\n   (Intercept)\nT1           1\nT2           1\nT3           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    193.5556\n\nMultivariate Tests: trt\n                 Df test stat approx F num Df den Df   Pr(>F)  \nPillai            2 0.7041229 7.139344      2      6 0.025902 *\nWilks             2 0.2958771 7.139344      2      6 0.025902 *\nHotelling-Lawley  2 2.3797814 7.139344      2      6 0.025902 *\nRoy               2 2.3797814 7.139344      2      6 0.025902 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2\nT1      1      0\nT2      0      1\nT3     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1    times2\ntimes1   2601 1258.0000\ntimes2   1258  608.4444\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1    0.9988 2010.298      2      5 5.4369e-08 ***\nWilks             1    0.0012 2010.298      2      5 5.4369e-08 ***\nHotelling-Lawley  1  804.1190 2010.298      2      5 5.4369e-08 ***\nRoy               1  804.1190 2010.298      2      5 5.4369e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: trt:times \n\n Response transformation matrix:\n   times1 times2\nT1      1      0\nT2      0      1\nT3     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1   times2\ntimes1     72 38.00000\ntimes2     38 28.22222\n\nMultivariate Tests: trt:times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            2  1.345125  6.16206      4     12 0.00620585 ** \nWilks             2  0.023398 13.84359      4     10 0.00043842 ***\nHotelling-Lawley  2 25.988095 25.98810      4      8 0.00012292 ***\nRoy               2 25.367215 76.10165      2      6 5.4552e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n             Sum Sq num Df Error SS den Df   F value    Pr(>F)    \n(Intercept) 10840.0      1   27.111      6 2399.0246 4.857e-09 ***\ntrt            64.5      2   27.111      6    7.1393 0.0259021 *  \ntimes        1301.0      2   12.889     12  605.6207 8.913e-13 ***\ntrt:times      41.5      4   12.889     12    9.6552 0.0009899 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n          Test statistic  p-value\ntimes            0.29964 0.049149\ntrt:times        0.29964 0.049149\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n           GG eps Pr(>F[GG])    \ntimes     0.58811  3.114e-08 ***\ntrt:times 0.58811   0.008332 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n             HF eps   Pr(>F[HF])\ntimes     0.6461293 7.092455e-09\ntrt:times 0.6461293 6.137921e-03\n```\n\n\n:::\n:::\n\n(Since I call things by the same names every time, my code for one of\nthese looks a lot like my code for any of the others.)\n\nStart near the bottom with the Mauchly tests for sphericity. These are *just* significant, so that instead of looking at the univariate tests, look at the bottom for the Huynh-Feldt adjusted P-values, the one for the interaction being 0.0061. This is a bit bigger than the one for the interaction in the univariate tests (0.0010), the latter being deceptively small because sphericity is actually no good.\n\nCompare this, if you like, with the multivariate tests for the interaction. In this case, these are actually not all the same; the largest P-value is the Pillai one, which at 0.0062 is very close to the properly adjusted univariate test.\n\nFinally, interpretation. We look *only* at the interaction. This\nis significant, so the effect of treatment is different at the\ndifferent times. And we **stop there**.\n\n\n$\\blacksquare$\n\n(g) How is your conclusion from the previous part consistent\nwith your spaghetti plot? Explain briefly.\n\nSolution\n\n\nThe thing that the interaction means is that the effect of\ntreatment is different over different times. That shows up in the\nspaghetti plot by treatment C being the same as the others at the\nbeginning, but clearly better than the others at the later\ntimes. That is to say, you can't talk about \"an\" effect of\ntreatment, because whether or not treatment C is better than the\nothers depends on which time you're looking at.\n\nExtra: we used the MANOVA way of doing the repeated-measures\nanalysis. There is another way, \"mixed models\", which is in some\nways less familiar and in some ways more. \n\nIn any analysis of variance, there are two kinds of effects of\nthings you may care about: fixed effects and random effects. Fixed\neffects are things like the treatment and time here, where the\nones you look at are the only ones you are interested in (in this\nstudy at least). If you had wanted to assess another treatment,\nyou would have designed it into the study; if you cared about\nother times, you would have measured `y` at those times\ntoo. The subjects, though, are different: they are a random sample\nof all possible people, and you want your results to generalize to\nthe population of all people of whom your subjects are a\nsample.^[In practice, things are usually fuzzier than this,      because the subjects in your study are typically the ones you      could get, rather than being a physical random sample of all      possible people, but we usually act as if our subjects are a      random sample of all possible subjects.]\nSo subjects are a different kind of thing and they have what are\ncalled random effects. When each subject only gives one\nmeasurement, as in all the things we've seen so\nfar,^[Including matched pairs, because what we do there is      to take the difference between the two measurements for each      person and throw away the actual measurements themselves, so      that each subject still only gives us one measurement.] it\ndoesn't matter how you treat (statistically) the subjects, but\nwhen each subject gives *more* than one measurement, it does\nmatter. Which is why we have to do the `idesign` stuff in\nthe MANOVA, or what you will see below.\n\nA model with both fixed and random effects is called a mixed model.\n\nWe're going to make the assumption that the effect of being one\nsubject rather than another is to move the value of `y` up\nor down by a fixed amount regardless of treatment or time, on\naverage (each subject is different, but *within* a subject\nthe random effect is the same size). That seems reasonable, given\nthe spaghetti plot, where some subjects seemed to give\nconsistently larger or smaller values of `y` than\nothers. This is a so-called \"random-intercepts\" model. In the\npackage `lme4`, there is a function `lmer` that\nlooks like `lm`, except for the way in which you specify\nthe random effects. It looks like this, noting that \n*it works with the tidy data frame* that we read in from the file and made\nthe spaghetti plot out of:\n\n::: {.cell}\n\n```{.r .cell-code}\ntreatment.4 <- lmer(y ~ trt * time + (1 | subject), data = treatments)\ndrop1(treatment.4, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\ny ~ trt * time + (1 | subject)\n         npar    AIC   LRT   Pr(Chi)    \n<none>        102.53                    \ntrt:time    4 120.44 25.91 3.299e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n     \n\nThe way to read that model is \"`y` depends on the combination of treatment and time and also on a random intercept for each subject\". \nThis is the way in which the model captures the idea that\neach subject is different. \n\nYou don't get a test for the random effects; you are *assuming* that the\nsubjects will be different from each other and you want to *adjust* for\nthat, rather than testing for it.^[This is rather like the test for blocks in a randomized  block design: you want to *allow* for differences among blocks,  but you don't especially care to test that there *are* any. In  fact, blocks are a lot like subjects, in that they are typically  things like different experimental plots in which plants are grown,  or different days on which the experiment is conducted, and you want to generalize from the blocks you observed, which are certainly *not* all possible blocks, to the population of all possible blocks.]\nAll you get is tests for the fixed effects that are currently up for\ngrabs, in this case the interaction, which is strongly significant.\n\n$\\blacksquare$\n\n\n",
=======
    "markdown": "# Repeated measures\n\nPackages for this chapter:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nlibrary(lme4)\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Effect of drug on rat weight\n\n\n Box (1950) gives data on the weights of three groups of\nrats. One group was given thyroxin in their drinking water, one group\nthiouracil, and the third group was a control. (This description comes\nfrom Christensen (2001).)^[References: Box, George EP, Problems in the analysis of growth and wear curves, Biometrics vol. 6, 362-369, 1950; Christensen R, Advanced Linear Modeling, 2nd edition, Springer, 2001. Yes, that's the Box-Cox Box.]\nWeights are measured in\ngrams at weekly intervals (over a 4-week period, so that each rat is\nmeasured 5 times). The data are in\n[link](http://ritsokiguess.site/datafiles/ratweight.csv) as a\n`.csv` file.\n\n\n\n(a) Read in the data and check that you have a column of\n`drug` and five columns of rat weights at different times.\n \n\n(b) Why would it be *wrong* to use something like\n`pivot_longer` to create one column of weights, and separate\ncolumns of drug and time, and then to run a two-way ANOVA? Explain briefly.\n \n\n(c) Create a suitable response variable and fit a suitable\n`lm` as the first step of the repeated-measures analysis.\n \n\n(d) Load the package `car` and run a suitable\n`Manova`. To do this, you will need to set up the right thing\nfor `idata` and `idesign`.\n \n\n(e) Take a look at the output from the MANOVA. Is there a\nsignificant interaction? What does its significance (or lack\nthereof) mean?\n \n\n(f) We are going to draw an interaction plot in a moment. To\nset that up, use `pivot_longer` as in the lecture notes to create\none column of weights and a second column of times. (You don't\nneed to do the `separate` thing that I did in class, though\nif you want to try it, go ahead.)\n \n\n(g) Obtain an interaction plot. Putting `time` as the\n`x` will put time along the horizontal axis, which is the\nway we're used to seeing such things. Begin by calculating the mean\n`weight` for each `time`-`drug` combination.\n \n\n(h) How does this plot show why the interaction was\nsignificant? Explain briefly.\n \n\n\n\n\n##  Social interaction among old people\n\n\n A graduate student wrote a thesis comparing different\ntreatments for increasing social interaction among geriatric\npatients. He recruited 21 patients at a state mental hospital and\nrandomly assigned them to treatments: Reality Orientation\n(`ro`), Behavior Therapy (`bt`) or no treatment\n(`ctrl`). Each subject was observed at four times, labelled\n`t1` through `t4` in the data file\n[link](http://ritsokiguess.site/datafiles/geriatrics.txt). The\nresponse variable was the percentage of time that the subject was\n\"engaging in the relevant social interaction\", so that a higher\nvalue is better.\n\nThe principal aim of the study was to see whether there were\ndifferences among the treatments (one would hope that the real\ntreatments were better than the control one), and whether there were\nany patterns over time.\n\n\n\n(a) Read in the data and display at least some of it.\n\n\n\n(b) Create a response variable and fit a suitable `lm`\nas the first stage of the repeated-measures analysis.\n\n\n\n(c) Run a suitable `Manova`. There is some setup\nfirst. Make sure you do that.\n\n\n\n(d) Display the results of your repeated-measures\nanalysis. What do you conclude? Explain briefly.\n\n\n\n(e) To understand the results that you got from the repeated\nmeasures analysis, you are going to draw a picture (or two). To do\n*that*, we are going to need the data in \"long\" format with\none response value per line (instead of four). Use `pivot_longer`\nsuitably to get the data in that format, and demonstrate that you\nhave done so.\n\n\n\n(f) Calculate and save the mean interaction percents for each\ntime-treatment combination.\n\n\n\n(g) Make an interaction plot. Arrange things so that time goes\nacross the page. Use your data frame of means that you just calculated.\n\n\n\n(h) Describe what you see on your interaction plot, and what it\nsays about why your repeated-measures analysis came out as it did.\n\n\n\n(i) Draw a spaghetti plot of these data. That is, use\n`ggplot` to plot the interaction percent against time for\neach subject,\njoining the points for the *same subject* by lines whose colour\nshows what treatment they were on. Use the \"long\" data frame for\nthis (not the data frame of means).\n\n\n\n\n\n\n##  Children's stress levels and airports\n\n\n If you did STAC32, you might remember this question, which\nwe can now do properly. Some of this question is a repeat from there.\n\nThe data in [link](http://ritsokiguess.site/datafiles/airport.txt)\nare based on a 1998 study of stress levels in children as a result of\nthe building of a new airport in Munich, Germany. A total of 200\nchildren had their epinephrine levels (a stress indicator) measured at\neach of four different times: before the airport was built, and 6, 18\nand 36 months after it was built. The four measurements are labelled\n`epi_1` through `epi_4`.  Out of the children, 100\nwere living near the new airport (location 1 in the data set), and\ncould be expected to suffer stress because of the new airport. The\nother 100 children lived in the same city, but outside the noise\nimpact zone. These children thus serve as a control group. The\nchildren are identified with numbers 1 through 200.\n\n\n\n(a) If we were testing for the effect of time, explain briefly\nwhat it is about the structure of the data that would make an\nanalysis of variance *inappropriate*.\n\n\n\n(b) Read the data into R and demonstrate that you have the right\nnumber of observations and variables.\n\n\n\n(c) Create and save a \"longer\" data frame with all the epinephrine\nvalues collected together into one column.\n\n\n\n(d) Make a \"spaghetti plot\" of these data: that is, a plot of\nepinephrine levels against time, with the locations identified by\ncolour, and the points for the same child joined by lines.  To do\nthis: (i) from the long data frame, create a new column containing\nonly the numeric values of time (1 through 4), (ii) plot epinephrine\nlevel against time with the points grouped by child and coloured by\nlocation (which you may have to turn from a number into a factor.)\n\n\n\n(e) What do you see on your spaghetti plot? We are looking ahead\nto possible effects of time, location and their interaction.\n\n\n\n(f) The spaghetti plot was hard to interpret because there are\nso many children. Calculate the mean epinephrine levels for each\nlocation-time combination, and make an interaction plot with time\non the $x$-axis and location as the second factor.\n\n\n\n(g) What do you conclude from your interaction plot? Is your\nconclusion clearer than from the spaghetti plot?\n\n\n\n(h) Run a repeated-measures analysis of variance and display the\nresults. Go back to your original data frame, the one you read in\nfrom the file, for this. You'll need to make sure your numeric\n`location` gets treated as a `factor`.\n\n\n\n(i) What do you conclude from the MANOVA? Is that consistent\nwith your graphs? Explain briefly.\n\n\n\n\n\n\n\n##  Body fat as repeated measures\n\n\n This one is also stolen from STAC32. Athletes are concerned\nwith measuring their body fat percentage. Two different methods are\navailable: one using ultrasound, and the other using X-ray\ntechnology. We are interested in whether there is a difference in the\nmean body fat percentage as measured by these two methods, and if so,\nhow big that difference is. Data on 16 athletes are at\n[link](http://ritsokiguess.site/datafiles/bodyfat.txt).\n\n\n\n(a) Read in the data and check that you have a sensible number of\nrows and columns.\n\n\n\n\n(b) Carry out a suitable (matched-pairs) $t$-test to determine\nwhether the means are the same or different. \n\n\n\n\n\n\n(c) What do you conclude from the test?\n\n\n\n\n(d) Run a repeated-measures analysis of variance, treating the two\nmethods of measuring body fat as the repeated measures (ie., playing\nthe role of \"time\" that you have seen in most of the other repeated\nmeasures analyses). There is no \"treatment\" here, so there is\nnothing to go on the right side of the squiggle. Insert a `1`\nthere to mean \"just an intercept\". Display the results.\n\n\n\n\n\n(e) Compare your repeated-measures analysis to your matched-pairs\none. Do you draw the same conclusions?\n\n\n\n\n\n\n\n\n\n\n##  Investigating motor activity in rats\n\n\n A researcher named King was investigating\nthe effect of the drug midazolam on motor activity in rats. Typically,\nthe first time the drug is injected, a rat's motor activity decreases\nsubstantially, but rats typically develop a \"tolerance\", so that\nfurther injections of the drug have less impact on the rat's motor\nactivity.\n\nThe data shown in\n[link](http://ritsokiguess.site/datafiles/king.csv) were all taken\nin one day, called the \"experiment day\" below. 24 different rats\nwere used. Each rat, on the experiment day, was injected with a fixed\namount of midazolam, and at each of six five-minute intervals after\nbeing injected, the rat's motor activity was measured (these are\nlabelled `i1` through `i6` in the data). The rats\ndiffered in how they had been treated before the experiment day. The\ncontrol group of rats had previously been injected repeatedly with a\nsaline solution (no active ingredient), so the experiment day was the\nfirst time this group of rats had received midazolam. The other two\ngroups of rats had both received midazolam repeatedly before the\nexperiment day: the \"same\" group was injected on experiment day in\nthe same environment that the previous injections had taken place (this\nis known in psychology as a \"conditioned tolerance\"), but the\n\"different\" group had the previous injections in a different\nenvironment than on experiment day.\n\nThe column `id` identifies the rat from which each sequence of\nvalues was obtained.\n\n\n\n(a) Explain briefly why we need to use a repeated measures\nanalysis for these data.\n\n\n\n(b) Read in the data and note that you have what was promised\nin the question.\n\n\n\n(c) We are going to do a repeated-measures analysis using the\n\"profile\" method shown in class. Create a suitable response\nvariable for this method.\n\n\n\n(d) Set up the \"within-subjects\" part of the analysis. That\nmeans getting hold of the names of the columns that hold the\ndifferent times, saving them, and also making a data frame out of them:\n\n\n\n(e) Fit the repeated-measures ANOVA. This will involve fitting\nan `lm` first, if you have not already done so.\n\n\n\n(f) What do you conclude from your repeated-measures ANOVA?\nExplain briefly, in the context of the data.\n\n\n\n(g) To understand the results of the previous part, we are going\nto make a spaghetti plot. In preparation for that, we need to save\nthe data in \"long format\" with one observation on *one* time\npoint in each row. Arrange that, and show by displaying (some of)\nthe data that you have done so.\n\n\n\n(h) Make a spaghetti plot: that is, plot motor activity against\nthe time points, joining the points for each *rat* by lines,\nand colouring the points and lines according to the *context*.\n\n\n\n(i) Looking at your spaghetti plot, why do you think your\nrepeated-measures ANOVA came out as it did? Explain briefly.\n\n\n\n\n\n\n\n\n\n##  Repeated measures with no background\n\n\n Nine people are randomly chosen to receive one of three\ntreatments, labelled A, B and C. Each person has their response\n`y` to the treatment measured at three times, labelled T1, T2\nand T3. The main aim of the study is to properly assess the effects of\nthe treatments. A higher value of `y` is better.\n\nThe data are in [link](http://ritsokiguess.site/datafiles/rm.txt).\n\n\n\n(a) There are $9 \\times 3=27$ observations  of `y` in\nthis study. Why would it be wrong to treat these as 27 independent\nobservations? Explain briefly.\n\n\n(b) Read in the data values. Are they tidy or untidy?  Explain\nbriefly. (The data values are separated by *tabs*, like the\nAustralian athlete data.)\n\n\n(c) Make a spaghetti plot: that is, a plot of `y`\nagainst time, with the observations for the same individual joined\nby lines which are coloured according to the treatment that\nindividual received.\n\n\n(d) On your spaghetti plot, how do the values of `y` for\nthe treatments compare over time?\n\n\n(e) Explain briefly how the data are in the wrong format for a\nrepeated-measures ANOVA (done using MANOVA, as in class), and use\n`pivot_wider` to get the data set into the right format.\n\n\n(f) Run a repeated-measures ANOVA the `Manova` way. What do you\nconclude from it?\n\n\n(g) How is your conclusion from the previous part consistent\nwith your spaghetti plot? Explain briefly.\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Effect of drug on rat weight\n\n\nBox (1950) gives data on the weights of three groups of\nrats.^[This is the same Box as Box-Cox and the M test.] One group was given thyroxin in their drinking water, one group\nthiouracil, and the third group was a control. (This description comes\nfrom Christensen (2001).)^[References: Box, George EP, Problems in the analysis of growth and wear curves, Biometrics vol. 6, 362-369, 1950; Christensen R, Advanced Linear Modeling, 2nd edition, Springer, 2001. Yes, that's the Box-Cox Box.]\nWeights are measured in\ngrams at weekly intervals (over a 4-week period, so that each rat is\nmeasured 5 times). The data are in\n[link](http://ritsokiguess.site/datafiles/ratweight.csv) as a\n`.csv` file.\n\n\n\n(a) Read in the data and check that you have a column of\n`drug` and five columns of rat weights at different times.\n \nSolution\n\n\nA `.csv` file, so `read_csv`. (I typed the data from\nChristensen (2001) into a spreadsheet.)\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/ratweight.csv\"\nweights <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 27 Columns: 7\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): drug\ndbl (6): rat, Time0, Time1, Time2, Time3, Time4\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nweights\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 27 x 7\n     rat drug       Time0 Time1 Time2 Time3 Time4\n   <dbl> <chr>      <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     1 thyroxin      59    85   121   156   191\n 2     2 thyroxin      54    71    90   110   138\n 3     3 thyroxin      56    75   108   151   189\n 4     4 thyroxin      59    85   116   148   177\n 5     5 thyroxin      57    72    97   120   144\n 6     6 thyroxin      52    73    97   116   140\n 7     7 thyroxin      52    70   105   138   171\n 8     8 thiouracil    61    86   109   120   129\n 9     9 thiouracil    59    80   101   111   122\n10    10 thiouracil    53    79   100   106   133\n# i 17 more rows\n```\n:::\n:::\n\n   \n\nThere are 27 rats altogether, each measured five times (labelled time\n0 through 4). The rats are also labelled `rat` (the first column), which will be useful later.\n \n$\\blacksquare$ \n\n(b) Why would it be *wrong* to use something like\n`pivot_longer` to create one column of weights, and separate\ncolumns of drug and time, and then to run a two-way ANOVA? Explain briefly.\n \nSolution\n\n\nSuch a solution would assume that we have measurements on\n*different* rats, one for each drug-time combination. But we\nhave sets of five measurements all on the *same* rat: that is\nto say, we have repeated measures, and the proper analysis will\ntake that into account.\n \n$\\blacksquare$\n\n(c) Create a suitable response variable and fit a suitable\n`lm` as the first step of the repeated-measures analysis.\n \nSolution\n\n\nThe response variable is the same idea as for any MANOVA: just\nglue the columns together:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(weights, cbind(Time0, Time1, Time2, Time3, Time4))\nweights.1 <- lm(response ~ drug, data = weights)\n```\n:::\n\n     \n::: {.cell}\n\n```{.r .cell-code}\nweights %>% select(starts_with(\"Time\")) %>% \n  as.matrix() -> y\n```\n:::\n\nNow, we *don't* look at `weights.1`, but we *do* use\nit as input to `Manova` in a moment.\n \n$\\blacksquare$\n\n(d) Load the package `car` and run a suitable\n`Manova`. To do this, you will need to set up the right thing\nfor `idata` and `idesign`.\n \nSolution\n\n\nSomething like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ntimes <- colnames(response)\ntimes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Time0\" \"Time1\" \"Time2\" \"Time3\" \"Time4\"\n```\n:::\n\n```{.r .cell-code}\ntimes.df <- data.frame(times=factor(times))\ntimes.df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  times\n1 Time0\n2 Time1\n3 Time2\n4 Time3\n5 Time4\n```\n:::\n\n```{.r .cell-code}\nweights.2 <- Manova(weights.1, idata = times.df, idesign = ~times)\n```\n:::\n\n     \n\nThe thought process is that the columns of the response\n(`Time.0` through `Time.4`) are all times. This is the\n\"within-subject design\" part of it: within a rat, the different\nresponse values are at different times. That's the only part of it\nthat is within subjects. The different drugs are a\n\"between-subjects\" factor: each rat only gets one of the\ndrugs.^[Things would be a lot more complicated if each rat got a different drug at a different time! But the rats each got one drug *once*, at the beginning, and the issue was the effect of that drug on all the growth that followed.]\n \n$\\blacksquare$\n\n(e) Take a look at all the output from the MANOVA. Is there a\nsignificant interaction? What does its significance (or lack\nthereof) mean?\n \nSolution\n\nLook at the `summary`, which is rather long:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(weights.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nTime0           1\nTime1           1\nTime2           1\nTime3           1\nTime4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     6875579\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1   0.99257 3204.089      1     24 < 2.22e-16 ***\nWilks             1   0.00743 3204.089      1     24 < 2.22e-16 ***\nHotelling-Lawley  1 133.50372 3204.089      1     24 < 2.22e-16 ***\nRoy               1 133.50372 3204.089      1     24 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug \n\n Response transformation matrix:\n      (Intercept)\nTime0           1\nTime1           1\nTime2           1\nTime3           1\nTime4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    33193.27\n\nMultivariate Tests: drug\n                 Df test stat approx F num Df den Df    Pr(>F)   \nPillai            2 0.3919186 7.734199      2     24 0.0025559 **\nWilks             2 0.6080814 7.734199      2     24 0.0025559 **\nHotelling-Lawley  2 0.6445166 7.734199      2     24 0.0025559 **\nRoy               2 0.6445166 7.734199      2     24 0.0025559 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3 times4\nTime0      1      0      0      0\nTime1      0      1      0      0\nTime2      0      0      1      0\nTime3      0      0      0      1\nTime4     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1 times2    times3   times4\ntimes1 235200.00 178920 116106.67 62906.67\ntimes2 178920.00 136107  88324.00 47854.00\ntimes3 116106.67  88324  57316.15 31053.93\ntimes4  62906.67  47854  31053.93 16825.04\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1   0.98265 297.3643      4     21 < 2.22e-16 ***\nWilks             1   0.01735 297.3643      4     21 < 2.22e-16 ***\nHotelling-Lawley  1  56.64082 297.3643      4     21 < 2.22e-16 ***\nRoy               1  56.64082 297.3643      4     21 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug:times \n\n Response transformation matrix:\n      times1 times2 times3 times4\nTime0      1      0      0      0\nTime1      0      1      0      0\nTime2      0      0      1      0\nTime3      0      0      0      1\nTime4     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n         times1   times2   times3   times4\ntimes1 9192.071 8948.843 6864.676 3494.448\ntimes2 8948.843 8787.286 6740.286 3381.529\ntimes3 6864.676 6740.286 5170.138 2594.103\ntimes4 3494.448 3381.529 2594.103 1334.006\n\nMultivariate Tests: drug:times\n                 Df test stat  approx F num Df den Df     Pr(>F)    \nPillai            2 0.8779119  4.303151      8     44 0.00069308 ***\nWilks             2 0.2654858  4.939166      8     42 0.00023947 ***\nHotelling-Lawley  2 2.2265461  5.566365      8     40 9.3465e-05 ***\nRoy               2 1.9494810 10.722146      4     22 5.6277e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n             Sum Sq num Df Error SS den Df   F value    Pr(>F)    \n(Intercept) 1375116      1  10300.2     24 3204.0892 < 2.2e-16 ***\ndrug           6639      2  10300.2     24    7.7342  0.002556 ** \ntimes        146292      4   4940.7     96  710.6306 < 2.2e-16 ***\ndrug:times     6777      8   4940.7     96   16.4606 4.185e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n           Test statistic   p-value\ntimes           0.0072565 1.781e-19\ndrug:times      0.0072565 1.781e-19\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n            GG eps Pr(>F[GG])    \ntimes      0.33165  < 2.2e-16 ***\ndrug:times 0.33165  2.539e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n              HF eps   Pr(>F[HF])\ntimes      0.3436277 9.831975e-26\ndrug:times 0.3436277 1.757377e-06\n```\n:::\n:::\n\nStart near the bottom with Mauchly's test. This is strongly significant (for the interaction, which is our focus here) and means that sphericity fails and the P-value for the interaction in the univariate test is not to be trusted (it is much too small). Look instead at the Huynh-Feldt adjusted P-value at the very bottom, $1.76 \\times 10^{-6}$. This is strongly significant still, but it is a *billion* times bigger than the one in the univariate table! For comparison, the test for interaction in the multivariate analysis has a P-value of 0.0007 or less, depending on which of the four tests you look at (this time, they are not all the same). As usual, the multivariate tests have bigger P-values than the appropriately adjusted univariate tests, but the P-values are all pointing in the same direction.\n\nThe significant interaction means that the effect of time on growth is different for\nthe different drugs: that is, the effect of drug is over the whole\ntime profile, not just something like \n\"a rat on Thyroxin is on average 10 grams heavier than a control rat, over all times\".\n\nSince the interaction is significant, that's where we stop, as far as\ninterpretation is concerned.\n \n$\\blacksquare$\n\n(f) We are going to draw an interaction plot in a moment. To\nset that up, use `pivot_longer` as in the lecture notes to create\none column of weights and a second column of times. (You don't\nneed to do the `separate` thing that I did in class, though\nif you want to try it, go ahead.)\n \nSolution\n\nLike this:\n\n::: {.cell}\n\n```{.r .cell-code}\nweights %>% \n  pivot_longer(starts_with(\"Time\"), names_to=\"time\", values_to=\"weight\") -> weights.long\nweights.long\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 135 x 4\n     rat drug     time  weight\n   <dbl> <chr>    <chr>  <dbl>\n 1     1 thyroxin Time0     59\n 2     1 thyroxin Time1     85\n 3     1 thyroxin Time2    121\n 4     1 thyroxin Time3    156\n 5     1 thyroxin Time4    191\n 6     2 thyroxin Time0     54\n 7     2 thyroxin Time1     71\n 8     2 thyroxin Time2     90\n 9     2 thyroxin Time3    110\n10     2 thyroxin Time4    138\n# i 125 more rows\n```\n:::\n:::\n\n\n\n     \n\nMy data frame was called `weights`, so I was OK with having a\nvariable called `weight`. Watch out for that if you call the\ndata frame `weight`, though.\n\nSince the piece of the time we want is the number,\n`parse_number` (from `readr`, part of the\n`tidyverse`) should also work:\n\n::: {.cell}\n\n```{.r .cell-code}\nweights %>% \n  pivot_longer(starts_with(\"Time\"), names_to=\"timex\", values_to=\"weight\") %>% \n  mutate(time = parse_number(timex)) -> weights2.long\nweights2.long %>% sample_n(20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 x 5\n     rat drug       timex weight  time\n   <dbl> <chr>      <chr>  <dbl> <dbl>\n 1    10 thiouracil Time0     53     0\n 2    23 control    Time3    131     3\n 3     9 thiouracil Time1     80     1\n 4    15 thiouracil Time0     58     0\n 5     9 thiouracil Time3    111     3\n 6    20 control    Time2    111     2\n 7     2 thyroxin   Time3    110     3\n 8    25 control    Time0     63     0\n 9    23 control    Time0     46     0\n10     1 thyroxin   Time2    121     2\n11    14 thiouracil Time3    103     3\n12    27 control    Time3    139     3\n13     3 thyroxin   Time2    108     2\n14    19 control    Time2    123     2\n15    19 control    Time0     60     0\n16     5 thyroxin   Time2     97     2\n17    27 control    Time2    110     2\n18    16 thiouracil Time0     46     0\n19    18 control    Time4    172     4\n20     3 thyroxin   Time1     75     1\n```\n:::\n:::\n\n \n\nI decided to show you a random collection of rows, so that you can see\nthat `parse_number` worked for various different times. \n \n$\\blacksquare$\n\n(g) Obtain an interaction plot. Putting `time` as the\n`x` will put time along the horizontal axis, which is the\nway we're used to seeing such things. Begin by calculating the mean\n`weight` for each `time`-`drug` combination.\n \nSolution\n\n\n`group_by`, `summarize` and `ggplot`, the\nlatter using the data frame that came out of the\n`summarize`. The second factor `drug` goes as the\n`colour` and `group` both, since `time` has\ngrabbed the `x` spot:\n\n::: {.cell}\n\n```{.r .cell-code}\nweights.long %>%\n  group_by(time, drug) %>%\n  summarize(mean.weight = mean(weight)) %>%\n  ggplot(aes(x = time, y = mean.weight, colour = drug, group = drug)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'time'. You can override using the\n`.groups` argument.\n```\n:::\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/ratweight-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n       \n \n$\\blacksquare$\n\n(h) How does this plot show why the interaction was\nsignificant? Explain briefly.\n \nSolution\n\n\nAt the beginning, all the rats have the same average growth, but\nfrom time 2 (or maybe even 1) or so, the rats on thiouracil\ngrew more slowly. The idea is not just that thiouracil has a\n*constant* effect over all times, but that the *pattern*\nof growth is different for the different drugs: whether or not\nthiouracil inhibits growth, and, if so, by how much, depends on\nwhat time point you are looking at.\n\nRats on thyroxin or the control drug grew at pretty much the same\nrate over all times, so I wouldn't concern myself with any\ndifferences there.\n\nWhat I thought would be interesting is to plot the growth curves for\n*all* the rats individually, colour-coded by which drug the rat\nwas on. This is the repeated-measures version of the ANOVA interaction\nplot with the data on it, a so-called spaghetti plot. (We don't use the lines for the means, here,\ninstead using them for joining the measurements belonging to the same\nsubject.)\n\nWhen I first used this data set, it didn't have a column identifying which rat was which, which made this plot awkward, but now it does (the column `rat`). So we can start directly from the dataframe I created above called `weights.long`:\n\n::: {.cell}\n\n```{.r .cell-code}\nweights.long\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 135 x 4\n     rat drug     time  weight\n   <dbl> <chr>    <chr>  <dbl>\n 1     1 thyroxin Time0     59\n 2     1 thyroxin Time1     85\n 3     1 thyroxin Time2    121\n 4     1 thyroxin Time3    156\n 5     1 thyroxin Time4    191\n 6     2 thyroxin Time0     54\n 7     2 thyroxin Time1     71\n 8     2 thyroxin Time2     90\n 9     2 thyroxin Time3    110\n10     2 thyroxin Time4    138\n# i 125 more rows\n```\n:::\n:::\n\n  \nEach rat is identified by `rat``, which repeats 5 times,\nonce for each value of `time`:\n\n::: {.cell}\n\n```{.r .cell-code}\nweights.long %>% count(rat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 27 x 2\n     rat     n\n   <dbl> <int>\n 1     1     5\n 2     2     5\n 3     3     5\n 4     4     5\n 5     5     5\n 6     6     5\n 7     7     5\n 8     8     5\n 9     9     5\n10    10     5\n# i 17 more rows\n```\n:::\n:::\n\nIn the data frame `weights.long`, we plot\n`time` ($x$) against `weight` ($y$), grouping the points\naccording to `rat` and colouring them according to\n`drug`. \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(weights.long, aes(time, weight, group = rat, colour = drug)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/mantequilla-1.pdf){fig-pos='H'}\n:::\n:::\n\nAs you see, \"spaghetti plot\" is a rather apt name for this kind of thing.\n\nI like this plot because, unlike the interaction plot, which shows\nonly means, this gives a sense of variability as well. The blue and\nred lines (thyroxin and control) are all intermingled and they go\nstraight up. So there is nothing to choose between these. The green\nlines, though, start off mixed up with the red and blue ones but\nfinish up at the bottom: the *pattern* of growth of the\nthiouracil rats is different from the others, which is why we had a\nsignificant interaction between drug and time.\n\n`drug` is categorical, so `ggplot`\nuses a set of distinguishable colours to mark the levels. If our\ncolour had been a numerical variable, `ggplot` would have used\na range of colours like light blue to dark blue, with lighter being\nhigher, for example.\n\nWhat, you want to see that? All right. This one is kind of silly, but\nyou see the point:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weights.long, aes(time, weight, group = rat, colour = weight)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/alphington-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThe line segments get lighter as you go up the page.\n\nSince we went to the trouble of making the \"long\" data frame, we can also run a repeated measures analysis using the\nmixed-model idea (described more fully in the problem of the children\nnear the new airport):\n\n::: {.cell}\n\n```{.r .cell-code}\nwt.1 <- lmer(weight ~ drug * time + (1 | rat), data = weights.long)\ndrop1(wt.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nweight ~ drug * time + (1 | rat)\n          npar    AIC   LRT   Pr(Chi)    \n<none>          990.5                    \ndrug:time    8 1067.8 93.27 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nThe drug-by-time interaction is even more strongly significant than in\nthe profile analysis. (The output from `drop1` reminds us that\nthe only thing we should be thinking about now is that interaction.)\n \n$\\blacksquare$\n\n\n\n\n##  Social interaction among old people\n\n\n A graduate student wrote a thesis comparing different\ntreatments for increasing social interaction among geriatric\npatients. He recruited 21 patients at a state mental hospital and\nrandomly assigned them to treatments: Reality Orientation\n(`ro`), Behavior Therapy (`bt`) or no treatment\n(`ctrl`). Each subject was observed at four times, labelled\n`t1` through `t4` in the data file\n[link](http://ritsokiguess.site/datafiles/geriatrics.txt). The\nresponse variable was the percentage of time that the subject was\n\"engaging in the relevant social interaction\", so that a higher\nvalue is better.\n\nThe principal aim of the study was to see whether there were\ndifferences among the treatments (one would hope that the real\ntreatments were better than the control one), and whether there were\nany patterns over time.\n\n\n\n(a) Read in the data and display at least some of it.\n\n\nSolution\n\n\nThe usual, separated by a single space:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/geriatrics.txt\"\ngeriatrics <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 21 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (1): treatment\ndbl (5): subject, t1, t2, t3, t4\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ngeriatrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 21 x 6\n   subject treatment    t1    t2    t3    t4\n     <dbl> <chr>     <dbl> <dbl> <dbl> <dbl>\n 1       1 bt          1.5   9     5     4  \n 2       2 bt          5    14     4.5   7  \n 3       3 bt          1     8     4.5   2.5\n 4       4 bt          5    14     8     5  \n 5       5 bt          3     8     4     4  \n 6       6 bt          0.5   3.5   1.3   1  \n 7       7 bt          0.5   3     1     0  \n 8       8 ro          2     5     5     1.5\n 9       9 ro          1.5   1.9   1.5   1  \n10      10 ro          3.5   7     8     4  \n# i 11 more rows\n```\n:::\n:::\n\n \n\nCorrectly 21 observations measured at 4 different times. We also have\nsubject numbers, which might be useful later.\n\n\n$\\blacksquare$\n\n(b) Create a response variable and fit a suitable `lm`\nas the first stage of the repeated-measures analysis.\n\n\nSolution\n\n\nThis:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(geriatrics, cbind(t1, t2, t3, t4))\ngeriatrics.1 <- lm(response ~ treatment, data = geriatrics)\n```\n:::\n\n     \n\nThere is no need to look at this, since we are going to feed it into\n`Manova` in a moment, but in case you're curious, you see (in `summary`) a\nregression of each of the four columns in `response` on\n`treatment`, one by one. \n    \n\n$\\blacksquare$\n\n(c) Run a suitable `Manova`. There is some setup\nfirst. Make sure you do that.\n\n\nSolution\n\n\nMake sure `car` is loaded, and do the `idata` and\n`idesign` thing:\n\n::: {.cell}\n\n```{.r .cell-code}\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\ngeriatrics.2 <- Manova(geriatrics.1, idata = times.df, idesign = ~times)\n```\n:::\n\n     \n\nIn case you're curious, `response` is an R `matrix`:\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n \n\nand\nnot a data frame (because it was created by `cbind` which makes\na matrix out of vectors). So, to pull the names off the top, we really\ndo need `colnames` (applied to a matrix) rather than just\n`names` (which applies to a data frame). \n    \n\n$\\blacksquare$\n\n(d) Display the results of your repeated-measures\nanalysis. What do you conclude? Explain briefly.\n\n\nSolution\n\n\nIts `summary` will get you what you want:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(geriatrics.2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in summary.Anova.mlm(geriatrics.2): HF eps > 1 treated as 1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    3286.252\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.7458921 52.83606      1     18 9.3318e-07 ***\nWilks             1 0.2541079 52.83606      1     18 9.3318e-07 ***\nHotelling-Lawley  1 2.9353366 52.83606      1     18 9.3318e-07 ***\nRoy               1 2.9353366 52.83606      1     18 9.3318e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    360.6695\n\nMultivariate Tests: treatment\n                 Df test stat approx F num Df den Df   Pr(>F)  \nPillai            2 0.2436597 2.899406      2     18 0.080994 .\nWilks             2 0.7563403 2.899406      2     18 0.080994 .\nHotelling-Lawley  2 0.3221562 2.899406      2     18 0.080994 .\nRoy               2 0.3221562 2.899406      2     18 0.080994 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1     times2    times3\ntimes1  0.5833333  -8.366667 -1.666667\ntimes2 -8.3666667 120.001905 23.904762\ntimes3 -1.6666667  23.904762  4.761905\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df   Pr(>F)    \nPillai            1 0.7214276  13.8119      3     16 0.000105 ***\nWilks             1 0.2785724  13.8119      3     16 0.000105 ***\nHotelling-Lawley  1 2.5897315  13.8119      3     16 0.000105 ***\nRoy               1 2.5897315  13.8119      3     16 0.000105 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment:times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1    times2    times3\ntimes1   8.166667 -27.33333 -4.933333\ntimes2 -27.333333  91.61524 17.569524\ntimes3  -4.933333  17.56952 11.443810\n\nMultivariate Tests: treatment:times\n                 Df test stat  approx F num Df den Df     Pr(>F)    \nPillai            2 0.9258067  4.883886      6     34 0.00107288 ** \nWilks             2 0.2190296  6.062534      6     32 0.00025426 ***\nHotelling-Lawley  2 2.9043306  7.260827      6     30 7.4555e-05 ***\nRoy               2 2.6552949 15.046671      3     17 4.8948e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                Sum Sq num Df Error SS den Df F value    Pr(>F)    \n(Intercept)     821.56      1   279.89     18 52.8361 9.332e-07 ***\ntreatment        90.17      2   279.89     18  2.8994   0.08099 .  \ntimes            87.07      3    72.25     54 21.6933 2.378e-09 ***\ntreatment:times  90.77      6    72.25     54 11.3067 3.827e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                Test statistic p-value\ntimes                  0.85209 0.75008\ntreatment:times        0.85209 0.75008\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                 GG eps Pr(>F[GG])    \ntimes           0.90848  1.108e-08 ***\ntreatment:times 0.90848  1.434e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                  HF eps   Pr(>F[HF])\ntimes           1.086414 2.377839e-09\ntreatment:times 1.086414 3.826914e-08\n```\n:::\n:::\n\nAs is the way, start at the bottom and go up to Mauchly's test for sphericity. No problem here, so you can use the P-value for interaction on the univariate test as is ($3.8 \\times 10^{-8}). By way of comparison, the Huynh-Feldt adjusted P-value is exactly the same (not actually adjusted at all), which makes sense because there was no lack of sphericity. The multivariate tests for the interaction have P-values that vary, but they are all (i) a bit bigger than the univariate one, and (ii) still significant.     \n\nThus, the interaction is significant, so the effects of the treatments are\ndifferent at different times. (It makes most sense to say it this way\naround, since treatment is something that was controlled and time was not.)\n\nYou, I hope, know better than to look at the main effects when there\nis a significant interaction!\n    \n\n$\\blacksquare$\n\n(e) To understand the results that you got from the repeated\nmeasures analysis, you are going to draw a picture (or two). To do\n*that*, we are going to need the data in \"long\" format with\none response value per line (instead of four). Use `pivot_longer`\nsuitably to get the data in that format, and demonstrate that you\nhave done so.\n\n\nSolution\n\n\nThe usual layout:\n\n::: {.cell}\n\n```{.r .cell-code}\ngeriatrics %>% \n  pivot_longer(t1:t4, names_to=\"time\", values_to = \"intpct\") -> geriatrics.long\ngeriatrics.long\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 84 x 4\n   subject treatment time  intpct\n     <dbl> <chr>     <chr>  <dbl>\n 1       1 bt        t1       1.5\n 2       1 bt        t2       9  \n 3       1 bt        t3       5  \n 4       1 bt        t4       4  \n 5       2 bt        t1       5  \n 6       2 bt        t2      14  \n 7       2 bt        t3       4.5\n 8       2 bt        t4       7  \n 9       3 bt        t1       1  \n10       3 bt        t2       8  \n# i 74 more rows\n```\n:::\n:::\n\n\nI have *one* column of interaction percents, and\n*one* column of times. If you check the whole thing, you'll see\nthat `pivot_longer` gives all the measurements for subject 1, then subject 2, and so on.\n\nThe long data frame is, well, long.\n\nIt's not necessary to pull out the numeric time values, though you\ncould if you wanted to, by using\n`parse_number`.\n    \n\n$\\blacksquare$\n\n(f) Calculate and save the mean interaction percents for each\ntime-treatment combination.\n\n\nSolution\n\n\n`group_by` followed by `summarize`, as ever:\n\n::: {.cell}\n\n```{.r .cell-code}\ngeriatrics.long %>%\n  group_by(treatment, time) %>%\n  summarize(mean = mean(intpct)) -> means\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'treatment'. You can override using the\n`.groups` argument.\n```\n:::\n\n```{.r .cell-code}\nmeans\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 12 x 3\n# Groups:   treatment [3]\n   treatment time   mean\n   <chr>     <chr> <dbl>\n 1 bt        t1     2.36\n 2 bt        t2     8.5 \n 3 bt        t3     4.04\n 4 bt        t4     3.36\n 5 ctrl      t1     2.64\n 6 ctrl      t2     2.23\n 7 ctrl      t3     1.63\n 8 ctrl      t4     2.14\n 9 ro        t1     1.86\n10 ro        t2     3.8 \n11 ro        t3     3.11\n12 ro        t4     1.86\n```\n:::\n:::\n\n       \n\n\n$\\blacksquare$\n\n(g) Make an interaction plot. Arrange things so that time goes\nacross the page. Use your data frame of means that you just calculated.\n\n\nSolution\n\n\nOnce you have the means, this is not too bad:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(means, aes(x = time, y = mean, group = treatment, colour = treatment)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/ger_int-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThe \"second factor\" `treatment` appears as both\n`group` and `colour`.\n    \n\n$\\blacksquare$\n\n(h) Describe what you see on your interaction plot, and what it\nsays about why your repeated-measures analysis came out as it did.\n\n\nSolution\n\n\nThe two \"real\" treatments `bt` and `ro` both go up\nsharply between time 1 and time 2, and then come back down so that\nby time 4 they are about where they started. The control group\nbasically didn't change at all, and if anything went *down*\nbetween times 1 and 2, a completely different pattern to the others.\nThe two treatments `bt` and `ro` are not exactly\nparallel, but they do at least  have qualitatively the same\npattern.^[That is to say, it's the same kind of shape.] It\nis, I think, the fact that the control group has a\n*completely* different pattern over time that makes the\ninteraction come out significant.^[I am kind of wrong about    that, as we see.]\nI'm going to explore that some more later, but first I want to get\nyou to draw a spaghetti plot.\n    \n\n$\\blacksquare$\n\n(i) Draw a spaghetti plot of these data. That is, use\n`ggplot` to plot the interaction percent against time for\neach subject,\njoining the points for the *same subject* by lines whose colour\nshows what treatment they were on. Use the \"long\" data frame for\nthis (not the data frame of means).\n\n\nSolution\n\n\nThis is almost easier to do than it is to ask you to do:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(geriatrics.long, aes(x = time, y = intpct, colour = treatment, group = subject)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/ger_spag-1.pdf){fig-pos='H'}\n:::\n:::\n\n   \n\nThe basic difficulty here is to get all the parts. We need both a\n`colour` and a `group`; the latter controls the joining\nof points by lines (if you have both). Fortunately we already had\nsubject numbers in the original data; if we had not had them, we would\nhave had to create them. `dplyr` has a function\n`row_number` that we could have used for that; we'd apply the row\nnumbers to the original wide data frame, before we made it long, so\nthat the correct subject numbers would get carried along.\n\nWhether you add a `geom_point()` to plot the data points, or not,\nis up to you. Logically, it makes sense to include the actual data,\nbut aesthetically, it looks more like spaghetti if you leave the\npoints out. Either way is good, as far as I'm concerned.\n\nI didn't ask you to comment on the spaghetti plot, because the story\nis much  the same as the interaction plot. There is a lot of\nvariability, but the story within each group is basically what we\nalready said: the red lines go sharply up and almost as sharply back\ndown again, the blue lines do something similar, only not as sharply\nup and down, and the green lines do basically nothing.\n\nI said that the control subjects' time pattern was noticeably\ndifferent from the others. Which made me think: what if we remove the\ncontrol subjects? Would there still be an interaction?^[This is rather like removing time zero in the example in lecture.]\n\nAll right, we need to start with the original wide data frame, and\nfrom *that* select everything but `ctrl`:\n\n::: {.cell}\n\n```{.r .cell-code}\ngg <- geriatrics %>% filter(treatment != \"ctrl\")\ngg\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 14 x 6\n   subject treatment    t1    t2    t3    t4\n     <dbl> <chr>     <dbl> <dbl> <dbl> <dbl>\n 1       1 bt          1.5   9     5     4  \n 2       2 bt          5    14     4.5   7  \n 3       3 bt          1     8     4.5   2.5\n 4       4 bt          5    14     8     5  \n 5       5 bt          3     8     4     4  \n 6       6 bt          0.5   3.5   1.3   1  \n 7       7 bt          0.5   3     1     0  \n 8       8 ro          2     5     5     1.5\n 9       9 ro          1.5   1.9   1.5   1  \n10      10 ro          3.5   7     8     4  \n11      11 ro          1.5   4.2   2     1.5\n12      12 ro          1.5   3.6   2     1  \n13      13 ro          1.5   2.5   2.8   4  \n14      14 ro          1.5   2.4   0.5   0  \n```\n:::\n:::\n\n \n\nSo now there are two treatments left, seven people on\neach:^[The factor `treatment` still has three levels,  but only two of them have any remaining data.] \n\n::: {.cell}\n\n```{.r .cell-code}\ngg %>% count(treatment)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n  treatment     n\n  <chr>     <int>\n1 bt            7\n2 ro            7\n```\n:::\n:::\n\n \n\nThen we do\nthe same stuff over again: construct the response, run the\n`lm`, create the stuff for `idata` and `idesign`,\nand run the `Manova`. There's really nothing new here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(gg, cbind(t1, t2, t3, t4))\ngg.1 <- lm(response ~ treatment, data = gg)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\ngg.2 <- Manova(gg.1, idata = times.df, idesign = ~times)\nsummary(gg.2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in summary.Anova.mlm(gg.2): HF eps > 1 treated as 1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    2920.346\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.765026 39.06941      1     12 4.2509e-05 ***\nWilks             1  0.234974 39.06941      1     12 4.2509e-05 ***\nHotelling-Lawley  1  3.255785 39.06941      1     12 4.2509e-05 ***\nRoy               1  3.255785 39.06941      1     12 4.2509e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    203.6829\n\nMultivariate Tests: treatment\n                 Df test stat approx F num Df den Df Pr(>F)\nPillai            1 0.1850562 2.724941      1     12 0.1247\nWilks             1 0.8149438 2.724941      1     12 0.1247\nHotelling-Lawley  1 0.2270784 2.724941      1     12 0.1247\nRoy               1 0.2270784 2.724941      1     12 0.1247\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1    times2   times3\ntimes1    3.5 -24.80000 -6.80000\ntimes2  -24.8 175.72571 48.18286\ntimes3   -6.8  48.18286 13.21143\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.817303 14.91185      3     10 0.00050726 ***\nWilks             1  0.182697 14.91185      3     10 0.00050726 ***\nHotelling-Lawley  1  4.473555 14.91185      3     10 0.00050726 ***\nRoy               1  4.473555 14.91185      3     10 0.00050726 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: treatment:times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1 times2    times3\ntimes1    3.5 -11.20  2.000000\ntimes2  -11.2  35.84 -6.400000\ntimes3    2.0  -6.40  1.142857\n\nMultivariate Tests: treatment:times\n                 Df test stat approx F num Df den Df   Pr(>F)  \nPillai            1 0.5816827 4.635099      3     10 0.027959 *\nWilks             1 0.4183173 4.635099      3     10 0.027959 *\nHotelling-Lawley  1 1.3905298 4.635099      3     10 0.027959 *\nRoy               1 1.3905298 4.635099      3     10 0.027959 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                Sum Sq num Df Error SS den Df F value    Pr(>F)    \n(Intercept)     730.09      1  224.243     12 39.0694 4.251e-05 ***\ntreatment        50.92      1  224.243     12  2.7249 0.1247005    \ntimes           136.04      3   60.551     36 26.9595 2.560e-09 ***\ntreatment:times  38.16      3   60.551     36  7.5629 0.0004777 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                Test statistic p-value\ntimes                  0.66019 0.48791\ntreatment:times        0.66019 0.48791\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                 GG eps Pr(>F[GG])    \ntimes           0.82418  5.012e-08 ***\ntreatment:times 0.82418   0.001217 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                HF eps   Pr(>F[HF])\ntimes           1.0546 2.560037e-09\ntreatment:times 1.0546 4.777491e-04\n```\n:::\n:::\n\nThe procedure, as before: Mauchly's test is not significant, so you can look at the univariate\ntest for interaction. For comparison, the P-values for interaction in the multivariate test (all the same) are quite a bit bigger but still significant.\n\nThere is still an interaction, but it's not as significant as it was\nbefore. I think it is still significant because the shape of the two\ntime trends is not the same: the red `bt` group goes up further\nand down further. I was musing that the higher values are also more\nvariable, which would suggest a transformation, but I haven't explored that.\n\nIf the interaction had turned out to be nonsignificant this way? You\nmight think about trying to remove it from the model, except that in\nthis kind of model, `treatment` is a \"between-subjects factor\" \nand `times` is a \"within-subjects factor\", so they\nare different kinds of things. What you do in that case is to ignore\nthe non-significant interaction and interpret the main effects: there\nis no way to \"gain df for error\" like we did in two-way\nANOVA. \n\nSupposing, in this case, that we were using $\\alpha=0.01$, we\nwould say that the interaction is not significant. Then we look at the\nmain effects: there is no effect of treatment, but there is an effect\nof time. Or, to put it another way, once you allow for an effect of\ntime, there is no difference between the two remaining\ntreatments.^[There is often an effect of time, which is why you would be taking multiple time measurements, but the issue is when you take that into account, you are giving yourself an improved chance, in general, to find a treatment effect. This is exactly the same idea as using a matched pairs design to give yourself an improved chance of finding a treatment effect, even though the subjects might be quite different from each other. In fact, repeated measures *is* matched pairs with more than two measurements on each person. Which makes me think, I should have you do a matched pairs as repeated measures.]\n\nThinking back to our spaghetti plot, we are now comparing the red and\nblue treatments. They both go up at time 2 and down afterwards, which\nis the time effect, but even once you allow for this time trend, there is\ntoo much scatter to be able to infer a difference between the treatments.\n\nExtra (maybe I could branch off into another question sometime?) I was thinking that this is not terribly clear, so I thought I would\nfake up some data where there is a treatment effect and a time effect\n(but no interaction), and draw a spaghetti plot, so you can see the\ndifference, idealized somewhat of course. Let's try to come up with\nsomething with the same kind of time effect, up at time 2 and then\ndown afterwards, that is the same for two drugs A and B. Here's what I\ncame up with:\n\n::: {.cell}\n\n```{.r .cell-code}\nfake <- read.csv(\"fake.csv\", header = T)\nfake\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   subject drug t1 t2 t3 t4\n1        1    a 10 15 13 11\n2        2    a 11 14 12  9\n3        3    a 12 16 13 11\n4        4    a 10 14 11 11\n5        5    a 11 13 10  9\n6        6    b  7 10  8  6\n7        7    b  8 12 11  9\n8        8    b  5  9  7  5\n9        9    b  7  8  6  5\n10      10    b  8 12 11  9\n```\n:::\n:::\n\n \n\nYou can kind of get the idea that the pattern over time is up and then\ndown, so that it finishes about where it starts, but the numbers for\ndrug A are usually bigger than the  ones for drug B, consistently over\ntime. So there ought not to be an interaction, but there ought to be\nboth a time effect and a drug effect.\n\nLet's see whether we can demonstrate that. First, a spaghetti plot,\nwhich involves getting the data in long format first. I'm saving the\nlong format to use again later.\n\n::: {.cell}\n\n```{.r .cell-code}\nfake %>% \n  pivot_longer(t1:t4, names_to=\"times\", values_to=\"score\") -> fake.long\nfake.long %>%\n  ggplot(aes(x = times, y = score, colour = drug, group = subject)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/geriatrics-12-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThe reds are consistently higher than the blues (drug effect), the\npattern over time goes up and then down (time effect), but the time\neffect is basically the same for both drugs (no interaction).\n\nI got the plot wrong the first time, because I forgot whether I was\ndoing an interaction plot (where `group=` and `colour=`\nare the same) or a spaghetti plot (where `group` has to be\n`subject` and the colour represents the treatment, usually). \n\nLet's do the repeated-measures ANOVA and see whether my guess above is right:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(fake, cbind(t1, t2, t3, t4))\nfake.1 <- lm(response ~ drug, data = fake)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\nfake.2 <- Manova(fake.1, idata = times.df, idesign = ~times)\n```\n:::\n\n \n\nAfter typing this kind of stuff out a few too many times, I hope\nyou're getting the idea \"function\". Also, the construction of the\nresponse is kind of annoying, where you have to list all the time\ncolumns. The trouble is, `response` has to be a `matrix`,\nwhich it is:\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n \n\nbut if you do the obvious thing of selecting the columns of the data\nframe that you want:\n\n::: {.cell}\n\n```{.r .cell-code}\nfake %>% select(t1:t4) -> r\nclass(r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"data.frame\"\n```\n:::\n:::\n\n \n\nyou get a data frame instead. I think this would work:\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- fake %>% select(t1:t4) %>% as.matrix()\nclass(r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n \n\nThe idea is that you select the columns you want as a data frame first\n(with `select`), and then turn it into a `matrix` at the\nend. \n\nThis is the kind of thing you'd have to do in a function, I think,\nsince you'd have to have some way of telling the function which are\nthe \"time\" columns. Anyway, hope you haven't forgotten what we were\ndoing:^[I got sidetracked, surprise surprise.]\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fake.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     15920.1\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1   0.98478 517.7268      1      8 1.4752e-08 ***\nWilks             1   0.01522 517.7268      1      8 1.4752e-08 ***\nHotelling-Lawley  1  64.71585 517.7268      1      8 1.4752e-08 ***\nRoy               1  64.71585 517.7268      1      8 1.4752e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug \n\n Response transformation matrix:\n   (Intercept)\nt1           1\nt2           1\nt3           1\nt4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)       532.9\n\nMultivariate Tests: drug\n                 Df test stat approx F num Df den Df    Pr(>F)   \nPillai            1   0.68417 17.33008      1      8 0.0031525 **\nWilks             1   0.31583 17.33008      1      8 0.0031525 **\nHotelling-Lawley  1   2.16626 17.33008      1      8 0.0031525 **\nRoy               1   2.16626 17.33008      1      8 0.0031525 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1 times2 times3\ntimes1    1.6   15.2    6.8\ntimes2   15.2  144.4   64.6\ntimes3    6.8   64.6   28.9\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1   0.98778 161.7086      3      6 3.9703e-06 ***\nWilks             1   0.01222 161.7086      3      6 3.9703e-06 ***\nHotelling-Lawley  1  80.85428 161.7086      3      6 3.9703e-06 ***\nRoy               1  80.85428 161.7086      3      6 3.9703e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: drug:times \n\n Response transformation matrix:\n   times1 times2 times3\nt1      1      0      0\nt2      0      1      0\nt3      0      0      1\nt4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1 times2 times3\ntimes1    0.4    0.8   -0.2\ntimes2    0.8    1.6   -0.4\ntimes3   -0.2   -0.4    0.1\n\nMultivariate Tests: drug:times\n                 Df test stat approx F num Df den Df   Pr(>F)  \nPillai            1 0.6490046  3.69808      3      6 0.081108 .\nWilks             1 0.3509954  3.69808      3      6 0.081108 .\nHotelling-Lawley  1 1.8490401  3.69808      3      6 0.081108 .\nRoy               1 1.8490401  3.69808      3      6 0.081108 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n            Sum Sq num Df Error SS den Df  F value    Pr(>F)    \n(Intercept) 3980.0      1     61.5      8 517.7268 1.475e-08 ***\ndrug         133.2      1     61.5      8  17.3301  0.003152 ** \ntimes         87.9      3     14.9     24  47.1812 3.233e-10 ***\ndrug:times     1.5      3     14.9     24   0.7919  0.510323    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n           Test statistic p-value\ntimes             0.18708 0.04852\ndrug:times        0.18708 0.04852\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n            GG eps Pr(>F[GG])    \ntimes      0.54943  1.886e-06 ***\ndrug:times 0.54943     0.4505    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n              HF eps   Pr(>F[HF])\ntimes      0.6735592 1.708486e-07\ndrug:times 0.6735592 4.709652e-01\n```\n:::\n:::\n\nThe usual procedure: check for sphericity first. Here, that is *just* rejected, but since the P-value on the sphericity test is only just less than 0.05, you would expect the P-values on the univariate test for interaction and the Huynh-Feldt adjustment to be similar, and they are (0.510 and 0.471 respectively). Scrolling up a bit further, the multivariate test for interaction only just fails to be significant, with a P-value of 0.081. It is a mild concern that this one differs so much from the others; normally the multivariate test(s) would tell a similar story to the others.\n\nThe drug-by-time interaction is not significant, so we go ahead and\ninterpret the main effects: there is a time effect (the increase at\ntime 2 that I put in on purpose), and, allowing for the time effect,\nthere is also a difference between the drugs (because the drug A\nscores are a bit higher than the drug B scores). The procedure is to look at the Huynh-Feldt adjusted P-value for time ($1.71 \\times 10^{-7}$), expecting it to be a bit bigger than the one in the univariate table (it is) and comparable to the one for time in the appropriate multivariate analysis ($3.97 \\times 10^{-6}$; it is, but remember to scroll back enough). In this kind of analysis, the effect of drug is averaged over time,^[See below.] so the test for the drug main effect is unaffected by sphericity. Its P-value, 0.0032, is identical in the univariate and multivariate tables, and you see that the drug main effect is not part of the sphericity testing.\n\nWhat if we ignored the time effect? You'd think we could do something\nlike this, treating the measurements at different times as replicates:\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(fake.long)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n  subject drug  times score\n    <int> <chr> <chr> <int>\n1       1 a     t1       10\n2       1 a     t2       15\n3       1 a     t3       13\n4       1 a     t4       11\n5       2 a     t1       11\n6       2 a     t2       14\n```\n:::\n\n```{.r .cell-code}\nfake.3 <- aov(score ~ drug, data = fake.long)\nsummary(fake.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ndrug         1  133.2  133.22   30.54 2.54e-06 ***\nResiduals   38  165.8    4.36                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nbut this would be *wrong*, because we are acting as if we have 40\nindependent observations, which we don't (this is the point of doing\nrepeated measures in the first place). It looks as if we have achieved\nsomething by getting a lower P-value for `drug`, but we\nhaven't really, because we have done so by cheating.\n\nWhat we could do instead is to average the scores for each subject\nover all the times,^[This would be allowable, since we are  averaging *over* the time-dependence; we are creating 10  independent averages, from the 10 subjects. People do this kind of  thing, instead of having to deal with the repeated measures.] for\nwhich we go back to the original data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\nfake\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   subject drug t1 t2 t3 t4\n1        1    a 10 15 13 11\n2        2    a 11 14 12  9\n3        3    a 12 16 13 11\n4        4    a 10 14 11 11\n5        5    a 11 13 10  9\n6        6    b  7 10  8  6\n7        7    b  8 12 11  9\n8        8    b  5  9  7  5\n9        9    b  7  8  6  5\n10      10    b  8 12 11  9\n```\n:::\n\n```{.r .cell-code}\nfake %>%\n  mutate(avg.score = (t1 + t2 + t3 + t4) / 4) %>%\n  aov(avg.score ~ drug, data = .) %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value  Pr(>F)   \ndrug         1  33.31   33.31   17.33 0.00315 **\nResiduals    8  15.37    1.92                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nAh, now, this is very interesting. I was hoping that by throwing away\nthe time information (which is useful), we would have diminished\nthe significance of the drug effect. By failing to include the\ntime-dependence in our model, we ought to have introduced some extra\nvariability, which ought to weaken our test. But this test gives\n*exactly the same* P-value as the ones in the MANOVA, and it looks\nlike exactly the same test (the $F$-value is the same too). So it\nlooks as if this is what the MANOVA is doing, to assess the\n`drug` effect: it's averaging over the times. Since the same\nfour (here) time points are being used to compute the average for each\nsubject, we are comparing like with like at least, and even if there\nis a large time effect, I suppose it's going to have the same effect\non each average. For example, if as here the scores at time 2 are\ntypically highest, all the averages are going to be composed of one\nhigh score and three lower ones. So maybe I have to go back and dilute\nmy conclusions about the significance of treatments earlier: it's actually\nsaying that there is a difference between the two remaining treatments\n*averaged over time* rather than *allowing for time* as I\nsaid earlier.\n    \n\n$\\blacksquare$\n\n\n\n\n##  Children's stress levels and airports\n\n\n If you did STAC32, you might remember this question, which\nwe can now do properly. Some of this question is a repeat from there.\n\nThe data in [link](http://ritsokiguess.site/datafiles/airport.txt)\nare based on a 1998 study of stress levels in children as a result of\nthe building of a new airport in Munich, Germany. A total of 200\nchildren had their epinephrine levels (a stress indicator) measured at\neach of four different times: before the airport was built, and 6, 18\nand 36 months after it was built. The four measurements are labelled\n`epi_1` through `epi_4`.  Out of the children, 100\nwere living near the new airport (location 1 in the data set), and\ncould be expected to suffer stress because of the new airport. The\nother 100 children lived in the same city, but outside the noise\nimpact zone. These children thus serve as a control group. The\nchildren are identified with numbers 1 through 200.\n\n\n\n(a) If we were testing for the effect of time, explain briefly\nwhat it is about the structure of the data that would make an\nanalysis of variance *inappropriate*.\n\n\nSolution\n\n\nIt is the fact that each child was measured four times, rather\nthan each measurement being on a *different* child (with\nthus $4\\times 200=800$ observations altogether). It's\nthe same distinction as between matched pairs and a two-sample\n$t$ test.\n\n$\\blacksquare$\n\n(b) Read the data into R and demonstrate that you have the right\nnumber of observations and variables.\n\n\nSolution\n\n\nThe usual, data values separated by one space:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/airport.txt\"\nairport <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 200 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\ndbl (6): epi_1, epi_2, epi_3, epi_4, location, child\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nairport\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 x 6\n    epi_1  epi_2 epi_3 epi_4 location child\n    <dbl>  <dbl> <dbl> <dbl>    <dbl> <dbl>\n 1  89.6  253.   214.   209.        1     1\n 2 -55.5   -1.45  26.0  259.        1     2\n 3 201.   280.   265.   174.        1     3\n 4 448.   349.   386.   225.        1     4\n 5  -4.60 315.   331.   333.        1     5\n 6 231.   237.   488.   319.        1     6\n 7 227.   469.   382.   359.        1     7\n 8 336.   280.   362.   472.        1     8\n 9  16.8  190.    90.9  145.        1     9\n10  54.5  359.   454.   199.        1    10\n# i 190 more rows\n```\n:::\n:::\n\n \n\nThere are 200 rows (children), with four `epi` measurements, a\nlocation and a child identifier, so that looks good.\n\n(I am mildly concerned about the negative `epi` measurements,\nbut I don't know what the scale is, so presumably they are all\nright. Possibly epinephrine is measured on a log scale, so that a\nnegative value here is less than 1 on the original scale that we don't\nsee.)\n\n$\\blacksquare$\n\n(c) Create and save a \"longer\" data frame with all the epinephrine\nvalues collected together into one column.\n\n\nSolution\n\n`pivot_longer`:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>% pivot_longer(starts_with(\"epi\"), names_to=\"when\", values_to=\"epinephrine\") -> airport.long\nairport.long\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 800 x 4\n   location child when  epinephrine\n      <dbl> <dbl> <chr>       <dbl>\n 1        1     1 epi_1       89.6 \n 2        1     1 epi_2      253.  \n 3        1     1 epi_3      214.  \n 4        1     1 epi_4      209.  \n 5        1     2 epi_1      -55.5 \n 6        1     2 epi_2       -1.45\n 7        1     2 epi_3       26.0 \n 8        1     2 epi_4      259.  \n 9        1     3 epi_1      201.  \n10        1     3 epi_2      280.  \n# i 790 more rows\n```\n:::\n:::\n\n \n\nSuccess. I'm saving the name `time` for later, so I've called\nthe time points `when` for now. There were 4 measurements on\neach of 200 children, so the long data frame should (and does) have\n$200\\times 4 = 800$ rows.\n\n$\\blacksquare$\n\n(d) Make a \"spaghetti plot\" of these data: that is, a plot of\nepinephrine levels against time, with the locations identified by\ncolour, and the points for the same child joined by lines.  To do\nthis: (i) from the long data frame, create a new column containing\nonly the numeric values of time (1 through 4), (ii) plot epinephrine\nlevel against time with the points grouped by child and coloured by\nlocation (which you may have to turn from a number into a factor.)\n\n\nSolution\n\nNote the use of the different things for `colour` and `group`, as usual for a spaghetti plot. Also, note that the locations are identified by number, but the number is only a label, and we want to use different colours for the different locations, so we need to turn `location` into a factor for this. \n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>%\n  mutate(time = parse_number(when)) %>%\n  ggplot(aes(x = time, y = epinephrine, colour = factor(location), group = child)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/toofat-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThis^[The term \"accidental aRt\" is sometimes used for graphs that cross the boundary between being informative and looking like a piece of art, particularly if it was not done on purpose. This one is a bit like that.] is different from the plot we had in C32, where I had you use a\ndifferent colour for each *child*, and we ended up with a huge\nlegend of all the children (which we then removed). \n\nIf you forget to turn `location` into a factor, `ggplot`\nwill assume that you want `location` to be on a continuous\nscale, and you'll get two shades of blue. \n\nAnother problem with this plot is that there are so many children, you\ncan't see the ones underneath because the ones on top are overwriting\nthem. The solution to that is to make the lines (partly) transparent,\nwhich is controlled by a parameter `alpha`:^[This is  different from the 0.05 $\\alpha$.]\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>%\n  mutate(time = parse_number(when)) %>%\n  ggplot(aes(x = time, y = epinephrine, colour = factor(location), group = child)) +\n  geom_point() + geom_line(alpha = 0.2)\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/skinnier-1.pdf){fig-pos='H'}\n:::\n:::\n\n   \nIt seems to make the lines skinnier, so they look more like\nthreads. Even given the lesser thickness, they seem to be a little bit\nsee-through as well. You can experiment with adding transparency to\nthe points in addition. \n\n$\\blacksquare$\n\n(e) What do you see on your spaghetti plot? We are looking ahead\nto possible effects of time, location and their interaction.\n\n\nSolution\n\n\nThis is not clear, so it's very much your call.\nI see the red spaghetti strands as going up further\n(especially) and maybe down further than the blue ones. The\nepinephrine levels of the children near the new airport are\ndefinitely more spread out, and maybe have a higher mean, than\nthose of the control group of children not near the airport.\nThe red spaghetti strands show something of an increase over\ntime, at least up to time 3, after which they seem to drop\nagain. The blue strands, however, don't show any kind of trend\nover time. Since the time trend is different for the two\nlocations, I would expect to see a significant interaction.\n\n$\\blacksquare$\n\n(f) The spaghetti plot was hard to interpret because there are\nso many children. Calculate the mean epinephrine levels for each\nlocation-time combination, and make an interaction plot with time\non the $x$-axis and location as the second factor.\n\n\nSolution\n\n\nWe've done this before:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>%\n  mutate(time = parse_number(when)) %>%\n  mutate(floc = factor(location)) %>%\n  group_by(floc, time) %>%\n  summarize(mean.epi = mean(epinephrine)) %>%\n  ggplot(aes(x = time, y = mean.epi, group = floc, colour = floc)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'floc'. You can override using the\n`.groups` argument.\n```\n:::\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/airport-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n         \n\nI wanted the actual numerical times, so I made them again. Also, it\nseemed to be easier to create a factor version of the numeric location\nup front, and then use it several times later. I'm actually not sure\nthat you need it here, since `group_by` works with the\ndistinct values of a variable, whatever they are, and `group`\nin a boxplot may or may not insist on something other than a number. I\nshould try it:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>%\n  mutate(time = parse_number(when)) %>%\n  group_by(location, time) %>%\n  summarize(mean.epi = mean(epinephrine)) %>%\n  ggplot(aes(x = time, y = mean.epi, group = location, colour = location)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n```\n:::\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/airport-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n          \n\nIt seems that `colour` requires a non-number:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>%\n  mutate(time = parse_number(when)) %>%\n  group_by(location, time) %>%\n  summarize(mean.epi = mean(epinephrine)) %>%\n  ggplot(aes(x = time, y = mean.epi, group = location, colour = factor(location))) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'location'. You can override using the\n`.groups` argument.\n```\n:::\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/airport-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n         \nWith a long pipeline like this, none of us get it right the first time (I\ncertainly didn't), so be prepared to debug it one line at a time. The\nway I like to do this is to take the pipe symbol and move it down to\nthe next line (moving the cursor to just before it and hitting\nEnter). This ends the pipe at the end of *this* line and displays\nwhat it produces so far. When you are happy with that, go to the start\nof the next line (that currently has a pipe symbol by itself) and hit\nBackspace to move the pipe symbol back where it was. Then go to the\nend of the next line (where the next pipe symbol is), move *that*\nto a line by itself, and so on. Keep going until each line produces\nwhat you want, and when you are finished, the whole pipeline will do\nwhat you want.\n\n$\\blacksquare$\n\n(g) What do you conclude from your interaction plot? Is your\nconclusion clearer than from the spaghetti plot?\n\n\nSolution\n\n\nThe two \"traces\" are not parallel, so I would expect to see\nan interaction between location and time. The big difference\nseems to be between times 1 and 2; the traces are the same at\ntime 1, and more or less parallel after time 2. Between times\n1 and 2, the mean epinephrine level of the children near the\nnew airport increases sharply, whereas for the children in the\ncontrol group it increases much less.\nThis, to my mind, is very much easier to interpret than the\nspaghetti plot, even the second version with the thinner\nstrands, because there is a lot of variability there that\nobscures the overall pattern. The interaction plot is plain as\nday, but it might be an oversimplification because it doesn't\nshow variability.\n\n$\\blacksquare$\n\n(h) Run a repeated-measures analysis of variance and display the\nresults. Go back to your original data frame, the one you read in\nfrom the file, for this. You'll need to make sure your numeric\n`location` gets treated as a `factor`.\n\n\nSolution\n\n\nThe usual process. I'll try the other way I used of making the\n`response`: \n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>%\n  select(epi_1:epi_4) %>%\n  as.matrix() -> response\nairport.1 <- lm(response ~ factor(location), data = airport)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\nairport.2 <- Manova(airport.1, idata = times.df, idesign = ~times)\nsummary(airport.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   268516272\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.920129     2281      1    198 < 2.22e-16 ***\nWilks             1  0.079871     2281      1    198 < 2.22e-16 ***\nHotelling-Lawley  1 11.520204     2281      1    198 < 2.22e-16 ***\nRoy               1 11.520204     2281      1    198 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: factor(location) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     3519790\n\nMultivariate Tests: factor(location)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.1311980 29.90002      1    198 1.3611e-07 ***\nWilks             1 0.8688020 29.90002      1    198 1.3611e-07 ***\nHotelling-Lawley  1 0.1510102 29.90002      1    198 1.3611e-07 ***\nRoy               1 0.1510102 29.90002      1    198 1.3611e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1     times2     times3\ntimes1  497500.84 -113360.08 -56261.667\ntimes2 -113360.08   25830.12  12819.731\ntimes3  -56261.67   12819.73   6362.552\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.3274131 31.80405      3    196 < 2.22e-16 ***\nWilks             1 0.6725869 31.80405      3    196 < 2.22e-16 ***\nHotelling-Lawley  1 0.4867966 31.80405      3    196 < 2.22e-16 ***\nRoy               1 0.4867966 31.80405      3    196 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: factor(location):times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1    times2      times3\ntimes1 533081.68 206841.01 -14089.6126\ntimes2 206841.01  80256.38  -5466.9104\ntimes3 -14089.61  -5466.91    372.3954\n\nMultivariate Tests: factor(location):times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.2373704 20.33516      3    196 1.6258e-11 ***\nWilks             1 0.7626296 20.33516      3    196 1.6258e-11 ***\nHotelling-Lawley  1 0.3112525 20.33516      3    196 1.6258e-11 ***\nRoy               1 0.3112525 20.33516      3    196 1.6258e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                         Sum Sq num Df Error SS den Df  F value    Pr(>F)    \n(Intercept)            67129068      1  5827073    198 2281.000 < 2.2e-16 ***\nfactor(location)         879947      1  5827073    198   29.900 1.361e-07 ***\ntimes                    475671      3  3341041    594   28.190 < 2.2e-16 ***\nfactor(location):times   366641      3  3341041    594   21.728 2.306e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                       Test statistic  p-value\ntimes                          0.9488 0.066194\nfactor(location):times         0.9488 0.066194\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                        GG eps Pr(>F[GG])    \ntimes                  0.96685  < 2.2e-16 ***\nfactor(location):times 0.96685   5.35e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                          HF eps   Pr(>F[HF])\ntimes                  0.9827628 8.418534e-17\nfactor(location):times 0.9827628 3.571900e-13\n```\n:::\n:::\n\n         \n\n$\\blacksquare$\n\n(i) What do you conclude from the MANOVA? Is that consistent\nwith your graphs? Explain briefly.\n\n\nSolution\n\nStart with Mauchly's test near the bottom. This is not quite significant (for the interaction), so we are entitled to look at the univariate test for interaction, which is $2.3 \\times 10^{-13}$, extremely significant. If you want a comparison, look at the Huynh-Feldt adjustment for the interaction, which is almost exactly the same ($3.57 \\times 10^{-13}$), or the multivariate tests for the interaction (almost the same again).\n\n\nSo, we start and end with the significant interaction: there is an\neffect of location, but the nature of that effect depends on\ntime. This is the same as we saw in the interaction plot:\nfrom time 2 on, the mean epinephrine levels for the children near\nthe new airport were clearly higher. \n\nIf you stare at the spaghetti plot, you *might* come to\nthe same conclusion. Or you might not! I suppose those red\ndots at time 2 are mostly at the top, and generally so\nafterwards, whereas at time 1 they are all mixed up with the\nblue ones.\n\nInteractions of this sort in this kind of analysis  are very\ncommon. There is an \"intervention\" or \"treatment\", and the\ntime points are chosen so that the first one is before the\ntreatment happens, and the other time points are after. Then,\nthe results are very similar for the first time point, and\nvery different after that, rather than being (say) always\nhigher for the treatment group by about the same amount for\nall times (in which case there would be no interaction). \n\nSo, you have some choices in practice as to how you might\ngo. You might do the MANOVA, get a significant interaction,\nand draw an interaction plot to see why. You might stop there,\nor you might do something like what we did in class: having\nseen that the first time point is different from the others\nfor reasons that you can explain, do the analysis again, but\nomitting the first time point. For the MANOVA, that means\ntweaking your definition of your `response` to omit the\nfirst time point. The rest of it stays the same, though you\nmight want to change your model numbers rather than re-using\nthe old ones as I did:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>%\n  select(epi_2:epi_4) %>%\n  as.matrix() -> response\nairport.1 <- lm(response ~ factor(location), data = airport)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\nairport.2 <- Manova(airport.1, idata = times.df, idesign = ~times)\nsummary(airport.2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in summary.Anova.mlm(airport.2): HF eps > 1 treated as 1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   165867956\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.918531 2232.383      1    198 < 2.22e-16 ***\nWilks             1  0.081469 2232.383      1    198 < 2.22e-16 ***\nHotelling-Lawley  1 11.274662 2232.383      1    198 < 2.22e-16 ***\nRoy               1 11.274662 2232.383      1    198 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: factor(location) \n\n Response transformation matrix:\n      (Intercept)\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     3567099\n\nMultivariate Tests: factor(location)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.1951509 48.00886      1    198 5.8219e-11 ***\nWilks             1 0.8048491 48.00886      1    198 5.8219e-11 ***\nHotelling-Lawley  1 0.2424690 48.00886      1    198 5.8219e-11 ***\nRoy               1 0.2424690 48.00886      1    198 5.8219e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2\nepi_2      1      0\nepi_3      0      1\nepi_4     -1     -1\n\nSum of squares and products for the hypothesis:\n         times1    times2\ntimes1 25830.12 12819.731\ntimes2 12819.73  6362.552\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df  Pr(>F)\nPillai            1 0.0123563 1.232325      2    197 0.29385\nWilks             1 0.9876437 1.232325      2    197 0.29385\nHotelling-Lawley  1 0.0125109 1.232325      2    197 0.29385\nRoy               1 0.0125109 1.232325      2    197 0.29385\n\n------------------------------------------\n \nTerm: factor(location):times \n\n Response transformation matrix:\n      times1 times2\nepi_2      1      0\nepi_3      0      1\nepi_4     -1     -1\n\nSum of squares and products for the hypothesis:\n         times1     times2\ntimes1 80256.38 -5466.9104\ntimes2 -5466.91   372.3954\n\nMultivariate Tests: factor(location):times\n                 Df test stat approx F num Df den Df    Pr(>F)   \nPillai            1 0.0508561 5.277736      2    197 0.0058507 **\nWilks             1 0.9491439 5.277736      2    197 0.0058507 **\nHotelling-Lawley  1 0.0535811 5.277736      2    197 0.0058507 **\nRoy               1 0.0535811 5.277736      2    197 0.0058507 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n                         Sum Sq num Df Error SS den Df   F value    Pr(>F)    \n(Intercept)            55289319      1  4903856    198 2232.3830 < 2.2e-16 ***\nfactor(location)        1189033      1  4903856    198   48.0089 5.822e-11 ***\ntimes                     12915      2  2281728    396    1.1207  0.327070    \nfactor(location):times    57397      2  2281728    396    4.9807  0.007306 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n                       Test statistic p-value\ntimes                         0.99068 0.39746\nfactor(location):times        0.99068 0.39746\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n                        GG eps Pr(>F[GG])   \ntimes                  0.99076   0.326702   \nfactor(location):times 0.99076   0.007483 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                         HF eps  Pr(>F[HF])\ntimes                  1.000731 0.327069634\nfactor(location):times 1.000731 0.007305714\n```\n:::\n:::\n\n         \n\nThe interaction is still significant. The sphericity test is not significant, so you can use the 0.0073 in the univariate tests as your P-value (note that the Huynh-Feldt adjustment is actually not adjusted at all).\n\nSo there is still not a\nconsistent effect of being near the new airport on epinephrine levels:\nthat is to say, the effect of the new airport *still* varies over\ntime. That might be because (looking at the interaction plot) for the\nchildren near the new airport, the mean epinephrine level went up\nagain between times 2 and 3, whereas for the control children it (for\nsome reason) went dramatically down over the same time period.\n\nWe have lots of data here (200 children), so the significant\ninteraction effect also might not be very big.\n\nExperimental designs like this are kind of awkward, because you expect\nthere to be some kind of pattern over time for the treatment group,\nthat will vary over time, whereas for the control group, you expect\nthere to be no pattern over time. So a significant difference shows up\nas an *interaction*, which is messier to interpret than you would\nlike. \n\nExtra: the other way to analyze repeated measures data ^[This is something *I* want to  understand, so I will share my findings with you. You can read them  or not, as you choose.] is to treat them as \"mixed models\", which\nrequires a different kind of analysis using the `lme4`\npackage. I always forget how these go, and I have to look them up when\nI need them, but the idea is this: the treatments you observe, and the\ntime points at which you observe them, are\ntypically the only ones you care about (a so-called \"fixed effect\"),\nbut the individuals (children, here)\nwhich you happen to observe are\nsomething like a random sample of all the children you might have\nobserved (a so-called \"random effect\"). Models with random effects\nin them are called \"mixed models\" (or, I suppose, models with both\nfixed and random effects). This matters because you have repeated\nobservations on the *same* child. Some people like to think of\nthis in terms of \"sources of variability\": the epinephrine levels\nvary because of the location and time at which they were observed, but\nalso because of the particular child they happen to have been observed\nfor: each child has a \"random effect\" that they carry with them\nthrough all the time points at which they are observed.\n\nLet's see if we can make it fly for this example. We need the data in\n\"long\" format, the way we arranged it for graphing: the data frame\n`airport.long`. I'd like to convert things to factors first:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.long %>% mutate(\n  fchild = factor(child),\n  flocation = factor(location)\n) -> fairport\n\nairport.3 <- lmer(epinephrine ~ flocation * when + (1 | fchild), data = fairport)\nanova(airport.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n               npar Sum Sq Mean Sq F value\nflocation         1 168177  168177  29.900\nwhen              3 475671  158557  28.190\nflocation:when    3 366641  122214  21.728\n```\n:::\n\n```{.r .cell-code}\ndrop1(airport.3, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nepinephrine ~ flocation * when + (1 | fchild)\n               npar    AIC    LRT   Pr(Chi)    \n<none>              9521.2                     \nflocation:when    3 9577.6 62.475 1.739e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nThe `anova` doesn't actually give us any tests, but what you\nsee in the ANOVA table are the fixed effects. These are testable. The\neasiest way to see what you can get rid of is `drop1`; the\nchi-squared test appears to be the right one (more on that\nbelow). This says that the interaction is strongly significant, and we\nshould not consider removing it, the same conclusion as our \n\"profile analysis\" before.\nThe other choice for\ntesting is to fit a model without what you\nare testing and use `anova` to compare the two models:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport.4 <- update(airport.3, . ~ . - flocation:when)\nanova(airport.4, airport.3)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: fairport\nModels:\nairport.4: epinephrine ~ flocation + when + (1 | fchild)\nairport.3: epinephrine ~ flocation * when + (1 | fchild)\n          npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nairport.4    7 9577.6 9610.4 -4781.8   9563.6                         \nairport.3   10 9521.2 9568.0 -4750.6   9501.2 62.475  3  1.739e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nThere are technical considerations involved in comparing the fit of\ntwo models (which is the reason for the \n\"refitting models with...\"): there is one method for estimating and a different\nmethod for testing. The test is based on \"likelihood ratio\", which\nmeans that the right test for the `drop1` above is\n`Chisq`. \n\nIf you omit the test in `drop1`, it just gives\nyou AIC values, which you can use for an informal assessment. In this\ncase, `<none>` has a much smaller AIC than the interaction\n(smaller by over 50), so there's no way we should entertain taking out\nthe interaction. However, if it had not been significant, we would\njust take it out by fitting a model like `airport4`: there is\nno distinction here between \"within-subject\" and \n\"between-subject\"\nfactors that prevented us from taking the interaction out in profile\nanalysis. \n\nAs ever when an interaction is significant, we might think  about\nsimple effects: that is, look at the two locations separately. That\nmakes sense here because of the kind of experimental design it is: we\n*expect* a different kind of relationship with time for the\n\"treatment\" children (the ones living near the new airport) as\ncompared to the control children, the ones who live farther away. That\napproach would work with either the profile-analysis way using\n`Manova` or the mixed-modelling way using `lmer`. In\neither case, we'd expect to see a time effect at location 1 but not at\nlocation 2. (Having separated out locations, only the time effect\nwould be left for testing.) I guess I have to show you that, but I\nhave to get ready for class first.\n\nLater\\ldots\n\nThe nice thing about Wednesday evenings is that I am so tired from\nclass that I have energy for almost nothing except playing with these\nthings. So let's have a crack at it. \n\nLet's start with location 1, at which we expect there to be\nsomething happening. This is really a simple effect of time at\nlocation 1, but in repeated measures guise. The awkwardness is that\nthe profile analysis needs the wide-format data, while the mixed-model\nanalysis needs long format, so we'll have to repeat our process, once\nfor each format of data set:\n\n::: {.cell}\n\n```{.r .cell-code}\nloc1 <- airport %>% filter(location == 1)\nresponse <- loc1 %>% select(epi_1:epi_4) %>% as.matrix()\nloc1.1 <- lm(response ~ 1, data = loc1)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\nloc1.2 <- Manova(loc1.1, idata = times.df, idesign = ~times)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNote: model has only an intercept; equivalent type-III tests substituted.\n```\n:::\n\n```{.r .cell-code}\nsummary(loc1.2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in summary.Anova.mlm(loc1.2): HF eps > 1 treated as 1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nType III Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   166760848\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.897288 864.8584      1     99 < 2.22e-16 ***\nWilks             1  0.102712 864.8584      1     99 < 2.22e-16 ***\nHotelling-Lawley  1  8.735944 864.8584      1     99 < 2.22e-16 ***\nRoy               1  8.735944 864.8584      1     99 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n           times1    times2     times3\ntimes1 1030275.32 87978.051 -71100.691\ntimes2   87978.05  7512.688  -6071.484\ntimes3  -71100.69 -6071.484   4906.755\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.4542272 26.90988      3     97 9.4963e-13 ***\nWilks             1 0.5457728 26.90988      3     97 9.4963e-13 ***\nHotelling-Lawley  1 0.8322643 26.90988      3     97 9.4963e-13 ***\nRoy               1 0.8322643 26.90988      3     97 9.4963e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n              Sum Sq num Df Error SS den Df F value    Pr(>F)    \n(Intercept) 41690212      1  4772262     99 864.858 < 2.2e-16 ***\ntimes         776618      3  2774062    297  27.716 7.869e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n      Test statistic p-value\ntimes        0.95865 0.53129\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n       GG eps Pr(>F[GG])    \ntimes 0.97423  1.748e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        HF eps   Pr(>F[HF])\ntimes 1.007064 7.869406e-16\n```\n:::\n:::\n\n \n\nThis is one of those times where a pipe doesn't quite do it. We need\nto grab the data for location 1, but then we need to do two things\nwith it: one, create a response variable, and two, feed the response\nvariable *and* the location 1 data into `lm`. Pipes are\nfor linear sequences of things to do, and this one diverges. So, save\nwhat you need to save, and then do it without a pipe.\n\nThis is one of those explanatory-variable-less repeated measures where\nyou *only* have things varying over time. Here, the treatment was\nlocation, and we're only looking at one of those locations. We still\nhave a within-subject design (the four times), but there is no\nbetween-subject design left.\n\nThe conclusion is that there is very strong evidence of a time effect\nfor location 1, as we would have guessed.\n\nNow, let's see whether the mixed-model analysis comes to the same\nconclusion. This time, we have to start from the *long* data set\n(the one I'm using is the one called `fairport` with things as\nfactors) and pull out the rows for location 1. The only remaining\nfixed effect is time:\n\n::: {.cell}\n\n```{.r .cell-code}\nfairport %>%\n  filter(location == 1) %>%\n  lmer(epinephrine ~ when + (1 | fchild), data = .) %>%\n  drop1(test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nepinephrine ~ when + (1 | fchild)\n       npar    AIC    LRT   Pr(Chi)    \n<none>      4964.1                     \nwhen      3 5032.1 74.048 5.796e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nThe effect of time is, once again, substantial. The P-value is not the\nsame, because the test is not the same, but it is very similar and\npoints to exactly the same conclusion.\n\nI should perhaps point out that you don't *have* to do these\nmodels in a pipe. I just did because it was easier for me. But there's\nnothing wrong with doing it like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp <- fairport %>% filter(location == 1)\ntmp.1 <- lmer(epinephrine ~ when + (1 | child), data = tmp)\ndrop1(tmp.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nepinephrine ~ when + (1 | child)\n       npar    AIC    LRT   Pr(Chi)    \n<none>      4964.1                     \nwhen      3 5032.1 74.048 5.796e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nand it gives exactly the same result.\n\nThis last model (in either of its versions) is a so-called random\nintercepts model. What it says is that epinephrine level for a child\ndepends on time, but the effect of being one child rather than another\nis to shunt the mean epinephrine level up or down by a fixed amount\nfor all times. It seems to me that this is a very reasonable way to\nmodel the child-to-child variation in this case, but in other cases,\nthings might be different. `lmer` allows more sophisticated\nthings, like for example the random child effect depending linearly on\ntime. To do that, you'd rewrite the above like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nfairport %>%\n  filter(location == 1) %>%\n  lmer(epinephrine ~ when + (1 + when | fchild), data = .) %>%\n  drop1(test = \"Chisq\")\n```\n:::\n\n \n\nThe change is on the `lmer` line: the bit in brackets has a\nlinear model in `when` for each child. I didn't run that, so\nI'm not certain it works,^[I'm suspicious about *when*  needing to be the numerical time inside. Not sure.] but that's the idea.\n\nAll right, let's get on to location 2. This was the control location,\nso we expect to see *no* dependence of epinephrine level on time,\neither in the profile analysis or in the mixed-model analysis. There\nis a large amount of copying and pasting coming up:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>% filter(location == 2) -> loc2\nloc2 %>% select(epi_1:epi_4) %>% as.matrix() -> response\nloc2.1 <- lm(response ~ 1, data = loc1)\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\nloc2.2 <- Manova(loc2.1, idata = times.df, idesign = ~times)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNote: model has only an intercept; equivalent type-III tests substituted.\n```\n:::\n\n```{.r .cell-code}\nsummary(loc2.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nType III Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n      (Intercept)\nepi_1           1\nepi_2           1\nepi_3           1\nepi_4           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)   105275213\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.961466 2470.169      1     99 < 2.22e-16 ***\nWilks             1  0.038534 2470.169      1     99 < 2.22e-16 ***\nHotelling-Lawley  1 24.951203 2470.169      1     99 < 2.22e-16 ***\nRoy               1 24.951203 2470.169      1     99 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n      times1 times2 times3\nepi_1      1      0      0\nepi_2      0      1      0\nepi_3      0      0      1\nepi_4     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1    times2     times3\ntimes1  307.1984  5502.882   749.4117\ntimes2 5502.8823 98573.811 13424.3048\ntimes3  749.4117 13424.305  1828.1931\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1 0.3023560 14.01313      3     97 1.1613e-07 ***\nWilks             1 0.6976440 14.01313      3     97 1.1613e-07 ***\nHotelling-Lawley  1 0.4333957 14.01313      3     97 1.1613e-07 ***\nRoy               1 0.4333957 14.01313      3     97 1.1613e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n              Sum Sq num Df Error SS den Df  F value    Pr(>F)    \n(Intercept) 26318803      1  1054811     99 2470.169 < 2.2e-16 ***\ntimes          65694      3   566979    297   11.471 3.884e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n      Test statistic    p-value\ntimes        0.77081 0.00011477\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n       GG eps Pr(>F[GG])    \ntimes 0.84304  2.433e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n         HF eps   Pr(>F[HF])\ntimes 0.8669702 1.838348e-06\n```\n:::\n:::\n\n \n\nI think I changed everything I needed to. \n\nThere actually *is* still an effect of time. This is true even though sphericity fails and so the Huynh-Feldt adjustment to the P-value is fairly substantial; even so, it is still clearly significant.\n\nWhat kind of effect?\nWe can look at that by finding epinephrine means at each time. That\nwould be easier if we had long format, but I think we can still do\nit. The magic word is `across`:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>%\n  filter(location == 2) %>%\n  summarize(across(starts_with(\"epi\"), \\(epi) mean(epi)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 4\n  epi_1 epi_2 epi_3 epi_4\n  <dbl> <dbl> <dbl> <dbl>\n1  249.  279.  251.  247.\n```\n:::\n:::\n\n(in words, \"for each column that starts with `epi`, find the mean of it.\")\n \n\nThe message here is that the mean epinephrine levels at time 2 were\nhigher than the others, so that's what seems to be driving the time\neffect. And remember, these were the control children, so they had no\nnew airport nearby.\n\nI want to try this:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>% nest_by(location)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise:  location\n  location               data\n     <dbl> <list<tibble[,5]>>\n1        1          [100 x 5]\n2        2          [100 x 5]\n```\n:::\n:::\n\n \n\nThis is like the idea in the Spilich problem: I want to write a\nfunction that calculates the means of the `epi_1` through\n`epi_4` columns of a data frame like `airport`, and\nthen apply it to each of those \"nested\" data frames.\n\n::: {.cell}\n\n```{.r .cell-code}\nepi.means <- function(x) {\n  x %>% summarize(across(starts_with(\"epi\"), \\(epi) mean(epi)))\n}\n\nepi.means(airport)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 4\n  epi_1 epi_2 epi_3 epi_4\n  <dbl> <dbl> <dbl> <dbl>\n1  248.  309.  304.  298.\n```\n:::\n:::\n\n \n\nOK, and then:\n\n::: {.cell}\n\n```{.r .cell-code}\nairport %>%\n  nest_by(location) %>%\n  rowwise() %>% \n  mutate(means = list(epi.means(data))) %>%\n  unnest(means)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 6\n  location               data epi_1 epi_2 epi_3 epi_4\n     <dbl> <list<tibble[,5]>> <dbl> <dbl> <dbl> <dbl>\n1        1          [100 x 5]  247.  340.  356.  349.\n2        2          [100 x 5]  249.  279.  251.  247.\n```\n:::\n:::\n\n \n\nOoh, nice. There are the means for the four time points at both of the\ntwo locations. At location 1, epinephrine starts out low, jumps high\nafter the airport is built, and stays there, while at location 2, the\nmean is mysteriously higher at time 2 and then epinephrine levels go\nback down again.\n\nEnough of that. Back to the mixed model analysis, with more copying\nand pasting. Here is the \"simple time effect\" at location 2:\n\n::: {.cell}\n\n```{.r .cell-code}\nfairport %>%\n  filter(location == 2) %>%\n  lmer(epinephrine ~ when + (1 | fchild), data = .) %>%\n  drop1(test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nepinephrine ~ when + (1 | fchild)\n       npar    AIC    LRT   Pr(Chi)    \n<none>      4336.8                     \nwhen      3 4363.7 32.889 3.399e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nThis also finds a time effect at location 2, with a very similar P-value.\n\nSo which method is preferred? They are, I think, two different ways of\napproaching the same problem, and so there is no good reason to prefer\none way over the other. The profile-analysis way is easier to follow\nif you are coming from a MANOVA direction: set it up the same way as\nyou would a MANOVA, with your repeated measures as your multivariate\nresponse, and distinguish between the within-subject design (times,\nfor us) and the between-subject design (treatments, stuff like\nthat). As for mixed models: when you get your head around that crazy\nway of specifying the random effects, which are typically \"subjects\"\nin this kind of analysis, the actual model statement is brief:\nresponse depends on fixed effects plus random effects with the\nbrackets and vertical bar. I always find the `lmer` models look\nvery simple once you have figured them out (like `ggplot` in\nthat regard). I also like the fact that `lmer` uses \"tidy data\", \nso that you can make graphs and do this flavour of analysis\nwith the same data frame. Having gotten my head around getting my data\ntidy, it seems odd to me that profile analysis requires the data to be\nuntidy, although it does so for a good reason: if you were genuinely\ndoing a MANOVA, you would *want* your multiple response\nvariables each in their own column. Doing repeated measures this way\nis thinking of your responses at different times like different\nvariables. \n\nThe approach I've taken in this course is for historical reasons. That\nis to say, *my* history, not because the historical perspective\nis necessarily the best one.  It began with doing repeated measures in\nSAS (this was many years ago; I don't teach SAS any more), and the only way I could get my\nhead around that was the profile-analysis way. I'm not even sure there\nwas software for doing mixed models in those days, certainly not in\nSAS. Besides, random effects scared me back then. Anyway, when I\nmoved things over to R, I\nrealized I was using basically the same idea in R: make it into a\nMANOVA and tweak things as needed. So I developed things by showing\nyou MANOVA first as ANOVA when you have lots of $y$-variables instead\nof just one, and then using it for repeated measures by thinking of\nthe repeated measures as \"different-but-related\" $y$-variables. To\nmy mind, this is a reasonable way of doing it in a course like this:\nyou get to see MANOVA, which is worth knowing about anyway, and then\nyou get to apply it to a very common situation.\n\nMixed models are actually quite recent, as a way of doing this kind of\nmodelling. Back when I was learning this stuff, you learned to\ndistinguish between \"fixed effects\", whose levels were the only ones\nyou cared about (like the two locations here), and \"random effects\",\nwhere the ones you observed were only a sample of the ones you might\nhave seen (like \"children\" or in general \"subjects\"). Then you\nwould test the fixed effects by hypothesizing that their mean effect\nwas zero and seeing whether you could reject that (in the same way\nthat you do in regression or regular ANOVA: in the latter case, you\ntest that the effects of all the different groups are zero, which\ncomes down to saying that all the groups have the same mean). The way\nyou handled random effects was to estimate their *variance*, and\nthen you would test whether a random effect exists or not by testing\nwhether the variance of that random effect is zero (does not exist) or\nis greater than zero. Zero is the smallest a variance (or SD) can be,\nwhich means that testing for it has always been kind of flakey and the\nstandard theory doesn't work for it (the technical term is \n\"on the boundary of the parameter space\", and the theory applies when the\nnull-hypothesis value is strictly *inside* the set of possible\nvalues the parameter can take). \n\nBack when we did this stuff by\nhand,^[I understand psychology students *still* do this  kind of stuff by hand.] we had to figure out whether we were testing\na fixed or a random effect, and there were rules, involving things\ncalled \"expected mean squares\", that told you how to get the tests\nright. Anyway, that is all considered rather old-fashioned now, and\nmixed models are where it is at. In these, you typically *only*\ntest the fixed effects, while estimating the size of the random\neffects, taking it for granted that they exist. This is an active area\nof research; the things that `lmer` fits are called \n\"linear mixed models\", and there are also now \n\"generalized linear mixed models\", things like logistic regression with random effects.\n\nWe haven't had to worry about this up to now because in most of the\nexperimental designs we have used, each subject only contributes\n*one*  measurement. In that case, you cannot separate out a\nsubject-specific \"random effect\" from random error, and so we lump\nit all into random error. The one experimental design where a subject\ngives us more than one measurement is matched pairs, and the way we\nhave handled that so far is to turn the two measurements into one by\ntaking the difference between them; even in that case, each subject\ncontributes only one *difference* to the analysis. Repeated\nmeasures, though, is genuinely different: you can't wish away multiple\nmeasurements on the same subject by taking differences any more, so\nyou have to face up to the issue at last. Either we think of it as\nseveral different measurements that might be correlated (the\nprofile-analysis MANOVA way), or we face up to the idea that different\nsubjects bring their own random effect to the table that shows up in\nevery measurement that the subject provides (mixed models).\n\nI did analysis of covariance as a separate mini-section, thinking of\nit as a regression, but if the numerical covariate is the same thing\nas the response but measured at a different time (eg., before rather than\nafter), that would also respond to a repeated-measures approach, or to\nthe taking of differences as matched pairs does. There is almost\nalways more than one way to look at these things.\n\n$\\blacksquare$\n\n\n\n\n\n##  Body fat as repeated measures\n\n\n This one is also stolen from STAC32. Athletes are concerned\nwith measuring their body fat percentage. Two different methods are\navailable: one using ultrasound, and the other using X-ray\ntechnology. We are interested in whether there is a difference in the\nmean body fat percentage as measured by these two methods, and if so,\nhow big that difference is. Data on 16 athletes are at\n[link](http://ritsokiguess.site/datafiles/bodyfat.txt).\n\n\n\n(a) Read in the data and check that you have a sensible number of\nrows and columns.\n\n\n\nSolution\n\n\nThis kind of thing:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/bodyfat.txt\"\nbodyfat <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 16 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\ndbl (3): athlete, xray, ultrasound\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nbodyfat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 16 x 3\n   athlete  xray ultrasound\n     <dbl> <dbl>      <dbl>\n 1       1  5          4.75\n 2       2  7          3.75\n 3       3  9.25       9   \n 4       4 12         11.8 \n 5       5 17.2       17   \n 6       6 29.5       27.5 \n 7       7  5.5        6.5 \n 8       8  6          6.75\n 9       9  8          8.75\n10      10  8.5        9.5 \n11      11  9.25       9.5 \n12      12 11         12   \n13      13 12         12.2 \n14      14 14         15.5 \n15      15 17         18   \n16      16 18         18.2 \n```\n:::\n:::\n\n \n\n16 rows (athletes) and 3 columns, one for each measurement\nmethod and one labelling the athletes. All good.\n\n\n\n$\\blacksquare$\n\n(b) Carry out a suitable (matched-pairs) $t$-test to determine\nwhether the means are the same or different. \n\n\n\nSolution\n\n\nFeed the two columns into `t.test` along with\n`paired=T`. Remember that this is (in effect) a one-sample\n$t$-test, so that you can't use a `data=`. You therefore have\nto wrap everything in a `with`:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(bodyfat, t.test(xray, ultrasound, paired = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  xray and ultrasound\nt = -0.30801, df = 15, p-value = 0.7623\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.7425068  0.5550068\nsample estimates:\nmean difference \n       -0.09375 \n```\n:::\n:::\n\n \n\nThe test we want is two-sided, so we didn't have to take any special\nsteps to get that.\n\n$\\blacksquare$\n\n\n\n(c) What do you conclude from the test?\n\n\n\nSolution\n\n\nThe P-value of 0.7623 is not at all small, so there is no way we can\nreject the null hypothesis.^[My hat stays on my head.] There\nis no evidence of a difference in means; we can act as if the two\nmethods produce the same mean body fat percentage. \nThat is to say, on this evidence we can use either method, whichever\none is cheaper or more convenient.\n\n \n$\\blacksquare$\n\n(d) Run a repeated-measures analysis of variance, treating the two\nmethods of measuring body fat as the repeated measures (ie., playing\nthe role of \"time\" that you have seen in most of the other repeated\nmeasures analyses). There is no \"treatment\" here, so there is\nnothing to go on the right side of the squiggle. Insert a `1`\nthere to mean \"just an intercept\". Display the results.\n\n\n\nSolution\n\n\nConstruct the response variable, run `lm`, construct the\nwithin-subjects part of the design, run `Manova`:\n\n::: {.cell}\n\n```{.r .cell-code}\nbodyfat %>%\n  select(xray:ultrasound) %>%\n  as.matrix() -> response\nbodyfat.1 <- lm(response ~ 1)\nmethods <- colnames(response)\nmethods.df <- data.frame(methods=factor(methods))\nbodyfat.2 <- Manova(bodyfat.1, idata = methods.df, idesign = ~methods)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNote: model has only an intercept; equivalent type-III tests substituted.\n```\n:::\n\n```{.r .cell-code}\nsummary(bodyfat.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nType III Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n           (Intercept)\nxray                 1\nultrasound           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)        9025\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.797341 59.01586      1     15 1.4135e-06 ***\nWilks             1  0.202659 59.01586      1     15 1.4135e-06 ***\nHotelling-Lawley  1  3.934390 59.01586      1     15 1.4135e-06 ***\nRoy               1  3.934390 59.01586      1     15 1.4135e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: methods \n\n Response transformation matrix:\n           methods1\nxray             -1\nultrasound        1\n\nSum of squares and products for the hypothesis:\n         methods1\nmethods1 0.140625\n\nMultivariate Tests: methods\n                 Df test stat   approx F num Df den Df  Pr(>F)\nPillai            1 0.0062849 0.09486999      1     15 0.76231\nWilks             1 0.9937151 0.09486999      1     15 0.76231\nHotelling-Lawley  1 0.0063247 0.09486999      1     15 0.76231\nRoy               1 0.0063247 0.09486999      1     15 0.76231\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n            Sum Sq num Df Error SS den Df F value    Pr(>F)    \n(Intercept) 4512.5      1  1146.94     15 59.0159 1.413e-06 ***\nmethods        0.1      1    11.12     15  0.0949    0.7623    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nThis is an unusual one in that the repeated measures variable is not time but method, and there is no actual\ntreatment. The consequence (with there being only two methods) is that sphericity is automatically good, and the univariate test for methods (at the bottom) has exactly the same P-value as all the multivariate tests, so that the conclusion is the same either way. (Also, see below about the matched pairs.)\n\n$\\blacksquare$\n\n\n(e) Compare your repeated-measures analysis to your matched-pairs\none. Do you draw the same conclusions?\n\n\n\nSolution\n\n\nThe P-value for `methods`, which is testing the same thing\nas the matched pairs, is 0.7623, which is actually\n*identical* to the matched pairs $t$-test, and so the\nconclusion is identical also. That is, there is no difference\nbetween the two methods for measuring body fat. \nThis goes to show that repeated measures gives the same answer as\na matched-pairs $t$-test in the situation where they both\napply. But repeated measures is, as we have seen, a lot more general.\n\nSince this really is repeated measures, we ought to be able to use\na mixed model here too. We need \"long\" or \"tidy\" format, which\nwe don't have yet. One pipe to save them all, to paraphrase Lord\nof the Rings^[The movies of Lord of the Rings were filmed      in New Zealand, which is also the country in which R was first      designed.] --- put all the fat measurements in one column with a\nlabel saying which `method` they were obtained with; create\na column which is the athlete number as a factor; fit the linear\nmixed model; see what we can drop from it:\n\n::: {.cell}\n\n```{.r .cell-code}\nbodyfat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 16 x 3\n   athlete  xray ultrasound\n     <dbl> <dbl>      <dbl>\n 1       1  5          4.75\n 2       2  7          3.75\n 3       3  9.25       9   \n 4       4 12         11.8 \n 5       5 17.2       17   \n 6       6 29.5       27.5 \n 7       7  5.5        6.5 \n 8       8  6          6.75\n 9       9  8          8.75\n10      10  8.5        9.5 \n11      11  9.25       9.5 \n12      12 11         12   \n13      13 12         12.2 \n14      14 14         15.5 \n15      15 17         18   \n16      16 18         18.2 \n```\n:::\n\n```{.r .cell-code}\nbodyfat %>%\n  pivot_longer(-athlete, names_to=\"method\", values_to=\"fat\") %>%\n  mutate(fathlete = factor(athlete)) %>%\n  lmer(fat ~ method + (1 | fathlete), data = .) %>%\n  drop1(test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nfat ~ method + (1 | fathlete)\n       npar    AIC     LRT Pr(Chi)\n<none>      161.34                \nmethod    1 159.44 0.10088  0.7508\n```\n:::\n:::\n\n     \n\nOnce again, there is no difference between methods, and though the\nP-value is different from the matched pairs or profile analysis, it is\nvery close to those.\n\nIf you're not clear about the tidy data frame used for input to\n`lmer`, pull the top two lines off the pipeline and see what they produce:\n\n::: {.cell}\n\n```{.r .cell-code}\nbodyfat %>%\n  pivot_longer(-athlete, names_to=\"method\", values_to=\"fat\") %>%\n  mutate(fathlete = factor(athlete))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 4\n   athlete method       fat fathlete\n     <dbl> <chr>      <dbl> <fct>   \n 1       1 xray        5    1       \n 2       1 ultrasound  4.75 1       \n 3       2 xray        7    2       \n 4       2 ultrasound  3.75 2       \n 5       3 xray        9.25 3       \n 6       3 ultrasound  9    3       \n 7       4 xray       12    4       \n 8       4 ultrasound 11.8  4       \n 9       5 xray       17.2  5       \n10       5 ultrasound 17    5       \n# i 22 more rows\n```\n:::\n:::\n\n \n\nEach athlete now appears twice: once with their `fat` measured\nby `xray`, and again with it measured by\n`ultrasound`. The column `fathlete` is a factor.\n\nThe mixed model took me two goes to get right: I forgot that I needed the\n`data=.` in `lmer`, because it works like `lm`\nwith the model formula first, not the input data. If the pipeline is\ngoing too fast for you, create the tidy data frame and save it, then\nuse the saved data frame as input to `lmer`.\n\n$\\blacksquare$\n\n\n\n\n\n\n\n##  Investigating motor activity in rats\n\n\nA researcher named King was investigating\nthe effect of the drug midazolam on motor activity in rats. Typically,\nthe first time the drug is injected, a rat's motor activity decreases\nsubstantially, but rats typically develop a \"tolerance\", so that\nfurther injections of the drug have less impact on the rat's motor\nactivity.\n\nThe data shown in\n[link](http://ritsokiguess.site/datafiles/king.csv) were all taken\nin one day, called the \"experiment day\" below. 24 different rats\nwere used. Each rat, on the experiment day, was injected with a fixed\namount of midazolam, and at each of six five-minute intervals after\nbeing injected, the rat's motor activity was measured (these are\nlabelled `i1` through `i6` in the data). The rats\ndiffered in how they had been treated before the experiment day. The\ncontrol group of rats had previously been injected repeatedly with a\nsaline solution (no active ingredient), so the experiment day was the\nfirst time this group of rats had received midazolam. The other two\ngroups of rats had both received midazolam repeatedly before the\nexperiment day: the \"same\" group was injected on experiment day in\nthe same environment that the previous injections had taken place (this\nis known in psychology as a \"conditioned tolerance\"), but the\n\"different\" group had the previous injections in a different\nenvironment than on experiment day.\n\nThe column `id` identifies the rat from which each sequence of\nvalues was obtained.\n\n\n\n(a) Explain briefly why we need to use a repeated measures\nanalysis for these data.\n\n\nSolution\n\n\nEach rat is measured at six different times (`i1` through\n`i6`): that is to say, each row of the data set consists of\nrepeated measurements on the *same* rat. (If each row had\nused six *different* rats to obtain the six measurements, we\nwould have been back in familiar territory and could have used a\nregular analysis of variance.)\n    \n$\\blacksquare$\n\n(b) Read in the data and note that you have what was promised\nin the question.\n\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/king.csv\"\nking <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 24 Columns: 8\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): context\ndbl (7): id, i1, i2, i3, i4, i5, i6\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nking\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 x 8\n      id context    i1    i2    i3    i4    i5    i6\n   <dbl> <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1   101 control   150    44    71    59   132    74\n 2   102 control   335   270   156   160   118   230\n 3   103 control   149    52    91   115    43   154\n 4   104 control   159    31   127   212    71   224\n 5   105 control   292   125   184   246   225   170\n 6   106 control   297   187    66    96   209    74\n 7   107 control   170    37    42    66   114    81\n 8   108 control   159     0    35    75    71    34\n 9   109 same      346   175   177   192   239   140\n10   110 same      426   329   236    76   102   232\n# i 14 more rows\n```\n:::\n:::\n\n     \n\nThere are 24 rats (rows). The columns label the rat (`id`) and\nthe times at which motor activity was measured (`i1` through\n`i6`). The remaining column, `context`, describes how\nthe rats were treated before experiment day, with the levels being the\nsame ones given in the question.\n    \n$\\blacksquare$\n\n(c) We are going to do a repeated-measures analysis using the\n\"profile\" method shown in class. Create a suitable response\nvariable for this method.\n\n\nSolution\n\n\n`cbind` the appropriate columns together, to make a matrix:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(king, cbind(i1, i2, i3, i4, i5, i6))\n```\n:::\n\n     \n\nThis is the \"simple\" but tedious way, and produces a matrix because\nthe `i1` through `i6` are vectors (single columns):\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n \n\n`i1:i6` does not work here, because we are outside of the\n`tidyverse`, and in that world, `:` only means\n\"through\" (as in \"this through that\") when the things on either\nside of it are or represent numbers.\n\nThe clever way to get the response is to `select` the columns\nand then turn them into a matrix. This *does* permit the colon\nbecause we are now in the `tidyverse`:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- king %>%\n  select(i1:i6) %>%\n  as.matrix()\nclass(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n \nIt is indeed a matrix.\n\nI tried to be extra-clever and use `starts_with`, but I have\nanother column `id` that starts with `i` that I do\n*not* want to be part of the response. So I had to abandon that\nidea, but not before trying this:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- king %>%\n  select(matches(\"i[0-9]\")) %>%\n  as.matrix()\nhead(response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      i1  i2  i3  i4  i5  i6\n[1,] 150  44  71  59 132  74\n[2,] 335 270 156 160 118 230\n[3,] 149  52  91 115  43 154\n[4,] 159  31 127 212  71 224\n[5,] 292 125 184 246 225 170\n[6,] 297 187  66  96 209  74\n```\n:::\n:::\n\n \n\n`head` displays the first six lines (of anything). We don't\nnormally need it because we are typically dealing with\n`tibble`-like data frames that display only ten rows of\nthemselves by default. But this worked. The `matches` part\ntakes a so-called \"regular expression\" which is a very flexible way\nof matching anything: in this case, a column whose name starts with\n`i` followed by exactly one digit (something between 0 and 9\ninclusive). \n    \n$\\blacksquare$\n\n(d) Set up the \"within-subjects\" part of the analysis. That\nmeans getting hold of the names of the columns that hold the\ndifferent times, saving them, and also making a data frame out of them:\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\n```\n:::\n\n   \n    \n$\\blacksquare$\n\n(e) Fit the repeated-measures ANOVA. This will involve fitting\nan `lm` first, if you have not already done so.\n\n\nSolution\n\n\nFit the `lm` first, and then pass that into `Manova`\nfrom `car`:\n\n::: {.cell}\n\n```{.r .cell-code}\nking.1 <- lm(response ~ context, data = king)\nking.2 <- Manova(king.1, idata = times.df, idesign = ~times)\nsummary(king.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\ni1           1\ni2           1\ni3           1\ni4           1\ni5           1\ni6           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    24399650\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df    Pr(>F)    \nPillai            1  0.913261 221.1069      1     21 1.273e-12 ***\nWilks             1  0.086739 221.1069      1     21 1.273e-12 ***\nHotelling-Lawley  1 10.528902 221.1069      1     21 1.273e-12 ***\nRoy               1 10.528902 221.1069      1     21 1.273e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: context \n\n Response transformation matrix:\n   (Intercept)\ni1           1\ni2           1\ni3           1\ni4           1\ni5           1\ni6           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)     1611524\n\nMultivariate Tests: context\n                 Df test stat approx F num Df den Df    Pr(>F)   \nPillai            2 0.4101695 7.301725      2     21 0.0039141 **\nWilks             2 0.5898305 7.301725      2     21 0.0039141 **\nHotelling-Lawley  2 0.6954024 7.301725      2     21 0.0039141 **\nRoy               2 0.6954024 7.301725      2     21 0.0039141 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2 times3 times4 times5\ni1      1      0      0      0      0\ni2      0      1      0      0      0\ni3      0      0      1      0      0\ni4      0      0      0      1      0\ni5      0      0      0      0      1\ni6     -1     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n            times1      times2       times3      times4      times5\ntimes1 463982.0417 18075.41667 -17241.16667 -47691.2917  973.291667\ntimes2  18075.4167   704.16667   -671.66667  -1857.9167   37.916667\ntimes3 -17241.1667  -671.66667    640.66667   1772.1667  -36.166667\ntimes4 -47691.2917 -1857.91667   1772.16667   4902.0417 -100.041667\ntimes5    973.2917    37.91667    -36.16667   -100.0417    2.041667\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.856997 20.37569      5     17 1.2326e-06 ***\nWilks             1  0.143003 20.37569      5     17 1.2326e-06 ***\nHotelling-Lawley  1  5.992849 20.37569      5     17 1.2326e-06 ***\nRoy               1  5.992849 20.37569      5     17 1.2326e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: context:times \n\n Response transformation matrix:\n   times1 times2 times3 times4 times5\ni1      1      0      0      0      0\ni2      0      1      0      0      0\ni3      0      0      1      0      0\ni4      0      0      0      1      0\ni5      0      0      0      0      1\ni6     -1     -1     -1     -1     -1\n\nSum of squares and products for the hypothesis:\n          times1    times2    times3    times4    times5\ntimes1 40376.583 44326.333 28404.667 -6315.458  9666.458\ntimes2 44326.333 93153.083 56412.417  3080.292 24377.208\ntimes3 28404.667 56412.417 34289.083  1235.458 14606.042\ntimes4 -6315.458  3080.292  1235.458  3241.583  1586.167\ntimes5  9666.458 24377.208 14606.042  1586.167  6573.083\n\nMultivariate Tests: context:times\n                 Df test stat approx F num Df den Df    Pr(>F)   \nPillai            2 0.8033948 2.417022     10     36 0.0256284 * \nWilks             2 0.3347058 2.476886     10     34 0.0237616 * \nHotelling-Lawley  2 1.5750953 2.520152     10     32 0.0230293 * \nRoy               2 1.2432100 4.475556      5     18 0.0079604 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n               Sum Sq num Df Error SS den Df  F value    Pr(>F)    \n(Intercept)   4066608      1   386233     21 221.1069 1.273e-12 ***\ncontext        268587      2   386233     21   7.3017 0.0039141 ** \ntimes          407439      5   273592    105  31.2736 < 2.2e-16 ***\ncontext:times   88901     10   273592    105   3.4119 0.0006746 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n              Test statistic  p-value\ntimes                0.24793 0.022324\ncontext:times        0.24793 0.022324\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n               GG eps Pr(>F[GG])    \ntimes         0.70016  3.152e-14 ***\ncontext:times 0.70016   0.003269 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                 HF eps   Pr(>F[HF])\ntimes         0.8573896 6.269000e-17\ncontext:times 0.8573896 1.422381e-03\n```\n:::\n:::\n\n     \n    \n$\\blacksquare$\n\n(f) What do you conclude from your repeated-measures ANOVA?\nExplain briefly, in the context of the data.\n\n\nSolution\n\n\nThe interaction term is significant, with a P-value less than\n0.05. This is where we start and stop looking.\n\nTo be precise, the sphericity test fails (P-value 0.022), so that the P-value in the univariate test for interaction is too small, and we should trust the Huynh-Feldt one of 0.0014 below it. This is still significant, but not as significant as you might have thought. For comparison, the multivariate test P-values vary between 0.008 and 0.026, a bit bigger but still significant.\n\n\nThis means that the effect of time on motor activity (that is, the\nway the motor activity depends on time) is different for each\n`context`. That's all we can say now.\nGrading note: as long as the setup and MANOVA are done somewhere,\nI don't mind which part they are labelled with. But you need to do\nthe setup, initial `lm` and `Manova`\n*somewhere* so that everything comes out right in the end.\n    \n$\\blacksquare$\n\n(g) To understand the results of the previous part, we are going\nto make a spaghetti plot. In preparation for that, we need to save\nthe data in \"long format\" with one observation on *one* time\npoint in each row. Arrange that, and show by displaying (some of)\nthe data that you have done so.\n\n\nSolution\n\n\nThis is `pivot_longer` yet again: gather up columns `i1`\nthrough `i6` and call them something like `activity`:\n\n::: {.cell}\n\n```{.r .cell-code}\nking %>% \n  pivot_longer(i1:i6, names_to=\"time\", values_to=\"activity\") -> king.long\nking.long\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 144 x 4\n      id context time  activity\n   <dbl> <chr>   <chr>    <dbl>\n 1   101 control i1         150\n 2   101 control i2          44\n 3   101 control i3          71\n 4   101 control i4          59\n 5   101 control i5         132\n 6   101 control i6          74\n 7   102 control i1         335\n 8   102 control i2         270\n 9   102 control i3         156\n10   102 control i4         160\n# i 134 more rows\n```\n:::\n:::\n\n     \nDisplaying the top 10 rows of the resulting data frame is a good way to display \"some of\" it. \nYou can always look at more if you like. There are more rows\nand fewer columns than there were before, which is\nencouraging. `pivot_longer` collects up all the time and activity values for the first rat, then the second, and so on. \n\n    \n$\\blacksquare$\n\n(h) Make a spaghetti plot: that is, plot motor activity against\nthe time points, joining the points for each *rat* by lines,\nand colouring the points and lines according to the *context*.\n\n\nSolution\n\n\nThat means this, using `group` to indicate which points to\njoin by lines, since it's different from the `colour`: \n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(king.long, aes(x = time, y = activity, colour = context, group = id)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/king-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nI'd say the `geom_point` is optional, so that this is also\ngood, perhaps better even:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(king.long, aes(x = time, y = activity, colour = context, group = id)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/king-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n    \n$\\blacksquare$\n\n(i) Looking at your spaghetti plot, why do you think your\nrepeated-measures ANOVA came out as it did? Explain briefly.\n\n\n\nSolution\n\n\nWhat you're after is an explanation of how the *patterns* over\ntime are different for the three `context`s. If you can find\nsomething that says that, I'm good. For example, even though all of\nthe rats experienced a decrease in motor activity between times 1\nand 2, the rats in the `same` group didn't decrease as\nmuch. Or, the rats in the `same` group continued to decrease\nbeyond time 2, whereas the rats in the `control` and\n`different` groups tended to level off after time 2, not\ndecreasing so much after that.\nIf you like, you can draw an interaction plot by working out the\nmeans for each `context`-`time` group first:\n\n::: {.cell}\n\n```{.r .cell-code}\nking.long %>%\n  group_by(context, time) %>%\n  summarize(m = mean(activity)) %>%\n  ggplot(aes(x = time, y = m, colour = context, group = context)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'context'. You can override using the\n`.groups` argument.\n```\n:::\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/king-11-1.pdf){fig-pos='H'}\n:::\n:::\n\n   \n\nThis seems to illustrate the same things as I found on the spaghetti\nplot. It gains in clarity by only looking at means, but loses by not\nconsidering the variability. Your call.\n\nThis kind of thing also runs with `lmer` from package\n`lme4`. It uses the long data frame, thus, treating `id`\n(identifying the rats) as a random effect:\n\n::: {.cell}\n\n```{.r .cell-code}\nking.3 <- lmer(activity ~ context * time + (1 | id), data = king.long)\n```\n:::\n\n \n\nWhat can we drop? The only thing under consideration is the interaction:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(king.3, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nactivity ~ context * time + (1 | id)\n             npar    AIC    LRT   Pr(Chi)    \n<none>            1609.0                     \ncontext:time   10 1622.7 33.764 0.0002025 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nand we get the same conclusion as before, but with a much smaller P-value.\n\nWith this kind of modelling, there is no distinction between\n\"within\" and \"between\", so that even though `time` is a\nwithin-subjects factor and `context` is between subjects, if\nthe interaction had not been significant, we could have dropped it\nfrom the model, and then we would have had an effect of `time`\nand an effect of `context`, independent of each other. I was\nactually looking for an example with a non-significant interaction,\nbut I couldn't find one. \n  \n$\\blacksquare$\n\n\n\n\n\n\n##  Repeated measures with no background\n\n\n Nine people are randomly chosen to receive one of three\ntreatments, labelled A, B and C. Each person has their response\n`y` to the treatment measured at three times, labelled T1, T2\nand T3. The main aim of the study is to properly assess the effects of\nthe treatments. A higher value of `y` is better.\n\nThe data are in [link](http://ritsokiguess.site/datafiles/rm.txt).\n\n\n\n(a) There are $9 \\times 3=27$ observations  of `y` in\nthis study. Why would it be wrong to treat these as 27 independent\nobservations? Explain briefly.\n\nSolution\n\n\nThere are only 9 people with 3 observations on each person. The\nthree observations on the same person are likely to be correlated\nwith each other, and so treating them as independent would be a\nmistake.\nThis is repeated-measures data. If you say that, that's useful,\nbut you also need to demonstrate that you know what repeated\nmeasures *means* and why it needs to be handled differently\nfrom one-observation-per-individual data. Another way to look at\nit is that individuals will differ from each other, and so there\nought to be an \"individual\" effect included in the model, in the\nsame way that you would include a block effect in a randomized\nblock design: not because you care about differences among\nindividuals, but because you are pretty sure they'll be there and\nyou want to account for them.\n\n$\\blacksquare$\n\n(b) Read in the data values. Are they tidy or untidy?  Explain\nbriefly. (The data values are separated by *tabs*, like the\nAustralian athlete data.)\n\nSolution\n\n\nWe therefore need `read_tsv`. I'm not quite sure what to\ncall this one:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/rm.txt\"\ntreatments <- read_tsv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 27 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr (2): trt, time\ndbl (2): subject, y\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ntreatments\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 27 x 4\n   trt   time  subject     y\n   <chr> <chr>   <dbl> <dbl>\n 1 A     T1          1    10\n 2 A     T1          2    12\n 3 A     T1          3    13\n 4 A     T2          1    16\n 5 A     T2          2    19\n 6 A     T2          3    20\n 7 A     T3          1    25\n 8 A     T3          2    27\n 9 A     T3          3    28\n10 B     T1          4    12\n# i 17 more rows\n```\n:::\n:::\n\n     \nFind a way to display what you have, so you can decide whether it is\ntidy or not.\nEach observation of `y` is in a row by itself, so this is\ntidy, or long format. You might even call this extra-tidy, because\neach person is spread over three rows, one for each time point.\nLooking ahead, this is ideal for making a graph, or for doing the\nadvanced version of the analysis with `lme4`, but it is not\nso good for our MANOVA way of doing a repeated measures\nanalysis. That we will have to prepare for.\n\n$\\blacksquare$\n\n(c) Make a spaghetti plot: that is, a plot of `y`\nagainst time, with the observations for the same individual joined\nby lines which are coloured according to the treatment that\nindividual received.\n\nSolution\n\n\nThe individuals are labelled in `subject` and the\ntreatments are in `trt`, which means we need to do this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(treatments, aes(x = time, y = y, colour = trt, group = subject)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](repeated-measures_files/figure-pdf/bock-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nI'm going to be all smug and tell you that I got this right first\ntime. (I'm telling you this because it doesn't happen often.) If it didn't come out that way for you, no stress: figure out what went wrong, fix it, and no one is any the wiser. On a spaghetti plot, you want to join together the points that belong to the same *subject*, and you want to colour them by *treatment*, so `colour` needs to be `trt` and `group` needs to be `subject`, since otherwise *all* the observations on the same treatment will get connected by lines, which is not what you want. You can use `group` in other contexts too, but for us we would normally distinguish groups by colour, and here we have two grouping variables (subject and treatment), which we want to distinguish in different ways.\n\n$\\blacksquare$\n\n(d) On your spaghetti plot, how do the values of `y` for\nthe treatments compare over time?\n\nSolution\n\n\nThe most obvious thing is that the values of `y` *all*\ngo up over time, regardless of which treatment they were from.\nAt the initial time T1, the treatments are all about the same, but\nat the second and third time points, `y` is bigger for\ntreatment C than for the other two treatments (which are about the\nsame as each other). If you like, say that the gap between\ntreatment C and the others is increasing over time, or that the\nlines for treatment C are steeper than for the other\ntreatments. Any of those ways of saying it comes to the same\nconclusion. \nExtra: if you look at the lines of the same colour (treatment),\nthey don't seem to cross over very much. That suggests that an\nindividual who starts with a larger value of `y`\n(relatively, compared to the other individuals on the same\ntreatment) tends to stay larger than the other individuals on the\nsame treatment all the way through. This would be another thing\nyou'd see if the measurements for the individuals are correlated,\nor if there is an \"individual effect\" to go along with a\ntreatment effect (and a time effect).\nIf you think of this like an individual-level version of an\ninteraction plot (which would be obtained by plotting the\n*means* for each treatment at each time), there is a\nsuggestion here of an interaction between treatment and time, as\nwell as a treatment effect (the latter because treatment C appears\nbetter than the rest). \n\n$\\blacksquare$\n\n(e) Explain briefly how the data are in the wrong format for a\nrepeated-measures ANOVA (done using MANOVA, as in class), and use\n`pivot_wider` to get the data set into the right format.\n\nSolution\n\n\nFor MANOVA, we want the three responses (here, the values of\n`y` at the three different times) in three separate\ncolumns, with *all* the measurements for one subject in one\nrow (rather than on three separate rows, as here).\n`pivot_wider` is the flip-side of `pivot_longer`: instead of\nmaking different columns that all measure the same thing into one\ncolumn, we split one column that contains things that are (slightly)\ndifferent from each other (here, `y` at different\ntimes). It needs two inputs: `names_from`, the current single column that\ncontains the column names you are going to make, and `values_from`, the values to\ncarry along with them:\n\n::: {.cell}\n\n```{.r .cell-code}\ntreatments %>% pivot_wider(names_from = time, values_from = y) -> tr2\ntr2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 5\n  trt   subject    T1    T2    T3\n  <chr>   <dbl> <dbl> <dbl> <dbl>\n1 A           1    10    16    25\n2 A           2    12    19    27\n3 A           3    13    20    28\n4 B           4    12    18    25\n5 B           5    11    20    26\n6 B           6    10    22    27\n7 C           7    10    22    31\n8 C           8    12    23    34\n9 C           9    13    22    33\n```\n:::\n:::\n\n  \n\n(I got this right the first time too. I must be having a good day!)\n\nTechnical detail: in `pivot_longer`, the column names are in quotes because they don't exist yet, but in `pivot_wider`, they are *not* in quotes because they are columns that already exist.\n\nNote that the `time` and `y` columns have\n*disappeared*: the columns labelled with the time points are\nwhere those values of `y` have gone. The nine subjects make up\nthe nine rows of the new \"wide\" data set, which is in the format we\nwant.\n\n$\\blacksquare$\n\n(f) Run a repeated-measures ANOVA the `Manova` way. What do you\nconclude from it?\n\nSolution\n\n\nCreate the response variable first, and use it in an `lm`:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(tr2, cbind(T1, T2, T3))\ntreatment.1 <- lm(response ~ trt, data = tr2)\n```\n:::\n\n     \n\nNow we have to construct the within-subject stuff, for which we need\nto get the different times we have. You can type them in again (fine\nhere), or get them from the `response` you just made:\n\n::: {.cell}\n\n```{.r .cell-code}\ntimes <- colnames(response)\ntimes.df <- data.frame(times=factor(times))\n```\n:::\n\n \n\nThis is where the possible time effect is accounted for. Because time\nis within-subjects (each subject is measured at several different\ntimes) but treatment is between subjects (each subject only gets one\ntreatment), the two things have to be treated separately, in this\napproach at least. \n\nThen, uppercase-M `Manova`:\n\n::: {.cell}\n\n```{.r .cell-code}\ntreatment.2 <- Manova(treatment.1, idata = times.df, idesign = ~times)\nsummary(treatment.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nType II Repeated Measures MANOVA Tests:\n\n------------------------------------------\n \nTerm: (Intercept) \n\n Response transformation matrix:\n   (Intercept)\nT1           1\nT2           1\nT3           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    32520.11\n\nMultivariate Tests: (Intercept)\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1    0.9975 2399.025      1      6 4.8568e-09 ***\nWilks             1    0.0025 2399.025      1      6 4.8568e-09 ***\nHotelling-Lawley  1  399.8374 2399.025      1      6 4.8568e-09 ***\nRoy               1  399.8374 2399.025      1      6 4.8568e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: trt \n\n Response transformation matrix:\n   (Intercept)\nT1           1\nT2           1\nT3           1\n\nSum of squares and products for the hypothesis:\n            (Intercept)\n(Intercept)    193.5556\n\nMultivariate Tests: trt\n                 Df test stat approx F num Df den Df   Pr(>F)  \nPillai            2 0.7041229 7.139344      2      6 0.025902 *\nWilks             2 0.2958771 7.139344      2      6 0.025902 *\nHotelling-Lawley  2 2.3797814 7.139344      2      6 0.025902 *\nRoy               2 2.3797814 7.139344      2      6 0.025902 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: times \n\n Response transformation matrix:\n   times1 times2\nT1      1      0\nT2      0      1\nT3     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1    times2\ntimes1   2601 1258.0000\ntimes2   1258  608.4444\n\nMultivariate Tests: times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1    0.9988 2010.298      2      5 5.4369e-08 ***\nWilks             1    0.0012 2010.298      2      5 5.4369e-08 ***\nHotelling-Lawley  1  804.1190 2010.298      2      5 5.4369e-08 ***\nRoy               1  804.1190 2010.298      2      5 5.4369e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------\n \nTerm: trt:times \n\n Response transformation matrix:\n   times1 times2\nT1      1      0\nT2      0      1\nT3     -1     -1\n\nSum of squares and products for the hypothesis:\n       times1   times2\ntimes1     72 38.00000\ntimes2     38 28.22222\n\nMultivariate Tests: trt:times\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            2  1.345125  6.16206      4     12 0.00620585 ** \nWilks             2  0.023398 13.84359      4     10 0.00043842 ***\nHotelling-Lawley  2 25.988095 25.98810      4      8 0.00012292 ***\nRoy               2 25.367215 76.10165      2      6 5.4552e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnivariate Type II Repeated-Measures ANOVA Assuming Sphericity\n\n             Sum Sq num Df Error SS den Df   F value    Pr(>F)    \n(Intercept) 10840.0      1   27.111      6 2399.0246 4.857e-09 ***\ntrt            64.5      2   27.111      6    7.1393 0.0259021 *  \ntimes        1301.0      2   12.889     12  605.6207 8.913e-13 ***\ntrt:times      41.5      4   12.889     12    9.6552 0.0009899 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n          Test statistic  p-value\ntimes            0.29964 0.049149\ntrt:times        0.29964 0.049149\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n           GG eps Pr(>F[GG])    \ntimes     0.58811  3.114e-08 ***\ntrt:times 0.58811   0.008332 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n             HF eps   Pr(>F[HF])\ntimes     0.6461293 7.092455e-09\ntrt:times 0.6461293 6.137921e-03\n```\n:::\n:::\n\n(Since I call things by the same names every time, my code for one of\nthese looks a lot like my code for any of the others.)\n\nStart near the bottom with the Mauchly tests for sphericity. These are *just* significant, so that instead of looking at the univariate tests, look at the bottom for the Huynh-Feldt adjusted P-values, the one for the interaction being 0.0061. This is a bit bigger than the one for the interaction in the univariate tests (0.0010), the latter being deceptively small because sphericity is actually no good.\n\nCompare this, if you like, with the multivariate tests for the interaction. In this case, these are actually not all the same; the largest P-value is the Pillai one, which at 0.0062 is very close to the properly adjusted univariate test.\n\nFinally, interpretation. We look *only* at the interaction. This\nis significant, so the effect of treatment is different at the\ndifferent times. And we **stop there**.\n\n\n$\\blacksquare$\n\n(g) How is your conclusion from the previous part consistent\nwith your spaghetti plot? Explain briefly.\n\nSolution\n\n\nThe thing that the interaction means is that the effect of\ntreatment is different over different times. That shows up in the\nspaghetti plot by treatment C being the same as the others at the\nbeginning, but clearly better than the others at the later\ntimes. That is to say, you can't talk about \"an\" effect of\ntreatment, because whether or not treatment C is better than the\nothers depends on which time you're looking at.\n\nExtra: we used the MANOVA way of doing the repeated-measures\nanalysis. There is another way, \"mixed models\", which is in some\nways less familiar and in some ways more. \n\nIn any analysis of variance, there are two kinds of effects of\nthings you may care about: fixed effects and random effects. Fixed\neffects are things like the treatment and time here, where the\nones you look at are the only ones you are interested in (in this\nstudy at least). If you had wanted to assess another treatment,\nyou would have designed it into the study; if you cared about\nother times, you would have measured `y` at those times\ntoo. The subjects, though, are different: they are a random sample\nof all possible people, and you want your results to generalize to\nthe population of all people of whom your subjects are a\nsample.^[In practice, things are usually fuzzier than this,      because the subjects in your study are typically the ones you      could get, rather than being a physical random sample of all      possible people, but we usually act as if our subjects are a      random sample of all possible subjects.]\nSo subjects are a different kind of thing and they have what are\ncalled random effects. When each subject only gives one\nmeasurement, as in all the things we've seen so\nfar,^[Including matched pairs, because what we do there is      to take the difference between the two measurements for each      person and throw away the actual measurements themselves, so      that each subject still only gives us one measurement.] it\ndoesn't matter how you treat (statistically) the subjects, but\nwhen each subject gives *more* than one measurement, it does\nmatter. Which is why we have to do the `idesign` stuff in\nthe MANOVA, or what you will see below.\n\nA model with both fixed and random effects is called a mixed model.\n\nWe're going to make the assumption that the effect of being one\nsubject rather than another is to move the value of `y` up\nor down by a fixed amount regardless of treatment or time, on\naverage (each subject is different, but *within* a subject\nthe random effect is the same size). That seems reasonable, given\nthe spaghetti plot, where some subjects seemed to give\nconsistently larger or smaller values of `y` than\nothers. This is a so-called \"random-intercepts\" model. In the\npackage `lme4`, there is a function `lmer` that\nlooks like `lm`, except for the way in which you specify\nthe random effects. It looks like this, noting that \n*it works with the tidy data frame* that we read in from the file and made\nthe spaghetti plot out of:\n\n::: {.cell}\n\n```{.r .cell-code}\ntreatment.4 <- lmer(y ~ trt * time + (1 | subject), data = treatments)\ndrop1(treatment.4, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\ny ~ trt * time + (1 | subject)\n         npar    AIC   LRT   Pr(Chi)    \n<none>        102.53                    \ntrt:time    4 120.44 25.91 3.299e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n     \n\nThe way to read that model is \"`y` depends on the combination of treatment and time and also on a random intercept for each subject\". \nThis is the way in which the model captures the idea that\neach subject is different. \n\nYou don't get a test for the random effects; you are *assuming* that the\nsubjects will be different from each other and you want to *adjust* for\nthat, rather than testing for it.^[This is rather like the test for blocks in a randomized  block design: you want to *allow* for differences among blocks,  but you don't especially care to test that there *are* any. In  fact, blocks are a lot like subjects, in that they are typically  things like different experimental plots in which plants are grown,  or different days on which the experiment is conducted, and you want to generalize from the blocks you observed, which are certainly *not* all possible blocks, to the population of all possible blocks.]\nAll you get is tests for the fixed effects that are currently up for\ngrabs, in this case the interaction, which is strongly significant.\n\n$\\blacksquare$\n\n\n",
>>>>>>> 038bb2509ac8e38facb2622be6ad3052b44aca34
    "supporting": [
      "repeated-measures_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}