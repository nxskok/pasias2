{
  "hash": "aff1129adce3115505bdb70da336058e",
  "result": {
    "engine": "knitr",
    "markdown": "# One-sample inference\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Hunter-gatherers in Australia\n\n\n A hunter-gatherer society is one where people get their food\nby hunting, fishing or foraging rather than by agriculture or by\nraising animals. Such societies tend to move from place to place.\nAnthropologists have studied hunter-gatherer societies in forest\necosystems across the world. The average population density of these\nsocieties is 7.38 people per 100 km$^2$. Hunter-gatherer societies on\ndifferent continents might have different population densities,\npossibly because of large-scale ecological constraints (such as\nresource availability), or because of other factors, possibly social\nand/or historic, determining population density.\n\nSome hunter-gatherer societies in Australia were studied, and the\npopulation density per 100 km$^2$ recorded for each. The data are in\n[http://ritsokiguess.site/datafiles/hg.txt](http://ritsokiguess.site/datafiles/hg.txt). \n\n\n\n(a) Read the data into R. Do you have the correct variables?\nHow many hunter-gatherer societies in Australia were studied?\nExplain briefly.\n\n\n\n(b) The question of interest is whether these Australian\nhunter-gatherer societies are like the rest of the world in terms of mean\npopulation density. State suitable null and alternative\nhypotheses. *Define any symbols you use*: that is, if you use a\nsymbol, you also have to say what it means. \n\n\n\n(c) Test your hypotheses using a suitable test. What do you\nconclude, in the context of the data?\n\n\n\n(d) Do you have any doubts about the validity of your test?\nExplain briefly, using a suitable graph to support your\nexplanation. \n\n\n\n\n\n\n\n\n\n##  Buses to Boulder\n\n\n A bus line operates a route from Denver to Boulder (these\nplaces are in Colorado). The\nschedule says that the journey time should be 60 minutes. 11\nrandomly chosen journey times were recorded, and these are in the\nfile [link](http://ritsokiguess.site/datafiles/buses.txt), with\njourney times shown in minutes.\n\n\n(a) Read the data into R, and display the data frame that you\nread in.\n \n\n(b) Run a suitable test to see whether there is evidence that\nthe mean journey time differs from 60 minutes. What do you\nconclude? (I want a conclusion that says something about journey\ntimes of buses.)\n \n\n(c) Give a 95\\% confidence interval for the mean journey\ntime. (No R code is needed here.)\n \n\n(d) Do you draw consistent conclusions from your test and\nconfidence interval? Explain briefly.\n \n\n(e) Draw a boxplot of the journey times. Do you see a reason\nto doubt the test that you did above?\n \n\n\n\n\n\n##  Length of gestation in North Carolina\n\n\n The data in file\n[link](http://ritsokiguess.site/datafiles/ncbirths2.csv) are about\n500 randomly chosen births of babies in North Carolina. There is a lot\nof information: not just the weight at birth of the baby, but whether\nthe baby was born prematurely, the ages of the parents, whether the\nparents are married, how long (in weeks) the pregnancy lasted (this is\ncalled the \"gestation\") and so on. We have seen these data before.\n\n\n\n(a) Read in the data from the file into R, bearing in mind what\ntype of file it is. \n\n\n\n(b) Find a 95\\% confidence interval for the mean birth weight of\nall babies born in North Carolina (of which these babies are a\nsample). At the end, you should state what the confidence interval is.\nGiving some output is necessary, but *not* enough by itself.\n\n\n\n\n\n(c) Birth weights of babies born in the United States have a mean\nof 7.3 pounds. Is there any evidence that babies born in North\nCarolina are less heavy on average? State appropriate hypotheses, do your\ntest, obtain a P-value and state your conclusion, in terms of the\noriginal data.\n\n\n\n\n(d) The theory behind the $t$-test says that the distribution of\nbirth weights should be (approximately) normally distributed. Obtain a\nhistogram of the birth weights. Does it look approximately normal?\nComment briefly. (You'll have to pick a number of bins for your\nhistogram first. I don't mind very much what you pick, as long as it's\nnot obviously too many or too few bins.)\n\n\n\n\n\n\n\n\n\n##  Inferring ice break-up in Nenana\n\n\n Nenana, Alaska, is about 50 miles west of Fairbanks.\nEvery spring, there is a contest in Nenana. A wooden tripod is\nplaced on the frozen river, and people try to guess the exact minute\nwhen the ice melts enough for the tripod to fall through the ice. The\ncontest started in 1917 as an amusement for railway workers, and has\ntaken place every year since. Now, hundreds of thousands of people\nenter their guesses on the Internet and the prize for the winner can\nbe as much as \\$300,000. \n\nBecause so much money is at stake, and because the exact same tripod\nis placed at the exact same spot on the ice every year, the data are\nconsistent and accurate. The data are in\n[link](http://ritsokiguess.site/datafiles/nenana.txt). \n\nYes, we saw these data before.\n\n\n\n(a) Read the data into R, as before, or use the data frame that\nyou read in before.  Note that the values are separated by\n*tabs* rather than spaces, so you'll need an appropriate\n`read_` to read it in.\n\n\n\n(b) Obtain a 90\\% confidence interval for the mean\n`JulianDate`. What interval do you get? Looking back at your\nhistogram, do you have any doubts about the validity of what you\nhave just done?\n\n\n\n(c) An old-timer in Nenana strokes his grey beard and says\n\"When I were young, I remember the tripod used to fall into the water around May 10\". \nIn a non-leap year, May 10 is Julian day 130. Test the null hypothesis that the \nmean `JulianDay` is 130, against the alternative that it is less.  What do you conclude?\nWhat practical implication does that have \n(assuming that the old-timer has a good memory)?\n\n\n\n(d) Plot `JulianDate` against `Year` on a\nscatterplot. What recent trends, if any, do you see? Comment\nbriefly. (You did this before, but I have some extra comments on\nthe graph this time, so feel free to just read this part.)\n\n\n\n\n\n\n\n\n## Diameters of trees\n\n The Wade Tract in Thomas County, Georgia, is an old-growth forest of longleaf pine trees. It has survived in a relatively undisturbed state since before settlements of the area by Europeans. For each tree in the tract, researchers measured the diameter at breast height. This is a standard measure in forestry: it is defined as the diameter of the tree at 4.5 feet above the ground.^[The height of a typical human breast off the ground. Men have a breast too, you know.]  They are interested in the mean diameter at breast height of the trees in this tract. These values are in [http://ritsokiguess.site/datafiles/treediameter.csv](http://ritsokiguess.site/datafiles/treediameter.csv). The diameters are measured in centimetres.\nThe easiest way to get the URL is to *right*-click on the blue text and select Copy URL. (If you copy and paste the actual text you might end up with extra spaces, especially if the printed URL goes over two lines.)\n\n\n\n(a) Read in and display (some of) the data.\n\n\n\n(b) Make a suitable plot of your dataframe.\n\n\n\n(c) Obtain a 95% confidence interval for the mean diameter.\n\n\n\n(d) Based on what you have seen so far, would you expect to reject a null hypothesis that the population mean diameter (of all longleaf pines like these) is 35 cm? Explain briefly. Then, carry out the test (against a two-sided alternative) and explain briefly whether you were right.\n\n\n\n(e) Would you expect 35 cm to be in a *99%* confidence interval for the mean diameter? Explain briefly, and then see if you were right.\n\n\n\n\n\n\n\n## One-sample cholesterol\n\n The data set [here](http://ritsokiguess.site/datafiles/cholest.csv) contains cholesterol\nmeasurements for heart attack patients (at several different times) as\nwell as for a group of control patients. We will focus on the control\npatients in this question. \n\n\n(a) Read in and display (some of) the data.\n\n\n\n(b) Make a suitable plot of the cholesterol levels of the\ncontrol patients, and comment briefly on the shape of the\ndistribution. \n\n\n\n(c) It is recommended that people in good health, such as the\nControl patients here, keep their cholesterol level below 200. Is\nthere evidence that the mean cholesterol level of the population of\npeople of which the Control patients are a sample is less than 200? Show that you understand the process,\nand state your conclusion in the context of the data.\n\n\n\n(d) What values could the population mean cholesterol level take? You\nmight need to get some more output to determine this.\n\n\n\n(e) Explain briefly why you would be reasonably happy to trust\nthe $t$ procedures in this question. (There are two points you need\nto make.)\n\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Hunter-gatherers in Australia\n\n\n A hunter-gatherer society is one where people get their food\nby hunting, fishing or foraging rather than by agriculture or by\nraising animals. Such societies tend to move from place to place.\nAnthropologists have studied hunter-gatherer societies in forest\necosystems across the world. The average population density of these\nsocieties is 7.38 people per 100 km$^2$. Hunter-gatherer societies on\ndifferent continents might have different population densities,\npossibly because of large-scale ecological constraints (such as\nresource availability), or because of other factors, possibly social\nand/or historic, determining population density.\n\nSome hunter-gatherer societies in Australia were studied, and the\npopulation density per 100 km$^2$ recorded for each. The data are in\n[http://ritsokiguess.site/datafiles/hg.txt](http://ritsokiguess.site/datafiles/hg.txt). \n\n\n\n(a) Read the data into R. Do you have the correct variables?\nHow many hunter-gatherer societies in Australia were studied?\nExplain briefly.\n\n\nSolution\n\n\nThe data values are separated by (single) spaces, so `read_delim`\nis the thing:\n\n::: {.cell}\n\n```{.r .cell-code}\nurl=\"http://ritsokiguess.site/datafiles/hg.txt\"\nsocieties=read_delim(url,\" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 13 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): name\ndbl (1): density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\nI like to put the URL in a variable first, because if I don't, the\n`read_delim` line can be rather long. But if you want to do it\nin one step, that's fine, as long as it's clear that you are doing the\nright thing.\n\nLet's look at the data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\nsocieties\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"density\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"jeidji\",\"2\":\"17.00\"},{\"1\":\"kuku\",\"2\":\"50.00\"},{\"1\":\"mamu\",\"2\":\"45.00\"},{\"1\":\"ngatjan\",\"2\":\"59.80\"},{\"1\":\"undanbi\",\"2\":\"21.74\"},{\"1\":\"jinibarra\",\"2\":\"16.00\"},{\"1\":\"ualaria\",\"2\":\"9.00\"},{\"1\":\"barkindji\",\"2\":\"15.43\"},{\"1\":\"wongaibon\",\"2\":\"5.12\"},{\"1\":\"jaralde\",\"2\":\"40.00\"},{\"1\":\"tjapwurong\",\"2\":\"35.00\"},{\"1\":\"tasmanians\",\"2\":\"13.35\"},{\"1\":\"badjalang\",\"2\":\"13.40\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nI have the name of each society and its population density, as\npromised (so that is correct). There were 13 societies that were\nstudied. For me, they were all displayed. For you, you'll probably see only the first ten, and you'll have to click Next to see the last three.\n    \n$\\blacksquare$\n\n(b) The question of interest is whether these Australian\nhunter-gatherer societies are like the rest of the world in terms of mean\npopulation density. State suitable null and alternative\nhypotheses. *Define any symbols you use*: that is, if you use a\nsymbol, you also have to say what it means. \n\n\nSolution\n\n\nThe mean for the world as a whole (\"average\", as stated earlier)\nis 7.38. Let $\\mu$ denote the population mean for Australia (of\nwhich these societies are a sample). Then our hypotheses are:\n$$ H_0: \\mu=7.38$$\nand\n$$ H_a: \\mu \\ne 7.38.$$\nThere is no reason for a one-sided alternative here, since all we\nare interested in is whether Australia is different from the rest\nof the world.\n*Expect to lose a point* if you use the symbol $\\mu$ without\nsaying what it means.\n\n$\\blacksquare$\n\n(c) Test your hypotheses using a suitable test. What do you\nconclude, in the context of the data?\n\n\nSolution\n\n\nA $t$-test, since we are testing a mean:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(societies$density,mu=7.38)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  societies$density\nt = 3.8627, df = 12, p-value = 0.002257\nalternative hypothesis: true mean is not equal to 7.38\n95 percent confidence interval:\n 15.59244 36.84449\nsample estimates:\nmean of x \n 26.21846 \n```\n\n\n:::\n:::\n\nThe P-value is 0.0023, less than the usual $\\alpha$ of 0.05, so we\n*reject* the null hypothesis and conclude that the mean\npopulation density is not equal to 7.38. That is to say, Australia is\ndifferent from the rest of the world in this sense.\n\nAs you know, \"reject the null hypothesis\" is only part of the\nanswer, so gets only part of the marks.\n    \n$\\blacksquare$\n\n(d) Do you have any doubts about the validity of your test?\nExplain briefly, using a suitable graph to support your\nexplanation. \n\n\nSolution\n\n\nThe assumption behind the $t$-test is that the data are\napproximately normal. We can assess that in several ways, but the\nsimplest (which is perfectly acceptable at this point) is a\nhistogram. You'll need to pick a suitable number of bins. This one\ncomes from Sturges' rule:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(societies,aes(x=density))+geom_histogram(bins=5)\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/schnagg-1.png){width=672}\n:::\n:::\n\nYour conclusion might depend on how many bins you chose for your\nhistogram. Here's 8 bins (which is really too many with only 13\nobservations, but it actually shows the shape well): \n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(societies,aes(x=density))+geom_histogram(bins=8)\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/fuos-1.png){width=672}\n:::\n:::\n\nor you can get a number of bins from one of the built-in functions,\nsuch as:\n\n::: {.cell}\n\n```{.r .cell-code}\nmybins=nclass.FD(societies$density)\nmybins\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\nThis one is small. The interquartile range is large and $n$ is small,\nso the binwidth will be large and therefore the number of bins will be\nsmall. \n\nOther choices: a one-group boxplot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(societies,aes(x=1,y=density))+geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/hotz-1.png){width=672}\n:::\n:::\n\nThis isn't the best for assessing normality as such, but it will tell\nyou about lack of symmetry and outliers, which are the most important\nthreats to the $t$-test, so it's fine here. Or, a normal quantile plot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(societies,aes(sample=density))+\nstat_qq()+stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/narten-1.png){width=672}\n:::\n:::\n\nThis is actually the best way to assess normality, but I'm not\nexpecting you to use this plot here, because we may not have gotten to\nit in class yet. (If you have read ahead and successfully use the\nplot, it's fine.)\n\nAfter you have drawn your chosen plot (you need *one* plot), you\nneed to say something about normality and thus whether you have any\ndoubts about the validity of your $t$-test. This will depend on the\ngraph you drew: if you think your graph is symmetric and outlier-free,\nyou should have no doubts about your $t$-test; if you think it has\nsomething wrong with it, you should say what it is and express your\ndoubts. My guess is that you will think this distribution is skewed to\nthe right. Most of my plots are saying that.^[The normal  quantile plot is rather interesting: it says that the uppermost  values are approximately normal, but the *smallest* eight or so  values are too bunched up to be normal. That is, normality fails not  because of the long tail on the right, but the bunching on the  left. Still right-skewed, though.]\n\nOn the website where I got these data, they were using the data as\nan example for another test, precisely *because* they thought the\ndistribution was right-skewed. Later on, we'll learn about the sign\ntest for the median, which I think is actually a better test here.\n    \n$\\blacksquare$\n\n\n\n\n\n\n\n##  Buses to Boulder\n\n\n A bus line operates a route from Denver to Boulder (these\nplaces are in Colorado). The\nschedule says that the journey time should be 60 minutes. 11\nrandomly chosen journey times were recorded, and these are in the\nfile [link](http://ritsokiguess.site/datafiles/buses.txt), with\njourney times shown in minutes.\n\n\n(a) Read the data into R, and display the data frame that you\nread in.\n \nSolution\n\n\nSince you can read the data directly from the URL, do that (if\nyou are online) rather than having to copy and paste and save,\nand then find the file you saved.\nAlso, there is only one column, so you can pretend that there\nwere multiple columns, separated by whatever you like. It's least\ntyping to pretend that they were separated by commas like a\n`.csv` file:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/buses.txt\"\njourney.times <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 11 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): minutes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\njourney.times\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"minutes\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"58\"},{\"1\":\"61\"},{\"1\":\"69\"},{\"1\":\"62\"},{\"1\":\"81\"},{\"1\":\"54\"},{\"1\":\"72\"},{\"1\":\"71\"},{\"1\":\"53\"},{\"1\":\"54\"},{\"1\":\"66\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n       \n\nUsing `read_delim` with any delimiter (such as `\" \"`)\nwill also work, and is thus also good.\n\nVariable names in R can have a dot (or an underscore, but not a space)\nin them. I have grown accustomed to using dots to separate words. This\nworks in R but not other languages, but is seen by some as\nold-fashioned, with underscores being the modern way.^[In some  languages, a dot is used to concatenate bits of text, or as a way of  calling a method on an object. But in R, a dot has no special  meaning, and is used in function names like `t.test`. Or  `p.value`.] \nYou can also use what is called \"camel case\" \nby starting each \"word\" after the first with an uppercase\nletter like this:\n\n::: {.cell}\n\n```{.r .cell-code}\njourneyTimes <- read_csv(my_url)\n```\n:::\n\n \n\nYou have to get the capitalization and punctuation right when you use your variables,\nno matter what they're called. In any of the cases above, there is no\nvariable called `journeytimes`.  As Jenny Bryan (in\n[link](http://www.stat.ubc.ca/~jenny/STAT545A/block01_basicsWorkspaceWorkingDirProject.html)) \nputs it, boldface in original:\nImplicit contract with the computer / scripting language: Computer\nwill do tedious computation for you. In return, you will be\ncompletely precise in your instructions. Typos matter. Case\nmatters. **Get better at typing.** \n \n$\\blacksquare$\n\n(b) Run a suitable test to see whether there is evidence that\nthe mean journey time differs from 60 minutes. What do you\nconclude? (I want a conclusion that says something about journey\ntimes of buses.)\n \nSolution\n\n\n`t.test` doesn't take a `data=` to say which\ndata frame to use. Wrap it in a `with`:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(journey.times, t.test(minutes, mu = 60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  minutes\nt = 1.382, df = 10, p-value = 0.1971\nalternative hypothesis: true mean is not equal to 60\n95 percent confidence interval:\n 57.71775 69.73680\nsample estimates:\nmean of x \n 63.72727 \n```\n\n\n:::\n:::\n\n       \n\nWe are testing that the mean journey time is 60 minutes, against the\ntwo-sided alternative (default) that the mean is not equal to 60 minutes. The\nP-value, 0.1971, is a lot bigger than the usual $\\alpha$ of 0.05, so we cannot\nreject the null hypothesis. That is, there is no evidence that the\nmean journey time differs from 60 minutes.\n\nAs you remember, we have not proved that the mean journey time\n*is* 60 minutes, which is what \"accepting the null hypothesis\"\nwould be. We have only failed to reject it, in a shoulder-shrugging\nkind of way: \"the mean journey time *could* be 60 minutes\". The\nother acceptable word is \"retain\"; when you say \"we retain the null hypothesis\", you imply something \nlike \"we act as if the mean is 60 minutes, at least until we find something better.\"\n \n$\\blacksquare$\n\n(c) Give a 95\\% confidence interval for the mean journey\ntime. (No R code is needed here.)\n \nSolution\n\n\nJust read it off from the output: 57.72 to 69.74 minutes.\n \n$\\blacksquare$\n\n(d) Do you draw consistent conclusions from your test and\nconfidence interval? Explain briefly.\n \nSolution\n\n\nThe test said that we should not reject a mean of 60\nminutes. The confidence interval says that 60 minutes is inside\nthe interval of plausible values for the population mean, which\nis another way of saying the same thing. (If we had rejected 60\nas a mean, 60 would have been *outside* the confidence interval.)\n \n$\\blacksquare$\n\n(e) Draw a boxplot of the journey times. Do you see a reason\nto doubt the test that you did above?\n \nSolution\n\n The grouping variable is a\n\"nothing\" as in the Ken and Thomas question (part (d)):\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(journey.times, aes(x = 1, y = minutes)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/brixham-1.png){width=672}\n:::\n:::\n\n       \n\nThe assumption behind the $t$-test is that the population from which\nthe data come has a normal distribution: ie.\\ symmetric with no\noutliers. A small sample (here we have 11 values) even from a normal\ndistribution might look quite non-normal (as in Assignment 0 from last\nweek), so I am not hugely concerned by this boxplot. However, it's\nperfectly all right to say that this distribution is skewed, and\ntherefore we should doubt the $t$-test, because the upper whisker is\nlonger than the lower one. In fact, the topmost value is very nearly\nan outlier:^[Whether you think it is or not may depend on how  many bins you have on your histogram. With 5 bins it looks like an  outlier, but with 6 it does not. Try it and see.]\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(journey.times, aes(x = minutes)) + geom_histogram(bins = 5)\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/babbacombe-1.png){width=672}\n:::\n:::\n\n \n\nand there might be skewness as well, so maybe I should have been concerned.\n\nI would be looking for some intelligent comment on the boxplot: what it\nlooks like vs.\\ what it ought to look like. I don't so much mind what\nthat comment is, as long as it's intelligent enough.\n\nPerhaps I should draw a normal quantile plot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(journey.times, aes(sample = minutes)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/boulder-4-1.png){width=672}\n:::\n:::\n\n \n\nThe normal quantile plot is saying that the problem is actually at the\n*bottom* of the distribution: the lowest value is not low enough,\nbut the highest value is actually *not* too high. So this one\nseems to be on the edge between OK and being right-skewed (too bunched\nup at the bottom). My take is that with this small sample this is not\ntoo bad. But you are free to disagree.\n\nIf you don't like the normality, you'd use a *sign test* and test\nthat the *median* is not 60 minutes, which you would (at my\nguess) utterly fail to reject:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\nsign_test(journey.times, minutes, 60)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$above_below\nbelow above \n    4     7 \n\n$p_values\n  alternative   p_value\n1       lower 0.8867187\n2       upper 0.2744141\n3   two-sided 0.5488281\n```\n\n\n:::\n\n```{.r .cell-code}\nci_median(journey.times, minutes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 54.00195 71.99023\n```\n\n\n:::\n:::\n\n \n\nand so we do. The median could easily be 60 minutes.\n \n$\\blacksquare$\n\n\n\n\n\n##  Length of gestation in North Carolina\n\n\n The data in file\n[link](http://ritsokiguess.site/datafiles/ncbirths.csv) are about\n500 randomly chosen births of babies in North Carolina. There is a lot\nof information: not just the weight at birth of the baby, but whether\nthe baby was born prematurely, the ages of the parents, whether the\nparents are married, how long (in weeks) the pregnancy lasted (this is\ncalled the \"gestation\") and so on. We have seen these data before.\n\n\n\n(a) Read in the data from the file into R, bearing in mind what\ntype of file it is. \n\n\nSolution\n\n\nThis is a `.csv` file (it came from a spreadsheet), so it\nneeds reading in accordingly. Work directly from the URL (rather\nthan downloading the file):\n\n::: {.cell}\n\n```{.r .cell-code}\nmyurl <- \"http://ritsokiguess.site/datafiles/ncbirths2.csv\"\nbw <- read_csv(myurl)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 500 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): father_age, mother_age, weeks_gestation, pre_natal_visits, marital...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n \n\n$\\blacksquare$\n\n(b) Find a 95\\% confidence interval for the mean birth weight of\nall babies born in North Carolina (of which these babies are a\nsample). At the end, you should state what the confidence interval is.\nGiving some output is necessary, but *not* enough by itself.\n\n\nSolution\n\n\nThis:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(bw$weight_pounds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  bw$weight_pounds\nt = 104.94, df = 499, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.936407 7.201093\nsample estimates:\nmean of x \n  7.06875 \n```\n\n\n:::\n:::\n\n \n\nor (the same, but remember to match your brackets):\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(bw, t.test(weight_pounds))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  weight_pounds\nt = 104.94, df = 499, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.936407 7.201093\nsample estimates:\nmean of x \n  7.06875 \n```\n\n\n:::\n:::\n\n \n\nThe confidence interval goes from 6.94 to 7.20 pounds.\n\nThere is an annoyance about `t.test`. Sometimes you can use\n`data=` with it, and sometimes not. When we do a two-sample\n$t$-test later, there is a \"model formula\" with a squiggle in it,\nand there we can use `data=`, but here not, so you have to use\nthe dollar sign or the `with` to say which data frame to get\nthings from. The distinction seems to be that *if you are using a\nmodel formula*, you can use `data=`, and if not, not.\n\nThis is one of those things that is a consequence of R's history. The\noriginal `t.test` was without the model formula and thus\nwithout the `data=`, but the model formula got \"retro-fitted\"\nto it later. Since the model formula comes from things like\nregression, where `data=` is legit, that had to be retro-fitted\nas well. Or, at least, that's my understanding.\n\n\n\n\n$\\blacksquare$\n\n(c) Birth weights of babies born in the United States have a mean\nof 7.3 pounds. Is there any evidence that babies born in North\nCarolina are less heavy on average? State appropriate hypotheses, do your\ntest, obtain a P-value and state your conclusion, in terms of the\noriginal data.\n\n\n\nSolution\n\n\nLet $\\mu$ be the population mean (the mean weight of all babies born\nin North Carolina). Null hypothesis is $H_0: \\mu=7.3$ pounds, and the alternative  is\nthat the mean is less: $H_a: \\mu<7.3$ pounds.\n\nNote that I defined $\\mu$ first before I used it.\n\nThis is a one-sided\nalternative, which we need to feed into `t.test`:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(bw$weight_pounds, mu = 7.3, alternative = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  bw$weight_pounds\nt = -3.4331, df = 499, p-value = 0.0003232\nalternative hypothesis: true mean is less than 7.3\n95 percent confidence interval:\n     -Inf 7.179752\nsample estimates:\nmean of x \n  7.06875 \n```\n\n\n:::\n:::\n\n\nOr with `with`. If you see what I mean.\n\nThe P-value is 0.0003, which is *less* than any $\\alpha$ we might\nhave chosen: we *reject* the null hypothesis in favour of the\nalternative, and thus we conclude that the mean birth weight of babies\nin North Carolina\nis indeed less than 7.3 pounds.\n\n\"Reject the null hypothesis\" is *not* a complete answer. You\nneed to say something about what rejecting the null hypothesis means\n*in this case*: that is, you must make a statement about birth\nweights of babies.\n\n\n\n\n$\\blacksquare$\n\n(d) The theory behind the $t$-test says that the distribution of\nbirth weights should be (approximately) normally distributed. Obtain a\nhistogram of the birth weights. Does it look approximately normal?\nComment briefly. (You'll have to pick a number of bins for your\nhistogram first. I don't mind very much what you pick, as long as it's\nnot obviously too many or too few bins.)\n\n\n\nSolution\n\n\nWe did this before (and discussed the number of bins before), so\nI'll just reproduce my 10-bin histogram (which is what I preferred,\nbut this is a matter of taste):\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(bw, aes(x = weight_pounds)) + geom_histogram(bins = 10)\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/ncbirths-inf-5-1.png){width=672}\n:::\n:::\n\n \n\nSo, we were assessing normality. What about that?\n\nIt is mostly normal-looking, but I am suspicious about those\n*very* low birth weights, the ones below about 4 pounds. There\nare too many of those, as I see it.\n\nIf you think this is approximately normal, you need to make some\ncomment along the lines of \"the shape is approximately symmetric with no outliers\". \nI think my first answer is better, but this answer is\nworth something, since it is a not completely unreasonable\ninterpretation of the histogram.\n\nA normal quantile plot is better for assessing normality\nthan a histogram is, but I won't make you do one until we have seen\nthe idea in class. Here's the normal quantile plot for these data:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(bw, aes(sample = weight_pounds)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/ncbirths-inf-6-1.png){width=672}\n:::\n:::\n\n \n\nThis is rather striking: the lowest birthweights (the ones below 5\npounds or so) are *way* too low for a normal distribution to\napply. The top end is fine (except perhaps for that one very heavy\nbaby), but there are too many low birthweights for a normal\ndistribution to be believable. Note how much clearer this story is\nthan on the histogram.\n\nHaving said that, the $t$-test, especially with a sample size as big\nas this (500), behaves *very* well when the data are somewhat\nnon-normal (because it takes advantage of the Central Limit Theorem:\nthat is, it's the *sampling distribution of the sample mean*\nwhose shape matters). So, even though the data are definitely not\nnormal, I wouldn't be too worried about our test.\n\nThis perhaps gives some insight as to why Freedman-Diaconis said we\nshould use so many bins for our histogram. We have a lot of low-end\noutliers, so that the IQR is actually *small* compared to the\noverall spread of the data (as measured, say, by the SD or the range)\nand so FD thinks we need a lot of bins to describe the shape. Sturges\nis based on data being approximately normal, so it will tend to\nproduce a small number of bins for data that have outliers.\n\n\n$\\blacksquare$\n\n\n\n\n\n\n##  Inferring ice break-up in Nenana\n\n\n Nenana, Alaska, is about 50 miles west of Fairbanks.\nEvery spring, there is a contest in Nenana. A wooden tripod is\nplaced on the frozen river, and people try to guess the exact minute\nwhen the ice melts enough for the tripod to fall through the ice. The\ncontest started in 1917 as an amusement for railway workers, and has\ntaken place every year since. Now, hundreds of thousands of people\nenter their guesses on the Internet and the prize for the winner can\nbe as much as \\$300,000. \n\nBecause so much money is at stake, and because the exact same tripod\nis placed at the exact same spot on the ice every year, the data are\nconsistent and accurate. The data are in\n[link](http://ritsokiguess.site/datafiles/nenana.txt). \n\nYes, we saw these data before.\n\n\n\n(a) Read the data into R, as before, or use the data frame that\nyou read in before.  Note that the values are separated by\n*tabs* rather than spaces, so you'll need an appropriate\n`read_` to read it in.\n\n\nSolution\n\n\nThese are \"tab-separated values\", so `read_tsv` is the\nthing, as for the Australian athletes:\n\n::: {.cell}\n\n```{.r .cell-code}\nmyurl <- \"http://ritsokiguess.site/datafiles/nenana.txt\"\nnenana <- read_tsv(myurl)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 87 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): Date&Time\ndbl (2): Year, JulianDate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n       \n\nUse whatever name you like for the data frame. One that is different\nfrom any of the column headers is smart; then it is clear whether you\nmean the whole data frame or one of its columns. `ice` or\n`melt` or anything like that would also be good.\n    \n$\\blacksquare$\n\n(b) Obtain a 90\\% confidence interval for the mean\n`JulianDate`. What interval do you get? Looking back at your\nhistogram, do you have any doubts about the validity of what you\nhave just done?\n\n\nSolution\n\n\nThis is a matter of using `t.test` and pulling out the\ninterval. Since we are looking for a non-standard interval, we\nhave to remember `conf.level` as the way to get the\nconfidence level that we want. I'm going with `with` this\ntime, though the dollar-sign thing is equally as good:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(nenana, t.test(JulianDate, conf.level = 0.90))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  JulianDate\nt = 197.41, df = 86, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 124.4869 126.6018\nsample estimates:\nmean of x \n 125.5443 \n```\n\n\n:::\n:::\n\n       \n\nBetween 124.5 and 126.6 days into the year. Converting that into\nsomething we can understand (because I want to), there are\n$31+28+31+30=120$ days in \nJanuary through April (in a non-leap year), so this says that the mean\nbreakup date is between about May 4 and May 6. \n\nThe $t$-test is based on an assumption of data coming from a normal\ndistribution. The histogram we made earlier looks pretty much normal,\nso there are no doubts about normality and thus no doubts about the\nvalidity of what we have done, on the evidence we have seen so far. (I\nhave some doubts on different grounds, based on another of the plots\nwe did earlier, which I'll explain later, but all I'm expecting you to\ndo is to look at the histogram and say \"Yep, that's normal enough\". \nBear in mind that the sample size is 87, which is large\nenough for the Central Limit Theorem to be pretty helpful, so that we don't need the data to be more than \"approximately normal\" for the sampling distribution of the sample mean to be very close to $t$ with the right df.)\n    \n$\\blacksquare$\n\n(c) An old-timer in Nenana strokes his grey beard and says\n\"When I were young, I remember the tripod used to fall into the water around May 10\". \nIn a non-leap year, May 10 is Julian day 130. Test the null hypothesis that the \nmean `JulianDay` is 130, against the alternative that it is less.  What do you conclude?\nWhat practical implication does that have \n(assuming that the old-timer has a good memory)?\n\n\nSolution\n\n\nThe test is `t.test` again, but this time we have to\nspecify a null mean and a direction of alternative:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(nenana, t.test(JulianDate, mu = 130, alternative = \"less\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  JulianDate\nt = -7.0063, df = 86, p-value = 2.575e-10\nalternative hypothesis: true mean is less than 130\n95 percent confidence interval:\n     -Inf 126.6018\nsample estimates:\nmean of x \n 125.5443 \n```\n\n\n:::\n:::\n\n       \n\nFor a test, look first at the P-value, which is 0.0000000002575: that\nis to say, the P-value is very small, definitely smaller than 0.05 (or\nany other $\\alpha$ you might have chosen). So we *reject* the\nnull hypothesis, and conclude that the mean `JulianDate` is actually\n*less* than 130.\n\nNow, this is the date on which the ice breaks up on average, and we\nhave concluded that it is *earlier* than it used to be, since we\nare assuming the old-timer's memory is correct. \n\nThis is evidence in\nfavour of global warming; a small piece of evidence, to be sure, but\nthe ice is melting earlier than it used to all over the Arctic, so\nit's not just in Nenana that it is happening. You don't need to get to\nthe \"global warming\" part, but I *do* want you to observe that\nthe ice is breaking up earlier than it used to.\n    \n$\\blacksquare$\n\n(d) Plot `JulianDate` against `Year` on a\nscatterplot. What recent trends, if any, do you see? Comment\nbriefly. (You did this before, but I have some extra comments on\nthe graph this time, so feel free to just read this part.)\n\n\nSolution\n\n\nI liked  the `ggplot` with a smooth trend on it:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(nenana, aes(x = Year, y = JulianDate)) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/nenana-inf-4-1.png){width=672}\n:::\n:::\n\n \n\nThere was something obvious to see: after about 1960, there is a clear\ndownward trend: the ice is breaking up earlier on average every\nyear. Even though there is a lot of variability, the overall trend,\nviewed this way, is clear (and consistent with the test we did\nearlier). Note that the old-timer's value of 130 is the kind of\n`JulianDate` we would typically observe around 1920, which\nwould make the old-timer over 90 years old.\n\nAll right, why did I say I had some doubts earlier? Well, because of\nthis downward trend, the mean is not actually the same all the way\nthrough, so it doesn't make all that much sense to estimate it, which\nis what we were doing earlier by doing a confidence interval or a\nhypothesis test. What would actually make more sense is to estimate\nthe mean `JulianDate` *for a particular year*. This could\nbe done by a regression: predict `JulianDate` from\n`Year`, and then get a \n\"confidence interval for the mean response\" \n(as you would have seen in B27 or will see in C67). The\ntrend isn't really linear, but is not that far off. I can modify the\nprevious picture to give you an idea. Putting in `method=\"lm\"`\nfits a line; as we see later, `lm` does regressions in R:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(nenana, aes(x = Year, y = JulianDate)) + geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/nenana-inf-5-1.png){width=672}\n:::\n:::\n\n \n\nCompare the confidence interval for the mean `JulianDate` in\n1920: 126 to 131 (the shaded area on the graph), with 2000: 121 to\n125. A change of about 5 days over 80 years. And with the recent trend\nthat we saw above, it's probably changing faster than that\nnow. Sobering indeed.\n      \n$\\blacksquare$\n\n\n\n\n\n\n## Diameters of trees\n\n The Wade Tract in Thomas County, Georgia, is an old-growth forest of longleaf pine trees. It has survived in a relatively undisturbed state since before settlements of the area by Europeans. For each tree in the tract, researchers measured the diameter at breast height. This is a standard measure in forestry: it is defined as the diameter of the tree at 4.5 feet above the ground.^[The height of a typical human breast off the ground. Men have a breast too, you know.]  They are interested in the mean diameter at breast height of the trees in this tract. These values are in [http://ritsokiguess.site/datafiles/treediameter.csv](http://ritsokiguess.site/datafiles/treediameter.csv). The diameters are measured in centimetres.\nThe easiest way to get the URL is to *right*-click on the blue text and select Copy URL. (If you copy and paste the actual text you might end up with extra spaces, especially if the printed URL goes over two lines.)\n\n\n\n(a) Read in and display (some of) the data.\n\nSolution\n\n\nThe obvious way is this:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/treediameter.csv\"\ntrees <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 40 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (1): diameter\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ntrees\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"diameter\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"10.5\"},{\"1\":\"13.3\"},{\"1\":\"26.0\"},{\"1\":\"18.3\"},{\"1\":\"52.2\"},{\"1\":\"9.2\"},{\"1\":\"26.1\"},{\"1\":\"17.6\"},{\"1\":\"40.5\"},{\"1\":\"31.8\"},{\"1\":\"47.2\"},{\"1\":\"11.4\"},{\"1\":\"2.7\"},{\"1\":\"69.3\"},{\"1\":\"44.4\"},{\"1\":\"16.9\"},{\"1\":\"35.7\"},{\"1\":\"5.4\"},{\"1\":\"44.2\"},{\"1\":\"2.2\"},{\"1\":\"4.3\"},{\"1\":\"7.8\"},{\"1\":\"38.1\"},{\"1\":\"2.2\"},{\"1\":\"11.4\"},{\"1\":\"51.5\"},{\"1\":\"4.9\"},{\"1\":\"39.7\"},{\"1\":\"32.6\"},{\"1\":\"51.8\"},{\"1\":\"43.6\"},{\"1\":\"2.3\"},{\"1\":\"44.6\"},{\"1\":\"31.5\"},{\"1\":\"40.3\"},{\"1\":\"22.3\"},{\"1\":\"43.3\"},{\"1\":\"37.5\"},{\"1\":\"29.1\"},{\"1\":\"27.9\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nCall the data frame what you like, though it is better to use a name that tells you what the dataframe contains (rather than something like `mydata`). \n\nExtra 1: there is only one column, so you can pretend the columns are separated by anything at all. Thus you could use this:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/treediameter.csv\"\ntrees <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 40 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (1): diameter\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ntrees\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"diameter\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"10.5\"},{\"1\":\"13.3\"},{\"1\":\"26.0\"},{\"1\":\"18.3\"},{\"1\":\"52.2\"},{\"1\":\"9.2\"},{\"1\":\"26.1\"},{\"1\":\"17.6\"},{\"1\":\"40.5\"},{\"1\":\"31.8\"},{\"1\":\"47.2\"},{\"1\":\"11.4\"},{\"1\":\"2.7\"},{\"1\":\"69.3\"},{\"1\":\"44.4\"},{\"1\":\"16.9\"},{\"1\":\"35.7\"},{\"1\":\"5.4\"},{\"1\":\"44.2\"},{\"1\":\"2.2\"},{\"1\":\"4.3\"},{\"1\":\"7.8\"},{\"1\":\"38.1\"},{\"1\":\"2.2\"},{\"1\":\"11.4\"},{\"1\":\"51.5\"},{\"1\":\"4.9\"},{\"1\":\"39.7\"},{\"1\":\"32.6\"},{\"1\":\"51.8\"},{\"1\":\"43.6\"},{\"1\":\"2.3\"},{\"1\":\"44.6\"},{\"1\":\"31.5\"},{\"1\":\"40.3\"},{\"1\":\"22.3\"},{\"1\":\"43.3\"},{\"1\":\"37.5\"},{\"1\":\"29.1\"},{\"1\":\"27.9\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nor even this:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/treediameter.csv\"\ntrees <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  diameter = col_double()\n)\n```\n\n\n:::\n\n```{.r .cell-code}\ntrees\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"diameter\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"10.5\"},{\"1\":\"13.3\"},{\"1\":\"26.0\"},{\"1\":\"18.3\"},{\"1\":\"52.2\"},{\"1\":\"9.2\"},{\"1\":\"26.1\"},{\"1\":\"17.6\"},{\"1\":\"40.5\"},{\"1\":\"31.8\"},{\"1\":\"47.2\"},{\"1\":\"11.4\"},{\"1\":\"2.7\"},{\"1\":\"69.3\"},{\"1\":\"44.4\"},{\"1\":\"16.9\"},{\"1\":\"35.7\"},{\"1\":\"5.4\"},{\"1\":\"44.2\"},{\"1\":\"2.2\"},{\"1\":\"4.3\"},{\"1\":\"7.8\"},{\"1\":\"38.1\"},{\"1\":\"2.2\"},{\"1\":\"11.4\"},{\"1\":\"51.5\"},{\"1\":\"4.9\"},{\"1\":\"39.7\"},{\"1\":\"32.6\"},{\"1\":\"51.8\"},{\"1\":\"43.6\"},{\"1\":\"2.3\"},{\"1\":\"44.6\"},{\"1\":\"31.5\"},{\"1\":\"40.3\"},{\"1\":\"22.3\"},{\"1\":\"43.3\"},{\"1\":\"37.5\"},{\"1\":\"29.1\"},{\"1\":\"27.9\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nExtra 2: you might be wondering how they measure the diameter without doing something like drilling a hole through the tree. They don't actually measure the diameter at all. What they measure is the *circumference* of the tree, which is easy enough to do with a tape measure. Longleaf pines are usually near circular, so you get the diameter by taking the circumference and dividing by $\\pi$. [This City of Portland website](https://www.portlandoregon.gov/trees/article/424017) shows you how it's done.\n\n\n$\\blacksquare$\n\n\n(b) Make a suitable plot of your dataframe.\n\nSolution\n\n\nOne quantitative variable, so a histogram. Choose a sensible number of bins. There are 40 observations, so a number of bins up to about 10 is good. Sturges' rule says 6 since $2^6=64$:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(trees, aes(x=diameter)) + geom_histogram(bins=6)\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/trees-4-1.png){width=672}\n:::\n:::\n\nExtra 1: comments come later, but you might care to note (if only for yourself) that the distribution is a little skewed to the right, or, perhaps better, has *no* left tail at all. You might even observe that diameters cannot be less than 0 (they are measurements), and so you might expect a skew away from the limit. \n\nAfter you've looked at the $t$ procedures for these data, we'll get back to the shape. \n\nExtra 2: later we look at a more precise tool for assessing normality, the normal quantile plot, which looks like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(trees, aes(sample=diameter)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/trees-5-1.png){width=672}\n:::\n:::\n\nIf the data come from a normal distribution, the points should follow the straight line, at least approximately. Here, most of the points do, except for the points on the left, which veer away upwards from the line: that is, the highest values, on the right, are about right for a normal distribution, but the lowest values, on the left, *don't go down low enough*.^[They cannot go down far enough, because they can't go below zero.]\nThus, the problem with normality is not the long tail on the right, but the short one on the left. It is hard to get this kind of insight from the histogram, but at the moment, it's the best we have.\n\nThe big problems, for things like $t$-tests that depend on means, is stuff like outliers, or long tails, with extreme values that might distort the mean. Having short tails, as the left tail here, will make the distribution look non-normal but won't cause any problems for the $t$-tests.\n\n\n\n$\\blacksquare$\n\n\n(c) Obtain a 95% confidence interval for the mean diameter.\n\nSolution\n\n\nThis is `t.test`, but with `conf.level` to get the interval (and then you ignore the P-value):\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(trees, t.test(diameter))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  diameter\nt = 9.748, df = 39, p-value = 5.245e-12\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.6274 32.9526\nsample estimates:\nmean of x \n    27.29 \n```\n\n\n:::\n:::\n\nThe mean diameter of a longleaf pine (like the ones in this tract) is between 21.6 and 33.0 centimetres.\n\nIf you prefer, do it this way:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(trees$diameter)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  trees$diameter\nt = 9.748, df = 39, p-value = 5.245e-12\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.6274 32.9526\nsample estimates:\nmean of x \n    27.29 \n```\n\n\n:::\n:::\n\n\nYou need to *state the answer* and *round it off suitably*. The actual diameters in the data have one decimal place, so you can give the same accuracy for the CI, or *at most* two decimals (so 21.63 to 32.95 cm would also be OK).^[One more decimal place than the data is the maximum you give in a CI.] Giving an answer with more decimals is something you cannot possibly justify. Worse even than giving too many decimals is not writing out the interval at all. *Never* make your reader find something in output. If they want it, tell them what it is. \n\nThus, here (if this were being graded), one mark for the output, one more for saying what the interval is, and the third if you give the interval with a sensible number of decimals.\n\n\n$\\blacksquare$\n\n\n(d) Based on what you have seen so far, would you expect to reject a null hypothesis that the population mean diameter (of all longleaf pines like these) is 35 cm? Explain briefly. Then, carry out the test (against a two-sided alternative) and explain briefly whether you were right.\n\nSolution\n\n\nThe logic is that \"plausible\" values for the population mean, ones you believe, are inside the interval, and implausible ones that you don't believe are outside. Remember that the interval is your best answer to \"what is the population mean\", and 35 is outside the interval so you don't think the population mean is 35, and thus you would reject it.\n\nAre we right? Take out the `conf.level` and put in a `mu`:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(trees, t.test(diameter, mu = 35))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  diameter\nt = -2.754, df = 39, p-value = 0.008895\nalternative hypothesis: true mean is not equal to 35\n95 percent confidence interval:\n 21.6274 32.9526\nsample estimates:\nmean of x \n    27.29 \n```\n\n\n:::\n:::\n\nThe P-value is less than our $\\alpha$ of 0.05, so we would indeed reject a mean of 35 cm (in favour of the mean being different from 35).\n\n\n$\\blacksquare$\n\n\n(e) Would you expect 35 cm to be in a *99%* confidence interval for the mean diameter? Explain briefly, and then see if you were right.\n\nSolution\n\n\nThe P-value is less than 0.01 (as well as being less than 0.05), so, in the same way that 35 was outside the 95% interval, it should be outside the 99% CI also. Maybe not by much, though, since the P-value is only just less than 0.01:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(trees, t.test(diameter, conf.level = 0.99))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  diameter\nt = 9.748, df = 39, p-value = 5.245e-12\nalternative hypothesis: true mean is not equal to 0\n99 percent confidence interval:\n 19.70909 34.87091\nsample estimates:\nmean of x \n    27.29 \n```\n\n\n:::\n:::\n\nIndeed so, outside, but only just.\n\n\n$\\blacksquare$\n\n\n\n\n\n\n## One-sample cholesterol\n\n The data set [here](http://ritsokiguess.site/datafiles/cholest.csv) contains cholesterol\nmeasurements for heart attack patients (at several different times) as\nwell as for a group of control patients. We will focus on the control\npatients in this question. \n\n\n(a) Read in and display (some of) the data.\n\nSolution\n\nThis is (as you might guess) a `.csv`, so:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/cholest.csv\"\ncholest <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 30 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): 2-Day, 4-Day, 14-Day, control\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ncholest\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"2-Day\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"4-Day\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"14-Day\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"control\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"270\",\"2\":\"218\",\"3\":\"156\",\"4\":\"196\"},{\"1\":\"236\",\"2\":\"234\",\"3\":\"NA\",\"4\":\"232\"},{\"1\":\"210\",\"2\":\"214\",\"3\":\"242\",\"4\":\"200\"},{\"1\":\"142\",\"2\":\"116\",\"3\":\"NA\",\"4\":\"242\"},{\"1\":\"280\",\"2\":\"200\",\"3\":\"NA\",\"4\":\"206\"},{\"1\":\"272\",\"2\":\"276\",\"3\":\"256\",\"4\":\"178\"},{\"1\":\"160\",\"2\":\"146\",\"3\":\"142\",\"4\":\"184\"},{\"1\":\"220\",\"2\":\"182\",\"3\":\"216\",\"4\":\"198\"},{\"1\":\"226\",\"2\":\"238\",\"3\":\"248\",\"4\":\"160\"},{\"1\":\"242\",\"2\":\"288\",\"3\":\"NA\",\"4\":\"182\"},{\"1\":\"186\",\"2\":\"190\",\"3\":\"168\",\"4\":\"182\"},{\"1\":\"266\",\"2\":\"236\",\"3\":\"236\",\"4\":\"198\"},{\"1\":\"206\",\"2\":\"244\",\"3\":\"NA\",\"4\":\"182\"},{\"1\":\"318\",\"2\":\"258\",\"3\":\"200\",\"4\":\"238\"},{\"1\":\"294\",\"2\":\"240\",\"3\":\"264\",\"4\":\"198\"},{\"1\":\"282\",\"2\":\"294\",\"3\":\"NA\",\"4\":\"188\"},{\"1\":\"234\",\"2\":\"220\",\"3\":\"264\",\"4\":\"166\"},{\"1\":\"224\",\"2\":\"200\",\"3\":\"NA\",\"4\":\"204\"},{\"1\":\"276\",\"2\":\"220\",\"3\":\"188\",\"4\":\"182\"},{\"1\":\"282\",\"2\":\"186\",\"3\":\"182\",\"4\":\"178\"},{\"1\":\"360\",\"2\":\"352\",\"3\":\"294\",\"4\":\"212\"},{\"1\":\"310\",\"2\":\"202\",\"3\":\"214\",\"4\":\"164\"},{\"1\":\"280\",\"2\":\"218\",\"3\":\"NA\",\"4\":\"230\"},{\"1\":\"278\",\"2\":\"248\",\"3\":\"198\",\"4\":\"186\"},{\"1\":\"288\",\"2\":\"278\",\"3\":\"NA\",\"4\":\"162\"},{\"1\":\"288\",\"2\":\"248\",\"3\":\"256\",\"4\":\"182\"},{\"1\":\"244\",\"2\":\"270\",\"3\":\"280\",\"4\":\"218\"},{\"1\":\"236\",\"2\":\"242\",\"3\":\"204\",\"4\":\"170\"},{\"1\":\"NA\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"200\"},{\"1\":\"NA\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"176\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nNote for yourself that there are 30 observations (and some missing\nones), and a column called \\verb=control= that is the one we'll be\nworking with.\n\nExtra: the 2-day, 4-day and 14-day columns need to be referred to with funny \"backticks\" around their names, because a column name cannot contain a `-` or start with a number. This is not a problem here, since we won't be using those columns, but if we wanted to, this would not work:\n\n::: {.cell}\n\n```{.r .cell-code}\ncholest %>% summarize(xbar = mean(2-Day))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in `summarize()`:\nℹ In argument: `xbar = mean(2 - Day)`.\nCaused by error in `h()`:\n! error in evaluating the argument 'x' in selecting a method for function 'mean': object 'Day' not found\n```\n\n\n:::\n:::\n\nbecause it is looking for a column called `Day`, which doesn't exist. The meaning of `2-Day` is \"take the column called `Day` and subtract it from 2\". To make this work, we have to supply the backticks ourselves:\n\n::: {.cell}\n\n```{.r .cell-code}\ncholest %>% summarize(xbar = mean(`2-Day`, na.rm = TRUE))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"xbar\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"253.9286\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThis column also has missing values (at the bottom), so here I've asked to remove the missing values^[In R, missing values are labelled `NA`, and `rm` is Unix/C shorthand for *remove*.] before working out the mean. Otherwise the mean is, unhelpfully, missing as well.\n\nYou might imagine that dealing with column names like this would get annoying. There is a package called `janitor` that has a function called `clean_names` to save you the trouble. Install it first, then load it:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(janitor)\n```\n:::\n\nand then pipe your dataframe into `clean_names` and see what happens:\n\n::: {.cell}\n\n```{.r .cell-code}\ncholest %>% clean_names() -> cholest1\ncholest1\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"x2_day\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"x4_day\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"x14_day\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"control\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"270\",\"2\":\"218\",\"3\":\"156\",\"4\":\"196\"},{\"1\":\"236\",\"2\":\"234\",\"3\":\"NA\",\"4\":\"232\"},{\"1\":\"210\",\"2\":\"214\",\"3\":\"242\",\"4\":\"200\"},{\"1\":\"142\",\"2\":\"116\",\"3\":\"NA\",\"4\":\"242\"},{\"1\":\"280\",\"2\":\"200\",\"3\":\"NA\",\"4\":\"206\"},{\"1\":\"272\",\"2\":\"276\",\"3\":\"256\",\"4\":\"178\"},{\"1\":\"160\",\"2\":\"146\",\"3\":\"142\",\"4\":\"184\"},{\"1\":\"220\",\"2\":\"182\",\"3\":\"216\",\"4\":\"198\"},{\"1\":\"226\",\"2\":\"238\",\"3\":\"248\",\"4\":\"160\"},{\"1\":\"242\",\"2\":\"288\",\"3\":\"NA\",\"4\":\"182\"},{\"1\":\"186\",\"2\":\"190\",\"3\":\"168\",\"4\":\"182\"},{\"1\":\"266\",\"2\":\"236\",\"3\":\"236\",\"4\":\"198\"},{\"1\":\"206\",\"2\":\"244\",\"3\":\"NA\",\"4\":\"182\"},{\"1\":\"318\",\"2\":\"258\",\"3\":\"200\",\"4\":\"238\"},{\"1\":\"294\",\"2\":\"240\",\"3\":\"264\",\"4\":\"198\"},{\"1\":\"282\",\"2\":\"294\",\"3\":\"NA\",\"4\":\"188\"},{\"1\":\"234\",\"2\":\"220\",\"3\":\"264\",\"4\":\"166\"},{\"1\":\"224\",\"2\":\"200\",\"3\":\"NA\",\"4\":\"204\"},{\"1\":\"276\",\"2\":\"220\",\"3\":\"188\",\"4\":\"182\"},{\"1\":\"282\",\"2\":\"186\",\"3\":\"182\",\"4\":\"178\"},{\"1\":\"360\",\"2\":\"352\",\"3\":\"294\",\"4\":\"212\"},{\"1\":\"310\",\"2\":\"202\",\"3\":\"214\",\"4\":\"164\"},{\"1\":\"280\",\"2\":\"218\",\"3\":\"NA\",\"4\":\"230\"},{\"1\":\"278\",\"2\":\"248\",\"3\":\"198\",\"4\":\"186\"},{\"1\":\"288\",\"2\":\"278\",\"3\":\"NA\",\"4\":\"162\"},{\"1\":\"288\",\"2\":\"248\",\"3\":\"256\",\"4\":\"182\"},{\"1\":\"244\",\"2\":\"270\",\"3\":\"280\",\"4\":\"218\"},{\"1\":\"236\",\"2\":\"242\",\"3\":\"204\",\"4\":\"170\"},{\"1\":\"NA\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"200\"},{\"1\":\"NA\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"176\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThese are all legit column names; the `-` has been replaced by an underscore, and each of the first three column names has gained an `x` on the front so that it no longer starts with a number. This then works:\n\n::: {.cell}\n\n```{.r .cell-code}\ncholest1 %>% summarize(xbar = mean(x2_day, na.rm = TRUE))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"xbar\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"253.9286\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n$\\blacksquare$\n\n\n(b) Make a suitable plot of the cholesterol levels of the\ncontrol patients, and comment briefly on the shape of the\ndistribution. \n\nSolution\n\nThere is one quantitative variable, so a histogram, as ever:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(cholest, aes(x=control)) + geom_histogram(bins=6)\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/luprands-1.png){width=672}\n:::\n:::\n\nPick a number of bins that shows the shape reasonably well. Too many\nor too few won't. (Sturges' rule says 6, since there are 30\nobservations and $2^5=32$.) Seven bins also works, but by the time you\nget to 8 bins or more, you are starting to lose a clear picture of the\nshape. Four bins is, likewise, about as low as you can go before\ngetting too crude a picture. \n\nChoosing one of these numbers of bins will make it clear that the\ndistribution is somewhat skewed to the right.\n\n\n\n$\\blacksquare$\n\n\n(c) It is recommended that people in good health, such as the\nControl patients here, keep their cholesterol level below 200. Is\nthere evidence that the mean cholesterol level of the population of\npeople of which the Control patients are a sample is less than 200? Show that you understand the process,\nand state your conclusion in the context of the data.\n\nSolution\n\nThe word \"evidence\" means to do a hypothesis test and get a\nP-value. Choose an $\\alpha$ first, such as 0.05. \n\nTesting a mean implies a one-sample $t$-test. We are trying to\nprove that the mean is less than 200, so that's our alternative:\n$H_a: \\mu < 200$, and therefore the null is that the mean is equal\nto 200: $H_0: \\mu = 200$. (You might think it makes more logical\nsense to have $H_0: \\mu \\ge 200$, which is also fine. As long as\nthe null hypothesis has an equals in it in a logical place, you\nare good.)\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(cholest, t.test(control, mu=200, alternative = \"less\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  control\nt = -1.6866, df = 29, p-value = 0.05121\nalternative hypothesis: true mean is less than 200\n95 percent confidence interval:\n     -Inf 200.0512\nsample estimates:\nmean of x \n 193.1333 \n```\n\n\n:::\n:::\n\nThis is also good:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(cholest$control, mu=200, alternative = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  cholest$control\nt = -1.6866, df = 29, p-value = 0.05121\nalternative hypothesis: true mean is less than 200\n95 percent confidence interval:\n     -Inf 200.0512\nsample estimates:\nmean of x \n 193.1333 \n```\n\n\n:::\n:::\n\nI like the first version better because a lot of what we do later\ninvolves giving a data frame, and then working with things in that\ndata frame. This is more like that.\n\nThis test is *one*-sided because we are looking for evidence of\n*less*; if the mean is actually *more* than 200, we don't\ncare about that. For a one-sided test, R requires you to say which\nside you are testing.\n\nThe P-value is not (quite) less than 0.05, so we cannot quite reject\nthe null. Therefore, there is no evidence that the mean cholesterol\nlevel (of the people of which the control group are a sample) is less\nthan 200. Or, this mean is not significantly less than 200. Or, we\nconclude that this mean is equal to 200. Or, we conclude that this\nmean could be 200. Any of those.\n\nIf you chose a different $\\alpha$, draw the right conclusion for the\n$\\alpha$ you chose. For example, with $\\alpha=0.10$, we *do* have\nevidence that the mean is less than 200. Being consistent is more\nimportant than getting the same answer as me.  \n\nWriting out all the steps correctly shows that you understand the process. Anything less doesn't. \n\n\n$\\blacksquare$\n\n\n(d) What values could the population mean cholesterol level take? You\nmight need to get some more output to determine this.\n\nSolution\n\n\nThis is *not* quoting the sample mean, giving that as your answer, and then stopping. The sample mean should, we hope, be somewhere the population mean, but it is almost certainly not the same as the population mean, because there is variability due to random sampling. (This is perhaps the most important thing in all of Statistics: recognizing that variability exists and dealing with it.)\n\nWith that in mind, the question means to get a range of values that the population mean could\nbe: that is to say, a confidence interval. The one that came out\nof the previous output is one-sided, to go with the one-sided\ntest, but confidence intervals for us are two-sided, so we have to\nrun the test again, but two-sided, to get it. To do that, take out\nthe \"alternative\", thus (you can also take out the null mean,\nsince a confidence interval has no null hypothesis):\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(cholest, t.test(control))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  control\nt = 47.436, df = 29, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 184.8064 201.4603\nsample estimates:\nmean of x \n 193.1333 \n```\n\n\n:::\n:::\n\nWith 95\\% confidence, the population mean cholesterol level is between\n184.8 and 201.5. \n\n*You need to state the interval, and you also need to round off\nthe decimal places to something sensible.* This is because in your\nstatistical life, you are providing results to someone else *in a\nmanner that they can read and understand.* They do not have time to go\nsearching in some output, or to fish through some excessive number of\ndecimal places. If that's what you give them, they will ask you to\nrewrite your report, wasting everybody's time when you could have done\nit right the first time.\n\nHow many decimal places is a good number? Look back at your data. In\nthis case, the cholesterol values are whole numbers (zero decimal\nplaces). A confidence interval is talking about a mean. In this case,\nwe have a sample size of 30, which is between 10 and 100, so we can\njustify one extra decimal place beyond the data, here one decimal\naltogether, or two *at the absolute outside*. (Two is more\njustifiable if the sample size is bigger than 100.) See, for example,\n[this](https://www2.southeastern.edu/Academics/Faculty/dgurney/Math241/StatTopics/SciNot.htm), \nin particular the piece at the bottom.\n\n$\\blacksquare$\n\n\n(e) Explain briefly why you would be reasonably happy to trust\nthe $t$ procedures in this question. (There are two points you need\nto make.)\n\nSolution\n\nThe first thing is to look back at the graph you made\nearlier. This was skewed to the right (\"moderately\" or\n\"somewhat\" or however you described it). This would seem to say\nthat the $t$ procedures were not very trustworthy, since the\npopulation distribution doesn't look very normal in shape.\n\nHowever, the second thing is to look at the sample size. We have\nthe central limit theorem, which says (for us) that the larger the\nsample is, the less the normality matters, when it comes to\nestimating the mean. Here, the sample size is 30, which, for the\ncentral limit theorem, is large enough to overcome moderate\nnon-normality in the data.\n\nMy take, which I was trying to guide you towards, is that our\nnon-normality was not too bad, and so our sample size is large\nenough to trust the $t$ procedures we used.\n\nExtra 1: \\textbf{There is nothing magical about a sample size of 30.}\nWhat matters is the tradeoff between sample size and the extent of\nthe non-normality. If your data is less normal, you need a larger\nsample size to overcome it. Even a sample size of 500 might not be\nenough if your distribution is very skewed, or if you have extreme\noutliers. \n\nThe place $n=30$ comes from is back from the days when we only\never used printed tables. In most textbooks, if you printed the\n$t$-table on one page in a decent-sized font, you'd get to about\n29 df before running out of space. Then they would say \"$\\infty$\ndf\" and put the normal-distribution $z$ numbers in. If the df you\nneeded was bigger than what you had in the table, you used this\nlast line: that is, you called the sample \"large\". Try it in\nyour stats textbooks: I bet the df go up to 30, then you get a few\nmore, then the $z$ numbers. \n\nExtra 2: By now you are probably thinking that this is very\nsubjective, and so it is. What actually matters is the shape of\nthe thing called the *sampling distribution of the sample mean*. That is to say, what kind of sample means you might get\nin repeated samples from your population. The problem is that you\ndon't know what the population looks like.^[If you did, all\nyour problems would be over.] But we can fake it up, in a couple\nof ways: we can play what-if and pretend we know what the population\nlooks like (to get some understanding for \"populations like\nthat\"), or we can use a technique called the \"bootstrap\" that\nwill tell us what kind of sample means we might get from the\npopulation that *our* sample came from (this seems like magic\nand, indeed, is).\n\nThe moral of the story is that the central limit theorem is more\npowerful than you think.\n\nTo illustrate my first idea, let's pretend the population looks like this,\nwith a flat top:\n\n::: {.cell}\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/brunhopter-1.png){width=672}\n:::\n:::\n\nOnly values between 0 and 1 are possible, and each of those is equally\nlikely. Not very normal in shape. So let's take some random samples of\nsize *three*, not in any sense a large sample, from this \"uniform\"\npopulation, and see what kind of sample means we get. This technique\nis called **simulation**: rather than working out the answer by\nmath, we're letting the computer approximate the answer for us. Here's\none simulated sample:\n\n::: {.cell}\n\n```{.r .cell-code}\nu <- runif(3)\nu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9475841 0.1245953 0.2277288\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(u)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4333027\n```\n\n\n:::\n:::\n\nand here's the same thing 1000 times, including a histogram of the\nsample means:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(runif(3))) %>% \n  mutate(my_mean = mean(my_sample)) %>% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12)\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/toeber-1.png){width=672}\n:::\n:::\n\nThis is our computer-generated assessment of what the sampling\ndistribution of the sample mean looks like. Isn't this looking like a\nnormal distribution?\n\nLet's take a moment to realize what this is saying. If the population\nlooks like the flat-topped uniform distribution, the central limit\ntheorem kicks in for a sample of size *three*, and thus if your\npopulation looks like this, $t$ procedures will be perfectly good for\n$n=3$ or bigger, *even though the population isn't normal*.\n\nThus, when you're thinking about whether to use a $t$-test or\nsomething else (that we'll learn about later), the distribution shape\nmatters, *but so does the sample size*.\n\nI should say a little about my code. I'm not expecting you to figure\nout details now (we see the ideas properly in simulating power of\ntests), but in words, one line at a time:\n\n- generate 1000 (\"many\") samples each of 3 observations from a\nuniform distribution\n- for each sample, work out the mean of it\n- turn those sample means into a data frame with a column called `value`\n- make a histogram of those.\n\nNow, the central limit theorem doesn't always work as nicely as this,\nbut maybe a sample size of 30 is large enough to overcome the skewness\nthat we had:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(cholest, aes(x=control)) + geom_histogram(bins=6)\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/schauffhusen-1.png){width=672}\n:::\n:::\n\nThat brings us to my second idea above.\n\nThe sample that we had is in some sense an \"estimate of the\npopulation\". To think about the sampling distribution of the sample\nmean, we need more estimates of the population. How might we get\nthose? The curious answer is to *sample from the sample*. This is\nthe idea behind the *bootstrap*. (This is what Lecture 3c is about.) The name comes from the\nexpression \"pulling yourself up by your own bootstraps\", meaning\n\"to begin an enterprise or recover from a setback without any outside\nhelp\" (from [here](https://www.yourdictionary.com/pull-oneself-up-by-one-s-bootstraps)), \nsomething that should be difficult or impossible. How is it\npossible to understand a sampling distribution with only one sample?\n\nWe have to be a bit careful. Taking a sample from the sample would\ngive us the original sample back. So, instead, we sample \n*with replacement*, so that each bootstrap sample is different:\n\n::: {.cell}\n\n```{.r .cell-code}\nsort(cholest$control)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 160 162 164 166 170 176 178 178 182 182 182 182 182 184 186 188 196 198 198\n[20] 198 200 200 204 206 212 218 230 232 238 242\n```\n\n\n:::\n\n```{.r .cell-code}\nsort(sample(cholest$control, replace=TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 164 166 166 166 166 176 178 178 182 182 182 182 182 188 198 198 198 200 200\n[20] 200 200 204 206 206 218 218 230 232 232 242\n```\n\n\n:::\n:::\n\nA bootstrap sample contains repeats of the original data values, and\nmisses some of the others. Here, the original data had values 160 and 162 that are missing in the bootstrap sample; the original data had one value 166,  but the bootstrap sample has *four*!\nI sorted the data and the bootstrap sample\nto make this clearer; you will not need to sort. This is a perfectly good bootstrap sample:\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(cholest$control, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 242 232 198 160 242 182 182 182 198 162 212 198 242 204 242 242 170 198 182\n[20] 206 232 170 218 188 166 178 164 160 218 196\n```\n\n\n:::\n:::\n\n\nSo now we know what to do: take lots of bootstrap samples, work out\nthe mean of each, plot the means, and see how normal it looks. The\nonly new idea here is the sampling with replacement:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(cholest$control, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) %>% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12)\n```\n\n::: {.cell-output-display}\n![](one-sample-inference_files/figure-html/schuomarchers-1.png){width=672}\n:::\n:::\n\nThat looks pretty normal, not obviously skewed, and so the $t$\nprocedures we used will be reliable enough.\n\n\n$\\blacksquare$\n\n\n\n",
    "supporting": [
      "one-sample-inference_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}