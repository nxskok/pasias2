{
  "hash": "1852cab69c67c401f4021a369109d2d2",
  "result": {
    "markdown": "# K-means cluster analysis\n\nPackages for this chapter:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(ggbiplot)\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Clustering the Australian athletes\n\n\n Recall the Australian athlete data (that we've seen so many\ntimes before). This time, we'll do some K-means clustering, and then\nsee whether athletes of certain genders and certain sports tend to end\nup in the same cluster.\n\n\n\n(a) Read in the data from\n[link](http://ritsokiguess.site/datafiles/ais.txt), recalling\nthat the data values are separated by tabs. Display (some of) the\ndata set.\n\n\n(b) From your data frame, select only the columns that are numbers\n(or get rid of the ones that are text), and standardize all of the\ncolumns you have left. This is, done the best way, a slick piece of\ncode. Display what you get.\n\n\n(c) Make a data frame that contains the total within-cluster sum\nof squares from a K-means clustering for each number of clusters\nfrom 2 to 20.\n\n\n(d) Use the data frame you just created to make a scree plot. What\ndoes the scree plot tell you?\n\n\n(e) Using a sensible number of clusters as deduced from your scree\nplot, run a K-means cluster analysis. Don't forget the\n`nstart`! \n\n\n(f) Make a data frame consisting of the athletes' sport and\ngender, and which of your clusters they belong to, taking the\nappropriate things from the appropriate one of your data frames.\n\n\n(g) Using the data frame you created in the previous part, display\nall the athletes in some of your clusters. Do the athletes within a\ncluster appear to have anything in common? (If a cluster has more\nthan 10 athletes in it, make sure to look at them all.)\n\n\n(h) Add the cluster membership to the data frame you read in from\nthe file, and do a discriminant analysis treating the clusters as\nknown groups. You can display the output.\n\n\n(i) How many linear discriminants do you have? How many do you\nthink are important?\n\n\n(j) Which variables seem to be important in distinguishing the\nclusters? Look only at the linear discriminants that you judged to\nbe important.\n\n\n(k) Draw a biplot (which shows the first two LDs), drawing the\nclusters in different colours. Comment briefly on anything\nespecially consistent or inconsistent with what you've seen so far.\n\n\n\n\n\n##  Running, jumping, and throwing\n\n\n The [decathlon](https://en.wikipedia.org/wiki/Decathlon) is a men's^[Women  compete in a similar competition called the *heptathlon* with seven  events.] track-and-field competition in which competitors complete 10\nevents over two days as follows, requiring the skills shown:\n\n::: {.cell}\n::: {.cell-output-display}\n|Event        |Skills                                 |\n|:------------|:--------------------------------------|\n|100m         |Running, speed                         |\n|Long jump    |Jumping, speed                         |\n|Shot put     |Throwing, strength                     |\n|High jump    |Jumping, agility                       |\n|400m         |Running, speed                         |\n|110m hurdles |Running, jumping, speed                |\n|Discus       |Throwing, agility (and maybe strength) |\n|Pole vault   |Jumping, agility                       |\n|Javelin      |Throwing, agility                      |\n|1500m        |Running, endurance                     |\n:::\n:::\n\n\n\\begin{tabular}{ll}\nEvent & Skills \\\\\n\\hline\n100m & Running, speed\\\\\nLong jump & Jumping, speed \\\\\nShot put & Throwing, strength\\\\\nHigh jump & Jumping, agility\\\\ \n400m & Running, speed\\\\\n110m hurdles & Running, jumping, speed\\\\\nDiscus & Throwing, agility (and maybe strength)\\\\\nPole vault & Jumping, agility\\\\\nJavelin & Throwing, agility\\\\\n1500m & Running, endurance\\\\\n\\hline\n\\end{tabular}\n\n(note: in the pdf version, this table might appear twice.)\n\n\nThese are a mixture of running, jumping and throwing disciplines. The\nperformance (time, distance or height) achieved in each event is\nconverted to a number of points using [standard tables](https://en.wikipedia.org/wiki/Decathlon#Points_system).\nand the winner of the entire decathlon is the\ncompetitor with the largest total of points. The basic idea is that a \"good\" performance in an event is worth 1000 points, and the score decreases if the athlete takes more seconds (running) or achieves fewer metres (jumping/throwing).\nA good decathlete has to\nbe at least reasonably good at all the disciplines.\n\nFor the decathlon competition at the 2013 Track and Field World\nChampionship, a record was kept of each competitor's performance in\neach event (for the competitors that competed in all ten\nevents). These values are in\n[link](http://ritsokiguess.site/datafiles/dec2013.txt). \n\n\n\n(a) Read in the data and verify that you have the right number\nof variables. \n\n\n\n(b) Some of the performances are times in seconds, and some of\nthem are distances (or heights) in metres. Also, some of the columns\nare more variable than others. Produce a matrix of standardized\nperformances in each event, making sure not to try to standardize\nthe names!\n\n\n\n(c) We are going to make a scree plot to decide on the number\nof clusters our K-means clustering should use. Using a loop, or\notherwise,^[I grew up in the UK, and when I saw that in an    exam, it was code for \"the way you'd think is obvious but long, and    the otherwise-way is clever but short\". I think this is one of    those.] obtain the total within-cluster sum of squares for these\ndata for each number of clusters for 2 up to 20.\n\n\n\n(d) Using what you calculated in the previous part, draw a scree\nplot. How does your\nscree plot tell you that 5 is a possible number of clusters? Explain\nbriefly.\n\n\n\n\n(e) Run K-means with 5 clusters. Produce an output that shows\nwhich competitors are in which cluster.\n\n\n\n\n\n(f) Display the cluster means for all of the events. (This has\nalready been calculated; you just have to display it.) Find the\ncluster mean, looking at all of the events, that is farthest from\nzero, and see if you can describe the strengths and weaknesses of the\nathletes in that cluster (look at all the events for the cluster that\nhas that extreme mean). Bear in mind (i) that these are the original\nperformances standardized, and (ii) for a running event, a\n*smaller* value is better.\n\n\n\n\n\n\n##  Clustering the Swiss bills\n\n\n This question is about the Swiss bank counterfeit bills\nagain. This time we're going to ignore whether each bill is\ncounterfeit or not, and see what groups they break into. Then, at\nthe end, we'll see whether cluster analysis was able to pick out the\ncounterfeit ones or not.\n\n\n(a) Read the data in again (just like last time), and look at\nthe first few rows. This is just the same as before.\n \n\n(b) The variables in this data frame are on different\nscales. Standardize them so that they all have mean 0 and standard\ndeviation 1. (Don't try to standardize the `status` column!)\n \n\n(c) We are going to make a scree plot. First, calculate the total\nwithin-cluster SS for each number of clusters from 2 to 10.\n \n\n(d) <a name=\"part:howmany\">*</a> Make a scree plot (creating a data frame\nfirst if you need). How many clusters do you think you\nshould use?\n \n\n(e) Run K-means with the number of clusters that you found in\n(<a href=\"#part:howmany\">here</a>). How many bills are in each cluster?\n \n\n(f) Make a table showing cluster membership against actual\nstatus (counterfeit or genuine). Are the counterfeit bills mostly\nin certain clusters?\n \n\n\n\n\n\n##  Grouping similar cars\n\n\n The file\n[link](http://ritsokiguess.site/datafiles/car-cluster.csv) contains\ninformation on seven variables \nfor 32 different cars. The variables are:\n\n\n\n* `Carname`: name of the car (duh!)\n\n* `mpg`: gas consumption in miles per US gallon (higher means the car uses less gas)\n\n* `disp`: engine displacement (total volume of cylinders in engine): higher is more powerful\n\n* `hp`: engine horsepower (higher means a more powerful engine)\n\n* `drat`: rear axle ratio (higher means more powerful but worse gas mileage)\n\n* `wt`: car weight in US tons\n\n* `qsec`: time needed for the car to cover a quarter mile (lower means faster)\n\n\n\n\n(a) Read in the data and display its structure. Do you have the\nright number of cars and variables?\n\n\n(b) The variables are all measured on different scales. Use\n`scale` to produce a matrix of standardized ($z$-score) values\nfor the columns of your data that are numbers.\n\n\n\n\n(c) Run a K-means cluster analysis for these data, obtaining 3\nclusters, and display the results. Take whatever action you need to\nobtain the best (random) result from a number of runs.\n\n\n\n\n(d) Display the car names together with which cluster they are\nin. If you display them all at once, sort by cluster so that it's\neasier to see which clusters contain which cars. (You may have to make\na data frame first.)\n\n\n\n\n(e) I have no idea whether 3 is a sensible number of clusters. To\nfind out, we will draw a scree plot (in a moment). Write a function\nthat accepts the number of clusters and the (scaled) data,\nand returns the total within-cluster sum of squares.\n\n\n\n\n(f) Calculate the total within-group sum of squares for each\nnumber of clusters from 2 to 10, using the function you just wrote.\n\n\n\n\n(g) Make a scree plot, using the total within-cluster sums of\nsquares values that you calculated in the previous part. \n\n\n\n\n(h) What is a suitable number of clusters for K-means, based on\nyour scree plot?\n\n\n\n\n(i) Run a K-means analysis using the number of clusters suggested\nby your scree plot, and list the car names together with the clusters\nthey belong to, *sorted by cluster*.\n\n\n\n\n\n##  Rating beer\n\n\n Thirty-two students each rated 10 brands of beer:\n\n\n* Anchor Steam\n\n* Bass\n\n* Beck's\n\n* Corona\n\n* Gordon Biersch\n\n* Guinness\n\n* Heineken\n\n* Pete's Wicked Ale\n\n* Sam Adams\n\n* Sierra Nevada\n\nThe ratings are on a scale of 1 to 9, with a higher\nrating being better.\nThe data are in\n[link](http://ritsokiguess.site/datafiles/beer.txt).  I\nabbreviated the beer names for the data file. I hope you can figure\nout which is which.\n\n\n(a) Read in the data, and look at the first few rows.\n \n\n(b) The researcher who collected the data wants to see which\nbeers are rated similarly to which other beers. Try to create a\ndistance matrix from these data and explain why it didn't do what\nyou wanted. (Remember to get rid of the `student` column\nfirst.) \n \n\n(c) The R function `t()` *transposes* a matrix: that\nis, it interchanges rows and columns. Feed the transpose of your\nread-in beer ratings into `dist`. Does this now give\ndistances between beers?\n \n\n(d) Try to explain briefly why I used `as.dist` in the\nclass example (the languages one) but `dist` here. (Think\nabout the form of the input to each function.)\n \n\n(e) <a name=\"part:beer-dendro\">*</a> Obtain a clustering of the beers, using Ward's method. Show\nthe dendrogram.\n \n\n(f) What seems to be a sensible number of clusters? Which\nbeers are in which cluster?\n \n\n(g) Re-draw your dendrogram with your clusters indicated.\n \n\n(h) Obtain a K-means\nclustering with 2 clusters.^[If you haven't gotten to K-means clustering yet, leave this and save it for later.]\nNote that you will need to use the (transposed) \n*original  data*, not the distances. Use a suitably large value of\n`nstart`. (The data are ratings all on the same scale, so\nthere is no need for `scale` here. In case you were\nwondering.) \n \n\n(i) How many beers are in each cluster?\n \n\n(j) *Which* beers are in each cluster? You can do this\nsimply by obtaining the cluster memberships and using\n`sort` as in the last question, or you can do it as I did\nin class by obtaining the \nnames of the things to be clustered and picking out the ones of\nthem that are in cluster 1, 2, 3, \\ldots .)\n \n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Clustering the Australian athletes\n\n\n Recall the Australian athlete data (that we've seen so many\ntimes before). This time, we'll do some K-means clustering, and then\nsee whether athletes of certain genders and certain sports tend to end\nup in the same cluster.\n\n\n\n(a) Read in the data from\n[link](http://ritsokiguess.site/datafiles/ais.txt), recalling\nthat the data values are separated by tabs. Display (some of) the\ndata set.\n\nSolution\n\n\nSo, `read_tsv`. \n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/ais.txt\"\nathletes <- read_tsv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 202 Columns: 13\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nathletes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 202 x 13\n   Sex    Sport     RCC   WCC    Hc    Hg  Ferr   BMI   SSF `%Bfat`   LBM    Ht\n   <chr>  <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>\n 1 female Netball  4.56  13.3  42.2  13.6    20  19.2  49      11.3  53.1  177.\n 2 female Netball  4.15   6    38    12.7    59  21.2 110.     25.3  47.1  173.\n 3 female Netball  4.16   7.6  37.5  12.3    22  21.4  89      19.4  53.4  176 \n 4 female Netball  4.32   6.4  37.7  12.3    30  21.0  98.3    19.6  48.8  170.\n 5 female Netball  4.06   5.8  38.7  12.8    78  21.8 122.     23.1  56.0  183 \n 6 female Netball  4.12   6.1  36.6  11.8    21  21.4  90.4    16.9  56.4  178.\n 7 female Netball  4.17   5    37.4  12.7   109  21.5 107.     21.3  53.1  177.\n 8 female Netball  3.8    6.6  36.5  12.4   102  24.4 157.     26.6  54.4  174.\n 9 female Netball  3.96   5.5  36.3  12.4    71  22.6 101.     17.9  56.0  174.\n10 female Netball  4.44   9.7  41.4  14.1    64  22.8 126.     25.0  51.6  174.\n# i 192 more rows\n# i 1 more variable: Wt <dbl>\n```\n:::\n:::\n\n     \n$\\blacksquare$\n\n(b) From your data frame, select only the columns that are numbers\n(or get rid of the ones that are text), and standardize all of the\ncolumns you have left. This is, done the best way, a slick piece of\ncode. Display what you get.\n\nSolution\n\nThis first one is a bit *too* slick:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes %>% mutate(across(where(is.numeric), \\(x) scale(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 202 x 13\n   Sex    Sport   RCC[,1] WCC[,1] Hc[,1] Hg[,1] Ferr[,1] BMI[,1] SSF[,1]\n   <chr>  <chr>     <dbl>   <dbl>  <dbl>  <dbl>    <dbl>   <dbl>   <dbl>\n 1 female Netball  -0.346   3.44  -0.243 -0.709  -1.20   -1.33    -0.615\n 2 female Netball  -1.24   -0.616 -1.39  -1.37   -0.376  -0.631    1.26 \n 3 female Netball  -1.22    0.273 -1.53  -1.66   -1.16   -0.543    0.613\n 4 female Netball  -0.870  -0.394 -1.47  -1.66   -0.987  -0.672    0.899\n 5 female Netball  -1.44   -0.727 -1.20  -1.30    0.0237 -0.414    1.63 \n 6 female Netball  -1.31   -0.560 -1.77  -2.03   -1.18   -0.550    0.656\n 7 female Netball  -1.20   -1.17  -1.55  -1.37    0.676  -0.519    1.16 \n 8 female Netball  -2.01   -0.283 -1.80  -1.59    0.529   0.522    2.69 \n 9 female Netball  -1.66   -0.893 -1.85  -1.59   -0.124  -0.114    0.985\n10 female Netball  -0.608   1.44  -0.462 -0.342  -0.271  -0.0544   1.76 \n# i 192 more rows\n# i 4 more variables: `%Bfat` <dbl[,1]>, LBM <dbl[,1]>, Ht <dbl[,1]>,\n#   Wt <dbl[,1]>\n```\n:::\n:::\n\nIt standardizes all the columns that are numeric all right, but any other columns it finds it leaves as they are, while we want to get rid of them first. So do it in two steps: get the numeric columns, and standardize *all* of those:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes %>% select(where(is.numeric)) %>% \n  mutate(across(everything(), \\(x) scale(x))) -> athletes.s\n```\n:::\n\n\nThis, in fact:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes.s\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 202 x 11\n   RCC[,1] WCC[,1] Hc[,1] Hg[,1] Ferr[,1] BMI[,1] SSF[,1] `%Bfat`[,1] LBM[,1]\n     <dbl>   <dbl>  <dbl>  <dbl>    <dbl>   <dbl>   <dbl>       <dbl>   <dbl>\n 1  -0.346   3.44  -0.243 -0.709  -1.20   -1.33    -0.615      -0.358  -0.898\n 2  -1.24   -0.616 -1.39  -1.37   -0.376  -0.631    1.26        1.90   -1.36 \n 3  -1.22    0.273 -1.53  -1.66   -1.16   -0.543    0.613       0.950  -0.875\n 4  -0.870  -0.394 -1.47  -1.66   -0.987  -0.672    0.899       0.989  -1.23 \n 5  -1.44   -0.727 -1.20  -1.30    0.0237 -0.414    1.63        1.55   -0.675\n 6  -1.31   -0.560 -1.77  -2.03   -1.18   -0.550    0.656       0.542  -0.644\n 7  -1.20   -1.17  -1.55  -1.37    0.676  -0.519    1.16        1.26   -0.900\n 8  -2.01   -0.283 -1.80  -1.59    0.529   0.522    2.69        2.11   -0.801\n 9  -1.66   -0.893 -1.85  -1.59   -0.124  -0.114    0.985       0.714  -0.681\n10  -0.608   1.44  -0.462 -0.342  -0.271  -0.0544   1.76        1.85   -1.01 \n# i 192 more rows\n# i 2 more variables: Ht <dbl[,1]>, Wt <dbl[,1]>\n```\n:::\n:::\n\n\nThe columns might have weird names, possibly because `scale` expects\na matrix or data frame (to standardize each column), and here it's\ngetting the columns one at a time.\n\nElsewhere, I stuck `scale()` on the end, which produces a\n*matrix*, which I should then display the top of (it has 200-plus rows):\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes %>% select(where(is.numeric)) %>% scale() %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            RCC        WCC         Hc         Hg        Ferr        BMI\n[1,] -0.3463363  3.4385826 -0.2434034 -0.7092631 -1.19736325 -1.3254121\n[2,] -1.2415791 -0.6157363 -1.3900079 -1.3698371 -0.37633203 -0.6305634\n[3,] -1.2197439  0.2728816 -1.5265084 -1.6634256 -1.15525908 -0.5432708\n[4,] -0.8703809 -0.3935818 -1.4719082 -1.6634256 -0.98684242 -0.6724638\n[5,] -1.4380958 -0.7268135 -1.1989072 -1.2964400  0.02365754 -0.4140778\n[6,] -1.3070846 -0.5601977 -1.7722094 -2.0304111 -1.17631117 -0.5502542\n            SSF      %Bfat        LBM         Ht         Wt\n[1,] -0.6148189 -0.3582372 -0.8977457 -0.3394075 -1.0849225\n[2,]  1.2644802  1.8986922 -1.3606308 -0.7708629 -0.8623105\n[3,]  0.6134811  0.9503618 -0.8747927 -0.4215895 -0.6253364\n[4,]  0.8990609  0.9891351 -1.2313290 -1.0482270 -1.0274742\n[5,]  1.6298994  1.5513480 -0.6751017  0.2975028 -0.1513883\n[6,]  0.6564716  0.5416266 -0.6444978 -0.1955890 -0.5104399\n```\n:::\n:::\n\n\nThe first athlete has a `WCC` value that is *very* large compared\nto the others.\n\nExtra: for those keeping track, sometimes you need an `across` and sometimes you don't. The place where you need `across` is when you want to apply something to a bunch of columns all at once. `select` doesn't need it, but something like `mutate` or `summarize` does, because you are changing the values in or summarizing several columns all at once. \n\nOne more: if the columns you are acting on in `across` are selected using a select helper (or by naming them or in some other way that depends on their *names*), you put that directly inside `across` (as in `across(everything())` above), but if you are choosing the columns to act on by a *property* of them (eg. that they are numbers), you have a `where` inside the `across`, as in `across(where(is.numeric))`. You typically will be closing several brackets at the end. In R Studio, when you type a close-bracket, it briefly shows you the matching open-bracket so that you can keep track. \n\n$\\blacksquare$\n\n(c) Make a data frame that contains the total within-cluster sum\nof squares from a K-means clustering for each number of clusters\nfrom 2 to 20.\n\nSolution\n\n\nI'm going to attempt a slick way of doing this, and then I'll talk\nabout how I'd expect *you* to tackle this. First, though, I\nset the random number seed so that everything comes out the same\nevery time I run it:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(457299)\n```\n:::\n\n     \nHere we go:\n\n::: {.cell}\n\n```{.r .cell-code}\nwithinss <- tibble(clusters = 2:20) %>%\n  rowwise() %>% \n  mutate(wss = kmeans(athletes.s, clusters, nstart = 20)$tot.withinss)\nwithinss\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 19 x 2\n# Rowwise: \n   clusters   wss\n      <int> <dbl>\n 1        2 1426.\n 2        3 1201.\n 3        4 1043.\n 4        5  968.\n 5        6  901.\n 6        7  836.\n 7        8  782.\n 8        9  735.\n 9       10  688.\n10       11  654.\n11       12  631.\n12       13  605.\n13       14  583.\n14       15  562.\n15       16  551.\n16       17  532.\n17       18  513.\n18       19  502.\n19       20  487.\n```\n:::\n:::\n\n     \n\nA one-liner, kinda. Remember that `kmeans` expects a single number of clusters, a value like 5, rather than a collection of possible numbers of clusters in a vector, so to do each of them, we need to work rowwise (and do one row at a time).\n\nThe advantage to this is that it looks exactly like\nthe `kmeans` that you would write.\n\nAll right then, what is a better way to do this? First write\na function to take a number of clusters and a data frame and return\nthe total within-cluster sum of squares:\n\n::: {.cell}\n\n```{.r .cell-code}\ntwss <- function(i, x) {\n  ans <- kmeans(x, i, nstart = 20)\n  ans$tot.withinss\n}\n```\n:::\n\n \n\nand test it (against my answer above):\n\n::: {.cell}\n\n```{.r .cell-code}\ntwss(3, athletes.s)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1201.346\n```\n:::\n:::\n\n \n\nCheck (with a few extra decimals).\n\nThen calculate\nall the total within-cluster sum of squares values by making a little\ndata frame with all your numbers of clusters:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(clusters = 2:20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 19 x 1\n   clusters\n      <int>\n 1        2\n 2        3\n 3        4\n 4        5\n 5        6\n 6        7\n 7        8\n 8        9\n 9       10\n10       11\n11       12\n12       13\n13       14\n14       15\n15       16\n16       17\n17       18\n18       19\n19       20\n```\n:::\n:::\n\n \nand then make a pipeline and save it, using `rowwise` and your function:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(clusters = 2:20) %>%\n  rowwise() %>% \n  mutate(wss = twss(clusters, athletes.s)) -> withinss\nwithinss\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 19 x 2\n# Rowwise: \n   clusters   wss\n      <int> <dbl>\n 1        2 1426.\n 2        3 1201.\n 3        4 1043.\n 4        5  968.\n 5        6  901.\n 6        7  836.\n 7        8  777.\n 8        9  733.\n 9       10  696.\n10       11  658.\n11       12  626.\n12       13  608.\n13       14  585.\n14       15  569.\n15       16  551.\n16       17  527.\n17       18  511.\n18       19  504.\n19       20  489.\n```\n:::\n:::\n\nThis is better because the `mutate` line is simpler; you have off-loaded the details of the thinking to your function. Read this as \"for each number of clusters, work out the total within-cluster sum of squares for that number of clusters.\" The important thing here is what you are doing, not how you are doing it; if you care about the how-you-are-doing-it, go back and look at your function. Remember that business about how you can only keep track of seven things, plus or minus two, at once? When you write a function, you are saving some of the things you have to keep track of.\n\n$\\blacksquare$\n\n(d) Use the data frame you just created to make a scree plot. What\ndoes the scree plot tell you?\n\nSolution\n\n\n`ggscreeplot` is for principal components; this one you can\nplot directly, with the points joined by lines:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(withinss, aes(x = clusters, y = wss)) + geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/ais-km-12-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nOn this plot, you are looking for \"elbows\", but ones sufficiently\nfar down the mountain. For example, that's an elbow at 4 clusters, but\nit's still up the mountain, which means that the total within-cluster\nsum of squares is quite large and that the athletes within those 4\nclusters might be quite dissimilar from each other. I see an elbow at\n12 clusters and possibly others at 14, 16 and 19; these are nearer the bottom\nof the mountain, so that the athletes within a cluster will be quite\nsimilar to each other. With over 200 athletes, there's no problem\nhaving as many as 19 clusters, because that will still offer you some\ninsight. \n\nSo I'm thinking 12 clusters (because I want to have a fairly small\nnumber of clusters to interpret later).\n\nThe other thing I'm thinking is I could have put a bigger number of\nclusters on the scree plot. The `wss` axis should go all the\nway down to 0 for 202 clusters, with each athlete in one cluster. So\nyou could make the point that even 20 clusters is still a fair way up\nthe mountain.\n\n$\\blacksquare$\n\n(e) Using a sensible number of clusters as deduced from your scree\nplot, run a K-means cluster analysis. Don't forget the\n`nstart`! \n\nSolution\n\n\nThis:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes.km <- kmeans(athletes.s, 11, nstart = 20)\n```\n:::\n\n \n\nor for your chosen number of clusters. \n\nI don't think there's any great need to display the output, since the\nmost interesting thing is which athletes are in which cluster, which\nwe'll get to next.\n\n$\\blacksquare$\n\n(f) Make a data frame consisting of the athletes' sport and\ngender, and which of your clusters they belong to, taking the\nappropriate things from the appropriate one of your data frames.\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes2 <- tibble(\n  gender = athletes$Sex,\n  sport = athletes$Sport,\n  cluster = athletes.km$cluster\n)\nathletes2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 202 x 3\n   gender sport   cluster\n   <chr>  <chr>     <int>\n 1 female Netball       3\n 2 female Netball       9\n 3 female Netball       9\n 4 female Netball       9\n 5 female Netball       9\n 6 female Netball       9\n 7 female Netball       9\n 8 female Netball      11\n 9 female Netball       9\n10 female Netball      11\n# i 192 more rows\n```\n:::\n:::\n\n     \n$\\blacksquare$\n\n(g) Using the data frame you created in the previous part, display\nall the athletes in some of your clusters. Do the athletes within a\ncluster appear to have anything in common? (If a cluster has more\nthan 10 athletes in it, make sure to look at them all.)\n\nSolution\n\n\nLet's start with my cluster 1:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes2 %>% filter(cluster == 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 27 x 3\n   gender sport   cluster\n   <chr>  <chr>     <int>\n 1 female Netball       4\n 2 female BBall         4\n 3 female BBall         4\n 4 female BBall         4\n 5 female BBall         4\n 6 female Row           4\n 7 female Row           4\n 8 female Row           4\n 9 female Row           4\n10 female Row           4\n# i 17 more rows\n```\n:::\n:::\n\n     \n\nThese are almost all female, and if you remember back to our study of\nheight and weight for these data, these are the kinds of sport that\nare played by shorter, lighter people.\nCluster 2:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes2 %>% filter(cluster == 2) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 25 x 3\n   gender sport   cluster\n   <chr>  <chr>     <int>\n 1 female Netball       2\n 2 female Row           2\n 3 female Swim          2\n 4 female Swim          2\n 5 female Swim          2\n 6 female Swim          2\n 7 female Swim          2\n 8 female Field         2\n 9 female T400m         2\n10 female T400m         2\n# i 15 more rows\n```\n:::\n:::\n\n \n\nMales, apparently some of the more muscular ones, but not the field\nathletes. \n\n::: {.cell}\n\n```{.r .cell-code}\nathletes2 %>% filter(cluster == 3) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 11 x 3\n   gender sport   cluster\n   <chr>  <chr>     <int>\n 1 female Netball       3\n 2 female T400m         3\n 3 female TSprnt        3\n 4 female TSprnt        3\n 5 female T400m         3\n 6 female TSprnt        3\n 7 female TSprnt        3\n 8 female Tennis        3\n 9 female Tennis        3\n10 male   Row           3\n11 male   T400m         3\n```\n:::\n:::\n\n \n\nThis is an odd one, since there is one male rower (rowers tend to be\nfairly big) along with a bunch of females mostly from sports involving\nrunning. I have a feeling this rower is a \"cox\", whose job is\n*not* to row, but to sit in the boat and keep everybody in time\nby yelling out \"stroke\" in rhythm. Since the cox is not rowing, they\nneed to be light in weight.\n\nLet's investigate:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes %>%\n  select(gender = Sex, sport = Sport, ht = Ht, wt = Wt) %>%\n  mutate(cluster = athletes.km$cluster) -> athletes2a\nathletes2a %>% filter(sport == \"Row\", cluster == 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 5\n  gender sport    ht    wt cluster\n  <chr>  <chr> <dbl> <dbl>   <int>\n1 male   Row    165.  53.8       3\n```\n:::\n:::\n\n \n\nHow does this athlete compare to the other rowers?\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes2a %>%\n  filter(sport == \"Row\") %>%\n  select(ht, wt) %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       ht              wt       \n Min.   :156.0   Min.   :49.80  \n 1st Qu.:179.3   1st Qu.:72.90  \n Median :181.8   Median :78.70  \n Mean   :182.4   Mean   :78.54  \n 3rd Qu.:186.3   3rd Qu.:87.20  \n Max.   :198.0   Max.   :97.00  \n```\n:::\n:::\n\n \n\nThe rower that is in cluster 3 is almost the lightest, and also almost\nthe shortest, of all the rowers.\nCluster 4:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes2 %>% filter(cluster == 4) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 27 x 3\n   gender sport   cluster\n   <chr>  <chr>     <int>\n 1 female Netball       4\n 2 female BBall         4\n 3 female BBall         4\n 4 female BBall         4\n 5 female BBall         4\n 6 female Row           4\n 7 female Row           4\n 8 female Row           4\n 9 female Row           4\n10 female Row           4\n# i 17 more rows\n```\n:::\n:::\n\n \n\nMales, but possibly more muscular ones.\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes2 %>% filter(cluster == 5) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 17 x 3\n   gender sport  cluster\n   <chr>  <chr>    <int>\n 1 male   Swim         5\n 2 male   Row          5\n 3 male   Row          5\n 4 male   Row          5\n 5 male   Row          5\n 6 male   Row          5\n 7 male   Field        5\n 8 male   TSprnt       5\n 9 male   Field        5\n10 male   WPolo        5\n11 male   WPolo        5\n12 male   WPolo        5\n13 male   WPolo        5\n14 male   WPolo        5\n15 male   WPolo        5\n16 male   WPolo        5\n17 male   WPolo        5\n```\n:::\n:::\n\n \n\nMore males, from similar sports. I wonder what makes these last two\nclusters different?\n\nOne more:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes2 %>% filter(cluster == 6) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 3\n   gender sport cluster\n   <chr>  <chr>   <int>\n 1 male   Swim        6\n 2 male   Swim        6\n 3 male   Swim        6\n 4 male   Swim        6\n 5 male   BBall       6\n 6 male   BBall       6\n 7 male   BBall       6\n 8 male   BBall       6\n 9 male   T400m       6\n10 male   T400m       6\n# i 22 more rows\n```\n:::\n:::\n\n \n\nThese are three of our \"big guys\", by the looks of it.\n\n$\\blacksquare$\n\n(h) Add the cluster membership to the data frame you read in from\nthe file, and do a discriminant analysis treating the clusters as\nknown groups. You can display the output.\n\nSolution\n\n\n`MASS` is already loaded (for me), so:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes.3 <- athletes %>%\n  mutate(cluster = athletes.km$cluster) %>%\n  lda(cluster ~ RCC + WCC + Hc + Hg + Ferr + BMI + SSF + `%Bfat` + LBM + Ht + Wt, data = .)\n```\n:::\n\n     \n\nWe can display all the output now. The\nproblem here, with 12 groups and 11 variables, is that there is rather\na lot of it:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes.3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nlda(cluster ~ RCC + WCC + Hc + Hg + Ferr + BMI + SSF + `%Bfat` + \n    LBM + Ht + Wt, data = .)\n\nPrior probabilities of groups:\n         1          2          3          4          5          6          7 \n0.01980198 0.12376238 0.05445545 0.13366337 0.08415842 0.15841584 0.04950495 \n         8          9         10         11 \n0.04455446 0.14356436 0.13366337 0.05445545 \n\nGroup means:\n        RCC      WCC       Hc       Hg      Ferr      BMI       SSF   `%Bfat`\n1  6.000000 8.150000 52.37500 17.87500  52.50000 23.91750  45.27500  8.655000\n2  4.292800 6.390000 39.96000 13.52400  70.08000 19.71680  52.53600 11.629600\n3  4.963636 8.054545 44.40000 14.63636  54.18182 19.73091  48.29091 10.690909\n4  4.592963 7.292593 42.65926 14.35926  46.03704 22.74333  86.31111 18.587778\n5  4.960000 9.729412 45.55294 15.51176 119.29412 25.20706  62.37059 11.151176\n6  4.962500 6.440625 45.20312 15.39375  63.15625 22.21344  39.48438  7.270625\n7  5.182000 7.210000 46.44000 15.93000 200.30000 22.96800  47.22000  8.737000\n8  5.077778 7.255556 46.32222 16.03333 136.88889 31.03222  94.40000 16.624444\n9  4.167241 5.993103 38.24828 12.68966  53.48276 22.05897  96.97241 19.652069\n10 4.964444 6.607407 44.95926 15.21481  78.62963 24.22037  54.69259  9.559630\n11 4.336364 8.818182 39.13636 13.21818  70.00000 25.03727 150.16364 26.948182\n        LBM       Ht        Wt\n1  71.00000 180.5250  77.85000\n2  47.89800 165.5560  54.21200\n3  52.16818 172.0909  58.42727\n4  58.60296 178.0926  72.02963\n5  79.23529 188.2059  89.25588\n6  68.31250 182.1719  73.64844\n7  68.60000 180.8200  75.27000\n8  85.20444 181.1667 102.02222\n9  56.45069 178.5828  70.30862\n10 81.96296 193.4630  90.67407\n11 57.36364 177.1273  78.66364\n\nCoefficients of linear discriminants:\n                LD1         LD2           LD3         LD4          LD5\nRCC     -1.28721668 -0.12082768 -0.1800745307 -1.32581822 -1.297736838\nWCC     -0.17572030  0.04445962  0.1815301070 -0.06148203  0.255180891\nHc      -0.06887672 -0.01316703  0.0006390582 -0.06578884  0.157668660\nHg      -0.22783310 -0.23151901  0.1206000078 -0.40505906  0.064329099\nFerr    -0.01265103  0.00215038  0.0204843448  0.00902723 -0.017202106\nBMI     -0.03888179  0.60898248 -0.7520786724 -2.05384021 -1.888427011\nSSF     -0.02326379  0.01808028  0.0068966220  0.05181218 -0.022649744\n`%Bfat`  0.33439665  0.31999943 -0.5379290003 -0.05571142 -0.008652337\nLBM      0.02714856  0.27988180 -0.7311870862  0.41363393 -0.137853460\nHt      -0.01345762  0.12483039 -0.2835260888 -0.48139186 -0.542575152\nWt      -0.09727561 -0.30961926  0.8755149125  0.28575160  0.770957102\n                 LD6         LD7          LD8           LD9         LD10\nRCC     -1.355071630 -0.68078007 -3.768561032 -1.5595553389  2.018076802\nWCC     -0.403452734  0.24436067  0.273515110 -0.1562834202  0.105214336\nHc       0.056928597  0.14865448 -0.020949089 -0.3197302898 -0.348771286\nHg       0.194535578  0.04096553  0.865388620  1.9932096086  0.167943193\nFerr     0.003547754  0.01070359 -0.002033483 -0.0006917844 -0.002233497\nBMI      0.357652569 -0.84389019  1.406309919 -0.9413566093  0.548574435\nSSF     -0.106308127 -0.08997047  0.002423251  0.0450725511  0.012458031\n`%Bfat`  0.372222136  0.97871158 -0.137349422  0.2769699401  0.752495767\nLBM     -0.170985948  0.57183247 -0.040113894  0.5003467041  1.165555975\nHt       0.008320643 -0.12624874  0.344376866 -0.1813212974  0.020106963\nWt       0.138538684 -0.27999325 -0.388631504 -0.2200445172 -1.130827203\n\nProportion of trace:\n   LD1    LD2    LD3    LD4    LD5    LD6    LD7    LD8    LD9   LD10 \n0.5062 0.2313 0.0903 0.0717 0.0307 0.0291 0.0265 0.0089 0.0044 0.0010 \n```\n:::\n:::\n\n \n$\\blacksquare$\n\n(i) How many linear discriminants do you have? How many do you\nthink are important?\n\nSolution\n\n\nProportion of trace, at the bottom of the output.\n\nIt's hard to draw the line here. The first two, or maybe the first\nseven, or something like that. Your call.\n\n$\\blacksquare$\n\n(j) Which variables seem to be important in distinguishing the\nclusters? Look only at the linear discriminants that you judged to\nbe important.\n\nSolution\n\n\nLook at the coefficients of linear discriminants.\nThis is rather large, since I had 12 clusters, and thus there are\n11 `LD`s.\n\nIf we go back to my thought of only using two linear discriminants:\nLD1 is mostly `RCC` positively and `BMI` negatively, in\nthat an athlete with large `RCC` and small `BMI` will\ntend to score high (positive) on LD1. `BMI` is the familiar\nbody fat index. LD2 depends on `RCC` again, but this time\nnegatively, and maybe percent body fat and `LBM`. And so on, if\nyou went on.\n\nIt may be that `RCC` is just very variable anyway, since it\nseems to appear just about everywhere.\n\nExtra: we can also look at the means on each variable by cluster,\nwhich is part of the output, in \"Group Means\".\nPerhaps the easiest thing to eyeball here is the cluster in which a\nvariable is noticeably biggest (or possibly smallest). For example,\n`WCC` is highest in cluster 4, and while Ferritin is high\nthere, it is higher still in cluster 5. `BMI` is highest in\ncluster 6 and lowest in clusters 1 and 3. Height is smallest in\ncluster 1, with weight being smallest there as well, and weight is\nmuch the biggest in cluster 6. \n\n$\\blacksquare$\n\n(k) Draw a biplot (which shows the first two LDs), drawing the\nclusters in different colours. Comment briefly on anything\nespecially consistent or inconsistent with what you've seen so far.\n\nSolution\n\n\nThe thing to get the colours is to feed a `groups` into\n`ggbiplot`. I suspect I need the `factor` in there\nbecause the clusters are numbers and I want them treated as\ncategorical (the numbers are labels). Also, note that we will have\na lot of colours here, so I am trying to make them more\ndistinguishable using `scale_colour_brewer` from the\n`RColorBrewer` package (loaded at the beginning):\n\n::: {.cell}\n\n```{.r .cell-code}\nggbiplot(athletes.3, groups = factor(athletes2$cluster)) +\n  scale_colour_brewer(palette = \"Set3\")\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/ais-km-25-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nWhat the biplot shows, that we haven't seen any hint of so far, is\nthat the clusters are pretty well separated on LD1 and LD2: there is\nnot a great deal of overlap. \n\nAnyway, low LD1 means high on BMI and low on RCC, as we saw\nbefore. The arrow for RCC points down as well as right, so it's part\nof LD2 as well. There isn't much else that points up or down, but\npercent body fat and LBM do as much as anything. This is all pretty\nmuch what we saw before.\n\nAs to where the clusters fall on the picture:\n\n\n\n* Cluster 1 in light blue was \"small and light\": small BMI, so\nought to be on the right. This cluster's RCC was also small, which\non balance puts them on the left, but then they should be *top*\nleft because RCC points down. I dunno.\n\n* Cluster 2 in dark blue was \"more muscular males\", mid-right,\nso above average on LD1 but about average on LD2.\n\n* Cluster 3, light green, was \"running females\" (mostly), lower\nleft, so below average on both LD1 and LD2.\n\n* Cluster 4, dark green, \"more muscular males\" again. There is a\nlot of overlap with cluster 2.\n\n* Cluster 5, pink, was \"yet more males\".  Mostly above average on\nLD1 and below average on LD2. The latter was what distinguished\nthese from clusters 4 and 2.\n\n* Cluster 6, red, was \"big guys\". The biggest on LD1 and almost\nthe biggest on LD2.\n\n\nThere is something a bit confusing in LD1, which contrasts RCC and\nBMI. You would expect, therefore, RCC and BMI to be negatively\ncorrelated, but if you look at the cluster means, that isn't really\nthe story: for example, cluster 1 has almost the lowest mean on both\nvariables, and the highest RCC, in cluster 11, goes with a middling\nBMI. \n\nI like these colours much better than the default ones. Much easier to\ntell apart.\nIn any case, RCC and BMI seem to be important, so let's plot them\nagainst each other, coloured by cluster:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes %>%\n  mutate(cluster = factor(athletes2$cluster)) %>%\n  ggplot(aes(x = RCC, y = BMI, colour = cluster)) +\n  geom_point() + scale_colour_brewer(palette = \"Paired\")\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/ais-km-26-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nI decided to create a column called `cluster` in the data\nframe, so that the legend would have a nice clear title. (If you do\nthe `factor(athletes2$cluster)` in the `ggplot`, that\nis what will appear as the legend title.)\n\nThere seems to be very little relationship here, in terms of an\noverall trend on the plot. But at least these two variables do\n*something* to distinguish the clusters. It's not as clear as\nusing LD1 and LD2 (as it won't be, since they're designed to be the\nbest at separating the groups), but you can see that the clusters are\nat least somewhat distinct.\n\nThe \"paired\" part of the colour palette indicates that successive\ncolours come in pairs: light and dark of blue, green, red, orange,\npurple and brown (if you think of yellow as being \"light brown\" or\nbrown as being \"dark yellow\", like bananas).\n\nA good resource for RColorBrewer is\n[link](https://moderndata.plot.ly/create-colorful-graphs-in-r-with-rcolorbrewer-and-plotly/). The\n\"qualitative palettes\" shown there are for distinguishing groups\n(what we want here); the sequential palettes are for distinguishing\nvalues on a continuous scale, and the diverging palettes are for\ndrawing attention to high and low.\n\n$\\blacksquare$\n\n\n\n\n##  Running, jumping, and throwing\n\n\n The [decathlon](https://en.wikipedia.org/wiki/Decathlon) is a men's^[Women  compete in a similar competition called the *heptathlon* with seven  events.] track-and-field competition in which competitors complete 10\nevents over two days as follows, requiring the skills shown:\n\n::: {.cell}\n::: {.cell-output-display}\n|Event        |Skills                                 |\n|:------------|:--------------------------------------|\n|100m         |Running, speed                         |\n|Long jump    |Jumping, speed                         |\n|Shot put     |Throwing, strength                     |\n|High jump    |Jumping, agility                       |\n|400m         |Running, speed                         |\n|110m hurdles |Running, jumping, speed                |\n|Discus       |Throwing, agility (and maybe strength) |\n|Pole vault   |Jumping, agility                       |\n|Javelin      |Throwing, agility                      |\n|1500m        |Running, endurance                     |\n:::\n:::\n\n\n\\begin{tabular}{ll}\nEvent & Skills \\\\\n\\hline\n100m & Running, speed\\\\\nLong jump & Jumping, speed \\\\\nShot put & Throwing, strength\\\\\nHigh jump & Jumping, agility\\\\ \n400m & Running, speed\\\\\n110m hurdles & Running, jumping, speed\\\\\nDiscus & Throwing, agility (and maybe strength)\\\\\nPole vault & Jumping, agility\\\\\nJavelin & Throwing, agility\\\\\n1500m & Running, endurance\\\\\n\\hline\n\\end{tabular}\n\n(note: in the pdf version, this table might appear twice.)\n\n\nThese are a mixture of running, jumping and throwing disciplines. The\nperformance (time, distance or height) achieved in each event is\nconverted to a number of points using [standard tables](https://en.wikipedia.org/wiki/Decathlon#Points_system).\nand the winner of the entire decathlon is the\ncompetitor with the largest total of points. The basic idea is that a \"good\" performance in an event is worth 1000 points, and the score decreases if the athlete takes more seconds (running) or achieves fewer metres (jumping/throwing).\nA good decathlete has to\nbe at least reasonably good at all the disciplines.\n\nFor the decathlon competition at the 2013 Track and Field World\nChampionship, a record was kept of each competitor's performance in\neach event (for the competitors that competed in all ten\nevents). These values are in\n[link](http://ritsokiguess.site/datafiles/dec2013.txt). \n\n\n\n(a) Read in the data and verify that you have the right number\nof variables. \n\n\nSolution\n\n\nChecking the file, this is delimited by single spaces. You might\nbe concerned by the quotes; we'll read them in and see what\nhappens to them.\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/dec2013.txt\"\ndecathlon0 <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 24 Columns: 11\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr  (1): name\ndbl (10): x100m, long.jump, shot.put, high.jump, x400m, x110mh, discus, pole...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndecathlon0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 x 11\n   name        x100m long.jump shot.put high.jump x400m x110mh discus pole.vault\n   <chr>       <dbl>     <dbl>    <dbl>     <dbl> <dbl>  <dbl>  <dbl>      <dbl>\n 1 Ashton Eat~  10.4      7.73     14.4      1.93  46.0   13.7   45          5.2\n 2 Damian War~  10.4      7.39     14.2      2.05  48.4   14.0   44.1        4.8\n 3 Rico Freim~  10.6      7.22     14.8      1.99  48.0   13.9   48.7        4.9\n 4 Mihail Dud~  10.7      7.51     13.4      1.96  47.7   14.6   44.1        4.9\n 5 Michael Sc~  10.7      7.85     14.6      1.99  47.7   14.3   46.4        5  \n 6 Carlos Chi~  10.8      7.54     14.5      1.96  48.8   14.0   45.8        5.1\n 7 Gunnar Nix~  10.8      7.8      14.7      2.14  48.6   14.6   42.4        4.6\n 8 Eelco Sint~  10.8      7.65     14.1      2.02  48.2   14.2   39.2        5.3\n 9 Pascal Beh~  11.0      7.19     15.9      1.99  48.4   14.5   45.7        4.7\n10 Willem Coe~  11.0      7.44     13.9      2.05  48.3   14.3   43.2        4.5\n# i 14 more rows\n# i 2 more variables: javelin <dbl>, x1500m <dbl>\n```\n:::\n:::\n\n     \n\nThe names got shortened for display, but the quotes seem to have\nproperly disappeared.\n\nNote that the columns that would otherwise start with digits have\n`x` on the front of their names, so as to guarantee that the\ncolumn names are legal variable names (and thus won't require any\nspecial treatment later).\n    \n$\\blacksquare$\n\n(b) Some of the performances are times in seconds, and some of\nthem are distances (or heights) in metres. Also, some of the columns\nare more variable than others. Produce a matrix of standardized\nperformances in each event, making sure not to try to standardize\nthe names!\n\n\nSolution\n\n\n`scale` is what I am trying to hint towards. Leave off the\nfirst column. I would rather specify this by name than by\nnumber. (People have an annoying habit of changing the order of\ncolumns, but the column *name* is more work to change and\nthus it is less likely that it will change.)\n\n::: {.cell}\n\n:::\n\n     \n::: {.cell}\n\n```{.r .cell-code}\ndecathlon0 %>%\n  select(-name) %>%\n  scale() -> decathlon\nround(decathlon, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      x100m long.jump shot.put high.jump x400m x110mh discus pole.vault javelin x1500m\n [1,] -2.21      1.24     0.29     -0.95 -2.43  -1.77   0.27       1.10    0.55  -0.49\n [2,] -1.92      0.16     0.03      0.74 -0.46  -1.23  -0.06      -0.39    0.52  -0.46\n [3,] -1.33     -0.38     0.96     -0.11 -0.75  -1.37   1.71      -0.02   -1.17   0.63\n [4,] -1.08      0.54    -1.24     -0.53 -1.02   0.17  -0.09      -0.02   -0.60  -0.93\n [5,] -0.87      1.62     0.57     -0.11 -1.08  -0.50   0.82       0.36    0.72  -1.10\n [6,] -0.69      0.64     0.46     -0.53 -0.13  -1.03   0.59       0.73   -0.42   0.37\n [7,] -0.48      1.46     0.77      2.01 -0.33   0.13  -0.73      -1.14   -0.82   0.35\n [8,] -0.45      0.99    -0.21      0.32 -0.59  -0.74  -1.95       1.48   -1.06  -1.20\n [9,] -0.10     -0.47     2.68     -0.11 -0.46  -0.12   0.53      -0.76    1.00   0.54\n[10,] -0.10      0.32    -0.54      0.74 -0.53  -0.47  -0.40      -1.51    1.45  -1.21\n[11,] -0.02      0.03    -0.54      0.74 -0.47  -0.39  -0.09       1.85   -0.52   0.50\n[12,] -0.02      0.26    -0.10     -0.53 -0.13   0.69   1.42      -1.14   -2.26   0.06\n[13,]  0.01     -0.12    -0.02     -1.37  1.80   1.60   0.37      -1.51    1.46   1.82\n[14,]  0.33     -0.03    -0.02     -1.37 -0.62   0.24   0.81      -0.02    1.30  -1.31\n[15,]  0.40      0.95    -1.04      0.74  0.03   0.33  -1.20       0.73    0.65   0.64\n[16,]  0.47     -0.79     0.36     -0.11  0.04  -0.68  -0.09       0.36   -0.05  -0.05\n[17,]  0.57     -0.19    -0.60      0.32  1.07   1.51  -0.69      -1.51   -0.95   0.72\n[18,]  0.61     -2.09    -1.63     -1.37 -0.24  -0.32  -2.39       0.73    0.46   0.36\n[19,]  0.75      0.16     1.03      1.59  0.57  -0.16   0.70       0.73   -0.42   0.88\n[20,]  0.82     -0.25    -0.86      1.59  1.36   1.74  -0.94      -0.02   -0.49   2.07\n[21,]  0.89      0.51    -0.73      0.74  0.47  -0.68   0.41       1.10    0.80  -1.14\n[22,]  1.24     -0.69     1.07     -0.11  0.73   0.06   1.26       0.36    0.16  -0.02\n[23,]  1.56     -2.28     0.98     -1.80  1.32   1.18  -0.69      -1.51   -1.44  -1.83\n[24,]  1.63     -1.58    -1.69     -0.53  1.85   1.80   0.39      -0.02    1.11   0.78\nattr(,\"scaled:center\")\n     x100m  long.jump   shot.put  high.jump      x400m     x110mh     discus pole.vault \n 10.977083   7.339167  14.209583   1.997500  48.960000  14.512500  44.288333   4.904167 \n   javelin     x1500m \n 62.069583 273.306667 \nattr(,\"scaled:scale\")\n     x100m  long.jump   shot.put  high.jump      x400m     x110mh     discus pole.vault \n0.28433720 0.31549708 0.61480629 0.07091023 1.20878667 0.44795429 2.60828224 0.26780779 \n   javelin     x1500m \n5.01529875 7.22352899 \n```\n:::\n:::\n\n     \nI think the matrix of standardized values is small enough to look at\nall  of, particularly if I round off the values to a small number of\ndecimals. (Note that the means and SDs\nappear at the bottom as \"attributes\".)\n    \n\n::: {.cell}\n\n:::\n\n$\\blacksquare$\n\n(c) We are going to make a scree plot to decide on the number\nof clusters our K-means clustering should use. Using a loop, or\notherwise,^[I grew up in the UK, and when I saw that in an    exam, it was code for \"the way you'd think is obvious but long, and    the otherwise-way is clever but short\". I think this is one of    those.] obtain the total within-cluster sum of squares for these\ndata for each number of clusters for 2 up to 20.\n\n\nSolution\n\n\nHaving kind of given the game away in the footnote, I guess I now\nhave to keep up my end of the deal and show you the obvious way\nand the clever way.\nThe obvious way is to do a Python-style loop, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nmaxclust\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 20\n```\n:::\n\n```{.r .cell-code}\nw <- numeric(0)\nfor (i in 2:maxclust) {\n  sol <- kmeans(decathlon, i, nstart = 20)\n  w[i] <- sol$tot.withinss\n}\nw\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]        NA 175.03246 151.08750 131.30247 113.59681 100.26941  89.51098  78.89089\n [9]  68.99662  60.77665  54.29991  47.64227  41.55046  35.39181  29.52008  25.05344\n[17]  21.02841  17.28444  13.80627  10.44197\n```\n:::\n:::\n\n\nI defined `maxclust` earlier, surreptitiously. (Actually, what\nhappened was that I changed my mind about how many clusters I wanted\nyou to go up to, so that instead of hard-coding the maximum number of\nclusters, I decided to put it in a variable so that I only had to\nchange it once if I changed my mind again.)\n\nI decided to split the stuff within the loop into two lines, first\ngetting the $i$-cluster solution, and then pulling out the total\nwithin-cluster sum of squares from it and saving it in the right place\nin `w`. You can do it in one step or two; I don't mind.\n\nThe first value in `w` is missing, because we didn't calculate\na value for 1 cluster (so that this `w` has 20 values, one of\nwhich is missing).\n\nNot that there's anything wrong with this,^[I have to sneak a  Seinfeld quote in there somewhere.] and if it works, it's good, but the\nTrue R Way^[Like Buddhism. I keep feeling that R should have  something called the Eight Noble Truths or similar. See the Extra at the end of this part.] is not to use a\nloop, but get the whole thing in one shot. \nThe first stage is to figure out what you want to do for some number of clusters. In this case, it's something like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nkmeans(decathlon, 3, nstart = 20)$tot.withinss\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 151.0875\n```\n:::\n:::\n\n \n\nThere's nothing special about 3; any number will do. \n\nThe second stage is to run this for each desired number of\nclusters, without using a loop. \nThere are two parts to this, in my favoured way of doing it. First, write a function that will get the total within-group sum of squares for any K-means analysis for any number of clusters (input) for any dataframe (also input):\n\n::: {.cell}\n\n```{.r .cell-code}\ntwss <- function(i, d) {\n  ans <- kmeans(d, i, nstart = 20)\n  ans$tot.withinss\n}\n```\n:::\n\nThe value of doing it this way is that you only ever have to write this function once, and you can use it for any K-means analysis you ever do afterwards. Or, copy this one and use it yourself.\n\nLet's make sure it works:\n\n::: {.cell}\n\n```{.r .cell-code}\ntwss(3, decathlon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 151.0875\n```\n:::\n:::\n\nCheck. \n\nSecond, use `rowwise` to work out the total within-group sum of squares for a variety of numbers of clusters. What you use depends on how much data you have, and therefore how many clusters you think it would be able to support (a smallish fraction of the total number of observations). I went from 2 to 20 before, so I'll do that again:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(clusters = 2:20) %>% \n  rowwise() %>% \n  mutate(wss = twss(clusters, decathlon)) -> wsss\nwsss\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 19 x 2\n# Rowwise: \n   clusters   wss\n      <int> <dbl>\n 1        2 175. \n 2        3 151. \n 3        4 131. \n 4        5 114. \n 5        6 100. \n 6        7  89.6\n 7        8  78.8\n 8        9  69.0\n 9       10  60.8\n10       11  54.1\n11       12  47.6\n12       13  41.4\n13       14  35.4\n14       15  30.9\n15       16  25.1\n16       17  21.0\n17       18  17.3\n18       19  13.9\n19       20  10.4\n```\n:::\n:::\n\nThis got a name that was `wss` with an extra `s`. Sometimes my imagination runs out. \n\nThere was (still is) also a function `sapply` that does the\nsame thing.\nI\nlearned `sapply` and friends a long time ago, and now, with the\narrival of `rowwise`, I think I need to unlearn them.^[I wrote that a few years ago, and you may be pleased to learn that I have indeed completely forgotten about `apply`, `sapply`, `lapply` and all the others. I remember struggling through them when I first learned R, but you are in the happy position of never having to worry about them.]\n\nExtra: I made a post on Twitter, [link](https://twitter.com/KenButler12/status/1100133496637542401). \nTo which Malcolm Barrett replied with this: [link](https://twitter.com/malco_barrett/status/1100141130186780672) \nand this: [link](https://twitter.com/malco_barrett/status/1100140736945647616). \nSo now you know all about the Four Noble R Truths.\n\n    \n$\\blacksquare$\n\n(d) Using what you calculated in the previous part, draw a scree\nplot. How does your\nscree plot tell you that 5 is a possible number of clusters? Explain\nbriefly.\n\n\n\nSolution\n\n\nThis requires a teeny bit of care. If you went the loop way, what I\ncalled `w` has a missing value first (unless you were\nespecially careful), so you have to plot it against *1* through 20:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(clusters = 1:maxclust, wss = w) %>%\n  ggplot(aes(x = clusters, y = wss)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n```\n:::\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/freddo-1.pdf){fig-pos='H'}\n:::\n:::\n\n   \n\nThe warning message is to say that you don't have a total\nwithin-cluster sum of squares for 1 cluster, which you knew already.\n\nOr you can save the data frame first and then feed it into\n`ggplot`. \n\nIf you went the `rowwise` way, you will have the `wss`\nvalues for 2 through 20 clusters already in a data\nframe, so it is a fair bit simpler:\n\n::: {.cell}\n\n```{.r .cell-code}\nwsss %>%\n  ggplot(aes(x = clusters, y = wss)) +\n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/bilharzia-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThere is, I suppose, the tiniest elbow at 5 clusters. It's not very\nclear, though. I would have liked it to be clearer.\n  \n$\\blacksquare$\n\n(e) Run K-means with 5 clusters. Produce an output that shows\nwhich competitors are in which cluster.\n\n\n\nSolution\n\n\nIn your Quarto document, you might like to start with this:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(457299)\n```\n:::\n\n \n\nor some other random number seed of your choice. Using\n`nstart=20` or similar will give you the same *clustering*,\nbut which cluster is cluster 1 might vary between runs. So if you talk\nabout cluster 1 (below), and re-render the document, you might otherwise\nfind that cluster 1 has changed identity since the last time you\nrender it. (I just remembered that for these solutions.)\n\nRunning the `kmeans` itself is a piece of cake, since you have\ndone it a bunch of times already (in your loop or `rowwise`):\n\n::: {.cell}\n\n```{.r .cell-code}\ndecathlon.1 <- kmeans(decathlon, 5, nstart = 20)\ndecathlon.1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK-means clustering with 5 clusters of sizes 4, 8, 5, 6, 1\n\nCluster means:\n        x100m   long.jump   shot.put     high.jump      x400m     x110mh     discus\n1  0.75760985 -0.53619092 -0.7922550 -1.554312e-15  1.5180512  1.6631161 -0.2159787\n2 -0.02051555  0.02245134  0.9034011  2.644188e-01 -0.0589434 -0.3097414  0.6739749\n3  0.28457995  0.07871177 -0.8288519  2.326886e-01 -0.1588370 -0.3582955 -1.0406594\n4 -0.97448850  0.64184430 -0.1484207 -2.467909e-01 -1.0216857 -0.5934385  0.2274805\n5  1.55771620 -2.27947172  0.9765949 -1.798048e+00  1.3236413  1.1775755 -0.6894704\n   pole.vault     javelin     x1500m\n1 -0.76236268  0.28321676  1.3477946\n2 -0.10890895 -0.49565010  0.3441300\n3  1.17932839  0.06548297 -0.1670467\n4 -0.07779211  0.65707285 -0.9136808\n5 -1.50916693 -1.43751822 -1.8269002\n\nClustering vector:\n [1] 4 4 2 4 4 2 2 3 2 4 3 2 1 4 3 2 1 3 2 1 3 2 5 1\n\nWithin cluster sum of squares by cluster:\n[1] 18.86978 41.40072 26.24500 27.08131  0.00000\n (between_SS / total_SS =  50.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n:::\n:::\n\n   \n\nI displayed the result, so that I would know which of the things I\nneeded later. The `Available components` at the bottom is a big\nhint with this.\n\nTo display who is in which cluster, it's easiest to\nmake a data frame of names and clusters and sort it:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(name = decathlon0$name, cluster = decathlon.1$cluster) %>%\n  arrange(cluster) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 x 2\n   name               cluster\n   <chr>                <int>\n 1 Sergey Sviridov          1\n 2 Brent Newdick            1\n 3 Maicel Uibo              1\n 4 Keisuke Ushiro           1\n 5 Rico Freimuth            2\n 6 Carlos Chinin            2\n 7 Gunnar Nixon             2\n 8 Pascal Behrenbruch       2\n 9 Eduard Mikhan            2\n10 Artem Lukyanenko         2\n# i 14 more rows\n```\n:::\n:::\n\n \n$\\blacksquare$\n\n\n(f) Display the cluster means for all of the events. (This has\nalready been calculated; you just have to display it.) Find the\ncluster mean, looking at all of the events, that is farthest from\nzero, and see if you can describe the strengths and weaknesses of the\nathletes in that cluster (look at all the events for the cluster that\nhas that extreme mean). Bear in mind (i) that these are the original\nperformances standardized, and (ii) for a running event, a\n*smaller* value is better.\n\n\n\nSolution\n\n\nThis is the thing called `centers`:^[We are no longer    in the `tidyverse`, so you no longer have the option of    using British or American spelling.]\n\n::: {.cell}\n\n```{.r .cell-code}\ndecathlon.1$centers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        x100m   long.jump   shot.put     high.jump      x400m     x110mh     discus\n1  0.75760985 -0.53619092 -0.7922550 -1.554312e-15  1.5180512  1.6631161 -0.2159787\n2 -0.02051555  0.02245134  0.9034011  2.644188e-01 -0.0589434 -0.3097414  0.6739749\n3  0.28457995  0.07871177 -0.8288519  2.326886e-01 -0.1588370 -0.3582955 -1.0406594\n4 -0.97448850  0.64184430 -0.1484207 -2.467909e-01 -1.0216857 -0.5934385  0.2274805\n5  1.55771620 -2.27947172  0.9765949 -1.798048e+00  1.3236413  1.1775755 -0.6894704\n   pole.vault     javelin     x1500m\n1 -0.76236268  0.28321676  1.3477946\n2 -0.10890895 -0.49565010  0.3441300\n3  1.17932839  0.06548297 -0.1670467\n4 -0.07779211  0.65707285 -0.9136808\n5 -1.50916693 -1.43751822 -1.8269002\n```\n:::\n:::\n\n   \n\nMy most extreme value is the $-2.28$ in the long jump column, cluster\n4. Yours may well be different, since the formation of clusters is\nrandom: it will probably not be the same number cluster, and it might\nnot even be the same value. Use whatever you have. (I asked you to\nfind the most extreme one so that the other events in the same cluster\nare likely to be extreme as well and you have something to say.)\n\nSo I have to look along my cluster 4 row. I see:\n\n\n\n* 100m run high (bad)\n\n* long jump low (bad)\n\n* shot put high (good)\n\n* high jump low (bad)\n\n* 400m run high (bad)\n\n* 110m hurdles run high (bad)\n\n* discus lowish (bad)\n\n* pole vault low (bad)\n\n* javelin low (bad)\n\n* 1500m low (good)\n\n\nThe only two good events here are shot put (throwing a heavy ball) and\n1500m (a long run). So what these athletes have in common is good strength\nand endurance, and bad speed and agility. (You can use my \n\"skills required\" in the table at the top of the question as a guide.)\n\nI said \"these athletes\". I actually meant \"this athlete\", since\nthis is the cluster with just Marcus Nilsson in it. I ought to have\nchecked that we were looking at a cluster with several athletes in it,\nand then this question would have made more sense, but the thought\nprocess is the same, so it doesn't matter so much.\n\nYour cluster may well be different; I'm looking for some sensible\ndiscussion based on the values you have. I'm hoping that the athletes\nin your cluster will tend to be good at something and bad at something\nelse, and the things they are good at (or bad at) will have something\nin common.\n\nWhat would have made more sense would have been to take the\n*biggest* cluster:\n\n::: {.cell}\n\n```{.r .cell-code}\ndecathlon.1$size\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4 8 5 6 1\n```\n:::\n:::\n\n \n\nwhich in this case is cluster 3, and then\n\n::: {.cell}\n\n```{.r .cell-code}\ndecathlon.1$centers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        x100m   long.jump   shot.put     high.jump      x400m     x110mh     discus\n1  0.75760985 -0.53619092 -0.7922550 -1.554312e-15  1.5180512  1.6631161 -0.2159787\n2 -0.02051555  0.02245134  0.9034011  2.644188e-01 -0.0589434 -0.3097414  0.6739749\n3  0.28457995  0.07871177 -0.8288519  2.326886e-01 -0.1588370 -0.3582955 -1.0406594\n4 -0.97448850  0.64184430 -0.1484207 -2.467909e-01 -1.0216857 -0.5934385  0.2274805\n5  1.55771620 -2.27947172  0.9765949 -1.798048e+00  1.3236413  1.1775755 -0.6894704\n   pole.vault     javelin     x1500m\n1 -0.76236268  0.28321676  1.3477946\n2 -0.10890895 -0.49565010  0.3441300\n3  1.17932839  0.06548297 -0.1670467\n4 -0.07779211  0.65707285 -0.9136808\n5 -1.50916693 -1.43751822 -1.8269002\n```\n:::\n:::\n\n \n\nwhich says that the eight athletes in cluster 3 are a bit above\naverage for shot put and discus, and below average for javelin, and,\ntaking a decision, about average for everything else. This is kind of\nodd, since these are all throwing events, but the javelin is propelled\na long way by running fast, and the other two are propelled mainly\nusing strength rather than speed, so it makes some kind of sense\n(after the fact, at least).\n\nMy guess is that someone good at javelin is likely to be good at\nsprint running and possibly also the long jump, since that depends\nprimarily on speed, once you have enough technique. Well, one way to\nfigure out whether I was right:\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(decathlon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 x100m   long.jump    shot.put   high.jump        x400m      x110mh\nx100m       1.00000000 -0.61351932 -0.17373396 -0.03703619  0.789091241  0.67372152\nlong.jump  -0.61351932  1.00000000  0.08369570  0.46379852 -0.548197160 -0.39484085\nshot.put   -0.17373396  0.08369570  1.00000000  0.02012049 -0.172516054 -0.28310469\nhigh.jump  -0.03703619  0.46379852  0.02012049  1.00000000  0.015217204 -0.08356323\nx400m       0.78909124 -0.54819716 -0.17251605  0.01521720  1.000000000  0.80285420\nx110mh      0.67372152 -0.39484085 -0.28310469 -0.08356323  0.802854203  1.00000000\ndiscus     -0.14989960  0.12891051  0.46449586 -0.11770266 -0.068778203 -0.13777771\npole.vault -0.12087966  0.21976890 -0.19328449  0.13565269 -0.361823592 -0.51871733\njavelin     0.02363715  0.01969302 -0.11313467 -0.12454417 -0.005823468 -0.05246857\nx1500m      0.14913949 -0.11672283 -0.06156793  0.27779220  0.446949386  0.39800522\n                discus  pole.vault      javelin       x1500m\nx100m      -0.14989960 -0.12087966  0.023637150  0.149139491\nlong.jump   0.12891051  0.21976890  0.019693022 -0.116722829\nshot.put    0.46449586 -0.19328449 -0.113134672 -0.061567926\nhigh.jump  -0.11770266  0.13565269 -0.124544175  0.277792195\nx400m      -0.06877820 -0.36182359 -0.005823468  0.446949386\nx110mh     -0.13777771 -0.51871733 -0.052468568  0.398005215\ndiscus      1.00000000 -0.10045072  0.020977427  0.019890861\npole.vault -0.10045072  1.00000000  0.052377148 -0.059888360\njavelin     0.02097743  0.05237715  1.000000000 -0.008858031\nx1500m      0.01989086 -0.05988836 -0.008858031  1.000000000\n```\n:::\n:::\n\n \n\nor, for this, maybe better:\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(decathlon) %>%\n  as.data.frame() %>%\n  rownames_to_column(\"event\") %>%\n  pivot_longer(-event, names_to=\"event2\", values_to=\"corr\") %>%\n  filter(event < event2) %>%\n  arrange(desc(abs(corr)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 45 x 3\n   event      event2      corr\n   <chr>      <chr>      <dbl>\n 1 x110mh     x400m      0.803\n 2 x100m      x400m      0.789\n 3 x100m      x110mh     0.674\n 4 long.jump  x100m     -0.614\n 5 long.jump  x400m     -0.548\n 6 pole.vault x110mh    -0.519\n 7 discus     shot.put   0.464\n 8 high.jump  long.jump  0.464\n 9 x1500m     x400m      0.447\n10 x110mh     x1500m     0.398\n# i 35 more rows\n```\n:::\n:::\n\n \n\nI should probably talk about the code:\n\n\n\n* I want to grab the event names from the row names of the\nmatrix. This is a bit awkward, because I want to turn the matrix\ninto a data frame, but if I turn it into a `tibble`, the row\nnames will disappear.\n\n* Thus, I turn it into an old-fashioned `data.frame`, and\nthen it has row names, which I can grab and put into a column called\n`event`.\n\n* Then I make the data frame longer, creating a column\n`event2` which is the second thing that each correlation will\nbe between.\n\n* The correlations between an event and itself will be 1, and\nbetween events B and A will be the same as between A and B. So I\ntake only the rows where the first event is alphabetically less than\nthe second one.\n\n* Then I arrange them in descending order of *absolute*\ncorrelation, since a large negative correlation is also interesting.\n\n\nThere are actually only a few high correlations:\n\n\n\n* 100m with long jump, 400m and 110m hurdles\n\n* long jump with 100m, high jump and 400m\n\n* shot put with discus\n\n* high jump with long jump\n\n* 400m with all the other running events plus long jump\n\n* 110m hurdles with the other running events plus pole vault\n\n* discus with shot put\n\n* pole vault with 110m hurdles and maybe 400m\n\n* javelin with *nothing*\n\n* 1500m with 400m\n\n\nSome of the correlations are negative as expected, since they are\nbetween a running event and a jumping/throwing event (that is, a long distance goes with a small time, both of which are good).\n\nI was wrong about javelin. It seems to be a unique skill in the\ndecathlon, which is presumably why it's there: you want 10 events that\nare as disparate as possible, rather than things that are highly\ncorrelated. \n  \n\n$\\blacksquare$\n\n\n\n##  Clustering the Swiss bills\n\n\nThis question is about the Swiss bank counterfeit bills\nagain. This time we're going to ignore whether each bill is\ncounterfeit or not, and see what groups they break into. Then, at\nthe end, we'll see whether cluster analysis was able to pick out the\ncounterfeit ones or not.\n\n\n(a) Read the data in again (just like last time), and look at\nthe first few rows. This is just the same as before.\n \nSolution\n\n\nThe data file was aligned in columns, so:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/swiss1.txt\"\nswiss <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n-- Column specification ------------------------------------------------------------------\ncols(\n  length = col_double(),\n  left = col_double(),\n  right = col_double(),\n  bottom = col_double(),\n  top = col_double(),\n  diag = col_double(),\n  status = col_character()\n)\n```\n:::\n\n```{.r .cell-code}\nswiss\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 x 7\n   length  left right bottom   top  diag status \n    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <chr>  \n 1   215.  131   131.    9     9.7  141  genuine\n 2   215.  130.  130.    8.1   9.5  142. genuine\n 3   215.  130.  130.    8.7   9.6  142. genuine\n 4   215.  130.  130.    7.5  10.4  142  genuine\n 5   215   130.  130.   10.4   7.7  142. genuine\n 6   216.  131.  130.    9    10.1  141. genuine\n 7   216.  130.  130.    7.9   9.6  142. genuine\n 8   214.  130.  129.    7.2  10.7  142. genuine\n 9   215.  129.  130.    8.2  11    142. genuine\n10   215.  130.  130.    9.2  10    141. genuine\n# i 190 more rows\n```\n:::\n:::\n\n       \n$\\blacksquare$\n\n(b) The variables in this data frame are on different\nscales. Standardize them so that they all have mean 0 and standard\ndeviation 1. (Don't try to standardize the `status` column!)\n \nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nswiss.s <- swiss %>%\n  select(-status) %>%\n  scale()\n```\n:::\n\n    \n\nWhat kind of thing do we have?\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(swiss.s)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n \n\nso something like this is needed to display some of it (rather than\nall of it):\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(swiss.s)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         length      left      right     bottom        top      diag\n[1,] -0.2549435  2.433346  2.8299417 -0.2890067 -1.1837648 0.4482473\n[2,] -0.7860757 -1.167507 -0.6347880 -0.9120152 -1.4328473 1.0557460\n[3,] -0.2549435 -1.167507 -0.6347880 -0.4966762 -1.3083061 1.4896737\n[4,] -0.2549435 -1.167507 -0.8822687 -1.3273542 -0.3119759 1.3161027\n[5,]  0.2761888 -1.444496 -0.6347880  0.6801176 -3.6745902 1.1425316\n[6,]  2.1351516  1.879368  1.3450576 -0.2890067 -0.6855997 0.7953894\n```\n:::\n:::\n\n \n \n$\\blacksquare$\n\n(c) We are going to make a scree plot. First, calculate the total\nwithin-cluster SS for each number of clusters from 2 to 10.\n \nSolution\n\n \nWhen I first made this problem (some years ago),\nI thought the obvious answer was a loop, but now that I've been\nsteeped in the Tidyverse a while, I think `rowwise` is much\nclearer, so I'll do that first.\nStart by making a `tibble` that has one column called `clusters` containing the numbers 2 through 10:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(clusters = 2:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 1\n  clusters\n     <int>\n1        2\n2        3\n3        4\n4        5\n5        6\n6        7\n7        8\n8        9\n9       10\n```\n:::\n:::\n\n \nNow, for each of these numbers of clusters, calculate the total within-cluster sum of squares for *it* (that number of clusters). To do that, think about how you'd do it for something like three clusters:\n\n::: {.cell}\n\n```{.r .cell-code}\nkmeans(swiss.s, 3, nstart = 20)$tot.withinss\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 576.1284\n```\n:::\n:::\n\n \n\nand then use that within your `rowwise`:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(clusters = 2:10) %>%\n  rowwise() %>% \n  mutate(wss = kmeans(swiss.s, clusters, nstart = 20)$tot.withinss) -> wssq\nwssq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 2\n# Rowwise: \n  clusters   wss\n     <int> <dbl>\n1        2  701.\n2        3  576.\n3        4  492.\n4        5  449.\n5        6  414.\n6        7  382.\n7        8  355.\n8        9  336.\n9       10  313.\n```\n:::\n:::\n\n \n\nAnother way is to save *all* the output from the `kmeans`, in a list-column, and then *extract* the thing you want, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(clusters = 2:10) %>%\n  rowwise() %>% \n  mutate(km = list(kmeans(swiss.s, clusters, nstart = 20))) %>%\n  mutate(wss = km$tot.withinss) -> wssq.2\nwssq.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 3\n# Rowwise: \n  clusters km         wss\n     <int> <list>   <dbl>\n1        2 <kmeans>  701.\n2        3 <kmeans>  576.\n3        4 <kmeans>  492.\n4        5 <kmeans>  449.\n5        6 <kmeans>  413.\n6        7 <kmeans>  382.\n7        8 <kmeans>  355.\n8        9 <kmeans>  335.\n9       10 <kmeans>  313.\n```\n:::\n:::\n\nThe output from `kmeans` is a collection of things, not just a single number, so when you create the column `km`, you need to put `list` around the `kmeans`, and then you'll create a list-column. `wss`, on the other hand, is a single number each time, so no `list` is needed, and `wss` is an ordinary column of numbers, labelled `dbl` at the top. \n\nThe most important thing in both of these is to remember the `rowwise`. Without it, everything will go horribly wrong! This is because `kmeans` expects a *single number* for the number of clusters, and `rowwise` will provide that single number (for the row you are looking at). If you forget the `rowwise`, the whole column `clusters` will get fed into `kmeans` all at once, and `kmeans` will get horribly confused.\n \n \nIf you insist, do it Python-style as a loop, like this:      \n\n::: {.cell}\n\n```{.r .cell-code}\nclus <- 2:10\nwss.1 <- numeric(0)\nfor (i in clus)\n{\n  wss.1[i] <- kmeans(swiss.s, i, nstart = 20)$tot.withinss\n}\nwss.1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]       NA 701.2054 576.4660 491.7085 449.3900 413.1265 381.5568 355.3900 334.6444\n[10] 312.8958\n```\n:::\n:::\n\n       \n\nNote that there are 10 `wss` values, but the first one is\nmissing, since we didn't do one cluster.^[R vectors start from  1, unlike C arrays or Python lists, which start from 0.]\n\nThe `numeric(0)` says \"`wss` has nothing in it, but if it had anything, it would be numbers\". Or, you can initialize\n`wss` to however long it's going to be (here 10), which is\nactually more efficient (R doesn't have to keep making it \n\"a bit longer\"). If you initialize it to length 10, the 10 values will have\n`NA`s in them when you start.\nIt doesn't matter what `nstart` is: Ideally, big enough to have a decent\nchance of finding the best clustering, but small enough that it\ndoesn't take too long to run.\nWhichever way you create your total within-cluster sums of squares, you can use it to make a scree plot (next part). \n\n \n$\\blacksquare$\n\n(d) <a name=\"part:howmany\">*</a> Make a scree plot (creating a data frame\nfirst if you need). How many clusters do you think you\nshould use?\n \nSolution\n\n\nThe easiest is to use the output from the `rowwise`,\nwhich I called `wssq`, this already being a dataframe:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(wssq, aes(x = clusters, y = wss)) + geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/swiss-cluster-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n       \nIf you did it the loop way, you'll have to make a data frame\nfirst, which you can then pipe into `ggplot`:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(clusters = 1:10, wss = wss.1) %>%\n  ggplot(aes(x = clusters, y = wss)) + geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n```\n:::\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/sasjhgajs-1.pdf){fig-pos='H'}\n:::\n:::\n\n       \nIf you started at 2 clusters, your `wss` will start at 2\nclusters also, and you'll need to be careful to have something like\n`clusters=2:10` (not `1:10`) in the definition of your\ndata frame. \n\nInterpretation: I see a small elbow at 4 clusters, so that's how many I think we\nshould use. Any place you can reasonably see an elbow is good.\n\nThe warning is about the missing within-cluster total sum of squares\nfor one cluster, since the loop way didn't supply a total\nwithin-cluster sum of squares for one cluster.\n \n$\\blacksquare$\n\n(e) Run K-means with the number of clusters that you found in\n(<a href=\"#part:howmany\">here</a>). How many bills are in each cluster?\n \nSolution\n\n\n\nI'm going to start by setting the random number seed (so that\nmy results don't change every time I run this). You don't need\nto do that, though you might want to in a Quarto document so that the random stuff doesn't change from one render to the next and you can safely talk about the results.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(457299)\n```\n:::\n\n\n\nNow, down to business:\n\n::: {.cell}\n\n```{.r .cell-code}\nswiss.7 <- kmeans(swiss.s, 4, nstart = 20)\nswiss.7$size\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50 32 68 50\n```\n:::\n:::\n\n       \nThis many. Note that my clusters 1 and 4 (and also 2 and 3) add up to\n100 bills. There were 100 genuine and 100 counterfeit bills in the\noriginal data set.\nI don't know why \"7\". I just felt like it.\nExtra: you might remember that back before I actually *ran*\nK-means on each of the numbers of clusters from 2 to 10. How can we\nextract that output? Something like this. Here's where the output was:\n\n::: {.cell}\n\n```{.r .cell-code}\nwssq.2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 3\n# Rowwise: \n  clusters km         wss\n     <int> <list>   <dbl>\n1        2 <kmeans>  701.\n2        3 <kmeans>  576.\n3        4 <kmeans>  492.\n4        5 <kmeans>  449.\n5        6 <kmeans>  413.\n6        7 <kmeans>  382.\n7        8 <kmeans>  355.\n8        9 <kmeans>  335.\n9       10 <kmeans>  313.\n```\n:::\n:::\n\n \n\nNow we need to pull out the 4th row and the `km` column. We need the output as an actual thing, not a data frame, so:\n\n::: {.cell}\n\n```{.r .cell-code}\nwssq.2 %>%\n  filter(clusters == 4) %>%\n  pull(km) -> swiss.7a\n```\n:::\n\n \n\nIs that the right thing?\n\n::: {.cell}\n\n```{.r .cell-code}\nswiss.7a\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\nK-means clustering with 4 clusters of sizes 50, 50, 68, 32\n\nCluster means:\n      length       left      right     bottom         top       diag\n1  0.1062264  0.6993965  0.8352473  0.1927865  1.18251937 -0.9316427\n2 -0.5683115  0.2617543  0.3254371  1.3197396  0.04670298 -0.8483286\n3 -0.2002681 -1.0290130 -0.9878119 -0.8397381 -0.71307204  0.9434354\n4  1.1475776  0.6848546  0.2855308 -0.5788787 -0.40538184  0.7764051\n\nClustering vector:\n  [1] 4 3 3 3 3 4 3 3 3 4 4 3 4 3 3 3 3 3 3 3 3 4 4 4 3 4 4 4 4 3 4 3 3 4 4 4 4 3 4 3 3 3\n [43] 3 4 3 3 3 3 3 3 3 4 3 4 3 3 4 3 4 3 3 3 3 3 3 4 3 3 3 1 3 3 3 3 3 3 3 3 4 3 3 3 3 4\n [85] 4 3 3 3 4 3 3 4 3 3 3 4 4 3 3 3 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 1 2 2 2 1 1 2 1 1 2 1\n[127] 1 1 1 1 2 2 1 1 2 2 2 1 2 2 1 2 2 1 2 2 2 1 2 1 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 1 4 1\n[169] 1 2 1 2 2 2 2 2 2 1 1 1 2 1 1 1 2 2 1 2 1 2 1 1 2 1 2 1 1 1 1 2\n\nWithin cluster sum of squares by cluster:\n[1] 137.68573  95.51948 166.12573  92.37757\n (between_SS / total_SS =  58.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n:::\n:::\n\n \n\nLooks like it. But I should check:\n\n::: {.cell}\n\n```{.r .cell-code}\nswiss.7a$centers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n \n\nAh. `swiss.7a` is actually a `list`, as evidenced by the `[[1]]` at the top of the output, so I get things from it thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nswiss.7a[[1]]$centers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      length       left      right     bottom         top       diag\n1  0.1062264  0.6993965  0.8352473  0.1927865  1.18251937 -0.9316427\n2 -0.5683115  0.2617543  0.3254371  1.3197396  0.04670298 -0.8483286\n3 -0.2002681 -1.0290130 -0.9878119 -0.8397381 -0.71307204  0.9434354\n4  1.1475776  0.6848546  0.2855308 -0.5788787 -0.40538184  0.7764051\n```\n:::\n:::\n\n \n\nThis would be because it came from a list-column; using `pull` removed the data-frameness from `swiss.7a`, but not its listness.\n \n$\\blacksquare$\n\n(f) Make a table showing cluster membership against actual\nstatus (counterfeit or genuine). Are the counterfeit bills mostly\nin certain clusters?\n \nSolution\n\n\n`table`. `swiss.7$cluster` shows the actual\ncluster numbers:\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(swiss$status, swiss.7$cluster)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             \n               1  2  3  4\n  counterfeit 50  1  0 49\n  genuine      0 31 68  1\n```\n:::\n:::\n\n       \n\nOr, if you prefer,\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(obs = swiss$status, pred = swiss.7$cluster) %>%\n  count(obs, pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n  obs          pred     n\n  <chr>       <int> <int>\n1 counterfeit     1    50\n2 counterfeit     2     1\n3 counterfeit     4    49\n4 genuine         2    31\n5 genuine         3    68\n6 genuine         4     1\n```\n:::\n:::\n\n \n\nor even\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(obs = swiss$status, pred = swiss.7$cluster) %>%\n  count(obs, pred) %>%\n  pivot_wider(names_from = obs, values_from = n, values_fill = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 3\n   pred counterfeit genuine\n  <int>       <int>   <int>\n1     1          50       0\n2     2           1      31\n3     4          49       1\n4     3           0      68\n```\n:::\n:::\n\n \n\nIn my case (yours might be different), 99 of the 100 counterfeit bills\nare in clusters 1 and 4, and 99 of the 100 genuine bills are in\nclusters 2 and 3.^[This is again where `set.seed` is  valuable: write this text once and it never needs to change.] So the\nclustering has done a very good job of distinguishing the genuine\nbills from the counterfeit ones. (You could imagine, if you were an\nemployee at the bank, saying that a bill in cluster 1 or 4 is\ncounterfeit, and being right 99\\% of the time.) This is kind of a\nby-product of the clustering, though: we weren't *trying* to\ndistinguish counterfeit bills (that would have been the discriminant\nanalysis that we did before); we were just trying to divide them into\ngroups of different ones, and part of what made them different was\nthat some of them were genuine bills and some of them were\ncounterfeit.\n \n$\\blacksquare$\n\n\n\n\n\n##  Grouping similar cars\n\n\n The file\n[link](http://ritsokiguess.site/datafiles/car-cluster.csv) contains\ninformation on seven variables \nfor 32 different cars. The variables are:\n\n\n\n* `Carname`: name of the car (duh!)\n\n* `mpg`: gas consumption in miles per US gallon (higher means the car uses less gas)\n\n* `disp`: engine displacement (total volume of cylinders in engine): higher is more powerful\n\n* `hp`: engine horsepower (higher means a more powerful engine)\n\n* `drat`: rear axle ratio (higher means more powerful but worse gas mileage)\n\n* `wt`: car weight in US tons\n\n* `qsec`: time needed for the car to cover a quarter mile (lower means faster)\n\n\n\n\n(a) Read in the data and display its structure. Do you have the\nright number of cars and variables?\n\nSolution\n\n::: {.cell}\n\n```{.r .cell-code}\n# my_url <- \"http://ritsokiguess.site/datafiles/car-cluster.csv\"\nmy_url <- \"http://ritsokiguess.site/datafiles/car-cluster.csv\"\ncars <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 32 Columns: 7\n-- Column specification ------------------------------------------------------------------\nDelimiter: \",\"\nchr (1): Carname\ndbl (6): mpg, disp, hp, drat, wt, qsec\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ncars\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 7\n   Carname             mpg  disp    hp  drat    wt  qsec\n   <chr>             <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Mazda RX4          21    160    110  3.9   2.62  16.5\n 2 Mazda RX4 Wag      21    160    110  3.9   2.88  17.0\n 3 Datsun 710         22.8  108     93  3.85  2.32  18.6\n 4 Hornet 4 Drive     21.4  258    110  3.08  3.22  19.4\n 5 Hornet Sportabout  18.7  360    175  3.15  3.44  17.0\n 6 Valiant            18.1  225    105  2.76  3.46  20.2\n 7 Duster 360         14.3  360    245  3.21  3.57  15.8\n 8 Merc 240D          24.4  147.    62  3.69  3.19  20  \n 9 Merc 230           22.8  141.    95  3.92  3.15  22.9\n10 Merc 280           19.2  168.   123  3.92  3.44  18.3\n# i 22 more rows\n```\n:::\n:::\n\n   \n\nCheck, both on number of cars and number of variables.\n\n$\\blacksquare$\n\n(b) The variables are all measured on different scales. Use\n`scale` to produce a matrix of standardized ($z$-score) values\nfor the columns of your data that are numbers.\n\n\n\nSolution\n\n\nAll but the first column needs to be scaled, so:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars %>% select(-Carname) %>% scale() -> cars.s\n```\n:::\n\n   \n\nThis is a `matrix`, as we've seen before.\n\nAnother way is like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars %>% select(where(is.numeric)) %>% scale() -> h\n```\n:::\n\n \nI would prefer to have a look at my result, so that I can see that it\nhas sane things in it:\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(cars.s)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            mpg        disp         hp       drat           wt       qsec\n[1,]  0.1508848 -0.57061982 -0.5350928  0.5675137 -0.610399567 -0.7771651\n[2,]  0.1508848 -0.57061982 -0.5350928  0.5675137 -0.349785269 -0.4637808\n[3,]  0.4495434 -0.99018209 -0.7830405  0.4739996 -0.917004624  0.4260068\n[4,]  0.2172534  0.22009369 -0.5350928 -0.9661175 -0.002299538  0.8904872\n[5,] -0.2307345  1.04308123  0.4129422 -0.8351978  0.227654255 -0.4637808\n[6,] -0.3302874 -0.04616698 -0.6080186 -1.5646078  0.248094592  1.3269868\n```\n:::\n:::\n\n \n\nor, \n\n::: {.cell}\n\n```{.r .cell-code}\nhead(h)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            mpg        disp         hp       drat           wt       qsec\n[1,]  0.1508848 -0.57061982 -0.5350928  0.5675137 -0.610399567 -0.7771651\n[2,]  0.1508848 -0.57061982 -0.5350928  0.5675137 -0.349785269 -0.4637808\n[3,]  0.4495434 -0.99018209 -0.7830405  0.4739996 -0.917004624  0.4260068\n[4,]  0.2172534  0.22009369 -0.5350928 -0.9661175 -0.002299538  0.8904872\n[5,] -0.2307345  1.04308123  0.4129422 -0.8351978  0.227654255 -0.4637808\n[6,] -0.3302874 -0.04616698 -0.6080186 -1.5646078  0.248094592  1.3269868\n```\n:::\n:::\n\n \n\nThese look right. Or, perhaps better, this:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(cars.s)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mpg               disp               hp               drat        \n Min.   :-1.6079   Min.   :-1.2879   Min.   :-1.3810   Min.   :-1.5646  \n 1st Qu.:-0.7741   1st Qu.:-0.8867   1st Qu.:-0.7320   1st Qu.:-0.9661  \n Median :-0.1478   Median :-0.2777   Median :-0.3455   Median : 0.1841  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.4495   3rd Qu.: 0.7688   3rd Qu.: 0.4859   3rd Qu.: 0.6049  \n Max.   : 2.2913   Max.   : 1.9468   Max.   : 2.7466   Max.   : 2.4939  \n       wt               qsec         \n Min.   :-1.7418   Min.   :-1.87401  \n 1st Qu.:-0.6500   1st Qu.:-0.53513  \n Median : 0.1101   Median :-0.07765  \n Mean   : 0.0000   Mean   : 0.00000  \n 3rd Qu.: 0.4014   3rd Qu.: 0.58830  \n Max.   : 2.2553   Max.   : 2.82675  \n```\n:::\n:::\n\n \nThe mean is exactly zero, for all variables, which is as it should\nbe. Also, the standardized values look about as they should; even the\nextreme ones don't go beyond $\\pm 3$. \n\nThis doesn't show the standard deviation of each variable, though,\nwhich should be exactly 1 (since that's what \"standardizing\"\nmeans). To get *that*, *this*:\n\n::: {.cell}\n\n```{.r .cell-code}\nas_tibble(cars.s) %>%\n  summarize(across(everything(), \\(x) sd(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 6\n    mpg  disp    hp  drat    wt  qsec\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     1     1     1     1     1     1\n```\n:::\n:::\n\n \n\nThe idea here is \"take the matrix `cars.s`, turn it into a data frame, and for each *column*, calculate the SD of it\".^[The *scale* function can take  a data frame, as here, but always produces a matrix. That's why we  had to turn it back into a data frame.]\n\nAs you realize now, the same idea will get the mean of each column too:\n\n::: {.cell}\n\n```{.r .cell-code}\nas_tibble(cars.s) %>%\n  summarize(across(everything(), \\(x) mean(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 6\n       mpg      disp       hp      drat       wt     qsec\n     <dbl>     <dbl>    <dbl>     <dbl>    <dbl>    <dbl>\n1 7.11e-17 -9.08e-17 1.04e-17 -2.92e-16 4.68e-17 5.30e-16\n```\n:::\n:::\n\n \n\nand we see that the means are all zero, to about 15 decimals, anyway.\n  \n\n$\\blacksquare$\n\n(c) Run a K-means cluster analysis for these data, obtaining 3\nclusters, and display the results. Take whatever action you need to\nobtain the best (random) result from a number of runs.\n\n\n\nSolution\n\n\nThe hint at the end says \"use `nstart`\", so something like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(457299)\ncars.1 <- kmeans(cars.s, 3, nstart = 20)\ncars.1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK-means clustering with 3 clusters of sizes 12, 6, 14\n\nCluster means:\n         mpg       disp         hp       drat         wt       qsec\n1  0.1384407 -0.5707543 -0.5448163  0.1887816 -0.2454544  0.5491221\n2  1.6552394 -1.1624447 -1.0382807  1.2252295 -1.3738462  0.3075550\n3 -0.8280518  0.9874085  0.9119628 -0.6869112  0.7991807 -0.6024854\n\nClustering vector:\n [1] 1 1 1 1 3 1 3 1 1 1 1 3 3 3 3 3 3 2 2 2 1 3 3 3 3 2 2 2 3 1 3 1\n\nWithin cluster sum of squares by cluster:\n[1] 24.95528  7.76019 33.37849\n (between_SS / total_SS =  64.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n:::\n:::\n\n   \n\nYou don't need the `set.seed`, but if you run again, you'll get\na different answer. With the `nstart`, you'll probably get the\nsame clustering every time you run, but the clusters might have\ndifferent numbers, so that when you talk about \"cluster 1\" and then\nre-run, what you were talking about might have moved to cluster 3, say.\n\nIn a Quarto document, for this reason, having a\n`set.seed` before anything involving random number generation\nis a smart move.^[I forgot this, and then realized that I would have to rewrite a whole paragraph after I rendered it again. In case you think I remember everything the first time.]\n  \n\n$\\blacksquare$\n\n(d) Display the car names together with which cluster they are\nin. If you display them all at once, sort by cluster so that it's\neasier to see which clusters contain which cars. (You may have to make\na data frame first.)\n\n\n\nSolution\n\n\nAs below. The car names are in the `Carname` column of the\noriginal `cars` data frame, and the cluster numbers are in\nthe `cluster` part of the output from `kmeans`. You'll\nneed to take some action to display everything (there are only 32\ncars, so it's perfectly all right to display all of them):\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(car = cars$Carname, cluster = cars.1$cluster) %>%\n  arrange(cluster) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 2\n   car            cluster\n   <chr>            <int>\n 1 Mazda RX4            1\n 2 Mazda RX4 Wag        1\n 3 Datsun 710           1\n 4 Hornet 4 Drive       1\n 5 Valiant              1\n 6 Merc 240D            1\n 7 Merc 230             1\n 8 Merc 280             1\n 9 Merc 280C            1\n10 Toyota Corona        1\n# i 22 more rows\n```\n:::\n:::\n\n   \n\nOr start from the original data frame as read in from the file and\ngrab only what you want:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars %>%\n  select(Carname) %>%\n  mutate(cluster = cars.1$cluster) %>%\n  arrange(cluster) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 2\n   Carname        cluster\n   <chr>            <int>\n 1 Mazda RX4            1\n 2 Mazda RX4 Wag        1\n 3 Datsun 710           1\n 4 Hornet 4 Drive       1\n 5 Valiant              1\n 6 Merc 240D            1\n 7 Merc 230             1\n 8 Merc 280             1\n 9 Merc 280C            1\n10 Toyota Corona        1\n# i 22 more rows\n```\n:::\n:::\n\n \n\nThis time we want to *keep* the car names and throw away\neverything else.\n  \n\n$\\blacksquare$\n\n(e) I have no idea whether 3 is a sensible number of clusters. To\nfind out, we will draw a scree plot (in a moment). Write a function\nthat accepts the number of clusters and the (scaled) data,\nand returns the total within-cluster sum of squares.\n\n\n\nSolution\n\n\nI failed to guess (in conversation with students, back when this was\na question to be handed in) what you might\ndo. There are two equally good ways to tackle this part and the next:\n\n\n* Write a function to calculate the total within-cluster  sum\nof squares (in this part) and somehow use it in the next part,\neg. via `rowwise`, to get the total\nwithin-cluster sum of squares for *each* number of clusters.\n\n* Skip the function-writing part and go directly to a loop in\nthe next part.\n\nI'm good with either approach: as long as you obtain, somehow, the\ntotal within-cluster sum of squares for each number of clusters, and\nuse them for making a scree plot, I think you should get the \npoints for this part and the next.\nI'll talk about the function way here and the loop way in the next part.\n\nThe function way is just like the one in the previous question:\n\n::: {.cell}\n\n```{.r .cell-code}\nwss <- function(howmany, data, nstart = 20) {\n  kmeans(data, howmany, nstart = 20)$tot.withinss\n}\n```\n:::\n\n \n\nThe data and number of clusters can have any names, as long as you use\nwhatever input names you chose within the function.\n\nI should probably check that this works, at least on 3\nclusters. Before we had\n\n::: {.cell}\n\n```{.r .cell-code}\ncars.1$tot.withinss\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 66.09396\n```\n:::\n:::\n\n \n\nand the function gives\n\n::: {.cell}\n\n```{.r .cell-code}\nwss(3, cars.s)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 66.09396\n```\n:::\n:::\n\n \n\nCheck.\nI need to make sure that I used my scaled `cars` data, but I\ndon't need to say anything about `nstart`, since that defaults\nto the perfectly suitable 20.\n  \n\n$\\blacksquare$\n\n(f) Calculate the total within-group sum of squares for each\nnumber of clusters from 2 to 10, using the function you just wrote.\n\n\n\nSolution\n\n\nThe loop way. I like to define my possible numbers of clusters into\na vector first:\n\n::: {.cell}\n\n```{.r .cell-code}\nw <- numeric(0)\nnclus <- 2:10\nfor (i in nclus) {\n  w[i] <- wss(i, cars.s)\n}\nw\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]       NA 87.29448 66.09396 50.94273 38.22004 29.28816 24.23138 20.76061 17.97491\n[10] 15.19850\n```\n:::\n:::\n\n\nNow that I look at this again, it occurs to me that there is no great\nneed to write a function to do this: you can just do what you need to\ndo within the loop, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nw <- numeric(0)\nnclus <- 2:10\nfor (i in nclus) {\n  w[i] <- kmeans(cars.s, i, nstart = 20)$tot.withinss\n}\nw\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]       NA 87.29448 66.09396 50.94273 38.22004 29.28816 24.23138 20.76061 17.33653\n[10] 15.19850\n```\n:::\n:::\n\n \n\nYou ought to have an `nstart` somewhere to make sure that\n`kmeans` gets run a number of times and the best result taken. \n\nIf you initialize your `w` with `numeric(10)` rather\nthan `numeric(0)`, it apparently gets filled with zeroes rather\nthan `NA` values. This means, later, when you come to plot your\n`w`-values, the within-cluster total sum of squares will appear\nto be zero, a legitimate value, for one cluster, even though it is\ndefinitely not. (Or, I suppose, you could start your loop at 1\ncluster, and get a legitimate, though very big, value for it.)\n\nIn both of the above cases, the curly brackets are optional because\nthere is only one line within the loop.^[I am accustomed to  using the curly brackets all the time, partly because my single-line loops have a habit of expanding to more than one line as I embellish what they do, and partly because I'm used to the programming language Perl where the curly brackets are obligatory even with only one line. Curly brackets in Perl serve the same purpose as indentation serves in Python: figuring out what is inside a loop or an *if* and what is outside.]\n\nWhat is *actually* happening here is an implicit\nloop-within-a-loop. There is a loop over `i` that goes over all\nclusters, and then there is a loop over another variable, `j`\nsay, that loops over the `nstart` runs that we're doing for\n`i` clusters, where we find the `tot.withinss` for\n`i` clusters on the `j`th run, and if it's the best one\nso far for `i` clusters, we save it. Or, at least,\n`kmeans` saves it.\n\nOr, using `rowwise`, which I like better:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(clusters = 2:10) %>%\n  rowwise() %>% \n  mutate(ss = wss(clusters, cars.s)) -> wwx\nwwx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 2\n# Rowwise: \n  clusters    ss\n     <int> <dbl>\n1        2  87.3\n2        3  66.1\n3        4  50.9\n4        5  38.2\n5        6  29.3\n6        7  24.2\n7        8  21.0\n8        9  17.3\n9       10  15.2\n```\n:::\n:::\n\n \n\nNote that `w` starts at 1, but `wwx` starts at 2. For\nthis way, you *have* to define a function first to calculate the\ntotal within-cluster sum of squares for a given number of clusters. If\nyou must, you can do the calculation in the `mutate` rather than writing a function, \nbut I find that very confusing to read, so I'd rather define the\nfunction first, and then use it later.  (The principle is to keep the `mutate` simple, and put the complexity in the function where it belongs.)\n\nAs I say, if you *must*:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(clusters = 2:10) %>%\n  rowwise() %>% \n  mutate(wss = kmeans(cars.s, \n                      clusters, \n                      nstart = 20)$tot.withinss) -> wwx\nwwx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 2\n# Rowwise: \n  clusters   wss\n     <int> <dbl>\n1        2  87.3\n2        3  66.1\n3        4  50.9\n4        5  38.2\n5        6  29.3\n6        7  24.2\n7        8  20.8\n8        9  17.3\n9       10  15.2\n```\n:::\n:::\n\n\n\n\nThe upshot of all of this is that if you had obtained a total\nwithin-cluster sum of squares for each number of clusters,\n*somehow*, and it's correct, you should have gotten some credit^[When this was a question to hand in, which it is not any  more.] for this part and the last part. This is a common principle\nof mine, and works on exams as well as assignments; it goes back to\nthe idea of \"get the job done first\" that you first saw in C32.\n\n  \n\n$\\blacksquare$\n\n(g) Make a scree plot, using the total within-cluster sums of\nsquares values that you calculated in the previous part. \n\n\n\nSolution\n\n\nIf you did this the loop way, it's tempting to leap into this:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- data.frame(clusters = nclus, wss = w)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in data.frame(clusters = nclus, wss = w): arguments imply differing number of rows: 9, 10\n```\n:::\n:::\n\n   \n\nand then wonder why it doesn't work. The problem is that `w`\nhas 10 things in it, including an `NA` at the front (as a\nplaceholder for 1 cluster):\n\n::: {.cell}\n\n```{.r .cell-code}\nw\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]       NA 87.29448 66.09396 50.94273 38.22004 29.28816 24.23138 20.76061 17.33653\n[10] 15.19850\n```\n:::\n\n```{.r .cell-code}\nnclus\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2  3  4  5  6  7  8  9 10\n```\n:::\n:::\n\n\n\nwhile `nclus` only has 9. So do something like this instead:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(clusters = 1:10, wss = w) %>%\n  ggplot(aes(x = clusters, y = wss)) + geom_point() + geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n```\n:::\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/carc-21-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThis gives a warning because there is no 1-cluster `w`-value,\nbut the point is properly omitted from the plot, so the plot you get\nis fine.\n\nOr plot the output from `rowwise`, which is easier since it's\nalready a data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\nwwx %>% ggplot(aes(x = clusters, y = wss)) + geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/carc-22-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\n  \n\n$\\blacksquare$\n\n(h) What is a suitable number of clusters for K-means, based on\nyour scree plot?\n\n\n\nSolution\n\n\nThat seems to me to have a clear elbow at 6, suggesting six\nclusters.^[We do something similar on scree plots for principal components later, but then, for reasons that will become clear then, we take elbow *minus 1*.] Look for where the plot \n\"turns the corner\" from going down to going out, or the point that is the \n\"last  one on the mountain and the first one on the scree\". This\nmountainside goes down to 6, and from there it seems to turn the\ncorner and go out after that. \n\nThis is a judgement call, but this particular one is about as clear as\nyou can expect to see.\n\nI wanted a picture of some real scree. This one shows what I mean:\n\n\n![](scree.png)\n\n\nNote the rock face and\nthe loose rock below, which is the scree. Imagine looking at the rock\nface and scree from side-on. This is in north Wales, the\nother end of Wales from Llanederyn/Llanedeyrn and Caldicot.\n\nThe above photo is from [link](http://www.geograph.org.uk/photo/159935).\n  \n\n$\\blacksquare$\n\n(i) Run a K-means analysis using the number of clusters suggested\nby your scree plot, and list the car names together with the clusters\nthey belong to, *sorted by cluster*.\n\n\n\nSolution\n\n\nThis is the same idea as above. The `arrange` idea from above\nseems to be the cleanest way to arrange the output:\nThe K-means analysis is thus:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars.2 <- kmeans(cars.s, 6, nstart = 20)\n```\n:::\n\n  \n\nor use whatever number of clusters you thought was good from your\nscree plot.\n\nThen display them:\n\n::: {.cell}\n\n```{.r .cell-code}\ncars %>%\n  select(Carname) %>%\n  mutate(cluster = cars.2$cluster) %>%\n  arrange(cluster) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 2\n   Carname        cluster\n   <chr>            <int>\n 1 Fiat 128             1\n 2 Honda Civic          1\n 3 Toyota Corolla       1\n 4 Fiat X1-9            1\n 5 Porsche 914-2        1\n 6 Lotus Europa         1\n 7 Mazda RX4            2\n 8 Mazda RX4 Wag        2\n 9 Datsun 710           2\n10 Merc 280             2\n# i 22 more rows\n```\n:::\n:::\n\n \n\nThe logic to this is the same as above.\nI don't have a good feeling for what the cars within a cluster have in\ncommon, by eyeballing the names, except for possibly a couple of\nthings: my cluster 1 seems to be mostly family cars, and my cluster 3\nappears to contain \"boats\" (large cars that consume a lot of\ngas). Your clusters ought to be about the same in terms of membership,\nbut might be numbered differently.\n\nExtra: to understand these clusters further, we can use them as input to a\ndiscriminant analysis. There isn't any real need to run a MANOVA\nfirst, since we kind of know that these groups will be different\n(that's why we ran a cluster analysis).\n\nSo, first we'll make a data frame with the whole original data set\nplus the clusters that came out of the K-means. We are adding the\nclusters to `cars`, so it makes sense to use the same ideas as I used\nabove (without the `arrange`, that being only for looking at,\nand without the `select`, since this time I want all the\nvariables that were in `cars`):\n\n::: {.cell}\n\n```{.r .cell-code}\ncarsx <- cars %>% mutate(cluster = cars.2$cluster)\n```\n:::\n\n \nNow we fire away:\n\n::: {.cell}\n\n```{.r .cell-code}\ncarsx.1 <- lda(cluster ~ mpg + disp + hp + drat + wt + qsec, data = carsx)\ncarsx.1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nlda(cluster ~ mpg + disp + hp + drat + wt + qsec, data = carsx)\n\nPrior probabilities of groups:\n      1       2       3       4       5       6 \n0.18750 0.21875 0.12500 0.15625 0.21875 0.09375 \n\nGroup means:\n       mpg     disp       hp     drat       wt     qsec\n1 30.06667  86.6500  75.5000 4.251667 1.873000 18.39833\n2 20.41429 147.0286 120.4286 3.888571 2.892143 17.62714\n3 14.60000 340.5000 272.2500 3.675000 3.537500 15.08750\n4 21.64000 178.1200  93.8000 3.430000 3.096000 20.51400\n5 16.78571 315.6286 170.0000 3.050000 3.688571 17.32000\n6 11.83333 457.3333 216.6667 3.053333 5.339667 17.74000\n\nCoefficients of linear discriminants:\n             LD1           LD2         LD3          LD4          LD5\nmpg  -0.19737944 -0.0155769096  0.27978549 -0.353766928  0.035582922\ndisp  0.01950855 -0.0001094137  0.02090998 -0.001034719  0.001680201\nhp    0.02804348  0.0251253160  0.01727355  0.015955928 -0.017220548\ndrat  0.94348424  1.8928372037 -0.56645563 -1.264185553 -2.015644662\nwt    0.39068831 -1.3973097325 -1.84808828 -2.963377419 -0.300573153\nqsec  0.33992344 -0.3010056176  0.66690927  0.755053279 -0.738889640\n\nProportion of trace:\n   LD1    LD2    LD3    LD4    LD5 \n0.7977 0.1234 0.0368 0.0299 0.0122 \n```\n:::\n:::\n\n \n\nAt the bottom (in `trace`) you see that `LD1` is clearly\nthe most important thing for splitting into groups, `LD2` might\nbe slightly relevant, and the other `LD`s are basically\nmeaningless. So a plot of the first two `LD`s should tell the story.\n\nBefore we get to that, however, we can take a look at the Coefficients\nof Linear Discriminants, for `LD1` and `LD2`\nanyway. `LD1` depends principally on `drat`, `wt`\nand `qsec` (positively) and maybe negatively on\n`mpg`. That means `LD1` will be large if the car is\npowerful, heavy, *slow* (since a larger `qsec` means the\ncar takes longer to go a quarter mile) and consumes a lot of gas. I\nthink I can summarize this as \"powerful\". \n\n`LD2` also depends on `drat` and `wt`,\nbut note the signs: it is contrasting `drat` (displacement\nratio) with `wt` (weight), so that a car with a large\ndisplacement ratio relative to its weight would be large (plus) on\n`LD2`. That is, `LD2` is \"powerful for its weight\".\n\nAll right, now for a plot, with the points colour-coded by\ncluster. There are two ways to do this; the easy one is\n`ggbiplot`. The only weirdness here is that the\n`cluster`s are numbered, so you have to turn that into a factor\nfirst (unless you like shades of blue). I didn't load the package\nfirst, so I call it here with the package name and the two colons:\n\n::: {.cell}\n\n```{.r .cell-code}\nggbiplot::ggbiplot(carsx.1, groups = factor(carsx$cluster))\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/carc-27-1.pdf){fig-pos='H'}\n:::\n:::\n\n \nOr you can do the predictions, then plot `LD1` against\n`LD2`, coloured by cluster:\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(carsx.1)\ndata.frame(p$x, cluster = factor(carsx$cluster)) %>%\n  ggplot(aes(x = LD1, y = LD2, colour = cluster)) + geom_point() +\n  coord_fixed()\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/carc-28-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThe pattern of coloured points is the same. The advantage to the\nbiplot is that you see which original variables contribute to the\n`LD` scores and thus distinguish the clusters; on the second\nplot, you have to figure out for yourself which original variables\ncontribute, and how, to the `LD` scores.\n\nYou should include `coord_fixed` to make the axis scales the\nsame, since allowing them to be different will distort the picture\n(the picture should come out square). You do the same thing in\nmultidimensional scaling.\n\nAs you see, `LD1` is doing the best job of separating the\nclusters, but `LD2` is also doing something: separating\nclusters 1 and 5, and also 2 and 4 (though 4 is a bit bigger than 2 on\n`LD1` also). \n\nI suggested above that `LD1` seems to be \"powerful\"\n(on the right) vs.\\ not (on the left). The displacement ratio is a\nmeasure of the power of an engine, so a car\nthat is large on `LD2` is powerful for  its weight.\n\nLet's find the clusters I mentioned before. Cluster 3 was the\n\"boats\": big engines and heavy cars, but not fast. So they\nshould be large `LD1` and small (negative)\n`LD2`. Cluster 1 I called \"family cars\": they are not\npowerful, but have moderate-to-good power for their weight.\n\nWith that in mind, we can have a crack at the other clusters. Cluster\n2 is neither powerful nor powerful-for-weight  (I don't know these\ncars, so can't \ncomment further) while cluster 5 is powerful and also powerful for\ntheir weight, so these\nmight be sports cars. Clusters 6 and 4 are less and more\npowerful, both averagely powerful for their size.\n  \n\n\n\n\n\n##  Rating beer\n\n\n Thirty-two students each rated 10 brands of beer:\n\n\n* Anchor Steam\n\n* Bass\n\n* Beck's\n\n* Corona\n\n* Gordon Biersch\n\n* Guinness\n\n* Heineken\n\n* Pete's Wicked Ale\n\n* Sam Adams\n\n* Sierra Nevada\n\nThe ratings are on a scale of 1 to 9, with a higher\nrating being better.\nThe data are in\n[link](http://ritsokiguess.site/datafiles/beer.txt).  I\nabbreviated the beer names for the data file. I hope you can figure\nout which is which.\n\n\n(a) Read in the data, and look at the first few rows.\n \nSolution\n\n\nData values are aligned in columns, so `read_table`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/beer.txt\"\nbeer <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n-- Column specification ------------------------------------------------------------------\ncols(\n  student = col_character(),\n  AnchorS = col_double(),\n  Bass = col_double(),\n  Becks = col_double(),\n  Corona = col_double(),\n  GordonB = col_double(),\n  Guinness = col_double(),\n  Heineken = col_double(),\n  PetesW = col_double(),\n  SamAdams = col_double(),\n  SierraN = col_double()\n)\n```\n:::\n\n```{.r .cell-code}\nbeer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 11\n   student AnchorS  Bass Becks Corona GordonB Guinness Heineken PetesW SamAdams SierraN\n   <chr>     <dbl> <dbl> <dbl>  <dbl>   <dbl>    <dbl>    <dbl>  <dbl>    <dbl>   <dbl>\n 1 S001          5     9     7      1       7        6        6      5        9       5\n 2 S008          7     5     6      8       8        4        8      8        7       7\n 3 S015          7     7     5      6       6        1        8      4        7       5\n 4 S022          7     7     5      2       5        8        4      6        8       9\n 5 S029          9     7     3      1       6        8        2      7        6       8\n 6 S036          7     6     4      3       7        6        6      5        4       9\n 7 S043          5     5     5      6       6        4        7      5        5       6\n 8 S050          5     3     1      5       5        5        3      5        5       9\n 9 S057          9     3     2      6       4        6        1      5        3       6\n10 S064          2     6     6      5       6        4        8      4        4       3\n# i 22 more rows\n```\n:::\n:::\n\n       \n32 rows (students), 11 columns (10 beers, plus a column of student\nIDs).  All seems to be kosher. If beer can be kosher.^[I  investigated. It can; in fact, I found a long list of kosher beers  that included Anchor Steam.]\n \n$\\blacksquare$ \n\n(b) The researcher who collected the data wants to see which\nbeers are rated similarly to which other beers. Try to create a\ndistance matrix from these data and explain why it didn't do what\nyou wanted. (Remember to get rid of the `student` column\nfirst.) \n \nSolution\n\n\nThe obvious thing is to feed these ratings into `dist`\n(we are *creating* distances rather than re-formatting\nthings that are already distances). We need to skip the first\ncolumn, since those are student identifiers:\n\n::: {.cell}\n\n```{.r .cell-code}\nbeer %>%\n  select(-student) %>%\n  dist() -> d\nglimpse(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 'dist' num [1:496] 9.8 8.49 6.56 8.89 8.19 ...\n - attr(*, \"Size\")= int 32\n - attr(*, \"Diag\")= logi FALSE\n - attr(*, \"Upper\")= logi FALSE\n - attr(*, \"method\")= chr \"euclidean\"\n - attr(*, \"call\")= language dist(x = .)\n```\n:::\n:::\n\n   \n\nThe 496 distances are:\n\n::: {.cell}\n\n```{.r .cell-code}\n32 * 31 / 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 496\n```\n:::\n:::\n\n \n\nthe number of ways of choosing 2 objects out of 32, when order does\nnot matter.\nFeel free to be offended by my choice of the letter `d` to\ndenote both data frames (that I didn't want to give a better name to)\nand dissimilarities in `dist` objects.\n\nYou can look at the whole thing if you like, though it is rather\nlarge. A `dist` object is stored internally as a long vector\n(here of 496 values); it's displayed as a nice triangle. The clue here\nis the thing called `Size`, which indicates that we have a\n$32\\times 32$ matrix of distances *between the 32 students*, so\nthat if we were to go on and do a cluster analysis based on this\n`d`, we'd get a clustering of the *students* rather than\nof the *beers*, as we want. (If you just print out `d`,\nyou'll see that is of distances between 32 (unlabelled) objects, which\nby inference must be the 32 students.)\n\nIt might be interesting to do a cluster analysis of the 32 students\n(it would tell you which of the students have similar taste in beer),\nbut that's not what we have in mind here.\n \n$\\blacksquare$\n\n(c) The R function `t()` *transposes* a matrix: that\nis, it interchanges rows and columns. Feed the transpose of your\nread-in beer ratings into `dist`. Does this now give\ndistances between beers?\n \nSolution\n\n\nAgain, omit the first column. The pipeline code looks a bit weird:\n\n::: {.cell}\n\n```{.r .cell-code}\nbeer %>%\n  select(-student) %>%\n  t() %>%\n  dist() -> d\n```\n:::\n\n   \n\nso you should feel free to do it in a couple of steps. This way shows\nthat you can also refer to columns by number:\n\n::: {.cell}\n\n```{.r .cell-code}\nbeer %>% select(-1) -> beer2\nd <- dist(t(beer2))\n```\n:::\n\n \n\nEither way gets you to the same place:\n\n::: {.cell}\n\n```{.r .cell-code}\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          AnchorS     Bass    Becks   Corona  GordonB Guinness Heineken   PetesW SamAdams\nBass     15.19868                                                                        \nBecks    16.09348 13.63818                                                               \nCorona   20.02498 17.83255 17.54993                                                      \nGordonB  13.96424 11.57584 14.42221 13.34166                                             \nGuinness 14.93318 13.49074 16.85230 20.59126 14.76482                                    \nHeineken 20.66398 15.09967 13.78405 14.89966 14.07125 18.54724                           \nPetesW   11.78983 14.00000 16.37071 17.72005 11.57584 14.28286 19.49359                  \nSamAdams 14.62874 11.61895 14.73092 14.93318 10.90871 15.90597 14.52584 14.45683         \nSierraN  12.60952 15.09967 17.94436 16.97056 11.74734 13.34166 19.07878 13.41641 12.12436\n```\n:::\n:::\n\n \n\nThere are 10 beers with these names, so this is good.\n \n$\\blacksquare$\n\n(d) Try to explain briefly why I used `as.dist` in the\nclass example (the languages one) but `dist` here. (Think\nabout the form of the input to each function.)\n \nSolution\n\n\n`as.dist` is used if you *already* have\ndissimilarities (and you just want to format them right), but\n`dist` is used if you have \n*data on variables* and you want to *calculate*\ndissimilarities. \n \n$\\blacksquare$\n\n(e) <a name=\"part:beer-dendro\">*</a> Obtain a clustering of the beers, using Ward's method. Show\nthe dendrogram.\n \nSolution\n\n\nThis:\n\n::: {.cell}\n\n```{.r .cell-code}\nbeer.1 <- hclust(d, method = \"ward.D\")\nplot(beer.1)\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/khas-1.pdf){fig-pos='H'}\n:::\n:::\n\n       \n$\\blacksquare$ \n\n(f) What seems to be a sensible number of clusters? Which\nbeers are in which cluster?\n \nSolution\n\n\nThis is a judgement call. Almost anything sensible is\nreasonable. I personally think that two clusters is good, beers\nAnchor Steam, Pete's Wicked Ale, Guinness and Sierra Nevada in\nthe first, and Bass, Gordon Biersch, Sam Adams, Corona, Beck's,\nand Heineken in the second.\nYou could make a case for three clusters, splitting off\nCorona, Beck's and Heineken  into their own cluster, or even\nabout 5 clusters as \nAnchor Steam, Pete's Wicked Ale; Guinness, Sierra Nevada; Bass,\nGordon Biersch, Sam Adams; Corona; Beck's, Heineken.\n\nThe idea is to have a number of clusters sensibly smaller than\nthe 10 observations, so that you are getting some actual\ninsight. Having 8 clusters for 10 beers wouldn't be very\ninformative! (This is where you use your own knowledge about\nbeer to help you rationalize your choice of number of clusters.) \n\nExtra: as to why the clusters split up like this, I think the four\nbeers on the left of my dendrogram are \"dark\" and the six on\nthe right are \"light\" (in colour), and I would expect the\nstudents to tend to like all the beers of one type and not so\nmuch all the beers of the other type.\n\nYou knew I would have to investigate this, didn't you? Let's aim\nfor a scatterplot of all the ratings for the dark  beers,\nagainst the ones for the light beers. \n\nStart with the data frame read in from the file:\n\n::: {.cell}\n\n```{.r .cell-code}\nbeer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 11\n   student AnchorS  Bass Becks Corona GordonB Guinness Heineken PetesW SamAdams SierraN\n   <chr>     <dbl> <dbl> <dbl>  <dbl>   <dbl>    <dbl>    <dbl>  <dbl>    <dbl>   <dbl>\n 1 S001          5     9     7      1       7        6        6      5        9       5\n 2 S008          7     5     6      8       8        4        8      8        7       7\n 3 S015          7     7     5      6       6        1        8      4        7       5\n 4 S022          7     7     5      2       5        8        4      6        8       9\n 5 S029          9     7     3      1       6        8        2      7        6       8\n 6 S036          7     6     4      3       7        6        6      5        4       9\n 7 S043          5     5     5      6       6        4        7      5        5       6\n 8 S050          5     3     1      5       5        5        3      5        5       9\n 9 S057          9     3     2      6       4        6        1      5        3       6\n10 S064          2     6     6      5       6        4        8      4        4       3\n# i 22 more rows\n```\n:::\n:::\n\n       \n\nThe aim is to find the average rating for a dark beer and a light beer\nfor each student, and then plot them against each other. Does a\nstudent who likes dark beer tend not to like light beer, and vice versa?\n\nLet's think about what to do first.\n\nWe need to: `pivot_longer` all the rating columns into one, labelled\nby `name` of beer. Then create a variable that is `dark`\nif we're looking at one of the dark beers and `light`\notherwise. `ifelse` works like \"if\" in a spreadsheet: a\nlogical thing that is either true or false, followed by a value if\ntrue and a value if false. There is a nice R command `%in%`\nwhich is `TRUE` if the thing in the first variable is to be\nfound somewhere in the list of things given next (here, one of the\napparently dark beers). (Another way to do this, which will appeal to\nyou more if you like databases, is to create a second data frame with\ntwo columns, the first being the beer names, and the second being\n`dark` or `light` as appropriate for that beer. Then you\nuse a \"left join\" to look up beer type from beer name.)\n\nNext, group by beer type within student. Giving two things to\n`group_by` does it this way: the second thing within \n(or \"for each of\") the first. \n\nThen calculate the mean\nrating within each group. This gives one column of students, one\ncolumn of beer types, \nand one column of rating means. \n\nThen we need to `pivot_wider` beer type\ninto two columns so that we can make a scatterplot of the mean ratings\nfor light and dark against\neach other. \n\nFinally, we make a scatterplot. \n\nYou'll see the final version of this that worked, but rest assured\nthat there were many intervening versions of this that didn't!\n\nI urge you to examine the chain one line at a time and see what each\nline does. That was how I debugged it.\n\nOff we go:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeer %>%\n  pivot_longer(-student, names_to=\"name\", values_to=\"rating\") %>%\n  mutate(beer.type = ifelse(name %in%\n    c(\"AnchorS\", \"PetesW\", \"Guinness\", \"SierraN\"), \"dark\", \"light\")) %>%\n  group_by(student, beer.type) %>%\n  summarize(mean.rat = mean(rating)) %>%\n  pivot_wider(names_from=beer.type, values_from=mean.rat) %>%\n  ggplot(aes(x = dark, y = light)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/iyrpoydf-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nAfter all that work, not really. There are some students who like\nlight beer but not dark beer (top left), there is a sort of vague\nstraggle down to the bottom right, where some students like dark beer\nbut not light beer, but there are definitely students at the top\nright, who just like beer! \n\nThe only really empty part of this plot is\nthe bottom left, which says that these students don't hate both kinds\nof beer; they like either dark beer, or light beer, or both.\n\nThe reason a `ggplot` fits into this \"workflow\" is that the\nfirst thing you feed into `ggplot` is a data frame, the one\ncreated by the chain here. Because it's in a pipeline, \nyou don't have the\nfirst thing on `ggplot`, so you can concentrate on the\n`aes` (\"what to plot\") and then the \"how to plot it\". \nNow back to your regularly-scheduled programming.\n \n$\\blacksquare$\n\n(g) Re-draw your dendrogram with your clusters indicated.\n \nSolution\n\n\n`rect.hclust`, with your chosen number  of clusters:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(beer.1)\nrect.hclust(beer.1, 2)\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/sdkjdh-1.pdf){fig-pos='H'}\n:::\n:::\n\n       \n\nOr if you prefer 5 clusters, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(beer.1)\nrect.hclust(beer.1, 5)\n```\n\n::: {.cell-output-display}\n![](kmeans-cluster_files/figure-pdf/ljashkjsdah-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nSame idea with any other number of clusters. If you follow through\nwith your preferred number of clusters from the previous part, I'm good.\n \n$\\blacksquare$\n\n(h) Obtain a K-means\nclustering with 2 clusters.^[If you haven't gotten to K-means clustering yet, leave this and save it for later.]\nNote that you will need to use the (transposed) \n*original  data*, not the distances. Use a suitably large value of\n`nstart`. (The data are ratings all on the same scale, so\nthere is no need for `scale` here. In case you were\nwondering.) \n \nSolution\n\n\n::: {.cell}\n\n:::\n\n       \nI used 20 for `nstart`. This is the pipe way:\n\n::: {.cell}\n\n```{.r .cell-code}\nbeer.2 <- beer %>%\n  select(-1) %>%\n  t() %>%\n  kmeans(2, nstart = 20)\n```\n:::\n\n       \n\nNot everyone (probably) will get the same answer, because of the\nrandom nature of the procedure, but the above code should be good\nwhatever output it produces.\n \n$\\blacksquare$\n\n(i) How many beers are in each cluster?\n \nSolution\n\n\nOn mine:\n\n::: {.cell}\n\n```{.r .cell-code}\nbeer.2$size\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6 4\n```\n:::\n:::\n\n       \n\nYou might get the same numbers the other way around.\n \n$\\blacksquare$\n\n(j) *Which* beers are in each cluster? You can do this\nsimply by obtaining the cluster memberships and using\n`sort` as in the last question, or you can do it as I did\nin class by obtaining the \nnames of the things to be clustered and picking out the ones of\nthem that are in cluster 1, 2, 3, \\ldots .)\n \nSolution\n\n\nThe cluster numbers of each beer are these:\n\n::: {.cell}\n\n```{.r .cell-code}\nbeer.2$cluster\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n AnchorS     Bass    Becks   Corona  GordonB Guinness Heineken   PetesW SamAdams  SierraN \n       2        1        1        1        1        2        1        2        1        2 \n```\n:::\n:::\n\n  \n\nThis is what is known in the business as a \"named vector\": it has values (the cluster numbers) and each value has a name attached to it (the name of a beer).\n\nNamed vectors are handily turned into a data frame with `enframe`:\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- enframe(beer.2$cluster)\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 2\n   name     value\n   <chr>    <int>\n 1 AnchorS      2\n 2 Bass         1\n 3 Becks        1\n 4 Corona       1\n 5 GordonB      1\n 6 Guinness     2\n 7 Heineken     1\n 8 PetesW       2\n 9 SamAdams     1\n10 SierraN      2\n```\n:::\n:::\n\nOr, to go back the other way, `deframe`:\n\n::: {.cell}\n\n```{.r .cell-code}\ndeframe(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n AnchorS     Bass    Becks   Corona  GordonB Guinness Heineken   PetesW SamAdams  SierraN \n       2        1        1        1        1        2        1        2        1        2 \n```\n:::\n:::\n\n \n\nor, give the columns better names and arrange them by cluster:\n\n::: {.cell}\n\n```{.r .cell-code}\nenframe(beer.2$cluster, name = \"beer\", value = \"cluster\") %>%\n  arrange(cluster)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 2\n   beer     cluster\n   <chr>      <int>\n 1 Bass           1\n 2 Becks          1\n 3 Corona         1\n 4 GordonB        1\n 5 Heineken       1\n 6 SamAdams       1\n 7 AnchorS        2\n 8 Guinness       2\n 9 PetesW         2\n10 SierraN        2\n```\n:::\n:::\n\n \n\nThese happen to be the same clusters as in my 2-cluster solution using\nWard's method.\n \n\n$\\blacksquare$\n\n\n",
    "supporting": [
      "kmeans-cluster_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}