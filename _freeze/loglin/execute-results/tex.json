{
  "hash": "6a3aa44249463fa17edb03ff0b6d4e53",
  "result": {
    "engine": "knitr",
    "markdown": "# Frequency table analysis\n\nPackages for this chapter:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  College plans\n\n\n 5199 male high school seniors in Wisconsin^[I don't  know why Wisconsin again, but that's what it is.] were classified by\nsocio-economic status (low, lower-middle, upper-middle, high), by\nthe degree that their parents encouraged them in their education (low\nor high),\nand whether or not they had plans to go to college (yes or no). How,\nif at all, are these categorical variables related? The data can be\nfound at\n[link](http://ritsokiguess.site/datafiles/college-plans.txt). \n\n\n\n(a) Read in the data and check that you have a column for each\nvariable and a column of frequencies.\n\n\n(b) Fit a log-linear model containing all possible\ninteractions. You don't need to examine it yet.\n\n\n(c) Find out which terms (interactions) could be removed. Do you\nthink removing any of them is a good idea?\n\n\n\n(d) Remove anything you can, and fit again. Hint: `update`.\n\n\n\n(e) Continue to examine what can be removed, and if reasonable,\nremove it, until you need to stop. Which terms are left in your final model?\n\n\n\n(f) Make suitable graphs of any remaining two-way interactions, and\ndescribe any relationships that you see.\n\n\n\n\n\n\n\n\n\n##  Predicting voting\n\n\n 1257 British voters were classified according\nto their social class, age (categorized), sex and the political party\nthey voted for (Labour or Conservative). Which, if any, of these\nfactors influences the party that someone votes for? The data are in\n[link](http://ritsokiguess.site/datafiles/voting.txt), one voter\nper line.\n\n\n\n(a) Read in the data and display (some of) the data frame.\n\n\n(b) There is no frequency column here, because each row of the\ndata frame only represents one voter. Count up the frequencies for\neach combo of the categorical variables, and save it (this is the\ndata frame that we will use for the analysis).\nDisplay the first few rows of the result. Do you now\nhave something that you need?\n\n\n(c) Fit a log-linear model with the appropriate interaction (as a\nstarting point).\n\n\n(d) Refine your model by taking out suitable non-significant\nterms, in multiple steps. What model do you finish with?\n\n\n(e) If we think of the party someone votes for as the final\noutcome (that depends on all the other things), what does our final\nmodel say that someone's vote depends on?\n\n\n(f) Obtain sub-tables that explain how `vote` depends on\nany of the things it's related to.\n\n\n\n\n## Brand M laundry detergent\n\n A survey was carried out comparing respondents' preferences of a laundry detergent M compared to a mystery brand X. For each respondent, the researchers recorded the temperature of the laundry load (low or high), whether or not they previously used brand M (yes or no), and the softness of the water used for the laundry load(hard, medium or soft). The aim of the survey was to find out what was associated with the respondents preferring brand M. The data are in <http://ritsokiguess.site/datafiles/brand_m.csv>.\n\n\n\n(a) Read in and display (some of) the data. Explain briefly how the data is laid out appropriately to fit a log-linear model.\n\n\n\n(b) Using backward elimination, build a suitable log-linear model for the associations between the variables. (Do *not* use `step`; do the elimination yourself).\n\n\n\n(c) What is associated with the brand a  respondent prefers? By obtaining suitable frequency tables, describe the nature of these associations.\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  College plans\n\n\n 5199 male high school seniors in Wisconsin^[I don't  know why Wisconsin again, but that's what it is.] were classified by\nsocio-economic status (low, lower-middle, upper-middle, high), by\nthe degree that their parents encouraged them in their education (low\nor high),\nand whether or not they had plans to go to college (yes or no). How,\nif at all, are these categorical variables related? The data can be\nfound at\n[link](http://ritsokiguess.site/datafiles/college-plans.txt). \n\n\n\n(a) Read in the data and check that you have a column for each\nvariable and a column of frequencies.\n\nSolution\n\n\nDelimited by one space:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/college-plans.txt\"\nwisc <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 16 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (3): social.stratum, encouragement, college.plans\ndbl (1): frequency\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nwisc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 16 x 4\n   social.stratum encouragement college.plans frequency\n   <chr>          <chr>         <chr>             <dbl>\n 1 lower          low           no                  749\n 2 lower          low           yes                  35\n 3 lower          high          no                  233\n 4 lower          high          yes                 133\n 5 lowermiddle    low           no                  627\n 6 lowermiddle    low           yes                  38\n 7 lowermiddle    high          no                  330\n 8 lowermiddle    low           no                  303\n 9 uppermiddle    low           no                  627\n10 uppermiddle    low           yes                  38\n11 uppermiddle    high          no                  374\n12 uppermiddle    high          yes                 467\n13 higher         low           no                  153\n14 higher         low           yes                  26\n15 higher         high          no                  266\n16 higher         high          yes                 800\n```\n\n\n:::\n:::\n\n     \n\nAs promised. We only have 16 observations, because we have all\npossible combinations of categorical variable combinations, 4 social\nstrata, times 2 levels of encouragement, times 2 levels of college\nplans. \n\nEach line of the data file summarizes a number of students, not just\none.  For example, the first line says that 749 students were in the\nlower social stratum, received low encouragement and have no college\nplans. If we sum up the frequencies, we should get 5199 because there\nwere that many students altogether:\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc %>% summarize(tot = sum(frequency))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 1\n    tot\n  <dbl>\n1  5199\n```\n\n\n:::\n:::\n\n \n\n$\\blacksquare$\n\n(b) Fit a log-linear model containing all possible\ninteractions. You don't need to examine it yet.\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.1 <- glm(frequency ~ social.stratum * encouragement * college.plans,\n  data = wisc, family = \"poisson\"\n)\n```\n:::\n\n   \n\n\n$\\blacksquare$\n\n(c) Find out which terms (interactions) could be removed. Do you\nthink removing any of them is a good idea?\n\n\nSolution\n\n\nThis is `drop1`. If you forget the `test=`, you won't\nget any P-values:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(wisc.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfrequency ~ social.stratum * encouragement * college.plans\n                                           Df Deviance    AIC   LRT Pr(>Chi)\n<none>                                          115.28 259.52               \nsocial.stratum:encouragement:college.plans  2   118.98 259.22 3.697   0.1575\n```\n\n\n:::\n:::\n\n   \n\nThis P-value is not small, so the three-way interaction can be removed.\n\n\n$\\blacksquare$\n\n(d) Remove anything you can, and fit again. Hint: `update`.\n\n\nSolution\n\n\nIn this kind of modelling, it's easier to describe what changes\nshould be  made to get from one model to another, rather than\nwriting out the whole thing from scratch again.\nAnyway, the three-way interaction can come out:\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.2 <- update(wisc.1, . ~ . - social.stratum:encouragement:college.plans)\n```\n:::\n\n   \n\n\n$\\blacksquare$\n\n(e) Continue to examine what can be removed, and if reasonable,\nremove it, until you need to stop. Which terms are left in your final model?\n\n\nSolution\n\n\nStart with `drop1`:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(wisc.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfrequency ~ social.stratum + encouragement + college.plans + \n    social.stratum:encouragement + social.stratum:college.plans + \n    encouragement:college.plans\n                             Df Deviance     AIC    LRT  Pr(>Chi)    \n<none>                            118.98  259.22                     \nsocial.stratum:encouragement  3   379.18  513.42 260.20 < 2.2e-16 ***\nsocial.stratum:college.plans  3   331.86  466.10 212.88 < 2.2e-16 ***\nencouragement:college.plans   1  1024.69 1162.94 905.72 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n   \n\nThese are all strongly significant, so they have to stay. There is\nnothing else we can remove. All the two-way interactions have to stay\nin the model.\n\n\n$\\blacksquare$\n\n(f) Make suitable graphs of any remaining two-way interactions, and\ndescribe any relationships that you see.\n\n\nSolution\n\n\nWe have three graphs to make.\n\nMy first one is social stratum by parental encouragement. Neither of\nthese is really a response, but I thought that social stratum would\ninfluence parental encouragement rather than the other way around, hence:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(wisc, aes(x = social.stratum, y = frequency, fill = encouragement)) +\n  geom_col(position = \"fill\")\n```\n\n::: {.cell-output-display}\n![](loglin_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-pos='H'}\n:::\n:::\n\nThe social strata came out in an illogical order (alphabetical), but if you look back at the data we read in, the order there is a much more logical one, hence:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(wisc, aes(x = fct_inorder(social.stratum), y = frequency, fill = encouragement)) +\n  geom_col(position = \"fill\")\n```\n\n::: {.cell-output-display}\n![](loglin_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n \n\nThis says that there tends to be more parental encouragement, the\nhigher the social stratum (for the most part). \n\nIf you don't like the $x$-axis label, define the `fct_inorder` thing into a new column first, and then plot that.\n\nNext, this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(wisc, aes(x = fct_inorder(social.stratum), y = frequency, fill = college.plans)) +\n  geom_col(position = \"fill\")\n```\n\n::: {.cell-output-display}\n![](loglin_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\nI expected the some problems with `social.stratum` again, so I took the same action. \nIn this one (and the next), `college.plans` is the response, so it gets to be `fill`.\n\nThe higher the social stratum, the more likely is a male\nhigh school senior to have plans to go to college. Once again, `lower` and `lowermiddle` are slightly out of order, but the overall pattern is clear enough.\n\nFinally, this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(wisc, aes(x = encouragement, y = frequency, fill = college.plans)) +\n  geom_col(position = \"fill\")\n```\n\n::: {.cell-output-display}\n![](loglin_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nAnd here you see an *enormous* effect of parental encouragement\non college plans: if it is low, the high-school senior is very\nunlikely to be considering college.\n\nNothing, in all honesty, that is very surprising here. But the two-way\ninteractions are easier to interpret than a three-way one would have\nbeen.\n\nHere, we think of college plans as being a response, and this\nanalysis has shown that whether or not a student has plans to go to\ncollege depends separately on the socio-economic status and the level\nof parental encouragement (rather than on the combination of both, as\nwould have been the case had the three-way interaction been\nsignificant). \n\n$\\blacksquare$\n\n\n\n\n\n\n\n##  Predicting voting\n\n\n 1257 British voters were classified according\nto their social class, age (categorized), sex and the political party\nthey voted for (Labour or Conservative). Which, if any, of these\nfactors influences the party that someone votes for? The data are in\n[link](http://ritsokiguess.site/datafiles/voting.txt), one voter\nper line.\n\n\n\n(a) Read in the data and display (some of) the data frame.\n\nSolution\n\n\nSpace-delimited:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/voting.txt\"\nvote0 <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 1257 Columns: 5\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (4): class, age, sex, vote\ndbl (1): id\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nvote0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,257 x 5\n      id class        age   sex    vote        \n   <dbl> <chr>        <chr> <chr>  <chr>       \n 1     1 upper middle >75   male   conservative\n 2     2 upper middle >75   male   conservative\n 3     3 upper middle >75   male   conservative\n 4     4 upper middle >75   male   conservative\n 5     5 upper middle >75   female conservative\n 6     6 upper middle >75   female conservative\n 7     7 upper middle >75   female conservative\n 8     8 upper middle >75   female conservative\n 9     9 upper middle >75   female conservative\n10    10 upper middle >75   female conservative\n# i 1,247 more rows\n```\n\n\n:::\n:::\n\n     \n\nI gave it a \"disposable\" name, since we make the \"real\" data set\nshortly. \n\n$\\blacksquare$\n\n(b) There is no frequency column here, because each row of the\ndata frame only represents one voter. Count up the frequencies for\neach combo of the categorical variables, and save it (this is the\ndata frame that we will use for the analysis).\nDisplay the first few rows of the result. Do you now\nhave something that you need?\n\nSolution\n\n\nI changed my mind about how to do this from last year. Using\n`count` is alarmingly more direct than the method I had before:\n\n::: {.cell}\n\n```{.r .cell-code}\nvotes <- vote0 %>% count(class, age, sex, vote)\nvotes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 58 x 5\n   class        age   sex    vote             n\n   <chr>        <chr> <chr>  <chr>        <int>\n 1 lower middle 26-40 female conservative    17\n 2 lower middle 26-40 female labour          13\n 3 lower middle 26-40 male   conservative    14\n 4 lower middle 26-40 male   labour          15\n 5 lower middle 41-50 female conservative    29\n 6 lower middle 41-50 female labour           4\n 7 lower middle 41-50 male   conservative    27\n 8 lower middle 41-50 male   labour          12\n 9 lower middle 51-75 female conservative    33\n10 lower middle 51-75 female labour           8\n# i 48 more rows\n```\n\n\n:::\n:::\n\n     \n\nExactly the right thing now: note the new column `n` with\nfrequencies in it. (Without a column of frequencies we can't fit a\nlog-linear model.) There are now only 58 combinations of the four\ncategorical variables, as opposed to 1247 rows in the original data\nset (with, inevitably, a lot of repeats).\n\n$\\blacksquare$\n\n(c) Fit a log-linear model with the appropriate interaction (as a\nstarting point).\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvote.1 <- glm(n ~ class * age * sex * vote, data = votes, family = \"poisson\")\n```\n:::\n\n     \n\n$\\blacksquare$\n\n(d) Refine your model by taking out suitable non-significant\nterms, in multiple steps. What model do you finish with?\n\nSolution\n\n\nAlternating `drop1` and `update` until everything\nremaining is significant:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(vote.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nn ~ class * age * sex * vote\n                   Df Deviance    AIC   LRT Pr(>Chi)\n<none>                   0.000 381.49               \nclass:age:sex:vote  7    8.086 375.58 8.086   0.3251\n```\n\n\n:::\n:::\n\n     \n\nNot anywhere near significant, so out it comes:\n\n::: {.cell}\n\n```{.r .cell-code}\nvote.2 <- update(vote.1, . ~ . - class:age:sex:vote)\ndrop1(vote.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nn ~ class + age + sex + vote + class:age + class:sex + age:sex + \n    class:vote + age:vote + sex:vote + class:age:sex + class:age:vote + \n    class:sex:vote + age:sex:vote\n               Df Deviance    AIC     LRT Pr(>Chi)  \n<none>               8.086 375.58                   \nclass:age:sex   8   11.244 362.74  3.1583  0.92404  \nclass:age:vote  7   21.962 375.46 13.8759  0.05343 .\nclass:sex:vote  2   10.142 373.64  2.0564  0.35765  \nage:sex:vote    4   14.239 373.73  6.1528  0.18802  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nTake out the first one, since it has the highest P-value:\n\n::: {.cell}\n\n```{.r .cell-code}\nvote.3 <- update(vote.2, . ~ . - class:age:sex)\ndrop1(vote.3, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nn ~ class + age + sex + vote + class:age + class:sex + age:sex + \n    class:vote + age:vote + sex:vote + class:age:vote + class:sex:vote + \n    age:sex:vote\n               Df Deviance    AIC     LRT Pr(>Chi)  \n<none>              11.244 362.74                   \nclass:age:vote  7   25.171 362.66 13.9262  0.05251 .\nclass:sex:vote  2   12.794 360.29  1.5498  0.46074  \nage:sex:vote    4   19.248 362.74  8.0041  0.09143 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\n`class:sex:vote`:\n\n::: {.cell}\n\n```{.r .cell-code}\nvote.4 <- update(vote.3, . ~ . - class:sex:vote)\ndrop1(vote.4, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nn ~ class + age + sex + vote + class:age + class:sex + age:sex + \n    class:vote + age:vote + sex:vote + class:age:vote + age:sex:vote\n               Df Deviance    AIC     LRT Pr(>Chi)  \n<none>              12.794 360.29                   \nclass:sex       2   13.477 356.97  0.6830  0.71070  \nclass:age:vote  7   26.698 360.19 13.9036  0.05292 .\nage:sex:vote    4   21.211 360.71  8.4172  0.07744 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\n`class:sex`:\n\n::: {.cell}\n\n```{.r .cell-code}\nvote.5 <- update(vote.4, . ~ . - class:sex)\ndrop1(vote.5, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nn ~ class + age + sex + vote + class:age + age:sex + class:vote + \n    age:vote + sex:vote + class:age:vote + age:sex:vote\n               Df Deviance    AIC     LRT Pr(>Chi)  \n<none>              13.477 356.97                   \nclass:age:vote  7   27.633 357.13 14.1555  0.04848 *\nage:sex:vote    4   22.081 357.57  8.6037  0.07181 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nI don't like having three-way interactions, so I'm going to yank\n`age:sex:vote` now, even though its P-value is smallish:\n\n::: {.cell}\n\n```{.r .cell-code}\nvote.6 <- update(vote.5, . ~ . - age:sex:vote)\ndrop1(vote.6, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nn ~ class + age + sex + vote + class:age + age:sex + class:vote + \n    age:vote + sex:vote + class:age:vote\n               Df Deviance    AIC     LRT  Pr(>Chi)    \n<none>              22.081 357.57                      \nage:sex         4   22.918 350.41  0.8372 0.9333914    \nsex:vote        1   33.018 366.51 10.9376 0.0009423 ***\nclass:age:vote  7   36.236 357.73 14.1555 0.0484843 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nThe age-sex interaction can go, but we must be near the end now:\n\n::: {.cell}\n\n```{.r .cell-code}\nvote.7 <- update(vote.6, . ~ . - age:sex)\ndrop1(vote.7, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nn ~ class + age + sex + vote + class:age + class:vote + age:vote + \n    sex:vote + class:age:vote\n               Df Deviance    AIC    LRT  Pr(>Chi)    \n<none>              22.918 350.41                     \nsex:vote        1   33.808 359.30 10.890 0.0009667 ***\nclass:age:vote  7   37.073 350.57 14.155 0.0484843 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nAnd that's it. The age and sex main effects are not included in the\nlist of droppable things because\nboth variables are part of higher-order interactions that are still in\nthe model.\n\nIf you want to, you can look at the `summary` of your final model:\n\n::: {.cell}\n\n:::\n\n \n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(vote.7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = n ~ class + age + sex + vote + class:age + class:vote + \n    age:vote + sex:vote + class:age:vote, family = \"poisson\", \n    data = votes)\n\nCoefficients: (1 not defined because of singularities)\n                                      Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                            2.50737    0.21613  11.601  < 2e-16 ***\nclassupper middle                     -0.45199    0.34188  -1.322 0.186151    \nclassworking                           0.37469    0.27696   1.353 0.176088    \nage>75                                -0.25783    0.32292  -0.798 0.424622    \nage26-40                               0.34294    0.27877   1.230 0.218619    \nage41-50                               0.93431    0.25162   3.713 0.000205 ***\nage51-75                               0.89794    0.25293   3.550 0.000385 ***\nsexmale                               -0.23242    0.08016  -2.900 0.003737 ** \nvotelabour                            -0.50081    0.33324  -1.503 0.132882    \nclassupper middle:age>75               0.25783    0.49713   0.519 0.604013    \nclassworking:age>75                    0.01097    0.41896   0.026 0.979113    \nclassupper middle:age26-40             0.82466    0.41396   1.992 0.046358 *  \nclassworking:age26-40                  0.41083    0.35167   1.168 0.242713    \nclassupper middle:age41-50             0.37788    0.39239   0.963 0.335542    \nclassworking:age41-50                 -0.28917    0.33310  -0.868 0.385327    \nclassupper middle:age51-75             0.43329    0.39277   1.103 0.269954    \nclassworking:age51-75                  0.10223    0.32668   0.313 0.754325    \nclassupper middle:votelabour          -0.12338    0.53898  -0.229 0.818936    \nclassworking:votelabour                1.05741    0.39259   2.693 0.007073 ** \nage>75:votelabour                     -0.72300    0.57745  -1.252 0.210547    \nage26-40:votelabour                    0.21667    0.41944   0.517 0.605452    \nage41-50:votelabour                   -0.93431    0.43395  -2.153 0.031315 *  \nage51-75:votelabour                   -0.62601    0.41724  -1.500 0.133526    \nsexmale:votelabour                     0.37323    0.11334   3.293 0.000992 ***\nclassupper middle:age>75:votelabour         NA         NA      NA       NA    \nclassworking:age>75:votelabour        -0.29039    0.68720  -0.423 0.672607    \nclassupper middle:age26-40:votelabour -0.53698    0.65445  -0.821 0.411931    \nclassworking:age26-40:votelabour      -0.28479    0.49429  -0.576 0.564516    \nclassupper middle:age41-50:votelabour -0.01015    0.68338  -0.015 0.988147    \nclassworking:age41-50:votelabour       1.06121    0.50772   2.090 0.036603 *  \nclassupper middle:age51-75:votelabour -0.06924    0.65903  -0.105 0.916328    \nclassworking:age51-75:votelabour       0.16608    0.49036   0.339 0.734853    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 797.594  on 57  degrees of freedom\nResidual deviance:  22.918  on 27  degrees of freedom\nAIC: 350.41\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n \n\nThese tend to be rather unwieldy, and we'll see a better way of\nunderstanding the results below, but you can look for the very\nsignificant results, bearing in mind that the first category is the\nbaseline, for example, more of the males in the survey voted Labour\n(than Conservative).\n\n$\\blacksquare$\n\n(e) If we think of the party someone votes for as the final\noutcome (that depends on all the other things), what does our final\nmodel say that someone's vote depends on?\n\nSolution\n\n\nFind out which of the surviving terms are interactions with\n`vote`. Here, there are two things, that `vote`\ndepends on separately:\n\n\n* `sex`\n\n* The `age`-`class` interaction.\n\n\n$\\blacksquare$\n\n(f) Obtain graphs that help you explain how `vote` depends on\nany of the things it's related to.\n\nSolution\n\nThe 3-way interaction is a bit\ntricky, so we'll do the simpler one first:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(votes, aes(x = sex, y = n, fill = vote)) + \n  geom_col(position = \"fill\")\n```\n\n::: {.cell-output-display}\n![](loglin_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe female voters slightly preferred to vote Conservative^[The colours came out the wrong way around: the Conservative colour is blue and the Labour colour is red, like the Conservatives and Liberals here.] and the male\nvoters slightly preferred to vote Labour. This is a small effect, but\nI guess the large number of voters made it big enough to be significant.\n\nI took it this way around because `vote` is the outcome, and\ntherefore I want to address things \nlike \"if a voter is female, how likely are they to vote Labour\", \nrather than conditioning the other\nway around (which would \nbe \"if a voter voted Labour, how likely are they to be female\", \nwhich doesn't make nearly so much sense). \n\nThen the tricky one. For this, we need to choose one  of the explanatory variables to be `x` and the other one to be `facets`. There is no particular reason to do it one way around rather than the other; experiment with switching `x` and the variable in `facet_wrap` around and see whether it's easier to interpret:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(votes, aes(x = age, y = n, fill = vote)) +\n  geom_col(position = \"fill\") + facet_wrap(~ class)\n```\n\n::: {.cell-output-display}\n![](loglin_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\nThe age and class categories are in an illogical alphabetical order, but they were out of order in the original dataset too, so I guess we have to live with what we see here.\n\nDoing it this way has produced different graphs for each\n`class`. This is actually OK, because we can \nsay \"if a voter was of lower middle class\" and then talk about the relationship\nbetween age and vote, as if we were looking at a simple effect:\n\n\n\n* If a voter was of lower-middle-class, they strongly favour voting\nConservative in all age groups except for `<26` and 26--40.\n\n* If a voter was of upper-middle-class, they even more strongly favour\nvoting Conservative in all age groups except for \"under 26\" and\nmaybe 26--40.\n\n* If a voter was of Working class, they strongly favour voting\nLabour, except in the \n\"over 75\" age group (and maybe 51--75 as well).\n\n\nIf the anomalous age group(s) had been the same one every time, there\nwould no longer have been an interaction between age and class in\ntheir effect on `vote`. But the anomalous age groups were\ndifferent for each class (\"different pattern\"), and that explains\nwhy there was a `vote:age:class` interaction: \" the way someone votes depends on the *combination* of age and social class\". \n\nExtra 1: I have been using the dataframe with frequencies in it, and telling you to use `geom_col` rather than the `geom_bar` you might have been expecting, which is because we have frequencies and each row of the dataframe represents a bunch of people rather than just one. But let's remind ourselves of the dataframe we had called vote0:\n\n::: {.cell}\n\n```{.r .cell-code}\nvote0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,257 x 5\n      id class        age   sex    vote        \n   <dbl> <chr>        <chr> <chr>  <chr>       \n 1     1 upper middle >75   male   conservative\n 2     2 upper middle >75   male   conservative\n 3     3 upper middle >75   male   conservative\n 4     4 upper middle >75   male   conservative\n 5     5 upper middle >75   female conservative\n 6     6 upper middle >75   female conservative\n 7     7 upper middle >75   female conservative\n 8     8 upper middle >75   female conservative\n 9     9 upper middle >75   female conservative\n10    10 upper middle >75   female conservative\n# i 1,247 more rows\n```\n\n\n:::\n:::\n\nThis has exactly one voter per row (there were 1257 people in the original survey), and so with this one you *can* use `geom_bar` to get the same graphs we got above using `geom_col`. For example:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(vote0, aes(x = age, fill = vote)) +\n  geom_bar(position = \"fill\") + facet_wrap(~ class)\n```\n\n::: {.cell-output-display}\n![](loglin_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n`geom_bar` doesn't allow a `y`, but `vote0` doesn't have the frequency column that we were using for the `y`, so we just take it out.\n\nExtra 2: The Labour Party in the UK is like the NDP here, in that it has strong\nties with \"working people\", trades unions in particular. The\nConservatives are like the Conservatives here (indeed, the nickname\n\"Tories\" comes from the UK; the Conservatives there were officially\nknown as the Tories many years ago). Many people are lifelong voters\nfor their party, and would never think of voting for the \"other side\", \nin the same way that many Americans vote either Democrat or\nRepublican without thinking about it too much. Our parliamentary\nsystem comes from the UK system (vote for a candidate in a riding, the\nleader of the party with the most elected candidates becomes Prime\nMinister), and a \"landslide\" victory often comes from persuading\nenough of the voters open to persuasion to switch sides. In the UK, as\nhere, the parties' share of the popular vote doesn't change all that\nmuch from election to election, even though the number of seats in\nParliament might change quite a lot. Provincial elections in Canada work the same way; you might remember that the current (as of 2022) Conservative majority in Ontario came from less than 40% of the vote.\n\n$\\blacksquare$\n\n\n\n## Brand M laundry detergent\n\n A survey was carried out comparing respondents' preferences of a laundry detergent M compared to a mystery brand X. For each respondent, the researchers recorded the temperature of the laundry load (low or high), whether or not they previously used brand M (yes or no), and the softness of the water used for the laundry load(hard, medium or soft). The aim of the survey was to find out what was associated with the respondents preferring brand M. The data are in <http://ritsokiguess.site/datafiles/brand_m.csv>.\n\n\n\n(a) Read in and display (some of) the data. Explain briefly how the data is laid out appropriately to fit a log-linear model.\n\nSolution\n\n\nThe reading-in is entirely familiar:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/brand_m.csv\"\nprefs <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 24 Columns: 5\n-- Column specification ------------------------------------------------------------------\nDelimiter: \",\"\nchr (4): softness, m_user, temperature, prefer\ndbl (1): frequency\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nprefs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 24 x 5\n   softness m_user temperature prefer frequency\n   <chr>    <chr>  <chr>       <chr>      <dbl>\n 1 hard     no     low         x             68\n 2 hard     no     low         m             42\n 3 hard     no     high        x             42\n 4 hard     no     high        m             30\n 5 hard     yes    low         x             37\n 6 hard     yes    low         m             52\n 7 hard     yes    high        x             24\n 8 hard     yes    high        m             43\n 9 medium   no     low         x             66\n10 medium   no     low         m             50\n# i 14 more rows\n```\n\n\n:::\n:::\n\nThis is good because we have each \"observation\" (frequency, here) in one row, or, said differently, we have a column of frequencies and each of the factors in a column of its own. (See the Extra for the kind of layout we might have had to deal with.)\n\nExtra: as you might expect, this is very much *not* how the data came to me. It was originally in a textbook, laid out like this:\n\n![](brand_m.jpg) \n\nThis is a common layout for frequency data, because it saves a lot of space. Multiple header rows are hard to deal with, though, so I combined the three column variables into one with a layout like this (aligned in columns):\n\n```\nsoftness no_low_x no_low_m no_high_x no_high_m yes_low_x yes_low_m yes_high_x yes_high_m\nhard        68       42        42       30         37        52        24        43\nmedium      66       50        33       23         47        55        23        47\nsoft        63       53        29       27         57        49        19        29\n```\n\nLet's read this in and then think about what to do with it:\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs0 <- read_table(\"brand_m.txt\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n-- Column specification ------------------------------------------------------------------\ncols(\n  softness = col_character(),\n  no_low_x = col_double(),\n  no_low_m = col_double(),\n  no_high_x = col_double(),\n  no_high_m = col_double(),\n  yes_low_x = col_double(),\n  yes_low_m = col_double(),\n  yes_high_x = col_double(),\n  yes_high_m = col_double()\n)\n```\n\n\n:::\n\n```{.r .cell-code}\nprefs0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 9\n  softness no_low_x no_low_m no_high_x no_high_m yes_low_x yes_low_m yes_high_x yes_high_m\n  <chr>       <dbl>    <dbl>     <dbl>     <dbl>     <dbl>     <dbl>      <dbl>      <dbl>\n1 hard           68       42        42        30        37        52         24         43\n2 medium         66       50        33        23        47        55         23         47\n3 soft           63       53        29        27        57        49         19         29\n```\n\n\n:::\n:::\n\nWe want to get all those frequencies into one column, which suggest some kind of `pivot_longer.`There are two ways to go about this. One is to try a regular `pivot_longer`and see what happens. I had to think for a moment about what to call the column that ended up as `combo`:\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs0 %>% pivot_longer(-softness, names_to = \"combo\", values_to = \"frequency\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 24 x 3\n   softness combo      frequency\n   <chr>    <chr>          <dbl>\n 1 hard     no_low_x          68\n 2 hard     no_low_m          42\n 3 hard     no_high_x         42\n 4 hard     no_high_m         30\n 5 hard     yes_low_x         37\n 6 hard     yes_low_m         52\n 7 hard     yes_high_x        24\n 8 hard     yes_high_m        43\n 9 medium   no_low_x          66\n10 medium   no_low_m          50\n# i 14 more rows\n```\n\n\n:::\n:::\n\nThis is the right kind of shape, but those things in the column `combo` are three variables all smooshed together: respectively, previous user (of brand M), temperature, preference (you can tell by the values, which are all different). These can be split up with `separate`, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs0 %>% pivot_longer(-softness, names_to = \"combo\", values_to = \"frequency\") %>% \n  separate_wider_delim(combo, \"_\", \n                       names = c(\"prev_user\", \"temperature\", \"preference\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 24 x 5\n   softness prev_user temperature preference frequency\n   <chr>    <chr>     <chr>       <chr>          <dbl>\n 1 hard     no        low         x                 68\n 2 hard     no        low         m                 42\n 3 hard     no        high        x                 42\n 4 hard     no        high        m                 30\n 5 hard     yes       low         x                 37\n 6 hard     yes       low         m                 52\n 7 hard     yes       high        x                 24\n 8 hard     yes       high        m                 43\n 9 medium   no        low         x                 66\n10 medium   no        low         m                 50\n# i 14 more rows\n```\n\n\n:::\n:::\n\nThat works, but the combination of `pivot_longer` and a `separate` is a common one, and so there is an \"advanced\" version of `pivot_longer` that does it all at once. The idea is that you enter three columns into `names_to` and then use `names_sep` to say what they're separated by:\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs0 %>% \n  pivot_longer(-softness, \n    names_to = c(\"m_user\", \"temperature\", \"prefer\"),\n    names_sep = \"_\", values_to = \"frequency\") \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 24 x 5\n   softness m_user temperature prefer frequency\n   <chr>    <chr>  <chr>       <chr>      <dbl>\n 1 hard     no     low         x             68\n 2 hard     no     low         m             42\n 3 hard     no     high        x             42\n 4 hard     no     high        m             30\n 5 hard     yes    low         x             37\n 6 hard     yes    low         m             52\n 7 hard     yes    high        x             24\n 8 hard     yes    high        m             43\n 9 medium   no     low         x             66\n10 medium   no     low         m             50\n# i 14 more rows\n```\n\n\n:::\n:::\n\nThis data frame is what I saved for you.\n\n\n$\\blacksquare$\n\n\n(b) Using backward elimination, build a suitable log-linear model for the associations between the variables. (Do *not* use `step`; do the elimination yourself).\n\nSolution\n\n\nThe first step is to fit a model containing all the interactions between the factors, using `frequency` as the response, and then to use `drop1` with `test=\"Chisq\"` to see what can come out. Don't forget the `family = \"poisson\"`, since that's what drives the modelling. I tnink it's easiest to number these models, since there might be a lot of them:\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs.1 <- glm(frequency~softness*m_user*temperature*prefer, family = \"poisson\", data=prefs)\ndrop1(prefs.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfrequency ~ softness * m_user * temperature * prefer\n                                   Df Deviance    AIC     LRT Pr(>Chi)\n<none>                                 0.00000 180.41                 \nsoftness:m_user:temperature:prefer  2  0.73732 177.15 0.73732   0.6917\n```\n\n\n:::\n:::\n\nTo our relief, the four-way interaction is not significant and can be removed. (I was *not* looking forward to the prospect of interpreting that!)\n\nNow write an `update` line that removes that four-way interaction from your model, as shown below, and copy-paste your `drop1` line from above, changing  the number  of your model to the one coming out of `update`. Copy-paste the complicated interaction from the `drop1` output:\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs.2 <- update(prefs.1, .~.-softness:m_user:temperature:prefer)\ndrop1(prefs.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfrequency ~ softness + m_user + temperature + prefer + softness:m_user + \n    softness:temperature + m_user:temperature + softness:prefer + \n    m_user:prefer + temperature:prefer + softness:m_user:temperature + \n    softness:m_user:prefer + softness:temperature:prefer + m_user:temperature:prefer\n                            Df Deviance    AIC    LRT Pr(>Chi)\n<none>                           0.7373 177.15                \nsoftness:m_user:temperature  2   2.1146 174.53 1.3773   0.5022\nsoftness:m_user:prefer       2   5.3086 177.72 4.5713   0.1017\nsoftness:temperature:prefer  2   0.8991 173.31 0.1618   0.9223\nm_user:temperature:prefer    1   2.9594 177.37 2.2220   0.1361\n```\n\n\n:::\n:::\n\nThere are now four three-way interactions that could be removed. You might suspect that they are all going to go eventually, but as in regression, we take them one at a time, starting with the one that has the highest P-value (just in case, for example, the P-value of the second one goes under 0.05 when we remove the others). The easiest way to do the coding is a vigorous amount of copying and pasting. Copy-paste your last code chunk:\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs.2 <- update(prefs.1, .~.-softness:m_user:temperature:prefer)\ndrop1(prefs.2, test = \"Chisq\")\n```\n:::\n\nChange the interaction in the `update` to the one you want to remove (from the `drop1` table), which is `softness:temperature:prefer` (you can copy-paste that too), and then increase all three of the model numbers by 1:\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs.3 <- update(prefs.2, .~.-softness:temperature:prefer)\ndrop1(prefs.3, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfrequency ~ softness + m_user + temperature + prefer + softness:m_user + \n    softness:temperature + m_user:temperature + softness:prefer + \n    m_user:prefer + temperature:prefer + softness:m_user:temperature + \n    softness:m_user:prefer + m_user:temperature:prefer\n                            Df Deviance    AIC    LRT Pr(>Chi)\n<none>                           0.8991 173.31                \nsoftness:m_user:temperature  2   2.2506 170.67 1.3514   0.5088\nsoftness:m_user:prefer       2   5.4952 173.91 4.5961   0.1005\nm_user:temperature:prefer    1   3.1148 173.53 2.2157   0.1366\n```\n\n\n:::\n:::\n\nThen, as they say, rinse and repeat. This one takes a while, but each step is just like the others.\n\ndrop `softness:m_user:temperature`\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs.4 <- update(prefs.3, .~.-softness:m_user:temperature)\ndrop1(prefs.4, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfrequency ~ softness + m_user + temperature + prefer + softness:m_user + \n    softness:temperature + m_user:temperature + softness:prefer + \n    m_user:prefer + temperature:prefer + softness:m_user:prefer + \n    m_user:temperature:prefer\n                          Df Deviance    AIC    LRT Pr(>Chi)  \n<none>                         2.2506 170.67                  \nsoftness:temperature       2   7.8155 172.23 5.5649  0.06189 .\nsoftness:m_user:prefer     2   7.0585 171.47 4.8080  0.09036 .\nm_user:temperature:prefer  1   4.5184 170.93 2.2678  0.13209  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\ndrop `m_user:temperature:prefer`\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs.5 <- update(prefs.4, .~.-m_user:temperature:prefer)\ndrop1(prefs.5, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfrequency ~ softness + m_user + temperature + prefer + softness:m_user + \n    softness:temperature + m_user:temperature + softness:prefer + \n    m_user:prefer + temperature:prefer + softness:m_user:prefer\n                       Df Deviance    AIC    LRT Pr(>Chi)  \n<none>                      4.5184 170.93                  \nsoftness:temperature    2  10.6035 173.02 6.0851  0.04771 *\nm_user:temperature      1   5.2477 169.66 0.7293  0.39311  \ntemperature:prefer      1   8.2467 172.66 3.7283  0.05350 .\nsoftness:m_user:prefer  2   9.8462 172.26 5.3278  0.06967 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\ndrop `m_user:temperature`\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs.6 <- update(prefs.5, .~.-m_user:temperature)\ndrop1(prefs.6, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfrequency ~ softness + m_user + temperature + prefer + softness:m_user + \n    softness:temperature + softness:prefer + m_user:prefer + \n    temperature:prefer + softness:m_user:prefer\n                       Df Deviance    AIC    LRT Pr(>Chi)  \n<none>                      5.2477 169.66                  \nsoftness:temperature    2  11.2951 171.71 6.0474  0.04862 *\ntemperature:prefer      1   9.5576 171.97 4.3099  0.03789 *\nsoftness:m_user:prefer  2  10.5860 171.00 5.3383  0.06931 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\ndrop `softness:m_user:prefer`\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs.7 <- update(prefs.6, .~.-softness:m_user:prefer)\ndrop1(prefs.7, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfrequency ~ softness + m_user + temperature + prefer + softness:m_user + \n    softness:temperature + softness:prefer + m_user:prefer + \n    temperature:prefer\n                     Df Deviance    AIC     LRT  Pr(>Chi)    \n<none>                    10.586 171.00                      \nsoftness:m_user       2   11.543 167.96  0.9569   0.61974    \nsoftness:temperature  2   16.633 173.05  6.0474   0.04862 *  \nsoftness:prefer       2   10.798 167.21  0.2120   0.89942    \nm_user:prefer         1   31.049 189.46 20.4633 6.079e-06 ***\ntemperature:prefer    1   14.896 173.31  4.3099   0.03789 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nMore two-ways.\n\ndrop `softness:prefer`\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs.8 <- update(prefs.7, .~.-softness:prefer)\ndrop1(prefs.8, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfrequency ~ softness + m_user + temperature + prefer + softness:m_user + \n    softness:temperature + m_user:prefer + temperature:prefer\n                     Df Deviance    AIC     LRT  Pr(>Chi)    \n<none>                    10.798 167.21                      \nsoftness:m_user       2   11.886 164.30  1.0885   0.58027    \nsoftness:temperature  2   16.910 169.33  6.1125   0.04706 *  \nm_user:prefer         1   31.393 185.81 20.5949 5.675e-06 ***\ntemperature:prefer    1   15.173 169.59  4.3750   0.03647 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nOne more?\n\ndrop `softness:m_user`\n\n::: {.cell}\n\n```{.r .cell-code}\nprefs.9 <- update(prefs.8, .~.-softness:m_user)\ndrop1(prefs.9, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nfrequency ~ softness + m_user + temperature + prefer + softness:temperature + \n    m_user:prefer + temperature:prefer\n                     Df Deviance    AIC     LRT  Pr(>Chi)    \n<none>                    11.886 164.30                      \nsoftness:temperature  2   17.986 166.40  6.0991   0.04738 *  \nm_user:prefer         1   32.468 182.88 20.5815 5.715e-06 ***\ntemperature:prefer    1   16.248 166.66  4.3616   0.03676 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nAnd finally we are done! There are three two-way interactions left, which shouldn't be too hard to interpret. That's in the next part.\n\n\n\n$\\blacksquare$\n\n\n(c) What is associated with the brand a  respondent prefers? By obtaining suitable graphs, describe the nature of these associations.\n\nSolution\n\n\nTo see what is associated with brand preference, look for significant associations with `prefer`. There are two of them, one with `m_user`, and, separately, one with temperature. This means that a respondent's brand preference depends on whether or not they previously used brand M, and also on what temperature the laundry was washed at.\n\nTo investigate these, we need the variation on the two-variable bar chart. Here's `m_user` (explanatory) with `prefer` (response):\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(prefs, aes(x = m_user, y = frequency, fill = prefer)) + \n  geom_col(position = \"fill\")\n```\n\n::: {.cell-output-display}\n![](loglin_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n:::\n\nOut of the people who were previous users of Brand M (on the right), slightly more of them preferred Brand M; out of the people who were not Brand M users (on the left), somewhat more of them preferred Brand X.\n\nAdvertisers use terms like \"brand familiarity\" to capture ideas like this: more people prefer Brand M in the survey if they have used it before. Not altogether surprising. \n\n\nOn to the effects of temperature on preference:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(prefs, aes(x = temperature, y = frequency, fill = prefer)) +\n  geom_col(position = \"fill\")\n```\n\n::: {.cell-output-display}\n![](loglin_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nOut of the people who used a high-temperature wash, a small majority of them preferred brand M, but out of the people who used a low-temperature wash, a small majority of them preferred brand X.\n\nI'm making it seem like this is a big difference, and of course it's a very small one, but the size of the survey makes even this tiny difference significant. \n\nThose are really the two effects of interest, since they are the ones associated with brand preference. But there was one more association that was significant: between temperature and softness. The softness in this case was of the water used to do the laundry (and not, for example, the softness of the clothes after they come out of the dryer).  There isn't really any reason to pick one of these as the response, but we have to make a choice for the graph. I decided to condition on the softness of the water, since that cannot be controlled by the person doing the laundry (the water just *is* hard or soft, depending on where you live and where your water comes from). \n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(prefs, aes(x = softness, y = frequency, fill = temperature)) +\n  geom_col(position = \"fill\")\n```\n\n::: {.cell-output-display}\n![](loglin_files/figure-pdf/unnamed-chunk-12-1.pdf){fig-pos='H'}\n:::\n:::\n\nIn each case, a majority of the washes were done at low temperature, but the softer the water, the bigger that majority was. Once again, the effect is not all that big, because the association was only just significant.\n\n$\\blacksquare$\n",
    "supporting": [
      "loglin_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}