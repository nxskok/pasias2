{
  "hash": "92217abd7ed6d7a74493aebd3649f0ad",
  "result": {
    "markdown": "# Analysis of variance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Movie ratings and lengths\n\n\n Before a movie is shown in theatres, it\nreceives a \"rating\" that says what kind of material it\ncontains. [link](https://en.wikipedia.org/wiki/Motion_Picture_Association_of_America_film_rating_system)\nexplains the categories, from G (suitable for children) to R (anyone\nunder 17 must be accompanied by parent/guardian). In 2011, two\nstudents collected data on the length (in minutes) and the rating\ncategory, for 15 movies of each rating category, randomly chosen from\nall the movies released that year. The data are at\n[link](http://ritsokiguess.site/datafiles/movie-lengths.csv).\n\n\n\n(a) Read the data into R, and display (some of) what you read in.\n\n\n\n(b) Count how many movies there are of each rating.\n\n\n\n(c) Carry out an ANOVA and a Tukey\nanalysis (if warranted).\n  \n\n\n\n(d) Make a graph to assess whether this ANOVA is\ntrustworthy. Discuss your graph and its implications briefly.\n\n\n\n\n\n\n\n##  Deer and how much they eat\n\n\n Do adult deer eat different amounts of food at different\ntimes of the year? The data in\n[link](http://ritsokiguess.site/datafiles/deer.txt) are the weights\nof food (in kilograms) consumed by randomly selected adult deer\nobserved at different times of the year (in February, May, August and\nNovember). We will assume that these were different deer observed in\nthe different months. (If the same animals had been observed at\ndifferent times, we would have been in the domain of \n\"repeated measures\", which would require a different analysis, \nbeyond the scope\nof this course.)\n\n\n\n(a) Read the data into R, and calculate numbers of observations\nand the median amounts of food\neaten each month.\n \n\n(b) Make side-by-side boxplots of the amount of food eaten each\nmonth. Comment briefly on what you see. \n \n\n(c) Run a Mood's median test as in lecture (ie.\\ not using\n`smmr`). What do you conclude, in the context of the data? \n \n\n(d) Run a Mood's median test using `smmr`, and compare the\nresults with the previous part.\n \n\n(e) How is it that Mood's median test does not completely answer the\nquestion you really want to answer? How might you get an answer to the\nquestion you *really* want answered? Explain briefly, and obtain\nthe answer you *really* want, discussing your results briefly.\n\n\n\n\n\n\n\n\n##  Movie ratings again\n\n\n This question again uses the movie rating data at\n[link](http://ritsokiguess.site/datafiles/movie-lengths.csv).\n\n\n\n(a) Read the data into R and obtain the number of movies of\neach rating and the *median* length of movies of each rating.\n\n\n\n(b) Obtain a suitable graph that assesses the assumptions for\nANOVA. Why do you think it is not reasonable to run ANOVA here? Explain\nbriefly. \n\n\n\n(c) Run a Mood's median test (use `smmr` if you\nlike). What do you conclude, in the context of the data?\n\n\n\n\n\n\n\n\n##  Atomic weight of carbon\n\n\n The atomic weight of the chemical element\ncarbon is 12. Two methods of measuring the atomic weight of samples of\ncarbon were compared. The results are shown in\n[link](http://ritsokiguess.site/datafiles/carbon.txt). The methods\nare labelled 1 and 2.  The first task is to find out whether the two\nmethods have different \"typical\" measures (mean or median, as\nappropriate) of the atomic weight of carbon.\n\nFor this question, compose a report in a Quarto document. \nSee part (a) for how to get this started.\n\nYour report should\nread like an actual report, not just the answers to some questions\nthat I set you. To help with that, write some text that links the\nparts of the report together smoothly, so that it reads as a coherent\nwhole. The grader had 3 discretionary marks to award for the overall\nquality of your writing. The scale for this was:\n\n\n\n* 3 points: excellent writing. The report flows smoothly, is easy\nto read, and contains everything it should (and nothing it\nshouldn't).\n\n* 2 points: satisfactory writing. Not the easiest to read, but\nsays what it should, and it looks at least somewhat like a report\nrather than a string of answers to questions.\n\n* 1 point: writing that is hard to read or to understand. If you\nget this (or 0), you should consider what you need to do to improve\nwhen you write your project.\n\n* 0 points: you answered the questions, but you did almost nothing\nto make it read like a report.\n\n\n\n(a) Create a new Quarto document. To do this, in R Studio, select File,\nNew File, Quarto Document. Type the report title and your name in the\nboxes, and leave the output on the default HTML. Click OK. \n\n\n(b) Write an introduction that explains the purpose of this\nstudy and the data collected in your own words.\n\n\n\n(c) Begin an appropriately-titled new section in your report,\nread the data into R and display the results.\n\n\n\n(d) Make an appropriate plot to compare the measurements\nobtained by the two methods. You might need to do something about\nthe two methods being given as numbers even though they are really\nonly identifiers. (If you do, your report ought to say what you did\nand why.)\n\n\n\n(e) Comment briefly on what you see in your plot.\n\n\n\n(f) Carry out the most appropriate $t$-test. (You might like to\nbegin another new section in your report here.)\n\n\n\n(g) Do the most appropriate test you know that does not assume\nnormally-distributed data.\n\n\n\n(h) Discuss the results of your tests and what they say about\nthe two methods for measuring the atomic weight of carbon. If it\nseems appropriate, put the discussion into a section called\nConclusions. \n\n\n\n\n\n\n\n##  Can caffeine improve your performance on a test?\n\n\n \nDoes caffeine help students do better on a certain test? To\nfind out, 36 students were randomly allocated to three groups (12 in\neach group).  Each student received a fixed number of cups of coffee\nwhile they were studying, but the students didn't know whether they\nwere receiving all full-strength coffee (\"high\"), all decaf coffee\n(\"low\") or a 50-50 mixture of the two (\"moderate\"). For each\nsubject, their group was recorded as well as their score on the\ntest. The data are in\n[link](http://ritsokiguess.site/datafiles/caffeine.csv), as a\n`.csv` file.\n\n\n\n(a) Read in and examine the data. How are the values laid out?\n\n\n\n(b) Explain briefly how the data are not \"tidy\".\n\n\n\n(c) Use a suitable tool from the `tidyverse` to create one\ncolumn of test scores and and one column of group labels. Call your\ncolumn of group labels `amount`. Is it a `factor`?\n\n\n\n(d) Obtain side-by-side boxplots of test scores by amount of caffeine.\n\n\n\n(e) Does caffeine amount seem to have an effect? If so, what\nkind of effect?\n\n\n\n(f) Run a suitable analysis of variance to determine whether\nthe mean test score is equal or unequal for the three groups. What\ndo you conclude?\n\n\n\n(g) Why is it a good idea to run Tukey's method here?\n \n\n\n(h) Run Tukey's method. What do you conclude?\n\n\n\n\n\n## Reggae music\n\n Reggae is a music genre that originated in Jamaica in the late 1960s. One of the most famous reggae bands was Bob Marley and the Wailers. \nIn a survey, 729 students were asked to rate reggae music on a scale from 1, \"don't like it at all\" to 6, \"like it a lot\". \nWe will treat the ratings as quantitative.\nEach student was also asked to classify their home town as one of \"big city\", \"suburban\", \"small town\", \"rural\". Does a student's opinion of reggae depend on the kind of home town they come from? The data are in [http://ritsokiguess.site/datafiles/reggae.csv](http://ritsokiguess.site/datafiles/reggae.csv). \n\n\n\n(a) Read in and display (some of) the data.\n\n\n\n(b) How many students are from each different size of town?\n\n\n\n(c) Make a suitable graph of the two variables in this data frame. \n\n\n\n(d) Discuss briefly why you might prefer to run Mood's median test to compare ratings among home towns.\n\n\n\n(e) Suppose that somebody wanted to run Welch ANOVA on these data. What would be a reasonable argument to support that?\n\n\n\n(f) Run Mood's median test and display the output.\n\n\n\n(g) Explain briefly why running pairwise median tests is a good idea, run them, and display the results.\n\n\n\n(h) Summarize, as concisely as possible, how the home towns differ in terms of their students' ratings of reggae music.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Watching TV and education\n\n The General Social Survey is a large survey of a large number of people. One of the questions on the survey is \"how many hours of TV do you watch in a typical day?\" Another is \"what is your highest level of education attained\", on this scale:\n\n- **HSorLess**: completed no more than high h school\n- **College**: completed some form of college, either a community college (like Centennial) or a four-year university (like UTSC)\n- **Graduate**: completed a graduate degree such as an MSc.\n\nDo people with more education tend to watch more TV? We will be exploring this. The data are in [http://ritsokiguess.site/datafiles/gss_tv.csv](http://ritsokiguess.site/datafiles/gss_tv.csv).\n\n\n\n(a) Read in and display (some of) the data.\n\n\n\n(b) For each level of education, obtain the number of observations, the mean and the median of the number of hours of TV watched.\n\n\n\n(c) What does your answer to the previous part tell you about the shapes of the distributions of the numbers of hours of TV watched? Explain briefly. \n\n\n\n(d) Obtain a suitable graph of your data frame.\n\n\n\n(e) Does your plot indicate that your guess about the distribution shape was correct? Explain briefly.\n\n\n\n(f) Run a suitable test to compare the average number of hours of TV watched for people with each amount of education. (\"Average\" could be mean or median, whichever you think is appropriate.)\n\n\n\n(g) What do you conclude from your test, in the context of the data?\n\n\n\n(h) Why might you now want to run some kind of follow-up test? Run the appropriate thing and explain briefly what you conclude from it, in the context of the data.\n\n\n\n\n\n\n\n\n\n\n## Death of poets\n\n Some people believe that poets, especially female poets, die younger than other types of writer. [William Butler Yeats](https://en.wikipedia.org/wiki/W._B._Yeats)^[An Irish, that is to say, Gaelic, poet (see below), but a male one.] wrote:\n\n> She is the Gaelic^[Gaelic is a language of Scotland and Ireland, and the culture of the people who speak it.] muse, for she gives inspiration to those she persecutes. The Gaelic poets die young, for she is restless, and will not let them remain long on earth.\n\nA literature student wanted to investigate this, and so collected a sample of 123 female writers (of three different types), and noted the age at death of each writer. \n\nThe data are in [http://ritsokiguess.site/datafiles/writers.csv](http://ritsokiguess.site/datafiles/writers.csv).\n\n\n\n(a) Read in and display (some of) the data.\n\n\n\n(b) Make a suitable plot of the ages and types of writing.\n\n\n\n(c) Obtain a summary table showing, for each type of writing, the number of writers of that type, along with the mean, median and standard deviation of their ages at death.\n\n\n\n(d) Run a complete analysis, starting with an ordinary (not Welch) analysis of variance, that ends with a conclusion in the context of the data and an assessment of assumptions.\n\n\n\n\n\n\n## Religion and studying\n\n Many students at a certain university were asked about the importance of religion in their lives (categorized as \"not\", \"fairly\", or \"very\" important), and also about the number of \nhours they spent studying per week. (This was part of a much larger survey.) We want to see whether there is any kind of relationship between these two variables. The data are in [here](http://ritsokiguess.site/datafiles/student_relig.csv). \n\n\n\n(a) Read in and display (some of) the data.\n\n\n\n(b) Obtain the number of observations and the mean and standard deviation of study hours for each level of importance.\n\n\n\n(c) Comment briefly on how the groups compare in terms of study hours.\n\n\n\n(d) Make a suitable graph of this data set.\n\n\n\n(e) The statistician in this study decided that the data were sufficiently normal in shape given the (very large) sample sizes, but was concerned about unequal spreads among the three groups. \nGiven this,\nrun a suitable analysis and display the output. (This includes a suitable follow-up test, if warranted.)\n\n\n\n(f) What do you conclude from your analysis of the previous part, in the context of the data?\n\n\n\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Movie ratings and lengths\n\n\nBefore a movie is shown in theatres, it\nreceives a \"rating\" that says what kind of material it\ncontains. [link](https://en.wikipedia.org/wiki/Motion_Picture_Association_of_America_film_rating_system)\nexplains the categories, from G (suitable for children) to R (anyone\nunder 17 must be accompanied by parent/guardian). In 2011, two\nstudents collected data on the length (in minutes) and the rating\ncategory, for 15 movies of each rating category, randomly chosen from\nall the movies released that year. The data are at\n[link](http://ritsokiguess.site/datafiles/movie-lengths.csv).\n\n\n\n(a) Read the data into R, and display (some of) what you read in.\n\n\nSolution\n\n\n`read_csv`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/movie-lengths.csv\"\nmovies <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 60 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): rating\ndbl (1): length\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nmovies\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 2\n   length rating\n    <dbl> <chr> \n 1     25 G     \n 2     75 G     \n 3     88 G     \n 4     63 G     \n 5     76 G     \n 6     97 G     \n 7     68 G     \n 8     82 G     \n 9     98 G     \n10     74 G     \n# i 50 more rows\n```\n:::\n:::\n\n     \n\nSomething that looks like a length in minutes, and a rating.\n\n$\\blacksquare$\n\n(b) Count how many movies there are of each rating.\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmovies %>% count(rating)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  rating     n\n  <chr>  <int>\n1 G         15\n2 PG        15\n3 PG-13     15\n4 R         15\n```\n:::\n:::\n\n       \n\nFifteen of each rating. (It's common to have the same number of\nobservations in each group, but not necessary for a one-way ANOVA.)\n\n$\\blacksquare$\n\n(c) Carry out an ANOVA and a Tukey\nanalysis (if warranted).\n  \n\n\nSolution\n\n\nANOVA first:\n\n::: {.cell}\n\n```{.r .cell-code}\nlength.1 <- aov(length ~ rating, data = movies)\nsummary(length.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nrating       3  14624    4875   11.72 4.59e-06 ***\nResiduals   56  23295     416                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nThis P-value is 0.00000459, which is way less than 0.05.\n\nHaving rejected the null (which said \"all means equal\"), we now need to\ndo Tukey, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(length.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = length ~ rating, data = movies)\n\n$rating\n               diff        lwr       upr     p adj\nPG-G      26.333333   6.613562 46.053104 0.0044541\nPG-13-G   42.800000  23.080229 62.519771 0.0000023\nR-G       30.600000  10.880229 50.319771 0.0007379\nPG-13-PG  16.466667  -3.253104 36.186438 0.1327466\nR-PG       4.266667 -15.453104 23.986438 0.9397550\nR-PG-13  -12.200000 -31.919771  7.519771 0.3660019\n```\n:::\n:::\n\n \n\nCast your eye down the `p adj` column and look for the ones\nthat are significant, here the first three. These are all comparisons\nwith the G (\"general\") movies, which are shorter on average than the\nothers (which are not significantly different from each other).\n\nIf you like, you can make a table of means to verify that:\n\n::: {.cell}\n\n```{.r .cell-code}\nmovies %>%\n  group_by(rating) %>%\n  summarize(mean = mean(length))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  rating  mean\n  <chr>  <dbl>\n1 G       80.6\n2 PG     107. \n3 PG-13  123. \n4 R      111. \n```\n:::\n:::\n\n \n\n$\\blacksquare$\n\n(d) Make a graph to assess whether this ANOVA is\ntrustworthy. Discuss your graph and its implications briefly.\n\n\nSolution\n\n\nThe obvious graph is a boxplot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(movies, aes(x = rating, y = length)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/stattschriber-1.pdf){fig-pos='H'}\n:::\n:::\n\n       \n\nFor ANOVA, we are looking for approximately normal distributions\nwithin each group and approximately equal spreads. Without the\noutliers, I would be more or less happy with that, but the G movies\nhave a low outlier that would pull the mean down and the PG and PG-13\nmovies have outliers that would pull the mean up. So a comparison of\nmeans might make the differences look more significant than they\nshould. Having said that, you could also say that the ANOVA is\n*very* significant, so even considering the effect of the\noutliers, the differences between G and the others are still likely to\nbe significant. \n\nExtra: the way to go if you don't trust the ANOVA is (as for the\ntwo-sample $t$) the Mood's median test. This applies to any number of\ngroups, and works in the same way as before:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\nmedian_test(movies, length, rating)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$grand_median\n[1] 100\n\n$table\n       above\ngroup   above below\n  G         2    13\n  PG        7     7\n  PG-13    12     3\n  R         8     6\n\n$test\n       what        value\n1 statistic 13.752380952\n2        df  3.000000000\n3   P-value  0.003262334\n```\n:::\n:::\n\n \n\nStill significant, though not quite as small a P-value as before\n(which echoes our thoughts about what the outliers might do to the\nmeans). If you look at the table above the test results, you see that\nthe G movies are mostly shorter than the overall median, but now the\nPG-13 movies are mostly *longer*. So the picture is a little\ndifferent. \n\nMood's median test does not naturally come with something like Tukey.\nWhat you can do is to do all the pairwise Mood's median tests, between\neach pair of groups, and then adjust to allow for your having done\nseveral tests at once. I thought this was generally useful enough that\nI put it into `smmr` under the name `pairwise_median_test`:\n\n::: {.cell}\n\n```{.r .cell-code}\npairwise_median_test(movies, length, rating)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n  g1    g2      p_value adj_p_value\n  <chr> <chr>     <dbl>       <dbl>\n1 G     PG    0.00799      0.0479  \n2 G     PG-13 0.0000590    0.000354\n3 G     R     0.0106       0.0635  \n4 PG    PG-13 0.0106       0.0635  \n5 PG    R     0.715        1       \n6 PG-13 R     0.273        1       \n```\n:::\n:::\n\n \n\nYou can ignore those (adjusted) P-values rather stupidly bigger than\n1. These are not significant.\n\nThere are two significant differences in median length: between G\nmovies and the two flavours of PG movies. The G movies are\nsignificantly shorter (as you can tell from the boxplot), but the\ndifference between G and R movies is no longer significant (a change\nfrom the regular ANOVA). \n\nYou may be puzzled by something in the boxplot: how is it that the G\nmovies are significantly shorter than the PG movies, but not\nsignificantly shorter than the R movies, *when the difference in\nmedians between G and R movies is bigger*? In Tukey, if the\ndifference in means is bigger, the P-value is\nsmaller.^[Actually, this doesn't always work if the sample  sizes in each group are different. If you're comparing two small  groups, it takes a *very large* difference in means to get a  small P-value. But in this case the sample sizes are all the same.]\nThe resolution to this puzzle, such as it is, is that Mood's median\ntest is not directly comparing the medians of the groups (despite its\nname); it's counting values above and below a *joint* median,\nwhich might be a different story.\n\n$\\blacksquare$\n\n\n\n\n\n##  Deer and how much they eat\n\n\n Do adult deer eat different amounts of food at different\ntimes of the year? The data in\n[link](http://ritsokiguess.site/datafiles/deer.txt) are the weights\nof food (in kilograms) consumed by randomly selected adult deer\nobserved at different times of the year (in February, May, August and\nNovember). We will assume that these were different deer observed in\nthe different months. (If the same animals had been observed at\ndifferent times, we would have been in the domain of \n\"repeated measures\", which would require a different analysis, \nbeyond the scope\nof this course.)\n\n\n\n(a) Read the data into R, and calculate numbers of observations\nand the median amounts of food\neaten each month.\n \nSolution\n\n\nThe usual stuff for data values separated by (single) spaces:\n\n::: {.cell}\n\n```{.r .cell-code}\nmyurl <- \"http://ritsokiguess.site/datafiles/deer.txt\"\ndeer <- read_delim(myurl, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 22 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (1): month\ndbl (1): food\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n     \n\nand then, recalling that `n()` is the handy way of getting the\nnumber of observations in each group:\n\n::: {.cell}\n\n```{.r .cell-code}\ndeer %>%\n  group_by(month) %>%\n  summarize(n = n(), med = median(food))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 3\n  month     n   med\n  <chr> <int> <dbl>\n1 Aug       6  4.7 \n2 Feb       5  4.8 \n3 May       6  4.35\n4 Nov       5  5.2 \n```\n:::\n:::\n\n \n\nWhen you want the number of observations *plus* some other\nsummaries, as here, the group-by and summarize idea is the way, using\n`n()` to get the number of observations in each\ngroup. `count` counts the number of observations per group when\nyou *only* have grouping variables.\n\nThe medians differ a bit, but it's hard to judge without a sense of\nspread, which the boxplots (next) provide. November is a bit higher\nand May a bit lower.\n \n$\\blacksquare$\n\n(b) Make side-by-side boxplots of the amount of food eaten each\nmonth. Comment briefly on what you see. \n \nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(deer, aes(x = month, y = food)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/endinger-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nThis offers the suggestion that maybe November will be significantly\nhigher than the rest and May significantly lower, or at least they\nwill be significantly different from each other.\n\nThis is perhaps getting ahead of the game: we should be thinking about\nspread and shape. Bear in mind that there are only 5 or 6 observations\nin each group, so you won't be able to say much about normality. In\nany case, we are going to be doing a Mood's median test, so any lack\nof normality doesn't matter (eg. perhaps that 4.4 observation in\nAugust). Given the small sample sizes, I actually think the spreads\nare quite similar.\n\nAnother way of looking at the data, especially with these small sample\nsizes, is a \"dot plot\": instead of making a boxplot for each month,\nwe plot the actual points for each month as if we were making a\nscatterplot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(deer, aes(x = month, y = food)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/vongiengen-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nWait a minute. There were five deer in February and six in\nAugust. Where did they go?\n\nThe problem is *overplotting*: more than one of the deer plotted\nin the same place on the plot, because the amounts of food eaten were\nonly given to one decimal place and there were some duplicated values.\nOne way to solve this is to randomly\nmove the points around so that no two of them plot in the same\nplace. This is called *jittering*, and is done like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(deer, aes(x = month, y = food)) + geom_jitter(width = 0, height = 0.05)\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/kaisers-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nNow you see all the deer, and you can see that two pairs of points in\nAugust and one pair of points in February are close enough on the\njittered plot that they would have been the same to one decimal place.\n\nI wanted to\nkeep the points above the months they belong to, so I only allowed vertical\njitter (that's the `width` and `height` in the\n`geom_jitter`; the width is zero so there is no horizontal\njittering). \nIf you like, you can colour the\nmonths; it's up to you whether you think that's making the plot easier\nto read, or is overkill (see my point on the facetted plots on the\n2017 midterm).\n\nThis way you see the whole distribution for each month. Normally it's\nnicer to see the summary made by the boxplots, but here there are not\nvery many points. The value of 4.4 in August does look quite a bit\nlower than the rest, but the other months look believably normal given\nthe small sample sizes. I don't know about equal spreads (November\nlooks more spread out), but normality looks believable. Maybe this is\nthe kind of situation in which Welch's ANOVA is a good idea. (If you\nbelieve that the normality-with-unequal-spreads is a reasonable\nassumption to make, then the Welch ANOVA will be more powerful than\nthe Mood's median test, and so should be preferred.)\n \n$\\blacksquare$\n\n(c) Run a Mood's median test as in lecture (ie.\\ not using\n`smmr`). What do you conclude, in the context of the data? \n \nSolution\n\n\nTo give you some practice with the mechanics, first find the\noverall median:\n\n::: {.cell}\n\n```{.r .cell-code}\ndeer %>% summarize(med = median(food))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 1\n    med\n  <dbl>\n1   4.7\n```\n:::\n:::\n\n     \n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian(deer$food)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.7\n```\n:::\n:::\n\n \n\nI like the first way because it's the same idea as we did before, just\nnot differentiating by month. I think there are some observations\nexactly equal to the median, which will mess things up later:\n\n::: {.cell}\n\n```{.r .cell-code}\ndeer %>% filter(food == 4.7)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  month  food\n  <chr> <dbl>\n1 Feb     4.7\n2 Feb     4.7\n3 Aug     4.7\n4 Aug     4.7\n```\n:::\n:::\n\n \n\nThere are, two in February and two in August.\n\nNext, make (and save) a table of the observations within each month\nthat are above and below this median:\n\n::: {.cell}\n\n```{.r .cell-code}\ntab1 <- with(deer, table(month, food < 4.7))\ntab1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     \nmonth FALSE TRUE\n  Aug     4    2\n  Feb     5    0\n  May     0    6\n  Nov     5    0\n```\n:::\n:::\n\n \n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\ntab2 <- with(deer, table(month, food > 4.7))\ntab2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     \nmonth FALSE TRUE\n  Aug     4    2\n  Feb     2    3\n  May     6    0\n  Nov     0    5\n```\n:::\n:::\n\n \n\nEither of these is good, but note that they are different. Two of the\nFebruary observations (the ones that were exactly 4.7) have \n\"switched sides\", \nand (look carefully) two of the August ones also.  Hence the\ntest results will be different, and `smmr` (later) will give\ndifferent results again:\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(tab1, correct = F)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in chisq.test(tab1, correct = F): Chi-squared approximation may be\nincorrect\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  tab1\nX-squared = 16.238, df = 3, p-value = 0.001013\n```\n:::\n\n```{.r .cell-code}\nchisq.test(tab2, correct = F)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in chisq.test(tab2, correct = F): Chi-squared approximation may be\nincorrect\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  tab2\nX-squared = 11.782, df = 3, p-value = 0.008168\n```\n:::\n:::\n\n \n\nThe warnings are because of the small frequencies. If you've done\nthese by hand before (which you will have if you took PSYC08), you'll\nremember that thing about \"expected frequencies less than 5\". This\nis that. It means \"don't take those P-values *too* seriously.\"\n\nThe P-values are different, but they are both clearly significant, so the\nmedian amounts of food eaten in the different months are not all the\nsame. (This is the same \"there are differences\" that you get from an\nANOVA, which you would follow up with Tukey.) Despite the injunction\nnot to take the P-values too seriously, I think these are small enough\nthat they could be off by a bit without affecting the conclusion.\n\nThe first table came out with a smaller P-value because it looked more\nextreme: all of the February measurements were taken as higher than\nthe overall median (since we were counting \"strictly less\" and \n\"the rest\"). In the second table, the February measurements look more\nevenly split, so the overall P-value is not quite so small.\n\nYou can make a guess as to what `smmr` will come out with\n(next), since it throws away any data values exactly equal to the median.\n \n$\\blacksquare$\n\n(d) Run a Mood's median test using `smmr`, and compare the\nresults with the previous part.\n \nSolution\n\n\nOff we go:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\nmedian_test(deer, food, month)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$grand_median\n[1] 4.7\n\n$table\n     above\ngroup above below\n  Aug     2     2\n  Feb     3     0\n  May     0     6\n  Nov     5     0\n\n$test\n       what        value\n1 statistic 13.950000000\n2        df  3.000000000\n3   P-value  0.002974007\n```\n:::\n:::\n\n     \n\nThe P-value came out in between the other two, but the conclusion is\nthe same all three ways: the months are not all the same in terms of\nmedian food eaten. The researchers can then go ahead and try to figure\nout *why* the animals eat different amounts in the different months.\n\nYou might be wondering how you could get rid of the equal-to-median\nvalues in the build-it-yourself way. This is `filter` from\n`dplyr`, which you use first:\n\n::: {.cell}\n\n```{.r .cell-code}\ndeer2 <- deer %>% filter(food != 4.7)\ntab3 <- with(deer2, table(month, food < 4.7))\ntab3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     \nmonth FALSE TRUE\n  Aug     2    2\n  Feb     3    0\n  May     0    6\n  Nov     5    0\n```\n:::\n\n```{.r .cell-code}\nchisq.test(tab3)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in chisq.test(tab3): Chi-squared approximation may be incorrect\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  tab3\nX-squared = 13.95, df = 3, p-value = 0.002974\n```\n:::\n:::\n\n \n\nwhich is exactly what `smmr` does, so the answer is\nidentical.^[The computer scientists among you will note that I  should not use equals or not-equals to compare a decimal  floating-point number, since decimal numbers are not represented exactly in the computer. R, however, is ahead of us here, since when you try to do \"food not equal to 4.7\", it tests whether food is more than a small distance away from 4.7, which is the right way to do it. In R, therefore, code like my `food !=  4.7` does exactly what I want, but in a language like C, it *does not*, and you have to be more careful: `abs(food-4.7)>1e-8`, or something like that. The small number `1e-8` ($10^{-8}$) is typically equal to **machine epsilon**, the smallest number on a computer that is distinguishable from zero.]\nHow would an ANOVA come out here? My guess is, very similarly:\n\n::: {.cell}\n\n```{.r .cell-code}\ndeer.1 <- aov(food ~ month, data = deer)\nsummary(deer.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nmonth        3 2.3065  0.7688   22.08 2.94e-06 ***\nResiduals   18 0.6267  0.0348                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nTukeyHSD(deer.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = food ~ month, data = deer)\n\n$month\n              diff         lwr        upr     p adj\nFeb-Aug  0.1533333 -0.16599282  0.4726595 0.5405724\nMay-Aug -0.3333333 -0.63779887 -0.0288678 0.0290758\nNov-Aug  0.5733333  0.25400718  0.8926595 0.0004209\nMay-Feb -0.4866667 -0.80599282 -0.1673405 0.0021859\nNov-Feb  0.4200000  0.08647471  0.7535253 0.0109631\nNov-May  0.9066667  0.58734052  1.2259928 0.0000013\n```\n:::\n:::\n\n \n\nThe conclusion is the same, but the P-value on the $F$-test is much\nsmaller. I think this is because the $F$-test uses the actual values,\nrather than just whether they are bigger or smaller than 4.7. The\nTukey says that all the months are different in terms of (now) mean,\nexcept for February and August, which were those two very similar ones\non the boxplot.\n \n\n$\\blacksquare$\n\n(e) How is it that Mood's median test does not completely answer the\nquestion you really want to answer? How might you get an answer to the\nquestion you *really* want answered? Explain briefly, and obtain\nthe answer you *really* want, discussing your results briefly.\n\n\n\nSolution\n\n\nThat's rather a lot, so let's take those things one at a\ntime.^[Most of these parts are old from assignment questions that I actually asked a previous class to do, but not this part. I added it later.]\n\nMood's median test is really like the $F$-test in ANOVA: it's testing\nthe null hypothesis\nthat the groups (months) all have the same median (of food eaten),\nagainst the alternative that the null is not true. We rejected this\nnull, but we don't know which months differ significantly from\nwhich. To resolve this in ANOVA, we do Tukey (or Games-Howell if we\ndid the Welch ANOVA). The corresponding thing here is to do all the\npossible two-group Mood tests on all the pairs of groups, and, after\nadjusting for doing (here) six tests at once, look at the adjusted\nP-values to see how the months differ in terms of food eaten.\n\nThis is accomplished in `smmr` via `pairwise_median_test`,\nthus: \n\n::: {.cell}\n\n```{.r .cell-code}\npairwise_median_test(deer, food, month)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n  g1    g2    p_value adj_p_value\n  <chr> <chr>   <dbl>       <dbl>\n1 Aug   Feb   0.147       0.884  \n2 Aug   May   0.0209      0.126  \n3 Aug   Nov   0.00270     0.0162 \n4 Feb   May   0.00157     0.00939\n5 Feb   Nov   0.0578      0.347  \n6 May   Nov   0.00157     0.00939\n```\n:::\n:::\n\n \n\nThis compares each month with each other month. Looking at the last\ncolumn, there are only three significant differences: August-November,\nFebruary-May and May-November. Going back to the table of medians we\nmade in (a), November is significantly higher (in terms of median food\neaten) than August and May (but not February), and February is\nsignificantly higher than May. The other differences are not big\nenough to be significant.\n\nExtra: Pairwise median tests done this way are not likely to be very\nsensitive (that is, powerful), for a couple of reasons: (i) the usual\none that the median tests don't use the data very efficiently, and\n(ii) the way I go from the unadjusted to the adjusted P-values is via\nBonferroni (here, multiply the P-values by 6), which is known to be\nsafe but conservative. This is why the Tukey produced more significant\ndifferences among the months than the pairwise median tests did.\n\n$\\blacksquare$\n\n\n\n\n\n##  Movie ratings again\n\n\n This question again uses the movie rating data at\n[link](http://ritsokiguess.site/datafiles/movie-lengths.csv).\n\n\n\n(a) Read the data into R and obtain the number of movies of\neach rating and the *median* length of movies of each rating.\n\n\nSolution\n\n\nReading in is as in the other question using these data (just copy\nyour code, or mine). \n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/movie-lengths.csv\"\nmovies <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 60 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): rating\ndbl (1): length\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nmovies\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 2\n   length rating\n    <dbl> <chr> \n 1     25 G     \n 2     75 G     \n 3     88 G     \n 4     63 G     \n 5     76 G     \n 6     97 G     \n 7     68 G     \n 8     82 G     \n 9     98 G     \n10     74 G     \n# i 50 more rows\n```\n:::\n:::\n\n \n\nNow, the actual for-credit part, which is a `group_by` and\n`summarize`: \n\n::: {.cell}\n\n```{.r .cell-code}\nmovies %>%\n  group_by(rating) %>%\n  summarize(count = n(), med = median(length))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 3\n  rating count   med\n  <chr>  <int> <dbl>\n1 G         15    82\n2 PG        15   100\n3 PG-13     15   117\n4 R         15   103\n```\n:::\n:::\n\n \n\nThe G movies have a smaller median than the others, but also the PG-13\nmovies seem to be longer on average (not what we found before). \n    \n\n$\\blacksquare$\n\n(b) Obtain a suitable graph that assesses the assumptions for\nANOVA. Why do you think it is not reasonable to run ANOVA here? Explain\nbriefly. \n\n\nSolution\n\n\nThe graph would seem to be a boxplot, side by side for each group:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(movies, aes(x = rating, y = length)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/bletzen-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nWe are looking for approximate normal distributions with approximately\nequal spreads, which I don't think we have: there are outliers, at the\nlow end for G movies, and at the high end for PG and PG-13\nmovies. Also, you might observe that the distribution of lengths for R\nmovies is skewed to the right. (Noting either the outliers or skewness\nas a reason for not believing normality is enough, since all we need\nis *one* way that normality fails.)\n\nI think the spreads (as measured by the interquartile ranges) are\nacceptably similar, but since we have rejected normality, it is a bit\nlate for that.\n\nSo I think it is far from reasonable to run an ANOVA here. In my\nopinion 15 observations in each group is not enough to gain much from\nthe Central Limit Theorem either.\n\nExtra: since part of the assumption for ANOVA is (approximate)\nnormality, it would also be entirely reasonable to make normal\nquantile plots, one for each movie type, facetted. Remember the\nprocess: you pretend that you are making a normal quantile plot for\nall the data together, regardless of group, and then at the last\nminute, you throw in a `facet_wrap`. I've written the code out\non three lines, so that you can see the pieces: the \"what to plot\",\nthen the normal quantile plot part, then the facetting:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(movies, aes(sample = length)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~rating)\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/fravel-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nSince there are four movie ratings, `facet_wrap` has arranged\nthem into a $2\\times 2$ grid, which satisfyingly means that each\nnormal quantile plot is more or less square and thus easy to\ninterpret. \n\nThe principal problem unveiled by these plots is outliers. It looks as\nif the G movies have one low outlier, the PG movies have two high\noutliers, the PG-13 movies have one or maybe three high outliers\n(depending on how you count them), and the R movies have none. Another\nway to look at the last two is you could call them curved, with too\nmuch bunching up at the bottom and (on PG-13) too much spread-out-ness\nat the top, indicating right-skewed distributions. The distribution\nof lengths of the R-rated movies is too bunched up at the bottom, but\nas you would expect for a normal at the top. The R movies show the\nright-skewedness in an odd way: usually this skewness shows up by\nhaving too many high values, but this time it's having too *few*\n*low* values.\n\nThe assumption for ANOVA is that all four of these are at least\napproximately normal (with the same spread). We found problems with\nthe normality on at least three of them, so we definitely have doubts\nabout trusting ANOVA here.\n\nI could have used `scales=free` here to get a separate $y$-axis\nfor each plot, but since the $y$-axis is movie length each time, and\nall four groups would be expected to have at least roughly similar\nmovie lengths, I left it as it was. (The other advantage of leaving\nthe scales the same is that you can compare spread by comparing the\nslopes of the lines on these graphs; since the lines connect the\nobserved and theoretical quartiles, a steeper slope means a larger\nIQR. Here, the R line is steepest and the PG line is flattest. Compare\nthis with the spreads of the boxplots.)\n\nExtra extra: if you want, you can compare the normal quantile plots\nwith the boxplots to see whether you get the same conclusion from\nboth. For the G movies, the low outlier shows up both ways, and the\nrest of the distribution is at least more or less normal. For the PG\nmovies, I'd say the distribution is basically normal except for the\nhighest two values (on both plots). For the PG-13 movies, only the\nhighest value shows up as an outlier, but the next two apparent\noutliers on the normal quantile plot are at the  upper end of the long\nupper whisker, so the boxplot is saying \"right-skewed with one upper outlier\" rather than \"three upper outliers\". The distribution of\nthe R movies is skewed right, with the bunching at the bottom showing\nup as the very small lower whisker.\n\nThe boxplots and the normal quantile plots are basically telling the\nsame story in each case, but they are doing it in a slightly different\nway. \n    \n\n$\\blacksquare$\n\n(c) Run a Mood's median test (use `smmr` if you\nlike). What do you conclude, in the context of the data?\n\n\nSolution\n\n\nThe smart way is to use `smmr`, since it is much easier:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\nmedian_test(movies, length, rating)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$grand_median\n[1] 100\n\n$table\n       above\ngroup   above below\n  G         2    13\n  PG        7     7\n  PG-13    12     3\n  R         8     6\n\n$test\n       what        value\n1 statistic 13.752380952\n2        df  3.000000000\n3   P-value  0.003262334\n```\n:::\n:::\n\n     \n\nThe movies do not all have the same median length, or at least one of\nthe rating types has movies of different median length from the\nothers. Or something equivalent to that. It's the same conclusion as\nfor ANOVA, only with medians instead of means.\n\nYou can speculate about why the test came out significant. \nMy guess is that the G movies are shorter than\naverage, and that the PG-13 movies are longer than average. (We had\nthe first conclusion before, but not the second. This is where medians\nare different from means.)\n\nThe easiest way to see which movie types really differ in length from\nwhich is to do all the pairwise median tests, which is in\n`smmr` thus:\n\n::: {.cell}\n\n```{.r .cell-code}\npairwise_median_test(movies, length, rating)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n  g1    g2      p_value adj_p_value\n  <chr> <chr>     <dbl>       <dbl>\n1 G     PG    0.00799      0.0479  \n2 G     PG-13 0.0000590    0.000354\n3 G     R     0.0106       0.0635  \n4 PG    PG-13 0.0106       0.0635  \n5 PG    R     0.715        1       \n6 PG-13 R     0.273        1       \n```\n:::\n:::\n\n \n\nThe inputs for this are the same ones in the same order as for\n`median_test`. (A design decision on my part, since otherwise\n*I* would never have been able to remember how to run these!)\nOnly the first two of these are significant (look in the last\ncolumn). We can remind ourselves of the sample medians:\n\n::: {.cell}\n\n```{.r .cell-code}\nmovies %>%\n  group_by(rating) %>%\n  summarize(count = n(), med = median(length))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 3\n  rating count   med\n  <chr>  <int> <dbl>\n1 G         15    82\n2 PG        15   100\n3 PG-13     15   117\n4 R         15   103\n```\n:::\n:::\n\n \n\nThe G movies are significantly shorter than the PG and PG-13 movies,\nbut not quite significantly different from the R movies. This is a\nlittle odd, since the difference in sample medians between G and PG,\nsignificant, is *less* than for G and R (not significant).\nThere are several Extras here, which you can skip if you don't care\nabout the background. First, we can do the median test by hand:\nThis has about four steps: (i) find the median of all the data, (ii) make a\ntable tabulating the number of values above and below the overall\nmedian for each group, (iii) test the table for association, (iv)\ndraw a conclusion.\nThus (i):\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian(movies$length)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100\n```\n:::\n:::\n\n  \n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\nmovies %>% summarize(med = median(length))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 1\n    med\n  <dbl>\n1   100\n```\n:::\n:::\n\n \n\nor store it in a variable,\nand then (ii):\n\n::: {.cell}\n\n```{.r .cell-code}\ntab1 <- with(movies, table(length < 100, rating))\ntab1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       rating\n         G PG PG-13  R\n  FALSE  2  8    12  9\n  TRUE  13  7     3  6\n```\n:::\n:::\n\n \n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\ntab2 <- with(movies, table(length > 100, rating))\ntab2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       rating\n         G PG PG-13  R\n  FALSE 13  8     3  7\n  TRUE   2  7    12  8\n```\n:::\n:::\n\n \n\nThese differ because there are evidently some movies of length exactly\n100 minutes, and it matters whether you count $<$ and $\\ge$ (as in\n`tab1`) or $>$ and $le$ (`tab2`). Either is good. \n\nWas I right about movies of length exactly 100 minutes?\n\n::: {.cell}\n\n```{.r .cell-code}\nmovies %>% filter(length == 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n  length rating\n   <dbl> <chr> \n1    100 PG    \n2    100 R     \n```\n:::\n:::\n\n \n\nOne PG and one R. It makes a difference to the R movies, but if you\nlook carefully, it makes a difference to the PG movies as well,\nbecause the False and True switch roles between `tab1` and\n`tab2` (compare the G movies, for instance).\nYou\nneed to store your table in a variable because it has to get passed on\nto `chisq.test` below, (iii):\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(tab1, correct = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  tab1\nX-squared = 14.082, df = 3, p-value = 0.002795\n```\n:::\n:::\n\n \n\nor \n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(tab2, correct = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  tab2\nX-squared = 13.548, df = 3, p-value = 0.003589\n```\n:::\n:::\n\n \n\nEither is correct, or, actually, without the `correct=FALSE`.^[See discussion elsewhere about Yates' Correction and fixed margins.]\n\nThe conclusion (iv) is the same either way: the null of no association\nis clearly rejected (with a P-value of 0.0028 or 0.0036 as\nappropriate), and therefore whether a movie is longer or shorter than\nmedian length depends on what rating it has: that is, the median\nlengths do differ among the ratings. The same conclusion, in other\nwords, as the $F$-test gave, though with not quite such a small\nP-value. \n\nSecond, you might be curious about how\nwe might do something like Tukey having found some significant\ndifferences (that is, what's lurking in the background of\n`pairwise_median_test`). \n\nLet's first suppose we are comparing G and PG movies. We need\nto pull out just those, and then compare them using\n`smmr`. Because the first input to `median_test` is a\ndata frame, it fits neatly into a pipe (with the data frame omitted):\n\n::: {.cell}\n\n```{.r .cell-code}\nmovies %>%\n  filter(rating == \"G\" | rating == \"PG\") %>%\n  median_test(length, rating)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$grand_median\n[1] 96\n\n$table\n     above\ngroup above below\n   G      4    11\n   PG    10     3\n\n$test\n       what       value\n1 statistic 7.035897436\n2        df 1.000000000\n3   P-value 0.007989183\n```\n:::\n:::\n\n \n\nWe're going to be doing this about six times --- ${4 \\choose 2}=6$ choices\nof two rating groups to compare out of the four --- so we should have a\nfunction to do it. I think the input to the function should be a data\nframe that has a column called `rating`, and two names of\nratings to compare:\n\n::: {.cell}\n\n```{.r .cell-code}\ncomp2 <- function(rat_1, rat_2, d) {\n  d %>%\n    filter(rating == rat_1 | rating == rat_2) %>%\n    median_test(length, rating)\n}\n```\n:::\n\n \n\nThe way I wrote this function is that you have to specify the movie\nratings in quotes. It is *possible* to write it in such a way\nthat you input them without quotes, `tidyverse` style, but that\ngets into \"non-standard evaluation\" and `enquo()` and\n`!!`, which (i) I have to look up every time I want to do it,\nand (ii) I am feeling that the effort involved in explaining it to you\nis going to exceed the benefit you will gain from it. I mastered it enough\nto make it work in `smmr` (note that you specify column names\nwithout quotes there). There are tutorials on this kind of thing if\nyou're interested.\n\nAnyway, testing:\n\n::: {.cell}\n\n```{.r .cell-code}\ncomp2(\"G\", \"PG\", movies)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$grand_median\n[1] 96\n\n$table\n     above\ngroup above below\n   G      4    11\n   PG    10     3\n\n$test\n       what       value\n1 statistic 7.035897436\n2        df 1.000000000\n3   P-value 0.007989183\n```\n:::\n:::\n\n \n\nThat works, but I really only want to pick out the P-value, which is\nin the list item `test` in the column `value`, the third\nentry. So let's rewrite the function to return just that:\n\n::: {.cell}\n\n```{.r .cell-code}\ncomp2 <- function(rat_1, rat_2, d) {\n  d %>%\n    filter(rating == rat_1 | rating == rat_2) %>%\n    median_test(length, rating) %>%\n    pluck(\"test\", \"value\", 3)\n}\ncomp2(\"G\", \"PG\", movies)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.007989183\n```\n:::\n:::\n\n \n\nGosh.\n\nWhat `median_test` returns is an R `list` that has two\nthings in it, one called `table` and one called\n`test`. The thing  called `test` is a data frame with a\ncolumn called `value` that contains the P-values. The third of\nthese is the two-sided P-value that we want.\n\nYou might not have seen `pluck` before. This is a way of\ngetting things out of complicated data structures. This one takes the\noutput from `median_test` and from it grabs the piece called\n`test`. This is a data frame. Next, we want the column called\n`value`, and from that we want the third row. These are\nspecified one after the other to `pluck` and it pulls out the\nright thing.\n\nSo now our function returns just the P-value.\n\nI have to say that it took me several goes and some playing around in\nR Studio to sort this one out. Once I thought I understood\n`pluck`, I wondered why my function was not returning a\nvalue. And then I realized that I was saving the value inside the\nfunction and not returning it. Ooops. The nice thing about\n`pluck` is that I can put it on the end of the pipeline and and\nit will pull out (and return) whatever I want it to.\n\nLet's grab a hold of the different rating groups we have:\n\n::: {.cell}\n\n```{.r .cell-code}\nthe_ratings <- unique(movies$rating)\nthe_ratings\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"G\"     \"PG-13\" \"PG\"    \"R\"    \n```\n:::\n:::\n\n \n\nThe Pythonisti among you will know how to finish this off: do a\nloop-inside-a-loop over the rating groups, and get the P-value for\neach pair. You can do that in R, if you must. It's not pretty at all,\nbut it works:\n\n::: {.cell}\n\n```{.r .cell-code}\nii <- character(0)\njj <- character(0)\npp <- numeric(0)\nfor (i in the_ratings) {\n  for (j in the_ratings) {\n    pval <- comp2(i, j, movies)\n    ii <- c(ii, i)\n    jj <- c(jj, j)\n    pp <- c(pp, pval)\n  }\n}\ntibble(ii, jj, pp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 16 x 3\n   ii    jj           pp\n   <chr> <chr>     <dbl>\n 1 G     G     1        \n 2 G     PG-13 0.0000590\n 3 G     PG    0.00799  \n 4 G     R     0.0106   \n 5 PG-13 G     0.0000590\n 6 PG-13 PG-13 1        \n 7 PG-13 PG    0.0106   \n 8 PG-13 R     0.273    \n 9 PG    G     0.00799  \n10 PG    PG-13 0.0106   \n11 PG    PG    1        \n12 PG    R     0.715    \n13 R     G     0.0106   \n14 R     PG-13 0.273    \n15 R     PG    0.715    \n16 R     R     1        \n```\n:::\n:::\n\n \n\nThis is a lot of fiddling about, since you have to initialize three\nvectors, and then update them every time through the loop. It's hard\nto read, because the actual business part of the loop is the\ncalculation of the P-value, and that's almost hidden by all the\nbook-keeping.  (It's also slow and inefficient, though the slowness\ndoesn't matter too much here since it's not a very big problem.)\n\nLet's try another way:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(first = the_ratings, second = the_ratings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 16 x 2\n   first second\n   <chr> <chr> \n 1 G     G     \n 2 G     PG    \n 3 G     PG-13 \n 4 G     R     \n 5 PG    G     \n 6 PG    PG    \n 7 PG    PG-13 \n 8 PG    R     \n 9 PG-13 G     \n10 PG-13 PG    \n11 PG-13 PG-13 \n12 PG-13 R     \n13 R     G     \n14 R     PG    \n15 R     PG-13 \n16 R     R     \n```\n:::\n:::\n\n`crossing` is a sort of model-free version of `datagrid`: it works out all possible combinations of whatever vectors you feed it (that don't have to be the same, but here are).\n\nWe\ndon't actually need all of that; we just need the ones where the first\none is (alphabetically) strictly less than the second one. This is\nbecause we're never comparing a rating with itself, and each pair of\nratings appears twice, once in alphabetical order, and once the other\nway around. The ones we need are these:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(first = the_ratings, second = the_ratings) %>%\n  filter(first < second)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 2\n  first second\n  <chr> <chr> \n1 G     PG    \n2 G     PG-13 \n3 G     R     \n4 PG    PG-13 \n5 PG    R     \n6 PG-13 R     \n```\n:::\n:::\n\n \n\nA technique thing to note: instead of asking \n\"how do I pick out the distinct pairs of ratings?\", \nI use two simpler tools: first I make\nall the combinations of pairs of ratings, and then out of those, pick\nthe ones that are alphabetically in ascending order, which we know how\nto do.\n\nNow we want to call our function `comp2` for each of the things\nin `first` *and* each of the things in `second`,\nand make a new column called `pval` that contains exactly\nthat. `comp2` expects single movie ratings for each of its inputs, not a vector of each, so the way to go about this is `rowwise`:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(first = the_ratings, second = the_ratings) %>%\n  filter(first < second) %>%\n  rowwise() %>% \n  mutate(pval = comp2(first, second, movies))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n# Rowwise: \n  first second      pval\n  <chr> <chr>      <dbl>\n1 G     PG     0.00799  \n2 G     PG-13  0.0000590\n3 G     R      0.0106   \n4 PG    PG-13  0.0106   \n5 PG    R      0.715    \n6 PG-13 R      0.273    \n```\n:::\n:::\n\n\nOne more thing: we're doing 6 tests at once here, so we're giving\nourselves 6 chances to reject a null (all medians equal) that might\nhave been true. So the true probability of a type I error is no longer\n0.05 but something bigger. \n\nThe easiest way around that is to do a so-called Bonferroni\nadjustment: instead of rejecting if the P-value is less than 0.05, we\nonly reject if it is less than $0.05/6$, since we are doing 6\ntests. This is a fiddly calculation to do by hand, but it's easy to\nbuild in another `mutate`, thus:^[In the pairwise median  test in `smmr`, I did this backwards: rather than changing the alpha that you compare each P-value with from 0.05 to 0.05/6, I  flip it around so that you adjust the P-values by *multiplying*  them by 6, and then comparing the adjusted P-values with the usual  0.05. It comes to the same place in the end, except that this way  you can get adjusted P-values that are greater than 1, which makes no sense. You read those as being definitely not significant.]\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(first = the_ratings, second = the_ratings) %>%\n  filter(first < second) %>%\n  rowwise() %>% \n  mutate(pval = comp2(first, second, movies)) %>% \n  mutate(reject = (pval < 0.05 / 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n# Rowwise: \n  first second      pval reject\n  <chr> <chr>      <dbl> <lgl> \n1 G     PG     0.00799   TRUE  \n2 G     PG-13  0.0000590 TRUE  \n3 G     R      0.0106    FALSE \n4 PG    PG-13  0.0106    FALSE \n5 PG    R      0.715     FALSE \n6 PG-13 R      0.273     FALSE \n```\n:::\n:::\n\n \n\nAnd not a loop in sight.\n\nThis is how I coded it in `pairwise_median_test`. If you want to\ncheck it, it's on Github:\n[link](https://raw.githubusercontent.com/nxskok/smmr/master/R/pairwise_median_test.R). \nThe function `median_test_pair` is the same as `comp2`\nabove. \n\nSo the only significant differences are now G compared to PG and\nPG-13. There is not a significant difference in median movie length\nbetween G and R, though it is a close call. We thought the PG-13\nmovies might have a significantly different median from other rating\ngroups beyond G, but they turn out not to have. (The third and fourth\ncomparisons would have been significant had we not made the Bonferroni\nadjustment to compensate for doing six tests at once; with that\nadjustment, we only reject if the P-value is less than\n$0.05/6=0.0083$, and so 0.0106 is not quite small enough to reject\nwith.) \n\nListing the rating groups sorted by median would give you an idea of\nhow far different the medians have to be to be significantly different:\n\n::: {.cell}\n\n```{.r .cell-code}\nmedians <- movies %>%\n  group_by(rating) %>%\n  summarize(med = median(length)) %>%\n  arrange(desc(med))\nmedians\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  rating   med\n  <chr>  <dbl>\n1 PG-13    117\n2 R        103\n3 PG       100\n4 G         82\n```\n:::\n:::\n\n \n\nSomething rather interesting has happened: even though the comparison of\nG and PG (18 apart) is significant, the comparison of G and R (21\napart) is not significant. This seems very odd, but it happens because\nthe Mood median test is not actually literally comparing the sample\nmedians, but only assessing the splits of values above and below the\nmedian of the combined sample. A subtlety, rather than an error, I'd say.\n\nHere's something extremely flashy to finish with:\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossing(first = the_ratings, second = the_ratings) %>%\n  filter(first < second) %>%\n  rowwise() %>% \n  mutate(pval = comp2(first, second, movies)) %>% \n  mutate(reject = (pval < 0.05 / 6)) %>% \n  left_join(medians, join_by(first == rating)) %>%\n  left_join(medians, join_by(second == rating))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 6\n# Rowwise: \n  first second      pval reject med.x med.y\n  <chr> <chr>      <dbl> <lgl>  <dbl> <dbl>\n1 G     PG     0.00799   TRUE      82   100\n2 G     PG-13  0.0000590 TRUE      82   117\n3 G     R      0.0106    FALSE     82   103\n4 PG    PG-13  0.0106    FALSE    100   117\n5 PG    R      0.715     FALSE    100   103\n6 PG-13 R      0.273     FALSE    117   103\n```\n:::\n:::\n\n \n\nThe additional two lines look up the medians of the rating groups in\n`first`, then `second`, so that you can see the actual\nmedians of the groups being compared each time. You see that medians\ndifferent by 30 are definitely different, ones differing by 15 or less\nare definitely not different, and ones differing by about 20 could go\neither way.\n\nI think that's *quite* enough of that.\n    \n$\\blacksquare$\n\n\n\n\n\n\n##  Atomic weight of carbon\n\n\n The atomic weight of the chemical element\ncarbon is 12. Two methods of measuring the atomic weight of samples of\ncarbon were compared. The results are shown in\n[link](http://ritsokiguess.site/datafiles/carbon.txt). The methods\nare labelled 1 and 2.  The first task is to find out whether the two\nmethods have different \"typical\" measures (mean or median, as\nappropriate) of the atomic weight of carbon.\n\nFor this question, compose a report in a Quarto document.  (\nSee part (a) for how to get this started.\n\nYour report should\nread like an actual report, not just the answers to some questions\nthat I set you. To help with that, write some text that links the\nparts of the report together smoothly, so that it reads as a coherent\nwhole. The grader had 3 discretionary marks to award for the overall\nquality of your writing. The scale for this was:\n\n\n\n* 3 points: excellent writing. The report flows smoothly, is easy\nto read, and contains everything it should (and nothing it\nshouldn't).\n\n* 2 points: satisfactory writing. Not the easiest to read, but\nsays what it should, and it looks at least somewhat like a report\nrather than a string of answers to questions.\n\n* 1 point: writing that is hard to read or to understand. If you\nget this (or 0), you should consider what you need to do to improve\nwhen you write your project.\n\n* 0 points: you answered the questions, but you did almost nothing\nto make it read like a report.\n\n\n\n(a) Create a new Quarto document. To do this, in R Studio, select File,\nNew File, Quarto Document. Type the report title and your name in the\nboxes, and leave the output on the default HTML. Click OK. \n\nSolution\n\n\nYou'll\nsee the title and your name in a section at the top of the document,\nand below that you'll see a template document. \n\n$\\blacksquare$\n\n(b) Write an introduction that explains the purpose of this\nstudy and the data collected in your own words.\n\n\nSolution\n\n\nSomething like this:\n\n> This study is intended to compare two different methods\n> (labelled 1 and 2) for measuring the atomic weight of carbon\n> (which is known in actual fact to be 12). Fifteen samples of\n> carbon were used; ten of these were assessed using method 1 and\n> the remaining five using method 2. The primary interest in this\n> particular study is to see whether there is a difference in the\n> mean or median atomic weight as measured by the two methods.\n\nBefore that, start a new section like this:\n`## Introduction`. \nAlso, get used to expressing your understanding in your words,\nnot mine. Using my words, in my courses, is likely to be\nworth very little.\n\n\n$\\blacksquare$\n\n(c) Begin an appropriately-titled new section in your report,\nread the data into R and display the results.\n\n\nSolution\n\n\nValues separated by spaces:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/carbon.txt\"\ncarbon <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 15 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\ndbl (2): method, weight\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ncarbon\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15 x 2\n   method weight\n    <dbl>  <dbl>\n 1      1   12.0\n 2      1   12.0\n 3      1   12.0\n 4      1   12.0\n 5      1   12.0\n 6      1   12.0\n 7      1   12.0\n 8      1   12.0\n 9      1   12.0\n10      1   12.0\n11      2   12.0\n12      2   12.0\n13      2   12.0\n14      2   12.0\n15      2   12.0\n```\n:::\n:::\n\n     \n\nI would expect you to include, without being told to include it, some\ntext in your report indicating that you have sensible data: two\nmethods labelled 1 and 2 as promised, and a bunch^[It's  probably better in a report to use language a bit more formal than  *a bunch*. Something like *a number* would be better.] \nof atomic\nweights close to the nominal figure of 12.\n\n\n$\\blacksquare$\n\n(d) Make an appropriate plot to compare the measurements\nobtained by the two methods. You might need to do something about\nthe two methods being given as numbers even though they are really\nonly identifiers. (If you do, your report ought to say what you did\nand why.)\n\n\nSolution\n\n\nThe appropriate plot, with a categorical method and quantitative\nweight, is something like a boxplot. If you're not careful,\n`method` will get treated as a quantitative variable,\nwhich you don't want; the easiest way around that, for a boxplot\nat least, is to turn it into a factor like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(carbon, aes(x = factor(method), y = weight)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/seewerin-1.pdf){fig-pos='H'}\n:::\n:::\n\n       \n\nIf you insist, you could do a faceted histogram (above and below, for preference):\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(carbon, aes(x = weight)) + geom_histogram(bins = 5) +\n  facet_wrap(~method, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/schuomarcher-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThere are really not enough data values for a histogram to be of much\nhelp, so I don't like this as much. \n\nIf you are thinking ahead (we are going to be doing a $t$-test), then\nyou'll realize that normality is the kind of thing we're looking for,\nin which case normal quantile plots would be the thing. However, we\nmight have to be rather forgiving for method 2 since there are only 5\nobservations: \n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(carbon, aes(sample = weight)) +\n  stat_qq() + stat_qq_line() +\n  facet_wrap(~method)\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/mulichs-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nI don't mind these coming out side by side, though I would rather have\nthem squarer.\n\nI would say, boxplots are the best, normal quantile plots are also\nacceptable, but expect to lose something for histograms because they\noffer only a rather crude comparison in this case.\n\n\n$\\blacksquare$\n\n(e) Comment briefly on what you see in your plot.\n\n\nSolution\n\n\nIn boxplots, if that's what you drew, there are several things\nthat deserve comment: the medians, the spreads and the\nshapes. The median for method 1 is a little bit lower than for\nmethod 2 (the means are probably more different, given the\nshapes of the boxes). The spread for method 2 is a lot\nbigger. (Looking forward, that suggests a Welch-Satterthwaite\nrather than a pooled test.) As for shape, the method 2\nmeasurements seem more or less symmetric (the whiskers are equal\nanyway, even if the position of the median in the box isn't),\nbut the method 1 measurements have a low outlier.\nThe histograms are hard to compare. Try to say something about\ncentre and spread and shape. I think the method 2 histogram has\na slightly higher centre and definitely bigger spread. On my\nhistogram for method 1, the distribution looks skewed left.\nIf you did normal quantile plots, say something sensible about\nnormality for each of the two methods. For method 1, I would say\nthe low value is an outlier and the rest of the values look\npretty straight. Up to you whether you think there is a curve on\nthe plot (which would indicate skewness, but then that highest\nvalue is too high: it would be bunched up with the other values\nbelow 12.01 if there were really skewness). \nFor method 2, it's really hard to say anything since there are\nonly five values. Given where the line goes, there isn't much\nyou can say to doubt normality.  Perhaps the best you can say\nhere is that in a sample of size 5, it's difficult to assess\nnormality at all.\n\n\n$\\blacksquare$\n\n(f) Carry out the most appropriate $t$-test. (You might like to\nbegin another new section in your report here.)\n\n\nSolution\n\n\nThis would be the Welch-Satterthwaite version of the two-sample\n$t$-test, since the two groups do appear to have different spreads:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(weight ~ method, data = carbon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  weight by method\nt = -1.817, df = 5.4808, p-value = 0.1238\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.027777288  0.004417288\nsample estimates:\nmean in group 1 mean in group 2 \n       12.00260        12.01428 \n```\n:::\n:::\n\n   \n\nImagining that this is a report that would go to your boss, you ought\nto defend your choice of the Welch-Satterthwaite test (as I did\nabove), and not just do the default $t$-test without comment.\n\nIf, in your discussion above, you thought the spreads were equal\nenough, then you should do the pooled $t$-test here, which goes like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(weight ~ method, data = carbon, var.equal = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tTwo Sample t-test\n\ndata:  weight by method\nt = -2.1616, df = 13, p-value = 0.04989\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -2.335341e-02 -6.588810e-06\nsample estimates:\nmean in group 1 mean in group 2 \n       12.00260        12.01428 \n```\n:::\n:::\n\n \n\nThe point here is that you should do the right test based on your\nconclusion. Being consistent is the most important thing. (In this\ncase, note that the P-values are very different. We'll get to that\nshortly.)\n\nYou might think we should do a test\nthat compares the two variances, so that then we'd know which flavour of test to do. Such tests exist, but I feel that it's\njust as good to eyeball the spreads and make a call about whether they\nare \"reasonably close\". Or even, to always do the\nWelch-Satterthwaite test on the basis that it is pretty good even if\nthe two populations have the same variance. (If this last point of\nview is one that you share, you ought to say something about that when\nyou do your $t$-test.) The problem is that doing a test to decide which test to do can change the actual P-value from what the output of your second test says; the second test, as far as R is concerned, is done without knowledge that you did another test first.\n\nExtra: I guess this is a good place to say something about tests for comparing\nvariances, given that you might be pondering that. There are\nseveral that I can think of, that R can do, of which I mention two.\n\nThe first is the $F$-test for variances that you might have learned in\nB57 (that is the basis for the ANOVA $F$-test):\n\n::: {.cell}\n\n```{.r .cell-code}\nvar.test(weight ~ method, data = carbon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tF test to compare two variances\n\ndata:  weight by method\nF = 0.35768, num df = 9, denom df = 4, p-value = 0.1845\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.04016811 1.68758230\nsample estimates:\nratio of variances \n         0.3576842 \n```\n:::\n:::\n\n \n\nThis, unfortunately, is rather dependent on the data in the two groups\nbeing approximately normal. Since we are talking variances rather than\nmeans, there is no Central Limit Theorem to rescue us for large\nsamples (quite aside from the fact that these samples are *not*\nlarge). Since the ANOVA $F$-test is based on the same theory, this is\nwhy normality is also more important in ANOVA than it is in a $t$-test.\n\nThe second is Levene's test. This doesn't depend on normality (at\nleast, not nearly so much), so I like it better in general:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nleveneTest(weight ~ factor(method), data = carbon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  0.9887 0.3382\n      13               \n```\n:::\n:::\n\n \n\nLevene's test takes a different approach: first the absolute\ndifferences from the group medians are calculated, and then an ANOVA\nis run on the absolute differences. If, say, one of the groups has a\nlarger spread than the other(s), its absolute differences from the\nmedian will tend to be bigger.^[The use of absolute  differences, and the median, downplays the influence of outliers. The assumption here is that the absolute differences from the medians are approximately normal, which seems a less big assumption than assuming the actual data are approximately normal.]\nAs for what we conclude here, well, neither of the variance tests show\nany significance at all, so from that point of view there is no\nevidence against using the pooled $t$-test. Having said that, the\nsamples are small, and so it would be difficult to *prove* that\nthe two methods have different variance, even if they actually\ndid.^[This is coming back to the *power* of something like Levene's test; the power of any test is not going to be very big if the sample sizes are small.]\n\nThings are never as clear-cut as you would like. In the end, it all\ncomes down to making a call and defending it.\n\n\n$\\blacksquare$\n\n(g) Do the most appropriate test you know that does not assume\nnormally-distributed data.\n\n\nSolution\n\n\nThat would be Mood's median test. Since I didn't say anything\nabout building it yourself, feel free to use `smmr`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\nmedian_test(carbon, weight, method)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$grand_median\n[1] 12.0064\n\n$table\n     above\ngroup above below\n    1     3     6\n    2     4     1\n\n$test\n       what      value\n1 statistic 2.80000000\n2        df 1.00000000\n3   P-value 0.09426431\n```\n:::\n:::\n\n \n\nAs an aside, if you have run into a non-parametric test such as\nMann-Whitney or Kruskal-Wallis that applies in this situation, be\ncareful about using it here, because they have additional assumptions\nthat you may not want to trust. Mann-Whitney started life as a test for\n\"equal distributions\".^[The test goes back to the 1940s.] This\nmeans that the null is equal location *and* equal spread, and if\nyou reject the null, one of those has failed. But here, we suspect that\nequal spread will fail, so that the Mann-Whitney test may end up\nrejecting *whether or not* the medians are different, so it won't\nanswer the question you want an answer to. Mood's median test doesn't\nhave that problem; all it's saying if the null is true is that the\nmedians are equal; the spreads could be anything at all.\n\nThe same kind of issues apply to the signed-rank test vs. the sign\ntest. In the case of the signed-rank test, the extra assumption is of\na symmetric distribution --- to my mind, if you don't believe\nnormality, you probably don't have much confidence in symmetry\neither. That's why I like the sign test and Mood's median test: in the\nsituation where you don't want to be dealing with assumptions, these\ntests don't make you worry about that.\n\nAnother comment that you don't need to make is based on the\nnot-quite-significance of the Mood test. The P-value is less than 0.10\nbut not less than 0.05, so it doesn't quite reach significance by the\nusual standard. But if you look up at the table, the frequencies seem\nrather unbalanced: 6 out of the remaining 9 weights in group 1 are\nbelow the overall median, but 4 out of 5 weights in group 2 are\nabove. This seems as if it ought to be significant, but bear in mind\nthat the sample sizes are small, and thus Mood's median test needs\n*very* unbalanced frequencies, which we don't quite have here.\n\n\n$\\blacksquare$\n\n(h) Discuss the results of your tests and what they say about\nthe two methods for measuring the atomic weight of carbon. If it\nseems appropriate, put the discussion into a section called\nConclusions. \n\n\nSolution\n\n\nBegin by pulling out the P-values for your preferred test(s) and\nsay what they mean. The P-value for the Welch-Satterthwaite\n$t$-test is 0.1238, which indicates no difference in mean atomic\nweights between the two methods. The Mood median test gives a\nsimilarly non-significant 0.0943, indicating no difference in\nthe *median* weights. If you think both tests are\nplausible, then give both P-values and do a compare-and-contrast\nwith them; if you think that one of the tests is clearly\npreferable, then say so (and why) and focus on that test's\nresults. \n\nIf you thought the pooled test was the right one, then you'll\nhave a bit more discussion to do, since its P-value is 0.0499,\nand at $\\alpha=0.05$ this test disagrees with the others. If you\nare comparing this test with the Mood test, you ought to make\nsome kind of reasoned recommendation about which test to\nbelieve. \n\nAs ever, be consistent in your reasoning.\n\nExtra: this dataset, where I found it, was actually being used to\nillustrate a case where the pooled and the Welch-Satterthwaite\ntests disagreed. The authors of the original paper that used\nthis dataset (a 1987 paper by Best and Rayner;^[Best, D. J., and J.  C. W. Rayner. Welchs Approximate Solution for the BehrensFisher Problem. Technometrics 29, no. 2 (May 1, 1987): 20510. doi:10.1080/00401706.1987.10488211. The data set is near the end.]\nthe data come from 1924!) point out that the\npooled $t$-test can be especially misleading when the smaller\nsample is also the one with the larger variance. This is what\nhappened here.\n\nIn the Best and Rayner paper, the Mood (or the Mann-Whitney) test was\nnot being considered, but I think it's good practice to draw a\npicture and make a call about which test is appropriate.\n   \n\n$\\blacksquare$\n\n\n\n\n\n##  Can caffeine improve your performance on a test?\n\n\n \nDoes caffeine help students do better on a certain test? To\nfind out, 36 students were randomly allocated to three groups (12 in\neach group).  Each student received a fixed number of cups of coffee\nwhile they were studying, but the students didn't know whether they\nwere receiving all full-strength coffee (\"high\"), all decaf coffee\n(\"low\") or a 50-50 mixture of the two (\"moderate\"). For each\nsubject, their group was recorded as well as their score on the\ntest. The data are in\n[link](http://ritsokiguess.site/datafiles/caffeine.csv), as a\n`.csv` file.\n\n\n\n(a) Read in and examine the data. How are the values laid out?\n\n\nSolution\n\n\n`read_csv` because it's a `.csv` file:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/caffeine.csv\"\ncaffeine.untidy <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 12 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (4): Sub, High, Moderate, None\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ncaffeine.untidy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 12 x 4\n     Sub  High Moderate  None\n   <dbl> <dbl>    <dbl> <dbl>\n 1     1    72       68    68\n 2     2    65       80    74\n 3     3    68       64    59\n 4     4    83       65    61\n 5     5    79       69    65\n 6     6    92       79    72\n 7     7    69       80    80\n 8     8    74       63    58\n 9     9    78       69    65\n10    10    83       70    60\n11    11    88       83    78\n12    12    71       75    75\n```\n:::\n:::\n\n     \n\nThe first column is the number of the subject (actually within each\ngroup, since each student only tried one amount of caffeine). Then\nfollow the test scores for the students in each group, one group per column.\n\nI gave the data frame a kind of dumb name, since (looking ahead) I\ncould see that I would need a less-dumb name for the tidied-up data,\nand it seemed sensible to keep `caffeine` for that.\n\n$\\blacksquare$\n\n(b) Explain briefly how the data are not \"tidy\".\n\n\nSolution\n\n\nThe last three columns are all scores on the test: that is, they\nall measure the same thing, so they should all be in the same column.\nOr, there should be a column of scores, and a separate column\nnaming the groups. Or, there were 36 observations in the data, so\nthere should be 36 rows. You always have a variety of ways to\nanswer these, any of which will do.\n\n$\\blacksquare$\n\n(c) Use a suitable tool from the `tidyverse` to create one\ncolumn of test scores and and one column of group labels. Call your\ncolumn of group labels `amount`. Is it a `factor`?\n\n\nSolution\n\n\nWe are combining several columns into one, so this is `pivot_longer`:\n\n::: {.cell}\n\n```{.r .cell-code}\ncaffeine.untidy %>% \n  pivot_longer(-Sub, names_to = \"amount\", values_to = \"score\") -> caffeine\n```\n:::\n\n\n\nI didn't ask you to list the resulting data frame, but it is smart to\nat least look for yourself, to make sure `pivot_longer` has done\nwhat you expected.\n\n::: {.cell}\n\n```{.r .cell-code}\ncaffeine\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 36 x 3\n     Sub amount   score\n   <dbl> <chr>    <dbl>\n 1     1 High        72\n 2     1 Moderate    68\n 3     1 None        68\n 4     2 High        65\n 5     2 Moderate    80\n 6     2 None        74\n 7     3 High        68\n 8     3 Moderate    64\n 9     3 None        59\n10     4 High        83\n# i 26 more rows\n```\n:::\n:::\n\n \n\nA column of amounts of caffeine, and a column of test scores. This is\nwhat we expected. There should be 12 each of the `amount`s,\nwhich you can check if you like:\n\n::: {.cell}\n\n```{.r .cell-code}\ncaffeine %>% count(amount)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 2\n  amount       n\n  <chr>    <int>\n1 High        12\n2 Moderate    12\n3 None        12\n```\n:::\n:::\n\n \n\nIndeed.\n\nNote that `amount` is text, not a factor. Does this matter? We'll see.\n\nThis is entirely the kind of situation where you need `pivot_longer`,\nso get used to seeing where it will be useful.\n\n$\\blacksquare$\n\n(d) Obtain side-by-side boxplots of test scores by amount of caffeine.\n\n\nSolution\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(caffeine, aes(x = amount, y = score)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/nayer-1.pdf){fig-pos='H'}\n:::\n:::\n\nNote that this is *much more difficult* if you don't have a tidy data frame. (Try it and see.) \n\n$\\blacksquare$\n\n(e) Does caffeine amount seem to have an effect? If so, what\nkind of effect?\n\n\nSolution\n\n\nOn average, exam scores seem to be higher when the amount of\ncaffeine is higher (with\nthe effect being particularly pronounced for High caffeine). \nIf you want to, you can also say the the effect of caffeine seems\nto be small, relative to the amount of variability there is (there\nis a lot). The point is that you say *something* supported by\nthe boxplot.\n\n$\\blacksquare$\n\n(f) Run a suitable analysis of variance to determine whether\nthe mean test score is equal or unequal for the three groups. What\ndo you conclude?\n\n\nSolution\n\n\nSomething like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ncaff.1 <- aov(score ~ amount, data = caffeine)\nsummary(caff.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value Pr(>F)  \namount       2  477.7  238.86   3.986 0.0281 *\nResiduals   33 1977.5   59.92                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n     \n\nThe P-value on the $F$-test is less than 0.05, so we reject the null\nhypothesis (which says that all the groups have equal means) in favour\nof the alternative: the group means are not all the same (one or more\nof them is different from the others).\n\nNotice that the boxplot and the `aov` are quite happy for\n`amount` to be text rather than a factor (they actually do want\na factor, but if the input is text, they'll create one).\n\n$\\blacksquare$\n\n(g) Why is it a good idea to run Tukey's method here?\n \n\nSolution\n\n\nThe analysis of variance $F$-test is significant, so that the\ngroups are not all the same. Tukey's method will tell us which\ngroup(s) differ(s) from the others. There are three groups, so\nthere are differences to find that we don't know about yet.\n\n$\\blacksquare$\n\n(h) Run Tukey's method. What do you conclude?\n\n\nSolution\n\n\nThis kind of thing:\n\n::: {.cell}\n\n```{.r .cell-code}\ncaff.3 <- TukeyHSD(caff.1)\ncaff.3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = score ~ amount, data = caffeine)\n\n$amount\n                   diff       lwr       upr     p adj\nModerate-High -4.750000 -12.50468  3.004679 0.3025693\nNone-High     -8.916667 -16.67135 -1.161987 0.0213422\nNone-Moderate -4.166667 -11.92135  3.588013 0.3952176\n```\n:::\n:::\n\n \n\nThe high-caffeine group definitely has a higher mean test score than\nthe no-caffeine group. (The Moderate group is not significantly\ndifferent from either of the other groups.)\nBoth the\ncomparisons involving Moderate could go either way (the interval for\nthe difference in means includes zero). The None-High comparison, \nhowever, is away from zero, so this is the significant one. As is\nusual, we are pretty sure that the difference in means (this way\naround) is negative, but we are not at all clear about how big it is,\nbecause the confidence interval is rather long.^[We'd need a  lot more students to make it narrower, but this is not surprising  since students vary in a lot of other ways that were not measured here.]\n\n\nExtra: the normality and equal spreads assumptions look perfectly good, given the boxplots, and I don't think there's any reason to consider any other test. You might like to assess that with normal quantile plots:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(caffeine, aes(sample=score)) + stat_qq() +\n  stat_qq_line() + facet_wrap(~amount, ncol=2)\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/frantz-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThere's nothing to worry about there normality-wise. If anything, there's a little evidence of *short* tails (in the None group especially), but you'll recall that short tails don't affect the mean and thus pose no problems for the ANOVA. Those three lines also have pretty much the same slope, indicating very similar spreads. Regular ANOVA is the best test here. (Running eg. Mood's median test would be a mistake here, because it doesn't use the data as efficiently (counting only aboves and belows) as the ANOVA does, and so the ANOVA will give a better picture of what differs from what.)\n\n\n$\\blacksquare$\n\n\n\n## Reggae music\n\n Reggae is a music genre that originated in Jamaica in the late 1960s. One of the most famous reggae bands was Bob Marley and the Wailers. \nIn a survey, 729 students were asked to rate reggae music on a scale from 1, \"don't like it at all\" to 6, \"like it a lot\". \nWe will treat the ratings as quantitative.\nEach student was also asked to classify their home town as one of \"big city\", \"suburban\", \"small town\", \"rural\". Does a student's opinion of reggae depend on the kind of home town they come from? The data are in [http://ritsokiguess.site/datafiles/reggae.csv](http://ritsokiguess.site/datafiles/reggae.csv). \n\n\n\n(a) Read in and display (some of) the data.\n\nSolution\n\n\nThis is (evidently) a `.csv`, so:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/reggae.csv\"\nreggae <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 729 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): home\ndbl (1): rating\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nreggae\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 729 x 2\n   home     rating\n   <chr>     <dbl>\n 1 big city      1\n 2 big city      2\n 3 big city      2\n 4 big city      2\n 5 big city      2\n 6 big city      2\n 7 big city      2\n 8 big city      2\n 9 big city      3\n10 big city      3\n# i 719 more rows\n```\n:::\n:::\n\nThe students shown are all from big cities, but there are others, as you can check by scrolling down.\n\n\n$\\blacksquare$\n\n\n(b) How many students are from each different size of town?\n\nSolution\n\n\nThis is the usual kind of application of `count`:\n\n::: {.cell}\n\n```{.r .cell-code}\nreggae %>% count(home)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  home           n\n  <chr>      <int>\n1 big city      89\n2 rural         96\n3 small town   176\n4 suburban     368\n```\n:::\n:::\n\nAnother, equally good, way (you can ignore the warning):\n\n::: {.cell}\n\n```{.r .cell-code}\nreggae %>% group_by(home) %>% \nsummarize(n=n())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  home           n\n  <chr>      <int>\n1 big city      89\n2 rural         96\n3 small town   176\n4 suburban     368\n```\n:::\n:::\n\nMost of the students in this data set are from suburbia.\n\n\n$\\blacksquare$\n\n\n(c) Make a suitable graph of the two variables in this data frame. \n\nSolution\n\n\nOne quantitative, one categorical: a boxplot, as ever:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(reggae, aes(x=home, y=rating)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/schuchmacher-1.pdf){fig-pos='H'}\n:::\n:::\n\nExtra 1: the last three boxplots really are identical, because the medians, means, quartiles and extreme values are all equal. However, the *data values* are not all the same, as you see below.\n\nExtra 2: I said that the ratings should be treated as quantitative, to guide you towards this plot. \nYou could otherwise have taken the point of view that the ratings were (ordered) categorical, in which case the right graph would have been a grouped bar chart, as below. \nThere is a question about which variable should be `x` and which should be `fill`. \nI am taking the point of view that we want to compare ratings within each category of `home`, which I think makes sense here (see discussion below), which breaks my \"rule\" that the categorical variable with fewer categories should be `x`.^[Perhaps a better word here would be *principle*, to convey the idea that you can do something else if it works better for your purposes.] \n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(reggae, aes(x=home, fill=factor(rating))) + geom_bar(position = \"dodge\")\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/schuochmacher-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n$\\blacksquare$\n\n\n(d) Discuss briefly why you might prefer to run Mood's median test to compare ratings among home towns.\n\nSolution\n\n\nThe issue here is whether *all* of the rating distributions (within each category of `home`) are sufficiently close to normal in shape.\nThe \"big city\" group is clearly skewed to the left. This is enough to make us favour Mood's median test over ANOVA. \n\nA part-marks answer is to note that the big-city group has smaller spread than the other groups (as measured by the IQR). This is answering the wrong question, though. \nRemember the process: first we assess normality. If that fails, we use Mood's median test. Then, with normality OK, we assess equal spreads. If *that* fails, we use Welch ANOVA, and if both normality and equal spreads pass, we use regular ANOVA. \n\n\n$\\blacksquare$\n\n\n(e) Suppose that somebody wanted to run Welch ANOVA on these data. What would be a reasonable argument to support that?\n\nSolution\n\n\nThe argument would have to be that normality is all right, given the sample sizes. We found earlier that there are between 89 and 368 students in each group. These are large samples, and might be enough to overcome the non-normality we see. \n\nThe only real concern I have is with the big city group. This is the least normal, and also the smallest sample. The other groups seem to have the kind of non-normality that will easily be taken care of by the sample sizes we have. \n\nExtra: the issue is really about the sampling distribution of the mean within each group. Does that look normal enough? This could be assessed by looking at each group, one at a time, and taking bootstrap samples. Here's the big-city group:\n\n::: {.cell}\n\n```{.r .cell-code}\nreggae %>% filter(home==\"big city\") -> bigs\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(bigs$rating, replace = T))) %>% \n  mutate(my_mean = mean(my_sample)) %>% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12)\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/ulins-1.pdf){fig-pos='H'}\n:::\n:::\n\nNot too much wrong with that. This shows that the sample size is indeed big enough to cope with the skewness. \n\nYou can do any of the others the same way. \n\nIf you're feeling bold, you can get hold of all three bootstrapped sampling distributions at once, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nreggae %>% \n  nest_by(home) %>% \n  mutate(sim = list(1:1000)) %>% \n  unnest(sim) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(data$rating, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) %>% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12) +\n  facet_wrap(~home, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/schlosser-1.pdf){fig-pos='H'}\n:::\n:::\n\nAll of these distributions look very much normal, so there is no cause for concern anywhere.\n\nThis was rather a lot of code, so let me take you through it. The first thing is that we want to treat the different students' homes separately, so the first step is this:\n\n::: {.cell}\n\n```{.r .cell-code}\nreggae %>% \n  nest_by(home) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n# Rowwise:  home\n  home                     data\n  <chr>      <list<tibble[,1]>>\n1 big city             [89 x 1]\n2 rural                [96 x 1]\n3 small town          [176 x 1]\n4 suburban            [368 x 1]\n```\n:::\n:::\n\nThis subdivides the students' reggae ratings according to where their home is. The things in `data` are data frames containing a column `rating` for in each case the students who had the `home` shown.\n\nNormally, we would start by making a dataframe with a column called `sim` that labels the 1000 or so simulations. This time, we want *four* sets of simulations, one for each `home`, which we can set up this way:\n\n::: {.cell}\n\n```{.r .cell-code}\nreggae %>% \n  nest_by(home) %>% \n  mutate(sim = list(1:1000)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 3\n# Rowwise:  home\n  home                     data sim          \n  <chr>      <list<tibble[,1]>> <list>       \n1 big city             [89 x 1] <int [1,000]>\n2 rural                [96 x 1] <int [1,000]>\n3 small town          [176 x 1] <int [1,000]>\n4 suburban            [368 x 1] <int [1,000]>\n```\n:::\n:::\n\nThe definition of `sim` happens by group, or rowwise, by `home` (however you want to look at it). Next, we need to spread out those `sim` values so that we'll have one row per bootstrap sample:\n\n::: {.cell}\n\n```{.r .cell-code}\nreggae %>% \n  nest_by(home) %>% \n  mutate(sim = list(1:1000)) %>% \n  unnest(sim) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4,000 x 3\n# Groups:   home [4]\n   home                   data   sim\n   <chr>    <list<tibble[,1]>> <int>\n 1 big city           [89 x 1]     1\n 2 big city           [89 x 1]     2\n 3 big city           [89 x 1]     3\n 4 big city           [89 x 1]     4\n 5 big city           [89 x 1]     5\n 6 big city           [89 x 1]     6\n 7 big city           [89 x 1]     7\n 8 big city           [89 x 1]     8\n 9 big city           [89 x 1]     9\n10 big city           [89 x 1]    10\n# i 3,990 more rows\n```\n:::\n:::\n\n$4 \\times 1000 = 4000$ rows. Note that the `data` column now contains multiple copies of all the ratings for the students with that `home`, which seems wasteful, but it makes our life easier because what we want is a bootstrap sample from the right set of students, namely the `rating` column from the dataframe `data` in each row. Thus, from here out, everything is the same as we have done before: work rowwise, get a bootstrap sample , find its mean, plot it. The one thing we need to be careful of is to make a *separate* histogram for each `home`, since each of the *four* distributions need to look normal. I used different scales for each one, since they are centred in different places; this has the side benefit of simplifying the choice of the number of bins. (See what happens if you omit the `scales = \"free\"`.)\n\nIn any case, all is absolutely fine. We'll see how this plays out below.\n\n\n$\\blacksquare$\n\n\n(f) Run Mood's median test and display the output.\n\nSolution\n\n\nData frame, quantitative column, categorical column:\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian_test(reggae, rating, home)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$grand_median\n[1] 4\n\n$table\n            above\ngroup        above below\n  big city      51    21\n  rural         25    49\n  small town    64    89\n  suburban     120   187\n\n$test\n       what        value\n1 statistic 2.733683e+01\n2        df 3.000000e+00\n3   P-value 5.003693e-06\n```\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(g) Explain briefly why running pairwise median tests is a good idea, run them, and display the results.\n\nSolution\n\n\nThe Mood's median test is significant, with a P-value of 0.000005, so the median ratings are not all the same. We want to find out how they differ. \n\n(The table of aboves and belows, and for that matter the boxplot earlier, suggest that big-city will be different from the rest, but it is not clear whether there will be any other significant differences.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairwise_median_test(reggae, rating, home)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n  g1         g2            p_value adj_p_value\n  <chr>      <chr>           <dbl>       <dbl>\n1 big city   rural      0.00000746  0.0000448 \n2 big city   small town 0.0000491   0.000295  \n3 big city   suburban   0.00000110  0.00000663\n4 rural      small town 0.788       1         \n5 rural      suburban   0.740       1         \n6 small town suburban   0.963       1         \n```\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(h) Summarize, as concisely as possible, how the home towns differ in terms of their students' ratings of reggae music.\n\nSolution\n\n\nThe students from big cities like reggae more than students from other places. The other kinds of hometown do not differ significantly.\n\nExtra 1: Given the previous discussion, you might be wondering how Welch ANOVA (and maybe even regular ANOVA) compare. Let's find out:\n\n::: {.cell}\n\n```{.r .cell-code}\noneway.test(rating~home,data=reggae)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne-way analysis of means (not assuming equal variances)\n\ndata:  rating and home\nF = 16.518, num df = 3.00, denom df = 257.07, p-value = 7.606e-10\n```\n:::\n:::\nand\n\n::: {.cell}\n\n```{.r .cell-code}\ngamesHowellTest(rating~factor(home),data=reggae)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n\tPairwise comparisons using Games-Howell test\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ndata: rating by factor(home)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n           big city rural small town\nrural      1.1e-07  -     -         \nsmall town 2.9e-06  0.74  -         \nsuburban   4.9e-09  0.91  0.94      \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nP value adjustment method: none\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nalternative hypothesis: two.sided\n```\n:::\n:::\n\nThe conclusions are identical with Mood's median test, and the P-values are not that different, either. \n\nThis makes me wonder how an ordinary ANOVA with Tukey would have come out:\n\n::: {.cell}\n\n```{.r .cell-code}\nreggae %>% \naov(rating~home, data=.) %>% \nTukeyHSD()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = rating ~ home, data = .)\n\n$home\n                           diff        lwr        upr     p adj\nrural-big city      -1.20681180 -1.8311850 -0.5824386 0.0000048\nsmall town-big city -1.00510725 -1.5570075 -0.4532070 0.0000194\nsuburban-big city   -1.09404006 -1.5952598 -0.5928203 0.0000002\nsmall town-rural     0.20170455 -0.3366662  0.7400753 0.7695442\nsuburban-rural       0.11277174 -0.3735106  0.5990540 0.9329253\nsuburban-small town -0.08893281 -0.4778062  0.2999406 0.9354431\n```\n:::\n:::\n\nAgain, almost identical.\n\nExtra 2: some Bob Marley and the Wailers for you:\n\n- [from 1980](https://www.youtube.com/watch?v=RhJ0q7X3DLM)\n- [from 1973](https://www.youtube.com/watch?v=rf8GjhXvOjU)\n\nReggae music at its finest.\n\n\n$\\blacksquare$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Watching TV and education\n\n The General Social Survey is a large survey of a large number of people. One of the questions on the survey is \"how many hours of TV do you watch in a typical day?\" Another is \"what is your highest level of education attained\", on this scale:\n\n- **HSorLess**: completed no more than high h school\n- **College**: completed some form of college, either a community college (like Centennial) or a four-year university (like UTSC)\n- **Graduate**: completed a graduate degree such as an MSc.\n\nDo people with more education tend to watch more TV? We will be exploring this. The data are in [http://ritsokiguess.site/datafiles/gss_tv.csv](http://ritsokiguess.site/datafiles/gss_tv.csv).\n\n\n\n(a) Read in and display (some of) the data.\n\nSolution\n\n\nExactly the usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/gss_tv.csv\"\ngss <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 905 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): degree\ndbl (1): tvhours\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ngss\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 905 x 2\n   degree   tvhours\n   <chr>      <dbl>\n 1 HSorLess       1\n 2 HSorLess       1\n 3 HSorLess       4\n 4 HSorLess       3\n 5 HSorLess       6\n 6 College        1\n 7 HSorLess       5\n 8 College        1\n 9 College        3\n10 HSorLess       1\n# i 895 more rows\n```\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(b) For each level of education, obtain the number of observations, the mean and the median of the number of hours of TV watched.\n\nSolution\n\n\n`group_by` and `summarize`, using `n()` to get the number of observations (rather than `count` because you want some numerical summaries as well):\n\n::: {.cell}\n\n```{.r .cell-code}\ngss %>% group_by(degree) %>% \nsummarise(n=n(), mean=mean(tvhours), med=median(tvhours))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 4\n  degree       n  mean   med\n  <chr>    <int> <dbl> <dbl>\n1 College    195  2.27     2\n2 Graduate    70  1.84     1\n3 HSorLess   640  3.33     3\n```\n:::\n:::\n\n\n\n$\\blacksquare$\n\n\n(c) What does your answer to the previous part tell you about the shapes of the distributions of the numbers of hours of TV watched? Explain briefly. \n\nSolution\n\n\nIn each of the three groups, the mean is greater than the median, so I think the distributions are skewed to the right. Alternatively, you could say that you expect to see some outliers at the upper end.\n\n\n$\\blacksquare$\n\n\n(d) Obtain a suitable graph of your data frame.\n\nSolution\n\n\nOne quantitative variable and one categorical one, so a boxplot. (I hope you are getting the hang of this by now.)\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(gss, aes(x=degree, y=tvhours)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/gss-tv-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(e) Does your plot indicate that your guess about the distribution shape was correct? Explain briefly.\n\nSolution\n\n\nI guessed before that the distributions would be right-skewed, and they indeed are, with the long upper tails. Or, if you suspected upper outliers, they are here as well.\n\nSay what you guessed before, and how your graph confirms it (or doesn't, if it doesn't.)\n\n\n$\\blacksquare$\n\n\n(f) Run a suitable test to compare the average number of hours of TV watched for people with each amount of education. (\"Average\" could be mean or median, whichever you think is appropriate.)\n\nSolution\n\n\nFrom the boxplot, the distributions are definitely not all normal; in fact, none of them are. So we should use Mood's median test, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian_test(gss, tvhours, degree)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$grand_median\n[1] 2\n\n$table\n          above\ngroup      above below\n  College     67    70\n  Graduate    18    36\n  HSorLess   355   126\n\n$test\n       what        value\n1 statistic 5.608269e+01\n2        df 2.000000e+00\n3   P-value 6.634351e-13\n```\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(g) What do you conclude from your test, in the context of the data?\n\nSolution\n\n\nThe P-value of $6.6\\times 10^{-13}$ is extremely small, so we conclude that not all of the education groups watch the same median amount of TV. \nOr, there are differences in the median amount of TV watched among the three groups.\n\nAn answer of \"the education groups are different\" is *wrong*, because you don't know that they are *all* different. It might be that some of them are different and some of them are the same. The next part gets into that.\n\n\n$\\blacksquare$\n\n\n(h) Why might you now want to run some kind of follow-up test? Run the appropriate thing and explain briefly what you conclude from it, in the context of the data.\n\nSolution\n\n\nThe overall Mood test is significant, so there are some differences between the education groups, but we don't know where they are.  Pairwise median tests will reveal where any differences are:\n\n::: {.cell}\n\n```{.r .cell-code}\npairwise_median_test(gss, tvhours, degree)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 4\n  g1       g2        p_value   adj_p_value\n  <chr>    <chr>       <dbl>         <dbl>\n1 College  Graduate 5.12e- 2 0.154        \n2 College  HSorLess 8.06e-10 0.00000000242\n3 Graduate HSorLess 3.06e- 7 0.000000919  \n```\n:::\n:::\n\nThe people whose education is high school or less are significantly different from the other two education levels. The boxplot reveals that this is because they watch *more* TV on average. The college and graduate groups are not significantly different (in median TV watching).\n\nExtra 1:\n\nYou might have been surprised that the College and Graduate medians were not significantly different. After all, they look quite different on the boxplot. Indeed, the P-value for comparing just those two groups is 0.0512, only just over 0.05. But remember that we are doing three tests at once, so the Bonferroni adjustment is to multiply the P-values by 3, so this P-value is \"really\" some way from being significant. \nI thought I would investigate this in more detail:\n\n::: {.cell}\n\n```{.r .cell-code}\ngss %>% filter(degree != \"HSorLess\") %>% \nmedian_test(tvhours, degree)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$grand_median\n[1] 2\n\n$table\n          above\ngroup      above below\n  College     67    70\n  Graduate    18    36\n\n$test\n       what     value\n1 statistic 3.8027625\n2        df 1.0000000\n3   P-value 0.0511681\n```\n:::\n:::\n\nThe College group are about 50-50 above and below the overall median, but the Graduate group are two-thirds below. This suggests that the Graduate group watches less TV, and with these sample sizes I would have expected a smaller P-value. But it didn't come out that way. \n\nYou might also be concerned that there are in total more values below the grand median (106) than above (only 85). This must mean that there are a lot of data values *equal* to the grand median:\n\n::: {.cell}\n\n```{.r .cell-code}\ngss %>% filter(degree != \"HSorLess\") -> gss1\ngss1 %>% summarize(med=median(tvhours))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 1\n    med\n  <dbl>\n1     2\n```\n:::\n:::\n\nand\n\n::: {.cell}\n\n```{.r .cell-code}\ngss1 %>% count(tvhours)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 2\n   tvhours     n\n     <dbl> <int>\n 1       0    15\n 2       1    91\n 3       2    74\n 4       3    51\n 5       4    17\n 6       5     5\n 7       6     6\n 8       7     2\n 9       8     3\n10      12     1\n```\n:::\n:::\n\nEverybody gave a whole number of hours, and there are not too many different ones; in addition, a lot of them are equal to the grand median of 2.\n\nExtra 2:\n\nRegular ANOVA and Welch ANOVA should be non-starters here because of the non-normality, but you might be curious about how they would perform:\n\n::: {.cell}\n\n```{.r .cell-code}\ngss.1 <- aov(tvhours~degree, data=gss)\nsummary(gss.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value   Pr(>F)    \ndegree        2    267  133.30   25.18 2.27e-11 ***\nResiduals   902   4774    5.29                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nTukeyHSD(gss.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = tvhours ~ degree, data = gss)\n\n$degree\n                        diff        lwr       upr     p adj\nGraduate-College  -0.4238095 -1.1763372 0.3287181 0.3831942\nHSorLess-College   1.0598958  0.6181202 1.5016715 0.0000001\nHSorLess-Graduate  1.4837054  0.8037882 2.1636225 0.0000011\n```\n:::\n:::\n\nand\n\n::: {.cell}\n\n```{.r .cell-code}\noneway.test(tvhours~degree, data=gss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne-way analysis of means (not assuming equal variances)\n\ndata:  tvhours and degree\nF = 37.899, num df = 2.00, denom df = 206.22, p-value = 9.608e-15\n```\n:::\n\n```{.r .cell-code}\ngamesHowellTest(tvhours~factor(degree), data=gss)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n\tPairwise comparisons using Games-Howell test\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ndata: tvhours by factor(degree)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n         College Graduate\nGraduate 0.12    -       \nHSorLess 2.4e-10 1.7e-10 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nP value adjustment method: none\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nalternative hypothesis: two.sided\n```\n:::\n:::\n\nThe conclusions are actually identical to our Mood test, and the P-values are actually not all that much different. Which makes me wonder just how bad the sampling distributions of the sample means are. Bootstrap to the rescue:\n\n::: {.cell}\n\n```{.r .cell-code}\ngss %>% \n  nest_by(degree) %>% \n  mutate(sim = list(1:1000)) %>% \n  unnest(sim) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(data$tvhours, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) %>% \n  ggplot(aes(x = my_mean)) + geom_histogram(bins = 12) +\n  facet_wrap(~degree, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/gss-tv-11-1.pdf){fig-pos='H'}\n:::\n:::\n\nCoding this made my head hurt, but building it one line at a time, I pretty much got it right first time. In words:\n\n- \"compress\" the dataframe to get one row per degree and a list-column called `data` with the number of hours of TV watched for each person with that `degree`\n- generate 1000 `sim`s for each `degree` (to guide the taking of bootstrap samples shortly)\n- organize into one row per `sim`\n- then take bootstrap samples as normal and work out the mean of each one\n- make histograms for each `degree`, using a different scale for each one. (This has the advantage that the normal number of `bins` will work for all the histograms.)\n\nIf you are not sure about what happened, run it one line at a time and see what the results look like after each one.\n\nAnyway, even though the data was very much not normal, these sampling distributions are very normal-looking, suggesting that something like Welch ANOVA would have been not nearly as bad as you would have guessed. This is evidently because of the big sample sizes. (This also explains why the two other flavours of ANOVA gave results very similar to Mood's median test.)\n\n\n$\\blacksquare$\n\n\n\n\n\n\n\n\n\n## Death of poets\n\n Some people believe that poets, especially female poets, die younger than other types of writer. [William Butler Yeats](https://en.wikipedia.org/wiki/W._B._Yeats)^[An Irish, that is to say, Gaelic, poet (see below), but a male one.] wrote:\n\n> She is the Gaelic^[Gaelic is a language of Scotland and Ireland, and the culture of the people who speak it.] muse, for she gives inspiration to those she persecutes. The Gaelic poets die young, for she is restless, and will not let them remain long on earth.\n\nA literature student wanted to investigate this, and so collected a sample of 123 female writers (of three different types), and noted the age at death of each writer. \n\nThe data are in [http://ritsokiguess.site/datafiles/writers.csv](http://ritsokiguess.site/datafiles/writers.csv).\n\n\n\n(a) Read in and display (some of) the data.\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/writers.csv\"\nwriters <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 123 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): Type\ndbl (2): Type1, Age\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nwriters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 123 x 3\n   Type1 Type     Age\n   <dbl> <chr>  <dbl>\n 1     1 Novels    57\n 2     1 Novels    90\n 3     1 Novels    67\n 4     1 Novels    56\n 5     1 Novels    90\n 6     1 Novels    72\n 7     1 Novels    56\n 8     1 Novels    90\n 9     1 Novels    80\n10     1 Novels    74\n# i 113 more rows\n```\n:::\n:::\n\nThere are indeed 123 writers. The second column shows the principal type of writing each writer did, and the third column shows their age at death. The first column is a numerical code for the type of writing, which we ignore (since we can handle the text writing type).\n\n\n$\\blacksquare$\n\n\n(b) Make a suitable plot of the ages and types of writing.\n\nSolution\n\n\nAs usual, one quantitative and one categorical, so a boxplot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(writers, aes(x=Type, y=Age)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/writers-2-1.pdf){fig-pos='H'}\n:::\n:::\n\nAt this point, a boxplot is best, since right now you are mostly after a general sense of what is going on, rather than assessing normality in particular (that will come later).\n\n\n$\\blacksquare$\n\n\n(c) Obtain a summary table showing, for each type of writing, the number of writers of that type, along with the mean, median and standard deviation of their ages at death.\n\nSolution\n\n\nThe customary `group_by` and `summarize`:\n\n::: {.cell}\n\n```{.r .cell-code}\nwriters %>% group_by(Type) %>% \nsummarize(n=n(), mean=mean(Age), med=median(Age), sd=sd(Age))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 5\n  Type           n  mean   med    sd\n  <chr>      <int> <dbl> <dbl> <dbl>\n1 Nonfiction    24  76.9  77.5  14.1\n2 Novels        67  71.4  73    13.1\n3 Poems         32  63.2  68    17.3\n```\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(d) Run a complete analysis, starting with an ordinary (not Welch) analysis of variance, that ends with a conclusion in the context of the data and an assessment of assumptions.\n\nSolution\n\n\nI've left this fairly open-ended, to see how well you know what needs to be included and what it means. There is a lot of room here for explanatory text to show that you know what you are doing. One output followed by another *without* any explanatory text suggests that you are just copying what I did without any idea about why you are doing it.\n\nThe place to start is the ordinary (not Welch) ANOVA. You may not think that this is the best thing to do (you'll have a chance to talk about that later), but I wanted to make sure that you practiced the procedure:\n\n::: {.cell}\n\n```{.r .cell-code}\nwriters.1 <- aov(Age~Type, data=writers)\nsummary(writers.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value  Pr(>F)   \nType          2   2744  1372.1   6.563 0.00197 **\nResiduals   120  25088   209.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nThis says that the mean ages at death of the three groups of writers are not all the same, or that there are differences among those writers (in terms of mean age at death). \"The mean ages of the types of writer are different\" is not accurate enough, because it comes too close to saying that *all three* groups are different, which is more than you can say right now.\n\nThe $F$-test is significant, meaning that there are some differences among^[There might be differences between two things, but among three or more.] the means, and Tukey's method will enable us to see which ones differ:\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(writers.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Age ~ Type, data = writers)\n\n$Type\n                        diff       lwr       upr     p adj\nNovels-Nonfiction  -5.427239 -13.59016  2.735681 0.2591656\nPoems-Nonfiction  -13.687500 -22.95326 -4.421736 0.0018438\nPoems-Novels       -8.260261 -15.63375 -0.886772 0.0240459\n```\n:::\n:::\n\nThere is a significant difference in mean age at death between the poets and both the other types of writer. The novelists and the nonfiction writers do not differ significantly in mean age at death.\n\nWe know from the boxplots (or the summary table) that this significant difference was because the poets died *younger* on average, which is exactly what the literature student was trying to find out. Thus, female poets really do die younger on average than female writers of other types. It is best to bring this point out, since this is the reason we (or the literature student) were doing this analysis in the first place. See Extra 1 for more.\n\nSo now we need to assess the assumptions on which the ANOVA depends.\n\nThe assumption we made is that the ages at death of the authors of each different type had approximately a normal distribution (given the sample sizes) with approximately equal spread. The boxplots definitely look skewed to the left (well, not the poets so much, but the others definitely). So now consider the sample sizes: 24, 67, and 32 for the three groups (respectively), and make a call about whether you think the normality is good enough. You are certainly entitled to declare the two outliers on the nonfiction writers to be too extreme given a sample size of only 24. Recall that once one sample fails normality, that's all you need.\n\nNow, since you specifically want normality, you could reasonably look at normal quantile plots instead of the boxplots. Don't just get normal quantile plots, though; say something about why you want them instead of the boxplots you drew earlier:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(writers, aes(sample = Age)) +\nstat_qq() + stat_qq_line() + \nfacet_wrap(~Type)\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/writers-6-1.pdf){fig-pos='H'}\n:::\n:::\n\nI see that the Nonfiction writers have two outliers at the low end (and are otherwise not bad); the writers of Novels don't go up high enough (it's almost as if there is some magic that stops them living beyond 90!); the writers of Poems have a short-tailed distribution. You'll remember that short tails are not a problem, since the mean is still descriptive of such a distribution; it's *long* tails or outliers or skewness that you need to be worried about. The outliers in the Nonfiction writers are the biggest concern.\n\nAre you concerned that these outliers are a problem, given the sample size? There are only 24 nonfiction writers (from your table of means earlier), so the Central Limit Theorem will help a bit. Make a call about whether these outliers are a big enough problem. You can go either way on this, as long as you raise the relevant issues.\n\nAnother approach you might take is to look at the P-values. The one in the $F$-test is really small, and so is one of the ones in the Tukey. So even if you think the analysis is a bit off, those conclusions are not likely to change. The 0.02 P-value in the Tukey, however, is another story. This could become non-significant in actual fact if the P-value is not to be trusted.\n\nYet another approach (looking at the bootstrapped sampling distributions of the sample means) is in Extra 3. This gets more than a little hairy with three groups, especially doing it the way I do.\n\nIf you think that the normality is not good enough, it's a good idea to suggest that we might do a Mood's Median Test instead, and you could even do it (followed up with pairwise median tests). If you think that normality is all right, you might then look at the spreads. I think you ought to conclude that these are close enough to equal (the SDs from the summary table or the heights of the boxes on the boxplots), and so there is no need to do a Welch ANOVA. (Disagree if you like, but be prepared to make the case.)\n\nI have several Extras:\n\nExtra 1: having come to that tidy conclusion, we really ought to back off a bit. These writers were (we assume) a random sample of some population, but they were actually mostly Americans, with a few Canadian and Mexican writers. So this appears to be true at least for North American writers. But this is (or might be) a different thing to the Yeats quote about female Gaelic poets.\n\nThere is a more prosaic reason. It is harder (in most places, but especially North America) to get poetry published than it is to find a market for other types of writing. (A would-be novelist, say, can be a journalist or write for magazines to pay the bills while they try to find a publisher for their novel.) Thus a poet is living a more precarious existence, and that might bring about health problems.\n\nExtra 2: with the non-normality in mind, maybe Mood's median test is the thing:\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian_test(writers, Age, Type)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$grand_median\n[1] 73\n\n$table\n            above\ngroup        above below\n  Nonfiction    17     6\n  Novels        33    30\n  Poems         10    22\n\n$test\n       what       value\n1 statistic 9.872664561\n2        df 2.000000000\n3   P-value 0.007180888\n```\n:::\n:::\n\nThe P-value here is a bit bigger than for the $F$-test, but it is still clearly significant. Hence, we do the pairwise median tests to find out which medians differ:\n\n::: {.cell}\n\n```{.r .cell-code}\npairwise_median_test(writers, Age, Type)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 4\n  g1         g2     p_value adj_p_value\n  <chr>      <chr>    <dbl>       <dbl>\n1 Nonfiction Novels 0.0531      0.159  \n2 Nonfiction Poems  0.00119     0.00358\n3 Novels     Poems  0.0142      0.0426 \n```\n:::\n:::\n\nThe conclusion here is exactly the same as for the ANOVA. The P-values have moved around a bit, though: the first one is a little closer to significance (remember, look at the last column since we are doing three tests at once) and the last one is now only just significant.\n\n::: {.cell}\n\n```{.r .cell-code}\nwriters %>% group_by(Type) %>% \nsummarize(n=n(), mean=mean(Age), med=median(Age), sd=sd(Age))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 5\n  Type           n  mean   med    sd\n  <chr>      <int> <dbl> <dbl> <dbl>\n1 Nonfiction    24  76.9  77.5  14.1\n2 Novels        67  71.4  73    13.1\n3 Poems         32  63.2  68    17.3\n```\n:::\n:::\n\nIn both of these two cases (Nonfiction-Novels and Novels-Poems), the medians are closer together than the means are. That would explain why the Novels-Poems P-value would increase, but not why the Nonfiction-Novels one would decrease.\n\nI would have no objection *in general* to your running a Mood's Median Test on these data, but the point of *this* problem was to give you practice with `aov`.\n\nExtra 3: the other way to assess if the normality is OK given the sample sizes is to obtain bootstrap sampling distributions of the sample means for each `Type`. The sample size for the novelists is 67, so I would expect the skewness there to be fine, but the two outliers among the Nonfiction writers may be cause for concern, since there are only 24 of those altogether.\n\nLet's see if we can do all three at once (I like living on the edge). I take things one step at a time, building up a pipeline as I go. Here's how it starts:\n\n::: {.cell}\n\n```{.r .cell-code}\nwriters %>% nest_by(Type)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 2\n# Rowwise:  Type\n  Type                     data\n  <chr>      <list<tibble[,2]>>\n1 Nonfiction           [24 x 2]\n2 Novels               [67 x 2]\n3 Poems                [32 x 2]\n```\n:::\n:::\n\nThe thing `data` is a so-called list-column. The dataframes we have mostly seen so far are like spreadsheets, in that each \"cell\" or \"entry\" in a dataframe has something like a number or a piece of text in it (or, occasionally, a thing that is True or False, or a date). Tibble-type dataframes are more flexible than that, however: each cell of a dataframe could contain *anything.*\n\nIn this one, the three things in the column `data` are each *dataframes*,^[Like those Russian dolls.] containing the column called `Age` from the original dataframe. These are the ages at death of the writers of that particular `Type`. These are the things we want bootstrap samples of.\n\nI'm not at all sure how this is going to go, so let's shoot for just 5 bootstrap samples to start with. If we can get it working, we can scale up the number of samples later, but having a smaller number of samples is easier to look at:\n\n::: {.cell}\n\n```{.r .cell-code}\nwriters %>% nest_by(Type) %>% \n  mutate(sim = list(1:5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 3\n# Rowwise:  Type\n  Type                     data sim      \n  <chr>      <list<tibble[,2]>> <list>   \n1 Nonfiction           [24 x 2] <int [5]>\n2 Novels               [67 x 2] <int [5]>\n3 Poems                [32 x 2] <int [5]>\n```\n:::\n:::\n\nLet me break off at this point to say that we want 1000 bootstrap samples for the writers of each type, so this is the kind of thing we need to start with. `nest_by` has an implied `rowwise`, so we get three lots of values in `sim`; the `list` is needed since each one is five values rather than just one. The next stage is to unnest these, and then do *another* `rowwise` to work with all the (more) rows of the dataframe we now have. After that, the process should look more or less familiar:\n\n::: {.cell}\n\n```{.r .cell-code}\nwriters %>% nest_by(Type) %>% \n  mutate(sim = list(1:5)) %>% \n  unnest(sim) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(data$Age, replace = TRUE)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15 x 4\n# Rowwise:  Type\n   Type                     data   sim my_sample \n   <chr>      <list<tibble[,2]>> <int> <list>    \n 1 Nonfiction           [24 x 2]     1 <dbl [24]>\n 2 Nonfiction           [24 x 2]     2 <dbl [24]>\n 3 Nonfiction           [24 x 2]     3 <dbl [24]>\n 4 Nonfiction           [24 x 2]     4 <dbl [24]>\n 5 Nonfiction           [24 x 2]     5 <dbl [24]>\n 6 Novels               [67 x 2]     1 <dbl [67]>\n 7 Novels               [67 x 2]     2 <dbl [67]>\n 8 Novels               [67 x 2]     3 <dbl [67]>\n 9 Novels               [67 x 2]     4 <dbl [67]>\n10 Novels               [67 x 2]     5 <dbl [67]>\n11 Poems                [32 x 2]     1 <dbl [32]>\n12 Poems                [32 x 2]     2 <dbl [32]>\n13 Poems                [32 x 2]     3 <dbl [32]>\n14 Poems                [32 x 2]     4 <dbl [32]>\n15 Poems                [32 x 2]     5 <dbl [32]>\n```\n:::\n:::\n\nThat seems to be about the right thing; the bootstrap samples appear to be the right size, considering how many writers of each type our dataset had. From here, work out the mean of each sample:\n\n::: {.cell}\n\n```{.r .cell-code}\nwriters %>% nest_by(Type) %>% \n  mutate(sim = list(1:5)) %>% \n  unnest(sim) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(data$Age, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15 x 5\n# Rowwise:  Type\n   Type                     data   sim my_sample  my_mean\n   <chr>      <list<tibble[,2]>> <int> <list>       <dbl>\n 1 Nonfiction           [24 x 2]     1 <dbl [24]>    73.9\n 2 Nonfiction           [24 x 2]     2 <dbl [24]>    80.4\n 3 Nonfiction           [24 x 2]     3 <dbl [24]>    77.3\n 4 Nonfiction           [24 x 2]     4 <dbl [24]>    83.8\n 5 Nonfiction           [24 x 2]     5 <dbl [24]>    79.8\n 6 Novels               [67 x 2]     1 <dbl [67]>    72.4\n 7 Novels               [67 x 2]     2 <dbl [67]>    72.7\n 8 Novels               [67 x 2]     3 <dbl [67]>    75.2\n 9 Novels               [67 x 2]     4 <dbl [67]>    70.8\n10 Novels               [67 x 2]     5 <dbl [67]>    69.1\n11 Poems                [32 x 2]     1 <dbl [32]>    61.7\n12 Poems                [32 x 2]     2 <dbl [32]>    60.7\n13 Poems                [32 x 2]     3 <dbl [32]>    59.2\n14 Poems                [32 x 2]     4 <dbl [32]>    60.2\n15 Poems                [32 x 2]     5 <dbl [32]>    66.3\n```\n:::\n:::\n\nand then you could plot those means. This seems to be working, so let's scale up to 1000 simulations, and make normal quantile plots of the bootstrapped sampling distributions, one for each Type of writer:\n\n::: {.cell}\n\n```{.r .cell-code}\nwriters %>% nest_by(Type) %>% \n  mutate(sim = list(1:1000)) %>% \n  unnest(sim) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(data$Age, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) %>% \n  ggplot(aes(sample = my_mean)) + stat_qq() + \n  stat_qq_line() + facet_wrap(~Type, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/writers-14-1.pdf){fig-pos='H'}\n:::\n:::\n\nThese three normal quantile plots are all acceptable, to my mind, although the Nonfiction one, with the two outliers and the smallest sample size, is still a tiny bit skewed to the left. Apart from that, the three sampling distributions of the sample means are close to normal, so our `aov` is much better than you might have thought from looking at the boxplots. That's the result of having large enough samples to get help from the Central Limit Theorem. \n\n\n$\\blacksquare$\n\n\n\n\n\n## Religion and studying\n\n Many students at a certain university were asked about the importance of religion in their lives (categorized as \"not\", \"fairly\", or \"very\" important), and also about the number of \nhours they spent studying per week. (This was part of a much larger survey.) We want to see whether there is any kind of relationship between these two variables. The data are in [here](http://ritsokiguess.site/datafiles/student_relig.csv). \n\n\n\n(a) Read in and display (some of) the data.\n\nSolution\n\n\nThe usual. This is a straightforward one:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/student_relig.csv\"\nstudent <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 686 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): ReligImp\ndbl (1): StudyHrs\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nstudent\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 686 x 2\n   ReligImp StudyHrs\n   <chr>       <dbl>\n 1 Fairly          3\n 2 Fairly         30\n 3 Fairly         16\n 4 Not             4\n 5 Not            12\n 6 Fairly         20\n 7 Fairly          4\n 8 Not            15\n 9 Fairly          7\n10 Fairly         40\n# i 676 more rows\n```\n:::\n:::\n\n686 students, with columns obviously named for religious importance and study hours.\n\nExtra: \n\nI said this came from a bigger survey, actually this one:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/student0405.csv\"\nstudent0 <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 690 Columns: 7\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (3): Sex, ReligImp, Seat\ndbl (4): GPA, MissClass, PartyDays, StudyHrs\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nstudent0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 690 x 7\n   Sex      GPA ReligImp MissClass Seat   PartyDays StudyHrs\n   <chr>  <dbl> <chr>        <dbl> <chr>      <dbl>    <dbl>\n 1 Female  3.7  Fairly           1 Back           5        3\n 2 Male    3.2  Fairly           3 Front          3       30\n 3 Female  3.01 Fairly           0 Middle         8       16\n 4 Female  3.77 Not              0 Middle         0        4\n 5 Male    3.28 Not              0 Middle         8       12\n 6 Female  2.8  Fairly           0 Middle         2       20\n 7 Male    2.5  Fairly           3 Back           1        4\n 8 Male    3.11 Not              0 Front          2       15\n 9 Male    3.15 Fairly           2 Back          15        7\n10 Male    3.44 Fairly           0 Middle         1       40\n# i 680 more rows\n```\n:::\n:::\n\nThere are four extra rows here. Why? Let's look at a `summary` of the dataframe:\n\n\\small\n::: {.cell}\n\n```{.r .cell-code}\nsummary(student0) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Sex                 GPA          ReligImp           MissClass     \n Length:690         Min.   :1.500   Length:690         Min.   :0.0000  \n Class :character   1st Qu.:2.930   Class :character   1st Qu.:0.0000  \n Mode  :character   Median :3.200   Mode  :character   Median :1.0000  \n                    Mean   :3.179                      Mean   :0.9064  \n                    3rd Qu.:3.515                      3rd Qu.:1.0000  \n                    Max.   :4.000                      Max.   :6.0000  \n                    NA's   :3                          NA's   :1       \n     Seat             PartyDays         StudyHrs    \n Length:690         Min.   : 0.000   Min.   : 0.00  \n Class :character   1st Qu.: 3.000   1st Qu.: 6.25  \n Mode  :character   Median : 7.000   Median :10.00  \n                    Mean   : 7.501   Mean   :13.16  \n                    3rd Qu.:11.000   3rd Qu.:16.00  \n                    Max.   :31.000   Max.   :70.00  \n                                     NA's   :4      \n```\n:::\n:::\n\\normalsize\n\nYou get information about each variable. For the text variables, you don't learn much, only how many there are. (See later for more on this.)\nFor each of the four quantitative variables, you see\nsome stats about each one, along with a count of missing values. The study hours variable is evidently skewed to the right (mean bigger than median), which we will have to think about later.\n\nR also has a \"factor\" variable type, which is the \"official\" way to handle categorical variables in R. Sometimes it matters, but most of the time leaving categorical variables as text is just fine. `summary` handles these\ndifferently. My second line of code below says \"for each variable that is text, make it into a factor\":\n\n::: {.cell}\n\n```{.r .cell-code}\nstudent0 %>% \nmutate(across(where(is.character), ~factor(.))) %>% \nsummary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Sex           GPA          ReligImp     MissClass          Seat    \n Female:382   Min.   :1.500   Fairly:319   Min.   :0.0000   Back  :134  \n Male  :308   1st Qu.:2.930   Not   :222   1st Qu.:0.0000   Front :151  \n              Median :3.200   Very  :149   Median :1.0000   Middle:404  \n              Mean   :3.179                Mean   :0.9064   NA's  :  1  \n              3rd Qu.:3.515                3rd Qu.:1.0000               \n              Max.   :4.000                Max.   :6.0000               \n              NA's   :3                    NA's   :1                    \n   PartyDays         StudyHrs    \n Min.   : 0.000   Min.   : 0.00  \n 1st Qu.: 3.000   1st Qu.: 6.25  \n Median : 7.000   Median :10.00  \n Mean   : 7.501   Mean   :13.16  \n 3rd Qu.:11.000   3rd Qu.:16.00  \n Max.   :31.000   Max.   :70.00  \n                  NA's   :4      \n```\n:::\n:::\n\nFor factors, you also get how many observations there are in each category, and the number of missing values, which we didn't get before.\nHowever, `ReligImp` does not have any missing values.\n\nI said there were four missing values for study hours, that is, four students who left that blank on their survey. \nWe want to get rid of those students (that is, remove those whole rows), and, to simplify things for you, let's keep only the study hours and importance of religion columns. That goes like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nstudent0 %>% drop_na(StudyHrs) %>% \nselect(ReligImp, StudyHrs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 686 x 2\n   ReligImp StudyHrs\n   <chr>       <dbl>\n 1 Fairly          3\n 2 Fairly         30\n 3 Fairly         16\n 4 Not             4\n 5 Not            12\n 6 Fairly         20\n 7 Fairly          4\n 8 Not            15\n 9 Fairly          7\n10 Fairly         40\n# i 676 more rows\n```\n:::\n:::\n\nThen I saved that for you. 686 rows instead of 690, having removed the four rows with missing `StudyHrs`.\n\n\nAnother (better, but more complicated) option is to use the package `pointblank`, which produces much more detailed data validation reports. You would start that by piping your data into `scan_data()` to get a (very) detailed report of missingness and data values, and then you can check your data for particular problems, such as missing values, or values bigger or smaller than they should be, for the variables you care about. See [here](https://github.com/rich-iannone/pointblank) for more.\n\n\n$\\blacksquare$\n\n\n(b) Obtain the number of observations and the mean and standard deviation of study hours for each level of importance.\n\nSolution\n\n\n`group_by` and `summarize` (spelling the latter with s or z as you prefer):\n\n::: {.cell}\n\n```{.r .cell-code}\nstudent %>% group_by(ReligImp) %>% \nsummarize(n=n(), mean_sh=mean(StudyHrs), sd_sh=sd(StudyHrs))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 4\n  ReligImp     n mean_sh sd_sh\n  <chr>    <int>   <dbl> <dbl>\n1 Fairly     316    12.9  9.00\n2 Not        222    11.7  8.49\n3 Very       148    16.0 11.3 \n```\n:::\n:::\n\n\n\n$\\blacksquare$\n\n\n(c) Comment briefly on how the groups compare in terms of study hours.\n\nSolution\n\n\nThe students who think religion is very important have a higher mean number of study hours. The other two groups seem similar. \n\nAs far as the SDs are concerned, make a call. You could say that the very-important group also has a (slightly) larger SD, or you could say that the SDs are all very similar.  \nI would actually favour the second one, but this is going to be a question about Welch ANOVA, so go whichever way you like.\n\n\n$\\blacksquare$\n\n\n(d) Make a suitable graph of this data set.\n\nSolution\n\n\nThis kind of data is one quantitative and one categorical variable, so once again a boxplot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(student, aes(x=ReligImp, y=StudyHrs)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/relig-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(e) The statistician in this study decided that the data were sufficiently normal in shape given the (very large) sample sizes, but was concerned about unequal spreads among the three groups. \nGiven this,\nrun a suitable analysis and display the output. (This includes a suitable follow-up test, if warranted.)\n\nSolution\n\n\nNormal-enough data (in the statistician's estimation) and unequal spreads means a Welch ANOVA:\n\n::: {.cell}\n\n```{.r .cell-code}\noneway.test(StudyHrs~ReligImp, data=student)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne-way analysis of means (not assuming equal variances)\n\ndata:  StudyHrs and ReligImp\nF = 7.9259, num df = 2.0, denom df = 350.4, p-value = 0.0004299\n```\n:::\n\n```{.r .cell-code}\ngamesHowellTest(StudyHrs~factor(ReligImp), data=student)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n\tPairwise comparisons using Games-Howell test\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ndata: StudyHrs by factor(ReligImp)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n     Fairly  Not    \nNot  0.26035 -      \nVery 0.00906 0.00026\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nP value adjustment method: none\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nalternative hypothesis: two.sided\n```\n:::\n:::\n\nGames-Howell is the suitable follow-up here, to go with the Welch ANOVA. It is warranted because the Welch ANOVA was significant.\n\nMake sure you have installed and loaded `PMCMRplus` before trying the second half of this.\n\nExtra: for large data sets, boxplots make it look as if the outlier problem is bad, because a boxplot of a large amount of data will almost certainly contain some outliers (according to Tukey's definition). \nTukey envisaged a boxplot as something you could draw by hand for a smallish data set, and couldn't foresee something like R and the kind of data we might be able to deal with. To show you the kind of thing I mean, let's draw some random samples of varying sizes from normal distributions, which should not have outliers, and see how their boxplots look:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(n=c(10, 30, 100, 300)) %>% \n  rowwise() %>% \n  mutate(my_sample = list(rnorm(n))) %>% \n  unnest(my_sample) %>% \n  ggplot(aes(x = factor(n), y = my_sample)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/relig-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nAs the sample size gets bigger, the number of outliers gets bigger, and the whiskers get longer. \nAll this means is that in a larger sample, you are more likely to see a small number of values that are further out, and that is not necessarily a reason for concern. Here, the outliers are only one value out of 100 and two out of 300, but they have what looks like an outsize influence on the plot. In the boxplot for our data, the distributions were a bit skewed, but the outliers may not have been as much of a problem as they looked.\n\n\n\n$\\blacksquare$\n\n\n(f) What do you conclude from your analysis of the previous part, in the context of the data?\n\nSolution\n\n\nThe Welch ANOVA was significant, so the religious-importance groups are not all the same in terms of mean study hours, and we need to figure out which groups differ from which. (Or say this in the previous part if you wish.)\n\nThe students for whom religion was very important had a significantly different mean number of study hours than the other students; the Fairly and Not groups were not significantly different from each other.\nLooking back at the means (or the boxplots), the significance was because the Very group studied for *more* hours than the other groups. \nIt seems that religion has to be very important to a student to positively affect how much they study.\n\nExtra: you might have been concerned that the study hours within the groups were not nearly normal enough to trust the Welch ANOVA. But the groups were large, so there is a lot of help from the Central Limit Theorem.\nEnough? Well, that is hard to judge.\n\nMy take on this is to bootstrap the sampling distribution of the sample mean for each group. If *that* looks normal, then we ought to be able to trust the $F$-test (regular or Welch, as appropriate). The code is complicated (I'll explain the ideas below):\n\n::: {.cell}\n\n```{.r .cell-code}\nstudent %>% \n  nest_by(ReligImp) %>% \n  mutate(sim = list(1:1000)) %>% \n  unnest(sim) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(data$StudyHrs, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) %>% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line() +\n  facet_wrap(~ReligImp, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](analysis-of-variance_files/figure-pdf/relig-11-1.pdf){fig-pos='H'}\n:::\n:::\n\nTo truly understand what's going on, you probably need to run this code one line at a time.\n\nAnyway, these normal quantile plots are *very* normal. This says that the sampling distributions of the sample means are *very much* normal in shape, which means that\nthe sample sizes are definitely large enough to overcome the apparently bad skewness that we saw on the boxplots. In other words, using a regular or Welch ANOVA will be perfectly good; there is no need to reach for Mood's median test here, despite what you might think from looking at the boxplots, because the sample sizes are so large.\n\nThe code, line by line:\n\n- create mini-data-frames called `data`, containing one column called `StudyHrs`, for each `ReligImp` group\n- set up for 1000 bootstrap samples for each group, and (next line) arrange for one row per bootstrap sample\n- work rowwise\n- generate the bootstrap samples\n- work out the mean of each bootstrap sample \n- plot normal quantile plots of them, using different facets for each group.\n\nFinally, you might have wondered whether we needed to do Welch:\n\n::: {.cell}\n\n```{.r .cell-code}\nstudent.1 <- aov(StudyHrs~ReligImp, data=student)\nsummary(student.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nReligImp      2   1721   860.7   9.768 6.57e-05 ***\nResiduals   683  60184    88.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nTukeyHSD(student.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = StudyHrs ~ ReligImp, data = student)\n\n$ReligImp\n                 diff        lwr       upr     p adj\nNot-Fairly  -1.195917 -3.1267811 0.7349462 0.3135501\nVery-Fairly  3.143047  0.9468809 5.3392122 0.0023566\nVery-Not     4.338964  1.9991894 6.6787385 0.0000454\n```\n:::\n:::\n\nIt didn't make much difference, and the conclusions are identical. So I think either way would have been defensible. \n\nThe value of doing Tukey is that we get confidence intervals for the difference of means between each group, and this gives us an \"effect size\": the students for whom religion was very important studied on average three or four hours per week more than the other students, and you can look at the confidence intervals to see how much uncertainty there is in those estimates. Students vary a lot in how much they study, but the sample sizes are large, so the intervals are not that long.\n\n\n$\\blacksquare$\n\n\n\n\n\n",
    "supporting": [
      "analysis-of-variance_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}