{
  "hash": "44739edc2a4e591b969ec87e3d351a87",
  "result": {
<<<<<<< HEAD
    "engine": "knitr",
    "markdown": "# Logistic regression with ordinal response\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Do you like your mobile phone?\n\n\n A phone company commissioned a survey of their customers'\nsatisfaction with their mobile devices. The responses to the survey\nwere on a so-called Likert scale of \"very unsatisfied\",\n\"unsatisfied\", \"satisfied\", \"very satisfied\". Also recorded were\neach customer's gender and age group (under 18, 18--24, 25--30, 31 or\nolder). (A survey of this kind does not ask its respondents for their\nexact age, only which age group they fall in.) The data, as\nfrequencies of people falling into each category combination, are in [link](http://ritsokiguess.site/datafiles/mobile.txt).\n\n\n\n(a) <a name=\"part:gather\">*</a> Read in the data and take a look at the format. Use a tool\nthat you know about to arrange the frequencies in just one column,\nwith other columns labelling the response categories that the\nfrequencies belong to. Save the new data frame. (Take a look at it\nif you like.)\n \n\n(b) We are going to fit ordered logistic models below. To\ndo that, we need our response variable to be a factor with its levels\nin the right order. By looking at the data frame\nyou just created, determine what kind\nof thing your intended response variable currently is.\n\n\n\n\n(c) If your intended response variable is not a factor, create a factor in your data frame with levels in the right order. Hint: look at the order your levels are *in the data*.\n\n\n\n\n(d) <a name=\"part:thefit\">*</a>\nFit ordered logistic models to predict satisfaction from (i) gender\nand age group, (ii) gender only, (iii) age group only. (You don't\nneed to examine the models.) Don't forget a suitable\n`weights`!\n \n\n(e) Use `drop1` on your model containing both explanatory\nvariables to determine whether you can remove either of them. Use\n`test=\"Chisq\"` to obtain P-values.\n\n\n\n\n(f) Use `anova` to decide whether we are justified in\nremoving `gender` from a model containing both `gender`\nand `age.group`. Compare your P-value with the one from `drop1`.\n \n\n(g) Use `anova` to see whether we are justified in removing\n`age.group` from a model  containing both `gender` and\n`age.group`. Compare your P-value with the one from\n`drop1` above.\n \n\n(h) Which of the models you have fit so far is the most\nappropriate one? Explain briefly.\n \n\n(i) Obtain predicted probabilities of a\ncustomer falling in the various satisfaction categories, as it\ndepends on gender and age group. To do that, you need to feed\n`predict` three things: the fitted model that contains both\nage group and gender, the data frame that you read in from the file\nback in part (<a href=\"#part:gather\">here</a>) (which contains all the combinations of age group\nand gender), and an appropriate `type`.\n\n \n\n(j) <a name=\"part:unload\">*</a> \nDescribe any patterns you see in the predictions, bearing in mind the\nsignificance or not of the explanatory variables.\n\n \n\n\n \n\n\n\n\n\n##  Finding non-missing values\n\n\n <a name=\"part:prepare-next\">*</a> This is to prepare you for something in the next\nquestion. It's meant to be easy.\n\nIn R, the code `NA` stands for \"missing value\" or\n\"value not known\". In R, `NA` should not have quotes around\nit. (It is a special code, not a piece of text.)\n\n\n(a) Create a vector `v` that contains some numbers and some\nmissing values, using `c()`. Put those values into a\none-column data frame.\n \n\n(b) Obtain a new column containing `is.na(v)`. When is this true and when is this false?\n \n\n(c) The symbol `!` means \"not\" in R (and other\nprogramming languages). What does `!is.na(v)` do? Create a\nnew column containing that.\n \n\n(d) Use `filter` to display just the\nrows of your data frame that have a non-missing value of `v`.\n\n \n\n\n\n\n\n\n##  High School and Beyond\n\n\n A survey called High School and Beyond was given to a large\nnumber of American high school seniors (grade 12) by the National\nCenter of Education Statistics. The data set at\n[link](http://ritsokiguess.site/datafiles/hsb.csv) is a random\nsample of 200 of those students.\n\nThe variables collected are:\n\n\n\n* `gender`: student's gender, female or male.\n\n* `race`: the student's race (African-American,\nAsian,^[I'm always amused at how Americans put all Asians    into one group.]  Hispanic, White).\n\n* `ses`: Socio-economic status of student's family (low,\nmiddle, or high)\n\n* `schtyp`: School type, public or private.\n\n* `prog`: Student's program, general, academic, or\nvocational. \n\n* `read`: Score on standardized reading test.\n\n* `write`: Score on standardized writing test.\n\n* `math`: Score on standardized math test.\n\n* `science`: Score on standardized science test.\n\n* `socst`: Score on standardized social studies test.\n\n\n\nOur aim is to see how socio-economic status is related to the other\nvariables. \n\n\n\n(a) Read in and display (some of) the data.\n\n\n(b) Explain briefly why an ordinal logistic regression is\nappropriate for our aims.\n\n\n(c) Fit an ordinal logistic regression predicting\nsocio-economic status from the scores on the five standardized\ntests. (You don't need to display the results.) You will probably\ngo wrong the first time. What kind of thing does your response\nvariable have to be? \n\n\n(d) Remove any non-significant explanatory variables one at a\ntime. Use `drop1` to decide which one to remove next.\n\n\n(e) The quartiles of the `science` test score are 44 \nand 58. The quartiles of the `socst` test score are 46 and 61. Make\na data frame that has all combinations of those quartiles. If your best\nregression had any other explanatory variables in it, also put the\n*means* of those variables into this data frame.\n\n\n\n(f) Use the data frame you created in the previous part, together\nwith your best model, to obtain predicted probabilities of being in\neach `ses` category. Display these predicted probabilities so that they are easy to read.\n\n\n\n(g) What is the effect of an increased science score on the\nlikelihood of a student being in the different socioeconomic groups,\nall else equal?  Explain briefly. In your explanation, state clearly\nhow you are using your answer to the previous part.\n\n\n\n\n\n\n\n\n##  How do you like your steak?\n\n\n When you order a steak in a restaurant, the server will ask\nyou how you would like it cooked, or to be precise, *how much*\nyou would like it cooked: rare (hardly cooked at all), through medium\nrare, medium, medium well to well (which means \"well done\", so that\nthe meat has only a little red to it). Could you guess how a person\nlikes their steak cooked, from some other information about them? The\nwebsite [link](fivethirtyeight.com) commissioned a survey where they\nasked a number of people how they preferred their steak, along with as\nmany other things as they could think of to ask. (Many of the\nvariables below are related to risk-taking, which was something the\npeople designing the survey thought might have something to do with\nliking steak rare.) The variables of interest are all factors or true/false:\n\n\n\n* `respondent_ID`: a ten-digit number identifying each\nperson who responded to the survey.\n\n* `lottery_a`: true if the respondent preferred lottery A\nwith a small chance to win a lot of money, to lottery B, with a\nlarger chance to win less money.\n\n* `smoke`: true if the respondent is currently a smoker\n\n* `alcohol`: true if the respondent at least occasionally\ndrinks alcohol.\n\n* `gamble`: true if the respondent likes to gamble (eg.\nbetting on horse racing or playing the lottery)\n\n* `skydiving`: true if the respondent has ever been\nskydiving.\n\n* `speed`: true if the respondent likes to drive fast\n\n* `cheated`: true if the respondent has ever cheated on a\nspouse or girlfriend/boyfriend\n\n* `steak`: true if the respondent likes to eat steak\n\n* `steak_prep` (response): how the respondent likes their\nsteak cooked (factor, as described above, with 5 levels).\n\n* `female`: true if the respondent is female\n\n* `age`: age group, from 18--29 to 60+.\n\n* `hhold_income`: household income group, from \\$0--24,999\nto \\$150,000+.\n\n* `educ`: highest level of education attained, from \n\"less than high school\" up to \"graduate degree\"\n\n* `region`: region (of the US)\nthat the respondent lives in (five values).\n\n\nThe data are in\n[link](http://ritsokiguess.site/datafiles/steak1.csv). This is the\ncleaned-up data from a previous question, with the missing values removed.\n\n\n\n(a) Read in the data and display the first few lines.\n\n\n\n\n(b) We are going to predict `steak_prep` from some of\nthe other variables. Why is the model-fitting function `polr`\nfrom package `MASS` the best choice for these data\n(alternatives being `glm` and `multinom` from package\n`nnet`)?\n\n\n\n(c) What are the levels of `steak_prep`, \n*in the  order that R thinks they are in?* If they are not in a sensible\norder, create an ordered factor where the levels are in a sensible order.\n\n\n\n(d) Fit a model predicting preferred steak preparation in an\nordinal logistic regression from `educ`, `female` and\n`lottery_a`. This ought to be easy from your previous work,\nbut you have to be careful about one thing. No need to print out the\nresults. \n\n\n\n(e) Run `drop1` on your fitted model, with\n`test=\"Chisq\"`. Which explanatory variable should be removed\nfirst, if any? Bear in mind that the variable with the\n*smallest* AIC should come out first, in case your table\ndoesn't get printed in order.\n\n\n\n(f) Remove the variable that should come out first, using\n`update`. (If all the variables should stay, you can skip\nthis part.)\n\n\n\n(g) Using the best model that you have so far, predict the\nprobabilities of preferring each different steak preparation (method\nof cooking) for each combination of the variables that\nremain. (Some of the variables are TRUE and FALSE rather than\nfactors. Bear this in mind.)\nDescribe the effects of each variable on the predicted\nprobabilities, if any. Note that there is exactly one person in the\nstudy whose educational level is \"less than high school\".\n\n\n\n(h) Is it reasonable to remove *all* the remaining\nexplanatory variables from your best model so far? Fit a model with no explanatory variables,\nand do a test. (In R, if the right side of the squiggle is a\n`1`, that means \"just an intercept\". Or you can remove\nwhatever remains using `update`.) What do you conclude?\nExplain briefly.\n\n\n\n(i) In the article for which these data were collected,\n[link](https://fivethirtyeight.com/datalab/how-americans-like-their-steak/),\ndoes the author obtain consistent conclusions with yours? Explain\nbriefly. (It's not a very long article, so it won't take you long to\nskim through, and the author's point is pretty clear.)\n\n\n\n\n\n\n##  How do you like your steak -- the data\n\n\n  <a name=\"q:steak-data\">*</a> \nThis question takes you through the data preparation for one\nof the other questions. You don't have to do *this*\nquestion, but you may find it interesting or useful.\n\nWhen you order a steak in a restaurant, the server will ask\nyou how you would like it cooked, or to be precise, *how much*\nyou would like it cooked: rare (hardly cooked at all), through medium\nrare, medium, medium well to well (which means \"well done\", so that\nthe meat has only a little red to it). Could you guess how a person\nlikes their steak cooked, from some other information about them? The\nwebsite [link](fivethirtyeight.com) commissioned a survey where they\nasked a number of people how they preferred their steak, along with as\nmany other things as they could think of to ask. (Many of the\nvariables below are related to risk-taking, which was something the\npeople designing the survey thought might have something to do with\nliking steak rare.) The variables of interest are all factors or true/false:\n\n\n\n* `respondent_ID`: a ten-digit number identifying each\nperson who responded to the survey.\n\n* `lottery_a`: true if the respondent preferred lottery A\nwith a small chance to win a lot of money, to lottery B, with a\nlarger chance to win less money.\n\n* `smoke`: true if the respondent is currently a smoker\n\n* `alcohol`: true if the respondent at least occasionally\ndrinks alcohol.\n\n* `gamble`: true if the respondent likes to gamble (eg.\nbetting on horse racing or playing the lottery)\n\n* `skydiving`: true if the respondent has ever been\nskydiving.\n\n* `speed`: true if the respondent likes to drive fast\n\n* `cheated`: true if the respondent has ever cheated on a\nspouse or girlfriend/boyfriend\n\n* `steak`: true if the respondent likes to eat steak\n\n* `steak_prep` (response): how the respondent likes their\nsteak cooked (factor, as described above, with 5 levels).\n\n* `female`: true if the respondent is female\n\n* `age`: age group, from 18--29 to 60+.\n\n* `hhold_income`: household income group, from \\$0--24,999\nto \\$150,000+.\n\n* `educ`: highest level of education attained, from \n\"less  than high school\" \nup to \"graduate degree\"\n\n* `region`: region (of the US)\nthat the respondent lives in (five values).\n\n\nThe data are in\n[link](https://raw.githubusercontent.com/nxskok/datafiles/master/steak.csv). \n\n\n\n(a) Read in the data and display the first few lines.\n\n\n\n(b) What do you immediately notice about your data frame? Run `summary` on the entire data frame. Would you say you have a lot of missing values, or only a few?\n\n\n(c) What does the function `drop_na` do when applied to a data frame with missing values? To find out, pass the data frame into `drop_na()`, then into `summary` again. What has happened?\n\n\n(d) Write the data into a `.csv` file, with a name like\n`steak1.csv`.  Open this file in a spreadsheet and (quickly)\nverify that you have the right columns and no missing values.\n\n\n\n\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Do you like your mobile phone?\n\n\n A phone company commissioned a survey of their customers'\nsatisfaction with their mobile devices. The responses to the survey\nwere on a so-called Likert scale of \"very unsatisfied\",\n\"unsatisfied\", \"satisfied\", \"very satisfied\". Also recorded were\neach customer's gender and age group (under 18, 18--24, 25--30, 31 or\nolder). (A survey of this kind does not ask its respondents for their\nexact age, only which age group they fall in.) The data, as\nfrequencies of people falling into each category combination, are in [link](http://ritsokiguess.site/datafiles/mobile.txt).\n\n\n\n(a) <a name=\"part:gather\">*</a> Read in the data and take a look at the format. Use a tool\nthat you know about to arrange the frequencies in just one column,\nwith other columns labelling the response categories that the\nfrequencies belong to. Save the new data frame. (Take a look at it\nif you like.)\n \nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/mobile.txt\"\nmobile <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 8 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (2): gender, age.group\ndbl (4): very.unsat, unsat, sat, very.sat\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nmobile\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 6\n  gender age.group very.unsat unsat   sat very.sat\n  <chr>  <chr>          <dbl> <dbl> <dbl>    <dbl>\n1 male   0-17               3     9    18       24\n2 male   18-24              6    13    16       28\n3 male   25-30              9    13    17       20\n4 male   31+                5     7    16       16\n5 female 0-17               4     8    11       25\n6 female 18-24              8    14    20       18\n7 female 25-30             10    15    16       12\n8 female 31+                5    14    12        8\n```\n\n\n:::\n:::\n\n \nWith multiple columns that are all frequencies, this is a job for\n`pivot_longer`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile %>% \n  pivot_longer(very.unsat:very.sat, \n               names_to=\"satisfied\", \n               values_to=\"frequency\") -> mobile.long\nmobile.long\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 32 x 4\n   gender age.group satisfied  frequency\n   <chr>  <chr>     <chr>          <dbl>\n 1 male   0-17      very.unsat         3\n 2 male   0-17      unsat              9\n 3 male   0-17      sat               18\n 4 male   0-17      very.sat          24\n 5 male   18-24     very.unsat         6\n 6 male   18-24     unsat             13\n 7 male   18-24     sat               16\n 8 male   18-24     very.sat          28\n 9 male   25-30     very.unsat         9\n10 male   25-30     unsat             13\n# i 22 more rows\n```\n\n\n:::\n:::\n\n     \n\nYep, all good. See how `mobile.long` contains what it should?\n(For those keeping track, the original data frame had 8 rows and 4\ncolumns to collect up, and the new one has $8\\times 4=32$ rows.)\n \n\n$\\blacksquare$\n\n(b) We are going to fit ordered logistic models below. To\ndo that, we need our response variable to be a factor with its levels\nin the right order. By looking at the data frame\nyou just created, determine what kind\nof thing your intended response variable currently is.\n\n\n\nSolution\n\n\nI looked at `mobile.long` in the previous part, but if you\ndidn't, look at it here:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.long\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 32 x 4\n   gender age.group satisfied  frequency\n   <chr>  <chr>     <chr>          <dbl>\n 1 male   0-17      very.unsat         3\n 2 male   0-17      unsat              9\n 3 male   0-17      sat               18\n 4 male   0-17      very.sat          24\n 5 male   18-24     very.unsat         6\n 6 male   18-24     unsat             13\n 7 male   18-24     sat               16\n 8 male   18-24     very.sat          28\n 9 male   25-30     very.unsat         9\n10 male   25-30     unsat             13\n# i 22 more rows\n```\n\n\n:::\n:::\n\n     \n\nMy intended response variable is what I called `satisfied`.\nThis is `chr` or \"text\", not the `factor` that I\nwant. \n\n\n\n$\\blacksquare$\n\n(c) If your intended response variable is not a factor, create a factor in your data frame with levels in the right order. Hint: look at the order your levels are *in the data*.\n\n\n\nSolution\n\n\nMy intended response `satisfied` is text, not a factor, so\nI need to do this part.\nThe hint is to look at the column `satisfied` in\n`mobile.long` and note that the satisfaction categories\nappear in the data *in the order that we want*. This is good\nnews, because we can use `fct_inorder` like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.long %>%\n  mutate(satis = fct_inorder(satisfied)) -> mobile.long\n```\n:::\n\n     \n\nIf you check, by looking at the data frame, `satis` is \na `factor`, and you can also do this to verify that its levels\nare in the right order:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(mobile.long, levels(satis))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n```\n\n\n:::\n:::\n\n \nSuccess.\n\nExtra: so now you are asking, what if the levels are in the *wrong* order in the data? Well, below is what you used to have to do, and it will work for this as well.\nI'll first find what levels of satisfaction I have. This\ncan be done by counting them, or by finding the distinct ones:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.long %>% count(satisfied)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 2\n  satisfied      n\n  <chr>      <int>\n1 sat            8\n2 unsat          8\n3 very.sat       8\n4 very.unsat     8\n```\n\n\n:::\n:::\n\n     \n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.long %>% distinct(satisfied)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 1\n  satisfied \n  <chr>     \n1 very.unsat\n2 unsat     \n3 sat       \n4 very.sat  \n```\n\n\n:::\n:::\n\n \n\nIf you count them, they come out in alphabetical order. If you ask for\nthe distinct ones, they come out in the order they were in\n`mobile.long`, which is the order the *columns* of those\nnames were in `mobile`, which is the order we want. \n\nTo actually grab those satisfaction levels as a vector (that we will\nneed in a minute), use `pluck` to pull the column out of the\ndata frame as a vector:\n\n::: {.cell}\n\n```{.r .cell-code}\nv1 <- mobile.long %>%\n  distinct(satisfied) %>%\n  pluck(\"satisfied\")\nv1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n```\n\n\n:::\n:::\n\n \n\nwhich is in the correct order, or\n\n::: {.cell}\n\n```{.r .cell-code}\nv2 <- mobile.long %>%\n  count(satisfied) %>%\n  pluck(\"satisfied\")\nv2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"sat\"        \"unsat\"      \"very.sat\"   \"very.unsat\"\n```\n\n\n:::\n:::\n\n \n\nwhich is in alphabetical order. The problem with the second one is\nthat we know the correct order, but there isn't a good way to code\nthat, so we have to rearrange it ourselves. The correct order from\n`v2` is 4, 2, 1, 3, so:\n\n::: {.cell}\n\n```{.r .cell-code}\nv3 <- c(v2[4], v2[2], v2[1], v2[3])\nv3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n```\n\n\n:::\n\n```{.r .cell-code}\nv4 <- v2[c(4, 2, 1, 3)]\nv4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n```\n\n\n:::\n:::\n\n \n\nEither of these will work. The first one is more typing, but is\nperhaps more obvious. There is a third way, which is to keep things as\na data frame until the end, and use `slice` to pick out the\nrows in the right order:\n\n::: {.cell}\n\n```{.r .cell-code}\nv5 <- mobile.long %>%\n  count(satisfied) %>%\n  slice(c(4, 2, 1, 3)) %>%\n  pluck(\"satisfied\")\nv5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n```\n\n\n:::\n:::\n\n \n\nIf you don't see how that works, run it yourself, one line at a time.\n\nThe other way of doing this is to physically type them into a vector,\nbut this carries the usual warnings of requiring you to be very\ncareful and that it won't be reproducible (eg. if you do another\nsurvey with different response categories). \n\nSo now create the proper response\nvariable thus, using your vector of categories:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.long %>%\n  mutate(satis = ordered(satisfied, v1)) -> mobile.long2\nmobile.long2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 32 x 5\n   gender age.group satisfied  frequency satis     \n   <chr>  <chr>     <chr>          <dbl> <ord>     \n 1 male   0-17      very.unsat         3 very.unsat\n 2 male   0-17      unsat              9 unsat     \n 3 male   0-17      sat               18 sat       \n 4 male   0-17      very.sat          24 very.sat  \n 5 male   18-24     very.unsat         6 very.unsat\n 6 male   18-24     unsat             13 unsat     \n 7 male   18-24     sat               16 sat       \n 8 male   18-24     very.sat          28 very.sat  \n 9 male   25-30     very.unsat         9 very.unsat\n10 male   25-30     unsat             13 unsat     \n# i 22 more rows\n```\n\n\n:::\n:::\n\n \n\n`satis` has the same values as `satisfied`, but its\nlabel `ord` means that it is an ordered factor, as we want.\n\n\n$\\blacksquare$\n\n(d) <a name=\"part:thefit\">*</a>\nFit ordered logistic models to predict satisfaction from (i) gender\nand age group, (ii) gender only, (iii) age group only. (You don't\nneed to examine the models.) Don't forget a suitable\n`weights`!\n \nSolution\n\n\n(i):\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nmobile.1 <- polr(satis ~ gender + age.group, weights = frequency, data = mobile.long)\n```\n:::\n\n \n\nFor (ii) and (iii), `update` is the thing (it works for any\nkind of model):\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.2 <- update(mobile.1, . ~ . - age.group)\nmobile.3 <- update(mobile.1, . ~ . - gender)\n```\n:::\n\n \n\nWe're not going to look at these, because the output from\n`summary` is not very illuminating. What we do next is to try\nto figure out which (if either) of the explanatory variables\n`age.group` and `gender` we need.\n \n\n$\\blacksquare$\n\n(e) Use `drop1` on your model containing both explanatory\nvariables to determine whether you can remove either of them. Use\n`test=\"Chisq\"` to obtain P-values.\n\n\n\nSolution\n\n\n`drop1` takes a fitted model, and tests each term in\nit in turn, and says which (if any) should be removed. Here's how it goes:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(mobile.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nsatis ~ gender + age.group\n          Df    AIC     LRT Pr(>Chi)   \n<none>       1101.8                    \ngender     1 1104.2  4.4089 0.035751 * \nage.group  3 1109.0 13.1641 0.004295 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nThe possibilities are to remove `gender`, to remove\n`age.group` or to remove nothing. \nThe best one is \"remove nothing\", because it's the one on the output with the smallest\nAIC. Both P-values are small, so it would be a mistake to remove\neither of the explanatory variables.\n\n\n$\\blacksquare$\n\n(f) Use `anova` to decide whether we are justified in\nremoving `gender` from a model containing both `gender`\nand `age.group`. Compare your P-value with the one from `drop1`.\n \nSolution\n\n\nThis is a comparison of the model with both variables\n(`mobile.1`) and the model with `gender` removed\n(`mobile.3`). Use `anova` for this, smaller\n(fewer-$x$) model first:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(mobile.3, mobile.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio tests of ordinal regression models\n\nResponse: satis\n               Model Resid. df Resid. Dev   Test    Df LR stat.    Pr(Chi)\n1          age.group       414   1092.213                                 \n2 gender + age.group       413   1087.804 1 vs 2     1  4.40892 0.03575146\n```\n\n\n:::\n:::\n\n     \n\nThe P-value is (just) less than 0.05, so the models are significantly\ndifferent. That means that the model with both variables in fits\nsignificantly better than the model with only `age.group`, and\ntherefore that taking `gender` out is a mistake.\n\nThe P-value is identical to the one from `drop1` (because they\nare both doing the same test).\n \n$\\blacksquare$\n\n(g) Use `anova` to see whether we are justified in removing\n`age.group` from a model  containing both `gender` and\n`age.group`. Compare your P-value with the one from\n`drop1` above.\n \nSolution\n\n\nExactly the same idea as the last part. In my case, I'm comparing\nmodels `mobile.2` and `mobile.1`:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(mobile.2, mobile.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio tests of ordinal regression models\n\nResponse: satis\n               Model Resid. df Resid. Dev   Test    Df LR stat.     Pr(Chi)\n1             gender       416   1100.968                                  \n2 gender + age.group       413   1087.804 1 vs 2     3 13.16411 0.004294811\n```\n\n\n:::\n:::\n\n    \n\nThis one is definitely significant, so I need to keep\n`age.group` for sure. Again, the P-value is the same as the one\nin `drop1`.\n \n$\\blacksquare$\n\n(h) Which of the models you have fit so far is the most\nappropriate one? Explain briefly.\n \nSolution\n\n\nI can't drop either of my variables, so I have to keep them both:\n`mobile.1`, with both `age.group` and `gender`.\n  \n$\\blacksquare$\n\n(i) Obtain predicted probabilities of a\ncustomer falling in the various satisfaction categories, as it\ndepends on gender and age group. To do that, you need to feed\n`predictions` three things: the fitted model that contains both\nage group and gender, the data frame that you read in from the file\nback in part (<a href=\"#part:gather\">here</a>) (which contains all the combinations of age group\nand gender), and an appropriate `type`.\n\n \nSolution\n\nMy model containing both $x$s was `mobile.1`, the data frame\nread in from the file was called `mobile`, and I  need\n`type=\"p\"` to get probabilities. The first thing is to get the genders and age groups to make combinations of them, which you can do like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = mobile.1, \n                gender = levels(factor(mobile$gender)),\n                age.group = levels(factor(mobile$age.group)))\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  frequency gender age.group rowid\n1    13.125 female      0-17     1\n2    13.125 female     18-24     2\n3    13.125 female     25-30     3\n4    13.125 female       31+     4\n5    13.125   male      0-17     5\n6    13.125   male     18-24     6\n7    13.125   male     25-30     7\n8    13.125   male       31+     8\n```\n\n\n:::\n:::\n\nThe `levels(factor)` thing turns the (text) variable into a factor so that you can extract the distinct values using `levels`. You could also `count` the genders and age groups to find out which ones there are:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile %>% count(gender, age.group)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 3\n  gender age.group     n\n  <chr>  <chr>     <int>\n1 female 0-17          1\n2 female 18-24         1\n3 female 25-30         1\n4 female 31+           1\n5 male   0-17          1\n6 male   18-24         1\n7 male   25-30         1\n8 male   31+           1\n```\n\n\n:::\n:::\n\nThis gives you all the combinations, and so will also serve as a `new` without needing to use `datagrid`. Your choice.\n\nHaving done that, you now have a `new` to feed into `predictions`, but some care is still required:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(mobile.1, newdata = new)) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nRe-fitting to get Hessian\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   rowid      group   estimate  std.error statistic      p.value   s.value\n1      1 very.unsat 0.08590744 0.01935201  4.439200 9.029377e-06  16.75694\n2      2 very.unsat 0.12858414 0.02486823  5.170620 2.333188e-07  22.03119\n3      3 very.unsat 0.18290971 0.03313939  5.519404 3.401515e-08  24.80925\n4      4 very.unsat 0.16112036 0.03279559  4.912866 8.975461e-07  20.08751\n5      5 very.unsat 0.06070852 0.01420828  4.272755 1.930724e-05  15.66050\n6      6 very.unsat 0.09212869 0.01921927  4.793559 1.638480e-06  19.21921\n7      7 very.unsat 0.13341018 0.02606206  5.118941 3.072553e-07  21.63406\n8      8 very.unsat 0.11667551 0.02538806  4.595684 4.313323e-06  17.82277\n9      1      unsat 0.18404109 0.02868504  6.415925 1.399702e-10  32.73416\n10     2      unsat 0.23872962 0.02921907  8.170336 3.075326e-16  51.53011\n11     3      unsat 0.28538804 0.03042338  9.380550 6.562918e-21  67.04615\n12     4      unsat 0.26929960 0.03193252  8.433397 3.357733e-17  54.72529\n13     5      unsat 0.14203024 0.02425803  5.854979 4.770707e-09  27.64315\n14     6      unsat 0.19320863 0.02710696  7.127640 1.021041e-12  39.83310\n15     7      unsat 0.24381101 0.02991897  8.149046 3.668079e-16  51.27582\n16     8      unsat 0.22529656 0.03117369  7.227138 4.932779e-13  40.88266\n17     1        sat 0.30117514 0.02422278 12.433551 1.718276e-35 115.48652\n18     2        sat 0.30914889 0.02313969 13.360111 1.034097e-40 132.82875\n19     3        sat 0.29200527 0.02470501 11.819678 3.088679e-32 104.67471\n20     4        sat 0.30087114 0.02432041 12.371138 3.744768e-35 114.36261\n21     5        sat 0.27528959 0.02602982 10.575930 3.853346e-26  84.42402\n22     6        sat 0.30447324 0.02352688 12.941505 2.624365e-38 124.84130\n23     7        sat 0.30845056 0.02324186 13.271336 3.394728e-40 131.11383\n24     8        sat 0.30979203 0.02308736 13.418254 4.727187e-41 133.95807\n25     1   very.sat 0.42887634 0.05211935  8.228735 1.892008e-16  52.23093\n26     2   very.sat 0.32353735 0.04275469  7.567295 3.810772e-14  44.57691\n27     3   very.sat 0.23969698 0.03781629  6.338458 2.320766e-10  32.00468\n28     4   very.sat 0.26870890 0.04424679  6.072958 1.255753e-09  29.56880\n29     5   very.sat 0.52197164 0.05117754 10.199232 1.998458e-24  78.72739\n30     6   very.sat 0.41018944 0.04662747  8.797163 1.403186e-18  59.30600\n31     7   very.sat 0.31432825 0.04318445  7.278737 3.369608e-13  41.43248\n32     8   very.sat 0.34823589 0.04969847  7.006973 2.435284e-12  38.57905\n     conf.low  conf.high frequency gender age.group      satis\n1  0.04797820 0.12383668    13.125 female      0-17 very.unsat\n2  0.07984331 0.17732496    13.125 female     18-24 very.unsat\n3  0.11795769 0.24786173    13.125 female     25-30 very.unsat\n4  0.09684218 0.22539854    13.125 female       31+ very.unsat\n5  0.03286080 0.08855625    13.125   male      0-17 very.unsat\n6  0.05445962 0.12979776    13.125   male     18-24 very.unsat\n7  0.08232947 0.18449088    13.125   male     25-30 very.unsat\n8  0.06691582 0.16643520    13.125   male       31+ very.unsat\n9  0.12781944 0.24026274    13.125 female      0-17 very.unsat\n10 0.18146129 0.29599795    13.125 female     18-24 very.unsat\n11 0.22575932 0.34501677    13.125 female     25-30 very.unsat\n12 0.20671301 0.33188618    13.125 female       31+ very.unsat\n13 0.09448538 0.18957511    13.125   male      0-17 very.unsat\n14 0.14007998 0.24633729    13.125   male     18-24 very.unsat\n15 0.18517092 0.30245110    13.125   male     25-30 very.unsat\n16 0.16419725 0.28639587    13.125   male       31+ very.unsat\n17 0.25369937 0.34865091    13.125 female      0-17 very.unsat\n18 0.26379592 0.35450186    13.125 female     18-24 very.unsat\n19 0.24358434 0.34042621    13.125 female     25-30 very.unsat\n20 0.25320401 0.34853826    13.125 female       31+ very.unsat\n21 0.22427207 0.32630710    13.125   male      0-17 very.unsat\n22 0.25836140 0.35058508    13.125   male     18-24 very.unsat\n23 0.26289735 0.35400377    13.125   male     25-30 very.unsat\n24 0.26454164 0.35504242    13.125   male       31+ very.unsat\n25 0.32672428 0.53102839    13.125 female      0-17 very.unsat\n26 0.23973969 0.40733501    13.125 female     18-24 very.unsat\n27 0.16557840 0.31381555    13.125 female     25-30 very.unsat\n28 0.18198679 0.35543102    13.125 female       31+ very.unsat\n29 0.42166550 0.62227778    13.125   male      0-17 very.unsat\n30 0.31880128 0.50157760    13.125   male     18-24 very.unsat\n31 0.22968828 0.39896822    13.125   male     25-30 very.unsat\n32 0.25082867 0.44564311    13.125   male       31+ very.unsat\n```\n\n\n:::\n:::\n\nThe predictions come out long, and we would like all the predictions for the same gender - age-group combination to come out in one row: That means pivoting the group column wider. I also took the opportunity to grab only the relevant columns:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(mobile.1, newdata = new)) %>% \n  select(gender, age.group, group, estimate) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nRe-fitting to get Hessian\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 6\n  gender age.group very.unsat unsat   sat very.sat\n  <chr>  <chr>          <dbl> <dbl> <dbl>    <dbl>\n1 female 0-17          0.0859 0.184 0.301    0.429\n2 female 18-24         0.129  0.239 0.309    0.324\n3 female 25-30         0.183  0.285 0.292    0.240\n4 female 31+           0.161  0.269 0.301    0.269\n5 male   0-17          0.0607 0.142 0.275    0.522\n6 male   18-24         0.0921 0.193 0.304    0.410\n7 male   25-30         0.133  0.244 0.308    0.314\n8 male   31+           0.117  0.225 0.310    0.348\n```\n\n\n:::\n:::\n\nDepending on the width of your display, you may or may not see all four probabilities.\n\nThis worked for me, but this might happen to you, with the same commands as above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(mobile.1, newdata = new)) %>% \n  MASS::select(gender, age.group, group, estimate) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in MASS::select(., gender, age.group, group, estimate): unused arguments (gender, age.group, group, estimate)\n```\n\n\n:::\n:::\n\n\nOh, this didn't work. Why not? There don't seem to be any errors.\n\nThis is the kind of thing that can bother you for *days*. The\nresolution (that it took me a long time to discover) is that you might\nhave the `tidyverse` *and also* `MASS` loaded, in\nthe wrong order, and `MASS` also has a `select` (that\ntakes different inputs and does something different). If you look back\nat part (<a href=\"#part:thefit\">here</a>), you might have seen a message there when you\nloaded `MASS` that `select` was \"masked\". When you\nhave two packages that both contain a function with the same name, the\none that you can see (and that will get used) is the one that was\nloaded *last*, which is the `MASS` select (not the one we\nactually wanted, which is the `tidyverse` select). There are a\ncouple of ways around this. One is to un-load the package we no longer\nneed (when we no longer need it). The mechanism for this is shown at\nthe end of part (<a href=\"#part:unload\">here</a>). The other is to say explicitly\nwhich package you want your function to come from, so that there is no\ndoubt. The `tidyverse` is actually a collection of\npackages. The best way to find out which one our `select` comes\nfrom is to go to the Console window in R Studio and ask for the help\nfor `select`. With both `tidyverse` and `MASS`\nloaded, the help window offers you a choice of both `select`s;\nthe one we want is \"select/rename variables by name\", and the actual\npackage it comes from is `dplyr`.\n\nThere is a third choice, which is the one I prefer now: install and load the package `conflicted`. When you run your code and it calls for something like `select` that is in two packages that you have loaded, it gives an error, like this:\n\n```\nError: [conflicted] `select` found in 2 packages.\nEither pick the one you want with `::` \n* MASS::select\n* dplyr::select\nOr declare a preference with `conflict_prefer()`\n* conflict_prefer(\"select\", \"MASS\")\n* conflict_prefer(\"select\", \"dplyr\")\n```\n\nFixing this costs you a bit of time upfront, but once you have fixed it, you know that the right thing is being run. What I do is to copy-paste one of those `conflict_prefer` lines, in this case the second one, and put it *before* the `select` that now causes the error. Right after the `library(conflicted)` is a good place. When you use `conflicted`, you will probably have to run several times to fix up all the conflicts, which will be a bit frustrating, and you will end up with several `conflict_prefer` lines, but once you have them there, you won't have to worry about the right function being called because you have explicitly said which one you want.\n\nThis is a non-standard use of `cbind` because I wanted to grab\nonly the gender and age group columns from `mobile` first, and\nthen `cbind` *that* to the predicted probabilities. The\nmissing first input to `cbind` is \n\"whatever came out of the previous step\", \nthat is, the first two columns of `mobile`.\n\nI only included the first two columns of `mobile` in the\n`cbind`, because the rest of the columns of `mobile`\nwere frequencies, which I don't need to see. (Having said that, it\nwould be interesting to make a *plot* using the observed\nproportions and predicted probabilities, but I didn't ask you for that.)\n\n\n$\\blacksquare$\n\n(j) <a name=\"part:unload\">*</a> \nDescribe any patterns you see in the predictions, bearing in mind the\nsignificance or not of the explanatory variables.\n\n \nSolution\n\n\nI had both explanatory variables being significant, so I would\nexpect to see both an age-group effect *and* a gender effect.\nFor both males and females, there seems to be a decrease in\nsatisfaction as the customers get older, at least until age 30 or\nso. I can see this because the predicted prob.\\ of \"very  satisfied\" \ndecreases, and the predicted prob.\nof \"very unsatisfied\" increases. The 31+ age group are very\nsimilar to the 25--30 group for both males and females. So that's\nthe age group effect.\nWhat about a gender effect? Well, for all the age groups, the males\nare more likely to be very satisfied than the females of the\ncorresponding age group, and also less likely to to be very\nunsatisfied. So the gender effect is that males are more satisfied\nthan females overall. (Or, the males are less discerning. Take your pick.)\nWhen we did the tests above, age group was very definitely\nsignificant, and gender less so (P-value around 0.03). This\nsuggests that the effect of age group ought to be large, and the\neffect of gender not so large. This is about what we observed: the\nage group effect was pretty clear, and the gender effect was\nnoticeable but small: the females were less satisfied than the\nmales, but there wasn't all that much difference.\n\n$\\blacksquare$\n\n\n \n\n\n\n\n\n##  Finding non-missing values\n\n\n <a name=\"part:prepare-next\">*</a> This is to prepare you for something in the next\nquestion. It's meant to be easy.\n\nIn R, the code `NA` stands for \"missing value\" or\n\"value not known\". In R, `NA` should not have quotes around\nit. (It is a special code, not a piece of text.)\n\n\n(a) Create a vector `v` that contains some numbers and some\nmissing values, using `c()`. Put those values into a\none-column data frame.\n \nSolution\n\n\nLike this. The arrangement of numbers and missing values doesn't\nmatter, as long as you have some of each:\n\n::: {.cell}\n\n```{.r .cell-code}\nv <- c(1, 2, NA, 4, 5, 6, 9, NA, 11)\nmydata <- tibble(v)\nmydata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 x 1\n      v\n  <dbl>\n1     1\n2     2\n3    NA\n4     4\n5     5\n6     6\n7     9\n8    NA\n9    11\n```\n\n\n:::\n:::\n\n     \n\nThis has one column called `v`.\n \n$\\blacksquare$\n\n(b) Obtain a new column containing `is.na(v)`. When is this true and when is this false?\n \nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata <- mydata %>% mutate(isna = is.na(v))\nmydata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 x 2\n      v isna \n  <dbl> <lgl>\n1     1 FALSE\n2     2 FALSE\n3    NA TRUE \n4     4 FALSE\n5     5 FALSE\n6     6 FALSE\n7     9 FALSE\n8    NA TRUE \n9    11 FALSE\n```\n\n\n:::\n:::\n\n     \n\nThis is `TRUE` if the corresponding element of `v` is\nmissing (in my case, the third value and the second-last one), and\n`FALSE` otherwise (when there is an actual value there).\n \n$\\blacksquare$\n\n(c) The symbol `!` means \"not\" in R (and other\nprogramming languages). What does `!is.na(v)` do? Create a\nnew column containing that.\n \nSolution\n\n\nTry it and see. Give it whatever name you like. My name reflects\nthat I know what it's going to do:\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata <- mydata %>% mutate(notisna = !is.na(v))\nmydata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 x 3\n      v isna  notisna\n  <dbl> <lgl> <lgl>  \n1     1 FALSE TRUE   \n2     2 FALSE TRUE   \n3    NA TRUE  FALSE  \n4     4 FALSE TRUE   \n5     5 FALSE TRUE   \n6     6 FALSE TRUE   \n7     9 FALSE TRUE   \n8    NA TRUE  FALSE  \n9    11 FALSE TRUE   \n```\n\n\n:::\n:::\n\n     \n\nThis is the logical opposite of `is.na`: it's true if there is\na value, and false if it's missing.\n \n$\\blacksquare$\n\n(d) Use `filter` to display just the\nrows of your data frame that have a non-missing value of `v`.\n\n \nSolution\n\n\n`filter` takes a column to say which rows to pick, in\nwhich case the column should contain something that either *is*\n`TRUE` or `FALSE`, or something that can be\ninterpreted that way:\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata %>% filter(notisna)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 x 3\n      v isna  notisna\n  <dbl> <lgl> <lgl>  \n1     1 FALSE TRUE   \n2     2 FALSE TRUE   \n3     4 FALSE TRUE   \n4     5 FALSE TRUE   \n5     6 FALSE TRUE   \n6     9 FALSE TRUE   \n7    11 FALSE TRUE   \n```\n\n\n:::\n:::\n\n   \n\nor you can provide `filter` something that can be calculated\nfrom what's in the data frame, and also returns something that is\neither true or false:\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata %>% filter(!is.na(v))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 x 3\n      v isna  notisna\n  <dbl> <lgl> <lgl>  \n1     1 FALSE TRUE   \n2     2 FALSE TRUE   \n3     4 FALSE TRUE   \n4     5 FALSE TRUE   \n5     6 FALSE TRUE   \n6     9 FALSE TRUE   \n7    11 FALSE TRUE   \n```\n\n\n:::\n:::\n\n \n\nIn either case, I only have non-missing values of `v`.\n \n$\\blacksquare$\n\n\n\n\n\n\n##  High School and Beyond\n\n\n A survey called High School and Beyond was given to a large\nnumber of American high school seniors (grade 12) by the National\nCenter of Education Statistics. The data set at\n[link](http://ritsokiguess.site/datafiles/hsb.csv) is a random\nsample of 200 of those students.\n\nThe variables collected are:\n\n\n\n* `gender`: student's gender, female or male.\n\n* `race`: the student's race (African-American,\nAsian,^[I'm always amused at how Americans put all Asians    into one group.]  Hispanic, White).\n\n* `ses`: Socio-economic status of student's family (low,\nmiddle, or high)\n\n* `schtyp`: School type, public or private.\n\n* `prog`: Student's program, general, academic, or\nvocational. \n\n* `read`: Score on standardized reading test.\n\n* `write`: Score on standardized writing test.\n\n* `math`: Score on standardized math test.\n\n* `science`: Score on standardized science test.\n\n* `socst`: Score on standardized social studies test.\n\n\n\nOur aim is to see how socio-economic status is related to the other\nvariables. \n\n\n\n(a) Read in and display (some of) the data.\n\nSolution\n\n\nThis is a `.csv` file (I tried to make it easy for you):\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/hsb.csv\"\nhsb <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 200 Columns: 11\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (5): race, ses, schtyp, prog, gender\ndbl (6): id, read, write, math, science, socst\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nhsb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 200 x 11\n      id race         ses    schtyp prog   read write  math science socst gender\n   <dbl> <chr>        <chr>  <chr>  <chr> <dbl> <dbl> <dbl>   <dbl> <dbl> <chr> \n 1    70 white        low    public gene~    57    52    41      47    57 male  \n 2   121 white        middle public voca~    68    59    53      63    61 female\n 3    86 white        high   public gene~    44    33    54      58    31 male  \n 4   141 white        high   public voca~    63    44    47      53    56 male  \n 5   172 white        middle public acad~    47    52    57      53    61 male  \n 6   113 white        middle public acad~    44    52    51      63    61 male  \n 7    50 african-amer middle public gene~    50    59    42      53    61 male  \n 8    11 hispanic     middle public acad~    34    46    45      39    36 male  \n 9    84 white        middle public gene~    63    57    54      58    51 male  \n10    48 african-amer middle public acad~    57    55    52      50    51 male  \n# i 190 more rows\n```\n\n\n:::\n:::\n\n       \n$\\blacksquare$\n\n(b) Explain briefly why an ordinal logistic regression is\nappropriate for our aims.\n\nSolution\n\n\nThe response variable `ses` is categorical, with\ncategories that come in order (low less than middle less than\nhigh). \n\n$\\blacksquare$\n\n(c) Fit an ordinal logistic regression predicting\nsocio-economic status from the scores on the five standardized\ntests. (You don't need to display the results.) You will probably\ngo wrong the first time. What kind of thing does your response\nvariable have to be? \n\nSolution\n\n\nIt has to be an `ordered` factor, which you can create in\nthe data frame (or outside, if you prefer):\n\n::: {.cell}\n\n```{.r .cell-code}\nhsb <- hsb %>% mutate(ses = ordered(ses, c(\"low\", \"middle\", \"high\")))\nhsb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 200 x 11\n      id race         ses    schtyp prog   read write  math science socst gender\n   <dbl> <chr>        <ord>  <chr>  <chr> <dbl> <dbl> <dbl>   <dbl> <dbl> <chr> \n 1    70 white        low    public gene~    57    52    41      47    57 male  \n 2   121 white        middle public voca~    68    59    53      63    61 female\n 3    86 white        high   public gene~    44    33    54      58    31 male  \n 4   141 white        high   public voca~    63    44    47      53    56 male  \n 5   172 white        middle public acad~    47    52    57      53    61 male  \n 6   113 white        middle public acad~    44    52    51      63    61 male  \n 7    50 african-amer middle public gene~    50    59    42      53    61 male  \n 8    11 hispanic     middle public acad~    34    46    45      39    36 male  \n 9    84 white        middle public gene~    63    57    54      58    51 male  \n10    48 african-amer middle public acad~    57    55    52      50    51 male  \n# i 190 more rows\n```\n\n\n:::\n:::\n\n  \n\n`ses` is now `ord`. Good. Now fit the model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nses.1 <- polr(ses ~ read + write + math + science + socst, data = hsb)\n```\n:::\n\n       \n\nNo errors is good.\n\n$\\blacksquare$\n\n(d) Remove any non-significant explanatory variables one at a\ntime. Use `drop1` to decide which one to remove next.\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(ses.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nses ~ read + write + math + science + socst\n        Df    AIC    LRT Pr(>Chi)   \n<none>     404.63                   \nread     1 403.09 0.4620 0.496684   \nwrite    1 403.81 1.1859 0.276167   \nmath     1 403.19 0.5618 0.453517   \nscience  1 404.89 2.2630 0.132499   \nsocst    1 410.08 7.4484 0.006349 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n       \n\nI would have expected the AIC column to come out in order, but it\ndoesn't. Never mind. Scan for the largest P-value, which belongs to\n`read`. (This also has the lowest AIC.) So, remove `read`:\n\n::: {.cell}\n\n```{.r .cell-code}\nses.2 <- update(ses.1, . ~ . - read)\ndrop1(ses.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nses ~ write + math + science + socst\n        Df    AIC    LRT Pr(>Chi)   \n<none>     403.09                   \nwrite    1 402.10 1.0124 0.314325   \nmath     1 402.04 0.9541 0.328689   \nscience  1 404.29 3.1968 0.073782 . \nsocst    1 410.58 9.4856 0.002071 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nNote how the P-value for `science` has come down a long way.\n\nA close call, but `math` goes next.  The `update`\ndoesn't take long to type:\n\n::: {.cell}\n\n```{.r .cell-code}\nses.3 <- update(ses.2, . ~ . - math)\ndrop1(ses.3, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nses ~ write + science + socst\n        Df    AIC     LRT  Pr(>Chi)    \n<none>     402.04                      \nwrite    1 400.60  0.5587 0.4547813    \nscience  1 405.41  5.3680 0.0205095 *  \nsocst    1 411.07 11.0235 0.0008997 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\n`science` has become significant now (probably because it was\nstrongly correlated with at least one of the variables we removed (at\nmy guess, `math`). That is, we didn't need both\n`science` and `math`, but we *do* need *one* of\nthem. \n\nI think we can guess what will happen now: `write` comes out,\nand the other two variables will stay, so that'll be where we stop:\n\n::: {.cell}\n\n```{.r .cell-code}\nses.4 <- update(ses.3, . ~ . - write)\ndrop1(ses.4, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nses ~ science + socst\n        Df    AIC     LRT  Pr(>Chi)    \n<none>     400.60                      \nscience  1 403.45  4.8511 0.0276291 *  \nsocst    1 409.74 11.1412 0.0008443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nIndeed so. We need just the science and social studies test scores to\npredict socio-economic status.\n\nUsing AIC to decide on which variable to remove next will give the\nsame answer here, but I would like to see the `test=` part in\nyour `drop1` to give P-values (expect to lose something, but\nnot too much, if that's not there).\n\nExtras: I talked about correlation among the explanatory variables\nearlier, which I can explore:\n\n::: {.cell}\n\n```{.r .cell-code}\nhsb %>% select(read:socst) %>% cor()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             read     write      math   science     socst\nread    1.0000000 0.5967765 0.6622801 0.6301579 0.6214843\nwrite   0.5967765 1.0000000 0.6174493 0.5704416 0.6047932\nmath    0.6622801 0.6174493 1.0000000 0.6307332 0.5444803\nscience 0.6301579 0.5704416 0.6307332 1.0000000 0.4651060\nsocst   0.6214843 0.6047932 0.5444803 0.4651060 1.0000000\n```\n\n\n:::\n:::\n\n \n\nThe first time I did this, I forgot that I had `MASS` loaded\n(for the `polr`), and so, to get the right `select`, I\nneeded to say which one I wanted.\n\nAnyway, the correlations are all moderately high. There's nothing that\nstands out as being much higher than the others. The lowest two are\nbetween social studies and math, and social studies and science. That\nwould be part of the reason that social studies needs to stay. The\nhighest correlation is between math and reading, which surprises me\n(they seem to be different skills).\n\nSo there was not as much insight there as I expected.\n\nThe other thing is that you can use `step` for the\nvariable-elimination task as well:\n\n::: {.cell}\n\n```{.r .cell-code}\nses.5 <- step(ses.1, direction = \"backward\", test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=404.63\nses ~ read + write + math + science + socst\n\n          Df    AIC    LRT Pr(>Chi)   \n- read     1 403.09 0.4620 0.496684   \n- math     1 403.19 0.5618 0.453517   \n- write    1 403.81 1.1859 0.276167   \n<none>       404.63                   \n- science  1 404.89 2.2630 0.132499   \n- socst    1 410.08 7.4484 0.006349 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=403.09\nses ~ write + math + science + socst\n\n          Df    AIC    LRT Pr(>Chi)   \n- math     1 402.04 0.9541 0.328689   \n- write    1 402.10 1.0124 0.314325   \n<none>       403.09                   \n- science  1 404.29 3.1968 0.073782 . \n- socst    1 410.58 9.4856 0.002071 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=402.04\nses ~ write + science + socst\n\n          Df    AIC     LRT  Pr(>Chi)    \n- write    1 400.60  0.5587 0.4547813    \n<none>       402.04                      \n- science  1 405.41  5.3680 0.0205095 *  \n- socst    1 411.07 11.0235 0.0008997 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=400.6\nses ~ science + socst\n\n          Df    AIC     LRT  Pr(>Chi)    \n<none>       400.60                      \n- science  1 403.45  4.8511 0.0276291 *  \n- socst    1 409.74 11.1412 0.0008443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nI would accept you doing it this way, again *as long as you have\nthe `test=` there as well*.\n\n$\\blacksquare$\n\n(e) The quartiles of the `science` test score are 44 \nand 58. The quartiles of the `socst` test score are 46 and 61. Make\na data frame that has all combinations of those quartiles. If your best\nregression had any other explanatory variables in it, also put the\n*means* of those variables into this data frame.\n\n\nSolution\n\n\nThis is what `datagrid` does by default (from package `marginaleffects`):\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = ses.5, science = c(44, 58), socst = c(46, 61))\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  science socst rowid\n1      44    46     1\n2      44    61     2\n3      58    46     3\n4      58    61     4\n```\n\n\n:::\n:::\n\n\nThis explicitly fills in mean values or most frequent categories for all the other variables in the dataset, even though those other variables are not in the model. The two variables you actually care about are over on the right. \n\nSince there are only two variables left, this `new` data frame has only\n$2^2=4$ rows.\n\nThere is a veiled hint here that these are the two variables that\nshould have remained in your regression. If that was not what you got,\nthe means of the other variables in the model will go automatically into your `new`:\n\n::: {.cell}\n\n```{.r .cell-code}\ndatagrid(model = ses.1, science = c(44, 58), socst = c(46, 61))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   read  write   math science socst rowid\n1 52.23 52.775 52.645      44    46     1\n2 52.23 52.775 52.645      44    61     2\n3 52.23 52.775 52.645      58    46     3\n4 52.23 52.775 52.645      58    61     4\n```\n\n\n:::\n:::\n\nso you don't have to do anything extra.\n\n$\\blacksquare$\n\n(f) Use the data frame you created in the previous part, together\nwith your best model, to obtain predicted probabilities of being in\neach `ses` category. Display these predicted probabilities so that they are easy to read.\n\n\nSolution\n\n\nThis is `predictions`, and we've done the setup. My best model\nwas called `ses.4`. \n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(ses.4, newdata = new)) %>% \n  select(group, estimate, science, socst) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nRe-fitting to get Hessian\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    group  estimate science socst\n1     low 0.3262510      44    46\n2     low 0.1885566      44    61\n3     low 0.2278105      58    46\n4     low 0.1240155      58    61\n5  middle 0.5056113      44    46\n6  middle 0.5150763      44    61\n7  middle 0.5230783      58    46\n8  middle 0.4672338      58    61\n9    high 0.1681377      44    46\n10   high 0.2963671      44    61\n11   high 0.2491112      58    46\n12   high 0.4087507      58    61\n```\n\n\n:::\n:::\n\n`predictions` always works by having *one* column of predictions. That isn't the best layout here, though; we want to see the three predicted probabilities for a particular value of `science` and `socst` all in one row, which means pivoting-wider:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(ses.4, newdata = new)) %>% \n  select(group, estimate, science, socst) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nRe-fitting to get Hessian\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 5\n  science socst   low middle  high\n    <dbl> <dbl> <dbl>  <dbl> <dbl>\n1      44    46 0.326  0.506 0.168\n2      44    61 0.189  0.515 0.296\n3      58    46 0.228  0.523 0.249\n4      58    61 0.124  0.467 0.409\n```\n\n\n:::\n:::\n\n\n\nThe easiest strategy seems to be to run `predictions` first, see that it comes out long, and then wonder how to fix it. Then pick the columns you care about: the predicted `group`, the predictions, and the columns for science and social science, and then pivot wider. \n   \n\n$\\blacksquare$\n\n(g) What is the effect of an increased science score on the\nlikelihood of a student being in the different socioeconomic groups,\nall else equal?  Explain briefly. In your explanation, state clearly\nhow you are using your answer to the previous part.\n\n\nSolution\n\n\nUse your predictions; hold the `socst` score constant (that's\nthe all else equal part). So compare the first and third rows (or,\nif you like, the second and fourth rows) of your predictions and see\nwhat happens as the science score goes from 44 to 58.\nWhat I see is that the probability of being `low` goes\nnoticeably *down* as the science score increases, the\nprobability of `middle` stays about the same, and the\nprobability of `high` goes `up` (by about the same\namount as the probability of `low` went down). \nIn other words, an increased science score goes with an increased\nchance of `high` (and a decreased chance of `low`).\n\nIf your best model doesn't have `science` in it, then you\nneed to say something like \"`science` has no effect on   socio-economic status\", \nconsistent with what you concluded before: if\nyou took it out, it's because you thought it had no effect.\n\nExtra: the effect of an increased social studies score is almost\nexactly the same as an increased science score (so I didn't ask you\nabout that). From a social-science  point of view, this makes\nperfect sense: the higher the social-economic stratum a student\ncomes from, the better they are likely to do in school. \nI've been phrasing this as \"association\", because really the cause\nand effect is the other way around: a student's family socioeconomic\nstatus is explanatory, and school performance is response. But this\nwas the nicest example I could find of an ordinal response data set.\n\n$\\blacksquare$\n\n\n\n\n\n\n##  How do you like your steak?\n\n\n When you order a steak in a restaurant, the server will ask\nyou how you would like it cooked, or to be precise, *how much*\nyou would like it cooked: rare (hardly cooked at all), through medium\nrare, medium, medium well to well (which means \"well done\", so that\nthe meat has only a little red to it). Could you guess how a person\nlikes their steak cooked, from some other information about them? The\nwebsite [link](fivethirtyeight.com) commissioned a survey where they\nasked a number of people how they preferred their steak, along with as\nmany other things as they could think of to ask. (Many of the\nvariables below are related to risk-taking, which was something the\npeople designing the survey thought might have something to do with\nliking steak rare.) The variables of interest are all factors or true/false:\n\n\n\n* `respondent_ID`: a ten-digit number identifying each\nperson who responded to the survey.\n\n* `lottery_a`: true if the respondent preferred lottery A\nwith a small chance to win a lot of money, to lottery B, with a\nlarger chance to win less money.\n\n* `smoke`: true if the respondent is currently a smoker\n\n* `alcohol`: true if the respondent at least occasionally\ndrinks alcohol.\n\n* `gamble`: true if the respondent likes to gamble (eg.\nbetting on horse racing or playing the lottery)\n\n* `skydiving`: true if the respondent has ever been\nskydiving.\n\n* `speed`: true if the respondent likes to drive fast\n\n* `cheated`: true if the respondent has ever cheated on a\nspouse or girlfriend/boyfriend\n\n* `steak`: true if the respondent likes to eat steak\n\n* `steak_prep` (response): how the respondent likes their\nsteak cooked (factor, as described above, with 5 levels).\n\n* `female`: true if the respondent is female\n\n* `age`: age group, from 18--29 to 60+.\n\n* `hhold_income`: household income group, from \\$0--24,999\nto \\$150,000+.\n\n* `educ`: highest level of education attained, from \n\"less than high school\" up to \"graduate degree\"\n\n* `region`: region (of the US)\nthat the respondent lives in (five values).\n\n\nThe data are in\n[link](http://ritsokiguess.site/datafiles/steak1.csv). This is the\ncleaned-up data from a previous question, with the missing values removed.\n\n\n\n(a) Read in the data and display the first few lines.\n\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak <- read_csv(\"steak1.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 331 Columns: 15\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (5): steak_prep, age, hhold_income, educ, region\ndbl (1): respondent_id\nlgl (9): lottery_a, smoke, alcohol, gamble, skydiving, speed, cheated, steak...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nsteak\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 331 x 15\n   respondent_id lottery_a smoke alcohol gamble skydiving speed cheated steak\n           <dbl> <lgl>     <lgl> <lgl>   <lgl>  <lgl>     <lgl> <lgl>   <lgl>\n 1    3234982343 TRUE      FALSE TRUE    FALSE  FALSE     FALSE FALSE   TRUE \n 2    3234973379 TRUE      FALSE TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 3    3234972383 FALSE     TRUE  TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 4    3234958833 FALSE     FALSE TRUE    FALSE  FALSE     TRUE  TRUE    TRUE \n 5    3234955240 TRUE      FALSE FALSE   FALSE  FALSE     TRUE  FALSE   TRUE \n 6    3234955010 TRUE      FALSE TRUE    TRUE   TRUE      TRUE  FALSE   TRUE \n 7    3234953052 TRUE      TRUE  TRUE    TRUE   FALSE     TRUE  FALSE   TRUE \n 8    3234951249 FALSE     FALSE TRUE    TRUE   FALSE     FALSE FALSE   TRUE \n 9    3234948883 FALSE     FALSE TRUE    FALSE  FALSE     TRUE  FALSE   TRUE \n10    3234948197 TRUE      FALSE FALSE   TRUE   FALSE     TRUE  FALSE   TRUE \n# i 321 more rows\n# i 6 more variables: steak_prep <chr>, female <lgl>, age <chr>,\n#   hhold_income <chr>, educ <chr>, region <chr>\n```\n\n\n:::\n:::\n\n\n \n$\\blacksquare$\n\n\n(b) We are going to predict `steak_prep` from some of\nthe other variables. Why is the model-fitting function `polr`\nfrom package `MASS` the best choice for these data\n(alternatives being `glm` and `multinom` from package\n`nnet`)?\n\n\nSolution\n\n\nIt all depends on the kind of response variable. We have a\nresponse variable with five ordered levels from Rare to\nWell. There are more than two levels (it is more than a\n\"success\" and \"failure\"), which rules out `glm`, and\nthe levels are ordered, which rules out `multinom`. As we\nknow, `polr` handles an ordered response, so it is the\nright choice.\n    \n$\\blacksquare$\n\n(c) What are the levels of `steak_prep`, \n*in the  order that R thinks they are in?* If they are not in a sensible\norder, create an ordered factor where the levels are in a sensible order.\n\n\nSolution\n\n\nThis is the most direct way to find out:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak %>% distinct(steak_prep) %>% pull(steak_prep) -> preps\npreps\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Medium rare\" \"Rare\"        \"Medium\"      \"Medium Well\" \"Well\"       \n```\n\n\n:::\n:::\n\n     \n\nThis is almost the right order (`distinct` uses the order in\nthe data frame). We just need to switch the first two around, and then\nwe'll be done:\n\n::: {.cell}\n\n```{.r .cell-code}\npreps1 <- preps[c(2, 1, 3, 4, 5)]\npreps1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Rare\"        \"Medium rare\" \"Medium\"      \"Medium Well\" \"Well\"       \n```\n\n\n:::\n:::\n\n \n\nIf you used `count`, there's a bit more work to do:\n\n::: {.cell}\n\n```{.r .cell-code}\npreps2 <- steak %>% count(steak_prep) %>% pull(steak_prep)\npreps2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Medium\"      \"Medium Well\" \"Medium rare\" \"Rare\"        \"Well\"       \n```\n\n\n:::\n:::\n\n \n\nbecause `count` puts them in alphabetical order, so:\n\n::: {.cell}\n\n```{.r .cell-code}\npreps3 <- preps2[c(4, 2, 1, 3, 5)]\npreps3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Rare\"        \"Medium Well\" \"Medium\"      \"Medium rare\" \"Well\"       \n```\n\n\n:::\n:::\n\n \n\nThese use the idea in the\nattitudes-to-abortion question: create a vector of the levels in the\n*right* order, then create an ordered factor with\n`ordered()`. If you like, you can type the levels in the right\norder (I won't penalize you for that here), but it's really better to\nget the levels without typing or copying-pasting, so that you don't\nmake any silly errors copying them (which will mess everything up\nlater).\n\nSo now I create my ordered response:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak <- steak %>% mutate(steak_prep_ord = ordered(steak_prep, preps1))\n```\n:::\n\n \nor using one of the other `preps` vectors containing the levels\nin the correct order.\nAs far as `polr` is concerned,\nit doesn't matter whether I start at `Rare` and go \"up\", or\nstart at `Well` and go \"down\". So if you do it the other way\naround, that's fine. As long as you get the levels in a sensible\norder, you're good.\n    \n$\\blacksquare$\n\n(d) Fit a model predicting preferred steak preparation in an\nordinal logistic regression from `educ`, `female` and\n`lottery_a`. This ought to be easy from your previous work,\nbut you have to be careful about one thing. No need to print out the\nresults. \n\n\nSolution\n\n\nThe thing you have to be careful about is that you use the\n*ordered* factor that you just created as the response:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.1 <- polr(steak_prep_ord ~ educ + female + lottery_a, data = steak)\n```\n:::\n\n     \n    \n$\\blacksquare$\n\n(e) Run `drop1` on your fitted model, with\n`test=\"Chisq\"`. Which explanatory variable should be removed\nfirst, if any? Bear in mind that the variable with the\n*smallest* AIC should come out first, in case your table\ndoesn't get printed in order.\n\n\nSolution\n\n\nThis:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(steak.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nsteak_prep_ord ~ educ + female + lottery_a\n          Df    AIC    LRT Pr(>Chi)  \n<none>       910.69                  \neduc       4 912.10 9.4107  0.05162 .\nfemale     1 908.70 0.0108  0.91715  \nlottery_a  1 909.93 1.2425  0.26498  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nMy table is indeed out of order (which is why I warned you about it,\nin case that happens to you as well). The smallest AIC goes with\n`female`, which also has a very non-significant P-value, so\nthis one should come out first.\n    \n$\\blacksquare$\n\n(f) Remove the variable that should come out first, using\n`update`. (If all the variables should stay, you can skip\nthis part.)\n\n\nSolution\n\n\nYou could type or copy-paste the whole model again, but\n`update` is quicker:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.2 <- update(steak.1, . ~ . - female)\n```\n:::\n\n     \n\nThat's all.\n\nI wanted to get some support for my `drop1` above (since I was\na bit worried about those out-of-order rows). Now that we have fitted\na model with `female` and one without, we can compare them\nusing `anova`:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(steak.2, steak.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio tests of ordinal regression models\n\nResponse: steak_prep_ord\n                      Model Resid. df Resid. Dev   Test    Df  LR stat.\n1          educ + lottery_a       322   890.7028                       \n2 educ + female + lottery_a       321   890.6920 1 vs 2     1 0.0108221\n    Pr(Chi)\n1          \n2 0.9171461\n```\n\n\n:::\n:::\n\n \n\nDon't get taken in by that \"LR stat\" that may be on the end of the first row of\nthe output table; the P-value might have wrapped onto the second line, and is in\nfact exactly the same as in the `drop1` output (it is doing\nexactly the same test). As non-significant as you could wish for.\n\nExtra: I was curious about whether either of the other $x$'s could come out now:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(steak.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nsteak_prep_ord ~ educ + lottery_a\n          Df    AIC    LRT Pr(>Chi)  \n<none>       908.70                  \neduc       4 910.13 9.4299  0.05121 .\nlottery_a  1 907.96 1.2599  0.26167  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\n`lottery_a` should come out, but `educ` is edging\ntowards significance. We are about to do predictions; in those, the above suggests that there may be some visible effect of education, but there may not be much effect of `lottery_a`.\n\nAll right, so what happens when we remove `lottery_a`? That we find out later.\n\n\n$\\blacksquare$\n\n(g) Using the best model that you have so far, predict the\nprobabilities of preferring each different steak preparation (method\nof cooking) for each combination of the variables that\nremain. (Some of the variables are TRUE and FALSE rather than\nfactors. Bear this in mind.)\nDescribe the effects of each variable on the predicted\nprobabilities, if any. Note that there is exactly one person in the\nstudy whose educational level is \"less than high school\".\n\n\nSolution\n\n\nAgain, I'm leaving it to you to follow all the steps. My variables\nremaining are `educ` and `lottery_a`, which are respectively categorical and logical.\n\nThe first step is to get all combinations of their values, along with \"typical\" values for the others:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = steak.2, \n                educ = levels(factor(steak$educ)),\n                lottery_a = c(FALSE, TRUE))\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                               educ lottery_a rowid\n1                   Bachelor degree     FALSE     1\n2                   Bachelor degree      TRUE     2\n3                   Graduate degree     FALSE     3\n4                   Graduate degree      TRUE     4\n5                High school degree     FALSE     5\n6                High school degree      TRUE     6\n7      Less than high school degree     FALSE     7\n8      Less than high school degree      TRUE     8\n9  Some college or Associate degree     FALSE     9\n10 Some college or Associate degree      TRUE    10\n```\n\n\n:::\n:::\n\nI wasn't sure how to handle the logical `lottery_a`, so I just typed the `TRUE` and `FALSE`.\n\nOn to the predictions, remembering to make them wider:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(steak.2, newdata = new)) %>% \n  select(rowid, group, estimate, educ, lottery_a) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nRe-fitting to get Hessian\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 8\n   rowid educ       lottery_a    Rare `Medium rare`  Medium `Medium Well`   Well\n   <dbl> <chr>      <lgl>       <dbl>         <dbl>   <dbl>         <dbl>  <dbl>\n 1     1 Bachelor ~ FALSE     5.19e-2   0.383       3.35e-1     0.172     0.0581\n 2     2 Bachelor ~ TRUE      4.18e-2   0.339       3.47e-1     0.201     0.0719\n 3     3 Graduate ~ FALSE     7.95e-2   0.469       2.92e-1     0.122     0.0376\n 4     4 Graduate ~ TRUE      6.44e-2   0.428       3.16e-1     0.145     0.0468\n 5     5 High scho~ FALSE     5.28e-2   0.387       3.33e-1     0.170     0.0571\n 6     6 High scho~ TRUE      4.25e-2   0.342       3.46e-1     0.198     0.0707\n 7     7 Less than~ FALSE     8.51e-8   0.00000111  4.00e-6     0.0000200 1.00  \n 8     8 Less than~ TRUE      6.78e-8   0.000000886 3.19e-6     0.0000159 1.00  \n 9     9 Some coll~ FALSE     5.50e-2   0.395       3.30e-1     0.165     0.0549\n10    10 Some coll~ TRUE      4.43e-2   0.351       3.44e-1     0.193     0.0679\n```\n\n\n:::\n:::\n\nThere are 5 levels of education, 2 levels of `lottery_a`, and 5 ways in which you might ask for your steak to be cooked, so the original output from `predictions` has $5 \\times 2 \\times 5 = 50$ rows, and the output you see above has $5 \\times 2 = 10$ rows.\n\n\nI find this hard to read, so I'm going to round off those\npredictions. Three or four decimals seems to be sensible. The time to do this is while they are all in one column, that is, before the `pivot_wider`. On my screen, the education levels also came out rather long, so I'm going to shorten them as well:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(steak.2, newdata = new)) %>% \n  select(rowid, group, estimate, educ, lottery_a) %>% \n  mutate(estimate = round(estimate, 3),\n         educ = abbreviate(educ, 15)) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nRe-fitting to get Hessian\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 8\n   rowid educ           lottery_a  Rare `Medium rare` Medium `Medium Well`  Well\n   <dbl> <chr>          <lgl>     <dbl>         <dbl>  <dbl>         <dbl> <dbl>\n 1     1 Bachelor degr~ FALSE     0.052         0.383  0.335         0.172 0.058\n 2     2 Bachelor degr~ TRUE      0.042         0.339  0.347         0.201 0.072\n 3     3 Graduate degr~ FALSE     0.08          0.469  0.292         0.122 0.038\n 4     4 Graduate degr~ TRUE      0.064         0.428  0.316         0.145 0.047\n 5     5 Highschooldeg~ FALSE     0.053         0.387  0.333         0.17  0.057\n 6     6 Highschooldeg~ TRUE      0.043         0.342  0.346         0.198 0.071\n 7     7 Lssthnhghschl~ FALSE     0             0      0             0     1    \n 8     8 Lssthnhghschl~ TRUE      0             0      0             0     1    \n 9     9 SmcllgorAssct~ FALSE     0.055         0.395  0.33          0.165 0.055\n10    10 SmcllgorAssct~ TRUE      0.044         0.351  0.344         0.193 0.068\n```\n\n\n:::\n:::\n\nThat's about as much as I can shorten the education levels while still having them readable.\n\n\nThen, say something about the effect of changing educational level on the\npredictions, and say something about the effect of favouring Lottery A\nvs.\\ not. I don't much mind what: you can say that there is not much\neffect (of either variable), or you can say something like \"people with a graduate degree are slightly more likely to like their steak rare and less likely to like it well done\" (for education level) and\n\"people who preferred Lottery A are slightly less likely to like their steak rare and slightly more likely to like it well done\" (for\neffect of Lottery A). You can see these by comparing the odd-numbered rows\nrows with each other to assess the effect of education while holding attitudes towards `lottery_a` constant (or the even-numbered rows, if you\nprefer), and you can compare eg. rows 1 and 2 to assess the effect of\nLottery A (compare two lines with the *same* educational level\nbut *different* preferences re Lottery A). \n\nI would keep away from saying anything about education level \n\"less than high school\", since this entire level is represented by exactly\none person.\n    \n$\\blacksquare$\n\n(h) Is it reasonable to remove *all* the remaining\nexplanatory variables from your best model so far? Fit a model with no explanatory variables,\nand do a test. (In R, if the right side of the squiggle is a\n`1`, that means \"just an intercept\". Or you can remove\nwhatever remains using `update`.) What do you conclude?\nExplain briefly.\n\n\nSolution\n\n\nThe fitting part is the challenge, since the testing part is\n`anova` again. The direct fit is this:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.3 <- polr(steak_prep_ord ~ 1, data = steak)\n```\n:::\n\n     \n\nand the `update` version is this, about equally long, starting\nfrom `steak.2` since that is the best model so far:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.3a <- update(steak.2, . ~ . - educ - lottery_a)\n```\n:::\n\n \n\nYou can use whichever you like. Either way, the second part is\n`anova`, and the two possible answers should be the same:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(steak.3, steak.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio tests of ordinal regression models\n\nResponse: steak_prep_ord\n             Model Resid. df Resid. Dev   Test    Df LR stat.    Pr(Chi)\n1                1       327   901.4467                                 \n2 educ + lottery_a       322   890.7028 1 vs 2     5 10.74387 0.05670146\n```\n\n\n:::\n:::\n\n \n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(steak.3a, steak.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio tests of ordinal regression models\n\nResponse: steak_prep_ord\n             Model Resid. df Resid. Dev   Test    Df LR stat.    Pr(Chi)\n1                1       327   901.4467                                 \n2 educ + lottery_a       322   890.7028 1 vs 2     5 10.74387 0.05670146\n```\n\n\n:::\n:::\n\n \n\nAt the 0.05 level, removing both of the remaining variables is fine:\nthat is, nothing (out of these variables) has any impact on the\nprobability that a diner will prefer their steak cooked a particular\nway. However, it is a very close call; the P-value is only *just* bigger than 0.05.\n\nHowever, with data like this and a rather exploratory analysis, I\nmight think about using a larger $\\alpha$ like 0.10, and at this\nlevel, taking out both these two variables is a bad idea. You could\nsay that one or both of them is \"potentially useful\" or\n\"provocative\" or something like that.\n\nIf you think that removing these two variables is questionable, you\nmight like to go back to that `drop1` output I had above:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(steak.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nsteak_prep_ord ~ educ + lottery_a\n          Df    AIC    LRT Pr(>Chi)  \n<none>       908.70                  \neduc       4 910.13 9.4299  0.05121 .\nlottery_a  1 907.96 1.2599  0.26167  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nThe smallest AIC goes with `lottery_a`, so that comes out (it\nis nowhere near significant):\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.4 <- update(steak.2, . ~ . - lottery_a)\ndrop1(steak.4, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nsteak_prep_ord ~ educ\n       Df    AIC   LRT Pr(>Chi)  \n<none>    907.96                 \neduc    4 909.45 9.484  0.05008 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nand what you see is that educational level is right on the edge of\nsignificance, so that may or may not have any impact. Make a call. But\nif anything, it's educational level that makes a difference.\n    \n$\\blacksquare$\n\n(i) In the article for which these data were collected,\n[link](https://fivethirtyeight.com/datalab/how-americans-like-their-steak/),\ndoes the author obtain consistent conclusions with yours? Explain\nbriefly. (It's not a very long article, so it won't take you long to\nskim through, and the author's point is pretty clear.)\n\n\n\nSolution\n\n\nThe article says that *nothing* has anything to do with steak\npreference. Whether you agree or not depends on what you thought\nabove about dropping those last two variables. So say something\nconsistent with what you said in the previous part. Two points for\nsaying that the author said \"nothing has any effect\", and one\npoint for how your findings square with that.\n\n\nExtra: now that you have worked through this great long question, this is\nwhere I tell you that I simplified things a fair bit for you! There\nwere lots of other variables that might have had an impact on how\npeople like their steaks, and we didn't even consider those. Why did\nI choose what I did here? Well, I wanted to fit a regression predicting\nsteak preference from everything else, do a big backward\nelimination, but:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.5 <- polr(steak_prep_ord ~ ., data = steak)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: algorithm did not converge\n```\n\n\n:::\n\n::: {.cell-output .cell-output-error}\n\n```\nError in polr(steak_prep_ord ~ ., data = steak): attempt to find suitable starting values failed\n```\n\n\n:::\n:::\n\n   \nThe `.` in place of explanatory\nvariables means \"all the other variables\", including the nonsensical\npersonal ID. That saved me having to type them all out. \n\nUnfortunately, however, it didn't work. The problem is a numerical\none. Regular regression has a well-defined procedure, where the computer follows\nthrough the steps and gets to the answer, every time. Once you go\nbeyond regression, however, the answer is obtained by a step-by-step\nmethod: the computer makes an initial guess, tries to improve it, then\ntries to improve it again, until it can't improve things any more, at\nwhich point it calls it good. The problem here is that `polr`\ncannot even get the initial guess! (It apparently is known to suck at\nthis, in problems as big and complicated as this one.)\n\nI don't normally recommend forward selection, but I wonder whether it\nworks here:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.5 <- polr(steak_prep_ord ~ 1, data = steak)\nsteak.6 <- step(steak.5,\n  scope = . ~ lottery_a + smoke + alcohol + gamble + skydiving +\n    speed + cheated + female + age + hhold_income + educ + region,\n  direction = \"forward\", test = \"Chisq\", trace = 0\n)\ndrop1(steak.6, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nsteak_prep_ord ~ educ\n       Df    AIC   LRT Pr(>Chi)  \n<none>    907.96                 \neduc    4 909.45 9.484  0.05008 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \n\nIt does, and it says the *only* thing to add out of all the\nvariables is education level. So, for you, I picked this along with a\ncouple of other plausible-sounding variables and had you start from there.\n\nForward selection starts from a model containing nothing and asks\n\"what can we add?\". This is a bit more complicated than backward\nelimination, because now you have to say what the candidate things to\nadd *are*. That's the purpose of that `scope` piece, and\nthere I had no alternative but to type the names of all the\nvariables. Backward elimination is easier, because the candidate\nvariables to remove are the ones in the model, and you don't need a\n`scope`. The `trace=0` says \"don't give me any output\"\n(you can change it to a different value if you want to see what that\ndoes), and last, the `drop1` looks at what is actually\n*in* the final model (with a view to asking what can be removed,\nbut we don't care about *that* here). \n  \n\n$\\blacksquare$\n\n\n\n##  How do you like your steak -- the data\n\n\nThis question takes you through the data preparation for one\nof the other questions. You don't have to do *this*\nquestion, but you may find it interesting or useful.\n\nWhen you order a steak in a restaurant, the server will ask\nyou how you would like it cooked, or to be precise, *how much*\nyou would like it cooked: rare (hardly cooked at all), through medium\nrare, medium, medium well to well (which means \"well done\", so that\nthe meat has only a little red to it). Could you guess how a person\nlikes their steak cooked, from some other information about them? The\nwebsite [link](fivethirtyeight.com) commissioned a survey where they\nasked a number of people how they preferred their steak, along with as\nmany other things as they could think of to ask. (Many of the\nvariables below are related to risk-taking, which was something the\npeople designing the survey thought might have something to do with\nliking steak rare.) The variables of interest are all factors or true/false:\n\n\n\n* `respondent_ID`: a ten-digit number identifying each\nperson who responded to the survey.\n\n* `lottery_a`: true if the respondent preferred lottery A\nwith a small chance to win a lot of money, to lottery B, with a\nlarger chance to win less money.\n\n* `smoke`: true if the respondent is currently a smoker\n\n* `alcohol`: true if the respondent at least occasionally\ndrinks alcohol.\n\n* `gamble`: true if the respondent likes to gamble (eg.\nbetting on horse racing or playing the lottery)\n\n* `skydiving`: true if the respondent has ever been\nskydiving.\n\n* `speed`: true if the respondent likes to drive fast\n\n* `cheated`: true if the respondent has ever cheated on a\nspouse or girlfriend/boyfriend\n\n* `steak`: true if the respondent likes to eat steak\n\n* `steak_prep` (response): how the respondent likes their\nsteak cooked (factor, as described above, with 5 levels).\n\n* `female`: true if the respondent is female\n\n* `age`: age group, from 18--29 to 60+.\n\n* `hhold_income`: household income group, from \\$0--24,999\nto \\$150,000+.\n\n* `educ`: highest level of education attained, from \n\"less  than high school\" \nup to \"graduate degree\"\n\n* `region`: region (of the US)\nthat the respondent lives in (five values).\n\n\nThe data are in\n[link](https://ritsokiguess.site/datafiles/steak.csv). \n\n\n\n(a) Read in the data and display the first few lines.\n\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"https://ritsokiguess.site/datafiles/steak.csv\"\nsteak0 <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 550 Columns: 15\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (5): steak_prep, age, hhold_income, educ, region\ndbl (1): respondent_id\nlgl (9): lottery_a, smoke, alcohol, gamble, skydiving, speed, cheated, steak...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nsteak0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 550 x 15\n   respondent_id lottery_a smoke alcohol gamble skydiving speed cheated steak\n           <dbl> <lgl>     <lgl> <lgl>   <lgl>  <lgl>     <lgl> <lgl>   <lgl>\n 1    3237565956 FALSE     NA    NA      NA     NA        NA    NA      NA   \n 2    3234982343 TRUE      FALSE TRUE    FALSE  FALSE     FALSE FALSE   TRUE \n 3    3234973379 TRUE      FALSE TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 4    3234972383 FALSE     TRUE  TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 5    3234958833 FALSE     FALSE TRUE    FALSE  FALSE     TRUE  TRUE    TRUE \n 6    3234955240 TRUE      FALSE FALSE   FALSE  FALSE     TRUE  FALSE   TRUE \n 7    3234955097 TRUE      FALSE TRUE    FALSE  FALSE     TRUE  TRUE    FALSE\n 8    3234955010 TRUE      FALSE TRUE    TRUE   TRUE      TRUE  FALSE   TRUE \n 9    3234953052 TRUE      TRUE  TRUE    TRUE   FALSE     TRUE  FALSE   TRUE \n10    3234951249 FALSE     FALSE TRUE    TRUE   FALSE     FALSE FALSE   TRUE \n# i 540 more rows\n# i 6 more variables: steak_prep <chr>, female <lgl>, age <chr>,\n#   hhold_income <chr>, educ <chr>, region <chr>\n```\n\n\n:::\n:::\n\n \n\nI'm using a temporary name for reasons that will become clear shortly.\n\n\n$\\blacksquare$\n\n(b) What do you immediately notice about your data frame? Run `summary` on the entire data frame. Would you say you have a lot of missing values, or only a few?\n\nSolution\n\n\nI see missing values, starting in the very first row.\nRunning the data frame through `summary` gives this, either as `summary(steak0)` or this way:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:279       FALSE:453       FALSE:125      \n Median :3.235e+09   TRUE :267       TRUE :84        TRUE :416      \n Mean   :3.235e+09   NA's :4         NA's :13        NA's :9        \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.238e+09                                                  \n   gamble        skydiving         speed          cheated       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:280       FALSE:502       FALSE:59        FALSE:447      \n TRUE :257       TRUE :36        TRUE :480       TRUE :92       \n NA's :13        NA's :12        NA's :11        NA's :11       \n                                                                \n                                                                \n   steak          steak_prep          female            age           \n Mode :logical   Length:550         Mode :logical   Length:550        \n FALSE:109       Class :character   FALSE:246       Class :character  \n TRUE :430       Mode  :character   TRUE :268       Mode  :character  \n NA's :11                           NA's :36                          \n                                                                      \n                                                                      \n hhold_income           educ              region         \n Length:550         Length:550         Length:550        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n```\n\n\n:::\n:::\n\n     \n\nMake a call about whether you think that's a lot of missing values or only a few. This might not be all of them, because missing text doesn't show here (we see later how to make it show up).\n\n$\\blacksquare$\n\n(c) What does the function `drop_na` do when applied to a data frame with missing values? To find out, pass the data frame into `drop_na()`, then into `summary` again. What has happened?\n\nSolution\n\n\nLet's try it and see.\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% drop_na() %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:171       FALSE:274       FALSE:65       \n Median :3.235e+09   TRUE :160       TRUE :57        TRUE :266      \n Mean   :3.235e+09                                                  \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.235e+09                                                  \n   gamble        skydiving         speed          cheated         steak        \n Mode :logical   Mode :logical   Mode :logical   Mode :logical   Mode:logical  \n FALSE:158       FALSE:308       FALSE:28        FALSE:274       TRUE:331      \n TRUE :173       TRUE :23        TRUE :303       TRUE :57                      \n                                                                               \n                                                                               \n                                                                               \n  steak_prep          female            age            hhold_income      \n Length:331         Mode :logical   Length:331         Length:331        \n Class :character   FALSE:174       Class :character   Class :character  \n Mode  :character   TRUE :157       Mode  :character   Mode  :character  \n                                                                         \n                                                                         \n                                                                         \n     educ              region         \n Length:331         Length:331        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n```\n\n\n:::\n:::\n\n \n\nThe missing values, the ones we can see anyway, have all gone. Precisely, `drop_na`, as its\nname suggests, drops all the rows that have missing values in them\nanywhere. This is potentially wasteful, since a row might be missing\nonly one value, and we drop the entire rest of the row, throwing away\nthe good data as well. If you check, we started with 550 rows, and we\nnow have only 311 left. Ouch.\n\nSo now we'll save this into our \"good\" data frame, which means doing it again (now that we know it works):\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% drop_na() -> steak\n```\n:::\n\n \n\nExtra: another way to handle missing data is called \"imputation\":\nwhat you do is to *estimate* a value for any missing data, and\nthen use that later on as if it were the truth. One way of estimating\nmissing values is to do a regression (of appropriate kind: regular or\nlogistic) to predict a column with missing values from all the other\ncolumns.\n\nExtra extra: below we see how we used to have to do this, for your information.\n\nFirst, we run `complete.cases` on the data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\ncomplete.cases(steak0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [37]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n [61] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n [73] FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [85] FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n [97]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n[109] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n[133] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[145]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[157]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n[169]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n[181] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n[205]  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[217] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[253] FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n[265] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n[277]  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[289]  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n[301] FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[313]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[325] FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n[337]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[349] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n[361] FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n[373] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n[385]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[397]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[409]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE\n[421]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[433]  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n[445]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n[457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[469]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n[481] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n[493] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[505]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n[517]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n[529] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n[541] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n```\n\n\n:::\n:::\n\n \n\nYou might be able to guess what this does, in the light of what we\njust did, but if not, you can investigate. Let's pick three rows where\n`complete.cases` is \n`TRUE` and three where it's\n`FALSE`, and see what happens.\n\nI'll pick rows 496, 497, and 498 for the TRUE rows, and 540, 541 and\n542 for the FALSE ones. Let's assemble these rows into a vector and\nuse `slice` to display the rows with these numbers:\n\n::: {.cell}\n\n```{.r .cell-code}\nrows <- c(496, 497, 498, 540, 541, 542)\nrows\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 496 497 498 540 541 542\n```\n\n\n:::\n:::\n\n \n\nLike this:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% slice(rows)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 15\n  respondent_id lottery_a smoke alcohol gamble skydiving speed cheated steak\n          <dbl> <lgl>     <lgl> <lgl>   <lgl>  <lgl>     <lgl> <lgl>   <lgl>\n1    3234776895 FALSE     FALSE FALSE   FALSE  FALSE     FALSE FALSE   TRUE \n2    3234776815 TRUE      TRUE  TRUE    FALSE  FALSE     TRUE  FALSE   TRUE \n3    3234776702 FALSE     FALSE FALSE   FALSE  FALSE     FALSE FALSE   TRUE \n4    3234763650 TRUE      FALSE FALSE   FALSE  FALSE     TRUE  FALSE   FALSE\n5    3234763171 TRUE      FALSE TRUE    TRUE   FALSE     TRUE  FALSE   TRUE \n6    3234762715 FALSE     FALSE FALSE   FALSE  FALSE     TRUE  FALSE   FALSE\n# i 6 more variables: steak_prep <chr>, female <lgl>, age <chr>,\n#   hhold_income <chr>, educ <chr>, region <chr>\n```\n\n\n:::\n:::\n\n \n\nWhat's the difference? \nThe rows where `complete.cases` is FALSE have one (or more)\nmissing values in them; where `complete.cases` is TRUE the\nrows have no missing values. (Depending on the rows you choose,\nyou may not see the missing value(s), as I didn't.)\nExtra (within \"extra extra\": I hope you are keeping track): this\nis a bit tricky to investigate more thoroughly, because the text\nvariables might have missing values in them, and they won't show\nup unless we turn them into a factor first:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>%\n  mutate(across(where(is.character), \\(x) factor(x))) %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:279       FALSE:453       FALSE:125      \n Median :3.235e+09   TRUE :267       TRUE :84        TRUE :416      \n Mean   :3.235e+09   NA's :4         NA's :13        NA's :9        \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.238e+09                                                  \n                                                                    \n   gamble        skydiving         speed          cheated       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:280       FALSE:502       FALSE:59        FALSE:447      \n TRUE :257       TRUE :36        TRUE :480       TRUE :92       \n NA's :13        NA's :12        NA's :11        NA's :11       \n                                                                \n                                                                \n                                                                \n   steak               steak_prep    female           age     \n Mode :logical   Medium     :132   Mode :logical   >60  :131  \n FALSE:109       Medium rare:166   FALSE:246       18-29:110  \n TRUE :430       Medium Well: 75   TRUE :268       30-44:133  \n NA's :11        Rare       : 23   NA's :36        45-60:140  \n                 Well       : 36                   NA's : 36  \n                 NA's       :118                              \n                                                              \n              hhold_income                               educ    \n $0 - $24,999       : 51   Bachelor degree                 :174  \n $100,000 - $149,999: 76   Graduate degree                 :133  \n $150,000+          : 54   High school degree              : 39  \n $25,000 - $49,999  : 77   Less than high school degree    :  2  \n $50,000 - $99,999  :172   Some college or Associate degree:164  \n NA's               :120   NA's                            : 38  \n                                                                 \n                region   \n Pacific           : 91  \n South Atlantic    : 88  \n East North Central: 86  \n Middle Atlantic   : 72  \n West North Central: 42  \n (Other)           :133  \n NA's              : 38  \n```\n\n\n:::\n:::\n\n     \n\nThere are missing values everywhere. What the `where` \ndoes is to do something for each column where the first thing is true:\nhere, if the column is text, then replace it by the factor version of\nitself. This makes for a better summary, one that shows how many\nobservations are in each category, and, more important for us, how\nmany are missing (a lot).\n\nAll right, so there are 15 columns, so let's investigate missingness\nin our rows by looking at the columns 1 through 8 and then 9 through\n15, so they all fit on the screen. Recall that you can `select`\ncolumns by number:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% select(1:8) %>% slice(rows)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 8\n  respondent_id lottery_a smoke alcohol gamble skydiving speed cheated\n          <dbl> <lgl>     <lgl> <lgl>   <lgl>  <lgl>     <lgl> <lgl>  \n1    3234776895 FALSE     FALSE FALSE   FALSE  FALSE     FALSE FALSE  \n2    3234776815 TRUE      TRUE  TRUE    FALSE  FALSE     TRUE  FALSE  \n3    3234776702 FALSE     FALSE FALSE   FALSE  FALSE     FALSE FALSE  \n4    3234763650 TRUE      FALSE FALSE   FALSE  FALSE     TRUE  FALSE  \n5    3234763171 TRUE      FALSE TRUE    TRUE   FALSE     TRUE  FALSE  \n6    3234762715 FALSE     FALSE FALSE   FALSE  FALSE     TRUE  FALSE  \n```\n\n\n:::\n:::\n\n \n\nand\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% select(9:15) %>% slice(rows)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 7\n  steak steak_prep  female age   hhold_income        educ                 region\n  <lgl> <chr>       <lgl>  <chr> <chr>               <chr>                <chr> \n1 TRUE  Medium rare TRUE   45-60 $0 - $24,999        Some college or Ass~ West ~\n2 TRUE  Medium      TRUE   45-60 $150,000+           Bachelor degree      New E~\n3 TRUE  Medium rare TRUE   >60   $50,000 - $99,999   Graduate degree      Mount~\n4 FALSE <NA>        FALSE  45-60 $100,000 - $149,999 Some college or Ass~ South~\n5 TRUE  Medium      FALSE  >60   <NA>                Graduate degree      Pacif~\n6 FALSE <NA>        FALSE  18-29 $50,000 - $99,999   Some college or Ass~ West ~\n```\n\n\n:::\n:::\n\n \n\nIn this case, the first three rows have no missing values anywhere,\nand the last three rows have exactly one missing value. This\ncorresponds to what we would expect, with `complete.cases`\nidentifying rows that have any missing values.\n\nWhat we now need to do is to obtain a data frame that contains only\nthe rows with non-missing values. This can be done by saving the\nresult of `complete.cases` in a variable first; `filter`\ncan take anything that produces a true or a false for each row, and\nwill return the rows for which the thing it was fed was true.\n\n::: {.cell}\n\n```{.r .cell-code}\ncc <- complete.cases(steak0)\nsteak0 %>% filter(cc) -> steak.complete\n```\n:::\n\n     \n\nA quick check that we got rid of the missing values:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.complete\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 331 x 15\n   respondent_id lottery_a smoke alcohol gamble skydiving speed cheated steak\n           <dbl> <lgl>     <lgl> <lgl>   <lgl>  <lgl>     <lgl> <lgl>   <lgl>\n 1    3234982343 TRUE      FALSE TRUE    FALSE  FALSE     FALSE FALSE   TRUE \n 2    3234973379 TRUE      FALSE TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 3    3234972383 FALSE     TRUE  TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 4    3234958833 FALSE     FALSE TRUE    FALSE  FALSE     TRUE  TRUE    TRUE \n 5    3234955240 TRUE      FALSE FALSE   FALSE  FALSE     TRUE  FALSE   TRUE \n 6    3234955010 TRUE      FALSE TRUE    TRUE   TRUE      TRUE  FALSE   TRUE \n 7    3234953052 TRUE      TRUE  TRUE    TRUE   FALSE     TRUE  FALSE   TRUE \n 8    3234951249 FALSE     FALSE TRUE    TRUE   FALSE     FALSE FALSE   TRUE \n 9    3234948883 FALSE     FALSE TRUE    FALSE  FALSE     TRUE  FALSE   TRUE \n10    3234948197 TRUE      FALSE FALSE   TRUE   FALSE     TRUE  FALSE   TRUE \n# i 321 more rows\n# i 6 more variables: steak_prep <chr>, female <lgl>, age <chr>,\n#   hhold_income <chr>, educ <chr>, region <chr>\n```\n\n\n:::\n:::\n\n \n\nThere are no missing values *there*. Of course, this is not a\nproof, and there might be some missing values further down, but at\nleast it suggests that we might be good.\n\nFor proof, this is the easiest way I know:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.complete %>%\n  mutate(across(where(is.character), \\(x) factor(x))) %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:171       FALSE:274       FALSE:65       \n Median :3.235e+09   TRUE :160       TRUE :57        TRUE :266      \n Mean   :3.235e+09                                                  \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.235e+09                                                  \n                                                                    \n   gamble        skydiving         speed          cheated         steak        \n Mode :logical   Mode :logical   Mode :logical   Mode :logical   Mode:logical  \n FALSE:158       FALSE:308       FALSE:28        FALSE:274       TRUE:331      \n TRUE :173       TRUE :23        TRUE :303       TRUE :57                      \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n       steak_prep    female           age                  hhold_income\n Medium     :109   Mode :logical   >60  :82   $0 - $24,999       : 37  \n Medium rare:128   FALSE:174       18-29:70   $100,000 - $149,999: 66  \n Medium Well: 56   TRUE :157       30-44:93   $150,000+          : 39  \n Rare       : 18                   45-60:86   $25,000 - $49,999  : 55  \n Well       : 20                              $50,000 - $99,999  :134  \n                                                                       \n                                                                       \n                               educ                    region  \n Bachelor degree                 :120   South Atlantic    :68  \n Graduate degree                 : 86   Pacific           :57  \n High school degree              : 20   East North Central:48  \n Less than high school degree    :  1   Middle Atlantic   :46  \n Some college or Associate degree:104   West North Central:29  \n                                        Mountain          :24  \n                                        (Other)           :59  \n```\n\n\n:::\n:::\n\n \n\nIf there were any missing values, they would be listed on the end of\nthe counts of observations for each level, or on the bottom of the\nfive-number sumamries. But there aren't.  So here's your proof.\n\n\n$\\blacksquare$\n\n(d) Write the data into a `.csv` file, with a name like\n`steak1.csv`.  Open this file in a spreadsheet and (quickly)\nverify that you have the right columns and no missing values.\n\n\nSolution\n\n\nThis is `write_csv`, using my output from \n`drop_na`:\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(steak, \"steak1.csv\")\n```\n:::\n\n     \n\nOpen up Excel, or whatever you have, and take a look. You should have\nall the right columns, and, scrolling down, no visible missing values.\n\n$\\blacksquare$\n\n\n\n\n\n",
=======
    "markdown": "# Logistic regression with ordinal response\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Do you like your mobile phone?\n\n\n A phone company commissioned a survey of their customers'\nsatisfaction with their mobile devices. The responses to the survey\nwere on a so-called Likert scale of \"very unsatisfied\",\n\"unsatisfied\", \"satisfied\", \"very satisfied\". Also recorded were\neach customer's gender and age group (under 18, 18--24, 25--30, 31 or\nolder). (A survey of this kind does not ask its respondents for their\nexact age, only which age group they fall in.) The data, as\nfrequencies of people falling into each category combination, are in [link](http://ritsokiguess.site/datafiles/mobile.txt).\n\n\n\n(a) <a name=\"part:gather\">*</a> Read in the data and take a look at the format. Use a tool\nthat you know about to arrange the frequencies in just one column,\nwith other columns labelling the response categories that the\nfrequencies belong to. Save the new data frame. (Take a look at it\nif you like.)\n \n\n(b) We are going to fit ordered logistic models below. To\ndo that, we need our response variable to be a factor with its levels\nin the right order. By looking at the data frame\nyou just created, determine what kind\nof thing your intended response variable currently is.\n\n\n\n\n(c) If your intended response variable is not a factor, create a factor in your data frame with levels in the right order. Hint: look at the order your levels are *in the data*.\n\n\n\n\n(d) <a name=\"part:thefit\">*</a>\nFit ordered logistic models to predict satisfaction from (i) gender\nand age group, (ii) gender only, (iii) age group only. (You don't\nneed to examine the models.) Don't forget a suitable\n`weights`!\n \n\n(e) Use `drop1` on your model containing both explanatory\nvariables to determine whether you can remove either of them. Use\n`test=\"Chisq\"` to obtain P-values.\n\n\n\n\n(f) Use `anova` to decide whether we are justified in\nremoving `gender` from a model containing both `gender`\nand `age.group`. Compare your P-value with the one from `drop1`.\n \n\n(g) Use `anova` to see whether we are justified in removing\n`age.group` from a model  containing both `gender` and\n`age.group`. Compare your P-value with the one from\n`drop1` above.\n \n\n(h) Which of the models you have fit so far is the most\nappropriate one? Explain briefly.\n \n\n(i) Obtain predicted probabilities of a\ncustomer falling in the various satisfaction categories, as it\ndepends on gender and age group. To do that, you need to feed\n`predict` three things: the fitted model that contains both\nage group and gender, the data frame that you read in from the file\nback in part (<a href=\"#part:gather\">here</a>) (which contains all the combinations of age group\nand gender), and an appropriate `type`.\n\n \n\n(j) <a name=\"part:unload\">*</a> \nDescribe any patterns you see in the predictions, bearing in mind the\nsignificance or not of the explanatory variables.\n\n \n\n\n \n\n\n\n\n\n##  Finding non-missing values\n\n\n <a name=\"part:prepare-next\">*</a> This is to prepare you for something in the next\nquestion. It's meant to be easy.\n\nIn R, the code `NA` stands for \"missing value\" or\n\"value not known\". In R, `NA` should not have quotes around\nit. (It is a special code, not a piece of text.)\n\n\n(a) Create a vector `v` that contains some numbers and some\nmissing values, using `c()`. Put those values into a\none-column data frame.\n \n\n(b) Obtain a new column containing `is.na(v)`. When is this true and when is this false?\n \n\n(c) The symbol `!` means \"not\" in R (and other\nprogramming languages). What does `!is.na(v)` do? Create a\nnew column containing that.\n \n\n(d) Use `filter` to display just the\nrows of your data frame that have a non-missing value of `v`.\n\n \n\n\n\n\n\n\n##  High School and Beyond\n\n\n A survey called High School and Beyond was given to a large\nnumber of American high school seniors (grade 12) by the National\nCenter of Education Statistics. The data set at\n[link](http://ritsokiguess.site/datafiles/hsb.csv) is a random\nsample of 200 of those students.\n\nThe variables collected are:\n\n\n\n* `gender`: student's gender, female or male.\n\n* `race`: the student's race (African-American,\nAsian,^[I'm always amused at how Americans put all Asians    into one group.]  Hispanic, White).\n\n* `ses`: Socio-economic status of student's family (low,\nmiddle, or high)\n\n* `schtyp`: School type, public or private.\n\n* `prog`: Student's program, general, academic, or\nvocational. \n\n* `read`: Score on standardized reading test.\n\n* `write`: Score on standardized writing test.\n\n* `math`: Score on standardized math test.\n\n* `science`: Score on standardized science test.\n\n* `socst`: Score on standardized social studies test.\n\n\n\nOur aim is to see how socio-economic status is related to the other\nvariables. \n\n\n\n(a) Read in and display (some of) the data.\n\n\n(b) Explain briefly why an ordinal logistic regression is\nappropriate for our aims.\n\n\n(c) Fit an ordinal logistic regression predicting\nsocio-economic status from the scores on the five standardized\ntests. (You don't need to display the results.) You will probably\ngo wrong the first time. What kind of thing does your response\nvariable have to be? \n\n\n(d) Remove any non-significant explanatory variables one at a\ntime. Use `drop1` to decide which one to remove next.\n\n\n(e) The quartiles of the `science` test score are 44 \nand 58. The quartiles of the `socst` test score are 46 and 61. Make\na data frame that has all combinations of those quartiles. If your best\nregression had any other explanatory variables in it, also put the\n*means* of those variables into this data frame.\n\n\n\n(f) Use the data frame you created in the previous part, together\nwith your best model, to obtain predicted probabilities of being in\neach `ses` category. Display these predicted probabilities so that they are easy to read.\n\n\n\n(g) What is the effect of an increased science score on the\nlikelihood of a student being in the different socioeconomic groups,\nall else equal?  Explain briefly. In your explanation, state clearly\nhow you are using your answer to the previous part.\n\n\n\n\n\n\n\n\n##  How do you like your steak?\n\n\n When you order a steak in a restaurant, the server will ask\nyou how you would like it cooked, or to be precise, *how much*\nyou would like it cooked: rare (hardly cooked at all), through medium\nrare, medium, medium well to well (which means \"well done\", so that\nthe meat has only a little red to it). Could you guess how a person\nlikes their steak cooked, from some other information about them? The\nwebsite [link](fivethirtyeight.com) commissioned a survey where they\nasked a number of people how they preferred their steak, along with as\nmany other things as they could think of to ask. (Many of the\nvariables below are related to risk-taking, which was something the\npeople designing the survey thought might have something to do with\nliking steak rare.) The variables of interest are all factors or true/false:\n\n\n\n* `respondent_ID`: a ten-digit number identifying each\nperson who responded to the survey.\n\n* `lottery_a`: true if the respondent preferred lottery A\nwith a small chance to win a lot of money, to lottery B, with a\nlarger chance to win less money.\n\n* `smoke`: true if the respondent is currently a smoker\n\n* `alcohol`: true if the respondent at least occasionally\ndrinks alcohol.\n\n* `gamble`: true if the respondent likes to gamble (eg.\nbetting on horse racing or playing the lottery)\n\n* `skydiving`: true if the respondent has ever been\nskydiving.\n\n* `speed`: true if the respondent likes to drive fast\n\n* `cheated`: true if the respondent has ever cheated on a\nspouse or girlfriend/boyfriend\n\n* `steak`: true if the respondent likes to eat steak\n\n* `steak_prep` (response): how the respondent likes their\nsteak cooked (factor, as described above, with 5 levels).\n\n* `female`: true if the respondent is female\n\n* `age`: age group, from 18--29 to 60+.\n\n* `hhold_income`: household income group, from \\$0--24,999\nto \\$150,000+.\n\n* `educ`: highest level of education attained, from \n\"less than high school\" up to \"graduate degree\"\n\n* `region`: region (of the US)\nthat the respondent lives in (five values).\n\n\nThe data are in\n[link](http://ritsokiguess.site/datafiles/steak1.csv). This is the\ncleaned-up data from a previous question, with the missing values removed.\n\n\n\n(a) Read in the data and display the first few lines.\n\n\n\n\n(b) We are going to predict `steak_prep` from some of\nthe other variables. Why is the model-fitting function `polr`\nfrom package `MASS` the best choice for these data\n(alternatives being `glm` and `multinom` from package\n`nnet`)?\n\n\n\n(c) What are the levels of `steak_prep`, \n*in the  order that R thinks they are in?* If they are not in a sensible\norder, create an ordered factor where the levels are in a sensible order.\n\n\n\n(d) Fit a model predicting preferred steak preparation in an\nordinal logistic regression from `educ`, `female` and\n`lottery_a`. This ought to be easy from your previous work,\nbut you have to be careful about one thing. No need to print out the\nresults. \n\n\n\n(e) Run `drop1` on your fitted model, with\n`test=\"Chisq\"`. Which explanatory variable should be removed\nfirst, if any? Bear in mind that the variable with the\n*smallest* AIC should come out first, in case your table\ndoesn't get printed in order.\n\n\n\n(f) Remove the variable that should come out first, using\n`update`. (If all the variables should stay, you can skip\nthis part.)\n\n\n\n(g) Using the best model that you have so far, predict the\nprobabilities of preferring each different steak preparation (method\nof cooking) for each combination of the variables that\nremain. (Some of the variables are TRUE and FALSE rather than\nfactors. Bear this in mind.)\nDescribe the effects of each variable on the predicted\nprobabilities, if any. Note that there is exactly one person in the\nstudy whose educational level is \"less than high school\".\n\n\n\n(h) Is it reasonable to remove *all* the remaining\nexplanatory variables from your best model so far? Fit a model with no explanatory variables,\nand do a test. (In R, if the right side of the squiggle is a\n`1`, that means \"just an intercept\". Or you can remove\nwhatever remains using `update`.) What do you conclude?\nExplain briefly.\n\n\n\n(i) In the article for which these data were collected,\n[link](https://fivethirtyeight.com/datalab/how-americans-like-their-steak/),\ndoes the author obtain consistent conclusions with yours? Explain\nbriefly. (It's not a very long article, so it won't take you long to\nskim through, and the author's point is pretty clear.)\n\n\n\n\n\n\n##  How do you like your steak -- the data\n\n\n  <a name=\"q:steak-data\">*</a> \nThis question takes you through the data preparation for one\nof the other questions. You don't have to do *this*\nquestion, but you may find it interesting or useful.\n\nWhen you order a steak in a restaurant, the server will ask\nyou how you would like it cooked, or to be precise, *how much*\nyou would like it cooked: rare (hardly cooked at all), through medium\nrare, medium, medium well to well (which means \"well done\", so that\nthe meat has only a little red to it). Could you guess how a person\nlikes their steak cooked, from some other information about them? The\nwebsite [link](fivethirtyeight.com) commissioned a survey where they\nasked a number of people how they preferred their steak, along with as\nmany other things as they could think of to ask. (Many of the\nvariables below are related to risk-taking, which was something the\npeople designing the survey thought might have something to do with\nliking steak rare.) The variables of interest are all factors or true/false:\n\n\n\n* `respondent_ID`: a ten-digit number identifying each\nperson who responded to the survey.\n\n* `lottery_a`: true if the respondent preferred lottery A\nwith a small chance to win a lot of money, to lottery B, with a\nlarger chance to win less money.\n\n* `smoke`: true if the respondent is currently a smoker\n\n* `alcohol`: true if the respondent at least occasionally\ndrinks alcohol.\n\n* `gamble`: true if the respondent likes to gamble (eg.\nbetting on horse racing or playing the lottery)\n\n* `skydiving`: true if the respondent has ever been\nskydiving.\n\n* `speed`: true if the respondent likes to drive fast\n\n* `cheated`: true if the respondent has ever cheated on a\nspouse or girlfriend/boyfriend\n\n* `steak`: true if the respondent likes to eat steak\n\n* `steak_prep` (response): how the respondent likes their\nsteak cooked (factor, as described above, with 5 levels).\n\n* `female`: true if the respondent is female\n\n* `age`: age group, from 18--29 to 60+.\n\n* `hhold_income`: household income group, from \\$0--24,999\nto \\$150,000+.\n\n* `educ`: highest level of education attained, from \n\"less  than high school\" \nup to \"graduate degree\"\n\n* `region`: region (of the US)\nthat the respondent lives in (five values).\n\n\nThe data are in\n[link](http://ritsokiguess.site/datafiles/steak.csv). \n\n\n\n(a) Read in the data and display the first few lines.\n\n\n\n(b) What do you immediately notice about your data frame? Run `summary` on the entire data frame. Would you say you have a lot of missing values, or only a few?\n\n\n(c) What does the function `drop_na` do when applied to a data frame with missing values? To find out, pass the data frame into `drop_na()`, then into `summary` again. What has happened?\n\n\n(d) Write the data into a `.csv` file, with a name like\n`steak1.csv`.  Open this file in a spreadsheet and (quickly)\nverify that you have the right columns and no missing values.\n\n\n\n\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Do you like your mobile phone?\n\n\n A phone company commissioned a survey of their customers'\nsatisfaction with their mobile devices. The responses to the survey\nwere on a so-called Likert scale of \"very unsatisfied\",\n\"unsatisfied\", \"satisfied\", \"very satisfied\". Also recorded were\neach customer's gender and age group (under 18, 18--24, 25--30, 31 or\nolder). (A survey of this kind does not ask its respondents for their\nexact age, only which age group they fall in.) The data, as\nfrequencies of people falling into each category combination, are in [link](http://ritsokiguess.site/datafiles/mobile.txt).\n\n\n\n(a) <a name=\"part:gather\">*</a> Read in the data and take a look at the format. Use a tool\nthat you know about to arrange the frequencies in just one column,\nwith other columns labelling the response categories that the\nfrequencies belong to. Save the new data frame. (Take a look at it\nif you like.)\n \nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/mobile.txt\"\nmobile <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 8 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (2): gender, age.group\ndbl (4): very.unsat, unsat, sat, very.sat\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nmobile\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 6\n  gender age.group very.unsat unsat   sat very.sat\n  <chr>  <chr>          <dbl> <dbl> <dbl>    <dbl>\n1 male   0-17               3     9    18       24\n2 male   18-24              6    13    16       28\n3 male   25-30              9    13    17       20\n4 male   31+                5     7    16       16\n5 female 0-17               4     8    11       25\n6 female 18-24              8    14    20       18\n7 female 25-30             10    15    16       12\n8 female 31+                5    14    12        8\n```\n:::\n:::\n\n \nWith multiple columns that are all frequencies, this is a job for\n`pivot_longer`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile %>% \n  pivot_longer(very.unsat:very.sat, \n               names_to=\"satisfied\", \n               values_to=\"frequency\") -> mobile.long\nmobile.long\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 4\n   gender age.group satisfied  frequency\n   <chr>  <chr>     <chr>          <dbl>\n 1 male   0-17      very.unsat         3\n 2 male   0-17      unsat              9\n 3 male   0-17      sat               18\n 4 male   0-17      very.sat          24\n 5 male   18-24     very.unsat         6\n 6 male   18-24     unsat             13\n 7 male   18-24     sat               16\n 8 male   18-24     very.sat          28\n 9 male   25-30     very.unsat         9\n10 male   25-30     unsat             13\n# i 22 more rows\n```\n:::\n:::\n\n     \n\nYep, all good. See how `mobile.long` contains what it should?\n(For those keeping track, the original data frame had 8 rows and 4\ncolumns to collect up, and the new one has $8\\times 4=32$ rows.)\n \n\n$\\blacksquare$\n\n(b) We are going to fit ordered logistic models below. To\ndo that, we need our response variable to be a factor with its levels\nin the right order. By looking at the data frame\nyou just created, determine what kind\nof thing your intended response variable currently is.\n\n\n\nSolution\n\n\nI looked at `mobile.long` in the previous part, but if you\ndidn't, look at it here:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.long\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 4\n   gender age.group satisfied  frequency\n   <chr>  <chr>     <chr>          <dbl>\n 1 male   0-17      very.unsat         3\n 2 male   0-17      unsat              9\n 3 male   0-17      sat               18\n 4 male   0-17      very.sat          24\n 5 male   18-24     very.unsat         6\n 6 male   18-24     unsat             13\n 7 male   18-24     sat               16\n 8 male   18-24     very.sat          28\n 9 male   25-30     very.unsat         9\n10 male   25-30     unsat             13\n# i 22 more rows\n```\n:::\n:::\n\n     \n\nMy intended response variable is what I called `satisfied`.\nThis is `chr` or \"text\", not the `factor` that I\nwant. \n\n\n\n$\\blacksquare$\n\n(c) If your intended response variable is not a factor, create a factor in your data frame with levels in the right order. Hint: look at the order your levels are *in the data*.\n\n\n\nSolution\n\n\nMy intended response `satisfied` is text, not a factor, so\nI need to do this part.\nThe hint is to look at the column `satisfied` in\n`mobile.long` and note that the satisfaction categories\nappear in the data *in the order that we want*. This is good\nnews, because we can use `fct_inorder` like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.long %>%\n  mutate(satis = fct_inorder(satisfied)) -> mobile.long\n```\n:::\n\n     \n\nIf you check, by looking at the data frame, `satis` is \na `factor`, and you can also do this to verify that its levels\nare in the right order:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(mobile.long, levels(satis))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n```\n:::\n:::\n\n \nSuccess.\n\nExtra: so now you are asking, what if the levels are in the *wrong* order in the data? Well, below is what you used to have to do, and it will work for this as well.\nI'll first find what levels of satisfaction I have. This\ncan be done by counting them, or by finding the distinct ones:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.long %>% count(satisfied)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  satisfied      n\n  <chr>      <int>\n1 sat            8\n2 unsat          8\n3 very.sat       8\n4 very.unsat     8\n```\n:::\n:::\n\n     \n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.long %>% distinct(satisfied)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 1\n  satisfied \n  <chr>     \n1 very.unsat\n2 unsat     \n3 sat       \n4 very.sat  \n```\n:::\n:::\n\n \n\nIf you count them, they come out in alphabetical order. If you ask for\nthe distinct ones, they come out in the order they were in\n`mobile.long`, which is the order the *columns* of those\nnames were in `mobile`, which is the order we want. \n\nTo actually grab those satisfaction levels as a vector (that we will\nneed in a minute), use `pluck` to pull the column out of the\ndata frame as a vector:\n\n::: {.cell}\n\n```{.r .cell-code}\nv1 <- mobile.long %>%\n  distinct(satisfied) %>%\n  pluck(\"satisfied\")\nv1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n```\n:::\n:::\n\n \n\nwhich is in the correct order, or\n\n::: {.cell}\n\n```{.r .cell-code}\nv2 <- mobile.long %>%\n  count(satisfied) %>%\n  pluck(\"satisfied\")\nv2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"sat\"        \"unsat\"      \"very.sat\"   \"very.unsat\"\n```\n:::\n:::\n\n \n\nwhich is in alphabetical order. The problem with the second one is\nthat we know the correct order, but there isn't a good way to code\nthat, so we have to rearrange it ourselves. The correct order from\n`v2` is 4, 2, 1, 3, so:\n\n::: {.cell}\n\n```{.r .cell-code}\nv3 <- c(v2[4], v2[2], v2[1], v2[3])\nv3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n```\n:::\n\n```{.r .cell-code}\nv4 <- v2[c(4, 2, 1, 3)]\nv4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n```\n:::\n:::\n\n \n\nEither of these will work. The first one is more typing, but is\nperhaps more obvious. There is a third way, which is to keep things as\na data frame until the end, and use `slice` to pick out the\nrows in the right order:\n\n::: {.cell}\n\n```{.r .cell-code}\nv5 <- mobile.long %>%\n  count(satisfied) %>%\n  slice(c(4, 2, 1, 3)) %>%\n  pluck(\"satisfied\")\nv5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"very.unsat\" \"unsat\"      \"sat\"        \"very.sat\"  \n```\n:::\n:::\n\n \n\nIf you don't see how that works, run it yourself, one line at a time.\n\nThe other way of doing this is to physically type them into a vector,\nbut this carries the usual warnings of requiring you to be very\ncareful and that it won't be reproducible (eg. if you do another\nsurvey with different response categories). \n\nSo now create the proper response\nvariable thus, using your vector of categories:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.long %>%\n  mutate(satis = ordered(satisfied, v1)) -> mobile.long2\nmobile.long2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 x 5\n   gender age.group satisfied  frequency satis     \n   <chr>  <chr>     <chr>          <dbl> <ord>     \n 1 male   0-17      very.unsat         3 very.unsat\n 2 male   0-17      unsat              9 unsat     \n 3 male   0-17      sat               18 sat       \n 4 male   0-17      very.sat          24 very.sat  \n 5 male   18-24     very.unsat         6 very.unsat\n 6 male   18-24     unsat             13 unsat     \n 7 male   18-24     sat               16 sat       \n 8 male   18-24     very.sat          28 very.sat  \n 9 male   25-30     very.unsat         9 very.unsat\n10 male   25-30     unsat             13 unsat     \n# i 22 more rows\n```\n:::\n:::\n\n \n\n`satis` has the same values as `satisfied`, but its\nlabel `ord` means that it is an ordered factor, as we want.\n\n\n$\\blacksquare$\n\n(d) <a name=\"part:thefit\">*</a>\nFit ordered logistic models to predict satisfaction from (i) gender\nand age group, (ii) gender only, (iii) age group only. (You don't\nneed to examine the models.) Don't forget a suitable\n`weights`!\n \nSolution\n\n\n(i):\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nmobile.1 <- polr(satis ~ gender + age.group, weights = frequency, data = mobile.long)\n```\n:::\n\n \n\nFor (ii) and (iii), `update` is the thing (it works for any\nkind of model):\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile.2 <- update(mobile.1, . ~ . - age.group)\nmobile.3 <- update(mobile.1, . ~ . - gender)\n```\n:::\n\n \n\nWe're not going to look at these, because the output from\n`summary` is not very illuminating. What we do next is to try\nto figure out which (if either) of the explanatory variables\n`age.group` and `gender` we need.\n \n\n$\\blacksquare$\n\n(e) Use `drop1` on your model containing both explanatory\nvariables to determine whether you can remove either of them. Use\n`test=\"Chisq\"` to obtain P-values.\n\n\n\nSolution\n\n\n`drop1` takes a fitted model, and tests each term in\nit in turn, and says which (if any) should be removed. Here's how it goes:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(mobile.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nsatis ~ gender + age.group\n          Df    AIC     LRT Pr(>Chi)   \n<none>       1101.8                    \ngender     1 1104.2  4.4089 0.035751 * \nage.group  3 1109.0 13.1641 0.004295 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nThe possibilities are to remove `gender`, to remove\n`age.group` or to remove nothing. \nThe best one is \"remove nothing\", because it's the one on the output with the smallest\nAIC. Both P-values are small, so it would be a mistake to remove\neither of the explanatory variables.\n\n\n$\\blacksquare$\n\n(f) Use `anova` to decide whether we are justified in\nremoving `gender` from a model containing both `gender`\nand `age.group`. Compare your P-value with the one from `drop1`.\n \nSolution\n\n\nThis is a comparison of the model with both variables\n(`mobile.1`) and the model with `gender` removed\n(`mobile.3`). Use `anova` for this, smaller\n(fewer-$x$) model first:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(mobile.3, mobile.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of ordinal regression models\n\nResponse: satis\n               Model Resid. df Resid. Dev   Test    Df LR stat.    Pr(Chi)\n1          age.group       414   1092.213                                 \n2 gender + age.group       413   1087.804 1 vs 2     1  4.40892 0.03575146\n```\n:::\n:::\n\n     \n\nThe P-value is (just) less than 0.05, so the models are significantly\ndifferent. That means that the model with both variables in fits\nsignificantly better than the model with only `age.group`, and\ntherefore that taking `gender` out is a mistake.\n\nThe P-value is identical to the one from `drop1` (because they\nare both doing the same test).\n \n$\\blacksquare$\n\n(g) Use `anova` to see whether we are justified in removing\n`age.group` from a model  containing both `gender` and\n`age.group`. Compare your P-value with the one from\n`drop1` above.\n \nSolution\n\n\nExactly the same idea as the last part. In my case, I'm comparing\nmodels `mobile.2` and `mobile.1`:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(mobile.2, mobile.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of ordinal regression models\n\nResponse: satis\n               Model Resid. df Resid. Dev   Test    Df LR stat.     Pr(Chi)\n1             gender       416   1100.968                                  \n2 gender + age.group       413   1087.804 1 vs 2     3 13.16411 0.004294811\n```\n:::\n:::\n\n    \n\nThis one is definitely significant, so I need to keep\n`age.group` for sure. Again, the P-value is the same as the one\nin `drop1`.\n \n$\\blacksquare$\n\n(h) Which of the models you have fit so far is the most\nappropriate one? Explain briefly.\n \nSolution\n\n\nI can't drop either of my variables, so I have to keep them both:\n`mobile.1`, with both `age.group` and `gender`.\n  \n$\\blacksquare$\n\n(i) Obtain predicted probabilities of a\ncustomer falling in the various satisfaction categories, as it\ndepends on gender and age group. To do that, you need to feed\n`predictions` three things: the fitted model that contains both\nage group and gender, the data frame that you read in from the file\nback in part (<a href=\"#part:gather\">here</a>) (which contains all the combinations of age group\nand gender), and an appropriate `type`.\n\n \nSolution\n\nMy model containing both $x$s was `mobile.1`, the data frame\nread in from the file was called `mobile`, and I  need\n`type=\"p\"` to get probabilities. The first thing is to get the genders and age groups to make combinations of them, which you can do like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = mobile.1, \n                gender = levels(factor(mobile$gender)),\n                age.group = levels(factor(mobile$age.group)))\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       satis frequency gender age.group\n1 very.unsat    13.125 female      0-17\n2 very.unsat    13.125 female     18-24\n3 very.unsat    13.125 female     25-30\n4 very.unsat    13.125 female       31+\n5 very.unsat    13.125   male      0-17\n6 very.unsat    13.125   male     18-24\n7 very.unsat    13.125   male     25-30\n8 very.unsat    13.125   male       31+\n```\n:::\n:::\n\nThe `levels(factor)` thing turns the (text) variable into a factor so that you can extract the distinct values using `levels`. You could also `count` the genders and age groups to find out which ones there are:\n\n::: {.cell}\n\n```{.r .cell-code}\nmobile %>% count(gender, age.group)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 3\n  gender age.group     n\n  <chr>  <chr>     <int>\n1 female 0-17          1\n2 female 18-24         1\n3 female 25-30         1\n4 female 31+           1\n5 male   0-17          1\n6 male   18-24         1\n7 male   25-30         1\n8 male   31+           1\n```\n:::\n:::\n\nThis gives you all the combinations, and so will also serve as a `new` without needing to use `datagrid`. Your choice.\n\nHaving done that, you now have a `new` to feed into `predictions`, but some care is still required:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(mobile.1, newdata = new)) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nRe-fitting to get Hessian\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n   rowid      group   estimate  std.error statistic      p.value   s.value\n1      1 very.unsat 0.08590744 0.01935201  4.439200 9.029377e-06  16.75694\n2      2 very.unsat 0.12858414 0.02486823  5.170620 2.333188e-07  22.03119\n3      3 very.unsat 0.18290971 0.03313939  5.519404 3.401515e-08  24.80925\n4      4 very.unsat 0.16112036 0.03279559  4.912866 8.975461e-07  20.08751\n5      5 very.unsat 0.06070852 0.01420828  4.272755 1.930724e-05  15.66050\n6      6 very.unsat 0.09212869 0.01921927  4.793559 1.638480e-06  19.21921\n7      7 very.unsat 0.13341018 0.02606206  5.118941 3.072553e-07  21.63406\n8      8 very.unsat 0.11667551 0.02538806  4.595684 4.313323e-06  17.82277\n9      1      unsat 0.18404109 0.02868504  6.415925 1.399702e-10  32.73416\n10     2      unsat 0.23872962 0.02921907  8.170336 3.075326e-16  51.53011\n11     3      unsat 0.28538804 0.03042338  9.380550 6.562918e-21  67.04615\n12     4      unsat 0.26929960 0.03193252  8.433397 3.357733e-17  54.72529\n13     5      unsat 0.14203024 0.02425803  5.854979 4.770707e-09  27.64315\n14     6      unsat 0.19320863 0.02710696  7.127640 1.021041e-12  39.83310\n15     7      unsat 0.24381101 0.02991897  8.149046 3.668079e-16  51.27582\n16     8      unsat 0.22529656 0.03117369  7.227138 4.932779e-13  40.88266\n17     1        sat 0.30117514 0.02422278 12.433551 1.718276e-35 115.48652\n18     2        sat 0.30914889 0.02313969 13.360111 1.034097e-40 132.82875\n19     3        sat 0.29200527 0.02470501 11.819678 3.088679e-32 104.67471\n20     4        sat 0.30087114 0.02432041 12.371138 3.744768e-35 114.36261\n21     5        sat 0.27528959 0.02602982 10.575930 3.853346e-26  84.42402\n22     6        sat 0.30447324 0.02352688 12.941505 2.624365e-38 124.84130\n23     7        sat 0.30845056 0.02324186 13.271336 3.394728e-40 131.11383\n24     8        sat 0.30979203 0.02308736 13.418254 4.727187e-41 133.95807\n25     1   very.sat 0.42887634 0.05211935  8.228735 1.892008e-16  52.23093\n26     2   very.sat 0.32353735 0.04275469  7.567295 3.810772e-14  44.57691\n27     3   very.sat 0.23969698 0.03781629  6.338458 2.320766e-10  32.00468\n28     4   very.sat 0.26870890 0.04424679  6.072958 1.255753e-09  29.56880\n29     5   very.sat 0.52197164 0.05117754 10.199232 1.998458e-24  78.72739\n30     6   very.sat 0.41018944 0.04662747  8.797163 1.403186e-18  59.30600\n31     7   very.sat 0.31432825 0.04318445  7.278737 3.369608e-13  41.43248\n32     8   very.sat 0.34823589 0.04969847  7.006973 2.435284e-12  38.57905\n     conf.low  conf.high      satis frequency gender age.group\n1  0.04797820 0.12383668 very.unsat    13.125 female      0-17\n2  0.07984331 0.17732496 very.unsat    13.125 female     18-24\n3  0.11795769 0.24786173 very.unsat    13.125 female     25-30\n4  0.09684218 0.22539854 very.unsat    13.125 female       31+\n5  0.03286080 0.08855625 very.unsat    13.125   male      0-17\n6  0.05445962 0.12979776 very.unsat    13.125   male     18-24\n7  0.08232947 0.18449088 very.unsat    13.125   male     25-30\n8  0.06691582 0.16643520 very.unsat    13.125   male       31+\n9  0.12781944 0.24026274 very.unsat    13.125 female      0-17\n10 0.18146129 0.29599795 very.unsat    13.125 female     18-24\n11 0.22575932 0.34501677 very.unsat    13.125 female     25-30\n12 0.20671301 0.33188618 very.unsat    13.125 female       31+\n13 0.09448538 0.18957511 very.unsat    13.125   male      0-17\n14 0.14007998 0.24633729 very.unsat    13.125   male     18-24\n15 0.18517092 0.30245110 very.unsat    13.125   male     25-30\n16 0.16419725 0.28639587 very.unsat    13.125   male       31+\n17 0.25369937 0.34865091 very.unsat    13.125 female      0-17\n18 0.26379592 0.35450186 very.unsat    13.125 female     18-24\n19 0.24358434 0.34042621 very.unsat    13.125 female     25-30\n20 0.25320401 0.34853826 very.unsat    13.125 female       31+\n21 0.22427207 0.32630710 very.unsat    13.125   male      0-17\n22 0.25836140 0.35058508 very.unsat    13.125   male     18-24\n23 0.26289735 0.35400377 very.unsat    13.125   male     25-30\n24 0.26454164 0.35504242 very.unsat    13.125   male       31+\n25 0.32672428 0.53102839 very.unsat    13.125 female      0-17\n26 0.23973969 0.40733501 very.unsat    13.125 female     18-24\n27 0.16557840 0.31381555 very.unsat    13.125 female     25-30\n28 0.18198679 0.35543102 very.unsat    13.125 female       31+\n29 0.42166550 0.62227778 very.unsat    13.125   male      0-17\n30 0.31880128 0.50157760 very.unsat    13.125   male     18-24\n31 0.22968828 0.39896822 very.unsat    13.125   male     25-30\n32 0.25082867 0.44564311 very.unsat    13.125   male       31+\n```\n:::\n:::\n\nThe predictions come out long, and we would like all the predictions for the same gender - age-group combination to come out in one row: That means pivoting the group column wider. I also took the opportunity to grab only the relevant columns:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(mobile.1, newdata = new)) %>% \n  select(gender, age.group, group, estimate) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nRe-fitting to get Hessian\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 6\n  gender age.group very.unsat unsat   sat very.sat\n  <chr>  <chr>          <dbl> <dbl> <dbl>    <dbl>\n1 female 0-17          0.0859 0.184 0.301    0.429\n2 female 18-24         0.129  0.239 0.309    0.324\n3 female 25-30         0.183  0.285 0.292    0.240\n4 female 31+           0.161  0.269 0.301    0.269\n5 male   0-17          0.0607 0.142 0.275    0.522\n6 male   18-24         0.0921 0.193 0.304    0.410\n7 male   25-30         0.133  0.244 0.308    0.314\n8 male   31+           0.117  0.225 0.310    0.348\n```\n:::\n:::\n\nDepending on the width of your display, you may or may not see all four probabilities.\n\nThis worked for me, but this might happen to you, with the same commands as above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(mobile.1, newdata = new)) %>% \n  MASS::select(gender, age.group, group, estimate) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in MASS::select(., gender, age.group, group, estimate): unused arguments (gender, age.group, group, estimate)\n```\n:::\n:::\n\n\nOh, this didn't work. Why not? There don't seem to be any errors.\n\nThis is the kind of thing that can bother you for *days*. The\nresolution (that it took me a long time to discover) is that you might\nhave the `tidyverse` *and also* `MASS` loaded, in\nthe wrong order, and `MASS` also has a `select` (that\ntakes different inputs and does something different). If you look back\nat part (<a href=\"#part:thefit\">here</a>), you might have seen a message there when you\nloaded `MASS` that `select` was \"masked\". When you\nhave two packages that both contain a function with the same name, the\none that you can see (and that will get used) is the one that was\nloaded *last*, which is the `MASS` select (not the one we\nactually wanted, which is the `tidyverse` select). There are a\ncouple of ways around this. One is to un-load the package we no longer\nneed (when we no longer need it). The mechanism for this is shown at\nthe end of part (<a href=\"#part:unload\">here</a>). The other is to say explicitly\nwhich package you want your function to come from, so that there is no\ndoubt. The `tidyverse` is actually a collection of\npackages. The best way to find out which one our `select` comes\nfrom is to go to the Console window in R Studio and ask for the help\nfor `select`. With both `tidyverse` and `MASS`\nloaded, the help window offers you a choice of both `select`s;\nthe one we want is \"select/rename variables by name\", and the actual\npackage it comes from is `dplyr`.\n\nThere is a third choice, which is the one I prefer now: install and load the package `conflicted`. When you run your code and it calls for something like `select` that is in two packages that you have loaded, it gives an error, like this:\n\n```\nError: [conflicted] `select` found in 2 packages.\nEither pick the one you want with `::` \n* MASS::select\n* dplyr::select\nOr declare a preference with `conflict_prefer()`\n* conflict_prefer(\"select\", \"MASS\")\n* conflict_prefer(\"select\", \"dplyr\")\n```\n\nFixing this costs you a bit of time upfront, but once you have fixed it, you know that the right thing is being run. What I do is to copy-paste one of those `conflict_prefer` lines, in this case the second one, and put it *before* the `select` that now causes the error. Right after the `library(conflicted)` is a good place. When you use `conflicted`, you will probably have to run several times to fix up all the conflicts, which will be a bit frustrating, and you will end up with several `conflict_prefer` lines, but once you have them there, you won't have to worry about the right function being called because you have explicitly said which one you want.\n\nThis is a non-standard use of `cbind` because I wanted to grab\nonly the gender and age group columns from `mobile` first, and\nthen `cbind` *that* to the predicted probabilities. The\nmissing first input to `cbind` is \n\"whatever came out of the previous step\", \nthat is, the first two columns of `mobile`.\n\nI only included the first two columns of `mobile` in the\n`cbind`, because the rest of the columns of `mobile`\nwere frequencies, which I don't need to see. (Having said that, it\nwould be interesting to make a *plot* using the observed\nproportions and predicted probabilities, but I didn't ask you for that.)\n\n\n$\\blacksquare$\n\n(j) <a name=\"part:unload\">*</a> \nDescribe any patterns you see in the predictions, bearing in mind the\nsignificance or not of the explanatory variables.\n\n \nSolution\n\n\nI had both explanatory variables being significant, so I would\nexpect to see both an age-group effect *and* a gender effect.\nFor both males and females, there seems to be a decrease in\nsatisfaction as the customers get older, at least until age 30 or\nso. I can see this because the predicted prob.\\ of \"very  satisfied\" \ndecreases, and the predicted prob.\nof \"very unsatisfied\" increases. The 31+ age group are very\nsimilar to the 25--30 group for both males and females. So that's\nthe age group effect.\nWhat about a gender effect? Well, for all the age groups, the males\nare more likely to be very satisfied than the females of the\ncorresponding age group, and also less likely to to be very\nunsatisfied. So the gender effect is that males are more satisfied\nthan females overall. (Or, the males are less discerning. Take your pick.)\nWhen we did the tests above, age group was very definitely\nsignificant, and gender less so (P-value around 0.03). This\nsuggests that the effect of age group ought to be large, and the\neffect of gender not so large. This is about what we observed: the\nage group effect was pretty clear, and the gender effect was\nnoticeable but small: the females were less satisfied than the\nmales, but there wasn't all that much difference.\n\n$\\blacksquare$\n\n\n \n\n\n\n\n\n##  Finding non-missing values\n\n\n <a name=\"part:prepare-next\">*</a> This is to prepare you for something in the next\nquestion. It's meant to be easy.\n\nIn R, the code `NA` stands for \"missing value\" or\n\"value not known\". In R, `NA` should not have quotes around\nit. (It is a special code, not a piece of text.)\n\n\n(a) Create a vector `v` that contains some numbers and some\nmissing values, using `c()`. Put those values into a\none-column data frame.\n \nSolution\n\n\nLike this. The arrangement of numbers and missing values doesn't\nmatter, as long as you have some of each:\n\n::: {.cell}\n\n```{.r .cell-code}\nv <- c(1, 2, NA, 4, 5, 6, 9, NA, 11)\nmydata <- tibble(v)\nmydata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 1\n      v\n  <dbl>\n1     1\n2     2\n3    NA\n4     4\n5     5\n6     6\n7     9\n8    NA\n9    11\n```\n:::\n:::\n\n     \n\nThis has one column called `v`.\n \n$\\blacksquare$\n\n(b) Obtain a new column containing `is.na(v)`. When is this true and when is this false?\n \nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata <- mydata %>% mutate(isna = is.na(v))\nmydata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 2\n      v isna \n  <dbl> <lgl>\n1     1 FALSE\n2     2 FALSE\n3    NA TRUE \n4     4 FALSE\n5     5 FALSE\n6     6 FALSE\n7     9 FALSE\n8    NA TRUE \n9    11 FALSE\n```\n:::\n:::\n\n     \n\nThis is `TRUE` if the corresponding element of `v` is\nmissing (in my case, the third value and the second-last one), and\n`FALSE` otherwise (when there is an actual value there).\n \n$\\blacksquare$\n\n(c) The symbol `!` means \"not\" in R (and other\nprogramming languages). What does `!is.na(v)` do? Create a\nnew column containing that.\n \nSolution\n\n\nTry it and see. Give it whatever name you like. My name reflects\nthat I know what it's going to do:\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata <- mydata %>% mutate(notisna = !is.na(v))\nmydata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 3\n      v isna  notisna\n  <dbl> <lgl> <lgl>  \n1     1 FALSE TRUE   \n2     2 FALSE TRUE   \n3    NA TRUE  FALSE  \n4     4 FALSE TRUE   \n5     5 FALSE TRUE   \n6     6 FALSE TRUE   \n7     9 FALSE TRUE   \n8    NA TRUE  FALSE  \n9    11 FALSE TRUE   \n```\n:::\n:::\n\n     \n\nThis is the logical opposite of `is.na`: it's true if there is\na value, and false if it's missing.\n \n$\\blacksquare$\n\n(d) Use `filter` to display just the\nrows of your data frame that have a non-missing value of `v`.\n\n \nSolution\n\n\n`filter` takes a column to say which rows to pick, in\nwhich case the column should contain something that either *is*\n`TRUE` or `FALSE`, or something that can be\ninterpreted that way:\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata %>% filter(notisna)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 x 3\n      v isna  notisna\n  <dbl> <lgl> <lgl>  \n1     1 FALSE TRUE   \n2     2 FALSE TRUE   \n3     4 FALSE TRUE   \n4     5 FALSE TRUE   \n5     6 FALSE TRUE   \n6     9 FALSE TRUE   \n7    11 FALSE TRUE   \n```\n:::\n:::\n\n   \n\nor you can provide `filter` something that can be calculated\nfrom what's in the data frame, and also returns something that is\neither true or false:\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata %>% filter(!is.na(v))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 x 3\n      v isna  notisna\n  <dbl> <lgl> <lgl>  \n1     1 FALSE TRUE   \n2     2 FALSE TRUE   \n3     4 FALSE TRUE   \n4     5 FALSE TRUE   \n5     6 FALSE TRUE   \n6     9 FALSE TRUE   \n7    11 FALSE TRUE   \n```\n:::\n:::\n\n \n\nIn either case, I only have non-missing values of `v`.\n \n$\\blacksquare$\n\n\n\n\n\n\n\n\n\n\n\n\n## High School and Beyond\n\nA survey called High School and Beyond was given to a large number of\nAmerican high school seniors (grade 12) by the National Center of\nEducation Statistics. The data set at\n[link](http://ritsokiguess.site/datafiles/hsb.csv) is a random sample of\n200 of those students.\n\nThe variables collected are:\n\n-   `gender`: student's gender, female or male.\n\n-   `race`: the student's race (African-American, Asian,[^_hsb-1]\n    Hispanic, White).\n\n-   `ses`: Socio-economic status of student's family (low, middle, or\n    high)\n\n-   `schtyp`: School type, public or private.\n\n-   `prog`: Student's program, general, academic, or vocational.\n\n-   `read`: Score on standardized reading test.\n\n-   `write`: Score on standardized writing test.\n\n-   `math`: Score on standardized math test.\n\n-   `science`: Score on standardized science test.\n\n-   `socst`: Score on standardized social studies test.\n\n[^_hsb-1]: I'm always amused at how Americans put all Asians into one\n    group.\n\nOur aim is to see how socio-economic status is related to the other\nvariables.\n\n(a) Read in and display (some of) the data.\n\nSolution\n\nThis is a `.csv` file (I tried to make it easy for you):\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/hsb.csv\"\nhsb <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 200 Columns: 11\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (5): race, ses, schtyp, prog, gender\ndbl (6): id, read, write, math, science, socst\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nhsb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 x 11\n      id race         ses    schtyp prog   read write  math science socst gender\n   <dbl> <chr>        <chr>  <chr>  <chr> <dbl> <dbl> <dbl>   <dbl> <dbl> <chr> \n 1    70 white        low    public gene~    57    52    41      47    57 male  \n 2   121 white        middle public voca~    68    59    53      63    61 female\n 3    86 white        high   public gene~    44    33    54      58    31 male  \n 4   141 white        high   public voca~    63    44    47      53    56 male  \n 5   172 white        middle public acad~    47    52    57      53    61 male  \n 6   113 white        middle public acad~    44    52    51      63    61 male  \n 7    50 african-amer middle public gene~    50    59    42      53    61 male  \n 8    11 hispanic     middle public acad~    34    46    45      39    36 male  \n 9    84 white        middle public gene~    63    57    54      58    51 male  \n10    48 african-amer middle public acad~    57    55    52      50    51 male  \n# i 190 more rows\n```\n:::\n:::\n\n$\\blacksquare$\n\n(b) Explain briefly why an ordinal logistic regression is appropriate\n    for our aims.\n\nSolution\n\nThe response variable `ses` is categorical, with categories that come in\norder (low less than middle less than high).\n\n$\\blacksquare$\n\n(c) Fit an ordinal logistic regression predicting socio-economic status\n    from the scores on the five standardized tests. (You don't need to\n    display the results.) You will probably go wrong the first time.\n    What kind of thing does your response variable have to be?\n\nSolution\n\nIt has to be an `ordered` factor, which you can create in the data frame\n(or outside, if you prefer):\n\n::: {.cell}\n\n```{.r .cell-code}\nhsb <- hsb %>% mutate(ses = ordered(ses, c(\"low\", \"middle\", \"high\")))\nhsb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 x 11\n      id race         ses    schtyp prog   read write  math science socst gender\n   <dbl> <chr>        <ord>  <chr>  <chr> <dbl> <dbl> <dbl>   <dbl> <dbl> <chr> \n 1    70 white        low    public gene~    57    52    41      47    57 male  \n 2   121 white        middle public voca~    68    59    53      63    61 female\n 3    86 white        high   public gene~    44    33    54      58    31 male  \n 4   141 white        high   public voca~    63    44    47      53    56 male  \n 5   172 white        middle public acad~    47    52    57      53    61 male  \n 6   113 white        middle public acad~    44    52    51      63    61 male  \n 7    50 african-amer middle public gene~    50    59    42      53    61 male  \n 8    11 hispanic     middle public acad~    34    46    45      39    36 male  \n 9    84 white        middle public gene~    63    57    54      58    51 male  \n10    48 african-amer middle public acad~    57    55    52      50    51 male  \n# i 190 more rows\n```\n:::\n:::\n\n`ses` is now `ord`. Good. Now fit the model:\n\n::: {.cell}\n\n```{.r .cell-code}\nses.1 <- polr(ses ~ read + write + math + science + socst, data = hsb)\n```\n:::\n\nNo errors is good.\n\n$\\blacksquare$\n\n(d) Remove any non-significant explanatory variables one at a time. Use\n    `drop1` to decide which one to remove next.\n\nSolution\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(ses.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nses ~ read + write + math + science + socst\n        Df    AIC    LRT Pr(>Chi)   \n<none>     404.63                   \nread     1 403.09 0.4620 0.496684   \nwrite    1 403.81 1.1859 0.276167   \nmath     1 403.19 0.5618 0.453517   \nscience  1 404.89 2.2630 0.132499   \nsocst    1 410.08 7.4484 0.006349 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nI would have expected the AIC column to come out in order, but it\ndoesn't. Never mind. Scan for the largest P-value, which belongs to\n`read`. (This also has the lowest AIC.) So, remove `read`:\n\n::: {.cell}\n\n```{.r .cell-code}\nses.2 <- update(ses.1, . ~ . - read)\ndrop1(ses.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nses ~ write + math + science + socst\n        Df    AIC    LRT Pr(>Chi)   \n<none>     403.09                   \nwrite    1 402.10 1.0124 0.314325   \nmath     1 402.04 0.9541 0.328689   \nscience  1 404.29 3.1968 0.073782 . \nsocst    1 410.58 9.4856 0.002071 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nNote how the P-value for `science` has come down a long way.\n\nA close call, but `math` goes next. The `update` doesn't take long to\ntype:\n\n::: {.cell}\n\n```{.r .cell-code}\nses.3 <- update(ses.2, . ~ . - math)\ndrop1(ses.3, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nses ~ write + science + socst\n        Df    AIC     LRT  Pr(>Chi)    \n<none>     402.04                      \nwrite    1 400.60  0.5587 0.4547813    \nscience  1 405.41  5.3680 0.0205095 *  \nsocst    1 411.07 11.0235 0.0008997 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n`science` has become significant now (probably because it was strongly\ncorrelated with at least one of the variables we removed (at my guess,\n`math`). That is, we didn't need both `science` and `math`, but we *do*\nneed *one* of them.\n\nI think we can guess what will happen now: `write` comes out, and the\nother two variables will stay, so that'll be where we stop:\n\n::: {.cell}\n\n```{.r .cell-code}\nses.4 <- update(ses.3, . ~ . - write)\ndrop1(ses.4, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nses ~ science + socst\n        Df    AIC     LRT  Pr(>Chi)    \n<none>     400.60                      \nscience  1 403.45  4.8511 0.0276291 *  \nsocst    1 409.74 11.1412 0.0008443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nIndeed so. We need just the science and social studies test scores to\npredict socio-economic status.\n\nUsing AIC to decide on which variable to remove next will give the same\nanswer here, but I would like to see the `test=` part in your `drop1` to\ngive P-values (expect to lose something, but not too much, if that's not\nthere).\n\nExtras: I talked about correlation among the explanatory variables\nearlier, which I can explore:\n\n::: {.cell}\n\n```{.r .cell-code}\nhsb %>% select(read:socst) %>% cor()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             read     write      math   science     socst\nread    1.0000000 0.5967765 0.6622801 0.6301579 0.6214843\nwrite   0.5967765 1.0000000 0.6174493 0.5704416 0.6047932\nmath    0.6622801 0.6174493 1.0000000 0.6307332 0.5444803\nscience 0.6301579 0.5704416 0.6307332 1.0000000 0.4651060\nsocst   0.6214843 0.6047932 0.5444803 0.4651060 1.0000000\n```\n:::\n:::\n\nThe first time I did this, I forgot that I had `MASS` loaded (for the\n`polr`), and so, to get the right `select`, I needed to say which one I\nwanted.\n\nAnyway, the correlations are all moderately high. There's nothing that\nstands out as being much higher than the others. The lowest two are\nbetween social studies and math, and social studies and science. That\nwould be part of the reason that social studies needs to stay. The\nhighest correlation is between math and reading, which surprises me\n(they seem to be different skills).\n\nSo there was not as much insight there as I expected.\n\nThe other thing is that you can use `step` for the variable-elimination\ntask as well:\n\n::: {.cell}\n\n```{.r .cell-code}\nses.5 <- step(ses.1, direction = \"backward\", test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=404.63\nses ~ read + write + math + science + socst\n\n          Df    AIC    LRT Pr(>Chi)   \n- read     1 403.09 0.4620 0.496684   \n- math     1 403.19 0.5618 0.453517   \n- write    1 403.81 1.1859 0.276167   \n<none>       404.63                   \n- science  1 404.89 2.2630 0.132499   \n- socst    1 410.08 7.4484 0.006349 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=403.09\nses ~ write + math + science + socst\n\n          Df    AIC    LRT Pr(>Chi)   \n- math     1 402.04 0.9541 0.328689   \n- write    1 402.10 1.0124 0.314325   \n<none>       403.09                   \n- science  1 404.29 3.1968 0.073782 . \n- socst    1 410.58 9.4856 0.002071 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=402.04\nses ~ write + science + socst\n\n          Df    AIC     LRT  Pr(>Chi)    \n- write    1 400.60  0.5587 0.4547813    \n<none>       402.04                      \n- science  1 405.41  5.3680 0.0205095 *  \n- socst    1 411.07 11.0235 0.0008997 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=400.6\nses ~ science + socst\n\n          Df    AIC     LRT  Pr(>Chi)    \n<none>       400.60                      \n- science  1 403.45  4.8511 0.0276291 *  \n- socst    1 409.74 11.1412 0.0008443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nI would accept you doing it this way, again *as long as you have the\n`test=` there as well*.\n\n$\\blacksquare$\n\n(e) The quartiles of the `science` test score are 44 and 58. The\n    quartiles of the `socst` test score are 46 and 61. Make a data frame\n    that has all combinations of those quartiles. If your best\n    regression had any other explanatory variables in it, also put the\n    *means* of those variables into this data frame.\n\nSolution\n\nThis is what `datagrid` does by default (from package\n`marginaleffects`):\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = ses.5, science = c(44, 58), socst = c(46, 61))\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     ses science socst\n1 middle      44    46\n2 middle      44    61\n3 middle      58    46\n4 middle      58    61\n```\n:::\n:::\n\nThis explicitly fills in mean values or most frequent categories for all\nthe other variables in the dataset, even though those other variables\nare not in the model. The two variables you actually care about are over\non the right.\n\nSince there are only two variables left, this `new` data frame has only\n$2^2=4$ rows.\n\nThere is a veiled hint here that these are the two variables that should\nhave remained in your regression. If that was not what you got, the\nmeans of the other variables in the model will go automatically into\nyour `new`:\n\n::: {.cell}\n\n```{.r .cell-code}\ndatagrid(model = ses.1, science = c(44, 58), socst = c(46, 61))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     ses  read  write   math science socst\n1 middle 52.23 52.775 52.645      44    46\n2 middle 52.23 52.775 52.645      44    61\n3 middle 52.23 52.775 52.645      58    46\n4 middle 52.23 52.775 52.645      58    61\n```\n:::\n:::\n\nso you don't have to do anything extra.\n\n$\\blacksquare$\n\n(f) Use the data frame you created in the previous part, together with\n    your best model, to obtain predicted probabilities of being in each\n    `ses` category. Display these predicted probabilities so that they\n    are easy to read.\n\nSolution\n\nThis is `predictions`, and we've done the setup. My best model was\ncalled `ses.4`.\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(ses.4, newdata = new)) %>% \n  select(group, estimate, science, socst) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nRe-fitting to get Hessian\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n    group  estimate science socst\n1     low 0.3262510      44    46\n2     low 0.1885566      44    61\n3     low 0.2278105      58    46\n4     low 0.1240155      58    61\n5  middle 0.5056113      44    46\n6  middle 0.5150763      44    61\n7  middle 0.5230783      58    46\n8  middle 0.4672338      58    61\n9    high 0.1681377      44    46\n10   high 0.2963671      44    61\n11   high 0.2491112      58    46\n12   high 0.4087507      58    61\n```\n:::\n:::\n\n`predictions` always works by having *one* column of predictions. That\nisn't the best layout here, though; we want to see the three predicted\nprobabilities for a particular value of `science` and `socst` all in one\nrow, which means pivoting-wider:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(ses.4, newdata = new)) %>% \n  select(group, estimate, science, socst) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nRe-fitting to get Hessian\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 5\n  science socst   low middle  high\n    <dbl> <dbl> <dbl>  <dbl> <dbl>\n1      44    46 0.326  0.506 0.168\n2      44    61 0.189  0.515 0.296\n3      58    46 0.228  0.523 0.249\n4      58    61 0.124  0.467 0.409\n```\n:::\n:::\n\nThe easiest strategy seems to be to run `predictions` first, see that it\ncomes out long, and then wonder how to fix it. Then pick the columns you\ncare about: the predicted `group`, the predictions, and the columns for\nscience and social science, and then pivot wider.\n\n$\\blacksquare$\n\n(g) What is the effect of an increased science score on the likelihood\n    of a student being in the different socioeconomic groups, all else\n    equal? Explain briefly. In your explanation, state clearly how you\n    are using your answer to the previous part.\n\nSolution\n\nUse your predictions; hold the `socst` score constant (that's the all\nelse equal part). So compare the first and third rows (or, if you like,\nthe second and fourth rows) of your predictions and see what happens as\nthe science score goes from 44 to 58. What I see is that the probability\nof being `low` goes noticeably *down* as the science score increases,\nthe probability of `middle` stays about the same, and the probability of\n`high` goes `up` (by about the same amount as the probability of `low`\nwent down). In other words, an increased science score goes with an\nincreased chance of `high` (and a decreased chance of `low`).\n\nIf your best model doesn't have `science` in it, then you need to say\nsomething like \"`science` has no effect on socio-economic status\",\nconsistent with what you concluded before: if you took it out, it's\nbecause you thought it had no effect.\n\nExtra: the effect of an increased social studies score is almost exactly\nthe same as an increased science score (so I didn't ask you about that).\nFrom a social-science point of view, this makes perfect sense: the\nhigher the social-economic stratum a student comes from, the better they\nare likely to do in school. I've been phrasing this as \"association\",\nbecause really the cause and effect is the other way around: a student's\nfamily socioeconomic status is explanatory, and school performance is\nresponse. But this was the nicest example I could find of an ordinal\nresponse data set.\n\n$\\blacksquare$\n\n\n##  How do you like your steak?\n\n\n When you order a steak in a restaurant, the server will ask\nyou how you would like it cooked, or to be precise, *how much*\nyou would like it cooked: rare (hardly cooked at all), through medium\nrare, medium, medium well to well (which means \"well done\", so that\nthe meat has only a little red to it). Could you guess how a person\nlikes their steak cooked, from some other information about them? The\nwebsite [link](fivethirtyeight.com) commissioned a survey where they\nasked a number of people how they preferred their steak, along with as\nmany other things as they could think of to ask. (Many of the\nvariables below are related to risk-taking, which was something the\npeople designing the survey thought might have something to do with\nliking steak rare.) The variables of interest are all factors or true/false:\n\n\n\n* `respondent_ID`: a ten-digit number identifying each\nperson who responded to the survey.\n\n* `lottery_a`: true if the respondent preferred lottery A\nwith a small chance to win a lot of money, to lottery B, with a\nlarger chance to win less money.\n\n* `smoke`: true if the respondent is currently a smoker\n\n* `alcohol`: true if the respondent at least occasionally\ndrinks alcohol.\n\n* `gamble`: true if the respondent likes to gamble (eg.\nbetting on horse racing or playing the lottery)\n\n* `skydiving`: true if the respondent has ever been\nskydiving.\n\n* `speed`: true if the respondent likes to drive fast\n\n* `cheated`: true if the respondent has ever cheated on a\nspouse or girlfriend/boyfriend\n\n* `steak`: true if the respondent likes to eat steak\n\n* `steak_prep` (response): how the respondent likes their\nsteak cooked (factor, as described above, with 5 levels).\n\n* `female`: true if the respondent is female\n\n* `age`: age group, from 18--29 to 60+.\n\n* `hhold_income`: household income group, from \\$0--24,999\nto \\$150,000+.\n\n* `educ`: highest level of education attained, from \n\"less than high school\" up to \"graduate degree\"\n\n* `region`: region (of the US)\nthat the respondent lives in (five values).\n\n\nThe data are in\n[link](http://ritsokiguess.site/datafiles/steak1.csv). This is the\ncleaned-up data from a previous question, with the missing values removed.\n\n\n\n(a) Read in the data and display the first few lines.\n\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak <- read_csv(\"steak1.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 331 Columns: 15\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (5): steak_prep, age, hhold_income, educ, region\ndbl (1): respondent_id\nlgl (9): lottery_a, smoke, alcohol, gamble, skydiving, speed, cheated, steak...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nsteak\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 331 x 15\n   respondent_id lottery_a smoke alcohol gamble skydiving speed cheated steak\n           <dbl> <lgl>     <lgl> <lgl>   <lgl>  <lgl>     <lgl> <lgl>   <lgl>\n 1    3234982343 TRUE      FALSE TRUE    FALSE  FALSE     FALSE FALSE   TRUE \n 2    3234973379 TRUE      FALSE TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 3    3234972383 FALSE     TRUE  TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 4    3234958833 FALSE     FALSE TRUE    FALSE  FALSE     TRUE  TRUE    TRUE \n 5    3234955240 TRUE      FALSE FALSE   FALSE  FALSE     TRUE  FALSE   TRUE \n 6    3234955010 TRUE      FALSE TRUE    TRUE   TRUE      TRUE  FALSE   TRUE \n 7    3234953052 TRUE      TRUE  TRUE    TRUE   FALSE     TRUE  FALSE   TRUE \n 8    3234951249 FALSE     FALSE TRUE    TRUE   FALSE     FALSE FALSE   TRUE \n 9    3234948883 FALSE     FALSE TRUE    FALSE  FALSE     TRUE  FALSE   TRUE \n10    3234948197 TRUE      FALSE FALSE   TRUE   FALSE     TRUE  FALSE   TRUE \n# i 321 more rows\n# i 6 more variables: steak_prep <chr>, female <lgl>, age <chr>,\n#   hhold_income <chr>, educ <chr>, region <chr>\n```\n:::\n:::\n\n\n \n$\\blacksquare$\n\n\n(b) We are going to predict `steak_prep` from some of\nthe other variables. Why is the model-fitting function `polr`\nfrom package `MASS` the best choice for these data\n(alternatives being `glm` and `multinom` from package\n`nnet`)?\n\n\nSolution\n\n\nIt all depends on the kind of response variable. We have a\nresponse variable with five ordered levels from Rare to\nWell. There are more than two levels (it is more than a\n\"success\" and \"failure\"), which rules out `glm`, and\nthe levels are ordered, which rules out `multinom`. As we\nknow, `polr` handles an ordered response, so it is the\nright choice.\n    \n$\\blacksquare$\n\n(c) What are the levels of `steak_prep`, \n*in the  order that R thinks they are in?* If they are not in a sensible\norder, create an ordered factor where the levels are in a sensible order.\n\n\nSolution\n\n\nThis is the most direct way to find out:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak %>% distinct(steak_prep) %>% pull(steak_prep) -> preps\npreps\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Medium rare\" \"Rare\"        \"Medium\"      \"Medium Well\" \"Well\"       \n```\n:::\n:::\n\n     \n\nThis is almost the right order (`distinct` uses the order in\nthe data frame). We just need to switch the first two around, and then\nwe'll be done:\n\n::: {.cell}\n\n```{.r .cell-code}\npreps1 <- preps[c(2, 1, 3, 4, 5)]\npreps1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Rare\"        \"Medium rare\" \"Medium\"      \"Medium Well\" \"Well\"       \n```\n:::\n:::\n\n \n\nIf you used `count`, there's a bit more work to do:\n\n::: {.cell}\n\n```{.r .cell-code}\npreps2 <- steak %>% count(steak_prep) %>% pull(steak_prep)\npreps2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Medium\"      \"Medium Well\" \"Medium rare\" \"Rare\"        \"Well\"       \n```\n:::\n:::\n\n \n\nbecause `count` puts them in alphabetical order, so:\n\n::: {.cell}\n\n```{.r .cell-code}\npreps3 <- preps2[c(4, 2, 1, 3, 5)]\npreps3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Rare\"        \"Medium Well\" \"Medium\"      \"Medium rare\" \"Well\"       \n```\n:::\n:::\n\n \n\nThese use the idea in the\nattitudes-to-abortion question: create a vector of the levels in the\n*right* order, then create an ordered factor with\n`ordered()`. If you like, you can type the levels in the right\norder (I won't penalize you for that here), but it's really better to\nget the levels without typing or copying-pasting, so that you don't\nmake any silly errors copying them (which will mess everything up\nlater).\n\nSo now I create my ordered response:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak <- steak %>% mutate(steak_prep_ord = ordered(steak_prep, preps1))\n```\n:::\n\n \nor using one of the other `preps` vectors containing the levels\nin the correct order.\nAs far as `polr` is concerned,\nit doesn't matter whether I start at `Rare` and go \"up\", or\nstart at `Well` and go \"down\". So if you do it the other way\naround, that's fine. As long as you get the levels in a sensible\norder, you're good.\n    \n$\\blacksquare$\n\n(d) Fit a model predicting preferred steak preparation in an\nordinal logistic regression from `educ`, `female` and\n`lottery_a`. This ought to be easy from your previous work,\nbut you have to be careful about one thing. No need to print out the\nresults. \n\n\nSolution\n\n\nThe thing you have to be careful about is that you use the\n*ordered* factor that you just created as the response:\n\n::: {.cell hash='ordinal-response_cache/pdf/finetti_4075216a7ad24efc30c83d955b22b8a1'}\n\n```{.r .cell-code}\nsteak.1 <- polr(steak_prep_ord ~ educ + female + lottery_a, data = steak)\n```\n:::\n\n     \n    \n$\\blacksquare$\n\n(e) Run `drop1` on your fitted model, with\n`test=\"Chisq\"`. Which explanatory variable should be removed\nfirst, if any? Bear in mind that the variable with the\n*smallest* AIC should come out first, in case your table\ndoesn't get printed in order.\n\n\nSolution\n\n\nThis:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(steak.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nsteak_prep_ord ~ educ + female + lottery_a\n          Df    AIC    LRT Pr(>Chi)  \n<none>       910.69                  \neduc       4 912.10 9.4107  0.05162 .\nfemale     1 908.70 0.0108  0.91715  \nlottery_a  1 909.93 1.2425  0.26498  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nMy table is indeed out of order (which is why I warned you about it,\nin case that happens to you as well). The smallest AIC goes with\n`female`, which also has a very non-significant P-value, so\nthis one should come out first.\n    \n$\\blacksquare$\n\n(f) Remove the variable that should come out first, using\n`update`. (If all the variables should stay, you can skip\nthis part.)\n\n\nSolution\n\n\nYou could type or copy-paste the whole model again, but\n`update` is quicker:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.2 <- update(steak.1, . ~ . - female)\n```\n:::\n\n     \n\nThat's all.\n\nI wanted to get some support for my `drop1` above (since I was\na bit worried about those out-of-order rows). Now that we have fitted\na model with `female` and one without, we can compare them\nusing `anova`:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(steak.2, steak.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of ordinal regression models\n\nResponse: steak_prep_ord\n                      Model Resid. df Resid. Dev   Test    Df  LR stat.\n1          educ + lottery_a       322   890.7028                       \n2 educ + female + lottery_a       321   890.6920 1 vs 2     1 0.0108221\n    Pr(Chi)\n1          \n2 0.9171461\n```\n:::\n:::\n\n \n\nDon't get taken in by that \"LR stat\" that may be on the end of the first row of\nthe output table; the P-value might have wrapped onto the second line, and is in\nfact exactly the same as in the `drop1` output (it is doing\nexactly the same test). As non-significant as you could wish for.\n\nExtra: I was curious about whether either of the other $x$'s could come out now:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(steak.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nsteak_prep_ord ~ educ + lottery_a\n          Df    AIC    LRT Pr(>Chi)  \n<none>       908.70                  \neduc       4 910.13 9.4299  0.05121 .\nlottery_a  1 907.96 1.2599  0.26167  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\n`lottery_a` should come out, but `educ` is edging\ntowards significance. We are about to do predictions; in those, the above suggests that there may be some visible effect of education, but there may not be much effect of `lottery_a`.\n\nAll right, so what happens when we remove `lottery_a`? That we find out later.\n\n\n$\\blacksquare$\n\n(g) Using the best model that you have so far, predict the\nprobabilities of preferring each different steak preparation (method\nof cooking) for each combination of the variables that\nremain. (Some of the variables are TRUE and FALSE rather than\nfactors. Bear this in mind.)\nDescribe the effects of each variable on the predicted\nprobabilities, if any. Note that there is exactly one person in the\nstudy whose educational level is \"less than high school\".\n\n\nSolution\n\n\nAgain, I'm leaving it to you to follow all the steps. My variables\nremaining are `educ` and `lottery_a`, which are respectively categorical and logical.\n\nThe first step is to get all combinations of their values, along with \"typical\" values for the others:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = steak.2, \n                educ = levels(factor(steak$educ)),\n                lottery_a = c(FALSE, TRUE))\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   steak_prep_ord                             educ lottery_a\n1     Medium rare                  Bachelor degree     FALSE\n2     Medium rare                  Bachelor degree      TRUE\n3     Medium rare                  Graduate degree     FALSE\n4     Medium rare                  Graduate degree      TRUE\n5     Medium rare               High school degree     FALSE\n6     Medium rare               High school degree      TRUE\n7     Medium rare     Less than high school degree     FALSE\n8     Medium rare     Less than high school degree      TRUE\n9     Medium rare Some college or Associate degree     FALSE\n10    Medium rare Some college or Associate degree      TRUE\n```\n:::\n:::\n\nI wasn't sure how to handle the logical `lottery_a`, so I just typed the `TRUE` and `FALSE`.\n\nOn to the predictions, remembering to make them wider:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(steak.2, newdata = new)) %>% \n  select(rowid, group, estimate, educ, lottery_a) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nRe-fitting to get Hessian\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 8\n   rowid educ       lottery_a    Rare `Medium rare`  Medium `Medium Well`   Well\n   <dbl> <chr>      <lgl>       <dbl>         <dbl>   <dbl>         <dbl>  <dbl>\n 1     1 Bachelor ~ FALSE     5.19e-2   0.383       3.35e-1     0.172     0.0581\n 2     2 Bachelor ~ TRUE      4.18e-2   0.339       3.47e-1     0.201     0.0719\n 3     3 Graduate ~ FALSE     7.95e-2   0.469       2.92e-1     0.122     0.0376\n 4     4 Graduate ~ TRUE      6.44e-2   0.428       3.16e-1     0.145     0.0468\n 5     5 High scho~ FALSE     5.28e-2   0.387       3.33e-1     0.170     0.0571\n 6     6 High scho~ TRUE      4.25e-2   0.342       3.46e-1     0.198     0.0707\n 7     7 Less than~ FALSE     8.51e-8   0.00000111  4.00e-6     0.0000200 1.00  \n 8     8 Less than~ TRUE      6.78e-8   0.000000886 3.19e-6     0.0000159 1.00  \n 9     9 Some coll~ FALSE     5.50e-2   0.395       3.30e-1     0.165     0.0549\n10    10 Some coll~ TRUE      4.43e-2   0.351       3.44e-1     0.193     0.0679\n```\n:::\n:::\n\nThere are 5 levels of education, 2 levels of `lottery_a`, and 5 ways in which you might ask for your steak to be cooked, so the original output from `predictions` has $5 \\times 2 \\times 5 = 50$ rows, and the output you see above has $5 \\times 2 = 10$ rows.\n\n\nI find this hard to read, so I'm going to round off those\npredictions. Three or four decimals seems to be sensible. The time to do this is while they are all in one column, that is, before the `pivot_wider`. On my screen, the education levels also came out rather long, so I'm going to shorten them as well:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(steak.2, newdata = new)) %>% \n  select(rowid, group, estimate, educ, lottery_a) %>% \n  mutate(estimate = round(estimate, 3),\n         educ = abbreviate(educ, 15)) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nRe-fitting to get Hessian\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 8\n   rowid educ           lottery_a  Rare `Medium rare` Medium `Medium Well`  Well\n   <dbl> <chr>          <lgl>     <dbl>         <dbl>  <dbl>         <dbl> <dbl>\n 1     1 Bachelor degr~ FALSE     0.052         0.383  0.335         0.172 0.058\n 2     2 Bachelor degr~ TRUE      0.042         0.339  0.347         0.201 0.072\n 3     3 Graduate degr~ FALSE     0.08          0.469  0.292         0.122 0.038\n 4     4 Graduate degr~ TRUE      0.064         0.428  0.316         0.145 0.047\n 5     5 Highschooldeg~ FALSE     0.053         0.387  0.333         0.17  0.057\n 6     6 Highschooldeg~ TRUE      0.043         0.342  0.346         0.198 0.071\n 7     7 Lssthnhghschl~ FALSE     0             0      0             0     1    \n 8     8 Lssthnhghschl~ TRUE      0             0      0             0     1    \n 9     9 SmcllgorAssct~ FALSE     0.055         0.395  0.33          0.165 0.055\n10    10 SmcllgorAssct~ TRUE      0.044         0.351  0.344         0.193 0.068\n```\n:::\n:::\n\nThat's about as much as I can shorten the education levels while still having them readable.\n\n\nThen, say something about the effect of changing educational level on the\npredictions, and say something about the effect of favouring Lottery A\nvs.\\ not. I don't much mind what: you can say that there is not much\neffect (of either variable), or you can say something like \"people with a graduate degree are slightly more likely to like their steak rare and less likely to like it well done\" (for education level) and\n\"people who preferred Lottery A are slightly less likely to like their steak rare and slightly more likely to like it well done\" (for\neffect of Lottery A). You can see these by comparing the odd-numbered rows\nrows with each other to assess the effect of education while holding attitudes towards `lottery_a` constant (or the even-numbered rows, if you\nprefer), and you can compare eg. rows 1 and 2 to assess the effect of\nLottery A (compare two lines with the *same* educational level\nbut *different* preferences re Lottery A). \n\nI would keep away from saying anything about education level \n\"less than high school\", since this entire level is represented by exactly\none person.\n    \n$\\blacksquare$\n\n(h) Is it reasonable to remove *all* the remaining\nexplanatory variables from your best model so far? Fit a model with no explanatory variables,\nand do a test. (In R, if the right side of the squiggle is a\n`1`, that means \"just an intercept\". Or you can remove\nwhatever remains using `update`.) What do you conclude?\nExplain briefly.\n\n\nSolution\n\n\nThe fitting part is the challenge, since the testing part is\n`anova` again. The direct fit is this:\n\n::: {.cell hash='ordinal-response_cache/pdf/fiorentina_3e0e9cdfaec8a93da4cac69e6ddcb6d8'}\n\n```{.r .cell-code}\nsteak.3 <- polr(steak_prep_ord ~ 1, data = steak)\n```\n:::\n\n     \n\nand the `update` version is this, about equally long, starting\nfrom `steak.2` since that is the best model so far:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.3a <- update(steak.2, . ~ . - educ - lottery_a)\n```\n:::\n\n \n\nYou can use whichever you like. Either way, the second part is\n`anova`, and the two possible answers should be the same:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(steak.3, steak.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of ordinal regression models\n\nResponse: steak_prep_ord\n             Model Resid. df Resid. Dev   Test    Df LR stat.    Pr(Chi)\n1                1       327   901.4467                                 \n2 educ + lottery_a       322   890.7028 1 vs 2     5 10.74387 0.05670146\n```\n:::\n:::\n\n \n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(steak.3a, steak.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of ordinal regression models\n\nResponse: steak_prep_ord\n             Model Resid. df Resid. Dev   Test    Df LR stat.    Pr(Chi)\n1                1       327   901.4467                                 \n2 educ + lottery_a       322   890.7028 1 vs 2     5 10.74387 0.05670146\n```\n:::\n:::\n\n \n\nAt the 0.05 level, removing both of the remaining variables is fine:\nthat is, nothing (out of these variables) has any impact on the\nprobability that a diner will prefer their steak cooked a particular\nway. However, it is a very close call; the P-value is only *just* bigger than 0.05.\n\nHowever, with data like this and a rather exploratory analysis, I\nmight think about using a larger $\\alpha$ like 0.10, and at this\nlevel, taking out both these two variables is a bad idea. You could\nsay that one or both of them is \"potentially useful\" or\n\"provocative\" or something like that.\n\nIf you think that removing these two variables is questionable, you\nmight like to go back to that `drop1` output I had above:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(steak.2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nsteak_prep_ord ~ educ + lottery_a\n          Df    AIC    LRT Pr(>Chi)  \n<none>       908.70                  \neduc       4 910.13 9.4299  0.05121 .\nlottery_a  1 907.96 1.2599  0.26167  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nThe smallest AIC goes with `lottery_a`, so that comes out (it\nis nowhere near significant):\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.4 <- update(steak.2, . ~ . - lottery_a)\ndrop1(steak.4, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nsteak_prep_ord ~ educ\n       Df    AIC   LRT Pr(>Chi)  \n<none>    907.96                 \neduc    4 909.45 9.484  0.05008 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nand what you see is that educational level is right on the edge of\nsignificance, so that may or may not have any impact. Make a call. But\nif anything, it's educational level that makes a difference.\n    \n$\\blacksquare$\n\n(i) In the article for which these data were collected,\n[link](https://fivethirtyeight.com/datalab/how-americans-like-their-steak/),\ndoes the author obtain consistent conclusions with yours? Explain\nbriefly. (It's not a very long article, so it won't take you long to\nskim through, and the author's point is pretty clear.)\n\n\n\nSolution\n\n\nThe article says that *nothing* has anything to do with steak\npreference. Whether you agree or not depends on what you thought\nabove about dropping those last two variables. So say something\nconsistent with what you said in the previous part. Two points for\nsaying that the author said \"nothing has any effect\", and one\npoint for how your findings square with that.\n\n\nExtra: now that you have worked through this great long question, this is\nwhere I tell you that I simplified things a fair bit for you! There\nwere lots of other variables that might have had an impact on how\npeople like their steaks, and we didn't even consider those. Why did\nI choose what I did here? Well, I wanted to fit a regression predicting\nsteak preference from everything else, do a big backward\nelimination, but:\n\n::: {.cell hash='ordinal-response_cache/pdf/udinese_8eec7b29a204ffd8410211a782c4c0dd'}\n\n```{.r .cell-code}\nsteak.5 <- polr(steak_prep_ord ~ ., data = steak)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: algorithm did not converge\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in polr(steak_prep_ord ~ ., data = steak): attempt to find suitable starting values failed\n```\n:::\n:::\n\n   \nThe `.` in place of explanatory\nvariables means \"all the other variables\", including the nonsensical\npersonal ID. That saved me having to type them all out. \n\nUnfortunately, however, it didn't work. The problem is a numerical\none. Regular regression has a well-defined procedure, where the computer follows\nthrough the steps and gets to the answer, every time. Once you go\nbeyond regression, however, the answer is obtained by a step-by-step\nmethod: the computer makes an initial guess, tries to improve it, then\ntries to improve it again, until it can't improve things any more, at\nwhich point it calls it good. The problem here is that `polr`\ncannot even get the initial guess! (It apparently is known to suck at\nthis, in problems as big and complicated as this one.)\n\nI don't normally recommend forward selection, but I wonder whether it\nworks here:\n\n::: {.cell hash='ordinal-response_cache/pdf/juventus_f62fff44781702a740d9d0e9fb2aa1cc'}\n\n```{.r .cell-code}\nsteak.5 <- polr(steak_prep_ord ~ 1, data = steak)\nsteak.6 <- step(steak.5,\n  scope = . ~ lottery_a + smoke + alcohol + gamble + skydiving +\n    speed + cheated + female + age + hhold_income + educ + region,\n  direction = \"forward\", test = \"Chisq\", trace = 0\n)\ndrop1(steak.6, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\nsteak_prep_ord ~ educ\n       Df    AIC   LRT Pr(>Chi)  \n<none>    907.96                 \neduc    4 909.45 9.484  0.05008 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n \n\nIt does, and it says the *only* thing to add out of all the\nvariables is education level. So, for you, I picked this along with a\ncouple of other plausible-sounding variables and had you start from there.\n\nForward selection starts from a model containing nothing and asks\n\"what can we add?\". This is a bit more complicated than backward\nelimination, because now you have to say what the candidate things to\nadd *are*. That's the purpose of that `scope` piece, and\nthere I had no alternative but to type the names of all the\nvariables. Backward elimination is easier, because the candidate\nvariables to remove are the ones in the model, and you don't need a\n`scope`. The `trace=0` says \"don't give me any output\"\n(you can change it to a different value if you want to see what that\ndoes), and last, the `drop1` looks at what is actually\n*in* the final model (with a view to asking what can be removed,\nbut we don't care about *that* here). \n  \n\n$\\blacksquare$\n\n\n\n##  How do you like your steak -- the data\n\n\nThis question takes you through the data preparation for one\nof the other questions. You don't have to do *this*\nquestion, but you may find it interesting or useful.\n\nWhen you order a steak in a restaurant, the server will ask\nyou how you would like it cooked, or to be precise, *how much*\nyou would like it cooked: rare (hardly cooked at all), through medium\nrare, medium, medium well to well (which means \"well done\", so that\nthe meat has only a little red to it). Could you guess how a person\nlikes their steak cooked, from some other information about them? The\nwebsite [link](fivethirtyeight.com) commissioned a survey where they\nasked a number of people how they preferred their steak, along with as\nmany other things as they could think of to ask. (Many of the\nvariables below are related to risk-taking, which was something the\npeople designing the survey thought might have something to do with\nliking steak rare.) The variables of interest are all factors or true/false:\n\n\n\n* `respondent_ID`: a ten-digit number identifying each\nperson who responded to the survey.\n\n* `lottery_a`: true if the respondent preferred lottery A\nwith a small chance to win a lot of money, to lottery B, with a\nlarger chance to win less money.\n\n* `smoke`: true if the respondent is currently a smoker\n\n* `alcohol`: true if the respondent at least occasionally\ndrinks alcohol.\n\n* `gamble`: true if the respondent likes to gamble (eg.\nbetting on horse racing or playing the lottery)\n\n* `skydiving`: true if the respondent has ever been\nskydiving.\n\n* `speed`: true if the respondent likes to drive fast\n\n* `cheated`: true if the respondent has ever cheated on a\nspouse or girlfriend/boyfriend\n\n* `steak`: true if the respondent likes to eat steak\n\n* `steak_prep` (response): how the respondent likes their\nsteak cooked (factor, as described above, with 5 levels).\n\n* `female`: true if the respondent is female\n\n* `age`: age group, from 18--29 to 60+.\n\n* `hhold_income`: household income group, from \\$0--24,999\nto \\$150,000+.\n\n* `educ`: highest level of education attained, from \n\"less  than high school\" \nup to \"graduate degree\"\n\n* `region`: region (of the US)\nthat the respondent lives in (five values).\n\n\nThe data are in\n[link](http://ritsokiguess.site/datafiles/steak.csv). \n\n\n\n(a) Read in the data and display the first few lines.\n\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/steak.csv\"\nsteak0 <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 550 Columns: 15\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (5): steak_prep, age, hhold_income, educ, region\ndbl (1): respondent_id\nlgl (9): lottery_a, smoke, alcohol, gamble, skydiving, speed, cheated, steak...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nsteak0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 550 x 15\n   respondent_id lottery_a smoke alcohol gamble skydiving speed cheated steak\n           <dbl> <lgl>     <lgl> <lgl>   <lgl>  <lgl>     <lgl> <lgl>   <lgl>\n 1    3237565956 FALSE     NA    NA      NA     NA        NA    NA      NA   \n 2    3234982343 TRUE      FALSE TRUE    FALSE  FALSE     FALSE FALSE   TRUE \n 3    3234973379 TRUE      FALSE TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 4    3234972383 FALSE     TRUE  TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 5    3234958833 FALSE     FALSE TRUE    FALSE  FALSE     TRUE  TRUE    TRUE \n 6    3234955240 TRUE      FALSE FALSE   FALSE  FALSE     TRUE  FALSE   TRUE \n 7    3234955097 TRUE      FALSE TRUE    FALSE  FALSE     TRUE  TRUE    FALSE\n 8    3234955010 TRUE      FALSE TRUE    TRUE   TRUE      TRUE  FALSE   TRUE \n 9    3234953052 TRUE      TRUE  TRUE    TRUE   FALSE     TRUE  FALSE   TRUE \n10    3234951249 FALSE     FALSE TRUE    TRUE   FALSE     FALSE FALSE   TRUE \n# i 540 more rows\n# i 6 more variables: steak_prep <chr>, female <lgl>, age <chr>,\n#   hhold_income <chr>, educ <chr>, region <chr>\n```\n:::\n:::\n\n \n\nI'm using a temporary name for reasons that will become clear shortly.\n\n\n$\\blacksquare$\n\n(b) What do you immediately notice about your data frame? Run `summary` on the entire data frame. Would you say you have a lot of missing values, or only a few?\n\nSolution\n\n\nI see missing values, starting in the very first row.\nRunning the data frame through `summary` gives this, either as `summary(steak0)` or this way:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:279       FALSE:453       FALSE:125      \n Median :3.235e+09   TRUE :267       TRUE :84        TRUE :416      \n Mean   :3.235e+09   NA's :4         NA's :13        NA's :9        \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.238e+09                                                  \n   gamble        skydiving         speed          cheated       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:280       FALSE:502       FALSE:59        FALSE:447      \n TRUE :257       TRUE :36        TRUE :480       TRUE :92       \n NA's :13        NA's :12        NA's :11        NA's :11       \n                                                                \n                                                                \n   steak          steak_prep          female            age           \n Mode :logical   Length:550         Mode :logical   Length:550        \n FALSE:109       Class :character   FALSE:246       Class :character  \n TRUE :430       Mode  :character   TRUE :268       Mode  :character  \n NA's :11                           NA's :36                          \n                                                                      \n                                                                      \n hhold_income           educ              region         \n Length:550         Length:550         Length:550        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n```\n:::\n:::\n\n     \n\nMake a call about whether you think that's a lot of missing values or only a few. This might not be all of them, because missing text doesn't show here (we see later how to make it show up).\n\n$\\blacksquare$\n\n(c) What does the function `drop_na` do when applied to a data frame with missing values? To find out, pass the data frame into `drop_na()`, then into `summary` again. What has happened?\n\nSolution\n\n\nLet's try it and see.\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% drop_na() %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:171       FALSE:274       FALSE:65       \n Median :3.235e+09   TRUE :160       TRUE :57        TRUE :266      \n Mean   :3.235e+09                                                  \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.235e+09                                                  \n   gamble        skydiving         speed          cheated         steak        \n Mode :logical   Mode :logical   Mode :logical   Mode :logical   Mode:logical  \n FALSE:158       FALSE:308       FALSE:28        FALSE:274       TRUE:331      \n TRUE :173       TRUE :23        TRUE :303       TRUE :57                      \n                                                                               \n                                                                               \n                                                                               \n  steak_prep          female            age            hhold_income      \n Length:331         Mode :logical   Length:331         Length:331        \n Class :character   FALSE:174       Class :character   Class :character  \n Mode  :character   TRUE :157       Mode  :character   Mode  :character  \n                                                                         \n                                                                         \n                                                                         \n     educ              region         \n Length:331         Length:331        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n```\n:::\n:::\n\n \n\nThe missing values, the ones we can see anyway, have all gone. Precisely, `drop_na`, as its\nname suggests, drops all the rows that have missing values in them\nanywhere. This is potentially wasteful, since a row might be missing\nonly one value, and we drop the entire rest of the row, throwing away\nthe good data as well. If you check, we started with 550 rows, and we\nnow have only 311 left. Ouch.\n\nSo now we'll save this into our \"good\" data frame, which means doing it again (now that we know it works):\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% drop_na() -> steak\n```\n:::\n\n \n\nExtra: another way to handle missing data is called \"imputation\":\nwhat you do is to *estimate* a value for any missing data, and\nthen use that later on as if it were the truth. One way of estimating\nmissing values is to do a regression (of appropriate kind: regular or\nlogistic) to predict a column with missing values from all the other\ncolumns.\n\nExtra extra: below we see how we used to have to do this, for your information.\n\nFirst, we run `complete.cases` on the data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\ncomplete.cases(steak0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [37]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n [61] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n [73] FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [85] FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n [97]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n[109] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n[133] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[145]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[157]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n[169]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n[181] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n[205]  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[217] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[253] FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n[265] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n[277]  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[289]  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n[301] FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[313]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[325] FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n[337]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[349] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n[361] FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n[373] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n[385]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[397]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[409]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE\n[421]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n[433]  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n[445]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n[457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n[469]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n[481] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n[493] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE\n[505]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n[517]  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n[529] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n[541] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n```\n:::\n:::\n\n \n\nYou might be able to guess what this does, in the light of what we\njust did, but if not, you can investigate. Let's pick three rows where\n`complete.cases` is \n`TRUE` and three where it's\n`FALSE`, and see what happens.\n\nI'll pick rows 496, 497, and 498 for the TRUE rows, and 540, 541 and\n542 for the FALSE ones. Let's assemble these rows into a vector and\nuse `slice` to display the rows with these numbers:\n\n::: {.cell}\n\n```{.r .cell-code}\nrows <- c(496, 497, 498, 540, 541, 542)\nrows\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 496 497 498 540 541 542\n```\n:::\n:::\n\n \n\nLike this:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% slice(rows)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 15\n  respondent_id lottery_a smoke alcohol gamble skydiving speed cheated steak\n          <dbl> <lgl>     <lgl> <lgl>   <lgl>  <lgl>     <lgl> <lgl>   <lgl>\n1    3234776895 FALSE     FALSE FALSE   FALSE  FALSE     FALSE FALSE   TRUE \n2    3234776815 TRUE      TRUE  TRUE    FALSE  FALSE     TRUE  FALSE   TRUE \n3    3234776702 FALSE     FALSE FALSE   FALSE  FALSE     FALSE FALSE   TRUE \n4    3234763650 TRUE      FALSE FALSE   FALSE  FALSE     TRUE  FALSE   FALSE\n5    3234763171 TRUE      FALSE TRUE    TRUE   FALSE     TRUE  FALSE   TRUE \n6    3234762715 FALSE     FALSE FALSE   FALSE  FALSE     TRUE  FALSE   FALSE\n# i 6 more variables: steak_prep <chr>, female <lgl>, age <chr>,\n#   hhold_income <chr>, educ <chr>, region <chr>\n```\n:::\n:::\n\n \n\nWhat's the difference? \nThe rows where `complete.cases` is FALSE have one (or more)\nmissing values in them; where `complete.cases` is TRUE the\nrows have no missing values. (Depending on the rows you choose,\nyou may not see the missing value(s), as I didn't.)\nExtra (within \"extra extra\": I hope you are keeping track): this\nis a bit tricky to investigate more thoroughly, because the text\nvariables might have missing values in them, and they won't show\nup unless we turn them into a factor first:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>%\n  mutate(across(where(is.character), \\(x) factor(x))) %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:279       FALSE:453       FALSE:125      \n Median :3.235e+09   TRUE :267       TRUE :84        TRUE :416      \n Mean   :3.235e+09   NA's :4         NA's :13        NA's :9        \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.238e+09                                                  \n                                                                    \n   gamble        skydiving         speed          cheated       \n Mode :logical   Mode :logical   Mode :logical   Mode :logical  \n FALSE:280       FALSE:502       FALSE:59        FALSE:447      \n TRUE :257       TRUE :36        TRUE :480       TRUE :92       \n NA's :13        NA's :12        NA's :11        NA's :11       \n                                                                \n                                                                \n                                                                \n   steak               steak_prep    female           age     \n Mode :logical   Medium     :132   Mode :logical   >60  :131  \n FALSE:109       Medium rare:166   FALSE:246       18-29:110  \n TRUE :430       Medium Well: 75   TRUE :268       30-44:133  \n NA's :11        Rare       : 23   NA's :36        45-60:140  \n                 Well       : 36                   NA's : 36  \n                 NA's       :118                              \n                                                              \n              hhold_income                               educ    \n $0 - $24,999       : 51   Bachelor degree                 :174  \n $100,000 - $149,999: 76   Graduate degree                 :133  \n $150,000+          : 54   High school degree              : 39  \n $25,000 - $49,999  : 77   Less than high school degree    :  2  \n $50,000 - $99,999  :172   Some college or Associate degree:164  \n NA's               :120   NA's                            : 38  \n                                                                 \n                region   \n Pacific           : 91  \n South Atlantic    : 88  \n East North Central: 86  \n Middle Atlantic   : 72  \n West North Central: 42  \n (Other)           :133  \n NA's              : 38  \n```\n:::\n:::\n\n     \n\nThere are missing values everywhere. What the `where` \ndoes is to do something for each column where the first thing is true:\nhere, if the column is text, then replace it by the factor version of\nitself. This makes for a better summary, one that shows how many\nobservations are in each category, and, more important for us, how\nmany are missing (a lot).\n\nAll right, so there are 15 columns, so let's investigate missingness\nin our rows by looking at the columns 1 through 8 and then 9 through\n15, so they all fit on the screen. Recall that you can `select`\ncolumns by number:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% select(1:8) %>% slice(rows)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 8\n  respondent_id lottery_a smoke alcohol gamble skydiving speed cheated\n          <dbl> <lgl>     <lgl> <lgl>   <lgl>  <lgl>     <lgl> <lgl>  \n1    3234776895 FALSE     FALSE FALSE   FALSE  FALSE     FALSE FALSE  \n2    3234776815 TRUE      TRUE  TRUE    FALSE  FALSE     TRUE  FALSE  \n3    3234776702 FALSE     FALSE FALSE   FALSE  FALSE     FALSE FALSE  \n4    3234763650 TRUE      FALSE FALSE   FALSE  FALSE     TRUE  FALSE  \n5    3234763171 TRUE      FALSE TRUE    TRUE   FALSE     TRUE  FALSE  \n6    3234762715 FALSE     FALSE FALSE   FALSE  FALSE     TRUE  FALSE  \n```\n:::\n:::\n\n \n\nand\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak0 %>% select(9:15) %>% slice(rows)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 7\n  steak steak_prep  female age   hhold_income        educ                 region\n  <lgl> <chr>       <lgl>  <chr> <chr>               <chr>                <chr> \n1 TRUE  Medium rare TRUE   45-60 $0 - $24,999        Some college or Ass~ West ~\n2 TRUE  Medium      TRUE   45-60 $150,000+           Bachelor degree      New E~\n3 TRUE  Medium rare TRUE   >60   $50,000 - $99,999   Graduate degree      Mount~\n4 FALSE <NA>        FALSE  45-60 $100,000 - $149,999 Some college or Ass~ South~\n5 TRUE  Medium      FALSE  >60   <NA>                Graduate degree      Pacif~\n6 FALSE <NA>        FALSE  18-29 $50,000 - $99,999   Some college or Ass~ West ~\n```\n:::\n:::\n\n \n\nIn this case, the first three rows have no missing values anywhere,\nand the last three rows have exactly one missing value. This\ncorresponds to what we would expect, with `complete.cases`\nidentifying rows that have any missing values.\n\nWhat we now need to do is to obtain a data frame that contains only\nthe rows with non-missing values. This can be done by saving the\nresult of `complete.cases` in a variable first; `filter`\ncan take anything that produces a true or a false for each row, and\nwill return the rows for which the thing it was fed was true.\n\n::: {.cell}\n\n```{.r .cell-code}\ncc <- complete.cases(steak0)\nsteak0 %>% filter(cc) -> steak.complete\n```\n:::\n\n     \n\nA quick check that we got rid of the missing values:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.complete\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 331 x 15\n   respondent_id lottery_a smoke alcohol gamble skydiving speed cheated steak\n           <dbl> <lgl>     <lgl> <lgl>   <lgl>  <lgl>     <lgl> <lgl>   <lgl>\n 1    3234982343 TRUE      FALSE TRUE    FALSE  FALSE     FALSE FALSE   TRUE \n 2    3234973379 TRUE      FALSE TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 3    3234972383 FALSE     TRUE  TRUE    TRUE   FALSE     TRUE  TRUE    TRUE \n 4    3234958833 FALSE     FALSE TRUE    FALSE  FALSE     TRUE  TRUE    TRUE \n 5    3234955240 TRUE      FALSE FALSE   FALSE  FALSE     TRUE  FALSE   TRUE \n 6    3234955010 TRUE      FALSE TRUE    TRUE   TRUE      TRUE  FALSE   TRUE \n 7    3234953052 TRUE      TRUE  TRUE    TRUE   FALSE     TRUE  FALSE   TRUE \n 8    3234951249 FALSE     FALSE TRUE    TRUE   FALSE     FALSE FALSE   TRUE \n 9    3234948883 FALSE     FALSE TRUE    FALSE  FALSE     TRUE  FALSE   TRUE \n10    3234948197 TRUE      FALSE FALSE   TRUE   FALSE     TRUE  FALSE   TRUE \n# i 321 more rows\n# i 6 more variables: steak_prep <chr>, female <lgl>, age <chr>,\n#   hhold_income <chr>, educ <chr>, region <chr>\n```\n:::\n:::\n\n \n\nThere are no missing values *there*. Of course, this is not a\nproof, and there might be some missing values further down, but at\nleast it suggests that we might be good.\n\nFor proof, this is the easiest way I know:\n\n::: {.cell}\n\n```{.r .cell-code}\nsteak.complete %>%\n  mutate(across(where(is.character), \\(x) factor(x))) %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n respondent_id       lottery_a         smoke          alcohol       \n Min.   :3.235e+09   Mode :logical   Mode :logical   Mode :logical  \n 1st Qu.:3.235e+09   FALSE:171       FALSE:274       FALSE:65       \n Median :3.235e+09   TRUE :160       TRUE :57        TRUE :266      \n Mean   :3.235e+09                                                  \n 3rd Qu.:3.235e+09                                                  \n Max.   :3.235e+09                                                  \n                                                                    \n   gamble        skydiving         speed          cheated         steak        \n Mode :logical   Mode :logical   Mode :logical   Mode :logical   Mode:logical  \n FALSE:158       FALSE:308       FALSE:28        FALSE:274       TRUE:331      \n TRUE :173       TRUE :23        TRUE :303       TRUE :57                      \n                                                                               \n                                                                               \n                                                                               \n                                                                               \n       steak_prep    female           age                  hhold_income\n Medium     :109   Mode :logical   >60  :82   $0 - $24,999       : 37  \n Medium rare:128   FALSE:174       18-29:70   $100,000 - $149,999: 66  \n Medium Well: 56   TRUE :157       30-44:93   $150,000+          : 39  \n Rare       : 18                   45-60:86   $25,000 - $49,999  : 55  \n Well       : 20                              $50,000 - $99,999  :134  \n                                                                       \n                                                                       \n                               educ                    region  \n Bachelor degree                 :120   South Atlantic    :68  \n Graduate degree                 : 86   Pacific           :57  \n High school degree              : 20   East North Central:48  \n Less than high school degree    :  1   Middle Atlantic   :46  \n Some college or Associate degree:104   West North Central:29  \n                                        Mountain          :24  \n                                        (Other)           :59  \n```\n:::\n:::\n\n \n\nIf there were any missing values, they would be listed on the end of\nthe counts of observations for each level, or on the bottom of the\nfive-number sumamries. But there aren't.  So here's your proof.\n\n\n$\\blacksquare$\n\n(d) Write the data into a `.csv` file, with a name like\n`steak1.csv`.  Open this file in a spreadsheet and (quickly)\nverify that you have the right columns and no missing values.\n\n\nSolution\n\n\nThis is `write_csv`, using my output from \n`drop_na`:\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(steak, \"steak1.csv\")\n```\n:::\n\n     \n\nOpen up Excel, or whatever you have, and take a look. You should have\nall the right columns, and, scrolling down, no visible missing values.\n\n$\\blacksquare$\n\n\n\n\n\n",
>>>>>>> 038bb2509ac8e38facb2622be6ad3052b44aca34
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}