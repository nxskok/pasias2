{
  "hash": "5db0940d76bc9d030eb13a97db6b70eb",
  "result": {
    "engine": "knitr",
    "markdown": "# Factor Analysis\n\nPackages for this chapter:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggbiplot)\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##   The Interpersonal Circumplex\n\n\n The \"IPIP Interpersonal Circumplex\" (see\n[link](http://ipip.ori.org/newIPIP-IPCSurvey.htm)) is a personal\nbehaviour survey, where respondents have to rate how accurately a\nnumber of statements about behaviour apply to them, on a scale from\n1 (\"very inaccurate\") to 5 (\"very accurate\"). A survey was done\non 459 people, using a 44-item variant of the above questionnaire,\nwhere the statements were as follows. Put an \"I\" or an \"I am\" in\nfront of each one:\n\n\n* talkative\n\n* find fault\n\n* do a thorough job\n\n* depressed\n\n* original\n\n* reserved\n\n* helpful\n\n* careless\n\n* relaxed\n\n* curious\n\n* full of energy\n\n* start quarrels\n\n* reliable\n\n* tense\n\n* ingenious\n\n* generate enthusiasm in others\n\n* forgiving\n\n* disorganized\n\n* worry\n\n* imaginative\n\n* quiet\n\n* trusting\n\n* lazy\n\n* emotionally stable\n\n* inventive\n\n* assertive\n\n* cold and aloof\n\n* persevere\n\n* moody\n\n* value artistic experiences\n\n* shy\n\n* considerate\n\n* efficient\n\n* calm in tense situations\n\n* prefer routine work\n\n* outgoing\n\n* sometimes rude\n\n* stick to plans\n\n* nervous\n\n* reflective\n\n* have few artistic interests\n\n* co-operative\n\n* distractible\n\n* sophisticated in art and music\n\nI don't know what a \"circumplex\" is, but I know it's not one of those \"hat\" accents that they have in French.\nThe data are in\n[link](http://ritsokiguess.site/datafiles/personality.txt). The\ncolumns `PERS01` through `PERS44` represent the above traits.\n\n\n(a) Read in the data and check that you have the right number\nof rows and columns.\n\n\n(b) There are some missing values among these\nresponses. Eliminate all the individuals with any missing values\n(since `princomp` can't handle them).\n\n\n(c) Carry out a principal components analysis and obtain a\nscree plot. \n\n\n(d) How many components/factors should you use? Explain briefly.\n\n\n(e) <a name=\"part:score\">*</a> Using your preferred number of factors, run a factor\nanalysis. Obtain \"r\"-type factor scores, as in class. You don't need to look at any output yet.\n\n\n(f) Obtain the factor loadings. How much of the variability\ndoes your chosen number of factors explain?\n\n\n(g) Interpret each of your chosen number of factors. That is,\nfor each factor, identify the items that load heavily on it (you\ncan be fairly crude about this, eg. use a cutoff like 0.4 in\nabsolute value), and translate these items into the statements\ngiven in each item. Then, if you can, name what the items loading\nheavily on each factor have in common. Interpret a negative\nloading as \"not\" whatever the item says.\n\n\n(h) Find a person who is extreme on each of your first three\nfactors (one at a time). For each of these people, what kind of\ndata should they have for the relevant ones of variables\n`PERS01` through `PERS44`? Do they? Explain\nreasonably briefly.\n\n\n(i) Check the uniquenesses. Which one(s) seem unusually\nhigh? Check these against the factor loadings. Are these what you\nwould expect?\n\n\n\n\n\n##  A correlation matrix\n\n\n Here is a correlation matrix between five variables. This\ncorrelation matrix was based on $n=50$ observations. Save the data\ninto a file.\n\n\n```\n\n1.00 0.90 -0.40 0.28 -0.05\n0.90 1.00 -0.60 0.43 -0.20\n-0.40 -0.60 1.00 -0.80 0.40\n0.28 0.43 -0.80 1.00 -0.70\n-0.05 -0.20 0.40 -0.70 1.00\n\n```\n\n\n\n\n(a) Read in the data, using `col_names=F` (why?). Check that you\nhave five variables with names invented by R.\n\n\n(b) Run a principal components analysis from this correlation matrix.\n\n\n(c) <a name=\"part:howmany\">*</a> Obtain a scree plot. Can you justify\nthe use of two components (later, factors), bearing in mind that we\nhave only five variables?\n\n\n(d) Take a look at the first two component loadings. Which variables appear\nto feature in which component? Do they have a positive or negative effect?\n\n\n(e) Create a \"covariance list\" (for the purposes of\nperforming a factor analysis on the correlation matrix).\n\n\n(f) Carry out a factor analysis with two factors. We'll\ninvestigate the bits of it in a \nmoment.  \n\n\n(g) <a name=\"part:load\">*</a> Look at the factor loadings. Describe how the factors are\nrelated to the original variables. Is the interpretation clearer\nthan for the principal components analysis?\n\n\n(h) Look at the uniquenesses. Are there any that are unusually\nhigh? Does that surprise you, given your answer to\n(<a href=\"#part:load\">here</a>)? (You will probably have to make a judgement call here.)\n\n\n\n\n##  Air pollution\n\n\n The data in\n[link](http://ritsokiguess.site/datafiles/airpollution.csv) are\nmeasurements of air-pollution variables recorded at 12 noon on 42\ndifferent days at a location in Los Angeles. The file is in\n`.csv` format, since it came from a spreadsheet.  Specifically,\nthe variables (in suitable units), in the same order as in the data\nfile, are:\n\n\n\n* wind speed\n\n* solar radiation\n\n* carbon monoxide\n\n* Nitric oxide (also known as nitrogen monoxide)\n\n* Nitrogen dioxide\n\n* Ozone\n\n* Hydrocarbons\n\n\nThe aim is to describe pollution using fewer than these seven variables.\n\n\n\n(a) Read in the data and demonstrate that you have the right\nnumber of rows and columns in your data frame.\n\n\n\n(b) <a name=\"part:fivenum\">*</a> \nObtain a five-number summary for each variable. You can do this in\none go for all seven variables.\n\n\n\n(c) Obtain a principal components analysis. Do it on the\ncorrelation matrix, since the variables are measured on different\nscales. You don't need to look at the results yet.\n\n\n\n(d) Obtain a scree plot. How many principal components might be\nworth looking at? Explain briefly. (There might be more than one\npossibility. If so, discuss them all.)\n\n\n\n(e) Look at the `summary` of the principal components\nobject. What light does this shed on the choice of number of\ncomponents? Explain briefly.\n\n\n\n(f) <a name=\"part:preferred\">*</a> How do each of your preferred number of components depend\non the variables that were measured? Explain briefly.\n\n\n\n(g) Make a data frame that contains (i) the original data, (ii) a column of row numbers, (iii) the principal component scores. Display some of it.\n\n\n(h) Display the row of your new data frame for the observation\nwith the smallest (most negative) score on component 1. Which row is\nthis? What makes this observation have the most negative score on\ncomponent 1?\n\n\n(i) Which observation has the lowest (most negative) value on\ncomponent 2? Which variables ought to be high or low for this\nobservation? Are they? Explain briefly.\n\n\n(j) Obtain a biplot, with the row numbers labelled, and explain briefly how your conclusions from the previous two parts are consistent with it.\n\n\n(k) Run a factor analysis on the same data, obtaining two factors. Look at the factor loadings. Is it clearer which variables belong to which factor, compared to the principal components analysis? Explain briefly.\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##   The Interpersonal Circumplex\n\n\n The \"IPIP Interpersonal Circumplex\" (see\n[link](http://ipip.ori.org/newIPIP-IPCSurvey.htm)) is a personal\nbehaviour survey, where respondents have to rate how accurately a\nnumber of statements about behaviour apply to them, on a scale from\n1 (\"very inaccurate\") to 5 (\"very accurate\"). A survey was done\non 459 people, using a 44-item variant of the above questionnaire,\nwhere the statements were as follows. Put an \"I\" or an \"I am\" in\nfront of each one:\n\n\n* talkative\n\n* find fault\n\n* do a thorough job\n\n* depressed\n\n* original\n\n* reserved\n\n* helpful\n\n* careless\n\n* relaxed\n\n* curious\n\n* full of energy\n\n* start quarrels\n\n* reliable\n\n* tense\n\n* ingenious\n\n* generate enthusiasm in others\n\n* forgiving\n\n* disorganized\n\n* worry\n\n* imaginative\n\n* quiet\n\n* trusting\n\n* lazy\n\n* emotionally stable\n\n* inventive\n\n* assertive\n\n* cold and aloof\n\n* persevere\n\n* moody\n\n* value artistic experiences\n\n* shy\n\n* considerate\n\n* efficient\n\n* calm in tense situations\n\n* prefer routine work\n\n* outgoing\n\n* sometimes rude\n\n* stick to plans\n\n* nervous\n\n* reflective\n\n* have few artistic interests\n\n* co-operative\n\n* distractible\n\n* sophisticated in art and music\n\nI don't know what a \"circumplex\" is, but I know it's not one of those \"hat\" accents that they have in French.\nThe data are in\n[link](http://ritsokiguess.site/datafiles/personality.txt). The\ncolumns `PERS01` through `PERS44` represent the above traits.\n\n\n(a) Read in the data and check that you have the right number\nof rows and columns.\n\nSolution\n\n\nSeparated by single spaces. \n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/personality.txt\"\npers <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 459 Columns: 45\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\ndbl (45): id, PERS01, PERS02, PERS03, PERS04, PERS05, PERS06, PERS07, PERS08...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\npers\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 459 x 45\n      id PERS01 PERS02 PERS03 PERS04 PERS05 PERS06 PERS07 PERS08 PERS09 PERS10\n   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1     1      5      4      5      1      4      3      3      1      2      3\n 2     2      1      1      5      2      1      2      5      1      5      1\n 3     3      4      1      5      3      3      4      5      3      1      4\n 4     4      4      2      5      1      4      3      4      4      4      5\n 5     5      2      3      5      1      2      4      5      2      3      3\n 6     6      1      1      5      4      3      4      4      2      1      4\n 7     7      3      2      5      1      2      1      1      2      5      4\n 8     8      5      2      4      2      4      1      4      3      3      5\n 9     9      5      1      4      3      2      1      4      4      2      3\n10    10      4      1      5      1      4      3      4      1      5      4\n# i 449 more rows\n# i 34 more variables: PERS11 <dbl>, PERS12 <dbl>, PERS13 <dbl>, PERS14 <dbl>,\n#   PERS15 <dbl>, PERS16 <dbl>, PERS17 <dbl>, PERS18 <dbl>, PERS19 <dbl>,\n#   PERS20 <dbl>, PERS21 <dbl>, PERS22 <dbl>, PERS23 <dbl>, PERS24 <dbl>,\n#   PERS25 <dbl>, PERS26 <dbl>, PERS27 <dbl>, PERS28 <dbl>, PERS29 <dbl>,\n#   PERS30 <dbl>, PERS31 <dbl>, PERS32 <dbl>, PERS33 <dbl>, PERS34 <dbl>,\n#   PERS35 <dbl>, PERS36 <dbl>, PERS37 <dbl>, PERS38 <dbl>, PERS39 <dbl>, ...\n```\n\n\n:::\n:::\n\n       \n\nYep, 459 people (in rows), and 44 items (in columns), plus one column\nof `id`s for the people who took the survey.\n\nIn case you were wondering about the \"I\" vs.\\ \"I am\" thing, the\nstory seems to be that each behaviour needs to have a verb. If the\nbehaviour has a verb, \"I\" is all you need, but if it doesn't, you\nhave to add one, ie.\\ \"I am\". \n\nAnother thing you might be concerned about is whether these data are\n\"tidy\" or not. To some extent, it depends on what you are going to\ndo with it. You could say that the `PERS` columns are all\nsurvey-responses, just to different questions, and you might think of\ndoing something like this:\n\n::: {.cell}\n\n```{.r .cell-code}\npers %>% pivot_longer(-id, names_to=\"item\", values_to=\"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20,196 x 3\n      id item   response\n   <dbl> <chr>     <dbl>\n 1     1 PERS01        5\n 2     1 PERS02        4\n 3     1 PERS03        5\n 4     1 PERS04        1\n 5     1 PERS05        4\n 6     1 PERS06        3\n 7     1 PERS07        3\n 8     1 PERS08        1\n 9     1 PERS09        2\n10     1 PERS10        3\n# i 20,186 more rows\n```\n\n\n:::\n:::\n\n \n\nto get a *really* long and skinny data frame. It all depends on\nwhat you are doing with it. Long-and-skinny is ideal if you are going\nto summarize the responses by survey item, or draw something like bar\ncharts of responses facetted by item:\n\n::: {.cell}\n\n```{.r .cell-code}\npers %>%\n  pivot_longer(-id, names_to=\"item\", values_to=\"response\") %>%\n  ggplot(aes(x = response)) + geom_bar() + facet_wrap(~item)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 371 rows containing non-finite outside the scale range\n(`stat_count()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](fa_files/figure-pdf/derhuter-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThe first time I did this, item `PERS36` appeared out of order\nat the end, and I was wondering what happened, until I realized it was\nactually misspelled as `PES36`! I corrected it in the data\nfile, and it should be good now (though I wonder how many years that\nerror persisted for).\n\nFor us, in this problem, though, we need the wide format.\n\n$\\blacksquare$\n\n(b) There are some missing values among these\nresponses. Eliminate all the individuals with any missing values\n(since `princomp` can't handle them).\n\nSolution\n\n\nThis is actually much easier than it was in the past.\nA way of asking \"are there any missing values anywhere?\" is:\n\n::: {.cell}\n\n```{.r .cell-code}\nany(is.na(pers))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n \n\nThere are.\nTo remove them, just this:\n\n::: {.cell}\n\n```{.r .cell-code}\npers %>% drop_na() -> pers.ok\n```\n:::\n\n       \n\nAre there any missings left?\n\n::: {.cell}\n\n```{.r .cell-code}\nany(is.na(pers.ok))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n \n\nNope.\nExtra: you might also have thought of the \"tidy, remove, untidy\"\nstrategy here. The trouble with that here is that you want to remove\n*all* the observations for a subject who has *any* missing\nones. This is unlike the multidimensional scaling one where\nwe wanted to remove all the distances for two cities *that we\nknew ahead of time*. \n\nThat gives me an idea, though. \n\n::: {.cell}\n\n```{.r .cell-code}\npers %>%\n  pivot_longer(-id, names_to=\"item\", values_to=\"rating\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20,196 x 3\n      id item   rating\n   <dbl> <chr>   <dbl>\n 1     1 PERS01      5\n 2     1 PERS02      4\n 3     1 PERS03      5\n 4     1 PERS04      1\n 5     1 PERS05      4\n 6     1 PERS06      3\n 7     1 PERS07      3\n 8     1 PERS08      1\n 9     1 PERS09      2\n10     1 PERS10      3\n# i 20,186 more rows\n```\n\n\n:::\n:::\n\n \n\nTo find out which *subjects* have any missing values, we can do a\n`group_by` and `summarize` on *subjects* (that\nmeans, the `id` column; the `PERS` in the column I\ncalled `item` means \"personality\", not \"person\"!). What do\nwe summarize? Any one of the standard things like `mean` will\nreturn `NA` if the thing whose mean you are finding has any NA\nvalues in it anywhere, and a number if it's \"complete\", so this kind\nof thing, adding to my pipeline:\n\n::: {.cell}\n\n```{.r .cell-code}\npers %>%\n  pivot_longer(-id, names_to=\"item\", values_to=\"rating\") %>% \n  group_by(id) %>%\n  summarize(m = mean(rating)) %>%\n  filter(is.na(m))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 26 x 2\n      id     m\n   <dbl> <dbl>\n 1    12    NA\n 2    40    NA\n 3    45    NA\n 4    49    NA\n 5    52    NA\n 6    58    NA\n 7    79    NA\n 8    84    NA\n 9    96    NA\n10   159    NA\n# i 16 more rows\n```\n\n\n:::\n:::\n\nThis is different from `drop_na`, which would remove any rows (of the long data frame) that have a missing response. This, though, is exactly what we *don't* want, since we are trying to keep track of the subjects that have missing values.\n\nMost of the subjects had an actual numerical  mean here, whose value\nwe don't care about; all we care about here is whether the mean is\nmissing, which implies that one (or more) of the responses was\nmissing. \n\nSo now we define a column `has_missing` that is true if the\nsubject has any missing values and false otherwise:\n\n::: {.cell}\n\n```{.r .cell-code}\npers %>%\n  pivot_longer(-id, names_to=\"item\", values_to=\"rating\") %>% \n  group_by(id) %>%\n  summarize(m = mean(rating)) %>%\n  mutate(has_missing = is.na(m)) -> pers.hm\npers.hm \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 459 x 3\n      id     m has_missing\n   <dbl> <dbl> <lgl>      \n 1     1  3.41 FALSE      \n 2     2  2.98 FALSE      \n 3     3  3.34 FALSE      \n 4     4  3.55 FALSE      \n 5     5  3.32 FALSE      \n 6     6  3.20 FALSE      \n 7     7  2.84 FALSE      \n 8     8  3.18 FALSE      \n 9     9  3.34 FALSE      \n10    10  3.18 FALSE      \n# i 449 more rows\n```\n\n\n:::\n:::\n\n \n\nThis data frame `pers.hm` has the same number of rows as the\noriginal data frame `pers`, one per subject, so we can just\nglue it onto the end of that:\n\n::: {.cell}\n\n```{.r .cell-code}\npers %>% bind_cols(pers.hm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNew names:\n* `id` -> `id...1`\n* `id` -> `id...46`\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 459 x 48\n   id...1 PERS01 PERS02 PERS03 PERS04 PERS05 PERS06 PERS07 PERS08 PERS09 PERS10\n    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1      1      5      4      5      1      4      3      3      1      2      3\n 2      2      1      1      5      2      1      2      5      1      5      1\n 3      3      4      1      5      3      3      4      5      3      1      4\n 4      4      4      2      5      1      4      3      4      4      4      5\n 5      5      2      3      5      1      2      4      5      2      3      3\n 6      6      1      1      5      4      3      4      4      2      1      4\n 7      7      3      2      5      1      2      1      1      2      5      4\n 8      8      5      2      4      2      4      1      4      3      3      5\n 9      9      5      1      4      3      2      1      4      4      2      3\n10     10      4      1      5      1      4      3      4      1      5      4\n# i 449 more rows\n# i 37 more variables: PERS11 <dbl>, PERS12 <dbl>, PERS13 <dbl>, PERS14 <dbl>,\n#   PERS15 <dbl>, PERS16 <dbl>, PERS17 <dbl>, PERS18 <dbl>, PERS19 <dbl>,\n#   PERS20 <dbl>, PERS21 <dbl>, PERS22 <dbl>, PERS23 <dbl>, PERS24 <dbl>,\n#   PERS25 <dbl>, PERS26 <dbl>, PERS27 <dbl>, PERS28 <dbl>, PERS29 <dbl>,\n#   PERS30 <dbl>, PERS31 <dbl>, PERS32 <dbl>, PERS33 <dbl>, PERS34 <dbl>,\n#   PERS35 <dbl>, PERS36 <dbl>, PERS37 <dbl>, PERS38 <dbl>, PERS39 <dbl>, ...\n```\n\n\n:::\n:::\n\n \n\nand then filter out the rows for which `has_missing` is true.\nWhat we did here is really a way of mimicking `complete.cases`,\nwhich is the way we used to have to do it, before `drop_na`\ncame on the scene.\n\n$\\blacksquare$\n\n(c) Carry out a principal components analysis and obtain a\nscree plot. \n\nSolution\n\n\nThis ought to be straightforward, but we've got to remember to\nuse only the columns with actual data in them: that is,\n`PERS01` through `PERS44`:\n\n::: {.cell}\n\n```{.r .cell-code}\npers.1 <- pers.ok %>%\n  select(starts_with(\"PERS\")) %>%\n  princomp(cor = T)\nggscreeplot(pers.1)\n```\n\n::: {.cell-output-display}\n![](fa_files/figure-pdf/saljhsajd-1.pdf){fig-pos='H'}\n:::\n:::\n\n       \n$\\blacksquare$\n\n(d) How many components/factors should you use? Explain briefly.\n\nSolution\n\n\nI think the clearest elbow is at 7, so we should use 6\ncomponents/factors. You could make a case that the elbow at 6 is\nalso part of the scree, and therefore you should use 5\ncomponents/factors. Another one of those judgement calls.\nIgnore the \"backwards elbow\"  at 5: this is\ndefinitely part of the mountain rather than the scree. Backwards\nelbows, as you'll recall, don't count as elbows anyway.\nWhen I drew this in R Studio, the elbow at 6 was clearer than\nthe one at 7, so I went with 5 components/factors below.\nThe other way to go is to take the number of eigenvalues bigger\nthan 1: \n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(pers.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          Comp.1     Comp.2     Comp.3     Comp.4     Comp.5\nStandard deviation     2.6981084 2.04738207 1.74372011 1.59610543 1.50114586\nProportion of Variance 0.1654497 0.09526758 0.06910363 0.05789892 0.05121452\nCumulative Proportion  0.1654497 0.26071732 0.32982096 0.38771988 0.43893440\n                          Comp.6     Comp.7     Comp.8     Comp.9    Comp.10\nStandard deviation     1.2627066 1.14816136 1.10615404 1.07405521 1.02180353\nProportion of Variance 0.0362370 0.02996078 0.02780856 0.02621806 0.02372915\nCumulative Proportion  0.4751714 0.50513218 0.53294074 0.55915880 0.58288795\n                          Comp.11    Comp.12    Comp.13    Comp.14    Comp.15\nStandard deviation     0.98309198 0.97514006 0.94861102 0.90832065 0.90680594\nProportion of Variance 0.02196522 0.02161132 0.02045143 0.01875105 0.01868857\nCumulative Proportion  0.60485317 0.62646449 0.64691592 0.66566698 0.68435554\n                          Comp.16    Comp.17    Comp.18    Comp.19   Comp.20\nStandard deviation     0.86798188 0.85762608 0.84515849 0.82819534 0.8123579\nProportion of Variance 0.01712256 0.01671642 0.01623393 0.01558881 0.0149983\nCumulative Proportion  0.70147810 0.71819452 0.73442845 0.75001726 0.7650156\n                          Comp.21    Comp.22    Comp.23    Comp.24    Comp.25\nStandard deviation     0.80910333 0.80435744 0.76594963 0.75946741 0.75434835\nProportion of Variance 0.01487837 0.01470434 0.01333361 0.01310888 0.01293276\nCumulative Proportion  0.77989393 0.79459827 0.80793188 0.82104076 0.83397352\n                          Comp.26    Comp.27   Comp.28    Comp.29    Comp.30\nStandard deviation     0.74494825 0.73105470 0.6956473 0.68327155 0.67765233\nProportion of Variance 0.01261245 0.01214639 0.0109983 0.01061045 0.01043665\nCumulative Proportion  0.84658597 0.85873236 0.8697307 0.88034111 0.89077776\n                          Comp.31     Comp.32     Comp.33     Comp.34\nStandard deviation     0.66847179 0.660473737 0.651473777 0.629487724\nProportion of Variance 0.01015578 0.009914217 0.009645865 0.009005791\nCumulative Proportion  0.90093355 0.910847763 0.920493629 0.929499420\n                           Comp.35     Comp.36     Comp.37     Comp.38\nStandard deviation     0.618765271 0.605892700 0.594231727 0.581419871\nProportion of Variance 0.008701601 0.008343317 0.008025258 0.007682933\nCumulative Proportion  0.938201021 0.946544338 0.954569596 0.962252530\n                           Comp.39     Comp.40     Comp.41     Comp.42\nStandard deviation     0.568951666 0.560084703 0.547059522 0.524949694\nProportion of Variance 0.007356955 0.007129429 0.006801685 0.006263004\nCumulative Proportion  0.969609484 0.976738913 0.983540598 0.989803602\n                           Comp.43     Comp.44\nStandard deviation     0.490608152 0.456010047\nProportion of Variance 0.005470372 0.004726026\nCumulative Proportion  0.995273974 1.000000000\n```\n\n\n:::\n:::\n\n       \nThere are actually 10 of these. But if you look at the scree plot,\nthere really seems to be no reason to take 10 factors rather than,\nsay, 11 or 12. There are a lot of eigenvalues (standard deviations)\nclose to (but just below) 1, and no obvious \"bargains\" in terms of\nvariance explained: the \"cumulative proportion\" just keeps going\ngradually up.\n\n$\\blacksquare$\n\n(e) <a name=\"part:score\">*</a> Using your preferred number of factors, run a factor\nanalysis. Obtain \"r\"-type factor scores, as in class. You don't need to look at any output yet.\n\nSolution\n\n\nI'm going to do the 5 factors that I preferred the first time I\nlooked at this. Don't forget to grab only the appropriate\ncolumns from `pers.ok`:\n\n::: {.cell}\n\n```{.r .cell-code}\npers.ok.1 <- pers.ok %>%\n  select(starts_with(\"PERS\")) %>%\n  factanal(5, scores = \"r\")\n```\n:::\n\n       \n\nIf you think 6 is better, you should feel free to use that here.\n\n$\\blacksquare$\n\n(f) Obtain the factor loadings. How much of the variability\ndoes your chosen number of factors explain?\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\npers.ok.1$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLoadings:\n       Factor1 Factor2 Factor3 Factor4 Factor5\nPERS01  0.130   0.752           0.279         \nPERS02          0.199   0.202  -0.545         \nPERS03  0.677                   0.160   0.126 \nPERS04 -0.143  -0.239   0.528  -0.195         \nPERS05  0.191   0.258  -0.180   0.159   0.520 \nPERS06  0.104  -0.658   0.185  -0.143         \nPERS07  0.313   0.113           0.533   0.130 \nPERS08 -0.558           0.168  -0.173         \nPERS09                 -0.641           0.136 \nPERS10  0.149   0.169                   0.445 \nPERS11  0.106   0.282  -0.224  -0.107   0.271 \nPERS12 -0.157                  -0.404         \nPERS13  0.577                   0.331   0.126 \nPERS14                  0.685  -0.135         \nPERS15  0.133                           0.480 \nPERS16  0.106   0.481                   0.335 \nPERS17                          0.277   0.102 \nPERS18 -0.641                                 \nPERS19         -0.159   0.596   0.109         \nPERS20          0.215   0.103           0.498 \nPERS21         -0.805   0.121                 \nPERS22  0.153   0.175           0.533         \nPERS23 -0.622           0.121  -0.259         \nPERS24          0.135  -0.582           0.153 \nPERS25          0.176  -0.124           0.517 \nPERS26  0.144   0.515  -0.215  -0.260   0.244 \nPERS27         -0.205   0.136  -0.438         \nPERS28  0.623                   0.245   0.132 \nPERS29                  0.477  -0.231         \nPERS30                                  0.500 \nPERS31         -0.619   0.191                 \nPERS32  0.322                   0.553   0.134 \nPERS33  0.622                                 \nPERS34  0.105          -0.566                 \nPERS35         -0.180                  -0.161 \nPERS36          0.675  -0.146   0.118   0.107 \nPERS37 -0.300   0.134          -0.495         \nPERS38  0.521                           0.169 \nPERS39         -0.333   0.530                 \nPERS40                          0.162   0.585 \nPERS41                         -0.171  -0.326 \nPERS42  0.271   0.179           0.552   0.139 \nPERS43 -0.465           0.236  -0.174         \nPERS44                                  0.449 \n\n               Factor1 Factor2 Factor3 Factor4 Factor5\nSS loadings      3.810   3.740   3.174   2.939   2.627\nProportion Var   0.087   0.085   0.072   0.067   0.060\nCumulative Var   0.087   0.172   0.244   0.311   0.370\n```\n\n\n:::\n:::\n\n       \n\nThe Cumulative Var line at the bottom says that our five factors\ntogether have\nexplained 37\\% of the variability. This is not great, but is the kind\nof thing we have to live with in this kind of analysis (like the\npersonality one in class).\n\n$\\blacksquare$\n\n(g) Interpret each of your chosen number of factors. That is,\nfor each factor, identify the items that load heavily on it (you\ncan be fairly crude about this, eg. use a cutoff like 0.4 in\nabsolute value), and translate these items into the statements\ngiven in each item. Then, if you can, name what the items loading\nheavily on each factor have in common. Interpret a negative\nloading as \"not\" whatever the item says.\n\nSolution\n\n\nThis is a lot of work, but I figure you should go through it at\nleast once! If you have some number of factors other than 5,\nyour results will be different from mine. Keep going as long as\nyou reasonably can.\nFactor 1: 3, 8 (negatively), 13, 18\n(negative), 23 (negative), 28, 33, 38 and maybe 43\n(negatively). These are: do a thorough job, not-careless,\nreliable, not-disorganized, not-lazy, persevere, efficient,\nstick to plans, not-distractible. These have the common theme of\npaying attention to detail and getting the job done properly.\n\nFactor 2: 1, not-6, 16, not-21, 26, not-31, 36. Talkative,\nnot-reserved, generates enthusiasm, not-quiet, assertive,\nnot-shy, outgoing. \"Extravert\" seems to capture all of those.\nFactor 3: 4, not-9, 14, 19, not-24, 29, not-34, 39. Depressed,\nnot-relaxed, tense, worried, not emotionally stable, moody,\nnot-calm-when-tense, nervous. \"Not happy\" or something like that.\n\nNotice how these seem to be jumping in steps of 5? The\npsychology scoring of assessments like this is that a person's\nscore on some dimension is found by adding up their scores on\ncertain of the questions and subtracting their scores on others\n(\"reverse-coded questions\"). I'm guessing that these guys have\n5 dimensions they are testing for, corresponding presumably to\nmy five factors. The questionnaire at\n[link](http://ipip.ori.org/newIPIP-IPCScoringKey.htm) is\ndifferent, but you see the same idea there. (The jumps there seem to\nbe in steps of 8, since they have 8 dimensions.)\n\nFactor 4: not-2, 7, not-12 (just), 22, not-27, 32, not-37,\n42. Doesn't find fault, helpful, doesn't start quarrels,\ntrusting, not-cold-and-aloof, considerate, not-sometimes-rude,\nco-operative. \"Helps without judgement\" or similar.\n\nFactor 5: 5, 10, 15, 20, 25, 30, 40, 44. Original, curious,\ningenious, imaginative, inventive, values artistic experiences,\nreflective, sophisticated in art and music. Creative.\n\nI remembered that psychologists like to talk about the \"big 5\"\npersonality traits. These are extraversion (factor 2 here),\nagreeableness (factor 4), openness (factor 5?),\nconscientiousness (factor 1), and neuroticism (factor 3). The\ncorrespondence appears to be pretty good. (I wrote my answers\nabove last year without thinking about \"big 5\" at all.)\n\nI wonder whether 6 factors is different?\n\n::: {.cell}\n\n```{.r .cell-code}\npers.ok.2 <- pers.ok %>%\n  select(starts_with(\"PERS\")) %>%\n  factanal(6, scores = \"r\")\npers.ok.2$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLoadings:\n       Factor1 Factor2 Factor3 Factor4 Factor5 Factor6\nPERS01  0.114  -0.668           0.343           0.290 \nPERS02                  0.204  -0.451           0.354 \nPERS03  0.658                   0.200           0.131 \nPERS04 -0.141   0.215   0.538  -0.196                 \nPERS05  0.212  -0.271  -0.168   0.115   0.571         \nPERS06          0.694   0.171  -0.102                 \nPERS07  0.315  -0.114           0.530   0.138         \nPERS08 -0.602           0.145                   0.180 \nPERS09                 -0.669   0.118           0.106 \nPERS10  0.125                           0.398   0.264 \nPERS11         -0.131  -0.257           0.173   0.445 \nPERS12 -0.178                  -0.348           0.188 \nPERS13  0.572                   0.332   0.129         \nPERS14          0.142   0.675                   0.121 \nPERS15  0.141                           0.495         \nPERS16         -0.322  -0.130   0.114   0.248   0.491 \nPERS17                 -0.111   0.346           0.139 \nPERS18 -0.655                                         \nPERS19          0.139   0.598   0.101   0.104         \nPERS20         -0.165                   0.489   0.166 \nPERS21          0.843                          -0.110 \nPERS22  0.123  -0.105           0.613           0.110 \nPERS23 -0.631           0.115  -0.232                 \nPERS24                 -0.603           0.113   0.143 \nPERS25         -0.135  -0.119           0.513   0.166 \nPERS26  0.113  -0.385  -0.225  -0.163   0.176   0.454 \nPERS27          0.275   0.128  -0.365           0.177 \nPERS28  0.617                   0.253   0.129         \nPERS29                  0.466  -0.161           0.164 \nPERS30                                  0.497         \nPERS31          0.659   0.166                         \nPERS32  0.297                   0.630           0.102 \nPERS33  0.597                                   0.239 \nPERS34                 -0.584                   0.153 \nPERS35          0.221                  -0.210         \nPERS36         -0.562  -0.171   0.214           0.385 \nPERS37 -0.320                  -0.439           0.220 \nPERS38  0.501                   0.123   0.130   0.182 \nPERS39 -0.123   0.410   0.512                   0.118 \nPERS40                          0.146   0.598         \nPERS41                         -0.106  -0.378   0.117 \nPERS42  0.259  -0.139           0.587   0.118         \nPERS43 -0.515           0.210                   0.253 \nPERS44                                  0.454         \n\n               Factor1 Factor2 Factor3 Factor4 Factor5 Factor6\nSS loadings      3.830   3.284   3.213   2.865   2.527   1.659\nProportion Var   0.087   0.075   0.073   0.065   0.057   0.038\nCumulative Var   0.087   0.162   0.235   0.300   0.357   0.395\n```\n\n\n:::\n:::\n\n       \nMuch of the same kind of thing seems to be happening, though it's a\nbit fuzzier. I suspect the devisers of this survey were adherents to\nthe \"big 5\" theory. The factor 6 here is items 11, 16 and 26, which\nwould be expected to belong to factor 2 here, given what we found\nbefore. I think these particular items are about generating enthusiasm\nin others, rather than (necessarily) about being extraverted oneself.\n\n\n$\\blacksquare$\n\n(h) Find a person who is extreme on each of your first three\nfactors (one at a time). For each of these people, what kind of\ndata should they have for the relevant ones of variables\n`PERS01` through `PERS44`? Do they? Explain\nreasonably briefly.\n\nSolution\n\n\nFor this, we need the factor scores obtained in part\n(<a href=\"#part:score\">here</a>).^[There are two types of scores here:        a person's scores on the psychological test, 1 through 5, and        their factor scores, which are decimal numbers centred at        zero. Try not to get these confused.]\nI'm thinking that I will create a data frame\nwith the original data (with the missing values removed) and the\nfactor scores together, and then look in there. This will have a\nlot of columns, but we'll only need to display some each time.\nThis is based on my 5-factor solution. I'm adding a column\n`id` so that I know which of the individuals (with no\nmissing data) we are looking at:\n\n::: {.cell}\n\n```{.r .cell-code}\nscores.1 <- as_tibble(pers.ok.1$scores) %>%\n  bind_cols(pers.ok) %>%\n  mutate(id = row_number())\nscores.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 433 x 50\n   Factor1 Factor2 Factor3 Factor4 Factor5    id PERS01 PERS02 PERS03 PERS04\n     <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <int>  <dbl>  <dbl>  <dbl>  <dbl>\n 1  1.11     0.543   0.530  -0.978 -0.528      1      5      4      5      1\n 2  0.930   -1.62   -0.823   1.11  -2.49       2      1      1      5      2\n 3  0.0405  -0.908   1.01    0.415 -0.169      3      4      1      5      3\n 4  0.123   -0.600   0.442   0.753  0.774      4      4      2      5      1\n 5  1.28    -1.30   -0.194  -0.170 -1.11       5      2      3      5      1\n 6  0.671   -2.10    1.09    0.507 -0.0442     6      1      1      5      4\n 7  0.474    0.467  -2.12    0.892 -2.57       7      3      2      5      1\n 8 -0.589    0.747  -0.257   1.18  -0.517      8      5      2      4      2\n 9 -0.836    1.02    0.620   1.36  -0.705      9      5      1      4      3\n10  0.984    0.298  -1.06    0.681 -0.553     10      4      1      5      1\n# i 423 more rows\n# i 40 more variables: PERS05 <dbl>, PERS06 <dbl>, PERS07 <dbl>, PERS08 <dbl>,\n#   PERS09 <dbl>, PERS10 <dbl>, PERS11 <dbl>, PERS12 <dbl>, PERS13 <dbl>,\n#   PERS14 <dbl>, PERS15 <dbl>, PERS16 <dbl>, PERS17 <dbl>, PERS18 <dbl>,\n#   PERS19 <dbl>, PERS20 <dbl>, PERS21 <dbl>, PERS22 <dbl>, PERS23 <dbl>,\n#   PERS24 <dbl>, PERS25 <dbl>, PERS26 <dbl>, PERS27 <dbl>, PERS28 <dbl>,\n#   PERS29 <dbl>, PERS30 <dbl>, PERS31 <dbl>, PERS32 <dbl>, PERS33 <dbl>, ...\n```\n\n\n:::\n:::\n\n   \nI did it this way, rather than using `data.frame`, so that I\nwould end up with a `tibble` that would display nicely rather\nthan running off the page. This meant turning the matrix of factor\nscores into a `tibble` first and then gluing everything onto\nit.  (There's no problem in using `data.frame` here if you\nprefer. You could even begin with `data.frame` and pipe the\nfinal result into `as_tibble` to end up with a\n`tibble`.)\nLet's start with factor 1. There are several ways to find the person\nwho scores highest and/or lowest on that factor:\n\n::: {.cell}\n\n```{.r .cell-code}\nscores.1 %>% filter(Factor1 == max(Factor1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 50\n  Factor1 Factor2 Factor3 Factor4 Factor5    id PERS01 PERS02 PERS03 PERS04\n    <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <int>  <dbl>  <dbl>  <dbl>  <dbl>\n1    1.70   -2.14   0.471  -0.525   0.681   231      1      4      5      4\n# i 40 more variables: PERS05 <dbl>, PERS06 <dbl>, PERS07 <dbl>, PERS08 <dbl>,\n#   PERS09 <dbl>, PERS10 <dbl>, PERS11 <dbl>, PERS12 <dbl>, PERS13 <dbl>,\n#   PERS14 <dbl>, PERS15 <dbl>, PERS16 <dbl>, PERS17 <dbl>, PERS18 <dbl>,\n#   PERS19 <dbl>, PERS20 <dbl>, PERS21 <dbl>, PERS22 <dbl>, PERS23 <dbl>,\n#   PERS24 <dbl>, PERS25 <dbl>, PERS26 <dbl>, PERS27 <dbl>, PERS28 <dbl>,\n#   PERS29 <dbl>, PERS30 <dbl>, PERS31 <dbl>, PERS32 <dbl>, PERS33 <dbl>,\n#   PERS34 <dbl>, PERS35 <dbl>, PERS36 <dbl>, PERS37 <dbl>, PERS38 <dbl>, ...\n```\n\n\n:::\n:::\n\n \n\nto display the maximum, or\n\n::: {.cell}\n\n```{.r .cell-code}\nscores.1 %>% arrange(Factor1) %>% slice(1:5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 50\n  Factor1 Factor2 Factor3 Factor4 Factor5    id PERS01 PERS02 PERS03 PERS04\n    <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <int>  <dbl>  <dbl>  <dbl>  <dbl>\n1   -2.82   1.33  -0.548    0.786  0.488    340      5      1      1      2\n2   -2.24  -0.510  0.125   -0.523 -0.0971   128      1      2      2      4\n3   -2.14  -0.841 -0.272   -1.12  -1.18     142      1      2      1      4\n4   -2.12   0.492  0.491   -0.259  0.259    387      5      3      3      4\n5   -2.09  -0.125 -0.0469   0.766 -0.229    396      3      1      2      4\n# i 40 more variables: PERS05 <dbl>, PERS06 <dbl>, PERS07 <dbl>, PERS08 <dbl>,\n#   PERS09 <dbl>, PERS10 <dbl>, PERS11 <dbl>, PERS12 <dbl>, PERS13 <dbl>,\n#   PERS14 <dbl>, PERS15 <dbl>, PERS16 <dbl>, PERS17 <dbl>, PERS18 <dbl>,\n#   PERS19 <dbl>, PERS20 <dbl>, PERS21 <dbl>, PERS22 <dbl>, PERS23 <dbl>,\n#   PERS24 <dbl>, PERS25 <dbl>, PERS26 <dbl>, PERS27 <dbl>, PERS28 <dbl>,\n#   PERS29 <dbl>, PERS30 <dbl>, PERS31 <dbl>, PERS32 <dbl>, PERS33 <dbl>,\n#   PERS34 <dbl>, PERS35 <dbl>, PERS36 <dbl>, PERS37 <dbl>, PERS38 <dbl>, ...\n```\n\n\n:::\n:::\n\n \n\nto display the minimum (and in this case the five smallest ones), or\n\n::: {.cell}\n\n```{.r .cell-code}\nscores.1 %>% filter(abs(Factor1) == max(abs(Factor1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 50\n  Factor1 Factor2 Factor3 Factor4 Factor5    id PERS01 PERS02 PERS03 PERS04\n    <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <int>  <dbl>  <dbl>  <dbl>  <dbl>\n1   -2.82    1.33  -0.548   0.786   0.488   340      5      1      1      2\n# i 40 more variables: PERS05 <dbl>, PERS06 <dbl>, PERS07 <dbl>, PERS08 <dbl>,\n#   PERS09 <dbl>, PERS10 <dbl>, PERS11 <dbl>, PERS12 <dbl>, PERS13 <dbl>,\n#   PERS14 <dbl>, PERS15 <dbl>, PERS16 <dbl>, PERS17 <dbl>, PERS18 <dbl>,\n#   PERS19 <dbl>, PERS20 <dbl>, PERS21 <dbl>, PERS22 <dbl>, PERS23 <dbl>,\n#   PERS24 <dbl>, PERS25 <dbl>, PERS26 <dbl>, PERS27 <dbl>, PERS28 <dbl>,\n#   PERS29 <dbl>, PERS30 <dbl>, PERS31 <dbl>, PERS32 <dbl>, PERS33 <dbl>,\n#   PERS34 <dbl>, PERS35 <dbl>, PERS36 <dbl>, PERS37 <dbl>, PERS38 <dbl>, ...\n```\n\n\n:::\n:::\n\n \n\nto display the largest one in size, whether positive or negative. The\ncode is a bit obfuscated because I have to take absolute values\ntwice. Maybe it would be clearer to create a column with the absolute\nvalues in it and look at that:\n\n::: {.cell}\n\n```{.r .cell-code}\nscores.1 %>%\n  mutate(abso = abs(Factor1)) %>%\n  filter(abso == max(abso))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 51\n  Factor1 Factor2 Factor3 Factor4 Factor5    id PERS01 PERS02 PERS03 PERS04\n    <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <int>  <dbl>  <dbl>  <dbl>  <dbl>\n1   -2.82    1.33  -0.548   0.786   0.488   340      5      1      1      2\n# i 41 more variables: PERS05 <dbl>, PERS06 <dbl>, PERS07 <dbl>, PERS08 <dbl>,\n#   PERS09 <dbl>, PERS10 <dbl>, PERS11 <dbl>, PERS12 <dbl>, PERS13 <dbl>,\n#   PERS14 <dbl>, PERS15 <dbl>, PERS16 <dbl>, PERS17 <dbl>, PERS18 <dbl>,\n#   PERS19 <dbl>, PERS20 <dbl>, PERS21 <dbl>, PERS22 <dbl>, PERS23 <dbl>,\n#   PERS24 <dbl>, PERS25 <dbl>, PERS26 <dbl>, PERS27 <dbl>, PERS28 <dbl>,\n#   PERS29 <dbl>, PERS30 <dbl>, PERS31 <dbl>, PERS32 <dbl>, PERS33 <dbl>,\n#   PERS34 <dbl>, PERS35 <dbl>, PERS36 <dbl>, PERS37 <dbl>, PERS38 <dbl>, ...\n```\n\n\n:::\n:::\n\n \n\nDoes\nthis work too?\n\n::: {.cell}\n\n```{.r .cell-code}\nscores.1 %>% arrange(desc(abs(Factor1))) %>% slice(1:5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 50\n  Factor1 Factor2 Factor3 Factor4 Factor5    id PERS01 PERS02 PERS03 PERS04\n    <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <int>  <dbl>  <dbl>  <dbl>  <dbl>\n1   -2.82   1.33  -0.548    0.786  0.488    340      5      1      1      2\n2   -2.24  -0.510  0.125   -0.523 -0.0971   128      1      2      2      4\n3   -2.14  -0.841 -0.272   -1.12  -1.18     142      1      2      1      4\n4   -2.12   0.492  0.491   -0.259  0.259    387      5      3      3      4\n5   -2.09  -0.125 -0.0469   0.766 -0.229    396      3      1      2      4\n# i 40 more variables: PERS05 <dbl>, PERS06 <dbl>, PERS07 <dbl>, PERS08 <dbl>,\n#   PERS09 <dbl>, PERS10 <dbl>, PERS11 <dbl>, PERS12 <dbl>, PERS13 <dbl>,\n#   PERS14 <dbl>, PERS15 <dbl>, PERS16 <dbl>, PERS17 <dbl>, PERS18 <dbl>,\n#   PERS19 <dbl>, PERS20 <dbl>, PERS21 <dbl>, PERS22 <dbl>, PERS23 <dbl>,\n#   PERS24 <dbl>, PERS25 <dbl>, PERS26 <dbl>, PERS27 <dbl>, PERS28 <dbl>,\n#   PERS29 <dbl>, PERS30 <dbl>, PERS31 <dbl>, PERS32 <dbl>, PERS33 <dbl>,\n#   PERS34 <dbl>, PERS35 <dbl>, PERS36 <dbl>, PERS37 <dbl>, PERS38 <dbl>, ...\n```\n\n\n:::\n:::\n\n \n\nIt looks as if it does: \"sort the Factor 1 scores in descending order by absolute value, and display the first few\". The most extreme\nscores on Factor 1 are all negative: the most positive one (found\nabove) was only about 1.70. \n\nFor you, you don't have to be this sophisticated. It's enough to\neyeball the factor scores on factor 1 and find one that's reasonably\nfar away from zero. Then you note its row and `slice` that row,\nlater. \n\nI think I like the idea of creating a new column with the absolute\nvalues in it and finding the largest of that. Before we pursue that,\nthough, remember that we don't need to look at *all* the\n`PERS` columns, because only some of them load highly on each\nfactor. These are the ones I defined into `f1` first; the first\nones have positive loadings and the last three have negative loadings:\n\n::: {.cell}\n\n```{.r .cell-code}\nf1 <- c(3, 13, 28, 33, 38, 8, 18, 23, 43)\nscores.1 %>%\n  mutate(abso = abs(Factor1)) %>%\n  filter(abso == max(abso)) %>%\n  select(id, Factor1, num_range(\"PERS\", f1, width = 2)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 11\n     id Factor1 PERS03 PERS13 PERS28 PERS33 PERS38 PERS08 PERS18 PERS23 PERS43\n  <int>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1   340   -2.82      1      3      3      1      2      5      5      4      5\n```\n\n\n:::\n:::\n\n \n\nI don't think I've used `num_range` before, like, ever. It is\none of the select-helpers like `starts_with`. It is used when\nyou have column names that are some constant text followed by variable\nnumbers, which is exactly what we have here: we want to select the\n`PERS` columns with numbers that we\nspecify. `num_range` requires (at least) two things: the text prefix,\nfollowed by a vector of numbers that are to be glued onto the\nprefix. I defined that first so as not to clutter up the\n`select` line. The third thing here is `width`: all the\n`PERS` columns have a name that ends with two digits, so\n`PERS03` rather than `PERS3`, and using `width`\nmakes sure that the zero gets inserted.\n\nIndividual 340 is a low scorer on factor 1, so they should have low\nscores on the first five items (the ones with positive loading on\nfactor 1) and high scores on the last four. This is indeed what\nhappened: 1s, 2s and 3s on the first five items and 4s and 5s on the\nlast four. \nHaving struggled through that, factors 2 and 3 are repeats of\nthis. The high loaders on factor 2 are the ones shown in `f2`\nbelow, with the first five loading positively and the last three\nnegatively.^[I think the last four items in the entire survey  are different; otherwise the total number of items would be a  multiple of 5.]\n\n::: {.cell}\n\n```{.r .cell-code}\nf2 <- c(1, 11, 16, 26, 36, 6, 21, 31)\nscores.1 %>%\n  mutate(abso = abs(Factor2)) %>%\n  filter(abso == max(abso)) %>%\n  select(id, Factor2, num_range(\"PERS\", f2, width = 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 10\n     id Factor2 PERS01 PERS11 PERS16 PERS26 PERS36 PERS06 PERS21 PERS31\n  <int>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1    58   -2.35      1      2      1      2      2      5      5      5\n```\n\n\n:::\n:::\n\n \n\nWhat kind of idiot, I was thinking, named the data frame of scores\n`scores.1` when there are going to be three factors to assess?\n\nThe first five scores are lowish, but the last three are definitely\nhigh (three 5s). This idea of a low score on the positive-loading\nitems and a high score on the negatively-loading ones is entirely\nconsistent with a negative factor score.\n\nFinally, factor 3, which loads highly on  items 4, 9 (neg), 14, 19, 24\n(neg), 29, 34 (neg), 39. (Item 44, which you'd expect to be part of\nthis factor, is actually in factor 5.) First we see which individual\nthis is:\n\n::: {.cell}\n\n```{.r .cell-code}\nf3 <- c(4, 14, 19, 29, 39, 9, 24, 34)\nscores.1 %>%\n  mutate(abso = abs(Factor3)) %>%\n  filter(abso == max(abso)) %>%\n  select(id, Factor3, num_range(\"PERS\", f3, width = 2)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 10\n     id Factor3 PERS04 PERS14 PERS19 PERS29 PERS39 PERS09 PERS24 PERS34\n  <int>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1   209   -2.27      1      1      4      1      1      5      4      4\n```\n\n\n:::\n:::\n\n \n\nThe only mysterious one there is item 19, which ought to be low,\nbecause it has a positive loading and the factor score is unusually\nnegative. But it is 4 on a 5-point scale. The others that are supposed\nto be low are 1 and the ones that are supposed to be high are 4 or 5, so\nthose all match up.\n\n$\\blacksquare$\n\n(i) Check the uniquenesses. Which one(s) seem unusually\nhigh? Check these against the factor loadings. Are these what you\nwould expect?\n\nSolution\n\n\nMine are\n\n::: {.cell}\n\n```{.r .cell-code}\npers.ok.1$uniquenesses\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   PERS01    PERS02    PERS03    PERS04    PERS05    PERS06    PERS07    PERS08 \n0.3276244 0.6155884 0.4955364 0.6035655 0.5689691 0.4980334 0.5884146 0.6299781 \n   PERS09    PERS10    PERS11    PERS12    PERS13    PERS14    PERS15    PERS16 \n0.5546981 0.7460655 0.7740590 0.8016644 0.5336047 0.5035412 0.7381636 0.6352166 \n   PERS17    PERS18    PERS19    PERS20    PERS21    PERS22    PERS23    PERS24 \n0.8978624 0.5881834 0.5949740 0.6900378 0.3274366 0.6564542 0.5279346 0.6107080 \n   PERS25    PERS26    PERS27    PERS28    PERS29    PERS30    PERS31    PERS32 \n0.6795545 0.5412962 0.7438329 0.5289192 0.7114735 0.7386601 0.5762901 0.5592906 \n   PERS33    PERS34    PERS35    PERS36    PERS37    PERS38    PERS39    PERS40 \n0.6029914 0.6573411 0.9306766 0.4966071 0.6396371 0.6804821 0.5933423 0.6204610 \n   PERS41    PERS42    PERS43    PERS44 \n0.8560531 0.5684220 0.6898732 0.7872295 \n```\n\n\n:::\n:::\n\n       \n\nYours will be different if you used a different number of factors. But\nthe procedure you follow will be the same as mine.\n\nI think the highest of these is 0.9307, for item 35. Also high is item\n17, 0.8979. If you look back at the table of loadings, item 35 has low\nloadings on *all* the factors: the largest in size is only\n0.180. The largest loading for item 17 is 0.277, on factor 4. This is\nnot high either. \n\nLooking down the loadings table, also item 41 has only a loading of\n$-0.326$ on factor 5, so its uniqueness ought to be pretty high as\nwell. At 0.8561, it is.\n\n$\\blacksquare$\n\n\n\n\n##  A correlation matrix\n\n\n Here is a correlation matrix between five variables. This\ncorrelation matrix was based on $n=50$ observations. Save the data\ninto a file.\n\n\n```\n\n1.00 0.90 -0.40 0.28 -0.05\n0.90 1.00 -0.60 0.43 -0.20\n-0.40 -0.60 1.00 -0.80 0.40\n0.28 0.43 -0.80 1.00 -0.70\n-0.05 -0.20 0.40 -0.70 1.00\n\n```\n\n\n\n\n(a) Read in the data, using `col_names=F` (why?). Check that you\nhave five variables with names invented by R.\n\nSolution\n\n\nI saved my data into `cov5.txt`,^[Not to be confused    with *covfefe*, which was current news when I wrote this question.] delimited by single spaces, so:\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr <- read_delim(\"cov5.txt\", \" \", col_names = F)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 5 Columns: 5\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\ndbl (5): X1, X2, X3, X4, X5\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ncorr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 5\n     X1    X2    X3    X4    X5\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1  1     0.9   -0.4  0.28 -0.05\n2  0.9   1     -0.6  0.43 -0.2 \n3 -0.4  -0.6    1   -0.8   0.4 \n4  0.28  0.43  -0.8  1    -0.7 \n5 -0.05 -0.2    0.4 -0.7   1   \n```\n\n\n:::\n:::\n\n   \n\nI needed to say that I have no variable names and I want R\nto provide some. As you see, it did: `X1` through\n`X5`. You can also supply your own names in this fashion:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_names <- c(\"first\", \"second\", \"third\", \"fourth\", \"fifth\")\ncorr2 <- read_delim(\"cov5.txt\", \" \", col_names = my_names)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 5 Columns: 5\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\ndbl (5): first, second, third, fourth, fifth\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ncorr2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 5\n  first second third fourth fifth\n  <dbl>  <dbl> <dbl>  <dbl> <dbl>\n1  1      0.9   -0.4   0.28 -0.05\n2  0.9    1     -0.6   0.43 -0.2 \n3 -0.4   -0.6    1    -0.8   0.4 \n4  0.28   0.43  -0.8   1    -0.7 \n5 -0.05  -0.2    0.4  -0.7   1   \n```\n\n\n:::\n:::\n\n \n$\\blacksquare$\n\n(b) Run a principal components analysis from this correlation matrix.\n\nSolution\n\n\nTwo lines, these:\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr.mat <- as.matrix(corr)\ncorr.1 <- princomp(covmat = corr.mat)\n```\n:::\n\n     \n\nOr do it in one step as\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr.1a <- princomp(as.matrix(corr))\n```\n:::\n\n \n\nif you like, but I think it's less clear what's going on.\n\n$\\blacksquare$\n\n(c) <a name=\"part:howmany\">*</a> Obtain a scree plot. Can you justify\nthe use of two components (later, factors), bearing in mind that we\nhave only five variables?\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggscreeplot(corr.1)\n```\n\n::: {.cell-output-display}\n![](fa_files/figure-pdf/sdljhljsahja-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nThere is kind of an elbow at 3, which would suggest two components/factors.^[There is also kind of an elbow at 4, which would suggest three factors, but that's really too many with only 5 variables. That wouldn't be much of a *reduction* in the number of variables, which is what principal components is trying to achieve.]\n\nYou can also use the eigenvalue-bigger-than-1 thing: \n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(corr.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          Comp.1    Comp.2     Comp.3     Comp.4    Comp.5\nStandard deviation     1.7185460 1.1686447 0.70207741 0.36584870 0.2326177\nProportion of Variance 0.5906801 0.2731461 0.09858254 0.02676905 0.0108222\nCumulative Proportion  0.5906801 0.8638262 0.96240875 0.98917780 1.0000000\n```\n\n\n:::\n:::\n\n \n\nOnly the first\n*two* eigenvalues are bigger than 1, and the third is quite a bit\nsmaller. So this would suggest two factors also. The third eigenvalue\nis in that kind of nebulous zone between between being big and being\nsmall, and the percent of variance explained is also ambiguous: is\n86\\% good enough or should I go for 96\\%? These questions rarely have\ngood answers. But an issue is that you want to summarize your\nvariables with a (much) smaller number of factors; with 5 variables,\nhaving two factors rather than more than two seems to be a way of\ngaining some insight.\n\n$\\blacksquare$\n\n(d) Take a look at the first two component loadings. Which variables appear\nto feature in which component? Do they have a positive or negative effect?\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr.1$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLoadings:\n   Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nX1  0.404  0.571  0.287  0.363  0.545\nX2  0.482  0.439  0.144 -0.363 -0.650\nX3 -0.501  0.122  0.652  0.412 -0.375\nX4  0.490 -0.390 -0.171  0.689 -0.323\nX5 -0.338  0.561 -0.665  0.304 -0.189\n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\nSS loadings       1.0    1.0    1.0    1.0    1.0\nProportion Var    0.2    0.2    0.2    0.2    0.2\nCumulative Var    0.2    0.4    0.6    0.8    1.0\n```\n\n\n:::\n:::\n\n     \nThis is messy. \n\nIn the first component, the loadings are all about the\nsame in size, but the ones for `X3` and `X5` are\nnegative and the rest are positive. Thus, component 1 is contrasting\n`X3` and `X5` with the others.\n\nIn the second component,  the emphasis is on `X1`, `X2`\nand `X5`, all with negative loadings, and possibly `X4`,\nwith a positive loading.\n\nNote that the first component is basically \"size\", since the\ncomponent loadings are all almost equal in absolute value. This often\nhappens in principal components; for example, it also happened with\nthe running records in class.\n\nI hope the factor analysis, with its rotation, will straighten things\nout some. \n\n$\\blacksquare$\n\n(e) Create a \"covariance list\" (for the purposes of\nperforming a factor analysis on the correlation matrix).\n\nSolution\n\n\nThis is about the most direct way:\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr.list <- list(cov = corr.mat, n.obs = 50)\n```\n:::\n\n     \nrecalling that there were 50 observations. The idea is that we feed\nthis into `factanal` instead of the correlation matrix, so that\nthe factor analysis knows how many individuals there were (for testing\nand such).\n\nNote that you need the correlation matrix *as a `matrix`*,\nnot as a data frame. If you ran the `princomp` all in one step,\nyou'll have to create the correlation matrix again, for example like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr.list2 <- list(cov = as.matrix(corr), n.obs = 50)\n```\n:::\n\n \n\nThe actual list looks like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr.list\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$cov\n        X1    X2   X3    X4    X5\n[1,]  1.00  0.90 -0.4  0.28 -0.05\n[2,]  0.90  1.00 -0.6  0.43 -0.20\n[3,] -0.40 -0.60  1.0 -0.80  0.40\n[4,]  0.28  0.43 -0.8  1.00 -0.70\n[5,] -0.05 -0.20  0.4 -0.70  1.00\n\n$n.obs\n[1] 50\n```\n\n\n:::\n:::\n\n \n\nAn R list is a collection of things *not all of the same type*,\nhere a matrix and a number, and is a handy way of keeping a bunch of\nconnected things together. You use the same dollar-sign notation as\nfor a data frame to\naccess the things in a list:\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr.list$n.obs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 50\n```\n\n\n:::\n:::\n\n \n\nand logically this is because, to R, a data frame *is* a special\nkind of a list, so anything that works for a list also works for a\ndata frame, plus some extra things besides.^[In computer  science terms, a data frame is said to **inherit** from a list: it is a list plus extra stuff.] \n\nThe same idea applies to extracting things from the output of a\nregression (with `lm`) or something like a `t.test`: the\noutput from those is also a list. But for those, I like `tidy`\nfrom `broom` better.\n\n$\\blacksquare$\n\n(f) Carry out a factor analysis with two factors. We'll\ninvestigate the bits of it in a \nmoment.  \n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr.2 <- factanal(factors = 2, covmat = corr.list)\n```\n:::\n\n     \n$\\blacksquare$\n\n(g) <a name=\"part:load\">*</a> Look at the factor loadings. Describe how the factors are\nrelated to the original variables. Is the interpretation clearer\nthan for the principal components analysis?\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr.2$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLoadings:\n     Factor1 Factor2\n[1,]          0.905 \n[2,] -0.241   0.968 \n[3,]  0.728  -0.437 \n[4,] -0.977   0.201 \n[5,]  0.709         \n\n               Factor1 Factor2\nSS loadings      2.056   1.987\nProportion Var   0.411   0.397\nCumulative Var   0.411   0.809\n```\n\n\n:::\n:::\n\n     \n\nOh yes, this is a lot clearer. Factor 1 is variables 3 and 5\ncontrasted with variable 4; factor 2 is variables 1 and 2. No\nhand-waving required.\n\nPerhaps now is a good time to look back at the correlation matrix and\nsee why the factor analysis came out this way:\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 5\n     X1    X2    X3    X4    X5\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1  1     0.9   -0.4  0.28 -0.05\n2  0.9   1     -0.6  0.43 -0.2 \n3 -0.4  -0.6    1   -0.8   0.4 \n4  0.28  0.43  -0.8  1    -0.7 \n5 -0.05 -0.2    0.4 -0.7   1   \n```\n\n\n:::\n:::\n\n \n\nVariable `X1` is highly correlated with `X2` but not\nreally any of the others. Likewise variables `X3, X4, X5` are\nmore or less highly correlated among themselves but not with the\nothers (`X2` and `X3` being an exception, but the big\npicture is as I described). So variables that appear in the same\nfactor should be highly correlated with each other and not with\nvariables that are in different factors. But it took the factor\nanalysis to really show this up.\n\n$\\blacksquare$\n\n(h) Look at the uniquenesses. Are there any that are unusually\nhigh? Does that surprise you, given your answer to\n(<a href=\"#part:load\">here</a>)? (You will probably have to make a judgement call here.)\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorr.2$uniquenesses\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       X1        X2        X3        X4        X5 \n0.1715682 0.0050000 0.2789445 0.0050000 0.4961028 \n```\n\n\n:::\n:::\n\n     \n\nThe ones for `X3` and `X5` are higher than the rest;\nthis is because their loadings on factor 1 are lower than the\nrest. Since those loadings are still high, I wouldn't be worried about\nthe uniquenesses.\n\nThe point here is that an (excessively) high uniqueness indicates a\nvariable that doesn't appear in *any* factor. The easy link to\nmake is \"all the variables appear in a factor, so there shouldn't be any very high uniquenesses\". If, say, `X3` doesn't have a high\nloading on any factor, `X3` would have a high uniqueness (like\n0.9, and none of these values approach that).\n\n$\\blacksquare$\n\n\n\n##  Air pollution\n\n\n The data in\n[link](http://ritsokiguess.site/datafiles/airpollution.csv) are\nmeasurements of air-pollution variables recorded at 12 noon on 42\ndifferent days at a location in Los Angeles. The file is in\n`.csv` format, since it came from a spreadsheet.  Specifically,\nthe variables (in suitable units), in the same order as in the data\nfile, are:\n\n\n\n* wind speed\n\n* solar radiation\n\n* carbon monoxide\n\n* Nitric oxide (also known as nitrogen monoxide)\n\n* Nitrogen dioxide\n\n* Ozone\n\n* Hydrocarbons\n\n\nThe aim is to describe pollution using fewer than these seven variables.\n\n\n\n(a) Read in the data and demonstrate that you have the right\nnumber of rows and columns in your data frame.\n\n\nSolution\n\n\nThis is a `.csv` file, so:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/airpollution.csv\"\nair <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 42 Columns: 7\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (7): wind, solar.radiation, CO, NO, NO2, O3, HC\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nair\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 42 x 7\n    wind solar.radiation    CO    NO   NO2    O3    HC\n   <dbl>           <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     8              98     7     2    12     8     2\n 2     7             107     4     3     9     5     3\n 3     7             103     4     3     5     6     3\n 4    10              88     5     2     8    15     4\n 5     6              91     4     2     8    10     3\n 6     8              90     5     2    12    12     4\n 7     9              84     7     4    12    15     5\n 8     5              72     6     4    21    14     4\n 9     7              82     5     1    11    11     3\n10     8              64     5     2    13     9     4\n# i 32 more rows\n```\n\n\n:::\n:::\n\n     \n\nThere should  be 42 rows (for the 42 days), and 7 columns (for the 7\nvariables), and there are. \n    \n$\\blacksquare$\n\n(b) <a name=\"part:fivenum\">*</a> \nObtain a five-number summary for each variable. You can do this in\none go for all seven variables.\n\n\nSolution\n\n\nLike this (the cleanest):\n\n::: {.cell}\n\n```{.r .cell-code}\nair %>% \n  summarize(across(everything(), \\(x) quantile(x)))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\ni Please use `reframe()` instead.\ni When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 7\n   wind solar.radiation    CO    NO   NO2    O3    HC\n  <dbl>           <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  5               30       2     1   5     2       2\n2  6               68.2     4     1   8     6       3\n3  8               76.5     4     2   9.5   8.5     3\n4  8.75            84.8     5     3  12    11       3\n5 10              107       7     5  21    25       5\n```\n\n\n:::\n:::\n\n\nI have to figure out how to\nidentify which number from the five number summary each of these is,\nbut in this case you can easily figure it out since the min is the\nsmallest and the max has to be the biggest in each column.\n\nOr, with some more work, this:\n\n::: {.cell}\n\n```{.r .cell-code}\nair %>%\n  pivot_longer(everything(), names_to=\"xname\", values_to=\"x\") %>% \n  nest_by(xname) %>%\n  rowwise() %>% \n  mutate(q = list(enframe(quantile(data$x)))) %>%\n  unnest(q) %>%\n  pivot_wider(names_from=name, values_from=value) %>% \n  select(-data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 x 6\n  xname            `0%` `25%` `50%` `75%` `100%`\n  <chr>           <dbl> <dbl> <dbl> <dbl>  <dbl>\n1 CO                  2   4     4    5         7\n2 HC                  2   3     3    3         5\n3 NO                  1   1     2    3         5\n4 NO2                 5   8     9.5 12        21\n5 O3                  2   6     8.5 11        25\n6 solar.radiation    30  68.2  76.5 84.8     107\n7 wind                5   6     8    8.75     10\n```\n\n\n:::\n:::\n\nThere's a lot here. Run it one line at a time to see what it does:\n\n- put the names of the variables in one column and the values in a second. This is the same trick as when we want to make plots of all the variables facetted.\n\n- the `nest_by` says: for each variable (whose names are now in `xname`), make a dataframe called `data` of the observations (in `x`) for that variable\n\n- the rest of the way, work one row at a time\n\n- work out the five-number summary for each variable, using the values `x` in the data frame `data` of each row of the list-column, one at a time. This is the base R `quantile`, working on a vector (the column `x` of the data frame `data`), so it gives you back a named vector. If you are not familiar with that, try running `quantile(1:10)` and see how the output has both the percentiles and, above them, the percents that they go with. The tidyverse doesn't like names, so my favourite way of keeping them with a named vector is to run it through `enframe`. This makes a two-column dataframe, with a column called `name` that is in this case the percents, and a column called `value` that is the percentiles. This is a dataframe rather than a single number, so it needs a `list` on the front as well (to make another list-column).\nThere are rather a lot of brackets to close here; if you are not sure you have enough, type another close bracket, pause, and see what it matches (R Studio will show you). If it matches nothing, you have too many close brackets.\n\n- show the values of the five-number summary for each variable (in long format, but with the percentages attached)\n\n- for human consumption, put the percentiles in columns, one row for each variable\n\n- finally, get rid of the dataframes of original values (that we don't need any more now that we have summarized them).\n\nExtra: say you wanted to make facetted histograms of each variable. You would begin the same way, with the `pivot_longer`, and at the end, `facet_wrap` with `scales = \"free\"` (since the variables are measured on different scales):\n\n::: {.cell}\n\n```{.r .cell-code}\nair %>%\n  pivot_longer(everything(), names_to=\"xname\", values_to=\"x\") %>% \n  ggplot(aes(x=x)) + geom_histogram(bins = 6) +\n  facet_wrap(~xname, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](fa_files/figure-pdf/airpollution-4-1.pdf){fig-pos='H'}\n:::\n:::\n\nExtra extra: I originally put a pipe symbol on the end of the line with the `geom_histogram` on it, and got an impenetrable error. However, googling the error message (often a good plan) gave me a first hit that told me exactly what I had done.\n    \n$\\blacksquare$\n\n(c) Obtain a principal components analysis. Do it on the\ncorrelation matrix, since the variables are measured on different\nscales. You don't need to look at the results yet.\n\n\nSolution\n\n\nThis too is all rather like the previous question:\n\n::: {.cell}\n\n```{.r .cell-code}\nair.1 <- princomp(air, cor = T)\n```\n:::\n\n     \n    \n$\\blacksquare$\n\n(d) Obtain a scree plot. How many principal components might be\nworth looking at? Explain briefly. (There might be more than one\npossibility. If so, discuss them all.)\n\n\nSolution\n\n\n`ggscreeplot` the thing you just obtained, having loaded\npackage `ggbiplot`: \n\n::: {.cell}\n\n```{.r .cell-code}\nggscreeplot(air.1)\n```\n\n::: {.cell-output-display}\n![](fa_files/figure-pdf/airpollution-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nThere is a technicality here, which is\nthat `ggbiplot`, the package, loads `plyr`, which\ncontains a lot of the same things as `dplyr` (the latter is a\ncut-down version of the former). If you load `dplyr` and\n*then* `plyr` (that is to say, if you load the\n`tidyverse` first and then `ggbiplot`), you will end up\nwith trouble, and probably the wrong version of a lot of functions. To\navoid this, load `ggbiplot` *first*, and then you'll be\nOK. \n\nNow, finally, we might diverge from the previous question. There are\nactually *two* elbows on this plot, at 2 and at 4, which means\nthat we should entertain the idea of either 1 or 3 components. I would\nbe inclined to say that the elbow at 2 is still \"too high up\" the\nmountain --- there is still some more mountain below it.\n\nThe points at 3 and 6 components look like elbows too, but they are\n*pointing the wrong way*. What you are looking for when you\nsearch for elbows are points that are the end of the mountain and the\nstart of the scree. The elbows at 2 (maybe) and 4 (definitely) are\nthis kind of thing, but the elbows at 3 and at 6 are not.\n    \n$\\blacksquare$\n\n(e) Look at the `summary` of the principal components\nobject. What light does this shed on the choice of number of\ncomponents? Explain briefly.\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(air.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          Comp.1    Comp.2    Comp.3    Comp.4     Comp.5\nStandard deviation     1.5286539 1.1772853 1.0972994 0.8526937 0.80837896\nProportion of Variance 0.3338261 0.1980001 0.1720094 0.1038695 0.09335379\nCumulative Proportion  0.3338261 0.5318262 0.7038356 0.8077051 0.90105889\n                           Comp.6     Comp.7\nStandard deviation     0.73259047 0.39484041\nProportion of Variance 0.07666983 0.02227128\nCumulative Proportion  0.97772872 1.00000000\n```\n\n\n:::\n:::\n\n     \n\nThe first component only explains 33\\% of the variability, not very\nmuch, but the first *three* components together explain 70\\%,\nwhich is much more satisfactory. So I would go with 3 components.\n\nThere are two things here: finding an elbow, *and* explaining a\nsensible fraction of the variability. You could explain more of the\nvariability by taking more components, but if you are not careful you\nend up explaining seven variables with, um, seven variables.\n\nIf you go back and look at the scree plot, you'll see that the first\nelbow is really rather high up the mountain, and it's really the\n*second* elbow that is the start of the scree.\n\nIf this part doesn't persuade you that three components is better than\none, you need to pick a number of components to use for the rest of\nthe question, and stick to it all the way through.\n    \n$\\blacksquare$\n\n(f) <a name=\"part:preferred\">*</a> How do each of your preferred number of components depend\non the variables that were measured? Explain briefly.\n\n\nSolution\n\n\nWhen this was a hand-in question, there were three marks for it,\nwhich was  a bit of a giveaway!\nOff we go:\n\n::: {.cell}\n\n```{.r .cell-code}\nair.1$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLoadings:\n                Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7\nwind             0.237  0.278  0.643  0.173  0.561  0.224  0.241\nsolar.radiation -0.206 -0.527  0.224  0.778 -0.156              \nCO              -0.551        -0.114         0.573  0.110 -0.585\nNO              -0.378  0.435 -0.407  0.291         0.450  0.461\nNO2             -0.498  0.200  0.197               -0.745  0.338\nO3              -0.325 -0.567  0.160 -0.508         0.331  0.417\nHC              -0.319  0.308  0.541 -0.143 -0.566  0.266 -0.314\n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7\nSS loadings     1.000  1.000  1.000  1.000  1.000  1.000  1.000\nProportion Var  0.143  0.143  0.143  0.143  0.143  0.143  0.143\nCumulative Var  0.143  0.286  0.429  0.571  0.714  0.857  1.000\n```\n\n\n:::\n:::\n\n     \n\nYou'll have to decide where to draw the line between \"zero\" and\n\"nonzero\". It doesn't matter so much where you put the line, so your\nanswers can differ from mine and still be correct.\n\nWe need to pick the loadings that are \"nonzero\", however we define\nthat, for example:\n\n\n\n* component 1 depends (negatively) on carbon monoxide and nitrogen dioxide.\n\n* component 2 depends (negatively) on solar radiation and ozone\nand possibly positively on nitric oxide.\n\n* component 3 depends (positively) on wind and hydrocarbons.\n\n\nIt is a good idea to translate the variable names (which are\nabbreviated) back into the long forms.\n    \n$\\blacksquare$\n\n(g) Make a data frame that contains (i) the original data, (ii) a column of row numbers, (iii) the principal component scores. Display some of it.\n\nSolution\n\n\nAll the columns contain numbers, so `cbind` will do\nit. (The component scores are seven columns, so\n`bind_cols` won't do it unless you are careful.):\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(air, air.1$scores) %>%  \n  mutate(row = row_number()) -> d\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  wind solar.radiation CO NO NO2 O3 HC      Comp.1     Comp.2     Comp.3\n1    8              98  7  2  12  8  2 -0.95292110 -0.9754106 -0.4266696\n2    7             107  4  3   9  5  3 -0.04941978 -0.4291403 -0.2925624\n3    7             103  4  3   5  6  3  0.53767776 -0.6491739 -0.5519781\n4   10              88  5  2   8 15  4 -0.37519620 -0.3609587  2.0031533\n5    6              91  4  2   8 10  3  0.19694099 -1.0955256 -0.4489016\n6    8              90  5  2  12 12  4 -1.12344250 -0.2297073  1.3544191\n     Comp.4     Comp.5     Comp.6     Comp.7 row\n1 1.4454463  2.0390917 -0.7280074 -0.5714587   1\n2 2.1070743 -0.7830499  0.1619189  0.1581294   2\n3 1.8839424 -0.7923010  1.1153956 -0.1744016   3\n4 0.1892788  0.2923780  1.4704360 -0.1020858   4\n5 0.5501374 -0.8853554  0.1186414 -0.1581801   5\n6 0.2851494 -0.4269438  0.1098276 -0.2316944   6\n```\n\n\n:::\n:::\n\n     \n\nThis is probably the easiest way, but you see that there is a mixture\nof base R and Tidyverse. The result is actually a base R `data.frame`, so displaying it will display *all* of it, hence my use of `head`.\nIf you want to do it the all-Tidyverse\nway^[There really ought to be a radio station *CTDY: All Tidyverse, All The Time*.]\nthen you need to bear in mind that `bind_cols` *only* \naccepts vectors or data frames, not matrices, so a bit of care is needed first:\n\n::: {.cell}\n\n```{.r .cell-code}\nair.1$scores %>%\n  as_tibble() %>%\n  bind_cols(air) %>%\n  mutate(row = row_number()) -> dd\ndd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 42 x 15\n    Comp.1 Comp.2 Comp.3 Comp.4  Comp.5 Comp.6 Comp.7  wind solar.radiation\n     <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> <dbl>           <dbl>\n 1 -0.953  -0.975 -0.427  1.45   2.04   -0.728 -0.571     8              98\n 2 -0.0494 -0.429 -0.293  2.11  -0.783   0.162  0.158     7             107\n 3  0.538  -0.649 -0.552  1.88  -0.792   1.12  -0.174     7             103\n 4 -0.375  -0.361  2.00   0.189  0.292   1.47  -0.102    10              88\n 5  0.197  -1.10  -0.449  0.550 -0.885   0.119 -0.158     6              91\n 6 -1.12   -0.230  1.35   0.285 -0.427   0.110 -0.232     8              90\n 7 -3.15    1.07   1.62   0.186  0.0374  1.84  -0.415     9              84\n 8 -3.98    0.926 -0.379 -0.619 -0.810  -1.29   0.735     5              72\n 9 -0.152  -0.974  0.337 -0.145  0.139  -0.681 -0.539     7              82\n10 -0.784   0.939  0.985 -0.632 -0.219  -0.303 -0.375     8              64\n# i 32 more rows\n# i 6 more variables: CO <dbl>, NO <dbl>, NO2 <dbl>, O3 <dbl>, HC <dbl>,\n#   row <int>\n```\n\n\n:::\n:::\n\n \n\nI think the best way to think about this is to start with what is\nfarthest from being a data frame or a vector (the matrix of principal\ncomponent scores, here), bash that into shape first, and then glue the\nrest of the things to it.\n\nNote that we used all Tidyverse stuff here, so the result is a\n`tibble`, and displaying it for me displays the first ten rows as\nyou'd expect. \n\n$\\blacksquare$\n\n(h) Display the row of your new data frame for the observation\nwith the smallest (most negative) score on component 1. Which row is\nthis? What makes this observation have the most negative score on\ncomponent 1?\n\nSolution\n\n\nI think the best strategy is to sort by  component 1 score (in the default ascending order), and then display the first row:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% arrange(Comp.1) %>% slice(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  wind solar.radiation CO NO NO2 O3 HC    Comp.1    Comp.2     Comp.3\n1    5              72  6  4  21 14  4 -3.981043 0.9262199 -0.3789783\n      Comp.4     Comp.5    Comp.6    Comp.7 row\n1 -0.6185173 -0.8095277 -1.290318 0.7352476   8\n```\n\n\n:::\n:::\n\n     \n\nIt's row 8.\n\nWe said earlier that component 1 depends negatively on carbon monoxide\nand nitrogen dioxide, so that an observation that is *low* on\ncomponent 1 should be *high* on these things.^[You might  have said that component 1 depended on other things as well, in  which case you ought to consider whether observation 8 is, as  appropriate, high or low on these as well.]\n\nSo are these values high or low? That was the reason for having you\nmake the five-number summary <a href=\"#part:fivenum\">here</a>. For\nobservation 8, `CO` is 6 and `NO2` is 21; looking back\nat the five-number summary, the value of `CO` is above Q3, and\nthe value of `NO2` is the highest of all. So this is entirely\nwhat we'd expect.\n\n$\\blacksquare$\n\n(i) Which observation has the lowest (most negative) value on\ncomponent 2? Which variables ought to be high or low for this\nobservation? Are they? Explain briefly.\n\nSolution\n\n\nThis is a repeat of the ideas we just saw:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% arrange(Comp.2) %>% slice(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  wind solar.radiation CO NO NO2 O3 HC     Comp.1    Comp.2    Comp.3    Comp.4\n1    6              75  4  1  10 24  3 -0.3848472 -2.331649 0.2453131 -1.765714\n      Comp.5    Comp.6    Comp.7 row\n1 -0.4522935 0.0884973 0.6669804  34\n```\n\n\n:::\n:::\n\n     \n\nand for convenience, we'll grab the quantiles again:\n\n::: {.cell}\n\n```{.r .cell-code}\nair %>% \n  summarize(across(everything(), \\(x) quantile(x)))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\ni Please use `reframe()` instead.\ni When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 7\n   wind solar.radiation    CO    NO   NO2    O3    HC\n  <dbl>           <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  5               30       2     1   5     2       2\n2  6               68.2     4     1   8     6       3\n3  8               76.5     4     2   9.5   8.5     3\n4  8.75            84.8     5     3  12    11       3\n5 10              107       7     5  21    25       5\n```\n\n\n:::\n:::\n\n \n\nDay 34 (at the end of the line). We said that component 2 depends (negatively) on solar\nradiation and ozone and possibly positively on nitric oxide. This\nmeans that day 34 ought to be *high* on the first two and low on\nthe last one (since it's at the low end of component 2). Solar\nradiation is, surprisingly, close to the median (75), but ozone, 24,\nis very near the highest, and nitric oxide, 1, is one of a large\nnumber of values equal to the lowest. So day 34 is pointing the right\nway, even if its variable values are not quite what you'd expect.\nThis business about figuring out whether values on variables are high\nor low is kind of fiddly, since you have to refer back to the\nfive-number summary to see where the values for a particular\nobservation come. Another way to approach this is to calculate\n*percentile ranks* for everything. Let's go back to our original data frame and replace everything with its percent rank: \n\n::: {.cell}\n\n```{.r .cell-code}\nair %>% mutate(across(everything(), \\(x) percent_rank(x))) -> pct_rank\npct_rank\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 42 x 7\n    wind solar.radiation    CO    NO   NO2    O3    HC\n   <dbl>           <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 0.488           0.951 0.902 0.317 0.707 0.439 0    \n 2 0.317           1     0.146 0.683 0.366 0.171 0.171\n 3 0.317           0.976 0.146 0.683 0     0.244 0.171\n 4 0.878           0.829 0.610 0.317 0.244 0.878 0.780\n 5 0.122           0.902 0.146 0.317 0.244 0.561 0.171\n 6 0.488           0.878 0.610 0.317 0.707 0.780 0.780\n 7 0.756           0.707 0.902 0.878 0.707 0.878 1    \n 8 0               0.390 0.829 0.878 1     0.829 0.780\n 9 0.317           0.659 0.610 0     0.610 0.707 0.171\n10 0.488           0.195 0.610 0.317 0.829 0.512 0.780\n# i 32 more rows\n```\n\n\n:::\n:::\n\n \n\nObservation 34 is row 34 of this:\n\n::: {.cell}\n\n```{.r .cell-code}\npct_rank %>% slice(34)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 7\n   wind solar.radiation    CO    NO   NO2    O3    HC\n  <dbl>           <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 0.122           0.463 0.146     0 0.512 0.976 0.171\n```\n\n\n:::\n:::\n\n \n\nVery high on ozone, (joint) lowest on nitric oxide, but middling on\nsolar radiation.\nThe one we looked at before, observation 8, is this:\n\n::: {.cell}\n\n```{.r .cell-code}\npct_rank %>% slice(8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 7\n   wind solar.radiation    CO    NO   NO2    O3    HC\n  <dbl>           <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     0           0.390 0.829 0.878     1 0.829 0.780\n```\n\n\n:::\n:::\n\n \n\nHigh on carbon monoxide, the highest on nitrogen dioxide.\n\n$\\blacksquare$\n\n(j) Obtain a biplot, with the row numbers labelled, and explain briefly how your conclusions from the previous two parts are consistent with it.\n\nSolution\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggbiplot(air.1, labels = d$row)\n```\n\n::: {.cell-output-display}\n![](fa_files/figure-pdf/airpollution-17-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nDay 8 is way over on the left. The things that point in the direction\nof observation 8 (`NO2, CO` and to a lesser extent `NO`\nand `HC`) are the things that observation 8 is high on. On the\nother hand, observation 8 is around the middle of the arrows for\n`wind`, `solar.radiation` and `O3`, so that day\nis not especially remarkable for those. \n\nObservation 34 is\nnearest the bottom, so we'd expect it to be high on ozone (yes), high\non solar radiation (no), low on nitric oxide (since that points the\nmost upward, yes) and also maybe low on wind, since observation 34 is\nat the \"back end\" of that arrow. Wind is 6, which is at the first\nquartile, low indeed.\n\nThe other thing that you see from the  biplot is that there are four\nvariables pointing more or less up and to the left, and at right\nangles to them, three other variables pointing up-and-right or\ndown-and-left. You could imagine rotating those arrows so that the\ngroup of 4 point upwards, and the other three point left and\nright. This is what factor analysis does, so you might imagine that\nthis technique might give a clearer picture of which variables belong\nin which factor than principal components does. Hence what follows.\n\n$\\blacksquare$\n\n(k) Run a factor analysis on the same data, obtaining two factors. Look at the factor loadings. Is it clearer which variables belong to which factor, compared to the principal components analysis? Explain briefly.\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nair.2 <- factanal(air, 2, scores = \"r\")\nair.2$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLoadings:\n                Factor1 Factor2\nwind            -0.176  -0.249 \nsolar.radiation          0.319 \nCO               0.797   0.391 \nNO               0.692  -0.152 \nNO2              0.602   0.152 \nO3                       0.997 \nHC               0.251   0.147 \n\n               Factor1 Factor2\nSS loadings      1.573   1.379\nProportion Var   0.225   0.197\nCumulative Var   0.225   0.422\n```\n\n\n:::\n:::\n\n \n\nI got the factor scores since I'm going to look at a biplot\nshortly. If you aren't, you don't need them.\n\nFactor 1 is rather more clearly carbon monoxide, nitric oxide and\nnitrogen dioxide. Factor 2 is mostly ozone, with a bit of solar\nradiation and carbon monoxide. I'd say this is clearer than before.\n\nA biplot would tell us whether the variables are better aligned with\nthe axes now:\n\n::: {.cell}\n\n```{.r .cell-code}\nbiplot(air.2$scores, air.2$loadings)\n```\n\n::: {.cell-output-display}\n![](fa_files/figure-pdf/airpollution-19-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nAt least somewhat. Ozone points straight up, since it is the dominant part of factor 2 and not part of factor 1 at all. Carbon monoxide and the two oxides of nitrogen point to the right. \n\nExtra: \n`wind`, `solar.radiation` and `HC` don't appear\nin either of our factors, which also shows up here:\n\n::: {.cell}\n\n```{.r .cell-code}\nair.2$uniquenesses\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           wind solar.radiation              CO              NO             NO2 \n      0.9070224       0.8953343       0.2126417       0.4983564       0.6144170 \n             O3              HC \n      0.0050000       0.9152467 \n```\n\n\n:::\n:::\n\n \n\nThose variables all have *high* uniquenesses.\n\nWhat with the high uniquenesses, and the fact that two factors explain\nonly 42\\% of the variability, we\nreally ought to look at 3 factors, the same way that we said we should look at\n3 components:\n\n::: {.cell}\n\n```{.r .cell-code}\nair.3 <- factanal(air, 3)\nair.3$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLoadings:\n                Factor1 Factor2 Factor3\nwind                    -0.210  -0.334 \nsolar.radiation          0.318         \nCO               0.487   0.318   0.507 \nNO               0.238  -0.269   0.931 \nNO2              0.989                 \nO3                       0.987   0.124 \nHC               0.427   0.103   0.172 \n\n               Factor1 Factor2 Factor3\nSS loadings      1.472   1.312   1.288\nProportion Var   0.210   0.187   0.184\nCumulative Var   0.210   0.398   0.582\n```\n\n\n:::\n:::\n\n \n\nIn case you are wondering, `factanal` automatically uses the\ncorrelation matrix, and so takes care of variables measured on\ndifferent scales without our having to worry about that.\n\nThe rotation has only helped somewhat here. Factor 1 is mainly\n`NO2` with some influence of `CO` and `HC`;\nfactor 2 is mainly ozone (with a bit of solar radiation and carbon monoxide),\nand factor 3 is mainly `NO` with a bit of `CO`.\n\nI think I mentioned most of the variables in there, so the uniquenesses\nshould not be too bad:\n\n::: {.cell}\n\n```{.r .cell-code}\nair.3$uniquenesses\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           wind solar.radiation              CO              NO             NO2 \n      0.8404417       0.8905074       0.4046425       0.0050000       0.0050000 \n             O3              HC \n      0.0050000       0.7776557 \n```\n\n\n:::\n:::\n\n \n\nWell, not great: `wind` and `solar.radiation` still have\nhigh uniquenesses because they are not *strongly* part of any\nfactors.\n\nIf you wanted to, you could obtain the factor scores for the 3-factor\nsolution, and plot them on a three-dimensional plot using\n`rgl`, rotating them to see the structure. A three dimensional\n\"biplot\"^[A three-dimensional biplot ought to be called a *triplot*.] \nwould also be a cool thing to look at.\n\n$\\blacksquare$\n\n\n",
    "supporting": [
      "fa_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}