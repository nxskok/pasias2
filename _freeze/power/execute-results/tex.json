{
  "hash": "a14b8cc1483299f57b0eb0ae2fa1c183",
  "result": {
    "markdown": "# Power and sample size\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Simulating power\n\n\n This question investigates power by\nsimulation.\n\n\n(a) Use `rnorm` to generate 10 random values from a\nnormal distribution with mean 20 and SD 2. Do your values look\nreasonable? Explain briefly. (You don't need to draw a graph.)\n\n\n\n(b) Estimate by simulation the power of a $t$-test to reject a\nnull hypothesis of 20 when the true mean is also 20, the\npopulation SD is 2, and the sample size is 10, against a (default)\ntwo-sided alternative. Remember the\nsteps: (i) generate a lot of random samples from the true\ndistribution, (ii) run the $t$-test with the required null mean, (iii) pull out the P-values, (iv) count how many of them are 0.05\nor less.\n\n\n\n(c) In the simulation you just did, was the null hypothesis true\nor false? Do you want to reject the null hypothesis or not?\nExplain briefly why the simulation results you got were (or were\nnot) about what you would expect.\n\n\n\n(d) By copying, pasting and editing your code from the previous\npart, estimate the power of the test of $H_0: \\mu=20$ (against a\ntwo-sided alternative) when the true population mean is 22 (rather\nthan 20).\n\n\n\n(e) Use R to calculate this power exactly (without\nsimulation). Compare the exact result with your simulation.\n\n\n\n\n\n\n\n\n##  Calculating power and sample size for estimating mean\n\n\n We are planning a study to estimate a population mean. The\npopulation standard deviation is believed to be 20, and the population\ndistribution is believed to be approximately normal. We will be\ntesting the null hypothesis that the population mean is 100. Suppose\nthe population mean is actually 110, and we want to determine how\nlikely we are to (correctly) reject the null hypothesis in this case,\nusing a two-sided (but one-sample) test with $\\alpha=0.05$.\n\n\n\n(a) We will take a sample of size $n=30$. Calculate the power of\nthis test.\n\n\n\n(b) Find the sample size necessary to obtain a power\nof at least 0.80 under these conditions. What sample size do you\nneed? Explain briefly how your answer is\nconsistent with (a).\n\n\n\n\n##  Simulating power for proportions\n\n\n In opinion surveys (and other places), we are testing for a\nproportion $p$ (for example, the proportion of people agreeing with\nsome statement). Often, we want to know whether the proportion is\n\"really\" greater than 0.5.^[That would mean assessing whether  an observed proportion could be greater than 0.5 just by chance, or  whether it is bigger enough than 0.5 to reject chance as a  plausible explanation.]  \nThat would entail testing a null\n$H_0: p=0.5$ against an alternative $H_a: p>0.5$. This is usually done\nby calculating the test statistic\n$$ z = { \\hat{p} - 0.5 \\over \\sqrt{0.25/n}},$$\nwhere $\\hat{p}$ is the observed proportion in the sample,\nand getting a P-value from the upper tail of a standard normal\ndistribution. (The 0.25 is $p(1-p)$ where $p=0.5$.) This is what\n`prop.test` does, as we investigate shortly.\n\n\n\n(a) Use `rbinom` to generate a random value from a\nbinomial distribution with $n=100$ and $p=0.6$. There are three\ninputs to `rbinom`: the first one should be the number 1, and\nthe second and third are the $n$ and $p$ of the binomial distribution.\n\n\n\n(b) Using the random binomial that you generated just above, use\n`prop.test` to test whether it could reasonably have come\nfrom a binomial population with $n=100$ and $p=0.5$, or whether $p$\nis actually bigger than 0.5. (Of course,\nyou know it actually did not come from a population with $p=0.5$.)\n`prop.test` has, for us, four inputs, thus:\n\n\n* the observed number of successes\n\n* the `n` of the binomial distribution\n\n* the null-hypothesis `p` of the binomial distribution\n\n* the alternative hypothesis, here \"greater\"\n\n\n\n\n(c) Run `prop.test` again, just as you did before, but this\ntime save the result, and extract the piece of it called\n`p.value`. Is that the P-value from your test?\n\n\n\n(d) Estimate the power of a test of\n$H_0: p=0.5$ against $H_a: p>0.5$ when $n=500$ and $p=0.56$, using\n$\\alpha=0.05$. There are three steps:\n\n\n* generate random samples from binomial\ndistributions with $n=500$ and $p=0.56$, repeated \"many\" times\n(something like 1000 or 10,000 is good)\n\n* run `prop.test` on each of those\nrandom samples \n\n* extract the P-value for each test and\nsave the results (in a column called, perhaps, `pvals`).\n\nSo I lied: the fourth and final step is to count how many of\nthose P-values are 0.05 or less.\n\n\n\n\n\n\n\n\n\n\n## Designing a study to have enough power\n\n You are designing a study to test the null hypothesis that a population mean is 0 against the alternative hypothesis that it is greater than 0. Assume that the population SD is $\\sigma=15$. It is important to detect the alternative $\\mu=2$; that is, we want to design the study so that most of the time the null hypothesis would be (correctly) rejected if in fact $\\mu=2$. A one-sample $t$-test will be used, and the data values are assumed to have a normal distribution.\n\n\n\n(a) Use simulation to estimate the power of this test when the sample size is 100. Use $\\alpha=0.05$.\n\n\n\n(b) Again by using simulation, estimate how large a sample size would be needed to obtain a power of 0.80. Show and briefly explain your process.\n\n\n\n\n\n\n## Power and $\\alpha$ in a skewed population\n\n A population of a large number of values `v` is at [http://ritsokiguess.site/datafiles/pop.csv](http://ritsokiguess.site/datafiles/pop.csv), in a CSV file. \n\n\n\n(a) Read in the population and display some of the values.\n\n\n\n\n(b) Obtain a suitable plot of your population. What do you notice?\n\n\n\n(c) If you take a sample of 10 observations from this population and run a $t$-test, how likely are you to (correctly) reject the null hypothesis $H_0: \\mu = 4$, against the alternative $H_a: \\mu > 4$? Investigate by simulation.\n\n\n\n\n\n(d) Try again with a sample size of 50 (leaving everything else the same). Explain briefly why the results so far are as you'd expect.\n\n\n\n(e) Again by simulation, estimate the probability that the null hypothesis $H_0: \\mu=5$ will be rejected when a sample of size 10 is taken from this population, in favour of the alternative $H_a: \\mu > 5$. Explain briefly why the answer is not what you would have expected, and why that happened here. (Hint: what is the population mean?)\n\n\n\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Simulating power\n\n\n This question investigates power by\nsimulation.\n\n\n(a) Use `rnorm` to generate 10 random values from a\nnormal distribution with mean 20 and SD 2. Do your values look\nreasonable? Explain briefly. (You don't need to draw a graph.)\n\n\nSolution\n\n\n`rnorm` with the number of values first, then the mean,\nthen the SD:\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx=rnorm(10,20,2)\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 21.59476 18.64044 21.83231 18.76556 18.64861 21.81889 21.62614 20.18249\n [9] 16.91266 20.63490\n```\n:::\n:::\n\n95\\% of the sampled values should be within 2 SDs of the mean, that\nis, between 16 and 24 (or 99.7\\% should be within 3 SDs of the mean,\nbetween 14 and 26). None of my values are even outside the interval 16\nto 24, though yours may be different.\n\nI saved mine in a variable and then displayed them, which you don't\nneed to do. I did because there's another way of assessing them for\nreasonableness: turn the sample into $z$-scores and see whether the\nvalues you get look like $z$-scores (that is, most of them are between\n$-2$ and 2, for example):\n\n::: {.cell}\n\n```{.r .cell-code}\n(x-20)/2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  0.79738130 -0.67977910  0.91615386 -0.61722168 -0.67569291  0.90944266\n [7]  0.81307163  0.09124563 -1.54367207  0.31744905\n```\n:::\n:::\n\nThese ones look very much like $z$-scores. This, if you think about\nit, is really the flip-side of 68--95--99.7, so it's another way of\nimplementing the same idea.\n\nYou might also think of finding the *sample* mean and SD, and\ndemonstrating that they are close to the right answers. Mine are:\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 20.06568\n```\n:::\n\n```{.r .cell-code}\nsd(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.731305\n```\n:::\n:::\n\nThe sample SD is more variable than the sample mean, so it can get\nfurther away from the population SD than the sample mean does from the\npopulation mean. \n\nThe downside to this idea is that it doesn't get at\nassessing the normality, which looking at $z$-scores or equivalent\ndoes. Maybe coupling the above with a boxplot would have helped, had I\nnot said \"no graphs\", since then you'd (hopefully) see no outliers\nand a roughly symmetric shape.\n\nThis is old-fashioned \"base R\" technology; you could do it with a\ndata frame like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tibble(x=rnorm(10,20,2))\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 1\n       x\n   <dbl>\n 1  20.6\n 2  19.3\n 3  20.2\n 4  18.7\n 5  21.3\n 6  17.8\n 7  19.8\n 8  20.9\n 9  21.7\n10  20.6\n```\n:::\n\n```{.r .cell-code}\nd %>% summarize(m=mean(x), s=sd(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n      m     s\n  <dbl> <dbl>\n1  20.1  1.21\n```\n:::\n:::\n\nThese are different random numbers, but are about equally what you'd\nexpect. (These ones are a bit less variable than you'd expect, but\nwith only ten values, don't expect perfection.)\n\nSome discussion about the kind of values you should get, and whether\nor not you get them, is what is called for here. I want you to say\nsomething convincing about how the values you get come from a normal\ndistribution with mean 20 *and* SD 2.  \"Close to 20\" is not the\nwhole answer here, because that doesn't get at \"how close to 20?\":\nthat is, it talks about the mean but not about the SD.\n\n\n$\\blacksquare$\n\n(b) Estimate by simulation the power of a $t$-test to reject a\nnull hypothesis of 20 when the true mean is also 20, the\npopulation SD is 2, and the sample size is 10, against a (default)\ntwo-sided alternative. Remember the\nsteps: (i) generate a lot of random samples from the true\ndistribution, (ii) run the $t$-test with the required null mean, (iii) pull out the P-values, (iv) count how many of them are 0.05\nor less.\n\n\nSolution\n\n\nOnce you get the hang of these, they all look almost the\nsame. This one is easier than some because we don't have to do\nanything special to get a two-sided alternative hypothesis. The initial setup is to make a dataframe with a column called something like `sim` to label the simulations, and then a `rowwise` to generate one random sample, $t$-test and P-value for each simulation:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %>% \n  mutate(t_test = list(t.test(norm_sample, mu = 20))) %>% \n  mutate(pval = t_test$p.value) %>% \n  count(pval <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `pval <= 0.05`     n\n  <lgl>          <int>\n1 FALSE            958\n2 TRUE              42\n```\n:::\n:::\n\n\n    \n\nThe power is about 4.2\\%. This seems depressingly small, but see the\nnext part. (Are you confused about something in this one? You have a right to be.)\n\n\n$\\blacksquare$\n\n(c) In the simulation you just did, was the null hypothesis true\nor false? Do you want to reject the null hypothesis or not?\nExplain briefly why the simulation results you got were (or were\nnot) about what you would expect.\n\n\nSolution\n\n\nThe null mean and the true  mean were both 20: that is, the\nnull hypothesis was correct, and rejecting it would be a\nmistake, to be precise a type I error. We were doing the test\nat $\\alpha=0.05$ (by comparing our collection of simulated\nP-values with 0.05), so we should be making a type I error 5\\%\nof the time. This is entirely in line with the 4.2\\% of\n(wrong) rejections that I had.\nYour estimation is likely to be different from mine, but you\nshould be rejecting about 5\\% of the time. If your result is\nvery different from 5\\%, that's an invitation to go back and\ncheck your code. On the other hand, if it *is* about 5\\%,\nthat ought to give you confidence to go on and use the same\nideas for the next part.\n\n\n$\\blacksquare$\n\n(d) By copying, pasting and editing your code from the previous\npart, estimate the power of the test of $H_0: \\mu=20$ (against a\ntwo-sided alternative) when the true population mean is 22 (rather\nthan 20).\n\n\nSolution\n\n\nHere's the code we just used:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %>% \n  mutate(t_test = list(t.test(norm_sample, mu = 20))) %>% \n  mutate(pval = t_test$p.value) %>% \n  count(pval <= 0.05)\n```\n:::\n\nOne of those 20s needs to become 22. Not the one in the\n`t.test`, since the hypotheses have not changed.  So we need to\nchange the 20 in the `rnorm` line to 22, since that's where\nwe're generating data from the true distribution. The rest of it stays\nthe same:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %>% \n  mutate(t_test = list(t.test(norm_sample, mu = 22))) %>% \n  mutate(pval = t_test$p.value) %>% \n  count(pval <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `pval <= 0.05`     n\n  <lgl>          <int>\n1 FALSE            192\n2 TRUE             808\n```\n:::\n:::\n         \n\nThis time, we *want* to reject, since the null hypothesis is\nfalse. So look at the `TRUE` count: the power is about\n$80\\%$. We are very likely to correctly reject a null\nof 20 when the mean is actually 22.\n\nExtra: another way to reason that the power should be fairly large is to\nthink about what kind of sample you are likely to get from the true\ndistribution: one with a mean around 22 and an SD around 2. Thus the\n$t$-statistic should be somewhere around this (we have a sample size\nof 10):\n\n::: {.cell}\n\n```{.r .cell-code}\nt_stat=(22-20)/(2/sqrt(10))\nt_stat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.162278\n```\n:::\n:::\n\nand the two-sided P-value should be about\n\n::: {.cell}\n\n```{.r .cell-code}\n2*(1-pt(t_stat,10-1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01150799\n```\n:::\n:::\n\nOf course, with your actual data, you will sometimes be less lucky\nthan this (a sample mean nearer 20 or a larger sample SD), but\nsometimes you will be luckier. But the suggestion is that most of the\ntime, the P-value will be pretty small and you will end up correctly\nrejecting. \n\nThe quantity `t_stat` above, 3.16, is known to some\npeople as an \"effect size\", and summarizes how far apart the null\nand true means are, relative to the amount of variability present (in\nthe sampling distribution of the sample mean). As effect sizes go,\nthis one is pretty large.\n\n\n$\\blacksquare$\n\n(e) Use R to calculate this power exactly (without\nsimulation). Compare the exact result with your simulation.\n\n\nSolution\n\n\nThis is `power.t.test`. The quantity `delta` is\nthe difference between true and null means:\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(n=10,delta=22-20,sd=2,type=\"one.sample\",alternative=\"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n     One-sample t test power calculation \n\n              n = 10\n          delta = 2\n             sd = 2\n      sig.level = 0.05\n          power = 0.8030962\n    alternative = two.sided\n```\n:::\n:::\n\nThis, 0.803, is very close to the value I got from my\nsimulation. Which makes me think I did them both right. This is not a watertight proof, though: for example, I might have made a mistake and gotten lucky somewhere. But it does at least give me confidence.\n\nExtra: when you estimate power by simulation, what you are doing is\nrejecting or not with a certain probability (which is the same for all\nsimulations). So the number of times you actually *do* reject has\na binomial distribution with $n$ equal to the number of simulated\nP-values you got (1000 in my case; you could do more) and a $p$ that\nthe simulation is trying to estimate. This is inference for a\nproportion, exactly what `prop.test` does.\n\nRecall that `prop.test` has as input:\n\n\n* a number of \"successes\" (rejections of the null in our case)\n\n* the number of trials (simulated tests)\n\n* the null-hypothesis value of `p` (optional if you only\nwant a CI)\n\n* (optional) a confidence level `conf.level`.\n\n\nIn part (b), we knew that the probability of\n(incorrectly) rejecting should have been 0.05 and we rejected 42 times\nout of 1000:\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(42,1000,0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t1-sample proportions test with continuity correction\n\ndata:  42 out of 1000, null probability 0.05\nX-squared = 1.1842, df = 1, p-value = 0.2765\nalternative hypothesis: true p is not equal to 0.05\n95 percent confidence interval:\n 0.03079269 0.05685194\nsample estimates:\n    p \n0.042 \n```\n:::\n:::\n\n\nLooking at the P-value, we definitely fail to reject that the\nprobability of (incorrectly) rejecting is the 0.05 that it should\nbe. Ouch. That's true, but unnecessarily confusing. Look at the\nconfidence interval instead, 0.031 to 0.057. The right answer is 0.05,\nwhich is inside that interval, so good.\n\nIn part (c), we didn't know what the power was going\nto be (not until we calculated it with `power.t.test`, anyway),\nso we go straight for a confidence interval; the default 95\\% confidence\nlevel is fine. We (correctly) rejected 798 times out of 1000:\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(798,1000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t1-sample proportions test with continuity correction\n\ndata:  798 out of 1000, null probability 0.5\nX-squared = 354.02, df = 1, p-value < 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.7714759 0.8221976\nsample estimates:\n    p \n0.798 \n```\n:::\n:::\n\nI left out the 3rd input since we're not doing a test, and ignore the\nP-value that comes out. (The default null proportion is 0.5, which\noften makes sense, but not here.)\n\nAccording to the confidence interval, the estimated power is between\n0.771 and 0.822. This interval definitely includes what we now know is\nthe right answer of 0.803.\n\nThis might be an accurate enough assessment of the power for you, but\nif not, you can do more simulations, say 10,000:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(norm_sample = list(rnorm(10, 20, 2))) %>% \n  mutate(t_test = list(t.test(norm_sample, mu = 22))) %>% \n  mutate(pval = t_test$p.value) %>% \n  count(pval <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `pval <= 0.05`     n\n  <lgl>          <int>\n1 FALSE           1923\n2 TRUE            8077\n```\n:::\n:::\n\n\nI copied and pasted my code again, which means that I'm dangerously\nclose to turning it into a function, but anyway.\n\nThe confidence interval for the power is then\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(7996,10000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t1-sample proportions test with continuity correction\n\ndata:  7996 out of 10000, null probability 0.5\nX-squared = 3589.2, df = 1, p-value < 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.7915892 0.8073793\nsample estimates:\n     p \n0.7996 \n```\n:::\n:::\n\nthat is, from 0.792 to 0.807, which once again includes the right\nanswer of 0.803. The first interval, based on 1,000 simulations, has\nlength 0.051, while this interval has length 0.015.  The first\ninterval is more than three times as long as the second, which is\nabout what you'd expect since the first one is based on 10 times fewer\nsimulations, and thus ought to be a factor of $\\sqrt{10}\\simeq 3.16$\ntimes longer.\n\nThis means that you can estimate power as accurately as you like by\ndoing a large enough (possibly very large) number of simulations. Provided, that is, that\nyou are prepared to wait a possibly long time for it to finish working!\n\n$\\blacksquare$\n\n\n\n\n\n\n##  Calculating power and sample size for estimating mean\n\n\n We are planning a study to estimate a population mean. The\npopulation standard deviation is believed to be 20, and the population\ndistribution is believed to be approximately normal. We will be\ntesting the null hypothesis that the population mean is 100. Suppose\nthe population mean is actually 110, and we want to determine how\nlikely we are to (correctly) reject the null hypothesis in this case,\nusing a two-sided (but one-sample) test with $\\alpha=0.05$.\n\n\n\n(a) We will take a sample of size $n=30$. Calculate the power of\nthis test.\n\n\nSolution\n\n\n`power.t.test`. Fill in: sample size `n`, difference\nin means `delta` ($10=110-100$), population SD `sd`,\ntype of test `type` (`one.sample`) and kind of\nalternative hypothesis `alternative`\n(`two.sided`). Leave out `power` since that's what\nwe want:\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(n=30,delta=10,sd=20,type=\"one.sample\",alternative=\"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n     One-sample t test power calculation \n\n              n = 30\n          delta = 10\n             sd = 20\n      sig.level = 0.05\n          power = 0.7539627\n    alternative = two.sided\n```\n:::\n:::\n\nI meant \"calculate\" exactly rather than \"estimate\" (by\nsimulation). Though if you want to, you can do that as well, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(samples = list(rnorm(30, 110, 20))) %>% \n  mutate(ttest = list(t.test(samples, mu= 100))) %>% \n  mutate(pvals = ttest$p.value) %>% \n  count(pvals<=0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `pvals <= 0.05`     n\n  <lgl>           <int>\n1 FALSE             257\n2 TRUE              743\n```\n:::\n:::\n\nThat came out alarmingly close to the exact answer. \n\n\n$\\blacksquare$\n\n(b) Find the sample size necessary to obtain a power\nof at least 0.80 under these conditions. What sample size do you\nneed? Explain briefly how your answer is\nconsistent with (a).\n\n\nSolution\n\n\nAgain, the implication is \"by calculation\".\nThis time, in `power.t.test`, put in 0.80 for\n`power` and leave out `n`. The order of things\ndoesn't matter (since I have named everything that's going into\n`power.t.test`): \n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(delta=10,power=0.80,sd=20,type=\"one.sample\",alternative=\"two.sided\")  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n     One-sample t test power calculation \n\n              n = 33.3672\n          delta = 10\n             sd = 20\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n```\n:::\n:::\n\nTo get sample size for power at least 0.80, we have to round 33.36\n*up* to the next whole number, ie.\\ $n=34$ is needed. (A sample\nof size 33 wouldn't quite have enough power.)\n\nThis answer is consistent with (a) because a sample size of 30 gave a\npower a bit less than 0.80, and so to increase the power by a little\n(0.75 to 0.80),\nwe had to increase the sample size by a little (30 to 34).\n\nExtra: estimating sample sizes by simulation is tricky, because the sample size\nhas to be input to the simulation. That means your only strategy is to\ntry different sample sizes until you find one that gives the right power.\n\nIn this case, we know that a sample of size 30 doesn't give quite\nenough power, so we have to up the sample size a bit. How about we try\n40? I copied and pasted my code from above and changed 30 to 40:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(samples = list(rnorm(40, 110, 20))) %>% \n  mutate(ttest = list(t.test(samples, mu= 100))) %>% \n  mutate(pvals = ttest$p.value) %>% \n  count(pvals<=0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `pvals <= 0.05`     n\n  <lgl>           <int>\n1 FALSE             130\n2 TRUE              870\n```\n:::\n:::\n\nNow the power is a bit too big, so we don't need a sample size quite\nas big as 40. So probably our next guess would be 35. But before we\ncopy and paste again, we should be thinking about making a function of\nit first, with the sample size as input. Copy-paste once more and edit:\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_power=function(n) {\n  tibble(sim = 1:1000) %>% \n    rowwise() %>% \n    mutate(samples = list(rnorm(n, 110, 20))) %>% \n    mutate(ttest = list(t.test(samples, mu= 100))) %>% \n    mutate(pvals = ttest$p.value) %>% \n    ungroup() %>% \n    count(pvals<=0.05)\n}\n```\n:::\n\nIn the grand scheme of things, we might want to have the null and true\nmeans, population SD and $\\alpha$ be inputs to the function as well,\nso that we have a more general tool, but this will do for now.\n\n\n\nLet's run it with a sample size of 35:\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_power(35)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n  `pvals <= 0.05`     n\n  <lgl>           <int>\n1 FALSE             183\n2 TRUE              817\n```\n:::\n:::\n\nand I'm going to call that good. (Because there is randomness in the\nestimation of the power, don't expect to get *too* close to the\nright answer. This one came out a fair bit less than the right answer;\nthe power for $n=35$ should be a bit *more* than 0.80.)\n\nNow that you have the software to do it, you can see that figuring out\na sample size like this, at least roughly, won't take very long: each\none of these simulations takes maybe seconds to run, and all you have\nto do is copy and paste the previous one, and edit it to contain the\nnew sample size before running it again. You're making the computer\nwork hard while you lazily sip your coffee, but there's no harm in\nthat: programmer's brain cells are more valuable than computer CPU\ncycles, and you might as well save your brain cells for when you\nreally need them.\n\nYou might even think about automating this further. The easiest way, now that we have the function, is something like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(ns = seq(20, 50, 5)) %>% \n  rowwise() %>% \n  mutate(power_tab = list(sim_power(ns))) %>% \n  unnest(power_tab) %>% \n  pivot_wider(names_from = `pvals <= 0.05`, values_from = n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 x 3\n     ns `FALSE` `TRUE`\n  <dbl>   <int>  <int>\n1    20     402    598\n2    25     321    679\n3    30     258    742\n4    35     188    812\n5    40     132    868\n6    45     100    900\n7    50      89    911\n```\n:::\n:::\n\nThe business end of this is happening in the first three lines. I wasn't thinking of this when I originally wrote `sim_power` to return a dataframe, so there is a  bit more fiddling after the simulations are done: I have to `unnest` to see what the list-column `power_tab` actually contains, and because of the layout of the output from unnesting `sim_power` (long format), it looks better if I pivot it wider, so that I can just cast my eye down the TRUE column and see the power increasing as the sample size increases.\n\nYou might  also think of something like bisection to find the sample size that has power 0.8, but it starts getting tricky because of the randomness; just by chance, it may be that sometimes the simulated power goes *down* as the sample size goes up. With 1000 simulations each time, it seems that the power ought to hit 80% with a sample size between 30 and 35.\n\n\n\n\n##  Simulating power for proportions\n\n\n In opinion surveys (and other places), we are testing for a\nproportion $p$ (for example, the proportion of people agreeing with\nsome statement). Often, we want to know whether the proportion is\n\"really\" greater than 0.5.^[That would mean assessing whether  an observed proportion could be greater than 0.5 just by chance, or  whether it is bigger enough than 0.5 to reject chance as a  plausible explanation.]  \nThat would entail testing a null\n$H_0: p=0.5$ against an alternative $H_a: p>0.5$. This is usually done\nby calculating the test statistic\n$$ z = { \\hat{p} - 0.5 \\over \\sqrt{0.25/n}},$$\nwhere $\\hat{p}$ is the observed proportion in the sample,\nand getting a P-value from the upper tail of a standard normal\ndistribution. (The 0.25 is $p(1-p)$ where $p=0.5$.) This is what\n`prop.test` does, as we investigate shortly.\n\n\n\n(a) Use `rbinom` to generate a random value from a\nbinomial distribution with $n=100$ and $p=0.6$. There are three\ninputs to `rbinom`: the first one should be the number 1, and\nthe second and third are the $n$ and $p$ of the binomial distribution.\n\n\nSolution\n\n\nI am doing some preparatory work that you don't need to do:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(457299)\n```\n:::\n\n \n\nBy setting the \"seed\" for the random number generator, I guarantee\nthat I will get the same answers every time I run my code below (and\ntherefore I can talk about my answers without worrying that they will\nchange). Up to you whether you do this. You can \"seed\" the random\nnumber generator with any number you like. A lot of people use\n`1`. Mahinda seems to like `123`. Mine is an old phone\nnumber. \n\nAnd so to work:\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(1, 100, 0.6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 60\n```\n:::\n:::\n\n\n\nI got exactly 60\\% successes this time. You probably won't get exactly\n60, but you should get somewhere close. (If you use my random number\nseed and use the random number generator exactly the same way I did,\nyou should get the same values I did.)\n\nFor fun, you can see what happens if you change the 1:\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(3, 100, 0.6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 58 57 55\n```\n:::\n:::\n\n \n\nThree random binomials, that happened to come out just below 60. We're\ngoing to leave the first input as 1, though, and let `rowwise`\nhandle \"lots of sampled values\" later.\n    \n \n$\\blacksquare$\n\n(b) Using the random binomial that you generated just above, use\n`prop.test` to test whether it could reasonably have come\nfrom a binomial population with $n=100$ and $p=0.5$, or whether $p$\nis actually bigger than 0.5. (Of course,\nyou know it actually did not come from a population with $p=0.5$.)\n`prop.test` has, for us, four inputs, thus:\n\n\n* the observed number of successes\n\n* the `n` of the binomial distribution\n\n* the null-hypothesis `p` of the binomial distribution\n\n* the alternative hypothesis, here \"greater\"\n\n\n\nSolution\n\n\nI got exactly 60 successes, so I do this:\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(60, 100, 0.5, alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t1-sample proportions test with continuity correction\n\ndata:  60 out of 100, null probability 0.5\nX-squared = 3.61, df = 1, p-value = 0.02872\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.5127842 1.0000000\nsample estimates:\n  p \n0.6 \n```\n:::\n:::\n\n     \n\nThe P-value should at least be fairly small, since 60 is a bit bigger\nthan 50. (Think about tossing a coin 100 times; would 60 heads make\nyou doubt the coin's fairness? The above says it should.)\n    \n\n$\\blacksquare$\n\n(c) Run `prop.test` again, just as you did before, but this\ntime save the result, and extract the piece of it called\n`p.value`. Is that the P-value from your test?\n\n\nSolution\n\n\nCopying and pasting:\n\n::: {.cell}\n\n```{.r .cell-code}\np_test <- prop.test(60, 100, 0.5, alternative = \"greater\")\np_test$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02871656\n```\n:::\n:::\n\n \n\nYep, the same.\n\n$\\blacksquare$\n\n(d) Estimate the power of a test of\n$H_0: p=0.5$ against $H_a: p>0.5$ when $n=500$ and $p=0.56$, using\n$\\alpha=0.05$. There are three steps:\n\n\n* generate random samples from binomial\ndistributions with $n=500$ and $p=0.56$, repeated \"many\" times\n(something like 1000 or 10,000 is good)\n\n* run `prop.test` on each of those\nrandom samples \n\n* extract the P-value for each test and\nsave the results (in a column called, perhaps, `pvals`).\n\nSo I lied: the fourth and final step is to count how many of\nthose P-values are 0.05 or less.\n\n\nSolution\n\nThe first part of the first step is to create a column called something like `sim` that labels each simulated sample, and to make sure that everything happens rowwise. After that, you follow the procedure:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(sample = rbinom(1, 500, 0.56)) %>% \n  mutate(test = list(prop.test(sample, 500, 0.5, alternative = \"greater\"))) %>% \n  mutate(pvals = test$p.value) %>% \n  count(pvals <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `pvals <= 0.05`     n\n  <lgl>           <int>\n1 FALSE             143\n2 TRUE              857\n```\n:::\n:::\n\n\n\nThe previous parts, using `rbinom` and `prop.test`,\nwere meant to provide you with the ingredients for this part.\nThe first step is to use `rbinom`. The first input is 1 since\nwe only want one random binomial each time (the `rowwise` will\nhandle the fact that you actually want lots of them; you only want one *per row* since you are working rowwise). The second step\nruns `prop.test`; the first input to that is each one of the\nnumbers of successes from the first step. The last part is to pull out all the P-values and\nmake a table of them, just like the example in lecture.\n\n\n \n\nThe estimated power is about 85\\%. That is, if $p$ is actually 0.56\nand we have a sample of size 500, we have a good chance of (correctly)\nrejecting that $p=0.5$. \n\n\nThe moral of this story is that when you have a decently large sample,\n$n=500$ in this case, $p$ doesn't have to get very far away from 0.5\nbefore you can correctly reject 0.5.  Bear in mind that sample sizes\nfor estimating proportions need to be larger than those for estimating\nmeans, so $n=500$ is large without being huge.  The practical upshot\nis that if you design a survey and give it to 500 (or more) randomly\nchosen people, the proportion of people in favour doesn't have to be\nmuch above 50\\% for you to correctly infer that it *is* above\n50\\%, most of the time.\n\n$\\blacksquare$\n\n\n\n\n\n\n\n\n## Designing a study to have enough power\n\n You are designing a study to test the null hypothesis that a population mean is 0 against the alternative hypothesis that it is greater than 0. Assume that the population SD is $\\sigma=15$. It is important to detect the alternative $\\mu=2$; that is, we want to design the study so that most of the time the null hypothesis would be (correctly) rejected if in fact $\\mu=2$. A one-sample $t$-test will be used, and the data values are assumed to have a normal distribution.\n\n\n\n(a) Use simulation to estimate the power of this test when the sample size is 100. Use $\\alpha=0.05$.\n\nSolution\n\n\n::: {.cell}\n\n:::\n\nUse at least 1000 simulations (more, if you're willing to wait for it). In `rnorm`, the sample size is first, then the (true) population mean, then the (assumed) population SD:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(rnorm(100, 2, 15))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE               639\n2 TRUE                361\n```\n:::\n:::\n\n\n\n\nThe power is (estimated as) a disappointing 0.361. Your answer won't (most likely) be the same as this, but it should be somewhere close. I would like to see you demonstrate that you know what power is, for example \"if the population mean is actually 2, the null hypothesis $H_0: \\mu = 0$, which is wrong, will only be rejected about 36% of the time\".^[This is why I called my result disappointing. I would like to reject a lot more of the time than this, but, given that the truth was not very far away from the null given the (large) population SD, I can't. See Extra 1.]\n\nThe test we are doing is one-sided, so you need the `alternative` in there. If you omit it, you'll have the answer to a different problem:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(rnorm(100, 2, 15))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 0))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE               740\n2 TRUE                260\n```\n:::\n:::\n\nThis is the probability that you reject $H_0: \\mu=0$ in favour of $H_a: \\mu \\ne 0$. This is smaller, because the test is \"wasting effort\" allowing the possibility of rejecting when the sample mean is far enough *less* than zero, when most of the time the samples drawn from the true distribution have mean greater than zero. (If you get a sample mean of 2.5, say, the P-value for a one-sided test will be smaller than for a two-sided one.)\n\nExtra 1:\n\nThis low power of 0.361 is because the population SD is large relative to the kind of difference from the null that we are hoping to find. To get a sense of how big the power might be, imagine you draw a \"typical\" sample from the true population: it will have a sample mean of 2 and a sample SD of 15, so that $t$ will be about\n\n::: {.cell}\n\n```{.r .cell-code}\n(2-0)/(15/sqrt(100))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.333333\n```\n:::\n:::\n\nYou won't reject with this ($t$ would have to be bigger than 2), so in the cases where you *do* reject, you'll have to be more lucky: you'll need a sample mean bigger than 2, or a sample SD smaller than 15. So the power won't be very big, less than 0.5, because about half the time you'll get a test statistic less than 1.33 and about half the time more, and not all of *those* will lead to rejection.\n\nExtra 2:\n\nThis is exactly the situation where `power.t.test` works, so we can get the exact answer (you need all the pieces):\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(n=100, delta=2-0, sd=15, type=\"one.sample\", \nalternative = \"one.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n     One-sample t test power calculation \n\n              n = 100\n          delta = 2\n             sd = 15\n      sig.level = 0.05\n          power = 0.3742438\n    alternative = one.sided\n```\n:::\n:::\n\nYour answer, from 1000 simulations, should be within about 3 percentage points of that. (Mine was only about 1 percentage point off.)\n\n\n$\\blacksquare$\n\n\n(b) Again by using simulation, estimate how large a sample size would be needed to obtain a power of 0.80. Show and briefly explain your process.\n\nSolution\n\n\nThe point of this one is the process as well as the final answer, so you need to show and justify what you are doing. Showing only a final answer *does not* show that you know how to do it. The *whole point* of this one is to make mistakes and fix them!\n\nThe simulation approach does not immediately give you a sample size for fixed power, so what you have to do is to try different sample sizes until you get one that gives a power close enough to 0.80. You have to decide what \"close enough\" means for you, given that the simulations have randomness in them. I'm going to use 10,000 simulations for each of my attempts, in the hope of getting a more accurate answer.\n\nFirst off, for a sample size of 100, the power was too small, so the answer had better be bigger than 100. I'll try 200. For these, copy and paste the code, changing the sample size each time:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(rnorm(200, 2, 15))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE              4064\n2 TRUE               5936\n```\n:::\n:::\n\n\n\nA sample size of 200 isn't big enough yet. I'll double again to 400:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(rnorm(400, 2, 15))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE              1582\n2 TRUE               8418\n```\n:::\n:::\n\nGetting closer. 400 is too big, but closer than 200. 350?\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(rnorm(350, 2, 15))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE              1932\n2 TRUE               8068\n```\n:::\n:::\n\nClose! I reckon you could call that good (see below), or try again with a sample size a bit less than 350:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(rnorm(345, 2, 15))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE              2124\n2 TRUE               7876\n```\n:::\n:::\n\n\n\n340 is definitely too small:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(rnorm(340, 2, 15))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 0, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE              2157\n2 TRUE               7843\n```\n:::\n:::\n\n\n\nThis is actually not as close as I was expecting. I think we are getting close to simulation accuracy for this number of simulations. If we do 10,000 simulations of an event with probability 0.8 (correctly rejecting this null), below are the kind of results we might get.^[If the power is really 0.8, the number of simulated tests that end up rejecting has a binomial distribution with n of 10000 and p of 0.80.] This is the middle 95% of that distribution.\n\n::: {.cell}\n\n```{.r .cell-code}\nqbinom(c(0.025,0.975), 10000, 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7921 8078\n```\n:::\n:::\n\nAnything between those limits is the kind of thing we might get by chance, so simulation doesn't let us distinguish between 347 and 350 as the correct sample size. Unless we do more than 10,000 simulations, of course!\n\nIf you stuck with 1000 simulations each time, these are the corresponding limits:\n\n::: {.cell}\n\n```{.r .cell-code}\nqbinom(c(0.025,0.975), 1000, 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 775 824\n```\n:::\n:::\n\nand any sample sizes that produce an estimated power between these are as accurate as you'll get. (Here you see the advantage of doing more simulations.)\n\nIf you've been using 10,000 simulations each time like me, you'll have noticed that these actually take a noticeable time to run. This is why coders always have a coffee or something else to sip on while their code runs; coders, like us, need to see the output to decide what to do next. Or you could install the [beepr package](https://www.r-project.org/nosvn/pandoc/beepr.html), and get some kind of sound when your simulation finishes, so that you'll know to get off Twitter^[Or Reddit or Quora or whatever your favourite time-killer is.] and see what happened. There are also packages that will send you a text message or will send a notification to all your devices.\n\nWhat I want to see from you here is some kind of trial and error that proceeds logically, sensibly increasing or decreasing the sample size at each trial, until you have gotten reasonably close to power 0.8.\n\nExtra: once again we can figure out the correct answer:\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(power = 0.80, delta=2-0, sd=15, type=\"one.sample\", \nalternative = \"one.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n     One-sample t test power calculation \n\n              n = 349.1256\n          delta = 2\n             sd = 15\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n```\n:::\n:::\n\nThis does not answer the question, though, since you need to do it by simulation with trial and error. If you want to do it this way, do it at the *end* as a check on your work; if the answer you get this way is very different from the simulation results, that's an invitation to check what you did.\n\n350 actually *is* the correct answer. But you will need to try different sample sizes until you get close enough to a power of 0.8; simply doing it for $n=350$ is not enough, because how did you know to try 350 and not some other sample size?\n\n\n$\\blacksquare$\n\n\n\n\n\n## Power and $\\alpha$ in a skewed population\n\n A population of a large number of values `v` is at [http://ritsokiguess.site/datafiles/pop.csv](http://ritsokiguess.site/datafiles/pop.csv), in a CSV file. \n\n\n\n(a) Read in the population and display some of the values.\n\nSolution\n\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/pop.csv\"\npop <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 10000 Columns: 1\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (1): v\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\npop\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10,000 x 1\n       v\n   <dbl>\n 1  9.97\n 2  2.18\n 3  6.20\n 4  2.11\n 5  6.30\n 6  1.54\n 7  5.77\n 8  2.94\n 9 16.8 \n10  1.95\n# i 9,990 more rows\n```\n:::\n:::\n\n10,000 values. A large population. (From these few values, `v` seems to be positive but rather variable.)\n\n\n$\\blacksquare$\n\n\n\n(b) Obtain a suitable plot of your population. What do you notice?\n\nSolution\n\n\nOne quantitative variable, so a histogram. The population is large, so you can use more bins than usual. Sturges' rule says 14 bins (the logarithm below is base 2, or, the next power of 2 above 10,000 is 16,384 which is $2^{14}$):\n\n::: {.cell}\n\n```{.r .cell-code}\nlog(10000, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 13.28771\n```\n:::\n\n```{.r .cell-code}\n2^14\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 16384\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pop, aes(x=v)) + geom_histogram(bins=14)\n```\n\n::: {.cell-output-display}\n![](power_files/figure-pdf/pop-power-4-1.pdf){fig-pos='H'}\n:::\n:::\n\nPick a number of bins: the default 30 bins is pretty much always too many. Any number of bins that shows this shape is good as an answer, but you also need to display some thinking about how many bins to use, either starting with a rule as I did, or experimenting with different numbers of bins. Rules are not hard and fast; it so happened that I liked the picture that 14 bins gave, so I stopped there. Thirty bins, the default, is actually not bad here:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pop, aes(x=v)) + geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](power_files/figure-pdf/pop-power-5-1.pdf){fig-pos='H'}\n:::\n:::\n\nbut if you do this, you need to say something that indicates some conscious thought, such as saying \"this number of bins gives a good picture of the shape of the distribution\", which I am OK with. *Have a reason for doing what you do*.\n\n\nThis is skewed to the right, or has a long right tail. This is a better description than \"outliers\": there are indeed some very large values (almost invisible on the histogram), but to say that is to imply that the rest of the distribution apart from the outliers has a regular shape, not something you can say here.^[The question to ask yourself is whether the shape comes from the entire distribution, as it does here (skewness), or whether it comes from a few unusual observations (outliers).]\n\nExtra: The issue that's coming up is whether this is normally-distributed, which of course it is not. This is a normal quantile plot. (Idea: if the points follow the line, at least approximately, the variable is normally distributed; if not, not.):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pop, aes(sample=v)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](power_files/figure-pdf/pop-power-6-1.pdf){fig-pos='H'}\n:::\n:::\n\nThat is your archetypal skewed-to-right. The two highest values are not a lot higher than the rest, again supporting a curved shape overall (skewness) on this plot, rather than the problem being outliers. (The highest values are consistent with the shape of the curve, rather than being unusually high compared to the curve.)\n\n\n$\\blacksquare$\n\n\n(c) If you take a sample of 10 observations from this population and run a $t$-test, how likely are you to (correctly) reject the null hypothesis $H_0: \\mu = 4$, against the alternative $H_a: \\mu > 4$? Investigate by simulation.\n\n\n\nSolution\n\n\nAs you noted, this is a one-sided alternative, so make sure your code does the right thing. Take a lot of random samples, run the $t$-test on each one, grab the P-value each time, count the number of P-values less or equal to your $\\alpha$. This is *not* a bootstrap, so the sampling needs to be *without* replacement, and you need to say how big the sample is:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(pop$v, 10))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 4, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE               830\n2 TRUE                170\n```\n:::\n:::\n\nThe estimated power is only about 0.19.\n\nAs to the code, well, the samples and the $t$-test both consist of more than one thing, so in the `mutate`s that create them, don't forget the `list` around the outside, which will create a list-column.\n\nHere, and elsewhere in this question, use at least 1000 simulations. More will give you more accurate results, but you'll have to wait longer for it to run. Your choice.\n\nAs a final remark, you *can not* do this one by algebra, as you might have done in other courses, because you do not know the functional form of the population distribution. The power calculations you may have done before as calculations typically assume a normal population, because if you don't, the algebra gets too messy too fast. (You'd need to know the distribution of the test statistic under the *alternative* hypothesis, which in cases beyond the normal is not usually known.)\n\n\n$\\blacksquare$\n\n\n(d) Try again with a sample size of 50 (leaving everything else the same). Explain briefly why the results so far are as you'd expect.\n\nSolution\n\n\nFor the code, this is copy-paste-edit. Just change the sample size:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(pop$v, 50))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 4, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE               262\n2 TRUE                738\n```\n:::\n:::\n\nThe power is now much bigger, around 73%. This is as expected because with a larger sample size we should be more likely to reject a false null hypothesis. \n\nThe reason for this is that the mean of a bigger sample should be closer to the population mean, because of the Law of Large Numbers, and thus further away from the incorrect null hypothesis and more likely far enough away to reject it. In this case, as you will see shortly, the population mean is 5, and so, with a bigger sample, the sample mean will almost certainly be closer to 5 and further away from 4.\n\nI have a feeling you could formalize this kind of argument with Chebyshev's inequality, which would apply to any kind of population.^[It has to have a standard deviation, though, but our population seems well-enough behaved to have a standard deviation.] I think I'd have to write it down to get it right, though.\n\n\n$\\blacksquare$\n\n\n(e) Again by simulation, estimate the probability that the null hypothesis $H_0: \\mu=5$ will be rejected when a sample of size 10 is taken from this population, in favour of the alternative $H_a: \\mu > 5$. Explain briefly why the answer is not what you would have expected, and why that happened here. (Hint: what is the population mean?)\n\nSolution\n\n\nTaking the hint first:\n\n::: {.cell}\n\n```{.r .cell-code}\npop %>% \nsummarize(m = mean(v))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 1\n      m\n  <dbl>\n1  5.00\n```\n:::\n:::\n\n(I'm hoping that some light dawns at this point), and copy-paste-edit your simulation code again, this time changing the null mean to 5:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(pop$v, 10))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 5, alternative = \"greater\"))) %>% \n  mutate(p_value = t_test$p.value) %>% \n  count(p_value <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_value <= 0.05`     n\n  <lgl>             <int>\n1 FALSE               980\n2 TRUE                 20\n```\n:::\n:::\n\nThe \"power\" is estimated to be 0.020. (Again, your value won't be exactly this, most likely, but it should be somewhere close.)\n\nSo what were we expecting? This time, the null hypothesis, that the population mean is 5, is actually *true*. So rejecting it is now a *type I error*, and the probability of that should be $\\alpha$, which was 0.05 here. In our simulation, though, the estimated probability is quite a bit *less* than 0.05. (Your result will probably differ from mine, but it is not likely to be bigger than 0.05). \n\nTo think about why that happened, remember that this is a very skewed population, and the sample size of 10 is not big, so this is not really the situation in which we should be using a $t$-test. The consequence of doing so anyway, which is what we investigated, is that the actual $\\alpha$ of our test is not 0.05, but something smaller: the test is not properly calibrated.\n\nIf you do this again for a sample of size 50, you'll find that the simulation tells you that $\\alpha$ is closer to 0.05, but still less. The population is skewed enough that the Central Limit Theorem still hasn't kicked in yet, and so we still cannot trust the $t$-test to give us a sensible P-value. \n\nExtra: a lot more discussion on what is happening here:\n\nThis test is what is known in the jargon as \"conservative\". To a statistician, this means that the probability of making a type I error is smaller than it should be. That is in some sense safe, in that if you reject, you can be pretty sure that this rejection is correct, but it makes it a lot harder than it should to reject in the first place, and thus you can fail to declare a discovery when you have really made one (but the test didn't say so).\n\nI  did some investigation to see what was going on. First, I ran the simulation again, but this time keeping the mean and SD of each sample, as well as the $t$-statistic, but not actually doing the $t$-test:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(pop$v, 10))) %>% \n  mutate(xbar = mean(my_sample),\n         s = sd(my_sample),\n         t_stat = (xbar - 5) / (s / sqrt(10))) -> mean_sd\nmean_sd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,000 x 5\n# Rowwise: \n     sim my_sample   xbar     s  t_stat\n   <int> <list>     <dbl> <dbl>   <dbl>\n 1     1 <dbl [10]>  4.59  1.91 -0.685 \n 2     2 <dbl [10]>  3.82  2.03 -1.84  \n 3     3 <dbl [10]>  3.67  1.10 -3.84  \n 4     4 <dbl [10]>  6.62  1.77  2.90  \n 5     5 <dbl [10]>  6.77  5.07  1.10  \n 6     6 <dbl [10]>  9.01  4.78  2.65  \n 7     7 <dbl [10]>  3.38  2.80 -1.82  \n 8     8 <dbl [10]>  6.08  2.74  1.25  \n 9     9 <dbl [10]>  4.81  4.00 -0.151 \n10    10 <dbl [10]>  5.09  3.93  0.0736\n# i 990 more rows\n```\n:::\n:::\n\nAs for coding, I made a dataframe with a column `sim` that numbers the individual samples, made sure I said that I wanted to work rowwise, generated a random sample from the population in each row of size 10, and found its mean, SD and the calculated-by-me $t$-statistic.^[This is not bootstrapping, but generating ordinary random samples from a presumed-known population, so there is no `replace = TRUE` here.]\n\n\nAfter that, I played around with several things, but I found something interesting when I plotted the sample mean and SD *against each other*:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mean_sd, aes(x=xbar, y=s)) + geom_point() + geom_smooth(se=F)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n```\n:::\n\n::: {.cell-output-display}\n![](power_files/figure-pdf/pop-power-12-1.pdf){fig-pos='H'}\n:::\n:::\n\n*When the sample mean is bigger, so is the sample standard deviation!*\n\nThis actually does make sense, if you stop to think about it. A sample with a large mean will have some of those values from the long right tail in it, and having those values will also make the sample more spread out. The same does not happen at the low end: if the mean is small, all the sample values must be close together and the SD will be small also.^[You might have something lurking in your mind about the sample mean and sample SD/variance being *independent*, which they clearly are not here. That is true if the samples come from a normal distribution, and from that comes independence of the top and bottom of the $t$-statistic. But here is an example of how everything fails once you go away from normality, and how you have to rely on the central limit theorem, or large sample sizes more generally, for most of your theory to be any good.]\n\nIt wasn't clear to me what that would do to the $t$-statistic. A larger sample mean would make the top of the test statistic bigger, but a larger sample mean would also go with a larger sample SD, and so the bottom of the test statistic would be bigger as well. That's why I included this in the simulation too:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mean_sd, aes(x=t_stat)) + geom_histogram(bins=12)\n```\n\n::: {.cell-output-display}\n![](power_files/figure-pdf/pop-power-13-1.pdf){fig-pos='H'}\n:::\n:::\n\nWell, well. Skewed to the *left*.\n\nThis too makes sense with a bit of thought. A small sample mean will also have a small sample SD, so the test statistic could be more negative. But a large sample mean will have a *large* sample SD, so the test statistic won't get so positive. Hence, in our simulation, the test statistic won't get large enough to reject with as often as it should. Thus, the type I error probability that is too small.\n\n\n\n$\\blacksquare$\n\n\n\n\n\n",
    "supporting": [
      "power_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}