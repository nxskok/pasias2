{
  "hash": "11084293329f7cfddb1a19ff8a64d179",
  "result": {
    "engine": "knitr",
    "markdown": "# Regression revisited\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n## Veggie burgers\n\nYou like hamburgers, but you are a vegetarian. What to do? Today, there are many brands of hamburgers without any meat in them. Some of these are designed to taste like meat, and some have their own flavour. A magazine rated the flavour and texture of 12 different (numbered) brands of meatless hamburgers (to give a rating score between 0 and 100), along with the price (in cents), the number of calories, the grams of fat, and the milligrams of sodium. These measurements are per burger. Is it possible to predict the rating score of a brand of meatless hamburger from the other measurements, and if so, how? The data are in [http://ritsokiguess.site/datafiles/veggie-burgers.txt](http://ritsokiguess.site/datafiles/veggie-burgers.txt), in aligned columns.\n\n(a) Read in and display (most of) the data.\n\n\n(b) Fit a suitable regression to predict score from the other measured variables. Display the results.\n\n\n\n(c) It looks as if both `price` and `sodium` will be able to be removed from this regression. Do so, explain briefly why another test is necessary, and do that other test. What do you conclude? (Note: if you display your output to the second regression, something rather odd will appear. You can safely ignore that.)\n\n\n(d) Another veggie burger (not in the original dataset) has the following values for the explanatory variables:  price 91, calories 140, fat 5, sodium 450. What can you say about the likely score for a veggie burger with these values? Obtain a suitable interval, for *each* of your two models.\n\n\n(e) Compare the lengths of your two intervals. Does it make sense that your \nshorter one should be shorter? Explain briefly.\n\n\n\n(f) Using our second model (the one with only `calories` and `fat` in it), find a suitable interval for the mean score when (i) calories is 140 and fat is 5, (ii) calories is 120 and fat is 3. (You should have two intervals.)\n\n\n(g) Explain briefly why the second interval is shorter than the first one. Make sure you justify your answer.\n\n\n\n## Blood pressure\n\nTwenty people with high blood pressure had various other measurements taken. The aim was to see which of these were associated with blood pressure, with the aim of understanding what causes high blood pressure. The variables observed were:\n\n- `Pt`: patient number (ignore)\n- `BP`: (diastolic) blood pressure, in mmHg\n- `Age` in years\n- `Weight` in kg\n- `BSA`: body surface area, in m$^2$\n- `Dur`: length of time since diagnosis of high blood pressure, in years\n- `Pulse`: pulse rate, in beats per minute\n- `Stress`: score on a questionnaire about stress levels (higher score is more stressed)\n\nThe data values, separated by *tabs*, are in [https://ritsokiguess.site/datafiles/bloodpress.txt](https://ritsokiguess.site/datafiles/bloodpress.txt).\n\n(a) Read in and display (some of) the data.\n\n\n(b) Make a plot of the blood pressure against each of the measured explanatory variables. Hint: use the idea from C32 of making a suitable long dataframe and using facets in your graph.\n\n\n(c) Which explanatory variables seem to have a moderate or strong linear relationship with blood pressure?\n\n\n\n(d) Run a regression predicting blood pressure from `BSA` and `Weight`, and display the output. Does the significance or lack of significance of each of your explanatory variables surprise you? Explain briefly.\n\n\n(e) Explain briefly why it does in fact make sense that the regression results came out as they did. You may wish to draw another graph to support your explanation.\n\n\n\n\n\n## Contraction of heart muscle\n\nAn experiment was carried out on heart muscle in rats. The original description of the experiment was as follows:\n\n> The purpose of this experiment was to assess the influence of calcium in solution on the contraction of heart muscle in rats. The left auricle of 21 rat hearts was isolated and on several occasions a constant-length strip of tissue was electrically stimulated and dipped into various concentrations of calcium chloride solution, after which the shortening of the strip was accurately measured as the response.\n\nThe data are in [http://ritsokiguess.site/datafiles/regression2_muscle.csv](http://ritsokiguess.site/datafiles/regression2_muscle.csv). There are three columns:\n\n- `Strip`: a label for the strip of tissue treated with calcium chloride (text, ignored by us)\n- `Conc`: the concentration of calcium chloride, in suitable units\n- `Length`: the change in length (shortening) of the strip of tissue, mm.\n\nThere are actually 60 measurements, so some of them came from the same rat, a fact that we ignore in this question.\n\n(a) Read in and display (some of) the data.\n\n\n(b) Make a suitable graph of the two quantitative variables, with a smooth trend.\n\n\n(c) Why does your plot suggest that a regression with a squared term would be useful? Fit a suitable regression, and display the results.\n\n\n\n(d) How do you know that adding the squared term was a good idea (or, was not a good idea, depending how your output came out)?\n\n\n(e) For concentrations of 2, 3, and 4 units, obtain 95% confidence intervals for the mean `Length`. Display only the relevant columns of your result, and save it.\n\n\n\n\n(f) Work out the length of each of your confidence intervals. Does it make sense that the lengths compare as they do? Explain briefly.\n\n\n(g) Suppose you have some new rat tissues, not part of the original dataset, and run the same experiment on these with concentrations 2, 3, and 4 units. What are 95% intervals for the predicted `Length`s that you will observe for these tissues? Display your intervals next to the concentrations they are predictions for.\n\n\n\n\n  \nMy solutions follow:\n\n\n\n\n## Veggie burgers\n\nYou like hamburgers, but you are a vegetarian. What to do? Today, there are many brands of hamburgers without any meat in them. Some of these are designed to taste like meat, and some have their own flavour. A magazine rated the flavour and texture of 12 different (numbered) brands of meatless hamburgers (to give a rating score between 0 and 100), along with the price (in cents), the number of calories, the grams of fat, and the milligrams of sodium. These measurements are per burger. Is it possible to predict the rating score of a brand of meatless hamburger from the other measurements, and if so, how? The data are in [http://ritsokiguess.site/datafiles/veggie-burgers.txt](http://ritsokiguess.site/datafiles/veggie-burgers.txt), in aligned columns.\n\n(a) Read in and display (most of) the data.\n\nSolution\n\nAligned columns says `read_table`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/veggie-burgers.txt\"\nburgers <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n-- Column specification --------------------------------------------------------\ncols(\n  brand = col_double(),\n  score = col_double(),\n  price = col_double(),\n  calories = col_double(),\n  fat = col_double(),\n  sodium = col_double()\n)\n```\n\n\n:::\n\n```{.r .cell-code}\nburgers\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 x 6\n   brand score price calories   fat sodium\n   <dbl> <dbl> <dbl>    <dbl> <dbl>  <dbl>\n 1     1    70    91      110     4    310\n 2     2    45    68       90     0    420\n 3     3    43    92       80     1    280\n 4     4    41    75      120     5    370\n 5     5    39    88       90     0    410\n 6     6    30    67      140     4    440\n 7     7    68    73      120     4    430\n 8     8    56    92      170     6    520\n 9     9    40    71      130     4    180\n10    10    34    67      110     2    180\n11    11    30    92      100     1    330\n12    12    26    95      130     2    340\n```\n\n\n:::\n:::\n\nThere are 12 rows, one per brand, and the columns are as promised (and all quantitative, except for `brand`, which is an identifier). \n\n$\\blacksquare$\n\n(b) Fit a suitable regression to predict score from the other measured variables. Display the results.\n\nSolution\n\nThe brand is an identifier, so skip that:\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers.1 <- lm(score ~ price + calories + fat + sodium, data = burgers)\nsummary(burgers.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = score ~ price + calories + fat + sodium, data = burgers)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.376  -5.358   1.843   7.027  13.454 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 59.84879   35.67526   1.678   0.1373  \nprice        0.12868    0.33907   0.380   0.7156  \ncalories    -0.58048    0.28876  -2.010   0.0843 .\nfat          8.49825    3.47215   2.448   0.0443 *\nsodium       0.04876    0.04062   1.200   0.2690  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.72 on 7 degrees of freedom\nMultiple R-squared:  0.4991,\tAdjusted R-squared:  0.2128 \nF-statistic: 1.744 on 4 and 7 DF,  p-value: 0.2443\n```\n\n\n:::\n:::\n\nMy numbering scheme for models is based on the name of the dataframe. Base yours on the response variable `score` if you prefer. But have a scheme, since there is going to be more than one model in this question.\n\n$\\blacksquare$\n\n\n(c) It looks as if both `price` and `sodium` will be able to be removed from this regression. Do so, explain briefly why another test is necessary, and do that other test. What do you conclude? (Note: if you display your output to the second regression, something rather odd will appear. You can safely ignore that.)\n\nSolution\n\nThere are several things to keep straight. The first thing is to fit a model without `price` and `sodium`. The easiest way to do this is to copy, paste and edit:\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers.2 <- lm(score ~ calories + fat, data = burgers)\nsummary(burgers.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = score ~ calories + fat, data = burgers)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.919  -6.786  -4.352  11.198  16.786 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  75.5907    24.4377   3.093   0.0129 *\ncalories     -0.4600     0.2701  -1.703   0.1227  \nfat           7.7047     3.3703   2.286   0.0481 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.56 on 9 degrees of freedom\nMultiple R-squared:  0.3725,\tAdjusted R-squared:  0.233 \nF-statistic: 2.671 on 2 and 9 DF,  p-value: 0.1228\n```\n\n\n:::\n:::\n\nAside: the odd thing is that `calories` is no longer significant. This is confusing, because usually what happens is that explanatory variables become *more* significant when other non-significant variables are removed. So now you may be thinking that `calories` should be removed as well. But see the Extra for what happens when you do that. This is why I said to ignore the odd thing. End of aside.\n\nHowever, we removed *two* explanatory variables at once. This is not supported by the $t$-tests in the output from `burgers.1`, because they only say what will happen if we remove *one* $x$-variable. Hence, we need a test that says whether removing those two $x$s at once was reasonable. That is this test:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(burgers.2, burgers.1, test = \"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: score ~ calories + fat\nModel 2: score ~ price + calories + fat + sodium\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1      9 1418.8                           \n2      7 1132.6  2    286.26 0.8846 0.4544\n```\n\n\n:::\n:::\n\n(That is `F` in quotes, saying to do an $F$-test, not that anything is `FALSE`.)\n\nWith a P-value of 0.45, this is saying that there is no significant difference in fit between the two models, and so we should prefer the smaller, simpler one `burgers.2`, with just `calories` and `fat` in it, because it fits just as well as the bigger, more complicated one (and therefore we do not need the extra complication).\n\nExtra: What happens if you do backward elimination from here, starting from the best model found so far?\n\nThe previous part told us that predicting score from just calories and fat was the best thing to do so far.  That was the model I called `burgers.2`:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(burgers.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = score ~ calories + fat, data = burgers)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.919  -6.786  -4.352  11.198  16.786 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  75.5907    24.4377   3.093   0.0129 *\ncalories     -0.4600     0.2701  -1.703   0.1227  \nfat           7.7047     3.3703   2.286   0.0481 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.56 on 9 degrees of freedom\nMultiple R-squared:  0.3725,\tAdjusted R-squared:  0.233 \nF-statistic: 2.671 on 2 and 9 DF,  p-value: 0.1228\n```\n\n\n:::\n:::\n\nEvidently, `calories` comes out now:\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers.3 <- lm(score ~ fat, data = burgers)\nsummary(burgers.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = score ~ fat, data = burgers)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.186  -8.538  -2.136   5.898  22.814 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   35.390      6.906   5.124 0.000448 ***\nfat            2.949      2.059   1.432 0.182578    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.7 on 10 degrees of freedom\nMultiple R-squared:  0.1702,\tAdjusted R-squared:  0.08724 \nF-statistic: 2.051 on 1 and 10 DF,  p-value: 0.1826\n```\n\n\n:::\n:::\n\nNow, there is only one explanatory variable left, and it is *no longer significant*, so it too has to come out now! This seems to make no sense, since `fat` was definitely significant before, and we would expect it still to be significant after removing something that was not significant. (Sometimes this happens; this is one of those cases.)\n\nAnother way of expressing your surprise is to look at the R-squared values (or the adjusted R-squared values) for the models we have fit so far:\n\n Model  Explanatory                         R-squared   Adj R-sq\n------  ---------------------------------  ----------  ---------\n  1     `price + calories + fat + sodium`    0.50       0.21\n  2     `calories + fat`                     0.37       0.23\n  3     `fat`                                0.17       0.09\n  \n  \nAs we go through the models, R-squared goes *dramatically* down (it will go down because it *always* goes down when you take things out, but this seems too dramatic). Adjusted R-squared goes up when we take out  `price` and `sodium`, but it too goes sharply down when we take out `calories`, which doesn't seem right.  \n\nThere is no need to go any further than this, but if you want to take out `fat` as well, leaving you with no explanatory variables at all, there are a couple of non-obvious ways to do it. One is to use `update`:\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers.4 <- update(burgers.3, . ~ . -fat)\nsummary(burgers.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = score ~ 1, data = burgers)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-17.50 -10.50  -3.00   4.25  26.50 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   43.500      4.139   10.51 4.49e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.34 on 11 degrees of freedom\n```\n\n\n:::\n:::\n\nThis says that the predicted `score` is 43.5, regardless of the values of anything else! There is no R-squared displayed, because that is *zero* for a model with no $x$-variables.\n\nThe other way is to find out that R understands `1` to mean a model with just an intercept:\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers.4a <- lm(score ~ 1, data = burgers)\nsummary(burgers.4a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = score ~ 1, data = burgers)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-17.50 -10.50  -3.00   4.25  26.50 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   43.500      4.139   10.51 4.49e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.34 on 11 degrees of freedom\n```\n\n\n:::\n:::\n\nOnce again, the R-squared is zero.\n\n\n$\\blacksquare$\n\n(d) Another veggie burger (not in the original dataset) has the following values for the explanatory variables:  price 91, calories 140, fat 5, sodium 450. What can you say about the likely score for a veggie burger with these values? Obtain a suitable interval, for *each* of your two models.\n\nSolution\n\nThis is talking about the predicted response for an individual (*this* burger), not the mean response for *all* veggie burgers with those values for the explanatory variables, so it calls for a *prediction interval* in each case. This is the one that uses `predict`, not the one that uses the `marginaleffects` package (the one we did second in lecture).\n\nThe first step is to make a dataframe, by my tradition called `new`, of values to predict for. Any way that produces you a one-row dataframe is good, for example:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- tribble(\n  ~price, ~calories, ~fat, ~sodium,\n  91,        140,      5,      450)\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 4\n  price calories   fat sodium\n  <dbl>    <dbl> <dbl>  <dbl>\n1    91      140     5    450\n```\n\n\n:::\n:::\n\nIf you are stuck, type the values into a file (or even a spreadsheet) and read that in. But find a way.\n\nFor the model with all four explanatory variables:\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(burgers.1, new, interval = \"p\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 54.72593 20.13798 89.31389\n```\n\n\n:::\n:::\n\nand for the model with only two:\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(burgers.2, new, interval = \"p\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 49.71981 18.63013 80.80949\n```\n\n\n:::\n:::\n\nThese intervals are distressingly wide, as is usually the way with prediction intervals (and we only have 12 observations to base the interval on). Also, it didn't matter that in the second case, `new` had some extra columns in it; these were just ignored.\n\nExtra: these values are one SD above the mean in each case. How did I work them out? Like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers %>% \n  summarize(across(price:sodium, list(mean = \\(x) mean(x), sd = \\(x) sd(x)))) %>% \n  pivot_longer(everything(), names_to = c(\"variable\", \".value\"), names_sep = \"_\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 3\n  variable   mean     sd\n  <chr>     <dbl>  <dbl>\n1 price     80.9   11.6 \n2 calories 116.    25.0 \n3 fat        2.75   2.01\n4 sodium   351.   103.  \n```\n\n\n:::\n:::\n\n\n\nTo work out summary statistics for a whole bunch of columns, use `across` inside the `summarize`.  First is the columns you want to summarize, and then is how you want to summarize them. In this case, I wanted the mean *and* SD of each variable, two things, so I had to put them in a `list`. Something possibly new here is that I \"named\" the elements of the `list` (the `mean =` and `sd =`); what this does is to add `mean` and `sd` onto the name of each variable, so that I can tell which variable and which statistic I am looking at:\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers %>% \n  summarize(across(price:sodium, list(mean = ~mean(.), sd = ~sd(.)))) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 8\n  price_mean price_sd calories_mean calories_sd fat_mean fat_sd sodium_mean\n       <dbl>    <dbl>         <dbl>       <dbl>    <dbl>  <dbl>       <dbl>\n1       80.9     11.6          116.        25.0     2.75   2.01        351.\n# i 1 more variable: sodium_sd <dbl>\n```\n\n\n:::\n:::\n\nThis is all right, but I was hoping for something tidier. How about the names of the variables in the rows, and the names of the statistics in the columns? This is evidently some kind of `pivot_longer`. It's one of the fancy ones where the column names we have here encode two things, separated by an underscore. That means having *two* things in `names_to`, and also having a `names_sep` that says what those two things are separated by (an underscore).\n\nTo get the variable names in rows, we need to create a new column called something like `variable`. This is the usual kind of `pivot_longer` thing: put that first in `names_to`, because the things that are going in `variable` are the first part of the column names we have here.  The second part of the column names we have so far, `mean` or `sd`, are going\nto make *names of new columns*, which is different from what would normally happen, which is this:\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers %>% \n  summarize(across(price:sodium, list(mean = ~mean(.), sd = ~sd(.)))) %>% \n  pivot_longer(everything(), names_to = c(\"variable\", \"stat\"), names_sep = \"_\", values_to = \"value\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 3\n  variable stat   value\n  <chr>    <chr>  <dbl>\n1 price    mean   80.9 \n2 price    sd     11.6 \n3 calories mean  116.  \n4 calories sd     25.0 \n5 fat      mean    2.75\n6 fat      sd      2.01\n7 sodium   mean  351.  \n8 sodium   sd    103.  \n```\n\n\n:::\n:::\n\nThis has created a *column* called `stat`, with the names of the statistics in it. This is all right, but is not as tidy as we^[Well, *I*.] would like.\n\nTo use the things in `stat` as column names (and to fill the columns with the thing currently in `value`), what you do is to replace the appropriate thing in `names_to` with the special label `.value`. When you do this, you can take out the `values_to`, since `pivot_longer` now knows where the values are going (into the new columns you are creating):\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers %>% \n  summarize(across(price:sodium, list(mean = \\(x) mean(x), sd = \\(x) sd(x)))) %>% \n  pivot_longer(everything(), names_to = c(\"variable\", \".value\"), names_sep = \"_\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 3\n  variable   mean     sd\n  <chr>     <dbl>  <dbl>\n1 price     80.9   11.6 \n2 calories 116.    25.0 \n3 fat        2.75   2.01\n4 sodium   351.   103.  \n```\n\n\n:::\n:::\n\nIsn't that pretty?\n\n$\\blacksquare$\n\n(e) Compare the lengths of your two intervals. Does it make sense that your \nshorter one should be shorter? Explain briefly.\n\nSolution\n\nAfter my long Extra, I need to display them again so that I can see them:\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(burgers.1, new, interval = \"p\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 54.72593 20.13798 89.31389\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(burgers.2, new, interval = \"p\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 49.71981 18.63013 80.80949\n```\n\n\n:::\n:::\n\nThe first interval is about 69 points long and the second one is about 62 points long. Therefore the second interval is shorter.\n\nWhy is that? This is different from the other interval comparisons we have done, because this time we are comparing the *same* prediction from two *different* models. \n\nOur previous work showed that the model `burgers.2` was better than `burgers.1`. This was because it had fewer explanatory variables in it, and we showed that the ones we removed from `burgers.1` could safely be removed (the $F$-test in `anova`). Or similar wording; you might have concluded that the extra explanatory variables in `burgers.1` were not needed and could be taken out.\n\nThis is another reason for trying to find a good model: not only is a smaller model easier to explain, but it also gives better predictions, in the sense that the uncertainty around the prediction (as measured by the length of the interval) is smaller.\n\n$\\blacksquare$\n\n\n(f) Using our second model (the one with only `calories` and `fat` in it), find a suitable interval for the mean score when (i) calories is 140 and fat is 5, (ii) calories is 120 and fat is 3. (You should have two intervals.)\n\nSolution\n\nThis is the \"mean of all possible scores\" when the explanatory variables take the values shown, so it's the confidence interval for the mean response rather than the prediction interval we had before. To start, make another `new` with the two rows of values in it: \n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- tribble(\n  ~calories, ~fat,\n  140,        1,\n  120,        3\n)\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  calories   fat\n     <dbl> <dbl>\n1      140     1\n2      120     3\n```\n\n\n:::\n:::\n\nCheck. Then, `predictions` (from the `marginaleffects` package, which of course you remembered to install (once) and load first):\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(marginaleffects)\ncbind(predictions(burgers.2, newdata = new)) %>% \n  select(calories, fat, estimate, conf.low, conf.high)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  calories fat estimate  conf.low conf.high\n1      140   1 18.90120 -5.444696  43.24710\n2      120   3 43.50967 36.296984  50.72236\n```\n\n\n:::\n:::\n\n(if you use the wrong model, you'll get an error, because the bigger model has some other things in it that we don't have values for.)\n\n$\\blacksquare$\n\n(g) Explain briefly why the second interval is shorter than the first one. Make sure you justify your answer.\n\nSolution\n\nFirst, verify that the second interval really *is* shorter than the first one. (If, for some reason, it is not, then *say that*.) The first interval is of length about $62-37 = 25$, and the second one is of length about $52-35 = 17$.\n\nAside: you might be thinking about working those out with R rather than by hand, and so you can: \n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(burgers.2, newdata = new)) %>% \n  mutate(ci_length = conf.high - conf.low) %>% \n  select(calories, fat, conf.low, conf.high, ci_length)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  calories fat  conf.low conf.high ci_length\n1      140   1 -5.444696  43.24710  48.69180\n2      120   3 36.296984  50.72236  14.42537\n```\n\n\n:::\n:::\n\n\nNow we are in the familiar situation where we are comparing predictions for different *values* for the *same* model. So you might be suspecting that the second pair of values is closer to the mean (for the data as a whole) than the first pair is.^[There is *one* pair of values in each case, hence a *singular* verb.] But we should check that this is indeed the case. \n\nLet's look at the values we predicted for first:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  calories   fat\n     <dbl> <dbl>\n1      140     1\n2      120     3\n```\n\n\n:::\n:::\n\nTo get the means, the easiest way is `summary`, on the whole dataframe or the bit of it containing only `calories` and `fat`:\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers %>% \n  select(c(calories, fat)) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    calories          fat      \n Min.   : 80.0   Min.   :0.00  \n 1st Qu.: 97.5   1st Qu.:1.00  \n Median :115.0   Median :3.00  \n Mean   :115.8   Mean   :2.75  \n 3rd Qu.:130.0   3rd Qu.:4.00  \n Max.   :170.0   Max.   :6.00  \n```\n\n\n:::\n:::\n\nThe values for  both variables are both above their means, but the second values for both `calories` and `fat` are closer to their means.\n\nThere is a tidyverse way to do this, which uses `across` again, but it's a bit simpler than the other one I did (in an Extra above):\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers %>% \n  summarize(across(c(calories, fat), \\(x) mean(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 2\n  calories   fat\n     <dbl> <dbl>\n1     116.  2.75\n```\n\n\n:::\n:::\n\n(\"for each of `calories` and `fat`, work out the mean of it\"). Another way to do the same thing is the `map` idea for running functions over each of something. `mean` returns a decimal number, so `map_dbl`:\n\n::: {.cell}\n\n```{.r .cell-code}\nburgers %>% \n  select(calories, fat) %>% \n  map_dbl(\\(x) mean(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ncalories      fat \n115.8333   2.7500 \n```\n\n\n:::\n:::\n\nThe `map`, by default, works on each *column* of the dataframe it is fed,^[Because a dataframe is, to R, a `list` of columns, and so the obvious thing to for-each over is what the dataframe is a list of.] namely the dataframe with only `calories` and `fat` in it.^[To be precise, `map_dbl` because the calculation of a mean is a decimal number, and the results get glued back together into a vector.]\n\nSomehow, make the assertion that the second values for `calories` and `fat` are closer to their means than the first values of each of them, and then demonstrate that this is indeed true. Or, think to yourself \"this probably depends on the means somehow\", find the means, and then say that the second values are closer to the means, so the prediction for them should be better (in the sense of having a shorter confidence interval).\n\n$\\blacksquare$\n\n\n## Blood pressure\n\nTwenty people with high blood pressure had various other measurements taken. The aim was to see which of these were associated with blood pressure, with the aim of understanding what causes high blood pressure. The variables observed were:\n\n- `Pt`: patient number (ignore)\n- `BP`: (diastolic) blood pressure, in mmHg\n- `Age` in years\n- `Weight` in kg\n- `BSA`: body surface area, in m$^2$\n- `Dur`: length of time since diagnosis of high blood pressure, in years\n- `Pulse`: pulse rate, in beats per minute\n- `Stress`: score on a questionnaire about stress levels (higher score is more stressed)\n\nThe data values, separated by *tabs*, are in [https://ritsokiguess.site/datafiles/bloodpress.txt](https://ritsokiguess.site/datafiles/bloodpress.txt).\n\n(a) Read in and display (some of) the data.\n\nSolution\n\nThe data values are separated by tabs, so `read_tsv` is what you need:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"https://ritsokiguess.site/datafiles/bloodpress.txt\"\nbp <- read_tsv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 20 Columns: 8\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\ndbl (8): Pt, BP, Age, Weight, BSA, Dur, Pulse, Stress\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nbp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 x 8\n      Pt    BP   Age Weight   BSA   Dur Pulse Stress\n   <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1     1   105    47   85.4  1.75   5.1    63     33\n 2     2   115    49   94.2  2.1    3.8    70     14\n 3     3   116    49   95.3  1.98   8.2    72     10\n 4     4   117    50   94.7  2.01   5.8    73     99\n 5     5   112    51   89.4  1.89   7      72     95\n 6     6   121    48   99.5  2.25   9.3    71     10\n 7     7   121    49   99.8  2.25   2.5    69     42\n 8     8   110    47   90.9  1.9    6.2    66      8\n 9     9   110    49   89.2  1.83   7.1    69     62\n10    10   114    48   92.7  2.07   5.6    64     35\n11    11   114    47   94.4  2.07   5.3    74     90\n12    12   115    49   94.1  1.98   5.6    71     21\n13    13   114    50   91.6  2.05  10.2    68     47\n14    14   106    45   87.1  1.92   5.6    67     80\n15    15   125    52  101.   2.19  10      76     98\n16    16   114    46   94.5  1.98   7.4    69     95\n17    17   106    46   87    1.87   3.6    62     18\n18    18   113    46   94.5  1.9    4.3    70     12\n19    19   110    48   90.5  1.88   9      71     99\n20    20   122    56   95.7  2.09   7      75     99\n```\n\n\n:::\n:::\n\n20 patients, and all the columns as listed. Remember that in R, uppercase and lowercase are *different*, so there is no problem in using (lowercase) `bp` for the name of a dataframe that has a column called (uppercase) `BP` in it. But if that confuses you, feel free to give your dataframe a more descriptive name like `blood_pressure` or even something like `health_stats`.\n\nExtra: you can also use `read_delim` for this, but you have to do it right.\nIf you do it as `read_delim(my_url, \" \")` or similar, it won't work, because the data values are not separated by single spaces:\n\n::: {.cell}\n\n```{.r .cell-code}\nbpxx <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 20 Columns: 1\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (1): Pt\tBP\tAge\tWeight\tBSA\tDur\tPulse\tStress\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nbpxx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 x 1\n   `Pt\\tBP\\tAge\\tWeight\\tBSA\\tDur\\tPulse\\tStress`\n   <chr>                                         \n 1 \"1\\t105\\t47\\t85.4\\t1.75\\t5.1\\t63\\t33\"         \n 2 \"2\\t115\\t49\\t94.2\\t2.10\\t3.8\\t70\\t14\"         \n 3 \"3\\t116\\t49\\t95.3\\t1.98\\t8.2\\t72\\t10\"         \n 4 \"4\\t117\\t50\\t94.7\\t2.01\\t5.8\\t73\\t99\"         \n 5 \"5\\t112\\t51\\t89.4\\t1.89\\t7.0\\t72\\t95\"         \n 6 \"6\\t121\\t48\\t99.5\\t2.25\\t9.3\\t71\\t10\"         \n 7 \"7\\t121\\t49\\t99.8\\t2.25\\t2.5\\t69\\t42\"         \n 8 \"8\\t110\\t47\\t90.9\\t1.90\\t6.2\\t66\\t8\"          \n 9 \"9\\t110\\t49\\t89.2\\t1.83\\t7.1\\t69\\t62\"         \n10 \"10\\t114\\t48\\t92.7\\t2.07\\t5.6\\t64\\t35\"        \n11 \"11\\t114\\t47\\t94.4\\t2.07\\t5.3\\t74\\t90\"        \n12 \"12\\t115\\t49\\t94.1\\t1.98\\t5.6\\t71\\t21\"        \n13 \"13\\t114\\t50\\t91.6\\t2.05\\t10.2\\t68\\t47\"       \n14 \"14\\t106\\t45\\t87.1\\t1.92\\t5.6\\t67\\t80\"        \n15 \"15\\t125\\t52\\t101.3\\t2.19\\t10.0\\t76\\t98\"      \n16 \"16\\t114\\t46\\t94.5\\t1.98\\t7.4\\t69\\t95\"        \n17 \"17\\t106\\t46\\t87.0\\t1.87\\t3.6\\t62\\t18\"        \n18 \"18\\t113\\t46\\t94.5\\t1.90\\t4.3\\t70\\t12\"        \n19 \"19\\t110\\t48\\t90.5\\t1.88\\t9.0\\t71\\t99\"        \n20 \"20\\t122\\t56\\t95.7\\t2.09\\t7.0\\t75\\t99\"        \n```\n\n\n:::\n:::\n\nAll of the data values have been smooshed together into a *single* column with a rather amusing name. The rather odd `\\t` is the key to understanding what has happened. The simplest way to make it work is to use `read_delim`, but *without* saying what the delimiter character is:\n\n::: {.cell}\n\n```{.r .cell-code}\nbpx <- read_delim(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 20 Columns: 8\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\ndbl (8): Pt, BP, Age, Weight, BSA, Dur, Pulse, Stress\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nbpx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 x 8\n      Pt    BP   Age Weight   BSA   Dur Pulse Stress\n   <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1     1   105    47   85.4  1.75   5.1    63     33\n 2     2   115    49   94.2  2.1    3.8    70     14\n 3     3   116    49   95.3  1.98   8.2    72     10\n 4     4   117    50   94.7  2.01   5.8    73     99\n 5     5   112    51   89.4  1.89   7      72     95\n 6     6   121    48   99.5  2.25   9.3    71     10\n 7     7   121    49   99.8  2.25   2.5    69     42\n 8     8   110    47   90.9  1.9    6.2    66      8\n 9     9   110    49   89.2  1.83   7.1    69     62\n10    10   114    48   92.7  2.07   5.6    64     35\n11    11   114    47   94.4  2.07   5.3    74     90\n12    12   115    49   94.1  1.98   5.6    71     21\n13    13   114    50   91.6  2.05  10.2    68     47\n14    14   106    45   87.1  1.92   5.6    67     80\n15    15   125    52  101.   2.19  10      76     98\n16    16   114    46   94.5  1.98   7.4    69     95\n17    17   106    46   87    1.87   3.6    62     18\n18    18   113    46   94.5  1.9    4.3    70     12\n19    19   110    48   90.5  1.88   9      71     99\n20    20   122    56   95.7  2.09   7      75     99\n```\n\n\n:::\n:::\n\nIf you look at the R Console now, you'll see a line in the file-reading message that says\n\n```\nDelimiter: \"\\t\"\n```\n\nWhat happened is that `read_delim` successfully *guessed* what was separating the data values, by looking at the first few lines of the data file and seeing that there were a lot of tabs apparently separating data values. This behaviour is new in the latest version of `readr`;^[The part of the `tidyverse` where the file-reading functions live.] if you use this idea, you should be able to explain why it worked. If you cannot, it looks like a lucky guess.\n\nBy this point, you might have guessed that the mysterious `\\t` is the way R represents a tab (you would be right), and that therefore you should be able to do this:^[This looks like two characters, but it is actually only one. The backslash is called an \"escape character\" and doesn't count as a character for these purposes. It's used as a way to represent some characters that you cannot normally print. Another one is `\\n`, which is the \"newline\" at the end of each line of a file. This is how R Studio or anything else knows to start a new line at this point and not somewhere else. Otherwise the file would be all one (very long) line on the screen. To convince yourself of that, run the code `cat(\"Hello\\nWorld\")` and see what it displays.]\n\n::: {.cell}\n\n```{.r .cell-code}\nbpxxxx <- read_delim(my_url, \"\\t\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 20 Columns: 8\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\ndbl (8): Pt, BP, Age, Weight, BSA, Dur, Pulse, Stress\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nbpxxxx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 x 8\n      Pt    BP   Age Weight   BSA   Dur Pulse Stress\n   <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1     1   105    47   85.4  1.75   5.1    63     33\n 2     2   115    49   94.2  2.1    3.8    70     14\n 3     3   116    49   95.3  1.98   8.2    72     10\n 4     4   117    50   94.7  2.01   5.8    73     99\n 5     5   112    51   89.4  1.89   7      72     95\n 6     6   121    48   99.5  2.25   9.3    71     10\n 7     7   121    49   99.8  2.25   2.5    69     42\n 8     8   110    47   90.9  1.9    6.2    66      8\n 9     9   110    49   89.2  1.83   7.1    69     62\n10    10   114    48   92.7  2.07   5.6    64     35\n11    11   114    47   94.4  2.07   5.3    74     90\n12    12   115    49   94.1  1.98   5.6    71     21\n13    13   114    50   91.6  2.05  10.2    68     47\n14    14   106    45   87.1  1.92   5.6    67     80\n15    15   125    52  101.   2.19  10      76     98\n16    16   114    46   94.5  1.98   7.4    69     95\n17    17   106    46   87    1.87   3.6    62     18\n18    18   113    46   94.5  1.9    4.3    70     12\n19    19   110    48   90.5  1.88   9      71     99\n20    20   122    56   95.7  2.09   7      75     99\n```\n\n\n:::\n:::\n\nand you would once again be right.\n\n$\\blacksquare$\n\n(b) Make a plot of the blood pressure against each of the measured explanatory variables. Hint: use the idea from C32 of making a suitable long dataframe and using facets in your graph.\n\nSolution\n\nUse `pivot_longer` with all the columns containing $x$-variables:\n\n::: {.cell}\n\n```{.r .cell-code}\nbp %>% \n  pivot_longer(Age:Stress, names_to = \"xname\", values_to = \"x\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 120 x 4\n      Pt    BP xname      x\n   <dbl> <dbl> <chr>  <dbl>\n 1     1   105 Age    47   \n 2     1   105 Weight 85.4 \n 3     1   105 BSA     1.75\n 4     1   105 Dur     5.1 \n 5     1   105 Pulse  63   \n 6     1   105 Stress 33   \n 7     2   115 Age    49   \n 8     2   115 Weight 94.2 \n 9     2   115 BSA     2.1 \n10     2   115 Dur     3.8 \n# i 110 more rows\n```\n\n\n:::\n:::\n\nand then plot `BP` against `x` facetting by `xname`:\n\n::: {.cell}\n\n```{.r .cell-code}\nbp %>% \n  pivot_longer(Age:Stress, names_to = \"xname\", values_to = \"x\") %>% \n  ggplot(aes(x = x, y = BP)) + geom_point() +\n  facet_wrap(~xname, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](regression-revisited_files/figure-pdf/unnamed-chunk-26-1.pdf){fig-pos='H'}\n:::\n:::\n\nUse `scales = \"free\"` to have each scatterplot fill its facet.\n\nIf you don't get to this, you are faced with making six scatterplots one by one, which will be a lot of work in comparison (and it's easy to mess up the copy/paste and miss one of them out).\n\n$\\blacksquare$\n\n(c) Which explanatory variables seem to have a moderate or strong linear relationship with blood pressure?\n\nSolution\n\nI would say `BSA`, `Weight` and maybe `Pulse`. For me, the `Age` relationship is not quite strong enough, and there is basically no relationship with `Dur` or `Stress`.\n\nI don't mind precisely where you draw the line. You could include `Pulse` and `Age` or not, but I think you should definitely include `BSA` and `Weight` and definitely exclude `Dur` and `Stress`.\n\nExtra: Stress is often considered to be a cause of high blood pressure, but it seems from this dataset that it is not.\n\n$\\blacksquare$\n\n\n(d) Run a regression predicting blood pressure from `BSA` and `Weight`, and display the output. Does the significance or lack of significance of each of your explanatory variables surprise you? Explain briefly.\n\nSolution\n\n::: {.cell}\n\n```{.r .cell-code}\nbp.1 <- lm(BP ~ BSA + Weight, data = bp)\nsummary(bp.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = BP ~ BSA + Weight, data = bp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8932 -1.1961 -0.4061  1.0764  4.7524 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   5.6534     9.3925   0.602    0.555    \nBSA           5.8313     6.0627   0.962    0.350    \nWeight        1.0387     0.1927   5.392 4.87e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.744 on 17 degrees of freedom\nMultiple R-squared:  0.9077,\tAdjusted R-squared:  0.8968 \nF-statistic: 83.54 on 2 and 17 DF,  p-value: 1.607e-09\n```\n\n\n:::\n:::\n\nsuppose we (foolishly) took out weight\n\n::: {.cell}\n\n```{.r .cell-code}\nbp.2 <- lm(BP ~ BSA, data = bp)\nsummary(bp.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = BP ~ BSA, data = bp)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.314 -1.963 -0.197  1.934  4.831 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   45.183      9.392   4.811  0.00014 ***\nBSA           34.443      4.690   7.343 8.11e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.79 on 18 degrees of freedom\nMultiple R-squared:  0.7497,\tAdjusted R-squared:  0.7358 \nF-statistic: 53.93 on 1 and 18 DF,  p-value: 8.114e-07\n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbp.3 <- lm(BP ~ Weight, data = bp)\nsummary(bp.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = BP ~ Weight, data = bp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6933 -0.9318 -0.4935  0.7703  4.8656 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.20531    8.66333   0.255    0.802    \nWeight       1.20093    0.09297  12.917 1.53e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.74 on 18 degrees of freedom\nMultiple R-squared:  0.9026,\tAdjusted R-squared:  0.8972 \nF-statistic: 166.9 on 1 and 18 DF,  p-value: 1.528e-10\n```\n\n\n:::\n:::\n\n\nBoth explanatory variables have a strong relationship with blood pressure according to the scatterplots, so we would expect them both to be significant. `Weight` is, but `BSA` is not. This I find surprising.\n\n$\\blacksquare$\n\n(e) Explain briefly why it does in fact make sense that the regression results came out as they did. You may wish to draw another graph to support your explanation.\n\nSolution\n\nThe way we learned this in C32 is that `BSA` has nothing to *add* to a regression that also contains `Weight`, when it comes to predicting blood pressure. That is to say, the way `BSA` is related to blood pressure is similar to the way `Weight` is . \n\nTo gain a bit more insight, we might be suffering from multicollinearity here, and the reason that `BSA` is not significant as we expected could be that it and `Weight` are related to *each other*. To find out about *that*, make a scatterplot of the two explanatory variables:^[Either axis for either variable is good, since neither of these is a response.]\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(bp, aes(y = BSA, x = Weight)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](regression-revisited_files/figure-pdf/unnamed-chunk-30-1.pdf){fig-pos='H'}\n:::\n:::\n\nThese are clearly related. To be more precise about \"not needing `BSA` as well\": if we know a person's weight, we already know something about their body surface area (if the weight is bigger, so is their body surface area). So there is less information than you would otherwise expect in `BSA` if you already know their weight.\n\nThis is a less dramatic example than the one in lecture (with the football punters), but the point is the same: if the $x$-variables are correlated, one or more of them could be less significant than you would expect, because the information it contains is mostly already contained in other $x$-variables.\n\nExtra: another way of looking at all this is via pairwise correlations:\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(bp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Pt         BP        Age     Weight         BSA       Dur\nPt      1.00000000 0.03113499 0.04269354 0.02485650 -0.03128800 0.1762455\nBP      0.03113499 1.00000000 0.65909298 0.95006765  0.86587887 0.2928336\nAge     0.04269354 0.65909298 1.00000000 0.40734926  0.37845460 0.3437921\nWeight  0.02485650 0.95006765 0.40734926 1.00000000  0.87530481 0.2006496\nBSA    -0.03128800 0.86587887 0.37845460 0.87530481  1.00000000 0.1305400\nDur     0.17624551 0.29283363 0.34379206 0.20064959  0.13054001 1.0000000\nPulse   0.11228508 0.72141316 0.61876426 0.65933987  0.46481881 0.4015144\nStress  0.34315169 0.16390139 0.36822369 0.03435475  0.01844634 0.3116398\n           Pulse     Stress\nPt     0.1122851 0.34315169\nBP     0.7214132 0.16390139\nAge    0.6187643 0.36822369\nWeight 0.6593399 0.03435475\nBSA    0.4648188 0.01844634\nDur    0.4015144 0.31163982\nPulse  1.0000000 0.50631008\nStress 0.5063101 1.00000000\n```\n\n\n:::\n:::\n\nThe correlations with `BP` (second column, or second row) are mostly as you would expect from the scatterplots: very high with `Weight` and `BSA`, moderately high for `Age` and `Pulse`, and low for the others. I usually find that the correlation suggests a stronger relationship than the scatterplot does.\n\nThe other thing is that the correlation between `Weight` and `BSA` is the highest of all the correlations between variables that are not the response. So if you know a person's weight, you can already make a good guess at their body surface area, even without having the actual values. Hence, including `BSA` in the regression when you already have `Weight` is not very helpful.\n\nThe data for this question came from [here](https://online.stat.psu.edu/stat501/lesson/12/12.1).\n\n$\\blacksquare$\n\n\n\n\n## Contraction of heart muscle\n\nAn experiment was carried out on heart muscle in rats. The original description of the experiment was as follows:\n\n> The purpose of this experiment was to assess the influence of calcium in solution on the contraction of heart muscle in rats. The left auricle of 21 rat hearts was isolated and on several occasions a constant-length strip of tissue was electrically stimulated and dipped into various concentrations of calcium chloride solution, after which the shortening of the strip was accurately measured as the response.\n\nThe data are in [http://ritsokiguess.site/datafiles/regression2_muscle.csv](http://ritsokiguess.site/datafiles/regression2_muscle.csv). There are three columns:\n\n- `Strip`: a label for the strip of tissue treated with calcium chloride (text, ignored by us)\n- `Conc`: the concentration of calcium chloride, in suitable units\n- `Length`: the change in length (shortening) of the strip of tissue, mm.\n\nThere are actually 60 measurements, so some of them came from the same rat, a fact that we ignore in this question.\n\n(a) Read in and display (some of) the data.\n\nSolution\n\nAs usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/regression2_muscle.csv\"\nmuscle <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 60 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): Strip\ndbl (2): Conc, Length\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nmuscle\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 60 x 3\n   Strip  Conc Length\n   <chr> <dbl>  <dbl>\n 1 S01    1      15.8\n 2 S01    2      20.8\n 3 S01    3      22.6\n 4 S01    4      23.8\n 5 S02    1      20.6\n 6 S02    2      26.8\n 7 S02    3      28.4\n 8 S02    4      27  \n 9 S03    0.25    7.2\n10 S03    0.5    15.4\n# i 50 more rows\n```\n\n\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(b) Make a suitable graph of the two quantitative variables, with a smooth trend.\n\nSolution\n\nScatterplot, with `Length` as the response:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(muscle, aes(x = Conc, y = Length)) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](regression-revisited_files/figure-pdf/unnamed-chunk-33-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(c) Why does your plot suggest that a regression with a squared term would be useful? Fit a suitable regression, and display the results.\n\nSolution\n\nThe trend is not linear; it appears to go up and level off, with maybe a hint that it is coming down again. This is the most complete answer; \"the relationship is curved\" is also reasonable.\n\nSo, we add a squared term to the regression right away:\n\n::: {.cell}\n\n```{.r .cell-code}\nmuscle.1 <- lm(Length ~ Conc + I(Conc^2), data = muscle)\nsummary(muscle.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Length ~ Conc + I(Conc^2), data = muscle)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.4159  -3.8516   0.6172   3.6172   8.5672 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   5.0144     1.8014   2.784  0.00728 ** \nConc         18.8273     2.3028   8.176 3.51e-11 ***\nI(Conc^2)    -3.4089     0.5644  -6.040 1.24e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.05 on 57 degrees of freedom\nMultiple R-squared:  0.6721,\tAdjusted R-squared:  0.6606 \nF-statistic: 58.41 on 2 and 57 DF,  p-value: 1.581e-14\n```\n\n\n:::\n:::\n\n\n$\\blacksquare$\n\n\n\n(d) How do you know that adding the squared term was a good idea (or, was not a good idea, depending how your output came out)?\n\nSolution\n\nThe coefficient of concentration squared is significantly different from zero. This shows that the relationship really does curve, more than chance (not a very surprising conclusion, given the scatterplot).\n\n$\\blacksquare$\n\n\n(e) For concentrations of 2, 3, and 4 units, obtain 95% confidence intervals for the mean `Length`. Display only the relevant columns of your result, and save it.\n\nSolution\n\nUse `predictions`, and to set that up, create a dataframe (by my tradition called `new`) that contains the concentrations you want to predict for:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- tibble(Conc = c(2, 3, 4))\ncbind(predictions(muscle.1, newdata = new)) %>% \n  select(Conc, estimate, conf.low, conf.high) -> preds\npreds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Conc estimate conf.low conf.high\n1    2 29.03327 26.96336  31.10317\n2    3 30.81589 28.61536  33.01642\n3    4 25.78063 21.36451  30.19674\n```\n\n\n:::\n:::\n\n\nExtra: I note that the predictions increase from 2 to 3, and then decrease sharply after that. This is not quite what the scatterplot said:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_predictions(muscle.1, condition = \"Conc\") +\n  geom_point(data = muscle, aes(x = Conc, y = Length))\n```\n\n::: {.cell-output-display}\n![](regression-revisited_files/figure-pdf/unnamed-chunk-36-1.pdf){fig-pos='H'}\n:::\n:::\n\nThe shape of the parabola seems to require a drop at concentration 4, judging by the way it is increasing and then levelling off before that. Maybe the data support that, maybe they don't.\n\n$\\blacksquare$\n\n\n\n(f) Work out the length of each of your confidence intervals. Does it make sense that the lengths compare as they do? Explain briefly.\n\nSolution\n\nI realized at this point that I needed to save the predictions, so I went back and did that. Thus I don't have to compute them again:\n\n::: {.cell}\n\n```{.r .cell-code}\npreds %>% \n  mutate(conf_len = conf.high - conf.low)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Conc estimate conf.low conf.high conf_len\n1    2 29.03327 26.96336  31.10317 4.139812\n2    3 30.81589 28.61536  33.01642 4.401065\n3    4 25.78063 21.36451  30.19674 8.832231\n```\n\n\n:::\n:::\n\nThe confidence interval for the mean response at 4 is the longest, which makes sense since it is at the upper extreme of the data and there are fewer nearby observations.\n\nThe shortest interval here is at a concentration of 2, which is presumably nearest the mean of the concentration values:\n\n::: {.cell}\n\n```{.r .cell-code}\nmuscle %>% \n  summarize(mean_conc = mean(Conc))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 1\n  mean_conc\n      <dbl>\n1      1.51\n```\n\n\n:::\n:::\n\nIt's the nearest of the ones here, anyway.\n\n$\\blacksquare$\n\n\n\n(g) (2 points) Suppose you have some new rat tissues, not part of the original dataset, and run the same experiment on these with concentrations 2, 3, and 4 units. What are 95% intervals for the predicted `Length`s that you will observe for these tissues? Display your intervals next to the concentrations they are predictions for.\n\n\nSolution\n\nThese are prediction intervals, not confidence intervals for the mean response, so we have to do them the other way:\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(muscle.1, new, interval = \"p\")\ncbind(new, p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Conc      fit      lwr      upr\n1    2 29.03327 18.70277 39.36376\n2    3 30.81589 20.45725 41.17453\n3    4 25.78063 14.70796 36.85329\n```\n\n\n:::\n:::\n\nExtra: these go down and up a lot further than the confidence intervals for the mean response, because there is more uncertainty involved: these are from individual rats, rather than the average of many, so by chance we could happen to get a `Length` value that is unusually low or high here.\n\n$\\blacksquare$\n",
    "supporting": [
      "regression-revisited_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}