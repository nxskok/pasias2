{
  "hash": "286b63ec5a1fe331bf6cfb380faae2c8",
  "result": {
    "markdown": "# Multiple regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Being satisfied with hospital\n\n\n A hospital administrator collects data to study the\neffect, if any, of a patient's age, the severity of their\nillness, and their anxiety level, on the patient's satisfaction with\ntheir hospital experience. The data, in the file\n[link](http://ritsokiguess.site/datafiles/satisfaction.txt), are\nfor 46 patients in a survey. The columns are: patient's satisfaction\nscore `satis`, on a scale of 0 to 100; the patient's `age` (in\nyears), the `severity` of the patient's illness (also on a\n0--100 scale), and the patient's `anxiety` score on a standard\nanxiety test (scale of 0--5). Higher scores mean greater satisfaction,\nincreased severity of illness and more anxiety.\n\n\n\n(a) Read in the data and check that you have four columns in\nyour data frame, one for each of your variables.   \n \n\n(b) <a name=\"part:scatmat\">*</a> Obtain scatterplots of the response variable\n`satis` against each of the other variables.\n \n\n(c) In your scatterplots of (<a href=\"#part:scatmat\">here</a>), which\nrelationship appears to be the strongest one?\n \n\n(d) <a name=\"part:corrmat\">*</a> Create a correlation matrix for all four \nvariables. Does your strongest trend of the previous part have the\nstrongest correlation?\n \n\n(e) Run a regression predicting satisfaction from the other\nthree variables, and display the output.\n \n\n(f) Does the regression fit well overall? How can you tell?\n \n\n(g) Test the null hypothesis that none of your explanatory\nvariables help, against the alternative that one or more of them\ndo. (You'll need an appropriate P-value. Which one is it?) What do\nyou conclude?\n \n\n(h) The correlation between `severity` and\n`satis` is not small, but in my regression I found that\n`severity` was nowhere near significant. Why is this? Explain briefly.\n\\clearpage\n \n\n(i) Carry out a backward elimination to determine which of\n`age`, `severity` and `anxiety` are needed to\npredict satisfaction. What do you get? Use $\\alpha = 0.10$.\n\n\n\n\n\n\n\n\n##  Salaries of mathematicians\n\n\n A researcher in a scientific\nfoundation wanted to evaluate the relationship between annual salaries\nof mathematicians and three explanatory variables:\n\n\n* an index of work quality\n\n* number of years of experience\n\n* an index of publication success.\n\n\nThe data can be found at\n[link](http://ritsokiguess.site/datafiles/mathsal.txt). Data from\nonly a relatively small number of mathematicians were available.\n\n\n\n(a) Read in the data and check that you have a sensible number\nof rows and the right number of columns. (What does \"a sensible  number of rows\" mean here?)\n\n\n(b) Make scatterplots of `salary` against each of the three explanatory variables. If you can, do this with *one* `ggplot`.\n\n\n(c) Comment briefly on the direction and strength of each\nrelationship with `salary`.\n\n\n(d) <a name=\"regone\">*</a> Fit a regression predicting salary from the other three\nvariables, and obtain a `summary` of the results.\n\n\n(e) How can we justify the statement \n\"one or more of the explanatory variables helps to predict salary\"? How is this\nconsistent with the value of R-squared?\n\n\n(f) Would you consider removing any of the variables from this\nregression? Why, or why not?\n\n\n(g) Do you think it would be a mistake to take *both* of\n`workqual` and `pubsucc` out of the regression? Do a\nsuitable test. Was your guess right?\n\n\n(h) Back in part (<a href=\"#regone\">here</a>), you fitted a regression with all\nthree explanatory variables. By making suitable plots, assess\nwhether there is any evidence that (i) that the linear model should\nbe a curve, (ii) that the residuals are not normally \ndistributed, (iii) that there is \"fan-out\", where the residuals are getting\nbigger *in size* as the fitted values get bigger? Explain\nbriefly how you came to your conclusions in each case.\n\n\n\n\n\n\n##  Predicting GPA of computer science students\n\n\n The file\n[link](http://ritsokiguess.site/datafiles/gpa.txt) contains some\nmeasurements of academic achievement for a number of university\nstudents studying computer science:\n\n\n\n* High school grade point average\n\n* Math SAT score\n\n* Verbal SAT score\n\n* Computer Science grade point average\n\n* Overall university grade point average.\n\n\n\n\n(a) Read in the data and display it (or at least the first ten lines).\n\n\n\n(b) <a name=\"part:hsu-scatter\">*</a> Make a scatterplot of high school GPA against university\nGPA. Which variable should be the response and which\nexplanatory? Explain briefly. Add a smooth trend to your plot.\n\n\n\n(c) Describe any relationship on your scatterplot: its direction, its\nstrength and its shape. Justify your description briefly.\n\n\n\n(d) <a name=\"part:highonly\">*</a> Fit a linear regression for predicting university GPA\nfrom high-school GPA and display the results.\n\n\n\n(e) Two students have been admitted to university. One has\na high school GPA of 3.0 and the other a high school GPA of   \n3.5. Obtain suitable intervals that summarize the GPAs that each of these\ntwo students might obtain in university.\n\n\n\n(f) <a name=\"part:all\">*</a> Now obtain a regression predicting university GPA from\nhigh-school GPA as well as the two SAT scores. Display your results.\n\n\n\n(g) Test whether adding the two SAT scores has improved the\nprediction of university GPA. What do you conclude?\n\n\n\n\n(h) Carry out a backward elimination starting out from your\nmodel in part (<a href=\"#part:all\">here</a>). Which model do you end up with?\nIs it the same model as you fit in (<a href=\"#part:highonly\">here</a>)?\n\n\n\n(i) These students were studying computer science at\nuniversity. Do you find your backward-elimination result\nsensible or surprising, given this? Explain briefly.\n\n\n\n\n\n\n\n\n## Fish and mercury\n\nEating fish is generally healthy, but sometimes the fish is contaminated with mercury. What might affect the amount of mercury in a fish? Large-mouth bass were studied in Florida lakes to examine factors that might affect the amount of mercury contamination. 38 lakes were used for the study. Water samples were taken from each lake and analyzed. Also, samples of fish were caught in each lake and the mercury concentration in their muscle tissue was measured. The resulting data are in [http://ritsokiguess.site/datafiles/mercury.txt](http://ritsokiguess.site/datafiles/mercury.txt), separated by single spaces. The variables measured were as follows:\n\n-   standardized mercury level (parts per million in 3-year-old fish)\n-   alkalinity of water (milligrams per litre)\n-   calcium level of water (milligrams per litre)\n-   pH of water (standard scale; see eg. [this](https://www.usgs.gov/special-topic/water-science-school/science/ph-and-water?qt-science_center_objects=0#qt-science_center_objects))\n\n\n\n(a) Read in and display (some of) the data.\n\n\n\n(b) Plot the mercury levels against each of the explanatory variables.\n\n\n\n(c) Describe any trends (or lack thereof) that you see on your graphs.\n\n\n\n(d) Concerned by some of what you see on your plots, you consult with a fisheries scientist, who says that using the logs of `alkalinity` and `calcium` are often used in modelling this kind of data. Add columns containing the logs of these two variables to your dataframe, and run a regression predicting `mercury` from `ph` and the two new variables. Display the output from your regression.\n\n\n\n(e) What might you learn from running Box-Cox here? Explain briefly, then run it (on the same variables as your regression above) and describe what the results tell you.\n\n\n\n(f) Using the results of the Box-Cox procedure and your previous work in the question, what seems to be the most appropriate regression now? Fit it, and display the results. \n\n\n\n(g) Obtain all the standard residual plots (the ones we have seen in this course) for this model. Describe any problems you see.\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n##  Being satisfied with hospital\n\n\n A hospital administrator collects data to study the\neffect, if any, of a patient's age, the severity of their\nillness, and their anxiety level, on the patient's satisfaction with\ntheir hospital experience. The data, in the file\n[link](http://ritsokiguess.site/datafiles/satisfaction.txt), are\nfor 46 patients in a survey. The columns are: patient's satisfaction\nscore `satis`, on a scale of 0 to 100; the patient's `age` (in\nyears), the `severity` of the patient's illness (also on a\n0--100 scale), and the patient's `anxiety` score on a standard\nanxiety test (scale of 0--5). Higher scores mean greater satisfaction,\nincreased severity of illness and more anxiety.\n\n\n\n(a) Read in the data and check that you have four columns in\nyour data frame, one for each of your variables.   \n \nSolution\n\n This one requires a little thought\nfirst. The data values are aligned in columns, and so are the\ncolumn headers. Thus, `read_table` is what we need:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/satisfaction.txt\"\nsatisf <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  satis = col_double(),\n  age = col_double(),\n  severity = col_double(),\n  anxiety = col_double()\n)\n```\n:::\n\n```{.r .cell-code}\nsatisf\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"satis\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"age\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"severity\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"anxiety\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"48\",\"2\":\"50\",\"3\":\"51\",\"4\":\"2.3\"},{\"1\":\"57\",\"2\":\"36\",\"3\":\"46\",\"4\":\"2.3\"},{\"1\":\"66\",\"2\":\"40\",\"3\":\"48\",\"4\":\"2.2\"},{\"1\":\"70\",\"2\":\"41\",\"3\":\"44\",\"4\":\"1.8\"},{\"1\":\"89\",\"2\":\"28\",\"3\":\"43\",\"4\":\"1.8\"},{\"1\":\"36\",\"2\":\"49\",\"3\":\"54\",\"4\":\"2.9\"},{\"1\":\"46\",\"2\":\"42\",\"3\":\"50\",\"4\":\"2.2\"},{\"1\":\"54\",\"2\":\"45\",\"3\":\"48\",\"4\":\"2.4\"},{\"1\":\"26\",\"2\":\"52\",\"3\":\"62\",\"4\":\"2.9\"},{\"1\":\"77\",\"2\":\"29\",\"3\":\"50\",\"4\":\"2.1\"},{\"1\":\"89\",\"2\":\"29\",\"3\":\"48\",\"4\":\"2.4\"},{\"1\":\"67\",\"2\":\"43\",\"3\":\"53\",\"4\":\"2.4\"},{\"1\":\"47\",\"2\":\"38\",\"3\":\"55\",\"4\":\"2.2\"},{\"1\":\"51\",\"2\":\"34\",\"3\":\"51\",\"4\":\"2.3\"},{\"1\":\"57\",\"2\":\"53\",\"3\":\"54\",\"4\":\"2.2\"},{\"1\":\"66\",\"2\":\"36\",\"3\":\"49\",\"4\":\"2.0\"},{\"1\":\"79\",\"2\":\"33\",\"3\":\"56\",\"4\":\"2.5\"},{\"1\":\"88\",\"2\":\"29\",\"3\":\"46\",\"4\":\"1.9\"},{\"1\":\"60\",\"2\":\"33\",\"3\":\"49\",\"4\":\"2.1\"},{\"1\":\"49\",\"2\":\"55\",\"3\":\"51\",\"4\":\"2.4\"},{\"1\":\"77\",\"2\":\"29\",\"3\":\"52\",\"4\":\"2.3\"},{\"1\":\"52\",\"2\":\"44\",\"3\":\"58\",\"4\":\"2.9\"},{\"1\":\"60\",\"2\":\"43\",\"3\":\"50\",\"4\":\"2.3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n     \n\n46 rows and 4 columns: satisfaction score (response), age, severity\nand anxiety (explanatory).\n\nThere is a small question about what to call the data\nframe. Basically, anything other than `satis` will do, since\nthere will be confusion if your data frame has the same name as one of\nits columns.\n \n$\\blacksquare$\n\n(b) <a name=\"part:scatmat\">*</a> Obtain scatterplots of the response variable\n`satis` against each of the other variables.\n \nSolution\n\n\nThe obvious way is to do these one after the other:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(satisf, aes(x = age, y = satis)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/hospital-2-1.png){width=384}\n:::\n\n```{.r .cell-code}\nggplot(satisf, aes(x = severity, y = satis)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/hospital-2-2.png){width=384}\n:::\n\n```{.r .cell-code}\nggplot(satisf, aes(x = anxiety, y = satis)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/hospital-2-3.png){width=384}\n:::\n:::\n\n       \n\nThis is fine, but there is also a way of getting all three plots with\n*one* `ggplot`. This uses the `facet_wrap` trick,\nbut to set *that* up, we have to have all the $x$-variables in\n*one* column, with an extra column labelling which $x$-variable\nthat value was. This uses `pivot_longer`. The right way to do this is\nin a pipeline:\n\n::: {.cell}\n\n```{.r .cell-code}\nsatisf %>%\n  pivot_longer(-satis, names_to=\"xname\", values_to=\"x\") %>%\n  ggplot(aes(x = x, y = satis)) + geom_point() +\n  facet_wrap(~xname, scales = \"free\", ncol = 2)\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/hospital-3-1.png){width=672}\n:::\n:::\n\n \n\nSteps: collect together the columns age through anxiety into one column\nwhose values go in `x`, with names in `xname`, then plot this new\n`x` against satisfaction score, with a separate facet for each\ndifferent $x$ (in `xname`). \n\nWhat's the difference\nbetween `facet_grid` and `facet_wrap`? The difference is that with\n`facet_wrap`, we are letting `ggplot` arrange the\nfacets how it wants to. In this case, we didn't care which explanatory\nvariable went on which facet, just as long as we saw all of them\nsomewhere. Inside `facet_wrap` there are *no dots*: a\nsquiggle, followed by the name(s) of the variable(s) that\ndistinguish(es) the facets.^[If there are more than one, they  should be separated by plus signs as in `lm`. Each facet then  has as many labels as variables. I haven't actually done this  myself, but from looking at examples, I think this is the way it  works.] \nThe only \"design\" decision I made here was that the facets\nshould be arranged somehow in two columns, but I didn't care which\nones should be where.\n\nIn `facet_grid`, you have a variable that you want to be\ndisplayed in rows or in columns (not just in \"different facets\"). \nI'll show you how that works here. Since I am going to draw\ntwo plots, I should save the long data frame first and re-use it,\nrather than calculating it twice (so that I ought now to go back and\ndo the other one using the saved data frame, really):\n\n::: {.cell}\n\n```{.r .cell-code}\nsatisf %>% \n  pivot_longer(age:anxiety, names_to=\"xname\", \n               values_to=\"x\") -> satisf.long\nsatisf.long\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"satis\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"xname\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"x\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"48\",\"2\":\"age\",\"3\":\"50.0\"},{\"1\":\"48\",\"2\":\"severity\",\"3\":\"51.0\"},{\"1\":\"48\",\"2\":\"anxiety\",\"3\":\"2.3\"},{\"1\":\"57\",\"2\":\"age\",\"3\":\"36.0\"},{\"1\":\"57\",\"2\":\"severity\",\"3\":\"46.0\"},{\"1\":\"57\",\"2\":\"anxiety\",\"3\":\"2.3\"},{\"1\":\"66\",\"2\":\"age\",\"3\":\"40.0\"},{\"1\":\"66\",\"2\":\"severity\",\"3\":\"48.0\"},{\"1\":\"66\",\"2\":\"anxiety\",\"3\":\"2.2\"},{\"1\":\"70\",\"2\":\"age\",\"3\":\"41.0\"},{\"1\":\"70\",\"2\":\"severity\",\"3\":\"44.0\"},{\"1\":\"70\",\"2\":\"anxiety\",\"3\":\"1.8\"},{\"1\":\"89\",\"2\":\"age\",\"3\":\"28.0\"},{\"1\":\"89\",\"2\":\"severity\",\"3\":\"43.0\"},{\"1\":\"89\",\"2\":\"anxiety\",\"3\":\"1.8\"},{\"1\":\"36\",\"2\":\"age\",\"3\":\"49.0\"},{\"1\":\"36\",\"2\":\"severity\",\"3\":\"54.0\"},{\"1\":\"36\",\"2\":\"anxiety\",\"3\":\"2.9\"},{\"1\":\"46\",\"2\":\"age\",\"3\":\"42.0\"},{\"1\":\"46\",\"2\":\"severity\",\"3\":\"50.0\"},{\"1\":\"46\",\"2\":\"anxiety\",\"3\":\"2.2\"},{\"1\":\"54\",\"2\":\"age\",\"3\":\"45.0\"},{\"1\":\"54\",\"2\":\"severity\",\"3\":\"48.0\"},{\"1\":\"54\",\"2\":\"anxiety\",\"3\":\"2.4\"},{\"1\":\"26\",\"2\":\"age\",\"3\":\"52.0\"},{\"1\":\"26\",\"2\":\"severity\",\"3\":\"62.0\"},{\"1\":\"26\",\"2\":\"anxiety\",\"3\":\"2.9\"},{\"1\":\"77\",\"2\":\"age\",\"3\":\"29.0\"},{\"1\":\"77\",\"2\":\"severity\",\"3\":\"50.0\"},{\"1\":\"77\",\"2\":\"anxiety\",\"3\":\"2.1\"},{\"1\":\"89\",\"2\":\"age\",\"3\":\"29.0\"},{\"1\":\"89\",\"2\":\"severity\",\"3\":\"48.0\"},{\"1\":\"89\",\"2\":\"anxiety\",\"3\":\"2.4\"},{\"1\":\"67\",\"2\":\"age\",\"3\":\"43.0\"},{\"1\":\"67\",\"2\":\"severity\",\"3\":\"53.0\"},{\"1\":\"67\",\"2\":\"anxiety\",\"3\":\"2.4\"},{\"1\":\"47\",\"2\":\"age\",\"3\":\"38.0\"},{\"1\":\"47\",\"2\":\"severity\",\"3\":\"55.0\"},{\"1\":\"47\",\"2\":\"anxiety\",\"3\":\"2.2\"},{\"1\":\"51\",\"2\":\"age\",\"3\":\"34.0\"},{\"1\":\"51\",\"2\":\"severity\",\"3\":\"51.0\"},{\"1\":\"51\",\"2\":\"anxiety\",\"3\":\"2.3\"},{\"1\":\"57\",\"2\":\"age\",\"3\":\"53.0\"},{\"1\":\"57\",\"2\":\"severity\",\"3\":\"54.0\"},{\"1\":\"57\",\"2\":\"anxiety\",\"3\":\"2.2\"},{\"1\":\"66\",\"2\":\"age\",\"3\":\"36.0\"},{\"1\":\"66\",\"2\":\"severity\",\"3\":\"49.0\"},{\"1\":\"66\",\"2\":\"anxiety\",\"3\":\"2.0\"},{\"1\":\"79\",\"2\":\"age\",\"3\":\"33.0\"},{\"1\":\"79\",\"2\":\"severity\",\"3\":\"56.0\"},{\"1\":\"79\",\"2\":\"anxiety\",\"3\":\"2.5\"},{\"1\":\"88\",\"2\":\"age\",\"3\":\"29.0\"},{\"1\":\"88\",\"2\":\"severity\",\"3\":\"46.0\"},{\"1\":\"88\",\"2\":\"anxiety\",\"3\":\"1.9\"},{\"1\":\"60\",\"2\":\"age\",\"3\":\"33.0\"},{\"1\":\"60\",\"2\":\"severity\",\"3\":\"49.0\"},{\"1\":\"60\",\"2\":\"anxiety\",\"3\":\"2.1\"},{\"1\":\"49\",\"2\":\"age\",\"3\":\"55.0\"},{\"1\":\"49\",\"2\":\"severity\",\"3\":\"51.0\"},{\"1\":\"49\",\"2\":\"anxiety\",\"3\":\"2.4\"},{\"1\":\"77\",\"2\":\"age\",\"3\":\"29.0\"},{\"1\":\"77\",\"2\":\"severity\",\"3\":\"52.0\"},{\"1\":\"77\",\"2\":\"anxiety\",\"3\":\"2.3\"},{\"1\":\"52\",\"2\":\"age\",\"3\":\"44.0\"},{\"1\":\"52\",\"2\":\"severity\",\"3\":\"58.0\"},{\"1\":\"52\",\"2\":\"anxiety\",\"3\":\"2.9\"},{\"1\":\"60\",\"2\":\"age\",\"3\":\"43.0\"},{\"1\":\"60\",\"2\":\"severity\",\"3\":\"50.0\"},{\"1\":\"60\",\"2\":\"anxiety\",\"3\":\"2.3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nIf, at this or any stage, you get confused, the way to un-confuse\nyourself is to *fire up R Studio and do this yourself*. You have\nall the data and code you need. If you do it yourself, you can run\npipes one line at a time, inspect things, and so on.\n\nFirst, making a *row* of plots, so that `xname` is the $x$\nof the facets:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(satisf.long, aes(x = x, y = satis)) + geom_point() +\n  facet_grid(. ~ xname, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/hospital-5-1.png){width=672}\n:::\n:::\n\n \n\nI find these too tall and skinny to see the trends, as on the first\n`facet_wrap` plot.\n\nAnd now, making a *column* of plots, with `xname` as $y$:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(satisf.long, aes(x = x, y = satis)) + geom_point() +\n  facet_grid(xname ~ ., scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/hospital-6-1.png){width=672}\n:::\n:::\n\n \n\nThis one looks weird because the three $x$-variables are on different\nscales. The effect of the `scales=\"free\"` is to allow the\n`satis` scale to vary, but the `x` scale cannot because\nthe facets are all in a line. Compare this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(satisf.long, aes(x = x, y = satis)) + geom_point() +\n  facet_wrap(~xname, ncol = 1, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/hospital-7-1.png){width=672}\n:::\n:::\n\n \n\nThis time, the $x$ scales came out different (and suitable), but I\nstill like squarer plots better for judging relationships.\n \n$\\blacksquare$\n\n(c) In your scatterplots of (<a href=\"#part:scatmat\">here</a>), which\nrelationship appears to be the strongest one?\n \nSolution\n\n\nAll the trends appear to be downward ones, but\nI think `satis` and `age` is the strongest\ntrend. The other ones look more scattered to me. \n \n$\\blacksquare$\n\n(d) <a name=\"part:corrmat\">*</a> Create a correlation matrix for all four \nvariables. Does your strongest trend of the previous part have the\nstrongest correlation?\n \nSolution\n\n\nThis is a matter of running the whole data frame through `cor`:\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(satisf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              satis        age   severity    anxiety\nsatis     1.0000000 -0.7736828 -0.5874444 -0.6023105\nage      -0.7736828  1.0000000  0.4666091  0.4976945\nseverity -0.5874444  0.4666091  1.0000000  0.7945275\nanxiety  -0.6023105  0.4976945  0.7945275  1.0000000\n```\n:::\n:::\n\n     \n\nIgnoring the correlations of variables with themselves, the\ncorrelation of `satisf` with `age`, the one I picked\nout, is the strongest (the most negative trend). If you picked one of\nthe other trends as the strongest, you need to note how close it is to\nthe maximum correlation: for example, if you picked `satis`\nand `severity`, that's the second highest correlation (in\nsize).\n \n$\\blacksquare$\n\n(e) Run a regression predicting satisfaction from the other\nthree variables, and display the output.\n \nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsatisf.1 <- lm(satis ~ age + severity + anxiety, data = satisf)\nsummary(satisf.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = satis ~ age + severity + anxiety, data = satisf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.954  -7.154   1.550   6.599  14.888 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 162.8759    25.7757   6.319 4.59e-06 ***\nage          -1.2103     0.3015  -4.015  0.00074 ***\nseverity     -0.6659     0.8210  -0.811  0.42736    \nanxiety      -8.6130    12.2413  -0.704  0.49021    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.29 on 19 degrees of freedom\nMultiple R-squared:  0.6727,\tAdjusted R-squared:  0.621 \nF-statistic: 13.01 on 3 and 19 DF,  p-value: 7.482e-05\n```\n:::\n:::\n\n     \n \n$\\blacksquare$\n\n(f) Does the regression fit well overall? How can you tell?\n \nSolution\n\n\nFor this, look at R-squared, which is 0.682 (68.2\\%). This is one\nof those things to have an opinion about. I'd say this is good but\nnot great. I would not call it \"poor\", since there definitely\n*is* a relationship, even if it's not a stupendously good one.\n \n$\\blacksquare$\n\n(g) Test the null hypothesis that none of your explanatory\nvariables help, against the alternative that one or more of them\ndo. (You'll need an appropriate P-value. Which one is it?) What do\nyou conclude?\n \nSolution\n\n\nThis one is the (global) $F$-test, whose P-value is at the\nbottom. It translates to 0.000000000154, so this is\n*definitely* small, and we reject the null. Thus, one or more\nof `age`, `severity` and `anxiety` helps to\npredict satisfaction. (I would like to see this last sentence,\nrather than just \"reject the null\".)\n \n$\\blacksquare$\n\n(h) The correlation between `severity` and\n`satis` is not small, but in my regression I found that\n`severity` was nowhere near significant. Why is this? Explain briefly.\n\\clearpage\n \nSolution\n\n\nThe key thing to observe is that the $t$-test in the regression\nsays how important a variable is \n*given the others that are already in the regression*, or, if you prefer, how much that\nvariable *adds* to the regression, on top of the ones that\nare already there. So here, we are saying\nthat `severity` has nothing to add, given that the\nregression already includes the others. (That is, high correlation\nand strong significance don't always go together.)\nFor a little more insight, look at the correlation matrix of\n(<a href=\"#part:corrmat\">here</a>) again. The strongest trend with\n`satis` is with `age`, and indeed `age` is\nthe one obviously significant  variable in the regression. The\ntrend of `severity` with `satis` is somewhat\ndownward, and you might otherwise have guessed that this is strong\nenough to be significant. But see that `severity`\n*also* has a clear relationship with `age`. A patient\nwith low severity of disease is probably also younger, and we know\nthat younger patients are likely to be more satisfied. Thus\nseverity has nothing (much) to add.\n\nExtra 1:\nThe multiple regression is actually doing something clever\nhere. Just looking at the correlations, it appears that all three\nvariables are helpful, but the regression is saying that once you\nhave looked at `age` (\"controlled for age\"),\nseverity of illness does not have an impact: the correlation of\n`severity` with `satis` is as big as it is almost\nentirely because of `age`. \nThis gets into the      domain of \"partial correlation\". If you like videos, you can \nsee [link](https://www.youtube.com/watch?v=LF0WAVBIhNA) for\nthis. I prefer regression, myself, since I find it clearer.\n`anxiety`\ntells a different story: this is close to significant (or\n*is* significant at the $\\alpha=0.10$ level), so the\nregression is saying that `anxiety` *does* appear to\nhave something to say about `satis` over and above\n`age`. This is rather odd, to my mind, since\n`anxiety` has only a slightly stronger correlation with\n`satis` and about the same with `age` as\n`severity` does. But the regression is telling the story to\nbelieve, because it handles all the inter-correlations, not just\nthe ones between pairs of variables.\n\nI thought it would be rather interesting to do some predictions\nhere. Let's predict satisfaction for all combinations of high and\nlow age, severity and anxiety. I'll use the quartiles for high and\nlow. \n\nThe easiest way to get those is via `summary`:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(satisf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     satis            age           severity        anxiety     \n Min.   :26.00   Min.   :28.00   Min.   :43.00   Min.   :1.800  \n 1st Qu.:50.00   1st Qu.:33.00   1st Qu.:48.00   1st Qu.:2.150  \n Median :60.00   Median :40.00   Median :50.00   Median :2.300  \n Mean   :61.35   Mean   :39.61   Mean   :50.78   Mean   :2.296  \n 3rd Qu.:73.50   3rd Qu.:44.50   3rd Qu.:53.50   3rd Qu.:2.400  \n Max.   :89.00   Max.   :55.00   Max.   :62.00   Max.   :2.900  \n```\n:::\n:::\n\nand then use `datagrid` to make combinations for prediction:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = satisf.1, age = c(33, 44.5), \n                severity = c(48, 53.5), anxiety = c(2.15, 2.4))\nnew\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"satis\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"age\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"severity\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"anxiety\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"61.34783\",\"2\":\"33.0\",\"3\":\"48.0\",\"4\":\"2.15\"},{\"1\":\"61.34783\",\"2\":\"33.0\",\"3\":\"48.0\",\"4\":\"2.40\"},{\"1\":\"61.34783\",\"2\":\"33.0\",\"3\":\"53.5\",\"4\":\"2.15\"},{\"1\":\"61.34783\",\"2\":\"33.0\",\"3\":\"53.5\",\"4\":\"2.40\"},{\"1\":\"61.34783\",\"2\":\"44.5\",\"3\":\"48.0\",\"4\":\"2.15\"},{\"1\":\"61.34783\",\"2\":\"44.5\",\"3\":\"48.0\",\"4\":\"2.40\"},{\"1\":\"61.34783\",\"2\":\"44.5\",\"3\":\"53.5\",\"4\":\"2.15\"},{\"1\":\"61.34783\",\"2\":\"44.5\",\"3\":\"53.5\",\"4\":\"2.40\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nEight rows for the $2^3 = 8$ combinations. Then get the predictions for these:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(satisf.1, newdata = new))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"rowid\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"satis\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"age\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"severity\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"anxiety\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"72.45391\",\"3\":\"2.844181\",\"4\":\"25.47444\",\"5\":\"3.785221e-143\",\"6\":\"66.87942\",\"7\":\"78.02840\",\"8\":\"61.34783\",\"9\":\"33.0\",\"10\":\"48.0\",\"11\":\"2.15\"},{\"1\":\"2\",\"2\":\"70.30065\",\"3\":\"4.435481\",\"4\":\"15.84961\",\"5\":\"1.414492e-56\",\"6\":\"61.60727\",\"7\":\"78.99404\",\"8\":\"61.34783\",\"9\":\"33.0\",\"10\":\"48.0\",\"11\":\"2.40\"},{\"1\":\"3\",\"2\":\"68.79143\",\"3\":\"4.699081\",\"4\":\"14.63934\",\"5\":\"1.575972e-48\",\"6\":\"59.58140\",\"7\":\"78.00146\",\"8\":\"61.34783\",\"9\":\"33.0\",\"10\":\"53.5\",\"11\":\"2.15\"},{\"1\":\"4\",\"2\":\"66.63817\",\"3\":\"3.661656\",\"4\":\"18.19892\",\"5\":\"5.263713e-74\",\"6\":\"59.46146\",\"7\":\"73.81489\",\"8\":\"61.34783\",\"9\":\"33.0\",\"10\":\"53.5\",\"11\":\"2.40\"},{\"1\":\"5\",\"2\":\"58.53525\",\"3\":\"3.370091\",\"4\":\"17.36904\",\"5\":\"1.415637e-67\",\"6\":\"51.93000\",\"7\":\"65.14051\",\"8\":\"61.34783\",\"9\":\"44.5\",\"10\":\"48.0\",\"11\":\"2.15\"},{\"1\":\"6\",\"2\":\"56.38200\",\"3\":\"4.233879\",\"4\":\"13.31686\",\"5\":\"1.847055e-40\",\"6\":\"48.08374\",\"7\":\"64.68025\",\"8\":\"61.34783\",\"9\":\"44.5\",\"10\":\"48.0\",\"11\":\"2.40\"},{\"1\":\"7\",\"2\":\"54.87277\",\"3\":\"4.595489\",\"4\":\"11.94057\",\"5\":\"7.271935e-33\",\"6\":\"45.86578\",\"7\":\"63.87976\",\"8\":\"61.34783\",\"9\":\"44.5\",\"10\":\"53.5\",\"11\":\"2.15\"},{\"1\":\"8\",\"2\":\"52.71951\",\"3\":\"2.725614\",\"4\":\"19.34225\",\"5\":\"2.369126e-83\",\"6\":\"47.37741\",\"7\":\"58.06162\",\"8\":\"61.34783\",\"9\":\"44.5\",\"10\":\"53.5\",\"11\":\"2.40\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nExtra 2: the standard errors vary quite a bit. The smallest ones are where age, severity, and anxiety are all high or all low (the last row and the first one). This is where most of the data is, because the three explanatory variables are positively correlated with each other (if you know that one of them is high, the others will probably be high too). The other standard errors are higher because there is not much data \"nearby\", and so we don't know as much about the quality of the predictions there.\n\nExtra 3: we had to copy the quartile values into the new dataframe we were making (to predict from), which ought to have caused you some concern, since there was no guarantee that we copied them correctly. It would be better to make a dataframe with just the quartiles, and feed that into `datagrid`. Here's how we can do that.\n\n::: {.cell}\n\n```{.r .cell-code}\nsatisf %>% \n  summarize(across(-satis,\n                   \\(x) quantile(x, c(0.25, 0.75)))) -> d\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n```\n:::\n\n```{.r .cell-code}\nd\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"age\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"severity\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"anxiety\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"33.0\",\"2\":\"48.0\",\"3\":\"2.15\"},{\"1\":\"44.5\",\"2\":\"53.5\",\"3\":\"2.40\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nTo summarize several columns at once, use `across`. This one reads \"for each column that is not `satis`, work out the first and third quartiles (25th and 75th percentiles) of it. Recall that the first input to `quantile` is what to compute percentiles of, and the optional^[If you don't give it, you get the five-number summary.] second input is which percentiles to compute. Also, when `summarize` is fed a summary that is more than one number long (two quartiles, here) it will automatically be unnested longer, which happens to be exactly what we want here.\n\nSo now, we need to take the columns from here and feed them into `datagrid`. The way to do that is to use `with`:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- with(d, datagrid(model = satisf.1, age = age, severity = severity, anxiety = anxiety))\nnew\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"satis\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"age\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"severity\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"anxiety\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"61.34783\",\"2\":\"33.0\",\"3\":\"48.0\",\"4\":\"2.15\"},{\"1\":\"61.34783\",\"2\":\"33.0\",\"3\":\"48.0\",\"4\":\"2.40\"},{\"1\":\"61.34783\",\"2\":\"33.0\",\"3\":\"53.5\",\"4\":\"2.15\"},{\"1\":\"61.34783\",\"2\":\"33.0\",\"3\":\"53.5\",\"4\":\"2.40\"},{\"1\":\"61.34783\",\"2\":\"44.5\",\"3\":\"48.0\",\"4\":\"2.15\"},{\"1\":\"61.34783\",\"2\":\"44.5\",\"3\":\"48.0\",\"4\":\"2.40\"},{\"1\":\"61.34783\",\"2\":\"44.5\",\"3\":\"53.5\",\"4\":\"2.15\"},{\"1\":\"61.34783\",\"2\":\"44.5\",\"3\":\"53.5\",\"4\":\"2.40\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThe clunky repetition is needed because the first (eg.) `age` in `age = age` is the name that the column in `new` is going to get, and the second `age` is the thing that supplies the values to be combined (the column of `d` called `age`). This is exactly the same `new` that we had before, and so the predictions will be exactly the same as they were before.\n\n$\\blacksquare$\n\n(i) Carry out a backward elimination to determine which of\n`age`, `severity` and `anxiety` are needed to\npredict satisfaction. What do you get? Use $\\alpha = 0.10$.\n\n\n\nSolution\n\n\nThis means starting with the regression containing all the explanatory\nvariables, which is the one I called `satisf.1`:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(satisf.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = satis ~ age + severity + anxiety, data = satisf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.954  -7.154   1.550   6.599  14.888 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 162.8759    25.7757   6.319 4.59e-06 ***\nage          -1.2103     0.3015  -4.015  0.00074 ***\nseverity     -0.6659     0.8210  -0.811  0.42736    \nanxiety      -8.6130    12.2413  -0.704  0.49021    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.29 on 19 degrees of freedom\nMultiple R-squared:  0.6727,\tAdjusted R-squared:  0.621 \nF-statistic: 13.01 on 3 and 19 DF,  p-value: 7.482e-05\n```\n:::\n:::\n\n \n\nPull out the least-significant (highest P-value) variable, which here\nis `severity`. We already decided that this had nothing to add:\n\n::: {.cell}\n\n```{.r .cell-code}\nsatisf.2 <- update(satisf.1, . ~ . - severity)\nsummary(satisf.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = satis ~ age + anxiety, data = satisf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.868  -6.649   2.506   6.445  16.120 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 147.0751    16.7334   8.789 2.64e-08 ***\nage          -1.2434     0.2961  -4.199 0.000442 ***\nanxiety     -15.8906     8.2556  -1.925 0.068593 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.2 on 20 degrees of freedom\nMultiple R-squared:  0.6613,\tAdjusted R-squared:  0.6275 \nF-statistic: 19.53 on 2 and 20 DF,  p-value: 1.985e-05\n```\n:::\n:::\n\n\n\nIf you like, copy and paste the first `lm`, edit it to get rid\nof `severity`, and run it again. But when I have a \n\"small change\" to make to a model, I like to use `update`.\n\n\n\nHaving taken `severity` out, `anxiety` has become\nsignificant (at $\\alpha = 0.10$). Since all of the explanatory variables are now\nsignificant, this is where we stop. If we're predicting satisfaction,\nwe need to know both a patient's age and their anxiety score: being\nolder or more anxious is associated with a *decrease* in satisfaction.\n\nThere is also a function `step` that will do this for you:\n\n::: {.cell}\n\n```{.r .cell-code}\nstep(satisf.1, direction = \"backward\", test = \"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=110.84\nsatis ~ age + severity + anxiety\n\n           Df Sum of Sq    RSS    AIC F value    Pr(>F)    \n- anxiety   1     52.41 2064.0 109.43  0.4951 0.4902110    \n- severity  1     69.65 2081.2 109.62  0.6579 0.4273559    \n<none>                  2011.6 110.84                      \n- age       1   1706.67 3718.3 122.97 16.1200 0.0007404 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=109.43\nsatis ~ age + severity\n\n           Df Sum of Sq    RSS    AIC F value    Pr(>F)    \n<none>                  2064.0 109.43                      \n- severity  1    402.78 2466.8 111.53  3.9029 0.0621629 .  \n- age       1   1960.56 4024.6 122.79 18.9977 0.0003042 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = satis ~ age + severity, data = satisf)\n\nCoefficients:\n(Intercept)          age     severity  \n    166.591       -1.260       -1.089  \n```\n:::\n:::\n\n \n\nwith the same result.^[This is because we used $\\alpha = 0.10$. `step` tends to keep explanatory variables that you might consider marginal because it uses AIC (see below) rather than P-values directly.] This function doesn't actually use P-values;\ninstead it uses a thing called AIC. At each step, the variable with\nthe lowest AIC comes out, and when `<none>` bubbles up to the\ntop, that's when you stop. The `test=\"F\"` means \n\"include an $F$-test\", but the procedure still uses AIC (it just shows you an\n$F$-test each time as well).  In this case, the other variables were\nin the same order throughout, but they don't have to be (in the same\nway that removing one variable from a multiple regression can\ndramatically change the P-values of the ones that remain). Here, at\nthe first step, `<none>` and `anxiety` were pretty\nclose, but when `severity` came out, taking out nothing was a\n*lot* better than taking out `anxiety`.\n\nThe `test=\"F\"` on the end gets you the P-values. Using the\n$F$-test is right for regressions; for things like logistic regression\nthat we see later, `test=\"Chisq\"` is the right one to \nuse.^[This is \"F\" in quotes, meaning $F$-test, not \"F\" without quotes, which means FALSE.]\n\n$\\blacksquare$\n\n\n\n\n\n##  Salaries of mathematicians\n\n\n A researcher in a scientific\nfoundation wanted to evaluate the relationship between annual salaries\nof mathematicians and three explanatory variables:\n\n\n* an index of work quality\n\n* number of years of experience\n\n* an index of publication success.\n\n\nThe data can be found at\n[link](http://ritsokiguess.site/datafiles/mathsal.txt). Data from\nonly a relatively small number of mathematicians were available.\n\n\n\n(a) Read in the data and check that you have a sensible number\nof rows and the right number of columns. (What does \"a sensible  number of rows\" mean here?)\n\nSolution\n\n\nThis is a tricky one. There are aligned columns, but *the column headers are not aligned with them*. \nThus `read_table2` is what you need.\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/mathsal.txt\"\nsalaries <- read_table2(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `read_table2()` was deprecated in readr 2.0.0.\nℹ Please use `read_table()` instead.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  salary = col_double(),\n  workqual = col_double(),\n  experience = col_double(),\n  pubsucc = col_double()\n)\n```\n:::\n\n```{.r .cell-code}\nsalaries\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"salary\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"workqual\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"experience\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"pubsucc\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"33.2\",\"2\":\"3.5\",\"3\":\"9\",\"4\":\"6.1\"},{\"1\":\"40.3\",\"2\":\"5.3\",\"3\":\"20\",\"4\":\"6.4\"},{\"1\":\"38.7\",\"2\":\"5.1\",\"3\":\"18\",\"4\":\"7.4\"},{\"1\":\"46.8\",\"2\":\"5.8\",\"3\":\"33\",\"4\":\"6.7\"},{\"1\":\"41.4\",\"2\":\"4.2\",\"3\":\"31\",\"4\":\"7.5\"},{\"1\":\"37.5\",\"2\":\"6.0\",\"3\":\"13\",\"4\":\"5.9\"},{\"1\":\"39.0\",\"2\":\"6.8\",\"3\":\"25\",\"4\":\"6.0\"},{\"1\":\"40.7\",\"2\":\"5.5\",\"3\":\"30\",\"4\":\"4.0\"},{\"1\":\"30.1\",\"2\":\"3.1\",\"3\":\"5\",\"4\":\"5.8\"},{\"1\":\"52.9\",\"2\":\"7.2\",\"3\":\"47\",\"4\":\"8.3\"},{\"1\":\"38.2\",\"2\":\"4.5\",\"3\":\"25\",\"4\":\"5.0\"},{\"1\":\"31.8\",\"2\":\"4.9\",\"3\":\"11\",\"4\":\"6.4\"},{\"1\":\"43.3\",\"2\":\"8.0\",\"3\":\"23\",\"4\":\"7.6\"},{\"1\":\"44.1\",\"2\":\"6.5\",\"3\":\"35\",\"4\":\"7.0\"},{\"1\":\"42.8\",\"2\":\"6.6\",\"3\":\"39\",\"4\":\"5.0\"},{\"1\":\"33.6\",\"2\":\"3.7\",\"3\":\"21\",\"4\":\"4.4\"},{\"1\":\"34.2\",\"2\":\"6.2\",\"3\":\"7\",\"4\":\"5.5\"},{\"1\":\"48.0\",\"2\":\"7.0\",\"3\":\"40\",\"4\":\"7.0\"},{\"1\":\"38.0\",\"2\":\"4.0\",\"3\":\"35\",\"4\":\"6.0\"},{\"1\":\"35.9\",\"2\":\"4.5\",\"3\":\"23\",\"4\":\"3.5\"},{\"1\":\"40.4\",\"2\":\"5.9\",\"3\":\"33\",\"4\":\"4.9\"},{\"1\":\"36.8\",\"2\":\"5.6\",\"3\":\"27\",\"4\":\"4.3\"},{\"1\":\"45.2\",\"2\":\"4.8\",\"3\":\"34\",\"4\":\"8.0\"},{\"1\":\"35.1\",\"2\":\"3.9\",\"3\":\"15\",\"4\":\"5.0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n        \n\n24 observations (\"only a relatively small number\") and 4 columns,\none for the response and one each for the explanatory variables.\n\nOr, if you like,\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(salaries)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 24  4\n```\n:::\n:::\n\n \n\nfor the second part: 24 rows and 4 columns again.\nI note, with only 24 observations, that we don't really have enough\ndata to investigate the effects of three explanatory variables, but\nwe'll do the best we can. If the pattern, whatever it is, is clear\nenough, we should be OK.\n \n$\\blacksquare$\n\n(b) Make scatterplots of `salary` against each of the three explanatory variables. If you can, do this with *one* `ggplot`.\n\nSolution\n\n\nThe obvious way to do this is as three separate scatterplots,\nand that will definitely work. But you can do it in one go if\nyou think about facets, and about having all the $x$-values in\none column (and the names of the $x$-variables in another\ncolumn):\n\n::: {.cell}\n\n```{.r .cell-code}\nsalaries %>%\n  pivot_longer(-salary, names_to=\"xname\", values_to=\"x\") %>%\n  ggplot(aes(x = x, y = salary)) + geom_point() +\n  facet_wrap(~xname, ncol = 2, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/ivybridge-1.png){width=672}\n:::\n:::\n\n       \n\nIf you don't see how that works, run it yourself, one line at a time. \n\nI was thinking ahead a bit while I was coding that: I wanted the three\nplots to come out about square, and I wanted the plots to have their\nown scales. The last thing in the `facet_wrap` does the latter,\nand arranging the plots in two columns (thinking of the plots as a set\nof four with one missing) gets them more or less square.\n\nIf you don't think of those, try it without, and then fix up what you\ndon't like.\n \n$\\blacksquare$\n\n(c) Comment briefly on the direction and strength of each\nrelationship with `salary`.\n\nSolution\n\n\nTo my mind, all of the three relationships are going uphill (that's the \"direction\" part). `experience` is the\nstrongest, and `pubsucc` looks the weakest (that's the\n\"strength\" part). If you want to say there is no relationship\nwith `pubsucc`, that's fine too. This is a judgement\ncall. \nNote that all the relationships are more or less linear (no\nobvious curves here). We could also investigate the relationships\namong the explanatory variables:\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(salaries)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              salary  workqual experience   pubsucc\nsalary     1.0000000 0.6670958  0.8585582 0.5581960\nworkqual   0.6670958 1.0000000  0.4669511 0.3227612\nexperience 0.8585582 0.4669511  1.0000000 0.2537530\npubsucc    0.5581960 0.3227612  0.2537530 1.0000000\n```\n:::\n:::\n\n       \nMentally cut off the first row and column (`salary` is the\nresponse). None of the remaining correlations are all that high, so we\nought not to have any multicollinearity problems.\n \n$\\blacksquare$\n\n(d) <a name=\"regone\">*</a> Fit a regression predicting salary from the other three\nvariables, and obtain a `summary` of the results.\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsalaries.1 <- lm(salary ~ workqual + experience + pubsucc, data = salaries)\nsummary(salaries.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ workqual + experience + pubsucc, data = salaries)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2463 -0.9593  0.0377  1.1995  3.3089 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17.84693    2.00188   8.915 2.10e-08 ***\nworkqual     1.10313    0.32957   3.347 0.003209 ** \nexperience   0.32152    0.03711   8.664 3.33e-08 ***\npubsucc      1.28894    0.29848   4.318 0.000334 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.753 on 20 degrees of freedom\nMultiple R-squared:  0.9109,\tAdjusted R-squared:  0.8975 \nF-statistic: 68.12 on 3 and 20 DF,  p-value: 1.124e-10\n```\n:::\n:::\n\n   \n \n$\\blacksquare$\n\n(e) How can we justify the statement \n\"one or more of the explanatory variables helps to predict salary\"? How is this\nconsistent with the value of R-squared?\n\nSolution\n\n\n\"One or more of the explanatory variables helps\" is an\ninvitation to consider the (global) $F$-test for the whole\nregression. This has the very small P-value of $1.124\\times\n10^{-10}$ (from the bottom line of the output): very small, so\none or more of the explanatory variables *does* help, and\nthe statement is correct.\nThe idea that something helps to predict salary suggests\n(especially with such a small number of observations) that we\nshould have a high R-squared. In this case, R-squared is 0.9109,\nwhich is indeed high.\n \n$\\blacksquare$\n\n(f) Would you consider removing any of the variables from this\nregression? Why, or why not?\n\nSolution\n\n\nLook at the P-values attached to each variable. These are all\nvery small: 0.003, 0.00000003 and 0.0003, way smaller than\n0.05. So it would be a mistake to \ntake any, even one, of the variables out: doing so would make the\nregression much worse.\nIf you need convincing of that, see what happens when we take\nthe variable with the highest P-value out --- this is `workqual`:\n\n::: {.cell}\n\n```{.r .cell-code}\nsalaries.2 <- lm(salary ~ experience + pubsucc, data = salaries)\nsummary(salaries.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = salary ~ experience + pubsucc, data = salaries)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2723 -0.7865 -0.3983  1.7277  3.2060 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21.02546    2.14819   9.788 2.82e-09 ***\nexperience   0.37376    0.04104   9.107 9.70e-09 ***\npubsucc      1.52753    0.35331   4.324    3e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.137 on 21 degrees of freedom\nMultiple R-squared:  0.8609,\tAdjusted R-squared:  0.8477 \nF-statistic:    65 on 2 and 21 DF,  p-value: 1.01e-09\n```\n:::\n:::\n\n       \n\nR-squared has gone down from 91\\% to 86\\%: maybe not so much in the\ngrand scheme of things, but it is noticeably less. Perhaps better,\nsince we are comparing models with different numbers of explanatory\nvariables, is to compare the *adjusted* R-squared: this has gone\ndown from 90\\% to 85\\%. The fact that this has gone down *at all*\nis enough to say that taking out `workqual` was a\nmistake.^[Adjusted R-squareds are easier to compare in this  context, since you don't have to make a judgement about whether it has changed substantially, whatever you think substantially means.]\n\nAnother way of seeing whether a variable has anything to add in a\nregression containing the others is a **partial regression  plot**. \nWe take the residuals from `salaries.2` above and plot\nthem against the variable we removed, namely\n`workqual`.^[The residuals have to be the ones from a  regression *not* including the $x$-variable you're testing.] If\n`workqual` has nothing to add, there will be no pattern; if it\n*does* have something to add, there will be a trend. Like\nthis. I use `augment` from `broom`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nsalaries.2 %>%\n  augment(salaries) %>%\n  ggplot(aes(x = workqual, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/dartington-1.png){width=672}\n:::\n:::\n\n \n\nThis is a mostly straight upward trend. So we\nneed to add a linear term in `workqual` to the\nregression.^[Or not take it out in the first place.]\n \n$\\blacksquare$\n\n(g) Do you think it would be a mistake to take *both* of\n`workqual` and `pubsucc` out of the regression? Do a\nsuitable test. Was your guess right?\n\nSolution\n\n\nI think it would be a big mistake. Taking even one of these\nvariables out of the regression is a bad idea (from the\n$t$-tests), so taking out more than one would be a *really* bad idea.\nTo perform a test, fit the model without these two explanatory variables:\n\n::: {.cell}\n\n```{.r .cell-code}\nsalaries.3 <- lm(salary ~ experience, data = salaries)\n```\n:::\n\n     \n\nand then use `anova` to compare the two regressions, smaller\nmodel first:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(salaries.3, salaries.1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Res.Df\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"RSS\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Df\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sum of Sq\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Pr(>F)\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"22\",\"2\":\"181.1912\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"_rn_\":\"1\"},{\"1\":\"20\",\"2\":\"61.4430\",\"3\":\"2\",\"4\":\"119.7482\",\"5\":\"19.48931\",\"6\":\"2.010732e-05\",\"_rn_\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nThe P-value is extremely small, so the bigger model is definitely\nbetter than the smaller one: we really do need all three\nvariables. Which is what we guessed.\n\n$\\blacksquare$\n\n(h) Back in part (<a href=\"#regone\">here</a>), you fitted a regression with all\nthree explanatory variables. By making suitable plots, assess\nwhether there is any evidence that (i) that the linear model should\nbe a curve, (ii) that the residuals are not normally \ndistributed, (iii) that there is \"fan-out\", where the residuals are getting\nbigger *in size* as the fitted values get bigger? Explain\nbriefly how you came to your conclusions in each case.\n\nSolution\n\n\nI intended that (i) should just be a matter of looking at residuals\nvs.\\ fitted values:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(salaries.1, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/mathsal-8-1.png){width=672}\n:::\n:::\n\n    \n\nThere is no appreciable pattern on here, so no evidence of a curve (or\napparently of any other problems).\n\nExtra: you might read this that we should check residuals against the\n$x$-variables as well, which is a similar trick to the above one for\nplotting response against each of the explanatories. There is one step\nfirst, though: use `augment` from `broom` first to get a\ndataframe with the original $x$-variables *and* the residuals in\nit. The following thus looks rather complicated, and if it confuses\nyou, run the code a piece at a time to see what it's doing:\n\n::: {.cell}\n\n```{.r .cell-code}\nsalaries.1 %>%\n  augment(salaries) %>%\n  pivot_longer(workqual:pubsucc, names_to=\"xname\", values_to=\"x\") %>%\n  ggplot(aes(x = x, y = .resid)) + geom_point() +\n  facet_wrap(~xname, scales = \"free\", ncol = 2)\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/mathsal-9-1.png){width=672}\n:::\n:::\n\n \n\nThese three residual plots are also pretty much textbook random, so no problems here either.\n\nFor (ii), look at a normal quantile plot of the residuals, which is not as difficult as the plot I just did:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(salaries.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/mathsal-10-1.png){width=672}\n:::\n:::\n\n \n\nThat is really pretty good. Maybe the *second* smallest point is\na bit far off the line, but otherwise there's nothing to worry about. A\nquick place to look for problems is the extreme observations, and the\nlargest and smallest residuals are almost exactly the size we'd expect\nthem to be.\n\nOur graph for assessing fan-in or fan-out is to plot the *absolute* values of the residuals against the fitted values. The plot from (i) suggests that we won't have any problems here, but to investigate:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(salaries.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/mathsal-11-1.png){width=672}\n:::\n:::\n\n \n\nThis is pretty nearly straight across. You might think it increases a\nbit at the beginning, but most of the evidence for that comes from the\none observation with fitted value near 30 that happens to have a\nresidual near zero. Conclusions based on one observation are not to be\ntrusted!\nIn summary, I'm happy with this linear multiple regression, and I\ndon't see any need to do anything more with it. I am, however, willing\nto have some sympathy with opinions that differ from mine, if they are\nsupported by those graphs above.\n \n\n$\\blacksquare$\n\n\n\n\n\n##  Predicting GPA of computer science students\n\n\n The file\n[link](http://ritsokiguess.site/datafiles/gpa.txt) contains some\nmeasurements of academic achievement for a number of university\nstudents studying computer science:\n\n\n\n* High school grade point average\n\n* Math SAT score\n\n* Verbal SAT score\n\n* Computer Science grade point average\n\n* Overall university grade point average.\n\n\n\n\n(a) Read in the data and display it (or at least the first ten lines).\n\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/gpa.txt\"\ngpa <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 105 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (5): high_GPA, math_SAT, verb_SAT, comp_GPA, univ_GPA\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ngpa\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"high_GPA\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"math_SAT\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"verb_SAT\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"comp_GPA\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"univ_GPA\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3.45\",\"2\":\"643\",\"3\":\"589\",\"4\":\"3.76\",\"5\":\"3.52\"},{\"1\":\"2.78\",\"2\":\"558\",\"3\":\"512\",\"4\":\"2.87\",\"5\":\"2.91\"},{\"1\":\"2.52\",\"2\":\"583\",\"3\":\"503\",\"4\":\"2.54\",\"5\":\"2.40\"},{\"1\":\"3.67\",\"2\":\"685\",\"3\":\"602\",\"4\":\"3.83\",\"5\":\"3.47\"},{\"1\":\"3.24\",\"2\":\"592\",\"3\":\"538\",\"4\":\"3.29\",\"5\":\"3.47\"},{\"1\":\"2.10\",\"2\":\"562\",\"3\":\"486\",\"4\":\"2.64\",\"5\":\"2.37\"},{\"1\":\"2.82\",\"2\":\"573\",\"3\":\"548\",\"4\":\"2.86\",\"5\":\"2.40\"},{\"1\":\"2.36\",\"2\":\"559\",\"3\":\"536\",\"4\":\"2.03\",\"5\":\"2.24\"},{\"1\":\"2.42\",\"2\":\"552\",\"3\":\"583\",\"4\":\"2.81\",\"5\":\"3.02\"},{\"1\":\"3.51\",\"2\":\"617\",\"3\":\"591\",\"4\":\"3.41\",\"5\":\"3.32\"},{\"1\":\"3.48\",\"2\":\"684\",\"3\":\"649\",\"4\":\"3.61\",\"5\":\"3.59\"},{\"1\":\"2.14\",\"2\":\"568\",\"3\":\"592\",\"4\":\"2.48\",\"5\":\"2.54\"},{\"1\":\"2.59\",\"2\":\"604\",\"3\":\"582\",\"4\":\"3.21\",\"5\":\"3.19\"},{\"1\":\"3.46\",\"2\":\"619\",\"3\":\"624\",\"4\":\"3.52\",\"5\":\"3.71\"},{\"1\":\"3.51\",\"2\":\"642\",\"3\":\"619\",\"4\":\"3.41\",\"5\":\"3.58\"},{\"1\":\"3.68\",\"2\":\"683\",\"3\":\"642\",\"4\":\"3.52\",\"5\":\"3.40\"},{\"1\":\"3.91\",\"2\":\"703\",\"3\":\"684\",\"4\":\"3.84\",\"5\":\"3.73\"},{\"1\":\"3.72\",\"2\":\"712\",\"3\":\"652\",\"4\":\"3.64\",\"5\":\"3.49\"},{\"1\":\"2.15\",\"2\":\"564\",\"3\":\"501\",\"4\":\"2.14\",\"5\":\"2.25\"},{\"1\":\"2.48\",\"2\":\"557\",\"3\":\"549\",\"4\":\"2.21\",\"5\":\"2.37\"},{\"1\":\"3.09\",\"2\":\"591\",\"3\":\"584\",\"4\":\"3.17\",\"5\":\"3.29\"},{\"1\":\"2.71\",\"2\":\"599\",\"3\":\"562\",\"4\":\"3.01\",\"5\":\"3.19\"},{\"1\":\"2.46\",\"2\":\"607\",\"3\":\"619\",\"4\":\"3.17\",\"5\":\"3.28\"},{\"1\":\"3.32\",\"2\":\"619\",\"3\":\"558\",\"4\":\"3.01\",\"5\":\"3.37\"},{\"1\":\"3.61\",\"2\":\"700\",\"3\":\"721\",\"4\":\"3.72\",\"5\":\"3.61\"},{\"1\":\"3.82\",\"2\":\"718\",\"3\":\"732\",\"4\":\"3.78\",\"5\":\"3.81\"},{\"1\":\"2.64\",\"2\":\"580\",\"3\":\"538\",\"4\":\"2.51\",\"5\":\"2.40\"},{\"1\":\"2.19\",\"2\":\"562\",\"3\":\"507\",\"4\":\"2.10\",\"5\":\"2.21\"},{\"1\":\"3.34\",\"2\":\"683\",\"3\":\"648\",\"4\":\"3.21\",\"5\":\"3.58\"},{\"1\":\"3.48\",\"2\":\"717\",\"3\":\"724\",\"4\":\"3.68\",\"5\":\"3.51\"},{\"1\":\"3.56\",\"2\":\"701\",\"3\":\"714\",\"4\":\"3.48\",\"5\":\"3.62\"},{\"1\":\"3.81\",\"2\":\"691\",\"3\":\"684\",\"4\":\"3.71\",\"5\":\"3.60\"},{\"1\":\"3.92\",\"2\":\"714\",\"3\":\"706\",\"4\":\"3.81\",\"5\":\"3.65\"},{\"1\":\"4.00\",\"2\":\"689\",\"3\":\"673\",\"4\":\"3.84\",\"5\":\"3.76\"},{\"1\":\"2.52\",\"2\":\"554\",\"3\":\"507\",\"4\":\"2.09\",\"5\":\"2.27\"},{\"1\":\"2.71\",\"2\":\"564\",\"3\":\"543\",\"4\":\"2.17\",\"5\":\"2.35\"},{\"1\":\"3.15\",\"2\":\"668\",\"3\":\"604\",\"4\":\"2.98\",\"5\":\"3.17\"},{\"1\":\"3.22\",\"2\":\"691\",\"3\":\"662\",\"4\":\"3.28\",\"5\":\"3.47\"},{\"1\":\"2.29\",\"2\":\"573\",\"3\":\"591\",\"4\":\"2.74\",\"5\":\"3.00\"},{\"1\":\"2.03\",\"2\":\"568\",\"3\":\"517\",\"4\":\"2.19\",\"5\":\"2.74\"},{\"1\":\"3.14\",\"2\":\"607\",\"3\":\"624\",\"4\":\"3.28\",\"5\":\"3.37\"},{\"1\":\"3.52\",\"2\":\"651\",\"3\":\"683\",\"4\":\"3.68\",\"5\":\"3.54\"},{\"1\":\"2.91\",\"2\":\"604\",\"3\":\"583\",\"4\":\"3.17\",\"5\":\"3.28\"},{\"1\":\"2.83\",\"2\":\"560\",\"3\":\"542\",\"4\":\"3.17\",\"5\":\"3.39\"},{\"1\":\"2.65\",\"2\":\"604\",\"3\":\"617\",\"4\":\"3.31\",\"5\":\"3.28\"},{\"1\":\"2.41\",\"2\":\"574\",\"3\":\"548\",\"4\":\"3.07\",\"5\":\"3.19\"},{\"1\":\"2.54\",\"2\":\"564\",\"3\":\"500\",\"4\":\"2.38\",\"5\":\"2.52\"},{\"1\":\"2.66\",\"2\":\"607\",\"3\":\"528\",\"4\":\"2.94\",\"5\":\"3.08\"},{\"1\":\"3.21\",\"2\":\"619\",\"3\":\"573\",\"4\":\"2.84\",\"5\":\"3.01\"},{\"1\":\"3.34\",\"2\":\"647\",\"3\":\"608\",\"4\":\"3.17\",\"5\":\"3.42\"},{\"1\":\"3.68\",\"2\":\"651\",\"3\":\"683\",\"4\":\"3.72\",\"5\":\"3.60\"},{\"1\":\"2.84\",\"2\":\"571\",\"3\":\"543\",\"4\":\"2.17\",\"5\":\"2.40\"},{\"1\":\"2.74\",\"2\":\"583\",\"3\":\"510\",\"4\":\"2.42\",\"5\":\"2.83\"},{\"1\":\"2.71\",\"2\":\"554\",\"3\":\"538\",\"4\":\"2.49\",\"5\":\"2.38\"},{\"1\":\"2.24\",\"2\":\"568\",\"3\":\"519\",\"4\":\"3.38\",\"5\":\"3.21\"},{\"1\":\"2.48\",\"2\":\"574\",\"3\":\"602\",\"4\":\"2.07\",\"5\":\"2.24\"},{\"1\":\"3.14\",\"2\":\"605\",\"3\":\"619\",\"4\":\"3.22\",\"5\":\"3.40\"},{\"1\":\"2.83\",\"2\":\"591\",\"3\":\"584\",\"4\":\"2.71\",\"5\":\"3.07\"},{\"1\":\"3.44\",\"2\":\"642\",\"3\":\"608\",\"4\":\"3.31\",\"5\":\"3.52\"},{\"1\":\"2.89\",\"2\":\"608\",\"3\":\"573\",\"4\":\"3.28\",\"5\":\"3.47\"},{\"1\":\"2.67\",\"2\":\"574\",\"3\":\"538\",\"4\":\"3.19\",\"5\":\"3.08\"},{\"1\":\"3.24\",\"2\":\"643\",\"3\":\"607\",\"4\":\"3.24\",\"5\":\"3.38\"},{\"1\":\"3.29\",\"2\":\"608\",\"3\":\"649\",\"4\":\"3.53\",\"5\":\"3.41\"},{\"1\":\"3.87\",\"2\":\"709\",\"3\":\"688\",\"4\":\"3.72\",\"5\":\"3.64\"},{\"1\":\"3.94\",\"2\":\"691\",\"3\":\"645\",\"4\":\"3.98\",\"5\":\"3.71\"},{\"1\":\"3.42\",\"2\":\"667\",\"3\":\"583\",\"4\":\"3.09\",\"5\":\"3.01\"},{\"1\":\"3.52\",\"2\":\"656\",\"3\":\"609\",\"4\":\"3.42\",\"5\":\"3.37\"},{\"1\":\"2.24\",\"2\":\"554\",\"3\":\"542\",\"4\":\"2.07\",\"5\":\"2.34\"},{\"1\":\"3.29\",\"2\":\"692\",\"3\":\"563\",\"4\":\"3.17\",\"5\":\"3.29\"},{\"1\":\"3.41\",\"2\":\"684\",\"3\":\"672\",\"4\":\"3.51\",\"5\":\"3.40\"},{\"1\":\"3.56\",\"2\":\"717\",\"3\":\"649\",\"4\":\"3.49\",\"5\":\"3.38\"},{\"1\":\"3.61\",\"2\":\"712\",\"3\":\"708\",\"4\":\"3.51\",\"5\":\"3.28\"},{\"1\":\"3.28\",\"2\":\"641\",\"3\":\"608\",\"4\":\"3.40\",\"5\":\"3.31\"},{\"1\":\"3.21\",\"2\":\"675\",\"3\":\"632\",\"4\":\"3.38\",\"5\":\"3.42\"},{\"1\":\"3.48\",\"2\":\"692\",\"3\":\"698\",\"4\":\"3.54\",\"5\":\"3.39\"},{\"1\":\"3.62\",\"2\":\"684\",\"3\":\"609\",\"4\":\"3.48\",\"5\":\"3.51\"},{\"1\":\"2.92\",\"2\":\"564\",\"3\":\"591\",\"4\":\"3.09\",\"5\":\"3.17\"},{\"1\":\"2.81\",\"2\":\"554\",\"3\":\"509\",\"4\":\"3.14\",\"5\":\"3.20\"},{\"1\":\"3.11\",\"2\":\"685\",\"3\":\"694\",\"4\":\"3.28\",\"5\":\"3.41\"},{\"1\":\"3.28\",\"2\":\"671\",\"3\":\"609\",\"4\":\"3.41\",\"5\":\"3.29\"},{\"1\":\"2.70\",\"2\":\"571\",\"3\":\"503\",\"4\":\"3.02\",\"5\":\"3.17\"},{\"1\":\"2.62\",\"2\":\"582\",\"3\":\"591\",\"4\":\"2.97\",\"5\":\"3.12\"},{\"1\":\"3.72\",\"2\":\"621\",\"3\":\"589\",\"4\":\"4.00\",\"5\":\"3.71\"},{\"1\":\"3.42\",\"2\":\"651\",\"3\":\"642\",\"4\":\"3.34\",\"5\":\"3.50\"},{\"1\":\"3.51\",\"2\":\"673\",\"3\":\"681\",\"4\":\"3.28\",\"5\":\"3.34\"},{\"1\":\"3.28\",\"2\":\"651\",\"3\":\"640\",\"4\":\"3.32\",\"5\":\"3.48\"},{\"1\":\"3.42\",\"2\":\"672\",\"3\":\"607\",\"4\":\"3.51\",\"5\":\"3.44\"},{\"1\":\"3.90\",\"2\":\"591\",\"3\":\"587\",\"4\":\"3.68\",\"5\":\"3.59\"},{\"1\":\"3.12\",\"2\":\"582\",\"3\":\"612\",\"4\":\"3.07\",\"5\":\"3.28\"},{\"1\":\"2.83\",\"2\":\"609\",\"3\":\"555\",\"4\":\"2.78\",\"5\":\"3.00\"},{\"1\":\"2.09\",\"2\":\"554\",\"3\":\"480\",\"4\":\"3.68\",\"5\":\"3.42\"},{\"1\":\"3.17\",\"2\":\"612\",\"3\":\"590\",\"4\":\"3.30\",\"5\":\"3.41\"},{\"1\":\"3.28\",\"2\":\"628\",\"3\":\"580\",\"4\":\"3.34\",\"5\":\"3.49\"},{\"1\":\"3.02\",\"2\":\"567\",\"3\":\"602\",\"4\":\"3.17\",\"5\":\"3.28\"},{\"1\":\"3.42\",\"2\":\"619\",\"3\":\"623\",\"4\":\"3.07\",\"5\":\"3.17\"},{\"1\":\"3.06\",\"2\":\"691\",\"3\":\"683\",\"4\":\"3.19\",\"5\":\"3.24\"},{\"1\":\"2.76\",\"2\":\"564\",\"3\":\"549\",\"4\":\"2.15\",\"5\":\"2.34\"},{\"1\":\"3.19\",\"2\":\"650\",\"3\":\"684\",\"4\":\"3.11\",\"5\":\"3.28\"},{\"1\":\"2.23\",\"2\":\"551\",\"3\":\"554\",\"4\":\"2.17\",\"5\":\"2.29\"},{\"1\":\"2.48\",\"2\":\"568\",\"3\":\"541\",\"4\":\"2.14\",\"5\":\"2.08\"},{\"1\":\"3.76\",\"2\":\"605\",\"3\":\"590\",\"4\":\"3.74\",\"5\":\"3.64\"},{\"1\":\"3.49\",\"2\":\"692\",\"3\":\"683\",\"4\":\"3.27\",\"5\":\"3.42\"},{\"1\":\"3.07\",\"2\":\"680\",\"3\":\"692\",\"4\":\"3.19\",\"5\":\"3.25\"},{\"1\":\"2.19\",\"2\":\"617\",\"3\":\"503\",\"4\":\"2.98\",\"5\":\"2.76\"},{\"1\":\"3.46\",\"2\":\"516\",\"3\":\"528\",\"4\":\"3.28\",\"5\":\"3.41\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nTwo SAT scores and three GPAs, as promised.\n\n$\\blacksquare$\n\n(b) <a name=\"part:hsu-scatter\">*</a> Make a scatterplot of high school GPA against university\nGPA. Which variable should be the response and which\nexplanatory? Explain briefly. Add a smooth trend to your plot.\n\n\nSolution\n\n\nHigh school comes before university, so high school should be\nexplanatory and university should be the response. (High school\ngrades are used as an admission criterion to university, so we\nwould hope they would have some predictive value.)\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(gpa, aes(x = high_GPA, y = univ_GPA)) + geom_point() +\n  geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/gpa-2-1.png){width=672}\n:::\n:::\n\n   \n    \n$\\blacksquare$\n\n(c) Describe any relationship on your scatterplot: its direction, its\nstrength and its shape. Justify your description briefly.\n\n\nSolution\n\n\nTaking these points one at a time:\n\n\n* direction: upward (a higher high-school GPA generally goes\nwith a higher university GPA as well. Or you can say that the\nlowest high-school GPAs go with the lowest university GPAs,\nand high with high, at least most of the time).\n\n* strength: something like moderately strong, since while\nthe trend is upward, there is quite a lot of scatter. (This is\na judgement call: something that indicates that you are basing\nyour description on something reasonable is fine.)\n\n* shape: I'd call this \"approximately linear\" since there\nis no clear curve here. The smooth trend wiggles a bit, but\nnot enough to make me doubt a straight line.\n\nLooking ahead, I also notice that when high-school GPA is high,\nuniversity GPA is also consistently high, but when high-school\nGPA is low, the university GPA is sometimes low and sometimes\nhigh, a lot more variable. (This suggests problems with fan-in\nlater.) In a practical sense, what this seems to show is that\npeople who do well at university usually did well in high-school\nas well, but sometimes their high-school grades were not\nthat good. This is especially true for people with university\nGPAs around 3.25.\n      \n$\\blacksquare$\n\n(d) <a name=\"part:highonly\">*</a> Fit a linear regression for predicting university GPA\nfrom high-school GPA and display the results.\n\n\nSolution\n\n\nJust this, therefore:\n\n::: {.cell}\n\n```{.r .cell-code}\ngpa.1 <- lm(univ_GPA ~ high_GPA, data = gpa)\nsummary(gpa.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = univ_GPA ~ high_GPA, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.69040 -0.11922  0.03274  0.17397  0.91278 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.09682    0.16663   6.583 1.98e-09 ***\nhigh_GPA     0.67483    0.05342  12.632  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2814 on 103 degrees of freedom\nMultiple R-squared:  0.6077,\tAdjusted R-squared:  0.6039 \nF-statistic: 159.6 on 1 and 103 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n    \n\nExtra: this question goes on too long, so I didn't ask you to look at the\nresiduals from this model, but my comments earlier suggested that we\nmight have had some problems with fanning-in (the variability of\npredictions getting less as the high-school GPA increases). In case\nyou are interested, I'll look at this here. First, residuals against\nfitted values:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(gpa.1, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/gpa-4-1.png){width=672}\n:::\n:::\n\n \n\nIs that evidence of a trend in the residuals? Dunno. I'm inclined to\ncall this an \"inconsequential wiggle\" and say it's OK. Note that the\ngrey envelope includes zero all the way across.\n\nNormal quantile plot of residuals:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(gpa.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/gpa-5-1.png){width=672}\n:::\n:::\n\n \n\nA somewhat long-tailed distribution: compared to a normal\ndistribution, the residuals are a bit too big in size, both on the\npositive and negative end.\n\nThe problem I was really worried about was the potential of\nfanning-in, which we can assess by looking at the absolute residuals:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(gpa.1, aes(x = .fitted, y = abs(.resid))) + geom_point() + geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/gpa-6-1.png){width=672}\n:::\n:::\n\n \n\nThat is definitely a downward trend in the size of the residuals, and\nI think I was right to be worried before. The residuals should be of\nsimilar size all the way across.\n\nThe usual problem of this kind is fanning-*out*, where the\nresiduals get *bigger* in size as the fitted values increase. The\nbigger values equals more spread is the kind of thing that a\ntransformation like taking logs will handle: the bigger values are all\nbrought downwards, so they will be both smaller and less variable than\nthey were. This one, though, goes the other way, so the only kind of\ntransformation that might help is one at the other end of the scale\n(think of the Box-Cox lambda scale), something like maybe reciprocal,\n$\\lambda=-1$ maybe.\n\nThe other thought I had was that there is this kind of break around a\nhigh-school GPA of 3 (go back to the scatterplot of (<a href=\"#part:hsu-scatter\">here</a>)): when the\nhigh-school GPA is higher than 3, the university GPA is very\nconsistent (and shows a clear upward trend), but when the high-school\nGPA is less than 3, the university GPA is very variable and there\ndoesn't seem to be any trend at all. So maybe two separate analyses\nwould be the way to go.\n\nAll right, how does Box-Cox work out here, if at all?\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nboxcox(univ_GPA ~ high_GPA, data = gpa)\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/gpa-7-1.png){width=672}\n:::\n:::\n\n \n\nIt doesn't. All right, that answers *that* question. \n\n\n \n        \n$\\blacksquare$\n\n(e) Two students have been admitted to university. One has\na high school GPA of 3.0 and the other a high school GPA of   \n3.5. Obtain suitable intervals that summarize the GPAs that each of these\ntwo students might obtain in university.\n\n\nSolution\n\n\nSince we are talking about individual students, rather than\nthe  mean of *all* students with these GPAs, prediction\nintervals are called for. The first step is to make a data\nframe to predict from. This has to contain columns for all the\nexplanatory variables, just `high_GPA` in this case:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = gpa.1, high_GPA = c(3,3.5))\nnew\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"univ_GPA\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"high_GPA\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3.172857\",\"2\":\"3.0\"},{\"1\":\"3.172857\",\"2\":\"3.5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nIn general, the advantage to doing it this way is that you also get representative values for any other explanatory variables (means for quantitative ones, the most common category for categorical ones). But in this case, the dataframe has just one column with two values in it, so any other way to create this dataframe is equally good, and, you might say, easier, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ngpa.new <- tibble(high_GPA = c(3, 3.5))\ngpa.new\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"high_GPA\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3.0\"},{\"1\":\"3.5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nThe next thing to consider is whether you want a confidence interval for the mean response (the kind of thing `predictions` gives you), or a prediction interval for an individual response. In this case, it's the prediction interval, because we want to infer how these individual students will fare. To get this, you need the old-fashioned `predict` rather than `predictions`:\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- predict(gpa.1, gpa.new, interval = \"p\")\npreds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 3.121313 2.560424 3.682202\n2 3.458728 2.896105 4.021351\n```\n:::\n:::\n\n \n\nand display that side by side with the values it was predicted from:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(gpa.new, preds)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"high_GPA\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fit\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lwr\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3.0\",\"2\":\"3.121313\",\"3\":\"2.560424\",\"4\":\"3.682202\",\"_rn_\":\"1\"},{\"1\":\"3.5\",\"2\":\"3.458728\",\"3\":\"2.896105\",\"4\":\"4.021351\",\"_rn_\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nor this way, if you like it better:\n\n::: {.cell}\n\n```{.r .cell-code}\nas_tibble(preds) %>% bind_cols(gpa.new) %>% select(high_GPA, everything())\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"high_GPA\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fit\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lwr\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"upr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3.0\",\"2\":\"3.121313\",\"3\":\"2.560424\",\"4\":\"3.682202\"},{\"1\":\"3.5\",\"2\":\"3.458728\",\"3\":\"2.896105\",\"4\":\"4.021351\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nThus the predicted university GPA for the student with high school GPA\n3.0 is between 2.6 and 3.7, and for the student with high school GPA\n3.5 is between 2.9 and 4.0. (I think this is a good number of decimals\nto give, but in any case, you should actually *say* what the\nintervals are.)\n\nExtra: I observe that these intervals are almost exactly the same\nlength. This surprises me a bit, since I would have said that 3.0 is\nclose to the average high-school GPA and 3.5 is noticeably higher. If\nthat's the case, the prediction interval for 3.5 should be longer\n(since there is less \"nearby data\"). Was I right about that?\n\n::: {.cell}\n\n```{.r .cell-code}\ngpa %>% summarize(\n  mean = mean(high_GPA),\n  med = median(high_GPA),\n  q1 = quantile(high_GPA, 0.25),\n  q3 = quantile(high_GPA, 0.75)\n)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mean\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"med\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"q1\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"q3\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3.076381\",\"2\":\"3.17\",\"3\":\"2.67\",\"4\":\"3.48\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nMore or less: the mean is close to 3, and 3.5 is close to the third\nquartile. So the thing about the length of the prediction interval is\na bit of a mystery. Maybe it works better for the confidence interval\nfor the mean:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(gpa.1, newdata = new))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"rowid\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.low\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"conf.high\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"univ_GPA\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"high_GPA\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"3.121313\",\"3\":\"0.02776760\",\"4\":\"112.40845\",\"5\":\"0\",\"6\":\"3.066889\",\"7\":\"3.175736\",\"8\":\"3.172857\",\"9\":\"3.0\"},{\"1\":\"2\",\"2\":\"3.458728\",\"3\":\"0.03558848\",\"4\":\"97.18673\",\"5\":\"0\",\"6\":\"3.388976\",\"7\":\"3.528480\",\"8\":\"3.172857\",\"9\":\"3.5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n\nThese intervals are a lot shorter, since we are talking about\n*all* students with the high-school GPAs in question, and we\ntherefore no longer have to worry about variation from student to\nstudent (which is considerable). But my supposition about length is\nnow correct: the interval for 3.5, which is further from the mean, is\na little longer than the interval for 3.0. Thinking about it, it seems\nthat the individual-to-individual variation, which is large, is\ndominating things for our prediction interval above.\n        \n$\\blacksquare$\n\n(f) <a name=\"part:all\">*</a> Now obtain a regression predicting university GPA from\nhigh-school GPA as well as the two SAT scores. Display your results.\n\n\nSolution\n\n\nCreate a new regression with all the explanatory variables you want in it:\n\n::: {.cell}\n\n```{.r .cell-code}\ngpa.2 <- lm(univ_GPA ~ high_GPA + math_SAT + verb_SAT, data = gpa)\nsummary(gpa.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + math_SAT + verb_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68186 -0.13189  0.01289  0.16186  0.93994 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.5793478  0.3422627   1.693   0.0936 .  \nhigh_GPA    0.5454213  0.0850265   6.415  4.6e-09 ***\nmath_SAT    0.0004893  0.0010215   0.479   0.6330    \nverb_SAT    0.0010202  0.0008123   1.256   0.2120    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2784 on 101 degrees of freedom\nMultiple R-squared:  0.6236,\tAdjusted R-squared:  0.6124 \nF-statistic: 55.77 on 3 and 101 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n \n        \n\n$\\blacksquare$\n\n(g) Test whether adding the two SAT scores has improved the\nprediction of university GPA. What do you conclude?\n\n\n\nSolution\n\n\nSince we added *two* explanatory variables, the $t$-tests in\n`gpa.2` don't apply (they tell us whether we can take out\n*one* $x$-variable). We might have some suspicions, but that's\nall they are.  So we have to do `anova`:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(gpa.1, gpa.2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Res.Df\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"RSS\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Df\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Sum of Sq\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Pr(>F)\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"103\",\"2\":\"8.158723\",\"3\":\"NA\",\"4\":\"NA\",\"5\":\"NA\",\"6\":\"NA\",\"_rn_\":\"1\"},{\"1\":\"101\",\"2\":\"7.828840\",\"3\":\"2\",\"4\":\"0.329883\",\"5\":\"2.127913\",\"6\":\"0.1243939\",\"_rn_\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n   \n\nIf you put the models the other way around, you'll get a negative\n$F$-statistic and degrees of freedom, which doesn't make\nmuch sense (although the test will still work).\n\nThe null hypothesis here is that the two models fit equally\nwell. Since the P-value is not small, we do not reject that null\nhypothesis, and therefore we conclude that the two models *do*\nfit equally well, and therefore we prefer the smaller one, the one\nthat predicts university GPA from just high-school GPA. (Or,\nequivalently, we conclude that those two SAT scores don't add anything\nto the prediction of how well a student will do at university, once\nyou know their high-school GPA.)\n\nThis might surprise you, given what the SATs are supposed to be\n*for*. But that's what the data say.\n  \n$\\blacksquare$\n\n(h) Carry out a backward elimination starting out from your\nmodel in part (<a href=\"#part:all\">here</a>). Which model do you end up with?\nIs it the same model as you fit in (<a href=\"#part:highonly\">here</a>)?\n\n\nSolution\n\n\nIn the model of (<a href=\"#part:all\">here</a>), `math_SAT` was the\nleast significant, so that comes out first. (I use\n`update` but I'm not insisting that you do:)\n\n::: {.cell}\n\n```{.r .cell-code}\ngpa.3 <- update(gpa.2, . ~ . - math_SAT)\nsummary(gpa.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + verb_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68430 -0.11268  0.01802  0.14901  0.95239 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.6838723  0.2626724   2.604   0.0106 *  \nhigh_GPA    0.5628331  0.0765729   7.350 5.07e-11 ***\nverb_SAT    0.0012654  0.0006283   2.014   0.0466 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2774 on 102 degrees of freedom\nMultiple R-squared:  0.6227,\tAdjusted R-squared:  0.6153 \nF-statistic: 84.18 on 2 and 102 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n \nHere is where we have to stop, since both high-school GPA and\nverbal SAT score are significant, and so taking either of them\nout would be a bad idea. \nThis is a *different* model than the one of\n(<a href=\"#part:highonly\">here</a>). This is the case, even though the model\nwith high-school GPA only was not significantly worse than the\nmodel containing everything. (This goes to show that\nmodel-building doesn't always have nice clear answers.)\n\nIn the model I called `gpa.2`, neither of the SAT\nscores were anywhere near significant (considered singly), but\nas soon as we took out one of the SAT scores (my model\n`gpa.3`), the other one became significant. This goes\nto show that you shouldn't take out more than one explanatory\nvariable based on the results of the $t$-tests, and even if\nyou test to see whether you should have taken out both of the SAT,\nyou won't necessarily get consistent\nresults. Admittedly, it's a close decision whether to\nkeep or remove `verb_SAT`, since its P-value is\nclose to 0.05.\n\nThe other way of tackling this one is via `step`, which\ndoes the backward elimination for you (not that it was much\nwork here):\n\n::: {.cell}\n\n```{.r .cell-code}\nstep(gpa.2, direction = \"backward\", test = \"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=-264.6\nuniv_GPA ~ high_GPA + math_SAT + verb_SAT\n\n           Df Sum of Sq     RSS     AIC F value    Pr(>F)    \n- math_SAT  1    0.0178  7.8466 -266.36  0.2294     0.633    \n- verb_SAT  1    0.1223  7.9511 -264.97  1.5777     0.212    \n<none>                   7.8288 -264.60                      \n- high_GPA  1    3.1896 11.0184 -230.71 41.1486 4.601e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStep:  AIC=-266.36\nuniv_GPA ~ high_GPA + verb_SAT\n\n           Df Sum of Sq     RSS     AIC F value    Pr(>F)    \n<none>                   7.8466 -266.36                      \n- verb_SAT  1    0.3121  8.1587 -264.26  4.0571   0.04662 *  \n- high_GPA  1    4.1562 12.0028 -223.73 54.0268 5.067e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + verb_SAT, data = gpa)\n\nCoefficients:\n(Intercept)     high_GPA     verb_SAT  \n   0.683872     0.562833     0.001265  \n```\n:::\n:::\n\n         \n\nThis gives the same result as we did from our backward\nelimination. The tables with AIC in them are each step of\nthe elimination, and the variable at the top is the one that comes out\nnext. (When `<none>` gets to the top, that's when you stop.)\nWhat happened is that the two SAT scores started off highest, but once\nwe removed `math_SAT`, `verb_SAT` jumped below\n`<none>` and so the verbal SAT score had to stay.\n\nBoth the P-value and the AIC say that the decision about keeping or\nremoving `verb_SAT` is very close.\n        \n$\\blacksquare$\n\n(i) These students were studying computer science at\nuniversity. Do you find your backward-elimination result\nsensible or surprising, given this? Explain briefly.\n\n\nSolution\n\n\nI would expect computer science students to be strong students\ngenerally, good at math and possibly not so good with\nwords. So it is not surprising that high-school GPA figures\ninto the prediction, but I would expect math SAT score to have\nan impact also, and it does not. It is also rather surprising\nthat verbal SAT score predicts success at university for these\ncomputer science students; you wouldn't think that having\nbetter skills with words would be helpful. So I'm surprised.\nHere, I'm looking for some kind of discussion about what's in\nthe final backward-elimination model, and what you would\nexpect to be true of computer science students. \n\nLet's look at the final model from the backward elimination again:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(gpa.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + verb_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68430 -0.11268  0.01802  0.14901  0.95239 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.6838723  0.2626724   2.604   0.0106 *  \nhigh_GPA    0.5628331  0.0765729   7.350 5.07e-11 ***\nverb_SAT    0.0012654  0.0006283   2.014   0.0466 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2774 on 102 degrees of freedom\nMultiple R-squared:  0.6227,\tAdjusted R-squared:  0.6153 \nF-statistic: 84.18 on 2 and 102 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n         \n\nThe two slope estimates are both positive, meaning that, all else\nequal, a higher value for each explanatory variable goes with a higher\nuniversity GPA. This indicates that a higher verbal SAT score goes\nwith a higher university GPA: this is across all the university\ncourses that a student takes, which you would expect to be math and\ncomputer science courses for a Comp Sci student, but might include\na few electives or writing courses. Maybe what is happening is that\nthe math SAT score is telling the same story as the high-school GPA\nfor these students, and the verbal SAT score is saying something\ndifferent. (For example, prospective computer science students are\nmostly likely to have high math SAT scores, so there's not much\ninformation there.)\n\nI think I need to look at the correlations:\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(gpa)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          high_GPA  math_SAT  verb_SAT  comp_GPA  univ_GPA\nhigh_GPA 1.0000000 0.7681423 0.7261478 0.7914721 0.7795631\nmath_SAT 0.7681423 1.0000000 0.8352272 0.6877209 0.6627837\nverb_SAT 0.7261478 0.8352272 1.0000000 0.6387512 0.6503012\ncomp_GPA 0.7914721 0.6877209 0.6387512 1.0000000 0.9390459\nuniv_GPA 0.7795631 0.6627837 0.6503012 0.9390459 1.0000000\n```\n:::\n:::\n\n \n\nWe'll ignore `comp_GPA` (i) because we haven't been thinking\nabout it and (ii) because it's highly correlated with the university\nGPA anyway. (There isn't a sense that one of the two university GPAs\nis explanatory and the other is a response, since students are taking\nthe courses that contribute to them at the same time.)\n\nThe highest correlation with university GPA of what remains is\nhigh-school GPA, so it's not at all surprising that this features in\nall our regressions. The correlations between university GPA and the\ntwo SAT scores are about equal, so there appears to be no reason to\nfavour one of the SAT scores over the other. But, the two SAT scores\nare highly correlated with *each other* (0.835), which suggests\nthat if you have one, you don't need the other, because they are\ntelling more or less the same story. \n\nThat makes me wonder how a regression with the SAT math score and\n*not* the SAT verbal score would look:\n\n::: {.cell}\n\n```{.r .cell-code}\ngpa.4 <- lm(univ_GPA ~ high_GPA + math_SAT, data = gpa)\nsummary(gpa.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = univ_GPA ~ high_GPA + math_SAT, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68079 -0.12264  0.00741  0.16579  0.90010 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.6072916  0.3425047   1.773   0.0792 .  \nhigh_GPA    0.5710745  0.0827705   6.899  4.5e-10 ***\nmath_SAT    0.0012980  0.0007954   1.632   0.1058    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2792 on 102 degrees of freedom\nMultiple R-squared:  0.6177,\tAdjusted R-squared:  0.6102 \nF-statistic:  82.4 on 2 and 102 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n \n\nMath SAT does not quite significantly add anything to the prediction,\nwhich confirms that we do better to use the verbal SAT score\n(surprising though it seems). Though, the two regressions with the\nsingle SAT scores, `gpa.3` and `gpa.4`, have almost the\nsame R-squared values. It's not clear-cut at all. In the end, you have\nto make a call and stand by it.\n        \n$\\blacksquare$\n\n\n\n\n\n\n## Fish and mercury\n\nEating fish is generally healthy, but sometimes the fish is contaminated with mercury. What might affect the amount of mercury in a fish? Large-mouth bass were studied in Florida lakes to examine factors that might affect the amount of mercury contamination. 38 lakes were used for the study. Water samples were taken from each lake and analyzed. Also, samples of fish were caught in each lake and the mercury concentration in their muscle tissue was measured. The resulting data are in [http://ritsokiguess.site/datafiles/mercury.txt](http://ritsokiguess.site/datafiles/mercury.txt), separated by single spaces. The variables measured were as follows:\n\n-   standardized mercury level (parts per million in 3-year-old fish)\n-   alkalinity of water (milligrams per litre)\n-   calcium level of water (milligrams per litre)\n-   pH of water (standard scale; see eg. [this](https://www.usgs.gov/special-topic/water-science-school/science/ph-and-water?qt-science_center_objects=0#qt-science_center_objects))\n\n\n\n(a) Read in and display (some of) the data.\n\nSolution\n\n\nThe data values were separated by single spaces, so this one is `read_delim`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/mercury.txt\"\nmercury <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 38 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (4): mercury, alkalinity, calcium, ph\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nmercury\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mercury\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"alkalinity\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"calcium\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ph\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1330\",\"2\":\"2.5\",\"3\":\"2.9\",\"4\":\"4.6\"},{\"1\":\"250\",\"2\":\"19.6\",\"3\":\"4.5\",\"4\":\"7.3\"},{\"1\":\"450\",\"2\":\"5.2\",\"3\":\"2.8\",\"4\":\"5.4\"},{\"1\":\"160\",\"2\":\"71.4\",\"3\":\"55.2\",\"4\":\"8.1\"},{\"1\":\"720\",\"2\":\"26.4\",\"3\":\"9.2\",\"4\":\"5.8\"},{\"1\":\"810\",\"2\":\"4.8\",\"3\":\"4.6\",\"4\":\"6.4\"},{\"1\":\"710\",\"2\":\"6.6\",\"3\":\"2.7\",\"4\":\"5.4\"},{\"1\":\"510\",\"2\":\"16.5\",\"3\":\"13.8\",\"4\":\"7.2\"},{\"1\":\"1000\",\"2\":\"7.1\",\"3\":\"5.2\",\"4\":\"5.8\"},{\"1\":\"150\",\"2\":\"83.7\",\"3\":\"66.5\",\"4\":\"8.2\"},{\"1\":\"190\",\"2\":\"108.5\",\"3\":\"35.6\",\"4\":\"8.7\"},{\"1\":\"1020\",\"2\":\"6.4\",\"3\":\"4.0\",\"4\":\"5.8\"},{\"1\":\"450\",\"2\":\"7.5\",\"3\":\"2.0\",\"4\":\"4.4\"},{\"1\":\"590\",\"2\":\"17.3\",\"3\":\"10.7\",\"4\":\"6.7\"},{\"1\":\"810\",\"2\":\"7.0\",\"3\":\"6.3\",\"4\":\"6.9\"},{\"1\":\"420\",\"2\":\"10.5\",\"3\":\"6.3\",\"4\":\"5.5\"},{\"1\":\"530\",\"2\":\"30.0\",\"3\":\"13.9\",\"4\":\"6.9\"},{\"1\":\"310\",\"2\":\"55.4\",\"3\":\"15.9\",\"4\":\"7.3\"},{\"1\":\"470\",\"2\":\"6.3\",\"3\":\"3.3\",\"4\":\"5.8\"},{\"1\":\"250\",\"2\":\"67.0\",\"3\":\"58.6\",\"4\":\"7.8\"},{\"1\":\"410\",\"2\":\"28.8\",\"3\":\"10.2\",\"4\":\"7.4\"},{\"1\":\"160\",\"2\":\"119.1\",\"3\":\"38.4\",\"4\":\"7.9\"},{\"1\":\"160\",\"2\":\"25.4\",\"3\":\"8.8\",\"4\":\"7.1\"},{\"1\":\"230\",\"2\":\"106.5\",\"3\":\"90.7\",\"4\":\"6.8\"},{\"1\":\"560\",\"2\":\"8.5\",\"3\":\"2.5\",\"4\":\"7.0\"},{\"1\":\"890\",\"2\":\"87.6\",\"3\":\"85.5\",\"4\":\"7.5\"},{\"1\":\"180\",\"2\":\"114.0\",\"3\":\"72.6\",\"4\":\"7.0\"},{\"1\":\"190\",\"2\":\"97.5\",\"3\":\"45.5\",\"4\":\"6.8\"},{\"1\":\"440\",\"2\":\"11.8\",\"3\":\"24.2\",\"4\":\"5.9\"},{\"1\":\"160\",\"2\":\"66.5\",\"3\":\"26.0\",\"4\":\"8.3\"},{\"1\":\"670\",\"2\":\"16.0\",\"3\":\"41.2\",\"4\":\"6.7\"},{\"1\":\"550\",\"2\":\"5.0\",\"3\":\"23.6\",\"4\":\"6.2\"},{\"1\":\"580\",\"2\":\"25.6\",\"3\":\"12.6\",\"4\":\"6.2\"},{\"1\":\"980\",\"2\":\"1.2\",\"3\":\"2.1\",\"4\":\"4.3\"},{\"1\":\"310\",\"2\":\"34.0\",\"3\":\"13.1\",\"4\":\"7.0\"},{\"1\":\"430\",\"2\":\"15.5\",\"3\":\"5.2\",\"4\":\"6.9\"},{\"1\":\"280\",\"2\":\"17.3\",\"3\":\"3.0\",\"4\":\"5.2\"},{\"1\":\"250\",\"2\":\"71.8\",\"3\":\"20.5\",\"4\":\"7.9\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nExtra: I found these data in a textbook, and I couldn't find them online anywhere, so I typed them in myself. This is how I did it.\n\nFirst, I entered the values as a piece of text:\n\n::: {.cell}\n\n```{.r .cell-code}\nmercury_txt <- \"\nmercury alkalinity calcium ph\n1330 2.5 2.9 4.6\n250 19.6 4.5 7.3\n450 5.2 2.8 5.4\n160 71.4 55.2 8.1\n720 26.4 9.2 5.8\n810 4.8 4.6 6.4\n710 6.6 2.7 5.4\n510 16.5 13.8 7.2\n1000 7.1 5.2 5.8\n150 83.7 66.5 8.2\n190 108.5 35.6 8.7\n1020 6.4 4.0 5.8\n450 7.5 2.0 4.4\n590 17.3 10.7 6.7\n810 7.0 6.3 6.9\n420 10.5 6.3 5.5\nO530 30.0 13.9 6.9\n310 55.4 15.9 7.3\n470 6.3 3.3 5.8\n250 67.0 58.6 7.8\n410 28.8 10.2 7.4\n160 119.1 38.4 7.9\n160 25.4 8.8 7.1\n230 106.5 90.7 6.8\n560 8.5 2.5 7.0\n890 87.6 85.5 7.5\n180 114.0 72.6 7.0\n190 97.5 45.5 6.8\n440 11.8 24.2 5.9\n160 66.5 26.0 8.3\n670 16.0 41.2 6.7\n550 5.0 23.6 6.2\n580 25.6 12.6 6.2\n980 1.2 2.1 4.3\n310 34.0 13.1 7.0\n430 15.5 5.2 6.9\n280 17.3 3.0 5.2\n250 71.8 20.5 7.9\n\"\n```\n:::\n\nThen, I wanted to get these into a file laid out just like that, which is what `writeLines` does:\n\n::: {.cell}\n\n```{.r .cell-code}\nwriteLines(mercury_txt, \"mercury.txt\")\n```\n:::\n\nand then I uploaded the file to the course website.\n\n\n$\\blacksquare$\n\n\n(b) Plot the mercury levels against each of the explanatory variables.\n\nSolution\n\n\nThe best way to do this is in one shot, using facets. This means organizing the dataframe so that there is one column of $y$-values, and also one column of $x$-values, with an additional column labelling which $x$ it is. This, as you'll recall, is exactly what `pivot_longer` does. To show you:\n\n::: {.cell}\n\n```{.r .cell-code}\nmercury %>% \npivot_longer(-mercury, names_to = \"xname\", values_to = \"xval\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mercury\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"xname\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"xval\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1330\",\"2\":\"alkalinity\",\"3\":\"2.5\"},{\"1\":\"1330\",\"2\":\"calcium\",\"3\":\"2.9\"},{\"1\":\"1330\",\"2\":\"ph\",\"3\":\"4.6\"},{\"1\":\"250\",\"2\":\"alkalinity\",\"3\":\"19.6\"},{\"1\":\"250\",\"2\":\"calcium\",\"3\":\"4.5\"},{\"1\":\"250\",\"2\":\"ph\",\"3\":\"7.3\"},{\"1\":\"450\",\"2\":\"alkalinity\",\"3\":\"5.2\"},{\"1\":\"450\",\"2\":\"calcium\",\"3\":\"2.8\"},{\"1\":\"450\",\"2\":\"ph\",\"3\":\"5.4\"},{\"1\":\"160\",\"2\":\"alkalinity\",\"3\":\"71.4\"},{\"1\":\"160\",\"2\":\"calcium\",\"3\":\"55.2\"},{\"1\":\"160\",\"2\":\"ph\",\"3\":\"8.1\"},{\"1\":\"720\",\"2\":\"alkalinity\",\"3\":\"26.4\"},{\"1\":\"720\",\"2\":\"calcium\",\"3\":\"9.2\"},{\"1\":\"720\",\"2\":\"ph\",\"3\":\"5.8\"},{\"1\":\"810\",\"2\":\"alkalinity\",\"3\":\"4.8\"},{\"1\":\"810\",\"2\":\"calcium\",\"3\":\"4.6\"},{\"1\":\"810\",\"2\":\"ph\",\"3\":\"6.4\"},{\"1\":\"710\",\"2\":\"alkalinity\",\"3\":\"6.6\"},{\"1\":\"710\",\"2\":\"calcium\",\"3\":\"2.7\"},{\"1\":\"710\",\"2\":\"ph\",\"3\":\"5.4\"},{\"1\":\"510\",\"2\":\"alkalinity\",\"3\":\"16.5\"},{\"1\":\"510\",\"2\":\"calcium\",\"3\":\"13.8\"},{\"1\":\"510\",\"2\":\"ph\",\"3\":\"7.2\"},{\"1\":\"1000\",\"2\":\"alkalinity\",\"3\":\"7.1\"},{\"1\":\"1000\",\"2\":\"calcium\",\"3\":\"5.2\"},{\"1\":\"1000\",\"2\":\"ph\",\"3\":\"5.8\"},{\"1\":\"150\",\"2\":\"alkalinity\",\"3\":\"83.7\"},{\"1\":\"150\",\"2\":\"calcium\",\"3\":\"66.5\"},{\"1\":\"150\",\"2\":\"ph\",\"3\":\"8.2\"},{\"1\":\"190\",\"2\":\"alkalinity\",\"3\":\"108.5\"},{\"1\":\"190\",\"2\":\"calcium\",\"3\":\"35.6\"},{\"1\":\"190\",\"2\":\"ph\",\"3\":\"8.7\"},{\"1\":\"1020\",\"2\":\"alkalinity\",\"3\":\"6.4\"},{\"1\":\"1020\",\"2\":\"calcium\",\"3\":\"4.0\"},{\"1\":\"1020\",\"2\":\"ph\",\"3\":\"5.8\"},{\"1\":\"450\",\"2\":\"alkalinity\",\"3\":\"7.5\"},{\"1\":\"450\",\"2\":\"calcium\",\"3\":\"2.0\"},{\"1\":\"450\",\"2\":\"ph\",\"3\":\"4.4\"},{\"1\":\"590\",\"2\":\"alkalinity\",\"3\":\"17.3\"},{\"1\":\"590\",\"2\":\"calcium\",\"3\":\"10.7\"},{\"1\":\"590\",\"2\":\"ph\",\"3\":\"6.7\"},{\"1\":\"810\",\"2\":\"alkalinity\",\"3\":\"7.0\"},{\"1\":\"810\",\"2\":\"calcium\",\"3\":\"6.3\"},{\"1\":\"810\",\"2\":\"ph\",\"3\":\"6.9\"},{\"1\":\"420\",\"2\":\"alkalinity\",\"3\":\"10.5\"},{\"1\":\"420\",\"2\":\"calcium\",\"3\":\"6.3\"},{\"1\":\"420\",\"2\":\"ph\",\"3\":\"5.5\"},{\"1\":\"530\",\"2\":\"alkalinity\",\"3\":\"30.0\"},{\"1\":\"530\",\"2\":\"calcium\",\"3\":\"13.9\"},{\"1\":\"530\",\"2\":\"ph\",\"3\":\"6.9\"},{\"1\":\"310\",\"2\":\"alkalinity\",\"3\":\"55.4\"},{\"1\":\"310\",\"2\":\"calcium\",\"3\":\"15.9\"},{\"1\":\"310\",\"2\":\"ph\",\"3\":\"7.3\"},{\"1\":\"470\",\"2\":\"alkalinity\",\"3\":\"6.3\"},{\"1\":\"470\",\"2\":\"calcium\",\"3\":\"3.3\"},{\"1\":\"470\",\"2\":\"ph\",\"3\":\"5.8\"},{\"1\":\"250\",\"2\":\"alkalinity\",\"3\":\"67.0\"},{\"1\":\"250\",\"2\":\"calcium\",\"3\":\"58.6\"},{\"1\":\"250\",\"2\":\"ph\",\"3\":\"7.8\"},{\"1\":\"410\",\"2\":\"alkalinity\",\"3\":\"28.8\"},{\"1\":\"410\",\"2\":\"calcium\",\"3\":\"10.2\"},{\"1\":\"410\",\"2\":\"ph\",\"3\":\"7.4\"},{\"1\":\"160\",\"2\":\"alkalinity\",\"3\":\"119.1\"},{\"1\":\"160\",\"2\":\"calcium\",\"3\":\"38.4\"},{\"1\":\"160\",\"2\":\"ph\",\"3\":\"7.9\"},{\"1\":\"160\",\"2\":\"alkalinity\",\"3\":\"25.4\"},{\"1\":\"160\",\"2\":\"calcium\",\"3\":\"8.8\"},{\"1\":\"160\",\"2\":\"ph\",\"3\":\"7.1\"},{\"1\":\"230\",\"2\":\"alkalinity\",\"3\":\"106.5\"},{\"1\":\"230\",\"2\":\"calcium\",\"3\":\"90.7\"},{\"1\":\"230\",\"2\":\"ph\",\"3\":\"6.8\"},{\"1\":\"560\",\"2\":\"alkalinity\",\"3\":\"8.5\"},{\"1\":\"560\",\"2\":\"calcium\",\"3\":\"2.5\"},{\"1\":\"560\",\"2\":\"ph\",\"3\":\"7.0\"},{\"1\":\"890\",\"2\":\"alkalinity\",\"3\":\"87.6\"},{\"1\":\"890\",\"2\":\"calcium\",\"3\":\"85.5\"},{\"1\":\"890\",\"2\":\"ph\",\"3\":\"7.5\"},{\"1\":\"180\",\"2\":\"alkalinity\",\"3\":\"114.0\"},{\"1\":\"180\",\"2\":\"calcium\",\"3\":\"72.6\"},{\"1\":\"180\",\"2\":\"ph\",\"3\":\"7.0\"},{\"1\":\"190\",\"2\":\"alkalinity\",\"3\":\"97.5\"},{\"1\":\"190\",\"2\":\"calcium\",\"3\":\"45.5\"},{\"1\":\"190\",\"2\":\"ph\",\"3\":\"6.8\"},{\"1\":\"440\",\"2\":\"alkalinity\",\"3\":\"11.8\"},{\"1\":\"440\",\"2\":\"calcium\",\"3\":\"24.2\"},{\"1\":\"440\",\"2\":\"ph\",\"3\":\"5.9\"},{\"1\":\"160\",\"2\":\"alkalinity\",\"3\":\"66.5\"},{\"1\":\"160\",\"2\":\"calcium\",\"3\":\"26.0\"},{\"1\":\"160\",\"2\":\"ph\",\"3\":\"8.3\"},{\"1\":\"670\",\"2\":\"alkalinity\",\"3\":\"16.0\"},{\"1\":\"670\",\"2\":\"calcium\",\"3\":\"41.2\"},{\"1\":\"670\",\"2\":\"ph\",\"3\":\"6.7\"},{\"1\":\"550\",\"2\":\"alkalinity\",\"3\":\"5.0\"},{\"1\":\"550\",\"2\":\"calcium\",\"3\":\"23.6\"},{\"1\":\"550\",\"2\":\"ph\",\"3\":\"6.2\"},{\"1\":\"580\",\"2\":\"alkalinity\",\"3\":\"25.6\"},{\"1\":\"580\",\"2\":\"calcium\",\"3\":\"12.6\"},{\"1\":\"580\",\"2\":\"ph\",\"3\":\"6.2\"},{\"1\":\"980\",\"2\":\"alkalinity\",\"3\":\"1.2\"},{\"1\":\"980\",\"2\":\"calcium\",\"3\":\"2.1\"},{\"1\":\"980\",\"2\":\"ph\",\"3\":\"4.3\"},{\"1\":\"310\",\"2\":\"alkalinity\",\"3\":\"34.0\"},{\"1\":\"310\",\"2\":\"calcium\",\"3\":\"13.1\"},{\"1\":\"310\",\"2\":\"ph\",\"3\":\"7.0\"},{\"1\":\"430\",\"2\":\"alkalinity\",\"3\":\"15.5\"},{\"1\":\"430\",\"2\":\"calcium\",\"3\":\"5.2\"},{\"1\":\"430\",\"2\":\"ph\",\"3\":\"6.9\"},{\"1\":\"280\",\"2\":\"alkalinity\",\"3\":\"17.3\"},{\"1\":\"280\",\"2\":\"calcium\",\"3\":\"3.0\"},{\"1\":\"280\",\"2\":\"ph\",\"3\":\"5.2\"},{\"1\":\"250\",\"2\":\"alkalinity\",\"3\":\"71.8\"},{\"1\":\"250\",\"2\":\"calcium\",\"3\":\"20.5\"},{\"1\":\"250\",\"2\":\"ph\",\"3\":\"7.9\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nand now we plot `mercury` against `xval` in facets according to `xname`. The other thing to remember is that the explanatory variables are on different scales, so we should use `scales=\"free\"` to allow each facet to have its own scales:\n\n::: {.cell}\n\n```{.r .cell-code}\nmercury %>% \npivot_longer(-mercury, names_to = \"xname\", values_to = \"xval\") %>% \nggplot(aes(x = xval, y = mercury)) + geom_point() + geom_smooth() + \nfacet_wrap(~xname, scales = \"free\", ncol = 2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/hachel-1.png){width=672}\n:::\n:::\n\nI did one more thing, which is to note that I am going to be assessing these relationships in a moment, so I would rather have squarer plots than the tall skinny ones that come out by default (that is to say these):\n\n::: {.cell}\n\n```{.r .cell-code}\nmercury %>% \npivot_longer(-mercury, names_to = \"xname\", values_to = \"xval\") %>% \nggplot(aes(x = xval, y = mercury)) + geom_point() + geom_smooth() + \nfacet_wrap(~xname, scales = \"free\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/endinger-1.png){width=672}\n:::\n:::\n\nTo make this happen, I added `ncol=2`, which says to arrange the facets in *two* columns (that is, as three cells of a $2\\times 2$ grid), and that makes them come out more landscape than portrait. `nrow=2` would have had the same effect.\n\nIf you are stuck, get three separate graphs, but note that you are making more work for yourself (that you will have to do again later).\n\n\n$\\blacksquare$\n\n\n(c) Describe any trends (or lack thereof) that you see on your graphs.\n\nSolution\n\n\nThink about \"form, direction, strength\" to guide you in interpreting what you see: is it a line or a curve, does it go up or down, and are the points mostly close to the trend or not? I think it makes most sense to talk about those things for the three trends one after the other:\n\n- `alkalinity`: this is a curved trend, but downward (the rate of decrease is fast at the beginning and then levels off). There is one clear outlier, but otherwise most of the points are close to the trend. \n- `calcium`: this is a down-and-up curved trend, though I think most of the evidence for the \"up\" part is the outlier on the middle right of the graph; without that, it would probably be a levelling-off decreasing trend as for `alkalinity`. There seem to be more outliers on this plot, and, on top of that, the points are not that close to the trend.\n- `ph`: this is a downward trend, more or less linear, but of only moderate strength. The points can be some way from the trend, but (in contrast to the other plots) there don't seem to be any points a *long* way away.\n\nIf you are going to talk about outliers, you need to be specific about where they are: describe where they are on the plot, or give approximate coordinates (you only need to be accurate enough to make it clear which point you are talking about). For example, I described one of the outliers on the `calcium` plot as \"middle right\", or you could describe the same point as having `calcium` above 80 and `mercury` near 1000, which narrows it down enough. There is a grey area between outliers and not being close to the trend overall; if it is a lot of points, I'd call it a weaker trend (as for `ph`), but if it's a few points that are a long way off, as for `calcium`, I'd call that outliers.\n\nExtra: most of the outliers are *above* the trends, which suggests that something to bring the high values down a bit would be helpful: that is, a transformation like log or square root. That's coming up later.\n\n\n$\\blacksquare$\n\n\n(d) Concerned by some of what you see on your plots, you consult with a fisheries scientist, who says that using the logs of `alkalinity` and `calcium` are often used in modelling this kind of data. Add columns containing the logs of these two variables to your dataframe, and run a regression predicting `mercury` from `ph` and the two new variables. Display the output from your regression.\n\nSolution\n\n\nI would create the two new variables and save them back into the original dataframe, myself:\n\n::: {.cell}\n\n```{.r .cell-code}\nmercury %>% \nmutate(log_alkalinity = log(alkalinity), log_calcium = log(calcium)) -> mercury\n```\n:::\n\nthough you could equally well create a new dataframe to hold them, as long as you remember to use the new dataframe from here on.\n\nThe regression has no great surprises:\n\n::: {.cell}\n\n```{.r .cell-code}\nmercury.1 <- lm(mercury ~ log_alkalinity + log_calcium + ph, data = mercury)\nsummary(mercury.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mercury ~ log_alkalinity + log_calcium + ph, data = mercury)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-326.02 -129.92  -27.67   71.17  581.76 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1181.20     244.21   4.837 2.79e-05 ***\nlog_alkalinity  -221.18      58.87  -3.757 0.000646 ***\nlog_calcium       87.90      51.43   1.709 0.096520 .  \nph               -36.63      51.09  -0.717 0.478362    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 204.4 on 34 degrees of freedom\nMultiple R-squared:  0.5578,\tAdjusted R-squared:  0.5188 \nF-statistic:  14.3 on 3 and 34 DF,  p-value: 3.424e-06\n```\n:::\n:::\n\n\n\n$\\blacksquare$\n\n\n(e) What might you learn from running Box-Cox here? Explain briefly, then run it (on the same variables as your regression above) and describe what the results tell you.\n\nSolution\n\n\nBox-Cox is to find out whether you need to transform the response variable, here `mercury`. The hint is that we have already transformed two of the explanatory variables, the ones that had some unusually large values, and so those are presumably now all right now.\n\n`boxcox` comes from the `MASS` package, so install and load that first.\n\n::: {.cell}\n\n```{.r .cell-code}\nboxcox(mercury ~ log_alkalinity + log_calcium + ph, data = mercury)\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/mercury-7-1.png){width=672}\n:::\n:::\n\nThe confidence interval for $\\lambda$ goes from about $-0.4$ to $0.5$. The only round-number powers in there are 0 (log) and 0.5 (square root, right on the end). The $\\lambda$ for the log transformation is right in the middle of the interval, so that's what I would choose. That means that we should use log of mercury instead of mercury itself in a regression.\n\nExtra: I looked at the residual plots of the regression `mercury.1`, in the hope that they would point you towards some kind of transformation of `mercury`, but they really didn't -- the biggest feature was an upper-end outlier,  more extreme than the one you see below, that appeared on all of them. So I didn't have you produce those graphs.\n\n\n$\\blacksquare$\n\n\n(f) Using the results of the Box-Cox procedure and your previous work in the question, what seems to be the most appropriate regression now? Fit it, and display the results. \n\nSolution\n\n\nThis says to predict log-mercury (Box-Cox) from log-alkalinity, log-calcium (the fisheries scientist) and pH:\n\n::: {.cell}\n\n```{.r .cell-code}\nmercury.2 <- lm(log(mercury) ~ log_alkalinity + log_calcium + ph, data = mercury)\nsummary(mercury.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(mercury) ~ log_alkalinity + log_calcium + ph, \n    data = mercury)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75244 -0.30191 -0.00783  0.23852  1.22932 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     7.55983    0.48981  15.434  < 2e-16 ***\nlog_alkalinity -0.45880    0.11807  -3.886 0.000449 ***\nlog_calcium     0.14702    0.10315   1.425 0.163185    \nph             -0.07998    0.10248  -0.780 0.440527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4099 on 34 degrees of freedom\nMultiple R-squared:  0.6069,\tAdjusted R-squared:  0.5723 \nF-statistic:  17.5 on 3 and 34 DF,  p-value: 4.808e-07\n```\n:::\n:::\n\nThere's no need to define a new column containing log-mercury in the dataframe, since you can define it in the `lm`. (Note for me: do I need to define new columns anywhere?)\n\n\n$\\blacksquare$\n\n\n(g) Obtain all the standard residual plots (the ones we have seen in this course) for this model. Describe any problems you see.\n\nSolution\n\n\nresiduals against fitted\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mercury.2, aes(x=.fitted, y=.resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/spaerwer-1.png){width=672}\n:::\n:::\n\nnormal quantile plot of residuals\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mercury.2, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/lugglinin-1.png){width=672}\n:::\n:::\n\nagainst the explanatory variables. This uses the ideas of `augment` (from `broom`) and pivoting longer:\n\n::: {.cell}\n\n```{.r .cell-code}\nmercury.2 %>% augment(mercury) %>% \npivot_longer(ph:log_calcium, names_to = \"xname\", values_to = \"xval\") %>% \nggplot(aes(x = xval, y = .resid)) + geom_point() +\nfacet_wrap(~xname, scales = \"free\", ncol = 2)\n```\n\n::: {.cell-output-display}\n![](multiple-regression_files/figure-html/aertzinars-1.png){width=672}\n:::\n:::\nI think the only troublesome feature on there is the large positive residual that appears at the top of all the plots. Other than that, I see nothing troubling. \n\nYou might, if you look a bit longer (but remember, apophenia!) see a tiny up and down on the plot with log-alkalinity, and maybe a small downward trend on the plots with the other two explanatory variables, but I would need a lot of convincing to say that these are more than chance. You are looking for *obvious trouble*, and I really don't think there's any sign of that here.\n\nExtra: you sometimes see a downward trend on residual plots that have an outlier on them. This is because if you have an outlier, it can change the slope disproportionately from what it ought to be, and then most of the observations at one end will be underestimated and most of the observations at the other end will be overestimated.\n\nExtra 2: which one was that outlier anyway?\n\n::: {.cell}\n\n```{.r .cell-code}\nmercury.2 %>% augment(mercury) %>% \nfilter(.resid > 1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mercury\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"alkalinity\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"calcium\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ph\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"log_alkalinity\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"log_calcium\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".fitted\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".resid\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".hat\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".sigma\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".cooksd\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".std.resid\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"890\",\"2\":\"87.6\",\"3\":\"85.5\",\"4\":\"7.5\",\"5\":\"4.472781\",\"6\":\"4.448516\",\"7\":\"5.561902\",\"8\":\"1.22932\",\"9\":\"0.1106393\",\"10\":\"0.3487183\",\"11\":\"0.3145577\",\"12\":\"3.180274\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\nHow do these compare to the other data values?\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mercury)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    mercury         alkalinity        calcium             ph       \n Min.   : 150.0   Min.   :  1.20   Min.   : 2.000   Min.   :4.300  \n 1st Qu.: 250.0   1st Qu.:  7.20   1st Qu.: 4.525   1st Qu.:5.800  \n Median : 445.0   Median : 18.45   Median :11.650   Median :6.850  \n Mean   : 488.4   Mean   : 37.15   Mean   :22.361   Mean   :6.634  \n 3rd Qu.: 650.0   3rd Qu.: 66.88   3rd Qu.:33.200   3rd Qu.:7.300  \n Max.   :1330.0   Max.   :119.10   Max.   :90.700   Max.   :8.700  \n log_alkalinity    log_calcium    \n Min.   :0.1823   Min.   :0.6931  \n 1st Qu.:1.9738   1st Qu.:1.5096  \n Median :2.9131   Median :2.4520  \n Mean   :3.0193   Mean   :2.4801  \n 3rd Qu.:4.2028   3rd Qu.:3.4938  \n Max.   :4.7800   Max.   :4.5076  \n```\n:::\n:::\n\nThe variable values are all high (even the pH, a modest-looking 7.5, is above Q3). \n\nRemember that the fitted value is of *log*-mercury, so we have to anti-log it to understand it (anti-log is \"exp\" since these are \"natural\" or base-$e$ logs):\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(5.561902)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 260.3175\n```\n:::\n:::\n\nThis was a *much* higher mercury value than expected, given the other variables.\n\n\n$\\blacksquare$\n\n\n\n\nnote to author: the datafile for _chemical.Rmd seems no longer to be readable.\n",
    "supporting": [
      "multiple-regression_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}