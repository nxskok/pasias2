{
  "hash": "3925b4773de26ce50d610dbb01be4f46",
  "result": {
    "engine": "knitr",
    "markdown": "# Discriminant analysis\n\nPackages for this chapter:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggbiplot)\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(car)\n```\n:::\n\n\n\n(Note: `ggbiplot` loads `plyr`, which overlaps a lot with `dplyr`\n(`filter`, `select` etc.). We want the `dplyr` stuff elsewhere, so we\nload `ggbiplot` *first*, and the things in `plyr` get hidden, as shown\nin the Conflicts. This, despite appearances, is what we want.)\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Telling whether a banknote is real or counterfeit\n\n\n <a name=\"sec:swiss-money\">*</a> A Swiss bank collected  a number of known counterfeit\n(fake) \nbills over time, and sampled a number of known genuine bills of the\nsame denomination.\nIs it possible to tell, from measurements taken from a bill, whether\nit is genuine or not? We will explore that issue here. The variables\nmeasured were:\n\n\n* length\n\n* right-hand width\n\n* left-hand width\n\n* top margin\n\n* bottom margin\n\n* diagonal\n\n\n\n(a) Read in the data from\n[link](http://ritsokiguess.site/datafiles/swiss1.txt), and\ncheck that you have 200 rows and 7 columns altogether.\n \n\n(b) Run a multivariate analysis of variance. What do you\nconclude? Is it worth running a discriminant analysis? (This is\nthe same procedure as with basic MANOVAs before.) \n \n\n(c) Run a discriminant analysis. Display the output.\n \n\n\n(d) How many linear \ndiscriminants did you get? Is that making sense? Explain briefly.\n \n\n(e) <a name=\"part:big\">*</a> \nUsing your output from the discriminant analysis, describe how\neach of the linear discriminants that you got is related to your\noriginal variables. (This can, maybe even should, be done crudely:\n\"does each variable feature in each linear discriminant: yes or no?\".)\n \n\n(f) What values of your variable(s) would make `LD1`\nlarge and positive?\n \n\n(g) <a name=\"part:means\">*</a> Find the means of each variable for each group (genuine\nand counterfeit bills). You can get this from your fitted linear\ndiscriminant object.\n \n\n(h) Plot your linear discriminant(s), however you like. Bear in\nmind that there is only one linear discriminant.\n \n\n(i) What kind of score on `LD1` do genuine bills\ntypically have? What kind of score do counterfeit bills typically\nhave? What characteristics of a bill, therefore, would you look at\nto determine if a bill is genuine or counterfeit?\n \n\n\n\n\n##  Urine and obesity: what makes a difference?\n\n\n A study was made of the characteristics of urine of young\nmen. The men were classified into four groups based on their degree of\nobesity. (The groups are labelled `a, b, c, d`.) Four variables\nwere measured, `x` (which you can ignore), pigment creatinine,\nchloride and chlorine. The data are in\n[link](http://ritsokiguess.site/datafiles/urine.csv) as a\n`.csv` file. There are 45 men altogether.\n\nYes, you may have seen this one before. What you found was something like this, probably also with the Box M test (which has a P-value that is small, but not small enough to be a concern):\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/urine.csv\"\nurine <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 45 Columns: 5\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): obesity\ndbl (4): x, creatinine, chloride, chlorine\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nresponse <- with(urine, cbind(creatinine, chlorine, chloride))\nurine.1 <- manova(response ~ obesity, data = urine)\nsummary(urine.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Df  Pillai approx F num Df den Df  Pr(>F)  \nobesity    3 0.43144   2.2956      9    123 0.02034 *\nResiduals 41                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\nOur aim is to understand why this result was significant.\n\n\n\n(a) Read in the data again (copy the code from above) and\nobtain a discriminant analysis.\n \n\n(b) How many linear discriminants were you expecting? Explain briefly.\n \n\n(c) Why do you think we should pay attention to the first two\nlinear discriminants but not the third? Explain briefly.\n \n\n(d) Plot the first two linear discriminant scores (against each\nother), with each obesity group being a different colour.\n \n\n(e) <a name=\"part:plot\">*</a> Looking at your plot, discuss how (if at all) the\ndiscriminants separate the obesity groups. (Where does each\nobesity group fall on the plot?)\n \n\n(f) <a name=\"part:table\">*</a> Obtain a table showing observed and predicted obesity\ngroups. Comment on the accuracy of the predictions.\n \n\n(g) Do your conclusions from (<a href=\"#part:plot\">here</a>) and\n(<a href=\"#part:table\">here</a>) appear to be consistent?\n \n\n\n\n\n\n##  Understanding a MANOVA\n\n\n One use of discriminant analysis is to\nunderstand the results of a MANOVA. This question is a followup to a\nprevious MANOVA that we did, the one with two variables `y1`\nand `y2` and three groups `a` through `c`. The\ndata were in [link](http://ritsokiguess.site/datafiles/simple-manova.txt).\n\n\n\n(a) Read the data in again and run the MANOVA that you did\nbefore. \n\n\n\n(b) Run a discriminant analysis \"predicting\" group from the\ntwo response variables. Display the output.\n\n\n\n(c) <a name=\"part:output\">*</a> In the output from the discriminant analysis,\nwhy are there exactly two linear discriminants `LD1` and\n`LD2`?\n\n\n\n\n(d) <a name=\"part:svd\">*</a> From the output, how would you say that the\nfirst linear discriminant `LD1` compares in importance to the\nsecond one `LD2`: much more important, more important, equally\nimportant, less important, much less important? Explain briefly. \n\n\n\n\n(e) Obtain a plot of the\ndiscriminant scores.\n\n\n\n\n(f) Describe briefly how `LD1` and/or `LD2`\nseparate the groups. Does your picture confirm the relative importance\nof `LD1` and `LD2` that you found  back in part (<a href=\"#part:svd\">here</a>)? Explain briefly.\n\n\n\n\n(g) What makes group `a` have a low score on `LD1`?\nThere are two steps that you need to make: consider the means of group\n`a` on variables `y1` and `y2` and how they\ncompare to the other groups, and consider how\n`y1` and `y2` play into the score on `LD1`.\n\n\n\n\n(h) Obtain predictions for the group memberships of each\nobservation, and make a table of the actual group memberships against\nthe predicted ones. How many of the observations were wrongly classified?\n\n\n\n\n\n\n\n##  What distinguishes people who do different jobs?\n\n\n 244^[Grammatically, I am supposed to write this as  \"two hundred and forty-four\" in words, since I am not supposed to  start a sentence with a number. But, I say, deal with it. Or, I  suppose, \"there are 244 people who work...\".] people work at a\ncertain company. \nThey each have one of three jobs: customer service, mechanic,\ndispatcher. In the data set, these are labelled 1, 2 and 3\nrespectively.  In addition, they each are rated on scales called\n`outdoor`, `social` and `conservative`. Do people\nwith different jobs tend to have different scores on these scales, or,\nto put it another way, if you knew a person's scores on\n`outdoor`, `social` and `conservative`, could you\nsay something about what kind of job they were likely to hold? The\ndata are in [link](http://ritsokiguess.site/datafiles/jobs.txt).\n\n\n\n(a) Read in the data and display some of it.\n\n\n\n(b) Note the types of each of the variables, and create any new\nvariables that you need to.\n\n\n\n(c) Run a multivariate analysis of variance to convince yourself\nthat there are some differences in scale scores among the jobs.\n\n\n\n(d) Run a discriminant analysis and display the output.\n\n\n\n(e) Which is the more important, `LD1` or `LD2`? How\nmuch more important? Justify your answer briefly.\n\n\n\n(f) Describe what values for an individual on the scales will make\neach of `LD1` and `LD2` high. \n\n\n\n(g) The first group of employees, customer service, have the\nhighest mean on `social` and the lowest mean on both of the\nother two scales. Would you expect the customer service employees to\nscore high or low on `LD1`? What about `LD2`?\n\n\n\n(h) Plot your discriminant scores (which you will have to obtain\nfirst), and see if you were right about the customer service\nemployees in terms of `LD1` and `LD2`. The job names\nare rather long, and there are a lot of individuals, so it is\nprobably best to plot the scores as coloured circles with a legend\nsaying which colour goes with which job (rather than labelling each\nindividual with the job they have).\n\n\n\n(i) <a name=\"part:predjob\">*</a> Obtain predicted job allocations for each individual (based on\ntheir scores on the three scales), and tabulate the true jobs\nagainst the predicted jobs. How would you describe the quality of\nthe classification? Is that in line with what the plot would suggest?\n\n\n\n(j) Consider an employee with these scores: 20 on\n`outdoor`, 17 on `social` and 8 on `conservative` What job do you think\nthey do, and how certain are you about that? Use `predict`,\nfirst making a data frame out of the values to predict for.\n\n\n\n\n\n\n##  Observing children with ADHD\n\n\n A number of children with ADHD were observed by their mother\nor their father (only one parent observed each child). Each parent was\nasked to rate occurrences of behaviours of four different types,\nlabelled `q1` through `q4` in the data set. Also\nrecorded was the identity of the parent doing the observation for each\nchild: 1 is father, 2 is mother.\n\nCan we tell (without looking at the `parent` column) which\nparent is doing the observation? Research suggests that rating the\ndegree of impairment in different categories depends on who is doing\nthe rating: for example, mothers may feel that a  child has difficulty\nsitting still, while fathers, who might do more observing of a child\nat play, might think of such a child as simply being \"active\" or\n\"just being a kid\". The data are in\n[link](http://ritsokiguess.site/datafiles/adhd-parents.txt). \n\n\n\n(a) Read in the data and confirm that you have four ratings and\na column labelling the parent who made each observation.\n\n\n\n(b) Run a suitable discriminant analysis and display the output.\n\n\n\n(c) Which behaviour item or items seem to be most helpful at\ndistinguishing the parent making the observations? Explain briefly.\n\n\n\n(d) Obtain the predictions from the `lda`, and make a\nsuitable plot of the discriminant scores, bearing in mind that you\nonly have one `LD`.  Do you think there will be any\nmisclassifications? Explain briefly.\n\n\n\n(e) Obtain the predicted group memberships and make a table of\nactual vs.\\ predicted. Were there any misclassifications? Explain\nbriefly. \n\n\n\n(f) Re-run the discriminant analysis using cross-validation,\nand again obtain a table of actual and predicted parents. Is the\npattern of misclassification different from before? Hints: (i) Bear in mind\nthat there is no `predict` step this time, because the\ncross-validation output includes predictions; (ii) use a different name\nfor the predictions this time because we are going to do a\ncomparison in a moment.\n\n\n\n(g) Display the original data (that you read in from the data\nfile) side by side with two sets of posterior probabilities: the\nones that you obtained with `predict` before, and the ones\nfrom the cross-validated analysis. Comment briefly on whether the\ntwo sets of posterior probabilities are similar. Hints: (i) use\n`data.frame` rather than `cbind`, for reasons that I\nexplain elsewhere; (ii) round the posterior probabilities to 3\ndecimals before you display them.\nThere are only 29 rows, so look at them all. I am going to add the\n`LD1` scores to my output and sort by that, but you don't\nneed to. (This is for something I am going to add later.)\n\n\n\n(h) Row 17 of your (original) data frame above, row 5 of the\noutput in the previous part, is the mother that was\nmisclassified as a father. Why is it that the cross-validated\nposterior probabilities are 1 and 0, while the previous posterior\nprobabilities are a bit less than 1 and a bit more than 0?\n\n\n\n(i) Find the parents where the cross-validated posterior\nprobability of being a father is \"non-trivial\": that is, not\nclose to zero and not close to 1. (You will have to make a judgement\nabout what \"close to zero or 1\" means for you.) What do these\nparents have in common, all of them or most of them?\n\n\n\n\n\n\n\n\n\n##  Growing corn\n\n\n A new type of corn seed has been developed.\nThe people developing it want to know if the type of soil the seed\nis planted in has an impact on how well the seed performs, and if so,\nwhat kind of impact. Three\noutcome measures were used: the yield of corn produced (from a fixed\namount of seed), the amount of water needed, and the amount of\nherbicide needed. The data are in\n[link](http://ritsokiguess.site/datafiles/cornseed.csv). 32 fields\nwere planted with the seed, 8 fields with each soil type.\n\n\n\n(a) Read in the data and verify that you have 32 observations\nwith the correct variables.\n\n\n\n(b) Run a multivariate analysis of variance to see whether\nthe type of soil has any effect on any of the variables. What do you\nconclude from it?\n\n\n\n(c) Run a discriminant analysis on these data, \"predicting\"\nsoil type from the three response variables. Display the results.\n\n\n\n(d) <a name=\"part:corn-svd\">*</a> \nWhich linear discriminants seem to be worth paying attention to?\nWhy did you get three linear discriminants? Explain briefly.\n\n\n\n(e) Which response variables do the important linear\ndiscriminants depend on? Answer this by extracting something from\nyour discriminant analysis output.\n\n\n\n(f) Obtain predictions for the discriminant analysis. (You\ndon't need to do anything with them yet.)\n\n\n\n(g) Plot the first two discriminant scores against each other,\ncoloured by soil type. You'll have to start by making a data frame\ncontaining what you need.\n\n\n\n(h) On your plot that you just made, explain briefly how `LD1`\ndistinguishes at least one of the soil types.\n\n\n\n\n(i) On your plot, does `LD2` appear to do anything to\nseparate the groups? Is this surprising given your earlier findings?\nExplain briefly.\n\n\n\n\n(j) Make a table of actual and predicted `soil`\ngroup. Which soil type was classified correctly the most often? \n\n\n\n\n\n\n\n\n##  Understanding athletes' height, weight, sport and gender\n\n\n On a previous assignment, we used MANOVA on the athletes\ndata to demonstrate that there was a significant relationship between\nthe combination of the athletes' height and weight, with the sport they\nplay and the athlete's gender. The problem with MANOVA is that it\ndoesn't give any information about the *kind* of relationship. To\nunderstand that, we need to do discriminant analysis, which is the\npurpose of this question.\n\nThe data can be found at\n[link](https://ritsokiguess.site/datafiles/ais.txt). \n\n\n\n(a) Once again, read in and display (some of) the data, bearing\nin mind that the data values are separated by *tabs*. (This\nought to be a free two marks.)\n\n\n(b) Use `unite` to make a new column in your data frame\nwhich contains the sport-gender *combination*. Display it. (You\nmight like to display only a few columns so that it is clear that\nyou did the right thing.) Hint: you've seen `unite` in the\npeanuts example in class.\n\n\n(c) Run a discriminant analysis \"predicting\" sport-gender\ncombo from height and weight. Display the results. (No comment\nneeded yet.)\n\n\n(d) What kind of height and weight would make an athlete have a\nlarge (positive) score on `LD1`? Explain briefly.\n\n\n(e) Make a guess at the sport-gender combination that has the\n*highest* score on LD1. Why did you choose the combination you did?\n\n\n(f) <a name=\"part:ld2\">*</a> What combination of height and weight would make an athlete have a\n*small* (that is, very negative) score on LD2? Explain briefly.\n\n\n(g) Obtain predictions for the discriminant analysis, and use\nthese to make a plot of `LD1` score against `LD2`\nscore, with the individual athletes distinguished by what sport they play\nand gender they are. (You can use colour to distinguish them, or you\ncan use shapes. If you want to go the latter way, there are clues in\nmy solutions to the MANOVA question about these athletes.)\n\n\n(h) Look on your graph for the four athletes with the smallest\n(most negative) scores on `LD2`. What do they have in common?\nDoes this make sense, given your answer to part (<a href=\"#part:ld2\">here</a>)?\nExplain briefly.\n\n\n(i) Obtain a (very large) square table, or a (very long) table\nwith frequencies, of actual and predicted sport-gender\ncombinations. You will probably have to make the square table very\nsmall to fit it on the page. For that, displaying the columns in two\nor more sets is OK (for example, six columns and all the rows, six\nmore columns and all the rows, then the last five columns for all\nthe rows).  Are there any sport-gender combinations that\nseem relatively easy to classify correctly?  Explain briefly.\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Telling whether a banknote is real or counterfeit\n\n\n <a name=\"sec:swiss-money\">*</a> A Swiss bank collected  a number of known counterfeit\n(fake) \nbills over time, and sampled a number of known genuine bills of the\nsame denomination.\nIs it possible to tell, from measurements taken from a bill, whether\nit is genuine or not? We will explore that issue here. The variables\nmeasured were:\n\n\n* length\n\n* right-hand width\n\n* left-hand width\n\n* top margin\n\n* bottom margin\n\n* diagonal\n\n\n\n(a) Read in the data from\n[link](http://ritsokiguess.site/datafiles/swiss1.txt), and\ncheck that you have 200 rows and 7 columns altogether.\n \nSolution\n\n\nCheck the data file first. It's aligned in columns, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/swiss1.txt\"\nswiss <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n-- Column specification --------------------------------------------------------\ncols(\n  length = col_double(),\n  left = col_double(),\n  right = col_double(),\n  bottom = col_double(),\n  top = col_double(),\n  diag = col_double(),\n  status = col_character()\n)\n```\n\n\n:::\n\n```{.r .cell-code}\nswiss\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 200 x 7\n   length  left right bottom   top  diag status \n    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <chr>  \n 1   215.  131   131.    9     9.7  141  genuine\n 2   215.  130.  130.    8.1   9.5  142. genuine\n 3   215.  130.  130.    8.7   9.6  142. genuine\n 4   215.  130.  130.    7.5  10.4  142  genuine\n 5   215   130.  130.   10.4   7.7  142. genuine\n 6   216.  131.  130.    9    10.1  141. genuine\n 7   216.  130.  130.    7.9   9.6  142. genuine\n 8   214.  130.  129.    7.2  10.7  142. genuine\n 9   215.  129.  130.    8.2  11    142. genuine\n10   215.  130.  130.    9.2  10    141. genuine\n# i 190 more rows\n```\n\n\n:::\n:::\n\n       \n\nYep, 200 rows and 7 columns.\n \n$\\blacksquare$\n\n(b) Run a multivariate analysis of variance. What do you\nconclude? Is it worth running a discriminant analysis? (This is\nthe same procedure as with basic MANOVAs before.) \n \nSolution\n\n\nSmall-m `manova` will do here:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(swiss, cbind(length, left, right, bottom, top, diag))\nswiss.1 <- manova(response ~ status, data = swiss)\nsummary(swiss.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Df  Pillai approx F num Df den Df    Pr(>F)    \nstatus      1 0.92415   391.92      6    193 < 2.2e-16 ***\nResiduals 198                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(BoxM(response, swiss$status))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Box's M Test \n\nChi-Squared Value = 121.8991 , df = 21  and p-value: 3.2e-16 \n```\n\n\n:::\n:::\n\nThat is a very significant Box's M test, which means that we shouldn't trust the MANOVA at all. It is only the fact that the MANOVA is *so* significant that provides any evidence that the discriminant analysis is worth doing.        \n\nExtra: you might be wondering whether you had to go to all that trouble to\nmake the response variable. Would this work?\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse2 <- swiss %>% select(length:diag)\nswiss.1a <- manova(response2 ~ status, data = swiss)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in model.frame.default(formula = response2 ~ status, data = swiss, : invalid type (list) for variable 'response2'\n```\n\n\n:::\n:::\n\n \n\nNo, because `response2` needs to be an R `matrix`, and it isn't:\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(response2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n```\n\n\n:::\n:::\n\n \n\nThe error message was a bit cryptic (nothing unusual there), but a\ndata frame (to R) is a special kind of `list`, so that R didn't\nlike `response2` being a data frame, which it\nthought was a list.\n\nThis, however, works, since it turns the data frame into a matrix:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse4 <- swiss %>% select(length:diag) %>% as.matrix()\nswiss.2a <- manova(response4 ~ status, data = swiss)\nsummary(swiss.2a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Df  Pillai approx F num Df den Df    Pr(>F)    \nstatus      1 0.92415   391.92      6    193 < 2.2e-16 ***\nResiduals 198                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n \nAnyway, the conclusion: the status of a bill (genuine or counterfeit)\ndefinitely has an influence on some or all of those other variables,\nsince the P-value $2.2 \\times 10^{-16}$ (or less) is really small. So\nit is apparently worth running a discriminant analysis to figure out where the\ndifferences lie.\n\nAs a piece of strategy, for creating the response matrix, you can\nalways either use `cbind`, which creates a `matrix`\ndirectly, or you can use `select`, which is often easier but\ncreates a data frame, and then turn *that* into a `matrix`\nusing `as.matrix`. As long as you end up with a\n`matrix`, it's all good.\n \n$\\blacksquare$\n\n(c) Run a discriminant analysis. Display the output.\n \n\nSolution\n\n \nNow we forget about all that\n`response` stuff. For a discriminant analysis, the\ngrouping variable (or combination of the grouping variables)\nis the \"response\", and the quantitative ones are\n\"explanatory\":\n\n::: {.cell}\n\n```{.r .cell-code}\nswiss.3 <- lda(status ~ length + left + right + bottom + top + diag, data = swiss)\nswiss.3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(status ~ length + left + right + bottom + top + diag, data = swiss)\n\nPrior probabilities of groups:\ncounterfeit     genuine \n        0.5         0.5 \n\nGroup means:\n             length    left   right bottom    top    diag\ncounterfeit 214.823 130.300 130.193 10.530 11.133 139.450\ngenuine     214.969 129.943 129.720  8.305 10.168 141.517\n\nCoefficients of linear discriminants:\n                LD1\nlength  0.005011113\nleft    0.832432523\nright  -0.848993093\nbottom -1.117335597\ntop    -1.178884468\ndiag    1.556520967\n```\n\n\n:::\n:::\n\n       \n \n$\\blacksquare$\n\n(d) How many linear \ndiscriminants did you get? Is that making sense? Explain briefly.\n \nSolution\n\n\nI got one discriminant, which makes sense because there are two\ngroups, and the smaller of 6 (variables, not counting the grouping\none) and $2-1$ is 1. \n \n$\\blacksquare$\n\n(e) <a name=\"part:big\">*</a> \nUsing your output from the discriminant analysis, describe how\neach of the linear discriminants that you got is related to your\noriginal variables. (This can, maybe even should, be done crudely:\n\"does each variable feature in each linear discriminant: yes or no?\".)\n \nSolution\n\n\nThis is the Coefficients of Linear Discriminants. Make a call about whether each of those coefficients is close to zero (small in size compared to the others), or definitely positive or definitely negative.\nThese are judgement calls: either you can say that LD1\ndepends mainly on `diag` (treating the other coefficients\nas \"small\" or close to zero), or you can say that `LD1`\ndepends on everything except `length`.\n \n$\\blacksquare$\n\n(f) What values of your variable(s) would make `LD1`\nlarge and positive?\n \nSolution\n\n\nDepending on your answer to the previous part: \nIf you said that only `diag` was important, `diag`\nbeing large would make `LD1` large and positive.\nIf you said that everything but `length` was important,\nthen it's a bit more complicated: `left` and\n`diag` large, `right`, `bottom` and\n`top` small (since their coefficients are negative). \n \n$\\blacksquare$\n\n(g) <a name=\"part:means\">*</a> Find the means of each variable for each group (genuine\nand counterfeit bills). You can get this from your fitted linear\ndiscriminant object.\n \nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nswiss.3$means\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             length    left   right bottom    top    diag\ncounterfeit 214.823 130.300 130.193 10.530 11.133 139.450\ngenuine     214.969 129.943 129.720  8.305 10.168 141.517\n```\n\n\n:::\n:::\n\n \n \n$\\blacksquare$\n\n(h) Plot your linear discriminant(s), however you like. Bear in\nmind that there is only one linear discriminant.\n \nSolution\n\n\nWith only one linear discriminant, we can plot `LD1` scores on\nthe $y$-axis and the grouping variable on the $x$-axis. How\nyou do that is up to you. \n\nBefore we start, though, we need the `LD1` scores. This means\ndoing predictions. The discriminant scores are in there. We take the\nprediction output and make a data frame with all the things in the\noriginal data. My current preference (it changes) is to store the\npredictions, and then `cbind` them with the original data,\nthus:\n\n::: {.cell}\n\n```{.r .cell-code}\nswiss.pred <- predict(swiss.3)\nd <- cbind(swiss, swiss.pred)\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  length  left right bottom  top  diag  status   class posterior.counterfeit\n1  214.8 131.0 131.1    9.0  9.7 141.0 genuine genuine          3.245560e-07\n2  214.6 129.7 129.7    8.1  9.5 141.7 genuine genuine          1.450624e-14\n3  214.8 129.7 129.7    8.7  9.6 142.2 genuine genuine          1.544496e-14\n4  214.8 129.7 129.6    7.5 10.4 142.0 genuine genuine          4.699587e-15\n5  215.0 129.6 129.7   10.4  7.7 141.8 genuine genuine          1.941700e-13\n6  215.7 130.8 130.5    9.0 10.1 141.4 genuine genuine          1.017550e-08\n  posterior.genuine      LD1\n1         0.9999997 2.150948\n2         1.0000000 4.587317\n3         1.0000000 4.578290\n4         1.0000000 4.749580\n5         1.0000000 4.213851\n6         1.0000000 2.649422\n```\n\n\n:::\n:::\n\n      \n\nI needed `head` because `cbind` makes an old-fashioned\n`data.frame` rather than a `tibble`, so if you display\nit, you get all of it.  \n\nThis gives the LD1 scores, predicted groups, and posterior\nprobabilities as well. That saves us having to pick out the other\nthings later.\nThe obvious thing is a boxplot. By examining `d` above (didn't\nyou?), you saw that the LD scores were in a column called\n`LD1`:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = status, y = LD1)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/antioch-1.pdf){fig-pos='H'}\n:::\n:::\n\n   \n\nThis shows that positive LD1 scores go (almost without exception) with\ngenuine bills, and negative ones with counterfeit bills.\nIt also shows that there are three outlier bills, two counterfeit ones\nwith unusually high LD1 score, and one genuine one with unusually\n*low* LD1 score, at least for a genuine bill.\n\nThis goes to show that (the Box M test notwithstanding) the two types of bill really are different in a way that is worth investigating.\n\nOr you could do faceted histograms of `LD1` by `status`:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = LD1)) + geom_histogram(bins = 10) + facet_grid(status ~ .)\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/swiss-money-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThis shows much the same thing as `plot(swiss.3)` does (try it).\n \n$\\blacksquare$\n\n(i) What kind of score on `LD1` do genuine bills\ntypically have? What kind of score do counterfeit bills typically\nhave? What characteristics of a bill, therefore, would you look at\nto determine if a bill is genuine or counterfeit?\n \nSolution\n\n\nThe genuine bills almost all have a *positive* score on\nLD1, while the counterfeit ones all have a *negative* one. \nThis means that the genuine bills (depending on your answer to\n(<a href=\"#part:big\">here</a>)) have a large `diag`, or they have a\nlarge `left` and `diag`, and a small\n`right`, `bottom` and `top`.\nIf you look at your table of means in (<a href=\"#part:means\">here</a>), you'll\nsee that the genuine bills do indeed have a large `diag`,\nor, depending on your earlier answer, a small `right`,\n`bottom` and `top`, but not actually a small\n`left` (the `left` values are very close for the\ngenuine and counterfeit coins).\n\nExtra: as to that last point, this is easy enough to think about. A\nboxplot seems a nice way to display it:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(y = left, x = status)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/gtabita-1.pdf){fig-pos='H'}\n:::\n:::\n\n      \n\nThere is a fair bit of overlap: the median is higher for the\ncounterfeit bills, but the highest value actually belongs to a genuine one.\n\nCompare that to `diag`:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(y = diag, x = status)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/iggle-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nHere, there is an almost complete separation of the genuine and\ncounterfeit bills, with just one low outlier amongst the genuine bills\nspoiling the pattern.\nI didn't look at the predictions (beyond the discriminant scores),\nsince this question (as set on an assignment a couple of years ago)\nwas already too long, but there is no difficulty in doing so.\nEverything is in the data frame I called `d`:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(d, table(obs = status, pred = class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             pred\nobs           counterfeit genuine\n  counterfeit         100       0\n  genuine               1      99\n```\n\n\n:::\n:::\n\n \n\n(this labels the rows and columns, which is not necessary but is nice.)\n\nThe `tidyverse` way is to make a data frame out of the actual\nand predicted statuses, and then `count` what's in there:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% count(status, class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       status       class   n\n1 counterfeit counterfeit 100\n2     genuine counterfeit   1\n3     genuine     genuine  99\n```\n\n\n:::\n:::\n\n \n\nThis gives a \"long\" table, with frequencies for each of the\ncombinations for which anything was observed.\n\nFrequency tables are usually wide, and we can make this one so by pivot-wider-ing `pred`:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(status, class) %>%\n  pivot_wider(names_from = class, values_from = n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 3\n  status      counterfeit genuine\n  <chr>             <int>   <int>\n1 counterfeit         100      NA\n2 genuine               1      99\n```\n\n\n:::\n:::\n\n \nOne of the genuine bills is incorrectly classified as a counterfeit\none (evidently that low outlier on LD1), but every single one of the\ncounterfeit bills is classified correctly. That missing value is\nactually a frequency that is zero, which you can fix up thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(status, class) %>%\n  pivot_wider(names_from = class, values_from = n, values_fill = 0) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 3\n  status      counterfeit genuine\n  <chr>             <int>   <int>\n1 counterfeit         100       0\n2 genuine               1      99\n```\n\n\n:::\n:::\n\n \n\nwhich turns any missing values into the zeroes they should be in this\nkind of problem.\nIt would be interesting to see what the posterior probabilities look\nlike for that misclassified bill:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% filter(status != class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   length  left right bottom  top  diag  status       class\n70  214.9 130.2 130.2      8 11.2 139.6 genuine counterfeit\n   posterior.counterfeit posterior.genuine        LD1\n70             0.9825773        0.01742267 -0.5805239\n```\n\n\n:::\n:::\n\n \n\nOn the basis of the six measured variables, this looks a lot more like\na counterfeit bill than a genuine one.\nAre there any other bills where there is any doubt? One way to find out is to find the maximum of the two posterior probabilities. If this is small, \nthere is some doubt about whether the bill is real or fake. 0.99 seems like a very stringent cutoff, but let's try it and see:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  mutate(max.post = pmax(posterior.counterfeit, posterior.genuine)) %>%\n  filter(max.post < 0.99) %>%\n  dplyr::select(-c(length:diag))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    status       class posterior.counterfeit posterior.genuine        LD1\n70 genuine counterfeit             0.9825773        0.01742267 -0.5805239\n    max.post\n70 0.9825773\n```\n\n\n:::\n:::\n\n \nThe only one is the bill that was misclassified: it was actually genuine, but\nwas classified as counterfeit. The posterior probabilities say that it\nwas pretty unlikely to be genuine, but it was the only bill for which\nthere was any noticeable doubt at all.\n\nI had to use `pmax` rather than `max` there, because I\nwanted `max.post` to contain the larger of the two\ncorresponding entries: that is, the first entry in `max.post`\nis the larger of the first entry of `counterfeit` and the first\nentry in `genuine`. If I used `max` instead, I'd get the\nlargest of *all* the entries in `counterfeit` and\n*all* the entries in `genuine`, repeated 200 times. (Try\nit and see.) `pmax` stands for \"parallel maximum\", that is,\nfor each row separately. This also should work:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  rowwise() %>% \n  mutate(max.post = max(posterior.counterfeit, posterior.genuine)) %>%\n  filter(max.post < 0.99) %>%\n  select(-c(length:diag))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 6\n# Rowwise: \n  status  class       posterior.counterfeit posterior.genuine    LD1 max.post\n  <chr>   <fct>                       <dbl>             <dbl>  <dbl>    <dbl>\n1 genuine counterfeit                 0.983            0.0174 -0.581    0.983\n```\n\n\n:::\n:::\n\n \nBecause we're using `rowwise`, `max` is applied to the pairs\nof values of `posterior.counterfeit` and `posterior.genuine`, \n*taken one row at a time.*\n\n$\\blacksquare$\n\n\n\n\n##  Urine and obesity: what makes a difference?\n\n\n A study was made of the characteristics of urine of young\nmen. The men were classified into four groups based on their degree of\nobesity. (The groups are labelled `a, b, c, d`.) Four variables\nwere measured, `x` (which you can ignore), pigment creatinine,\nchloride and chlorine. The data are in\n[link](http://ritsokiguess.site/datafiles/urine.csv) as a\n`.csv` file. There are 45 men altogether.\n\nYes, you may have seen this one before. What you found was something like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/urine.csv\"\nurine <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 45 Columns: 5\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): obesity\ndbl (4): x, creatinine, chloride, chlorine\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nresponse <- with(urine, cbind(creatinine, chlorine, chloride))\nurine.1 <- manova(response ~ obesity, data = urine)\nsummary(urine.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Df  Pillai approx F num Df den Df  Pr(>F)  \nobesity    3 0.43144   2.2956      9    123 0.02034 *\nResiduals 41                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(BoxM(response, urine$obesity))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Box's M Test \n\nChi-Squared Value = 30.8322 , df = 18  and p-value: 0.0301 \n```\n\n\n:::\n:::\n\n \n\nOur aim is to understand why this result was significant. (Remember that the P-value on Box's M test is not small enough to be worried about.)\n\n\n\n(a) Read in the data again (copy the code from above) and\nobtain a discriminant analysis.\n \nSolution\n\n\nAs above, plus:\n\n::: {.cell}\n\n```{.r .cell-code}\nurine.1 <- lda(obesity ~ creatinine + chlorine + chloride, data = urine)\nurine.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(obesity ~ creatinine + chlorine + chloride, data = urine)\n\nPrior probabilities of groups:\n        a         b         c         d \n0.2666667 0.3111111 0.2444444 0.1777778 \n\nGroup means:\n  creatinine chlorine chloride\na   15.89167 5.275000 6.012500\nb   17.82143 7.450000 5.214286\nc   16.34545 8.272727 5.372727\nd   11.91250 9.675000 3.981250\n\nCoefficients of linear discriminants:\n                   LD1        LD2         LD3\ncreatinine  0.24429462 -0.1700525 -0.02623962\nchlorine   -0.02167823 -0.1353051  0.11524045\nchloride    0.23805588  0.3590364  0.30564592\n\nProportion of trace:\n   LD1    LD2    LD3 \n0.7476 0.2430 0.0093 \n```\n\n\n:::\n:::\n\n     \n$\\blacksquare$ \n\n(b) How many linear discriminants were you expecting? Explain briefly.\n \nSolution\n\n\nThere are 3 variables and 4 groups, so the smaller of 3 and\n$4-1=3$: that is, 3.\n \n$\\blacksquare$\n\n(c) Why do you think we should pay attention to the first two\nlinear discriminants but not the third? Explain briefly.\n \nSolution\n\n The first two \"proportion of\ntrace\" values are a lot bigger than the third (or, the third\none is close to 0).\n \n$\\blacksquare$\n\n(d) Plot the first two linear discriminant scores (against each\nother), with each obesity group being a different colour.\n \nSolution\n\n First obtain the predictions, and\nthen make a data frame out of the original data and the\npredictions. \n\n::: {.cell}\n\n```{.r .cell-code}\nurine.pred <- predict(urine.1)\nd <- cbind(urine, urine.pred)\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  obesity  x creatinine chloride chlorine class posterior.a posterior.b\n1       a 24       17.6     5.15      7.5     b   0.2327008   0.4124974\n2       a 32       13.4     5.75      7.1     a   0.3599095   0.2102510\n3       a 17       20.3     4.35      2.3     b   0.2271118   0.4993603\n4       a 30       22.3     7.55      4.0     b   0.2935374   0.4823766\n5       a 30       20.5     8.50      2.0     a   0.4774623   0.3258104\n6       a 27       18.5    10.25      2.0     a   0.6678748   0.1810762\n  posterior.c posterior.d      x.LD1      x.LD2      x.LD3\n1   0.3022445 0.052557333  0.3926519 -0.3290621 -0.0704284\n2   0.2633959 0.166443708 -0.4818807  0.6547023  0.1770694\n3   0.2519562 0.021571722  0.9745295 -0.3718462 -0.9850425\n4   0.2211991 0.002886957  2.1880446  0.2069465  0.1364540\n5   0.1933571 0.003370286  2.0178238  1.1247359  0.2435680\n6   0.1482500 0.002799004  1.9458323  2.0931546  0.8309276\n```\n\n\n:::\n:::\n\n             \n\n`urine` produced the first five columns and `urine.pred`\nproduced the rest.\n\nTo go a more tidyverse way, we can combine the original data frame and\nthe predictions using `bind_cols`, but we have to be more\ncareful that the things we are gluing together are both data frames:\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(urine)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n```\n\n\n:::\n\n```{.r .cell-code}\nclass(urine.pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"list\"\n```\n\n\n:::\n:::\n\n \n\n`urine` is a `tibble` all right, but `urine.pred` is a `list`. What does it look like?\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(urine.pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 3\n $ class    : Factor w/ 4 levels \"a\",\"b\",\"c\",\"d\": 2 1 2 2 1 1 3 3 1 1 ...\n $ posterior: num [1:45, 1:4] 0.233 0.36 0.227 0.294 0.477 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:45] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:4] \"a\" \"b\" \"c\" \"d\"\n $ x        : num [1:45, 1:3] 0.393 -0.482 0.975 2.188 2.018 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:45] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:3] \"LD1\" \"LD2\" \"LD3\"\n```\n\n\n:::\n:::\n\n \nA data frame is a list for which all the items are the same length,\nbut some of the things in here are matrices. You can tell because they\nhave a number of rows, 45, *and* a number of columns, 3 or\n4. They *do* have the right number of rows, though, so something\nlike `as.data.frame` (a base R function) will smoosh them all\ninto one data frame, grabbing the columns from the matrices:\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(as.data.frame(urine.pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  class posterior.a posterior.b posterior.c posterior.d      x.LD1      x.LD2\n1     b   0.2327008   0.4124974   0.3022445 0.052557333  0.3926519 -0.3290621\n2     a   0.3599095   0.2102510   0.2633959 0.166443708 -0.4818807  0.6547023\n3     b   0.2271118   0.4993603   0.2519562 0.021571722  0.9745295 -0.3718462\n4     b   0.2935374   0.4823766   0.2211991 0.002886957  2.1880446  0.2069465\n5     a   0.4774623   0.3258104   0.1933571 0.003370286  2.0178238  1.1247359\n6     a   0.6678748   0.1810762   0.1482500 0.002799004  1.9458323  2.0931546\n       x.LD3\n1 -0.0704284\n2  0.1770694\n3 -0.9850425\n4  0.1364540\n5  0.2435680\n6  0.8309276\n```\n\n\n:::\n:::\n\n \n\nYou see that the columns that came from matrices have gained two-part names, the first part from the name of the matrix, the second part from the column name within that matrix. Then we can do this:\n\n::: {.cell}\n\n```{.r .cell-code}\ndd <- bind_cols(urine, as.data.frame(urine.pred))\ndd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 45 x 13\n   obesity     x creatinine chloride chlorine class posterior.a posterior.b\n * <chr>   <dbl>      <dbl>    <dbl>    <dbl> <fct>       <dbl>       <dbl>\n 1 a          24       17.6     5.15      7.5 b           0.233      0.412 \n 2 a          32       13.4     5.75      7.1 a           0.360      0.210 \n 3 a          17       20.3     4.35      2.3 b           0.227      0.499 \n 4 a          30       22.3     7.55      4   b           0.294      0.482 \n 5 a          30       20.5     8.5       2   a           0.477      0.326 \n 6 a          27       18.5    10.2       2   a           0.668      0.181 \n 7 a          25       12.1     5.95     16.8 c           0.167      0.208 \n 8 a          30       12       6.3      14.5 c           0.230      0.197 \n 9 a          28       10.1     5.45      0.9 a           0.481      0.0752\n10 a          24       14.7     3.75      2   a           0.323      0.247 \n# i 35 more rows\n# i 5 more variables: posterior.c <dbl>, posterior.d <dbl>, x.LD1 <dbl>,\n#   x.LD2 <dbl>, x.LD3 <dbl>\n```\n\n\n:::\n:::\n\n \n\nIf you want to avoid base R altogether, though, and go straight to\n`bind_cols`, you have to be more careful about the types of\nthings. `bind_cols` *only* works with vectors and data\nframes, not matrices, so that is what it is up to you to make sure you\nhave. That means pulling out the pieces, turning them from matrices\ninto data frames, and then gluing everything back together:\n\n::: {.cell}\n\n```{.r .cell-code}\npost <- as_tibble(urine.pred$posterior)\nld <- as_tibble(urine.pred$x)\nddd <- bind_cols(urine, class = urine.pred$class, ld, post)\nddd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 45 x 13\n   obesity     x creatinine chloride chlorine class    LD1     LD2     LD3     a\n   <chr>   <dbl>      <dbl>    <dbl>    <dbl> <fct>  <dbl>   <dbl>   <dbl> <dbl>\n 1 a          24       17.6     5.15      7.5 b      0.393 -0.329  -0.0704 0.233\n 2 a          32       13.4     5.75      7.1 a     -0.482  0.655   0.177  0.360\n 3 a          17       20.3     4.35      2.3 b      0.975 -0.372  -0.985  0.227\n 4 a          30       22.3     7.55      4   b      2.19   0.207   0.136  0.294\n 5 a          30       20.5     8.5       2   a      2.02   1.12    0.244  0.477\n 6 a          27       18.5    10.2       2   a      1.95   2.09    0.831  0.668\n 7 a          25       12.1     5.95     16.8 c     -0.962 -0.365   1.39   0.167\n 8 a          30       12       6.3      14.5 c     -0.853  0.0890  1.23   0.230\n 9 a          28       10.1     5.45      0.9 a     -1.23   1.95   -0.543  0.481\n10 a          24       14.7     3.75      2   a     -0.530  0.406  -1.06   0.323\n# i 35 more rows\n# i 3 more variables: b <dbl>, c <dbl>, d <dbl>\n```\n\n\n:::\n:::\n\n \nThat's a lot of work, but you might say that it's worth it because you\nare now absolutely sure what kind of thing everything is. I also had\nto be slightly careful with the vector of `class` values; in\n`ddd` it has to have a name, so I have to make sure I give it\none.^[If you run into an error like \"Argument 2 must have names\" here, that means that the second thing, `class`, needs  to have a name and doesn't have one.]\nAny of these ways (in general) is good. The last way is a more\ncareful approach, since you are making sure things are of the right\ntype rather than relying on R to convert them for you, but I don't\nmind which way you go.\nNow make the plot, making sure that you are using columns with the right names. I'm using my first data frame, with the two-part names:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = x.LD1, y = x.LD2, colour = obesity)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/urine2-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n \n$\\blacksquare$\n\n(e) <a name=\"part:plot\">*</a> Looking at your plot, discuss how (if at all) the\ndiscriminants separate the obesity groups. (Where does each\nobesity group fall on the plot?)\n \nSolution\n\n My immediate reaction was\n\"they don't much\". If you look a bit more closely, the\n`b` group, in green, is on the right (high\n`LD1`) and the `d` group (purple) is on the\nleft (low `LD1`). The `a` group, red, is\nmostly at the top (high `LD2`) but the `c`\ngroup, blue, really is all over the place.\n\nThe way to tackle interpreting a plot like this is to look\nfor each group individually and see if that group is only\nor mainly found on a certain part of the plot. \n\nThis can be rationalized by looking at \nthe \"coefficients of linear discriminants\" on the output. `LD1` is\nlow if creatinine and chloride are low (it has nothing\nmuch to do with `chlorine` since that coefficient\nis near zero). Group `d` is lowest on both\ncreatinine and chloride, so that will be lowest on\n`LD1`.  `LD2` is high if `chloride`\nis high, or `creatinine` and `chlorine` are\nlow. Out of the groups `a, b, c`, `a` has\nthe highest mean on chloride and lowest means on the other\ntwo variables, so this should be highest on `LD2`\nand (usually) is.\n\nLooking at the means is only part of the story; if the\nindividuals within a group are very variable, as they are\nhere (especially group `c`), then that group will\nappear all over the plot. The table of means only says how\nthe *average* individual within a group stacks up.\n\n::: {.cell}\n\n```{.r .cell-code}\nggbiplot(urine.1, groups = urine$obesity)\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/urine2-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThis shows (in a way that is perhaps easier to see) how the linear\ndiscriminants are related to the original variables, and thus how the\ngroups differ in terms of the original variables.^[This was why  we were doing discriminant analysis in the first place.] \nMost of the B's are high creatinine and high chloride (on the right); most of the D's are low on both (on the left). LD2 has a bit of `chloride`, but not much of anything else.\nExtra: the way we used to do this was with \"base graphics\", which involved plotting the `lda` output itself:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(urine.1)\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/urine2-11-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nwhich is a plot of each discriminant score against each other\none. You can plot just the first two, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(urine.1, dimen = 2)\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/urine2-12-1.pdf){fig-pos='H'}\n:::\n:::\n\n \nThis is easier than using `ggplot`, but (i) less flexible and\n(ii) you have to figure out how it works rather than doing things the\nstandard `ggplot` way. So I went with constructing a data frame\nfrom the predictions, and then\n`ggplot`ting that. It's a matter of taste which way is better.\n \n$\\blacksquare$\n\n(f) <a name=\"part:table\">*</a> Obtain a table showing observed and predicted obesity\ngroups. Comment on the accuracy of the predictions.\n \nSolution\n\n\nMake a table, one way or another:\n\n::: {.cell}\n\n```{.r .cell-code}\ntab <- with(d, table(obesity, class))\ntab\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       class\nobesity a b c d\n      a 7 3 2 0\n      b 2 9 2 1\n      c 3 4 1 3\n      d 2 0 1 5\n```\n\n\n:::\n:::\n\n   \n\n`class` is always the *predicted* group in these. You can\nalso name things in `table`.\nOr, if you prefer (equally good), the `tidyverse` way of\ncounting all the combinations of true `obesity` and predicted\n`class`, which can be done all in one go, or in\ntwo steps by saving the data frame first. I'm saving my results for\nlater:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% count(obesity, class) -> tab\ntab\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   obesity class n\n1        a     a 7\n2        a     b 3\n3        a     c 2\n4        b     a 2\n5        b     b 9\n6        b     c 2\n7        b     d 1\n8        c     a 3\n9        c     b 4\n10       c     c 1\n11       c     d 3\n12       d     a 2\n13       d     c 1\n14       d     d 5\n```\n\n\n:::\n:::\n\n \nor if you prefer to make it look more like a table of frequencies:\n\n::: {.cell}\n\n```{.r .cell-code}\ntab %>% pivot_wider(names_from=class, values_from=n, values_fill = list(n=0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 5\n  obesity     a     b     c     d\n  <chr>   <int> <int> <int> <int>\n1 a           7     3     2     0\n2 b           2     9     2     1\n3 c           3     4     1     3\n4 d           2     0     1     5\n```\n\n\n:::\n:::\n\n \n\nThe thing on the end fills in zero frequencies as such (they would\notherwise be `NA`, which they are not: we know they are zero).\nMy immediate reaction to this is \"it's terrible\"! But at least some\nof the men have their obesity group correctly predicted: 7 of the\n$7+3+2+0=12$ \nmen that are actually in group `a` are predicted to be in\n`a`; 9 of the 14 actual `b`'s are predicted to be\n`b`'s; 5 of the 8 actual `d`'s are predicted to be\n`d`'s. These are not so awful. But only 1 of the 11\n`c`'s is correctly predicted to be a `c`!\n\nAs for what I want to see: I am looking for some kind of statement\nabout how good you think the predictions are (the word \"terrible\" is\nfine for this) with some kind of support for your statement. For\nexample, \"the predictions are not that good, but at least group B is predicted with some accuracy (9 out of 14).\"\n\nI think looking at how well the individual groups were predicted is\nthe most incisive way of getting at this, because the `c` men\nare the hardest to get right and the others are easier, but you could\nalso think about an overall misclassification rate. This comes most\neasily from the \"tidy\" table:\n\n::: {.cell}\n\n```{.r .cell-code}\ntab %>% count(correct = (obesity == class), wt = n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  correct  n\n1   FALSE 23\n2    TRUE 22\n```\n\n\n:::\n:::\n\n \nYou can count anything, not just columns that already exist. This one\nis a kind of combined mutate-and-count to create the (logical) column\ncalled `correct`. \n\nIt's a shortcut for this:\n\n::: {.cell}\n\n```{.r .cell-code}\ntab %>%\n  mutate(is_correct = (obesity == class)) %>%\n  count(is_correct, wt = n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  is_correct  n\n1      FALSE 23\n2       TRUE 22\n```\n\n\n:::\n:::\n\n \nIf I don't put the `wt`, `count` counts the number of\n*rows* for which the true and predicted obesity group is the\nsame. But that's not what I want here: I want the number of\n*observations* totalled up, which is what the `wt=`\ndoes. It says \"use the things in the given column as weights\", which\nmeans to total them up rather than count up the number of rows.\n\nThis says that 22 men were classified correctly and 23 were gotten\nwrong. We can find the proportions correct and wrong:\n\n::: {.cell}\n\n```{.r .cell-code}\ntab %>%\n  count(correct = (obesity == class), wt = n) %>%\n  mutate(proportion = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  correct  n proportion\n1   FALSE 23  0.5111111\n2    TRUE 22  0.4888889\n```\n\n\n:::\n:::\n\n \n\nand we see that 51\\% of men had their obesity group predicted\nwrongly. This is the overall misclassification rate, which is a simple\nsummary of how good a job the discriminant analysis did.\n\nThere is a subtlety here. `n` has changed its meaning in the\nmiddle of this calculation! In `tab`, `n` is counting\nthe number of obesity observed and predicted combinations, but now it\nis counting the number of men classified correctly and\nincorrectly. The `wt=n` uses the first `n`, but the\n`mutate` line uses the *new* `n`, the result of the\n`count` line here. (I think `count` used to use\n`nn` for the result of the second `count`, so that you\ncould tell them apart, but it no longer seems to do so.)\n\nI said above that the obesity groups were not equally easy to\npredict. A small modification of the above will get the\nmisclassification rates by (true) obesity group. This is done by\nputting an appropriate `group_by` in at the front, before we\ndo any summarizing:\n\n::: {.cell}\n\n```{.r .cell-code}\ntab %>%\n  group_by(obesity) %>%\n  count(correct = (obesity == class), wt = n) %>%\n  mutate(proportion = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 4\n# Groups:   obesity [4]\n  obesity correct     n proportion\n  <chr>   <lgl>   <int>      <dbl>\n1 a       FALSE       5     0.417 \n2 a       TRUE        7     0.583 \n3 b       FALSE       5     0.357 \n4 b       TRUE        9     0.643 \n5 c       FALSE      10     0.909 \n6 c       TRUE        1     0.0909\n7 d       FALSE       3     0.375 \n8 d       TRUE        5     0.625 \n```\n\n\n:::\n:::\n\n \n\nThis gives the proportion wrong and correct for each (true) obesity\ngroup. I'm going to do the one more cosmetic thing to make it easier to\nread, a kind of \"untidying\":\n\n::: {.cell}\n\n```{.r .cell-code}\ntab %>%\n  group_by(obesity) %>%\n  count(correct = (obesity == class), wt = n) %>%\n  mutate(proportion = n / sum(n)) %>%\n  select(-n) %>%\n  pivot_wider(names_from=correct, values_from=proportion)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 3\n# Groups:   obesity [4]\n  obesity `FALSE` `TRUE`\n  <chr>     <dbl>  <dbl>\n1 a         0.417 0.583 \n2 b         0.357 0.643 \n3 c         0.909 0.0909\n4 d         0.375 0.625 \n```\n\n\n:::\n:::\n\n \n\nLooking down the ``TRUE`` column, groups A, B and D were gotten\nabout 60\\% correct (and 40\\% wrong), but group C is much worse. The\noverall misclassification rate is made bigger by the fact that C is so\nhard to predict.\n\nFind out for yourself what happens if I fail to remove the `n`\ncolumn before doing the `pivot_wider`.\n\nA slightly more elegant look is obtained this way, by making nicer\nvalues than TRUE and FALSE:\n\n::: {.cell}\n\n```{.r .cell-code}\ntab %>%\n  group_by(obesity) %>%\n  mutate(prediction_stat = ifelse(obesity == class, \"correct\", \"wrong\")) %>%\n  count(prediction_stat, wt = n) %>%\n  mutate(proportion = n / sum(n)) %>%\n  select(-n) %>%\n  pivot_wider(names_from=prediction_stat, values_from=proportion)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 3\n# Groups:   obesity [4]\n  obesity correct wrong\n  <chr>     <dbl> <dbl>\n1 a        0.583  0.417\n2 b        0.643  0.357\n3 c        0.0909 0.909\n4 d        0.625  0.375\n```\n\n\n:::\n:::\n\n \n \n$\\blacksquare$\n\n(g) Do your conclusions from (<a href=\"#part:plot\">here</a>) and\n(<a href=\"#part:table\">here</a>) appear to be consistent?\n \nSolution\n\n\nOn the plot of (<a href=\"#part:plot\">here</a>), we said that there was a\nlot of scatter, but that groups `a`, `b` and\n`d` tended to be found at the top, right and left\nrespectively of the plot. That suggests that these three\ngroups should be somewhat predictable. The `c`'s, on\nthe other hand, were all over the place on the plot, and\nwere mostly predicted wrong.\n\nThe idea is that the stories you pull from the plot and the\npredictions should be more or less consistent. There are\nseveral ways you might say that: another approach is to say\nthat the observations are all over the place on the plot,\nand the predictions are all bad. This is not as insightful\nas my comments above, but if that's what the plot told you,\nthat's what the predictions would seem to be saying as\nwell. (Or even, the predictions are not so bad compared to\nthe apparently random pattern on the plot, if that's what\nyou saw. There are different ways to say something more or\nless sensible.)\n \n$\\blacksquare$\n\n\n\n\n\n##  Understanding a MANOVA\n\n\n One use of discriminant analysis is to\nunderstand the results of a MANOVA. This question is a followup to a\nprevious MANOVA that we did, the one with two variables `y1`\nand `y2` and three groups `a` through `c`. The\ndata were in [link](http://ritsokiguess.site/datafiles/simple-manova.txt).\n\n\n\n(a) Read the data in again and run the MANOVA that you did\nbefore. \n\n\nSolution\n\n\nThis is an exact repeat of what you did before:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/simple-manova.txt\"\n# my_url <- \"https://ritsokiguess.site/datafiles/simple-manova.txt\"\nsimple <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 12 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (1): group\ndbl (2): y1, y2\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nsimple\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 x 3\n   group    y1    y2\n   <chr> <dbl> <dbl>\n 1 a         2     3\n 2 a         3     4\n 3 a         5     4\n 4 a         2     5\n 5 b         4     8\n 6 b         5     6\n 7 b         5     7\n 8 c         7     6\n 9 c         8     7\n10 c        10     8\n11 c         9     5\n12 c         7     6\n```\n\n\n:::\n\n```{.r .cell-code}\nresponse <- with(simple, cbind(y1, y2))\nsimple.3 <- manova(response ~ group, data = simple)\nsummary(simple.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Df Pillai approx F num Df den Df    Pr(>F)    \ngroup      2 1.3534   9.4196      4     18 0.0002735 ***\nResiduals  9                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n     \n\nThis P-value is small, so there is some way in which some of the\ngroups differ on some of the variables.^[That sounds like the  ultimate in evasiveness!]\n\nWe should check that we believe this, using Box's M test:^[Every time I do this, I forget to put the `summary` around the outside, so I get the long ugly version of the output rather than the short pretty version.]\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(BoxM(response, simple$group))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Box's M Test \n\nChi-Squared Value = 3.517357 , df = 6  and p-value: 0.742 \n```\n\n\n:::\n:::\n\nThere is no problem here: no evidence that any of the response variables differ in spread across the groups.\n    \n$\\blacksquare$\n\n(b) Run a discriminant analysis \"predicting\" group from the\ntwo response variables. Display the output.\n\n\nSolution\n\n\nThis:\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple.4 <- lda(group ~ y1 + y2, data = simple)\nsimple.4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(group ~ y1 + y2, data = simple)\n\nPrior probabilities of groups:\n        a         b         c \n0.3333333 0.2500000 0.4166667 \n\nGroup means:\n        y1  y2\na 3.000000 4.0\nb 4.666667 7.0\nc 8.200000 6.4\n\nCoefficients of linear discriminants:\n         LD1        LD2\ny1 0.7193766  0.4060972\ny2 0.3611104 -0.9319337\n\nProportion of trace:\n   LD1    LD2 \n0.8331 0.1669 \n```\n\n\n:::\n:::\n\n   \nNote that this is the other way around from MANOVA: here, we are\n\"predicting the group\" from the response variables, in the same\nmanner as one of the flavours of logistic regression: \n\"what makes the groups different, in terms of those response variables?\".\n\n    \n$\\blacksquare$\n\n(c) <a name=\"part:output\">*</a> In the output from the discriminant analysis,\nwhy are there exactly two linear discriminants `LD1` and\n`LD2`?\n\n\n\nSolution\n\n\nThere are two linear discriminants because there are 3 groups and two\nvariables, so there are the smaller of $3-1$ and 2 discriminants.\n\n  \n$\\blacksquare$\n\n(d) <a name=\"part:svd\">*</a> From the output, how would you say that the\nfirst linear discriminant `LD1` compares in importance to the\nsecond one `LD2`: much more important, more important, equally\nimportant, less important, much less important? Explain briefly. \n\n\n\nSolution\n\n\nLook at the `Proportion of trace` at the bottom of the output.\nThe first number is much bigger than the second, so the first linear\ndiscriminant is much more important than the second. (I care about\nyour reason; you can say it's \"more important\" rather than \n\"much more important\" and I'm good with that.) \n  \n$\\blacksquare$\n\n(e) Obtain a plot of the\ndiscriminant scores.\n\n\n\nSolution\n\n\nThis was the old-fashioned way:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(simple.4)\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/manova1a-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n   \n\nIt needs cajoling to produce colours, but we can do better. The first\nthing is to obtain the predictions:\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple.pred <- predict(simple.4)\n```\n:::\n\n \n\nThen we make a data frame out of the discriminant scores and the true\ngroups, using `cbind`:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- cbind(simple, simple.pred)\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  group y1 y2 class posterior.a  posterior.b  posterior.c      x.LD1      x.LD2\n1     a  2  3     a 0.999836110 0.0001636933 1.964310e-07 -3.5708196  1.1076359\n2     a  3  4     a 0.994129686 0.0058400248 3.028912e-05 -2.4903326  0.5817994\n3     a  5  4     a 0.953416498 0.0267238544 1.985965e-02 -1.0515795  1.3939939\n4     a  2  5     a 0.957685668 0.0423077129 6.618865e-06 -2.8485988 -0.7562315\n5     b  4  8     b 0.001068057 0.9978789644 1.052978e-03 -0.3265145 -2.7398380\n6     b  5  6     b 0.107572389 0.8136017106 7.882590e-02 -0.3293587 -0.4698735\n```\n\n\n:::\n:::\n\n \nor like this, for fun:^[For suitable definitions of fun.]\n\n::: {.cell}\n\n```{.r .cell-code}\nld <- as_tibble(simple.pred$x)\npost <- as_tibble(simple.pred$posterior)\ndd <- bind_cols(simple, class = simple.pred$class, ld, post)\ndd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 x 9\n   group    y1    y2 class     LD1    LD2             a        b           c\n   <chr> <dbl> <dbl> <fct>   <dbl>  <dbl>         <dbl>    <dbl>       <dbl>\n 1 a         2     3 a     -3.57    1.11  1.00          0.000164 0.000000196\n 2 a         3     4 a     -2.49    0.582 0.994         0.00584  0.0000303  \n 3 a         5     4 a     -1.05    1.39  0.953         0.0267   0.0199     \n 4 a         2     5 a     -2.85   -0.756 0.958         0.0423   0.00000662 \n 5 b         4     8 b     -0.327  -2.74  0.00107       0.998    0.00105    \n 6 b         5     6 b     -0.329  -0.470 0.108         0.814    0.0788     \n 7 b         5     7 b      0.0318 -1.40  0.00772       0.959    0.0335     \n 8 c         7     6 c      1.11    0.342 0.00186       0.0671   0.931      \n 9 c         8     7 c      2.19   -0.184 0.0000127     0.0164   0.984      \n10 c        10     8 c      3.99   -0.303 0.00000000317 0.000322 1.00       \n11 c         9     5 c      2.19    2.09  0.0000173     0.000181 1.00       \n12 c         7     6 c      1.11    0.342 0.00186       0.0671   0.931      \n```\n\n\n:::\n:::\n\n \nAfter that, we plot the first one against the second one, colouring by\ntrue groups:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = x.LD1, y = x.LD2, colour = group)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/manova1a-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nI wanted to compare this plot with the original plot of `y1`\nvs.\\ `y2`, coloured by groups:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(simple, aes(x = y1, y = y2, colour = group)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/manova1a-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThe difference between this plot and the one of `LD1` vs.\\\n`LD2` is that things have been rotated a bit so that most of\nthe separation of groups is done by `LD1`. This is reflected in\nthe fact that `LD1` is quite a bit more important than\n`LD2`: the latter doesn't help much in separating the groups.\n\nWith that in mind, we could also plot just `LD1`, presumably\nagainst groups via boxplot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = group, y = x.LD1)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/manova1a-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThis shows that LD1 does a pretty fine job of separating the groups,\nand `LD2` doesn't really have much to add to the picture. \n  \n$\\blacksquare$\n\n(f) Describe briefly how `LD1` and/or `LD2`\nseparate the groups. Does your picture confirm the relative importance\nof `LD1` and `LD2` that you found  back in part (<a href=\"#part:svd\">here</a>)? Explain briefly.\n\n\n\nSolution\n\n\n`LD1` separates the groups left to right: group `a` is\nlow on `LD1`, `b` is in the middle and `c` is\nhigh on `LD1`. (There is no intermingling of the groups on\n`LD1`, so it separates the groups perfectly.)\n\nAs for `LD2`, all it does (possibly) is to distinguish\n`b` (low) from `a` and `c` (high). Or you can,\njust as reasonably, take the view that it doesn't really separate\nany of the groups.\n\nBack in part (<a href=\"#part:svd\">here</a>), you said (I hope) that `LD1`\nwas (very) important compared to `LD2`. This shows up here in\nthat `LD1` does a very good job of distinguishing the groups,\nwhile `LD2` does a poor to non-existent job of separating any\ngroups. (If you didn't\nsay that before, here is an invitation to reconsider what you\n*did* say there.)\n  \n$\\blacksquare$\n\n(g) What makes group `a` have a low score on `LD1`?\nThere are two steps that you need to make: consider the means of group\n`a` on variables `y1` and `y2` and how they\ncompare to the other groups, and consider how\n`y1` and `y2` play into the score on `LD1`.\n\n\n\nSolution\n\n\nThe information you need is in the big output.\n\nThe means of `y1` and `y2` for group `a` are 3\nand 4 respectively, which are the lowest of all the groups. That's\nthe first thing. \n\nThe second thing is the coefficients of\n`LD1` in terms of `y1` and `y2`, which are both\n*positive*. That means, for any observation, if its `y1`\nand `y2` values are *large*, that observation's score on\n`LD1` will be large as well. Conversely, if its values are\n*small*, as the ones in group `a` are, its score on\n`LD1` will be small. \n\nYou need these two things.\n\nThis explains why the group `a` observations are on the left\nof the plot. It also explains why the group `c` observations\nare on the right: they are *large* on both `y1` and\n`y2`, and so large on `LD1`.\n\nWhat about `LD2`? This is a little more confusing (and thus I\ndidn't ask you about that). Its \"coefficients of linear discriminant\" \nare positive on `y1` and negative on\n`y2`, with the latter being bigger in size. Group `b`\nis about average on `y1` and distinctly *high* on\n`y2`; the second of these coupled with the negative\ncoefficient on `y2` means that the `LD2` score for\nobservations in group `b` will be *negative*.\n\nFor `LD2`, group `a` has a low mean on both variables\nand group `c` has a high mean, so for both groups there is a\nkind of cancelling-out happening, and neither group `a` nor\ngroup `c` will be especially remarkable on `LD2`.\n  \n$\\blacksquare$\n\n(h) Obtain predictions for the group memberships of each\nobservation, and make a table of the actual group memberships against\nthe predicted ones. How many of the observations were wrongly classified?\n\n\n\nSolution\n\n\nUse the\n`simple.pred` that you got earlier. This is the\n`table` way:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(d, table(obs = group, pred = class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   pred\nobs a b c\n  a 4 0 0\n  b 0 3 0\n  c 0 0 5\n```\n\n\n:::\n:::\n\n   \nEvery single one of the 12 observations has been classified into its\ncorrect group. (There is nothing off the diagonal of this table.) \nThe alternative to `table` is the `tidyverse` way:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% count(group, class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  group class n\n1     a     a 4\n2     b     b 3\n3     c     c 5\n```\n\n\n:::\n:::\n\n \n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(group, class) %>%\n  pivot_wider(names_from=class, values_from=n, values_fill = list(n=0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 4\n  group     a     b     c\n  <chr> <int> <int> <int>\n1 a         4     0     0\n2 b         0     3     0\n3 c         0     0     5\n```\n\n\n:::\n:::\n\n \n\nif you want something that looks like a frequency table.\nAll the `a`s got classified as `a`, and so on. \nThat's the end of what I asked you to do, but as ever I wanted to\npress on. The next question to ask after getting the predicted groups\nis \"what are the posterior probabilities of being in each group for each observation\": \nthat is, not just which group do I think it\nbelongs in, but how sure am I about that call? The posterior\nprobabilities in my `d` start with `posterior`. These\nhave a ton of decimal places which I like to round off first before I\ndisplay them, eg. to 3 decimals here:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  select(y1, y2, group, class, starts_with(\"posterior\")) %>%\n  mutate(across(starts_with(\"posterior\"), \\(post) round(post, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   y1 y2 group class posterior.a posterior.b posterior.c\n1   2  3     a     a       1.000       0.000       0.000\n2   3  4     a     a       0.994       0.006       0.000\n3   5  4     a     a       0.953       0.027       0.020\n4   2  5     a     a       0.958       0.042       0.000\n5   4  8     b     b       0.001       0.998       0.001\n6   5  6     b     b       0.108       0.814       0.079\n7   5  7     b     b       0.008       0.959       0.034\n8   7  6     c     c       0.002       0.067       0.931\n9   8  7     c     c       0.000       0.016       0.984\n10 10  8     c     c       0.000       0.000       1.000\n11  9  5     c     c       0.000       0.000       1.000\n12  7  6     c     c       0.002       0.067       0.931\n```\n\n\n:::\n:::\n\n \n\n\nYou see that the posterior probability of an observation being in the\ngroup it actually *was* in is close to 1 all the way down. The\nonly one with any doubt at all is observation \\#6, which is actually\nin group `b`, but has \"only\" probability 0.814 of being a\n`b` based on its `y1` and `y2` values. What else\ncould it be? Well, it's about equally split between being `a`\nand `c`. Let me see if I can display this observation on the\nplot in a different way. First I need to make a new column picking out\nobservation 6, and then I use this new variable as the `shape`\nof the point I plot:\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple %>%\n  mutate(is6 = (row_number() == 6)) %>%\n  ggplot(aes(x = y1, y = y2, colour = group, shape = is6)) +\n  geom_point(size = 3)\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/manova1a-14-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThat makes it stand out a bit: if you look carefully, one of the green points (observation 6) is plotted as a triangle rather than a circle, as the legend for `is6` indicates.\n(I plotted all the points bigger to make this easier to see.)\n\nSince observation \\#6 is in group `b`, it appears as a green triangle. What makes it least like a `b`? \nWell, it has the smallest `y2` value of any of the `b`'s (which makes it most like an `a` of any of the `b`'s), and it has the largest `y1` value (which makes it most like a `c` of any of the `b`'s). \nBut still, it's nearer the greens than anything else, so it's still more like a `b` than it is like any of the other groups. \n  \n$\\blacksquare$\n\n\n\n\n\n\n\n\n\n\n## What distinguishes people who do different jobs?\n\n244 people work at a certain company. They each have one of\nthree jobs: customer service, mechanic, dispatcher. In the data set,\nthese are labelled 1, 2 and 3 respectively. In addition, they each are\nrated on scales called `outdoor`, `social` and `conservative`. Do people\nwith different jobs tend to have different scores on these scales, or,\nto put it another way, if you knew a person's scores on `outdoor`,\n`social` and `conservative`, could you say something about what kind of\njob they were likely to hold? The data are in\n[link](http://ritsokiguess.site/datafiles/jobs.txt).\n\n\n(a) Read in the data and display some of it.\n\nSolution\n\nThe usual. This one is aligned columns. I'm using a \"temporary\" name for\nmy read-in data frame, since I'm going to create the proper one in a\nmoment.\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/jobs.txt\"\njobs0 <- read_table(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Missing column names filled in: 'X6' [6]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n-- Column specification --------------------------------------------------------\ncols(\n  outdoor = col_double(),\n  social = col_double(),\n  conservative = col_double(),\n  job = col_double(),\n  id = col_double(),\n  X6 = col_character()\n)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: 244 parsing failures.\nrow col  expected    actual                                          file\n  1  -- 6 columns 5 columns 'http://ritsokiguess.site/datafiles/jobs.txt'\n  2  -- 6 columns 5 columns 'http://ritsokiguess.site/datafiles/jobs.txt'\n  3  -- 6 columns 5 columns 'http://ritsokiguess.site/datafiles/jobs.txt'\n  4  -- 6 columns 5 columns 'http://ritsokiguess.site/datafiles/jobs.txt'\n  5  -- 6 columns 5 columns 'http://ritsokiguess.site/datafiles/jobs.txt'\n... ... ......... ......... .............................................\nSee problems(...) for more details.\n```\n\n\n:::\n\n```{.r .cell-code}\njobs0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 244 x 6\n   outdoor social conservative   job    id X6   \n     <dbl>  <dbl>        <dbl> <dbl> <dbl> <chr>\n 1      10     22            5     1     1 <NA> \n 2      14     17            6     1     2 <NA> \n 3      19     33            7     1     3 <NA> \n 4      14     29           12     1     4 <NA> \n 5      14     25            7     1     5 <NA> \n 6      20     25           12     1     6 <NA> \n 7       6     18            4     1     7 <NA> \n 8      13     27            7     1     8 <NA> \n 9      18     31            9     1     9 <NA> \n10      16     35           13     1    10 <NA> \n# i 234 more rows\n```\n\n\n:::\n:::\n\nWe got all that was promised, plus a label `id` for each employee, which\nwe will from here on ignore.[^_jobs-2]\n\n[^_jobs-2]: Until much later.\n\n$\\blacksquare$\n\n(b) Note the types of each of the variables, and create any new\n    variables that you need to.\n\nSolution\n\nThese are all `int` or whole numbers. But, the job ought to be a\n`factor`: the labels 1, 2 and 3 have no meaning as such, they just label\nthe three different jobs. (I gave you a hint of this above.) So we need\nto turn `job` into a factor. I think the best way to do that is via\n`mutate`, and then we save the new data frame into one called `jobs`\nthat we actually use for the analysis below:\n\n::: {.cell}\n\n```{.r .cell-code}\njob_labels <- c(\"custserv\", \"mechanic\", \"dispatcher\")\njobs0 %>%\n  mutate(job = factor(job, labels = job_labels)) -> jobs\n```\n:::\n\nI lived on the edge and saved my factor `job` into a variable with the\nsame name as the numeric one. I should check that I now have the right\nthing:\n\n::: {.cell}\n\n```{.r .cell-code}\njobs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 244 x 6\n   outdoor social conservative job         id X6   \n     <dbl>  <dbl>        <dbl> <fct>    <dbl> <chr>\n 1      10     22            5 custserv     1 <NA> \n 2      14     17            6 custserv     2 <NA> \n 3      19     33            7 custserv     3 <NA> \n 4      14     29           12 custserv     4 <NA> \n 5      14     25            7 custserv     5 <NA> \n 6      20     25           12 custserv     6 <NA> \n 7       6     18            4 custserv     7 <NA> \n 8      13     27            7 custserv     8 <NA> \n 9      18     31            9 custserv     9 <NA> \n10      16     35           13 custserv    10 <NA> \n# i 234 more rows\n```\n\n\n:::\n:::\n\nI like this better because you see the actual factor levels rather than\nthe underlying numeric values by which they are stored.\n\nAll is good here. If you forget the `labels` thing, you'll get a factor,\nbut its levels will be 1, 2, and 3, and you will have to remember which\njobs they go with. I'm a fan of giving factors named levels, so that you\ncan remember what stands for what.[^_jobs-3]\n\n[^_jobs-3]: When you're *recording* the data, you may find it convenient\n    to use short codes to represent the possibly long factor levels, but\n    in that case you should also use a\n    [**codebook**](https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html)\n    so that you know what the codes represent. When I read the data into\n    R, I would create a factor with named levels, like I did here, if I\n    don't already have one.\n\nExtra: another way of doing this is to make a lookup table, that is, a\nlittle table that shows which job goes with which number:\n\n::: {.cell}\n\n```{.r .cell-code}\nlookup_tab <- tribble(\n  ~job, ~jobname,\n  1, \"custserv\",\n  2, \"mechanic\",\n  3, \"dispatcher\"\n)\nlookup_tab\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 2\n    job jobname   \n  <dbl> <chr>     \n1     1 custserv  \n2     2 mechanic  \n3     3 dispatcher\n```\n\n\n:::\n:::\n\nI carefully put the numbers in a column called `job` because I want to\nmatch these with the column called `job` in `jobs0`:\n\n::: {.cell}\n\n```{.r .cell-code}\njobs0 %>%\n  left_join(lookup_tab) -> jobs\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(job)`\n```\n\n\n:::\n\n```{.r .cell-code}\njobs %>% slice_sample(n = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 x 7\n   outdoor social conservative   job    id X6    jobname   \n     <dbl>  <dbl>        <dbl> <dbl> <dbl> <chr> <chr>     \n 1       9     24            6     1    63 <NA>  custserv  \n 2       8     29           14     1    20 <NA>  custserv  \n 3      17     20            6     1    81 <NA>  custserv  \n 4      28     16           10     2    23 <NA>  mechanic  \n 5      10     31           13     1    15 <NA>  custserv  \n 6      18     21           13     2     7 <NA>  mechanic  \n 7      15     22           13     2    59 <NA>  mechanic  \n 8      20     27            6     2     1 <NA>  mechanic  \n 9      14     25           15     1    78 <NA>  custserv  \n10      19     23           12     2    55 <NA>  mechanic  \n11      13     21           14     1    34 <NA>  custserv  \n12      16     28           13     2    11 <NA>  mechanic  \n13      19     23           12     2    26 <NA>  mechanic  \n14      19     13            7     2    33 <NA>  mechanic  \n15      14     26           15     3    45 <NA>  dispatcher\n16      14     19           14     2    39 <NA>  mechanic  \n17      14     24            7     2    17 <NA>  mechanic  \n18      14     16            7     3    41 <NA>  dispatcher\n19      20     13           19     3    36 <NA>  dispatcher\n20      22     12            8     2    27 <NA>  mechanic  \n```\n\n\n:::\n:::\n\nYou see that each row has the *name* of the job that employee has, in\nthe column `jobname`, because the job `id` was looked up in our lookup\ntable. (I displayed some random rows so you could see that it worked.)\n\n$\\blacksquare$\n\n(c) Run a multivariate analysis of variance to convince yourself that\n    there are some differences in scale scores among the jobs.\n\nSolution\n\nYou know how to do this, right? This one is the easy way:\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(jobs, cbind(social, outdoor, conservative))\nresponse.1 <- manova(response ~ jobname, data = jobs)\nsummary(response.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Df  Pillai approx F num Df den Df    Pr(>F)    \njobname     2 0.76207   49.248      6    480 < 2.2e-16 ***\nResiduals 241                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nOr you can use `Manova`. That is mostly for practice here, since there\nis no reason to make things difficult for yourself:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nresponse.2 <- lm(response ~ job, data = jobs)\nsummary(Manova(response.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nType II MANOVA Tests:\n\nSum of squares and products for error:\n                social   outdoor conservative\nsocial       4503.0829  669.0553     161.5635\noutdoor       669.0553 5236.1506    -190.3804\nconservative  161.5635 -190.3804    2739.6832\n\n------------------------------------------\n \nTerm: job \n\nSum of squares and products for the hypothesis:\n                social    outdoor conservative\nsocial        2792.339 -1128.5471   -1331.9406\noutdoor      -1128.547   456.1116     538.3148\nconservative -1331.941   538.3148     635.3331\n\nMultivariate Tests: job\n                 Df test stat approx F num Df den Df     Pr(>F)    \nPillai            1  0.519350 86.44129      3    240 < 2.22e-16 ***\nWilks             1  0.480650 86.44129      3    240 < 2.22e-16 ***\nHotelling-Lawley  1  1.080516 86.44129      3    240 < 2.22e-16 ***\nRoy               1  1.080516 86.44129      3    240 < 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\nThis version gives the four different versions of the test (rather than\njust the Pillai test that `manova` gives), but the results are in this\ncase identical for all of them.\n\nSo: oh yes, there are differences (on some or all of the variables, for\nsome or all of the groups). So we need something like discriminant\nanalysis to understand the differences.\n\nWe really ought to follow this up with Box's M test, to be sure that the\nvariances and correlations for each variable are equal enough across the\ngroups, but we note off the top that the P-values (all of them) are\nreally small, so there ought not to be much doubt about the conclusion\nanyway:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(BoxM(response, jobs$job))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Box's M Test \n\nChi-Squared Value = 25.64176 , df = 12  and p-value: 0.0121 \n```\n\n\n:::\n:::\n\nThis is small, but not (for this test) small enough to worry about (it's\nnot less than 0.001).\n\nThis, and the `lda` below, actually works perfectly well if you use the\noriginal (integer) job, but then you have to remember which job number\nis which.\n\n$\\blacksquare$\n\n(d) Run a discriminant analysis and display the output.\n\nSolution\n\nNow `jobname` is the \"response\":\n\n::: {.cell}\n\n```{.r .cell-code}\njob.1 <- lda(jobname ~ social + outdoor + conservative, data = jobs)\njob.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(jobname ~ social + outdoor + conservative, data = jobs)\n\nPrior probabilities of groups:\n  custserv dispatcher   mechanic \n 0.3483607  0.2704918  0.3811475 \n\nGroup means:\n             social  outdoor conservative\ncustserv   24.22353 12.51765     9.023529\ndispatcher 15.45455 15.57576    13.242424\nmechanic   21.13978 18.53763    10.139785\n\nCoefficients of linear discriminants:\n                     LD1         LD2\nsocial        0.19427415 -0.04978105\noutdoor      -0.09198065 -0.22501431\nconservative -0.15499199  0.08734288\n\nProportion of trace:\n   LD1    LD2 \n0.7712 0.2288 \n```\n\n\n:::\n:::\n\n$\\blacksquare$\n\n(e) Which is the more important, `LD1` or `LD2`? How much more\n    important? Justify your answer briefly.\n\nSolution\n\nLook at the \"proportion of trace\" at the bottom. The value for `LD1` is\nquite a bit higher, so `LD1` is quite a bit more important when it comes\nto separating the groups. `LD2` is, as I said, less important, but is\nnot completely worthless, so it will be worth taking a look at it.\n\n$\\blacksquare$\n\n(f) Describe what values for an individual on the scales will make each\n    of `LD1` and `LD2` high.\n\nSolution\n\nThis is a two-parter: decide whether each scale makes a positive,\nnegative or zero contribution to the linear discriminant (looking at the\n\"coefficients of linear discriminants\"), and then translate that into\nwhat would make each `LD` high. Let's start with `LD1`:\n\nIts coefficients on the three scales are respectively negative\n($-0.19$), zero (0.09; my call) and positive (0.15). Where you draw the\nline is up to you: if you want to say that `outdoor`'s contribution is\npositive, go ahead. This means that `LD1` will be high if `social` is\n*low* and if `conservative` is *high*. (If you thought that `outdoor`'s\ncoefficient was positive rather than zero, if `outdoor` is high as\nwell.)\n\nNow for `LD2`: I'm going to call `outdoor`'s coefficient of $-0.22$\nnegative and the other two zero, so that `LD2` is high if `outdoor` is\n*low*. Again, if you made a different judgement call, adapt your answer\naccordingly.\n\n$\\blacksquare$\n\n(g) The first group of employees, customer service, have the highest\n    mean on `social` and the lowest mean on both of the other two\n    scales. Would you expect the customer service employees to score\n    high or low on `LD1`? What about `LD2`?\n\nSolution\n\nIn the light of what we said in the previous part, the customer service\nemployees, who are high on `social` and low on `conservative`, should be\n*low* (negative) on `LD1`, since both of these means are pointing that\nway. As I called it, the only thing that matters to `LD2` is `outdoor`,\nwhich is *low* for the customer service employees, and thus `LD2` for\nthem will be *high* (negative coefficient).\n\n$\\blacksquare$\n\n(h) Plot your discriminant scores (which you will have to obtain first),\n    and see if you were right about the customer service employees in\n    terms of `LD1` and `LD2`. The job names are rather long, and there\n    are a lot of individuals, so it is probably best to plot the scores\n    as coloured circles with a legend saying which colour goes with\n    which job (rather than labelling each individual with the job they\n    have).\n\nSolution\n\nPredictions first, then make a data frame combining the predictions with\nthe original data:\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(job.1)\nas.data.frame(p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         class posterior.custserv posterior.dispatcher posterior.mechanic\n1     custserv       9.037622e-01         7.289988e-03       0.0889478485\n2     mechanic       3.677743e-01         1.432468e-01       0.4889789008\n3     custserv       7.302117e-01         3.186265e-04       0.2694697105\n4     custserv       8.100756e-01         7.751215e-03       0.1821731894\n5     custserv       7.677607e-01         7.185490e-03       0.2250538225\n6     mechanic       1.682521e-01         4.692305e-02       0.7848248752\n7     custserv       9.408328e-01         1.670509e-02       0.0424620706\n8     custserv       8.790086e-01         2.349121e-03       0.1186423050\n9     custserv       6.767464e-01         1.551373e-03       0.3217022453\n10    custserv       8.643564e-01         8.641095e-04       0.1347795344\n11    custserv       4.950388e-01         1.348603e-02       0.4914751264\n12    custserv       9.537446e-01         2.525084e-03       0.0437303474\n13    custserv       5.240823e-01         9.388337e-03       0.4665293631\n14    custserv       6.819795e-01         1.573579e-01       0.1606625929\n15    custserv       9.613543e-01         2.086923e-03       0.0365588154\n16    mechanic       4.887584e-01         4.700656e-03       0.5065409138\n17    custserv       9.974534e-01         8.554731e-04       0.0016911727\n18    custserv       8.370707e-01         4.502895e-02       0.1179003525\n19    custserv       5.004012e-01         5.768116e-02       0.4419175871\n20    custserv       9.649054e-01         5.828045e-03       0.0292665325\n21    custserv       9.815677e-01         3.084262e-03       0.0153480620\n22    custserv       5.697748e-01         2.159598e-01       0.2142654531\n23    custserv       9.991982e-01         2.761694e-05       0.0007741549\n24    custserv       8.837774e-01         1.039703e-03       0.1151828840\n25    custserv       4.938044e-01         1.583747e-01       0.3478209184\n26    mechanic       3.564096e-01         3.616675e-03       0.6399736784\n27    custserv       8.693126e-01         7.267402e-03       0.1234199965\n28    custserv       7.215912e-01         4.799088e-02       0.2304179226\n29    custserv       9.883365e-01         1.319044e-03       0.0103444748\n30    custserv       9.359042e-01         2.676529e-03       0.0614193101\n31    custserv       8.018758e-01         1.744651e-01       0.0236590252\n32    custserv       9.618798e-01         1.296402e-02       0.0251561754\n33    custserv       9.129230e-01         5.565968e-03       0.0815110137\n34  dispatcher       3.220516e-01         3.632646e-01       0.3146838699\n35    custserv       8.909918e-01         5.313327e-03       0.1036949094\n36    custserv       7.734984e-01         1.132376e-01       0.1132639919\n37    custserv       7.928969e-01         1.548856e-01       0.0522175881\n38    custserv       7.152302e-01         8.860092e-02       0.1961688726\n39    custserv       4.830847e-01         1.145445e-01       0.4023708150\n40    mechanic       9.523627e-02         1.568069e-02       0.8890830359\n41    custserv       7.679833e-01         1.538158e-03       0.2304785704\n42    custserv       8.582757e-01         7.923928e-03       0.1338004111\n43    custserv       8.727131e-01         5.298216e-03       0.1219886414\n44    custserv       5.832550e-01         1.533108e-02       0.4014139446\n45    custserv       9.432314e-01         1.480633e-03       0.0552880094\n46    custserv       6.748657e-01         1.019602e-01       0.2231741012\n47    mechanic       2.955104e-01         1.781956e-01       0.5262939623\n48    custserv       6.496219e-01         1.267779e-02       0.3377002656\n49    mechanic       3.774214e-01         1.227084e-01       0.4998701545\n50    custserv       5.755170e-01         6.138769e-03       0.4183442282\n51    custserv       8.603505e-01         1.363857e-02       0.1260109570\n52    custserv       9.764155e-01         3.449384e-03       0.0201350730\n53  dispatcher       1.013578e-01         5.274616e-01       0.3711805789\n54    mechanic       1.373009e-01         8.015011e-03       0.8546840791\n55    mechanic       1.302936e-01         1.986045e-02       0.8498459777\n56    custserv       6.305186e-01         2.427472e-03       0.3670539517\n57    mechanic       3.052008e-01         7.175295e-02       0.6230462861\n58    custserv       8.650641e-01         4.211839e-03       0.1307240754\n59    custserv       6.014337e-01         5.053717e-03       0.3935126078\n60    custserv       6.910126e-01         1.105727e-02       0.2979301609\n61  dispatcher       4.375162e-01         4.532249e-01       0.1092588927\n62    custserv       9.114106e-01         3.742499e-02       0.0511644494\n63    custserv       9.487266e-01         3.323158e-03       0.0479502714\n64    custserv       6.396191e-01         2.204597e-01       0.1399212131\n65    mechanic       3.769863e-01         4.149835e-03       0.6188639027\n66    custserv       8.150930e-01         1.474024e-01       0.0375046016\n67    custserv       4.804647e-01         1.771258e-01       0.3424095294\n68    custserv       9.668855e-01         4.240998e-03       0.0288735299\n69    custserv       5.642826e-01         5.785442e-02       0.3778629731\n70    custserv       7.988256e-01         1.571578e-03       0.1996028063\n71    custserv       6.135977e-01         2.507652e-02       0.3613257474\n72    custserv       8.493025e-01         1.853127e-03       0.1488443717\n73    custserv       5.817686e-01         3.615675e-02       0.3820746444\n74    custserv       9.051632e-01         6.094582e-03       0.0887421887\n75    custserv       9.713098e-01         3.491677e-04       0.0283410443\n76    custserv       9.172652e-01         3.144031e-02       0.0512945333\n77  dispatcher       1.068060e-01         7.582388e-01       0.1349552313\n78    custserv       5.077766e-01         1.359402e-01       0.3562832211\n79    custserv       5.859625e-01         2.751358e-03       0.4112861174\n80    custserv       8.717307e-01         1.559568e-03       0.1267097808\n81    mechanic       2.979887e-01         4.689900e-02       0.6551123471\n82    custserv       6.344252e-01         8.995264e-02       0.2756221181\n83    custserv       7.226154e-01         1.483026e-01       0.1290819384\n84    custserv       7.280124e-01         4.041574e-02       0.2315718430\n85    mechanic       4.695896e-01         4.001790e-02       0.4903925046\n86    mechanic       3.810625e-01         3.046185e-03       0.6158913595\n87    mechanic       1.450807e-02         3.877670e-01       0.5977248885\n88    custserv       6.570708e-01         2.201784e-02       0.3209113839\n89    custserv       8.245977e-01         1.831683e-03       0.1735706360\n90    custserv       8.507090e-01         2.224719e-02       0.1270438510\n91  dispatcher       4.444566e-05         9.596984e-01       0.0402571239\n92    mechanic       1.140489e-01         2.496113e-01       0.6363397688\n93    custserv       5.068084e-01         5.082414e-02       0.4423674151\n94    custserv       5.176276e-01         1.503273e-01       0.3320451101\n95    mechanic       2.617847e-01         8.627973e-02       0.6519356111\n96    custserv       5.946078e-01         2.240108e-02       0.3829911730\n97    mechanic       3.580478e-01         1.603001e-01       0.4816520105\n98    mechanic       9.047848e-03         2.817818e-01       0.7091703489\n99  dispatcher       1.818092e-01         4.722545e-01       0.3459362957\n100   custserv       9.232779e-01         1.474794e-04       0.0765745994\n101   mechanic       5.371204e-02         4.043876e-01       0.5419003972\n102   custserv       7.274351e-01         1.143381e-02       0.2611311186\n103 dispatcher       2.236793e-01         5.076275e-01       0.2686932558\n104   mechanic       3.010574e-01         1.215299e-01       0.5774126448\n105   mechanic       1.322283e-01         1.526186e-01       0.7151531191\n106   mechanic       3.935759e-01         6.471784e-03       0.5999523328\n107   custserv       4.700075e-01         1.664739e-01       0.3635186070\n108   mechanic       1.913184e-03         1.385018e-01       0.8595850073\n109   mechanic       1.282814e-01         2.757856e-01       0.5959330417\n110   mechanic       7.359440e-03         1.797894e-01       0.8128511865\n111   mechanic       1.546963e-01         9.800578e-02       0.7472978899\n112   mechanic       6.366740e-03         4.327645e-01       0.5608687606\n113   mechanic       4.352380e-02         9.788893e-02       0.8585872696\n114   mechanic       5.093261e-02         1.125220e-01       0.8365453708\n115   mechanic       2.222210e-01         1.026739e-01       0.6751051113\n116   mechanic       1.042359e-01         1.297222e-02       0.8827918327\n117   mechanic       1.138676e-01         8.956908e-02       0.7965633033\n118   mechanic       2.766735e-02         3.846997e-01       0.5876329522\n119   custserv       5.051718e-01         2.363000e-02       0.4711982009\n120   mechanic       2.712627e-01         6.754493e-03       0.7219828096\n121   mechanic       9.323552e-02         5.987836e-02       0.8468861175\n122   mechanic       1.013269e-01         4.130693e-01       0.4856037318\n123   custserv       8.289650e-01         3.296750e-02       0.1380675414\n124 dispatcher       1.413975e-01         5.585446e-01       0.3000578672\n125   mechanic       4.375726e-01         7.067738e-03       0.5553596192\n126 dispatcher       1.945372e-01         4.396160e-01       0.3658468484\n127 dispatcher       6.659222e-03         5.770406e-01       0.4163001775\n128   mechanic       4.035569e-03         3.196286e-01       0.6763358520\n129   mechanic       2.026747e-01         6.923004e-02       0.7280952982\n130   mechanic       8.833131e-02         9.358450e-02       0.8180841927\n131   mechanic       1.372798e-01         3.125813e-02       0.8314620896\n132   mechanic       3.511831e-01         5.650817e-02       0.5923086915\n133   mechanic       3.989089e-01         1.858018e-02       0.5825108760\n134   mechanic       2.687072e-01         7.690800e-03       0.7236019746\n135   mechanic       1.781320e-01         2.064777e-01       0.6153903320\n136   mechanic       1.058156e-01         3.459499e-01       0.5482344743\n137   mechanic       1.389531e-01         1.739788e-01       0.6870680964\n138   custserv       5.162079e-01         1.525508e-02       0.4685369972\n139   mechanic       3.154765e-02         1.315983e-01       0.8368540071\n140   mechanic       1.546963e-01         9.800578e-02       0.7472978899\n141   mechanic       4.462685e-01         4.199934e-02       0.5117321428\n142   mechanic       4.215770e-01         1.778044e-02       0.5606425665\n143   mechanic       1.014840e-01         5.901682e-02       0.8394991446\n144   mechanic       3.143532e-01         2.140259e-01       0.4716208615\n145   mechanic       2.656731e-01         3.553211e-02       0.6987948125\n146 dispatcher       1.547478e-01         4.902368e-01       0.3550153647\n147   custserv       5.990222e-01         1.963323e-02       0.3813445332\n148   mechanic       1.004930e-02         3.833298e-01       0.6066208578\n149   mechanic       7.703519e-02         6.935664e-02       0.8536081721\n150 dispatcher       1.384048e-01         5.930786e-01       0.2685166334\n151   mechanic       7.745584e-02         5.268551e-03       0.9172756109\n152   mechanic       8.957511e-02         9.873244e-03       0.9005516501\n153   custserv       6.677764e-01         3.477501e-03       0.3287461102\n154   mechanic       6.596322e-02         6.301330e-02       0.8710234752\n155   mechanic       1.089597e-01         4.191985e-01       0.4718417916\n156   mechanic       2.593547e-01         4.776533e-02       0.6928799361\n157   mechanic       5.691481e-02         9.503874e-02       0.8480464521\n158   mechanic       1.631958e-01         1.675390e-01       0.6692651852\n159   mechanic       5.820853e-02         8.456147e-02       0.8572299996\n160   mechanic       7.491403e-03         2.845455e-01       0.7079630895\n161 dispatcher       6.574656e-03         5.473649e-01       0.4460604063\n162   mechanic       2.215688e-01         2.478046e-01       0.5306266834\n163   mechanic       1.814155e-02         1.317208e-01       0.8501376257\n164   mechanic       1.263200e-01         7.827628e-02       0.7954036797\n165   mechanic       1.040750e-02         2.553373e-01       0.7342552318\n166   mechanic       2.471054e-01         1.075548e-02       0.7421391527\n167   mechanic       5.202222e-02         2.140698e-01       0.7339080028\n168   custserv       7.534177e-01         1.880719e-04       0.2463942665\n169   mechanic       1.369770e-02         3.111132e-01       0.6751890552\n170   mechanic       1.290933e-01         2.225753e-01       0.6483313458\n171   custserv       5.292573e-01         6.885111e-03       0.4638575918\n172 dispatcher       3.434966e-02         6.633134e-01       0.3023369650\n173   mechanic       3.300682e-01         5.865320e-02       0.6112786186\n174   mechanic       2.632700e-02         4.564469e-01       0.5172261334\n175   mechanic       2.439916e-01         8.880757e-02       0.6672007895\n176   mechanic       1.787608e-02         3.674828e-02       0.9453756458\n177   mechanic       2.628249e-01         1.293962e-01       0.6077789461\n178   mechanic       7.519155e-02         1.451341e-01       0.7796743157\n179 dispatcher       2.233844e-02         6.021588e-01       0.3755027546\n180 dispatcher       5.558202e-02         5.125425e-01       0.4318755298\n181 dispatcher       2.279882e-01         6.933352e-01       0.0786765659\n182 dispatcher       1.455729e-01         6.377571e-01       0.2166700625\n183   custserv       5.068084e-01         5.082414e-02       0.4423674151\n184 dispatcher       4.711858e-03         8.828228e-01       0.1124653067\n185 dispatcher       9.615280e-04         9.635310e-01       0.0355074481\n186 dispatcher       1.666968e-01         5.399112e-01       0.2933919879\n187   mechanic       4.391730e-02         3.717385e-01       0.5843441781\n188 dispatcher       1.139711e-02         7.707428e-01       0.2178601020\n189 dispatcher       4.111778e-03         9.476083e-01       0.0482799441\n190   mechanic       3.619326e-02         2.215318e-01       0.7422749633\n191   mechanic       1.290640e-02         1.738047e-01       0.8132888754\n192 dispatcher       8.788252e-03         8.915707e-01       0.0996410258\n193 dispatcher       4.626438e-02         5.203113e-01       0.4334243141\n194 dispatcher       2.627441e-03         9.130421e-01       0.0843304228\n195 dispatcher       2.125497e-01         5.908120e-01       0.1966382663\n196 dispatcher       7.692116e-02         6.335984e-01       0.2894804124\n197 dispatcher       1.700342e-02         5.690896e-01       0.4139070326\n198 dispatcher       2.290562e-01         4.417444e-01       0.3291994564\n199 dispatcher       7.068771e-03         9.568860e-01       0.0360451874\n200 dispatcher       1.526915e-02         8.169172e-01       0.1678136599\n201 dispatcher       1.633462e-01         5.514045e-01       0.2852492423\n202 dispatcher       5.232025e-03         9.075164e-01       0.0872516234\n203 dispatcher       8.628193e-04         9.943258e-01       0.0048113373\n204 dispatcher       1.333064e-02         9.597850e-01       0.0268843510\n205   mechanic       3.273139e-01         2.622426e-01       0.4104435410\n206 dispatcher       1.120707e-02         6.833573e-01       0.3054355934\n207 dispatcher       4.565093e-03         9.318041e-01       0.0636307793\n208 dispatcher       8.965272e-03         8.021289e-01       0.1889058551\n209 dispatcher       2.771083e-03         9.499278e-01       0.0473010784\n210 dispatcher       1.017657e-02         6.852794e-01       0.3045440295\n211 dispatcher       3.579839e-03         8.478059e-01       0.1486142380\n212 dispatcher       1.661938e-01         8.079125e-01       0.0258936674\n213 dispatcher       5.261037e-03         9.592218e-01       0.0355171413\n214 dispatcher       3.628261e-04         9.583099e-01       0.0413272608\n215 dispatcher       9.772434e-02         6.483131e-01       0.2539625966\n216 dispatcher       1.014576e-01         7.813402e-01       0.1172021794\n217 dispatcher       1.566086e-03         9.906338e-01       0.0078001082\n218   custserv       8.699108e-01         4.515167e-02       0.0849374934\n219   mechanic       2.624197e-01         2.610486e-01       0.4765316600\n220 dispatcher       5.555677e-02         4.722668e-01       0.4721764764\n221 dispatcher       2.501282e-04         9.652656e-01       0.0344842774\n222 dispatcher       5.418486e-03         6.211936e-01       0.3733879008\n223   custserv       5.772471e-01         9.201760e-02       0.3307353202\n224   mechanic       3.669802e-03         3.209918e-01       0.6753383620\n225 dispatcher       2.034627e-04         9.832229e-01       0.0165736451\n226   mechanic       3.046592e-01         1.992892e-01       0.4960516106\n227 dispatcher       7.702310e-02         7.803817e-01       0.1425952452\n228 dispatcher       9.478974e-03         6.498195e-01       0.3407014879\n229 dispatcher       1.917110e-01         5.306246e-01       0.2776644348\n230   mechanic       1.386787e-01         3.651632e-01       0.4961580594\n231   mechanic       2.611049e-02         1.984311e-01       0.7754584493\n232   mechanic       1.347321e-01         2.417816e-01       0.6234862324\n233 dispatcher       1.826868e-03         8.861906e-01       0.1119825092\n234   mechanic       1.401549e-01         2.188120e-01       0.6410331082\n235 dispatcher       1.338773e-02         9.817725e-01       0.0048397672\n236 dispatcher       3.125918e-02         6.666292e-01       0.3021116335\n237 dispatcher       2.218212e-02         8.501670e-01       0.1276509099\n238 dispatcher       3.347095e-02         7.776216e-01       0.1889074349\n239 dispatcher       5.434963e-04         9.439490e-01       0.0555075385\n240 dispatcher       8.940124e-04         9.245479e-01       0.0745581347\n241   mechanic       2.853946e-01         1.586444e-01       0.5559610200\n242 dispatcher       1.391454e-02         9.666323e-01       0.0194531176\n243 dispatcher       1.209752e-02         8.754662e-01       0.1124362561\n244   mechanic       1.434582e-01         1.499332e-01       0.7066085684\n           x.LD1         x.LD2\n1    1.642315532  7.147735e-01\n2    0.148030225  1.509644e-01\n3    2.641521325 -1.683261e+00\n4    1.549368056  7.764901e-02\n5    1.547231403 -1.599412e-01\n6    0.220387584 -1.073313e+00\n7    1.388133530  1.726612e+00\n8    2.027760343 -3.448896e-02\n9    2.034969711 -1.183999e+00\n10   2.376059646 -5.837230e-01\n11   1.116297472 -7.476412e-01\n12   2.072282636  8.903634e-01\n13   1.271289458 -8.349841e-01\n14   0.402379646  1.413517e+00\n15   2.150846955  9.654870e-01\n16   1.489292781 -1.234684e+00\n17   2.603540829  3.240069e+00\n18   0.945919923  1.226612e+00\n19   0.601726507 -2.336475e-02\n20   1.791267975  1.602421e+00\n21   2.051656939  1.889983e+00\n22   0.205968848  1.225707e+00\n23   3.869342546  2.428997e+00\n24   2.324327985 -3.590654e-01\n25   0.242147527  6.135689e-01\n26   1.449043794 -1.684356e+00\n27   1.615482874  4.149922e-01\n28   0.838386292  6.141164e-01\n29   2.375057239  1.865188e+00\n30   2.030863823  6.152114e-01\n31   0.502703908  3.175522e+00\n32   1.508116662  2.076888e+00\n33   1.746745682  6.775683e-01\n34  -0.222828431  8.755975e-01\n35   1.744609029  4.399781e-01\n36   0.584204290  1.625955e+00\n37   0.510880104  2.438122e+00\n38   0.620382969  1.013816e+00\n39   0.344441024  3.387735e-01\n40   0.383555765 -1.898055e+00\n41   2.101084529 -8.466558e-01\n42   1.576200714  3.774303e-01\n43   1.731192700  2.900875e-01\n44   1.143130131 -4.478599e-01\n45   2.251003799  4.531015e-01\n46   0.541818650  9.386928e-01\n47  -0.020378089  8.841658e-02\n48   1.260976609 -3.351744e-01\n49   0.213178216  7.619736e-02\n50   1.465563603 -8.847652e-01\n51   1.384063222  6.648016e-01\n52   1.998958450  1.702530e+00\n53  -0.823340502  3.998696e-01\n54   0.772104056 -1.997618e+00\n55   0.424974578 -1.622903e+00\n56   1.840695565 -1.134218e+00\n57   0.313335060 -4.361882e-01\n58   1.807620367  1.276209e-01\n59   1.555407599 -8.973410e-01\n60   1.339540929 -2.600508e-01\n61  -0.140193803  2.012532e+00\n62   1.078149559  1.901298e+00\n63   1.967852486  9.275686e-01\n64   0.260803989  1.650750e+00\n65   1.423177963 -1.572027e+00\n66   0.552298916  2.713274e+00\n67   0.191585691  6.637066e-01\n68   1.906977801  1.477516e+00\n69   0.654424995  1.640877e-01\n70   2.114500858 -6.967652e-01\n71   0.990274797 -1.229268e-01\n72   2.090771680 -3.468462e-01\n73   0.835282812 -3.558397e-02\n74   1.707463523  6.400065e-01\n75   2.807960405  4.160872e-01\n76   1.143297549  1.826531e+00\n77  -0.894528035  1.449627e+00\n78   0.307295517  5.388019e-01\n79   1.762131246 -1.209342e+00\n80   2.169335999 -2.717225e-01\n81   0.454910717 -6.734217e-01\n82   0.554268151  6.764733e-01\n83   0.455078135  1.600969e+00\n84   0.903534282  5.393494e-01\n85   0.704020004 -2.981601e-01\n86   1.538887789 -1.696932e+00\n87  -1.504350548 -9.752022e-01\n88   1.068839117 -4.780320e-02\n89   2.077355351 -4.967368e-01\n90   1.203205405  8.644733e-01\n91  -4.030881267 -7.401587e-01\n92  -0.527739687 -3.368169e-01\n93   0.652288343 -7.350244e-02\n94   0.281429686  6.511307e-01\n95   0.184208905 -4.611741e-01\n96   1.016140628 -2.352557e-01\n97   0.097468389  2.011021e-01\n98  -1.583881695 -1.462436e+00\n99  -0.548365384  6.628024e-01\n100  3.060005755 -8.333416e-01\n101 -0.994852297 -3.123785e-01\n102  1.352957258 -1.101601e-01\n103 -0.482250566  1.000146e+00\n104  0.121197567 -1.488170e-01\n105 -0.296320035 -5.866264e-01\n106  1.281602307 -1.334794e+00\n107  0.202865367  5.760070e-01\n108 -1.953940941 -2.600083e+00\n109 -0.514323358 -1.869263e-01\n110 -1.509590681 -1.862493e+00\n111 -0.076180059 -7.487362e-01\n112 -1.869169661 -1.225559e+00\n113 -0.585678309 -1.411560e+00\n114 -0.572261980 -1.261669e+00\n115  0.055082749 -4.861601e-01\n116  0.487985915 -1.935261e+00\n117 -0.168160707 -9.737506e-01\n118 -1.243961584 -6.876401e-01\n119  0.924159979 -4.602700e-01\n120  1.111057339 -1.634932e+00\n121 -0.105149370 -1.286108e+00\n122 -0.745743010  6.288312e-02\n123  1.048213420  9.518162e-01\n124 -0.703357370  7.501453e-01\n125  1.295018636 -1.184903e+00\n126 -0.497803548  6.126647e-01\n127 -1.943460675 -8.255024e-01\n128 -1.948700808 -1.712793e+00\n129  0.157376246 -7.609555e-01\n130 -0.286007186 -1.086436e+00\n131  0.283398921 -1.385670e+00\n132  0.457047369 -4.358315e-01\n133  0.908606998 -8.477508e-01\n134  1.060495503 -1.584794e+00\n135 -0.280767053 -1.991455e-01\n136 -0.669315344 -9.958341e-02\n137 -0.322185865 -4.742975e-01\n138  1.090431642 -6.353124e-01\n139 -0.819234614 -1.399341e+00\n140 -0.076180059 -7.487362e-01\n141  0.664737844 -3.357219e-01\n142  0.947889157 -8.101890e-01\n143 -0.065867210 -1.248546e+00\n144 -0.057523596  2.884449e-01\n145  0.506642377 -8.980794e-01\n146 -0.626929704  5.876788e-01\n147  1.066702464 -2.853934e-01\n148 -1.646893032 -1.150079e+00\n149 -0.234275525 -1.311094e+00\n150 -0.729223201  8.624741e-01\n151  0.692572909 -2.484851e+00\n152  0.525131422 -2.135289e+00\n153  1.738402068 -8.594226e-01\n154 -0.262278009 -1.436355e+00\n155 -0.721047005  1.250743e-01\n156  0.390932551 -7.731747e-01\n157 -0.467831830 -1.298874e+00\n158 -0.243621546 -3.991739e-01\n159 -0.417269994 -1.349012e+00\n160 -1.662446014 -1.537560e+00\n161 -1.932180998 -9.132019e-01\n162 -0.253934395  1.006358e-01\n163 -1.040341418 -1.649341e+00\n164 -0.078316712 -9.863264e-01\n165 -1.494037699 -1.475012e+00\n166  0.905503517 -1.497451e+00\n167 -0.790265303 -8.619692e-01\n168  2.847075147 -1.820742e+00\n169 -1.452618887 -1.199860e+00\n170 -0.437895691 -3.493928e-01\n171  1.386999284 -9.598888e-01\n172 -1.328565447  2.122263e-01\n173  0.417765210 -4.733934e-01\n174 -1.320389251 -5.251736e-01\n175  0.144926745 -4.987360e-01\n176 -0.590918443 -2.298850e+00\n177  0.042633248 -2.239406e-01\n178 -0.506147162 -9.243262e-01\n179 -1.473244584 -2.002405e-01\n180 -1.057863635 -2.130522e-05\n181 -0.540021770  2.199793e+00\n182 -0.727086548  1.100064e+00\n183  0.652288343 -7.350244e-02\n184 -2.184226348  3.362268e-01\n185 -2.804194291  6.855983e-01\n186 -0.624793051  8.252689e-01\n187 -1.047550786 -4.998310e-01\n188 -1.809094386  8.677407e-02\n189 -2.231684703  1.036065e+00\n190 -0.947393942 -1.012217e+00\n191 -1.273897723 -1.637122e+00\n192 -1.935117061  7.114885e-01\n193 -1.136427954 -7.514496e-02\n194 -2.417782653  3.484461e-01\n195 -0.545261904  1.312503e+00\n196 -0.989612164  5.749120e-01\n197 -1.565225232 -4.252548e-01\n198 -0.430518905  7.754879e-01\n199 -2.008441247  1.523655e+00\n200 -1.703697409  4.616790e-01\n201 -0.639379205  8.498982e-01\n202 -2.142807536  6.113788e-01\n203 -2.782434348  2.372480e+00\n204 -1.745915631  2.048808e+00\n205 -0.109255257  5.131026e-01\n206 -1.785365208 -2.631449e-01\n207 -2.194539196  8.360365e-01\n208 -1.913524536  1.239792e-01\n209 -2.388813342  8.858176e-01\n210 -1.824647368 -3.007068e-01\n211 -2.289623326 -3.867814e-02\n212 -0.678493946  3.086727e+00\n213 -2.126287726  1.410970e+00\n214 -3.195846063  1.354601e-01\n215 -0.897631516  7.999263e-01\n216 -0.920393866  1.561956e+00\n217 -2.562294372  2.210370e+00\n218  0.972752581  1.526393e+00\n219 -0.201235906  2.880883e-01\n220 -1.031997804 -1.123501e-01\n221 -3.339558372  1.351035e-01\n222 -2.047890825 -7.882972e-01\n223  0.501569662  4.890208e-01\n224 -1.987982968 -1.750355e+00\n225 -3.400433057  6.850508e-01\n226 -0.046243920  2.007454e-01\n227 -1.037070520  1.274750e+00\n228 -1.838063697 -4.505974e-01\n229 -0.560814886  9.250219e-01\n230 -0.577334695  1.254309e-01\n231 -1.039374590 -1.237231e+00\n232 -0.449175368 -2.616933e-01\n233 -2.562461790 -6.402074e-02\n234 -0.398613532 -3.118310e-01\n235 -1.687010181  3.535661e+00\n236 -1.367847606  1.746645e-01\n237 -1.559018271  8.741458e-01\n238 -1.378160455  6.744741e-01\n239 -3.040854077  4.811723e-02\n240 -2.846579931 -1.663819e-03\n241  0.005487741 -2.391225e-02\n242 -1.719082972  2.348589e+00\n243 -1.805990906  7.364744e-01\n244 -0.257037875 -5.490645e-01\n```\n\n\n:::\n\n```{.r .cell-code}\nd <- cbind(jobs, p)\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    outdoor social conservative job id   X6    jobname      class\n1        10     22            5   1  1 <NA>   custserv   custserv\n2        14     17            6   1  2 <NA>   custserv   mechanic\n3        19     33            7   1  3 <NA>   custserv   custserv\n4        14     29           12   1  4 <NA>   custserv   custserv\n5        14     25            7   1  5 <NA>   custserv   custserv\n6        20     25           12   1  6 <NA>   custserv   mechanic\n7         6     18            4   1  7 <NA>   custserv   custserv\n8        13     27            7   1  8 <NA>   custserv   custserv\n9        18     31            9   1  9 <NA>   custserv   custserv\n10       16     35           13   1 10 <NA>   custserv   custserv\n11       17     25            8   1 11 <NA>   custserv   custserv\n12       10     29           11   1 12 <NA>   custserv   custserv\n13       17     25            7   1 13 <NA>   custserv   custserv\n14       10     22           13   1 14 <NA>   custserv   custserv\n15       10     31           13   1 15 <NA>   custserv   custserv\n16       18     25            5   1 16 <NA>   custserv   mechanic\n17        0     27           11   1 17 <NA>   custserv   custserv\n18       10     24           12   1 18 <NA>   custserv   custserv\n19       15     23           10   1 19 <NA>   custserv   custserv\n20        8     29           14   1 20 <NA>   custserv   custserv\n21        6     27           11   1 21 <NA>   custserv   custserv\n22       10     17            8   1 22 <NA>   custserv   custserv\n23        1     30            6   1 23 <NA>   custserv   custserv\n24       14     29            7   1 24 <NA>   custserv   custserv\n25       13     21           11   1 25 <NA>   custserv   custserv\n26       21     31           11   1 26 <NA>   custserv   mechanic\n27       12     26            9   1 27 <NA>   custserv   custserv\n28       12     22            9   1 28 <NA>   custserv   custserv\n29        5     25            7   1 29 <NA>   custserv   custserv\n30       10     24            5   1 30 <NA>   custserv   custserv\n31        3     20           14   1 31 <NA>   custserv   custserv\n32        6     25           12   1 32 <NA>   custserv   custserv\n33       11     27           10   1 33 <NA>   custserv   custserv\n34       13     21           14   1 34 <NA>   custserv dispatcher\n35       11     23            5   1 35 <NA>   custserv   custserv\n36        8     18            8   1 36 <NA>   custserv   custserv\n37        5     17            9   1 37 <NA>   custserv   custserv\n38       11     22           11   1 38 <NA>   custserv   custserv\n39       14     22           11   1 39 <NA>   custserv   custserv\n40       22     22            6   1 40 <NA>   custserv   mechanic\n41       16     28            6   1 41 <NA>   custserv   custserv\n42       12     25            8   1 42 <NA>   custserv   custserv\n43       12     25            7   1 43 <NA>   custserv   custserv\n44       15     21            4   1 44 <NA>   custserv   custserv\n45       11     28            8   1 45 <NA>   custserv   custserv\n46       11     20            9   1 46 <NA>   custserv   custserv\n47       15     19            9   1 47 <NA>   custserv   mechanic\n48       15     24            7   1 48 <NA>   custserv   custserv\n49       15     21           10   1 49 <NA>   custserv   mechanic\n50       17     26            7   1 50 <NA>   custserv   custserv\n51       12     28           13   1 51 <NA>   custserv   custserv\n52        7     28           12   1 52 <NA>   custserv   custserv\n53       14     12            6   1 53 <NA>   custserv dispatcher\n54       22     24            6   1 54 <NA>   custserv   mechanic\n55       22     27           12   1 55 <NA>   custserv   mechanic\n56       18     30            9   1 56 <NA>   custserv   custserv\n57       16     18            5   1 57 <NA>   custserv   mechanic\n58       12     23            4   1 58 <NA>   custserv   custserv\n59       16     22            2   1 59 <NA>   custserv   custserv\n60       15     26            9   1 60 <NA>   custserv   custserv\n61        7     13            7   1 61 <NA>   custserv dispatcher\n62        6     18            6   1 62 <NA>   custserv   custserv\n63        9     24            6   1 63 <NA>   custserv   custserv\n64        9     20           12   1 64 <NA>   custserv   custserv\n65       20     28            8   1 65 <NA>   custserv   mechanic\n66        5     22           15   1 66 <NA>   custserv   custserv\n67       14     26           17   1 67 <NA>   custserv   custserv\n68        8     28           12   1 68 <NA>   custserv   custserv\n69       14     22            9   1 69 <NA>   custserv   custserv\n70       15     26            4   1 70 <NA>   custserv   custserv\n71       15     25           10   1 71 <NA>   custserv   custserv\n72       14     27            6   1 72 <NA>   custserv   custserv\n73       15     25           11   1 73 <NA>   custserv   custserv\n74       11     26            9   1 74 <NA>   custserv   custserv\n75       10     28            5   1 75 <NA>   custserv   custserv\n76        7     22           10   1 76 <NA>   custserv   custserv\n77       11     15           12   1 77 <NA>   custserv dispatcher\n78       14     25           15   1 78 <NA>   custserv   custserv\n79       18     28            7   1 79 <NA>   custserv   custserv\n80       14     29            8   1 80 <NA>   custserv   custserv\n81       17     20            6   1 81 <NA>   custserv   mechanic\n82       13     25           14   1 82 <NA>   custserv   custserv\n83        9     21           12   1 83 <NA>   custserv   custserv\n84       13     26           13   1 84 <NA>   custserv   custserv\n85       16     24           10   1 85 <NA>   custserv   mechanic\n86       20     27            6   2  1 <NA>   mechanic   mechanic\n87       21     15           10   2  2 <NA>   mechanic   mechanic\n88       15     27           12   2  3 <NA>   mechanic   custserv\n89       15     29            8   2  4 <NA>   mechanic   custserv\n90       11     25           11   2  5 <NA>   mechanic   custserv\n91       24      9           17   2  6 <NA>   mechanic dispatcher\n92       18     21           13   2  7 <NA>   mechanic   mechanic\n93       14     18            4   2  8 <NA>   mechanic   custserv\n94       13     22           12   2  9 <NA>   mechanic   custserv\n95       17     21            9   2 10 <NA>   mechanic   mechanic\n96       16     28           13   2 11 <NA>   mechanic   custserv\n97       15     22           12   2 12 <NA>   mechanic   mechanic\n98       24     20           15   2 13 <NA>   mechanic   mechanic\n99       14     19           13   2 14 <NA>   mechanic dispatcher\n100      14     28            1   2 15 <NA>   mechanic   custserv\n101      18     17           11   2 16 <NA>   mechanic   mechanic\n102      14     24            7   2 17 <NA>   mechanic   custserv\n103      12     16           10   2 18 <NA>   mechanic dispatcher\n104      16     21           10   2 19 <NA>   mechanic   mechanic\n105      18     19            9   2 20 <NA>   mechanic   mechanic\n106      19     26            7   2 21 <NA>   mechanic   mechanic\n107      13     20           10   2 22 <NA>   mechanic   custserv\n108      28     16           10   2 23 <NA>   mechanic   mechanic\n109      17     19           11   2 24 <NA>   mechanic   mechanic\n110      24     14            7   2 25 <NA>   mechanic   mechanic\n111      19     23           12   2 26 <NA>   mechanic   mechanic\n112      22     12            8   2 27 <NA>   mechanic   mechanic\n113      22     21           11   2 28 <NA>   mechanic   mechanic\n114      21     19            9   2 29 <NA>   mechanic   mechanic\n115      18     24           13   2 30 <NA>   mechanic   mechanic\n116      23     27           11   2 31 <NA>   mechanic   mechanic\n117      20     23           12   2 32 <NA>   mechanic   mechanic\n118      19     13            7   2 33 <NA>   mechanic   mechanic\n119      17     28           13   2 34 <NA>   mechanic   custserv\n120      20     24            5   2 35 <NA>   mechanic   mechanic\n121      21     23           11   2 36 <NA>   mechanic   mechanic\n122      17     21           15   2 37 <NA>   mechanic   mechanic\n123      11     25           12   2 38 <NA>   mechanic   custserv\n124      14     19           14   2 39 <NA>   mechanic dispatcher\n125      18     24            5   2 40 <NA>   mechanic   mechanic\n126      13     14            7   2 41 <NA>   mechanic dispatcher\n127      22     18           16   2 42 <NA>   mechanic dispatcher\n128      25     17           13   2 43 <NA>   mechanic   mechanic\n129      19     25           13   2 44 <NA>   mechanic   mechanic\n130      20     20            9   2 45 <NA>   mechanic   mechanic\n131      21     25           11   2 46 <NA>   mechanic   mechanic\n132      17     24           11   2 47 <NA>   mechanic   mechanic\n133      18     26           10   2 48 <NA>   mechanic   mechanic\n134      21     29           11   2 49 <NA>   mechanic   mechanic\n135      17     21           12   2 50 <NA>   mechanic   mechanic\n136      17     19           12   2 51 <NA>   mechanic   mechanic\n137      17     16            6   2 52 <NA>   mechanic   mechanic\n138      16     22            5   2 53 <NA>   mechanic   custserv\n139      22     19           10   2 54 <NA>   mechanic   mechanic\n140      19     23           12   2 55 <NA>   mechanic   mechanic\n141      16     23            9   2 56 <NA>   mechanic   mechanic\n142      18     27           11   2 57 <NA>   mechanic   mechanic\n143      21     24           12   2 58 <NA>   mechanic   mechanic\n144      15     22           13   2 59 <NA>   mechanic   mechanic\n145      19     26           12   2 60 <NA>   mechanic   mechanic\n146      14     17           11   2 61 <NA>   mechanic dispatcher\n147      15     23            7   2 62 <NA>   mechanic   custserv\n148      23     20           16   2 63 <NA>   mechanic   mechanic\n149      22     26           15   2 64 <NA>   mechanic   mechanic\n150      13     16           11   2 65 <NA>   mechanic dispatcher\n151      25     29           11   2 66 <NA>   mechanic   mechanic\n152      23     24            7   2 67 <NA>   mechanic   mechanic\n153      17     29            9   2 68 <NA>   mechanic   custserv\n154      21     19            7   2 69 <NA>   mechanic   mechanic\n155      15     13            6   2 70 <NA>   mechanic   mechanic\n156      19     27           14   2 71 <NA>   mechanic   mechanic\n157      22     24           14   2 72 <NA>   mechanic   mechanic\n158      17     18            8   2 73 <NA>   mechanic   mechanic\n159      21     19            8   2 74 <NA>   mechanic   mechanic\n160      24     18           13   2 75 <NA>   mechanic   mechanic\n161      21     12            9   2 76 <NA>   mechanic dispatcher\n162      15     17            8   2 77 <NA>   mechanic   mechanic\n163      24     22           14   2 78 <NA>   mechanic   mechanic\n164      19     19            7   2 79 <NA>   mechanic   mechanic\n165      23     16           10   2 80 <NA>   mechanic   mechanic\n166      21     29           12   2 81 <NA>   mechanic   mechanic\n167      20     19           11   2 82 <NA>   mechanic   mechanic\n168      18     28            0   2 83 <NA>   mechanic   custserv\n169      23     21           16   2 84 <NA>   mechanic   mechanic\n170      17     17            8   2 85 <NA>   mechanic   mechanic\n171      17     24            5   2 86 <NA>   mechanic   custserv\n172      17     18           15   2 87 <NA>   mechanic dispatcher\n173      17     23           10   2 88 <NA>   mechanic   mechanic\n174      19     15           10   2 89 <NA>   mechanic   mechanic\n175      17     20            8   2 90 <NA>   mechanic   mechanic\n176      25     20            8   2 91 <NA>   mechanic   mechanic\n177      16     19            8   2 92 <NA>   mechanic   mechanic\n178      19     16            6   2 93 <NA>   mechanic   mechanic\n179      19     19           16   3  1 <NA> dispatcher dispatcher\n180      17     17           12   3  2 <NA> dispatcher dispatcher\n181       8     17           14   3  3 <NA> dispatcher dispatcher\n182      13     20           16   3  4 <NA> dispatcher dispatcher\n183      14     18            4   3  5 <NA> dispatcher   custserv\n184      17     12           13   3  6 <NA> dispatcher dispatcher\n185      17     12           17   3  7 <NA> dispatcher dispatcher\n186      14     21           16   3  8 <NA> dispatcher dispatcher\n187      19     18           12   3  9 <NA> dispatcher   mechanic\n188      18     16           15   3 10 <NA> dispatcher dispatcher\n189      15     14           17   3 11 <NA> dispatcher dispatcher\n190      20     15            7   3 12 <NA> dispatcher   mechanic\n191      24     20           13   3 13 <NA> dispatcher   mechanic\n192      16     16           17   3 14 <NA> dispatcher dispatcher\n193      17     15           10   3 15 <NA> dispatcher dispatcher\n194      17     10           12   3 16 <NA> dispatcher dispatcher\n195      11     16           11   3 17 <NA> dispatcher dispatcher\n196      15     18           14   3 18 <NA> dispatcher dispatcher\n197      20     19           16   3 19 <NA> dispatcher dispatcher\n198      14     22           16   3 20 <NA> dispatcher dispatcher\n199      13     15           18   3 21 <NA> dispatcher dispatcher\n200      16     14           13   3 22 <NA> dispatcher dispatcher\n201      12     12            6   3 23 <NA> dispatcher dispatcher\n202      17     17           19   3 24 <NA> dispatcher dispatcher\n203      10      8           16   3 25 <NA> dispatcher dispatcher\n204      11     17           20   3 26 <NA> dispatcher dispatcher\n205      13     16            7   3 27 <NA> dispatcher   mechanic\n206      19     15           13   3 28 <NA> dispatcher dispatcher\n207      15     11           13   3 29 <NA> dispatcher dispatcher\n208      17     11           10   3 30 <NA> dispatcher dispatcher\n209      15     10           13   3 31 <NA> dispatcher dispatcher\n210      19     14           12   3 32 <NA> dispatcher dispatcher\n211      19     14           15   3 33 <NA> dispatcher dispatcher\n212       4     12           11   3 34 <NA> dispatcher dispatcher\n213      13     12           15   3 35 <NA> dispatcher dispatcher\n214      20     13           19   3 36 <NA> dispatcher dispatcher\n215      14     18           14   3 37 <NA> dispatcher dispatcher\n216      10     12            9   3 38 <NA> dispatcher dispatcher\n217      11     12           19   3 39 <NA> dispatcher dispatcher\n218       8     20            8   3 40 <NA> dispatcher   custserv\n219      14     16            7   3 41 <NA> dispatcher   mechanic\n220      18     20           15   3 42 <NA> dispatcher dispatcher\n221      19      7           13   3 43 <NA> dispatcher dispatcher\n222      21     13           11   3 44 <NA> dispatcher dispatcher\n223      14     26           15   3 45 <NA> dispatcher   custserv\n224      25     16           12   3 46 <NA> dispatcher   mechanic\n225      18     11           19   3 47 <NA> dispatcher dispatcher\n226      14     16            6   3 48 <NA> dispatcher   mechanic\n227      13     20           18   3 49 <NA> dispatcher dispatcher\n228      20     16           14   3 50 <NA> dispatcher dispatcher\n229      12     14            8   3 51 <NA> dispatcher dispatcher\n230      16     19           12   3 52 <NA> dispatcher   mechanic\n231      21     15            7   3 53 <NA> dispatcher   mechanic\n232      18     23           15   3 54 <NA> dispatcher   mechanic\n233      19     11           13   3 55 <NA> dispatcher dispatcher\n234      17     18            9   3 56 <NA> dispatcher   mechanic\n235       4     10           15   3 57 <NA> dispatcher dispatcher\n236      17     17           14   3 58 <NA> dispatcher dispatcher\n237      14     13           12   3 59 <NA> dispatcher dispatcher\n238      15     16           14   3 60 <NA> dispatcher dispatcher\n239      20     13           18   3 61 <NA> dispatcher dispatcher\n240      20     14           18   3 62 <NA> dispatcher dispatcher\n241      16     22           12   3 63 <NA> dispatcher   mechanic\n242       9     13           16   3 64 <NA> dispatcher dispatcher\n243      15     13           13   3 65 <NA> dispatcher dispatcher\n244      18     20           10   3 66 <NA> dispatcher   mechanic\n    posterior.custserv posterior.dispatcher posterior.mechanic        x.LD1\n1         9.037622e-01         7.289988e-03       0.0889478485  1.642315532\n2         3.677743e-01         1.432468e-01       0.4889789008  0.148030225\n3         7.302117e-01         3.186265e-04       0.2694697105  2.641521325\n4         8.100756e-01         7.751215e-03       0.1821731894  1.549368056\n5         7.677607e-01         7.185490e-03       0.2250538225  1.547231403\n6         1.682521e-01         4.692305e-02       0.7848248752  0.220387584\n7         9.408328e-01         1.670509e-02       0.0424620706  1.388133530\n8         8.790086e-01         2.349121e-03       0.1186423050  2.027760343\n9         6.767464e-01         1.551373e-03       0.3217022453  2.034969711\n10        8.643564e-01         8.641095e-04       0.1347795344  2.376059646\n11        4.950388e-01         1.348603e-02       0.4914751264  1.116297472\n12        9.537446e-01         2.525084e-03       0.0437303474  2.072282636\n13        5.240823e-01         9.388337e-03       0.4665293631  1.271289458\n14        6.819795e-01         1.573579e-01       0.1606625929  0.402379646\n15        9.613543e-01         2.086923e-03       0.0365588154  2.150846955\n16        4.887584e-01         4.700656e-03       0.5065409138  1.489292781\n17        9.974534e-01         8.554731e-04       0.0016911727  2.603540829\n18        8.370707e-01         4.502895e-02       0.1179003525  0.945919923\n19        5.004012e-01         5.768116e-02       0.4419175871  0.601726507\n20        9.649054e-01         5.828045e-03       0.0292665325  1.791267975\n21        9.815677e-01         3.084262e-03       0.0153480620  2.051656939\n22        5.697748e-01         2.159598e-01       0.2142654531  0.205968848\n23        9.991982e-01         2.761694e-05       0.0007741549  3.869342546\n24        8.837774e-01         1.039703e-03       0.1151828840  2.324327985\n25        4.938044e-01         1.583747e-01       0.3478209184  0.242147527\n26        3.564096e-01         3.616675e-03       0.6399736784  1.449043794\n27        8.693126e-01         7.267402e-03       0.1234199965  1.615482874\n28        7.215912e-01         4.799088e-02       0.2304179226  0.838386292\n29        9.883365e-01         1.319044e-03       0.0103444748  2.375057239\n30        9.359042e-01         2.676529e-03       0.0614193101  2.030863823\n31        8.018758e-01         1.744651e-01       0.0236590252  0.502703908\n32        9.618798e-01         1.296402e-02       0.0251561754  1.508116662\n33        9.129230e-01         5.565968e-03       0.0815110137  1.746745682\n34        3.220516e-01         3.632646e-01       0.3146838699 -0.222828431\n35        8.909918e-01         5.313327e-03       0.1036949094  1.744609029\n36        7.734984e-01         1.132376e-01       0.1132639919  0.584204290\n37        7.928969e-01         1.548856e-01       0.0522175881  0.510880104\n38        7.152302e-01         8.860092e-02       0.1961688726  0.620382969\n39        4.830847e-01         1.145445e-01       0.4023708150  0.344441024\n40        9.523627e-02         1.568069e-02       0.8890830359  0.383555765\n41        7.679833e-01         1.538158e-03       0.2304785704  2.101084529\n42        8.582757e-01         7.923928e-03       0.1338004111  1.576200714\n43        8.727131e-01         5.298216e-03       0.1219886414  1.731192700\n44        5.832550e-01         1.533108e-02       0.4014139446  1.143130131\n45        9.432314e-01         1.480633e-03       0.0552880094  2.251003799\n46        6.748657e-01         1.019602e-01       0.2231741012  0.541818650\n47        2.955104e-01         1.781956e-01       0.5262939623 -0.020378089\n48        6.496219e-01         1.267779e-02       0.3377002656  1.260976609\n49        3.774214e-01         1.227084e-01       0.4998701545  0.213178216\n50        5.755170e-01         6.138769e-03       0.4183442282  1.465563603\n51        8.603505e-01         1.363857e-02       0.1260109570  1.384063222\n52        9.764155e-01         3.449384e-03       0.0201350730  1.998958450\n53        1.013578e-01         5.274616e-01       0.3711805789 -0.823340502\n54        1.373009e-01         8.015011e-03       0.8546840791  0.772104056\n55        1.302936e-01         1.986045e-02       0.8498459777  0.424974578\n56        6.305186e-01         2.427472e-03       0.3670539517  1.840695565\n57        3.052008e-01         7.175295e-02       0.6230462861  0.313335060\n58        8.650641e-01         4.211839e-03       0.1307240754  1.807620367\n59        6.014337e-01         5.053717e-03       0.3935126078  1.555407599\n60        6.910126e-01         1.105727e-02       0.2979301609  1.339540929\n61        4.375162e-01         4.532249e-01       0.1092588927 -0.140193803\n62        9.114106e-01         3.742499e-02       0.0511644494  1.078149559\n63        9.487266e-01         3.323158e-03       0.0479502714  1.967852486\n64        6.396191e-01         2.204597e-01       0.1399212131  0.260803989\n65        3.769863e-01         4.149835e-03       0.6188639027  1.423177963\n66        8.150930e-01         1.474024e-01       0.0375046016  0.552298916\n67        4.804647e-01         1.771258e-01       0.3424095294  0.191585691\n68        9.668855e-01         4.240998e-03       0.0288735299  1.906977801\n69        5.642826e-01         5.785442e-02       0.3778629731  0.654424995\n70        7.988256e-01         1.571578e-03       0.1996028063  2.114500858\n71        6.135977e-01         2.507652e-02       0.3613257474  0.990274797\n72        8.493025e-01         1.853127e-03       0.1488443717  2.090771680\n73        5.817686e-01         3.615675e-02       0.3820746444  0.835282812\n74        9.051632e-01         6.094582e-03       0.0887421887  1.707463523\n75        9.713098e-01         3.491677e-04       0.0283410443  2.807960405\n76        9.172652e-01         3.144031e-02       0.0512945333  1.143297549\n77        1.068060e-01         7.582388e-01       0.1349552313 -0.894528035\n78        5.077766e-01         1.359402e-01       0.3562832211  0.307295517\n79        5.859625e-01         2.751358e-03       0.4112861174  1.762131246\n80        8.717307e-01         1.559568e-03       0.1267097808  2.169335999\n81        2.979887e-01         4.689900e-02       0.6551123471  0.454910717\n82        6.344252e-01         8.995264e-02       0.2756221181  0.554268151\n83        7.226154e-01         1.483026e-01       0.1290819384  0.455078135\n84        7.280124e-01         4.041574e-02       0.2315718430  0.903534282\n85        4.695896e-01         4.001790e-02       0.4903925046  0.704020004\n86        3.810625e-01         3.046185e-03       0.6158913595  1.538887789\n87        1.450807e-02         3.877670e-01       0.5977248885 -1.504350548\n88        6.570708e-01         2.201784e-02       0.3209113839  1.068839117\n89        8.245977e-01         1.831683e-03       0.1735706360  2.077355351\n90        8.507090e-01         2.224719e-02       0.1270438510  1.203205405\n91        4.444566e-05         9.596984e-01       0.0402571239 -4.030881267\n92        1.140489e-01         2.496113e-01       0.6363397688 -0.527739687\n93        5.068084e-01         5.082414e-02       0.4423674151  0.652288343\n94        5.176276e-01         1.503273e-01       0.3320451101  0.281429686\n95        2.617847e-01         8.627973e-02       0.6519356111  0.184208905\n96        5.946078e-01         2.240108e-02       0.3829911730  1.016140628\n97        3.580478e-01         1.603001e-01       0.4816520105  0.097468389\n98        9.047848e-03         2.817818e-01       0.7091703489 -1.583881695\n99        1.818092e-01         4.722545e-01       0.3459362957 -0.548365384\n100       9.232779e-01         1.474794e-04       0.0765745994  3.060005755\n101       5.371204e-02         4.043876e-01       0.5419003972 -0.994852297\n102       7.274351e-01         1.143381e-02       0.2611311186  1.352957258\n103       2.236793e-01         5.076275e-01       0.2686932558 -0.482250566\n104       3.010574e-01         1.215299e-01       0.5774126448  0.121197567\n105       1.322283e-01         1.526186e-01       0.7151531191 -0.296320035\n106       3.935759e-01         6.471784e-03       0.5999523328  1.281602307\n107       4.700075e-01         1.664739e-01       0.3635186070  0.202865367\n108       1.913184e-03         1.385018e-01       0.8595850073 -1.953940941\n109       1.282814e-01         2.757856e-01       0.5959330417 -0.514323358\n110       7.359440e-03         1.797894e-01       0.8128511865 -1.509590681\n111       1.546963e-01         9.800578e-02       0.7472978899 -0.076180059\n112       6.366740e-03         4.327645e-01       0.5608687606 -1.869169661\n113       4.352380e-02         9.788893e-02       0.8585872696 -0.585678309\n114       5.093261e-02         1.125220e-01       0.8365453708 -0.572261980\n115       2.222210e-01         1.026739e-01       0.6751051113  0.055082749\n116       1.042359e-01         1.297222e-02       0.8827918327  0.487985915\n117       1.138676e-01         8.956908e-02       0.7965633033 -0.168160707\n118       2.766735e-02         3.846997e-01       0.5876329522 -1.243961584\n119       5.051718e-01         2.363000e-02       0.4711982009  0.924159979\n120       2.712627e-01         6.754493e-03       0.7219828096  1.111057339\n121       9.323552e-02         5.987836e-02       0.8468861175 -0.105149370\n122       1.013269e-01         4.130693e-01       0.4856037318 -0.745743010\n123       8.289650e-01         3.296750e-02       0.1380675414  1.048213420\n124       1.413975e-01         5.585446e-01       0.3000578672 -0.703357370\n125       4.375726e-01         7.067738e-03       0.5553596192  1.295018636\n126       1.945372e-01         4.396160e-01       0.3658468484 -0.497803548\n127       6.659222e-03         5.770406e-01       0.4163001775 -1.943460675\n128       4.035569e-03         3.196286e-01       0.6763358520 -1.948700808\n129       2.026747e-01         6.923004e-02       0.7280952982  0.157376246\n130       8.833131e-02         9.358450e-02       0.8180841927 -0.286007186\n131       1.372798e-01         3.125813e-02       0.8314620896  0.283398921\n132       3.511831e-01         5.650817e-02       0.5923086915  0.457047369\n133       3.989089e-01         1.858018e-02       0.5825108760  0.908606998\n134       2.687072e-01         7.690800e-03       0.7236019746  1.060495503\n135       1.781320e-01         2.064777e-01       0.6153903320 -0.280767053\n136       1.058156e-01         3.459499e-01       0.5482344743 -0.669315344\n137       1.389531e-01         1.739788e-01       0.6870680964 -0.322185865\n138       5.162079e-01         1.525508e-02       0.4685369972  1.090431642\n139       3.154765e-02         1.315983e-01       0.8368540071 -0.819234614\n140       1.546963e-01         9.800578e-02       0.7472978899 -0.076180059\n141       4.462685e-01         4.199934e-02       0.5117321428  0.664737844\n142       4.215770e-01         1.778044e-02       0.5606425665  0.947889157\n143       1.014840e-01         5.901682e-02       0.8394991446 -0.065867210\n144       3.143532e-01         2.140259e-01       0.4716208615 -0.057523596\n145       2.656731e-01         3.553211e-02       0.6987948125  0.506642377\n146       1.547478e-01         4.902368e-01       0.3550153647 -0.626929704\n147       5.990222e-01         1.963323e-02       0.3813445332  1.066702464\n148       1.004930e-02         3.833298e-01       0.6066208578 -1.646893032\n149       7.703519e-02         6.935664e-02       0.8536081721 -0.234275525\n150       1.384048e-01         5.930786e-01       0.2685166334 -0.729223201\n151       7.745584e-02         5.268551e-03       0.9172756109  0.692572909\n152       8.957511e-02         9.873244e-03       0.9005516501  0.525131422\n153       6.677764e-01         3.477501e-03       0.3287461102  1.738402068\n154       6.596322e-02         6.301330e-02       0.8710234752 -0.262278009\n155       1.089597e-01         4.191985e-01       0.4718417916 -0.721047005\n156       2.593547e-01         4.776533e-02       0.6928799361  0.390932551\n157       5.691481e-02         9.503874e-02       0.8480464521 -0.467831830\n158       1.631958e-01         1.675390e-01       0.6692651852 -0.243621546\n159       5.820853e-02         8.456147e-02       0.8572299996 -0.417269994\n160       7.491403e-03         2.845455e-01       0.7079630895 -1.662446014\n161       6.574656e-03         5.473649e-01       0.4460604063 -1.932180998\n162       2.215688e-01         2.478046e-01       0.5306266834 -0.253934395\n163       1.814155e-02         1.317208e-01       0.8501376257 -1.040341418\n164       1.263200e-01         7.827628e-02       0.7954036797 -0.078316712\n165       1.040750e-02         2.553373e-01       0.7342552318 -1.494037699\n166       2.471054e-01         1.075548e-02       0.7421391527  0.905503517\n167       5.202222e-02         2.140698e-01       0.7339080028 -0.790265303\n168       7.534177e-01         1.880719e-04       0.2463942665  2.847075147\n169       1.369770e-02         3.111132e-01       0.6751890552 -1.452618887\n170       1.290933e-01         2.225753e-01       0.6483313458 -0.437895691\n171       5.292573e-01         6.885111e-03       0.4638575918  1.386999284\n172       3.434966e-02         6.633134e-01       0.3023369650 -1.328565447\n173       3.300682e-01         5.865320e-02       0.6112786186  0.417765210\n174       2.632700e-02         4.564469e-01       0.5172261334 -1.320389251\n175       2.439916e-01         8.880757e-02       0.6672007895  0.144926745\n176       1.787608e-02         3.674828e-02       0.9453756458 -0.590918443\n177       2.628249e-01         1.293962e-01       0.6077789461  0.042633248\n178       7.519155e-02         1.451341e-01       0.7796743157 -0.506147162\n179       2.233844e-02         6.021588e-01       0.3755027546 -1.473244584\n180       5.558202e-02         5.125425e-01       0.4318755298 -1.057863635\n181       2.279882e-01         6.933352e-01       0.0786765659 -0.540021770\n182       1.455729e-01         6.377571e-01       0.2166700625 -0.727086548\n183       5.068084e-01         5.082414e-02       0.4423674151  0.652288343\n184       4.711858e-03         8.828228e-01       0.1124653067 -2.184226348\n185       9.615280e-04         9.635310e-01       0.0355074481 -2.804194291\n186       1.666968e-01         5.399112e-01       0.2933919879 -0.624793051\n187       4.391730e-02         3.717385e-01       0.5843441781 -1.047550786\n188       1.139711e-02         7.707428e-01       0.2178601020 -1.809094386\n189       4.111778e-03         9.476083e-01       0.0482799441 -2.231684703\n190       3.619326e-02         2.215318e-01       0.7422749633 -0.947393942\n191       1.290640e-02         1.738047e-01       0.8132888754 -1.273897723\n192       8.788252e-03         8.915707e-01       0.0996410258 -1.935117061\n193       4.626438e-02         5.203113e-01       0.4334243141 -1.136427954\n194       2.627441e-03         9.130421e-01       0.0843304228 -2.417782653\n195       2.125497e-01         5.908120e-01       0.1966382663 -0.545261904\n196       7.692116e-02         6.335984e-01       0.2894804124 -0.989612164\n197       1.700342e-02         5.690896e-01       0.4139070326 -1.565225232\n198       2.290562e-01         4.417444e-01       0.3291994564 -0.430518905\n199       7.068771e-03         9.568860e-01       0.0360451874 -2.008441247\n200       1.526915e-02         8.169172e-01       0.1678136599 -1.703697409\n201       1.633462e-01         5.514045e-01       0.2852492423 -0.639379205\n202       5.232025e-03         9.075164e-01       0.0872516234 -2.142807536\n203       8.628193e-04         9.943258e-01       0.0048113373 -2.782434348\n204       1.333064e-02         9.597850e-01       0.0268843510 -1.745915631\n205       3.273139e-01         2.622426e-01       0.4104435410 -0.109255257\n206       1.120707e-02         6.833573e-01       0.3054355934 -1.785365208\n207       4.565093e-03         9.318041e-01       0.0636307793 -2.194539196\n208       8.965272e-03         8.021289e-01       0.1889058551 -1.913524536\n209       2.771083e-03         9.499278e-01       0.0473010784 -2.388813342\n210       1.017657e-02         6.852794e-01       0.3045440295 -1.824647368\n211       3.579839e-03         8.478059e-01       0.1486142380 -2.289623326\n212       1.661938e-01         8.079125e-01       0.0258936674 -0.678493946\n213       5.261037e-03         9.592218e-01       0.0355171413 -2.126287726\n214       3.628261e-04         9.583099e-01       0.0413272608 -3.195846063\n215       9.772434e-02         6.483131e-01       0.2539625966 -0.897631516\n216       1.014576e-01         7.813402e-01       0.1172021794 -0.920393866\n217       1.566086e-03         9.906338e-01       0.0078001082 -2.562294372\n218       8.699108e-01         4.515167e-02       0.0849374934  0.972752581\n219       2.624197e-01         2.610486e-01       0.4765316600 -0.201235906\n220       5.555677e-02         4.722668e-01       0.4721764764 -1.031997804\n221       2.501282e-04         9.652656e-01       0.0344842774 -3.339558372\n222       5.418486e-03         6.211936e-01       0.3733879008 -2.047890825\n223       5.772471e-01         9.201760e-02       0.3307353202  0.501569662\n224       3.669802e-03         3.209918e-01       0.6753383620 -1.987982968\n225       2.034627e-04         9.832229e-01       0.0165736451 -3.400433057\n226       3.046592e-01         1.992892e-01       0.4960516106 -0.046243920\n227       7.702310e-02         7.803817e-01       0.1425952452 -1.037070520\n228       9.478974e-03         6.498195e-01       0.3407014879 -1.838063697\n229       1.917110e-01         5.306246e-01       0.2776644348 -0.560814886\n230       1.386787e-01         3.651632e-01       0.4961580594 -0.577334695\n231       2.611049e-02         1.984311e-01       0.7754584493 -1.039374590\n232       1.347321e-01         2.417816e-01       0.6234862324 -0.449175368\n233       1.826868e-03         8.861906e-01       0.1119825092 -2.562461790\n234       1.401549e-01         2.188120e-01       0.6410331082 -0.398613532\n235       1.338773e-02         9.817725e-01       0.0048397672 -1.687010181\n236       3.125918e-02         6.666292e-01       0.3021116335 -1.367847606\n237       2.218212e-02         8.501670e-01       0.1276509099 -1.559018271\n238       3.347095e-02         7.776216e-01       0.1889074349 -1.378160455\n239       5.434963e-04         9.439490e-01       0.0555075385 -3.040854077\n240       8.940124e-04         9.245479e-01       0.0745581347 -2.846579931\n241       2.853946e-01         1.586444e-01       0.5559610200  0.005487741\n242       1.391454e-02         9.666323e-01       0.0194531176 -1.719082972\n243       1.209752e-02         8.754662e-01       0.1124362561 -1.805990906\n244       1.434582e-01         1.499332e-01       0.7066085684 -0.257037875\n            x.LD2\n1    7.147735e-01\n2    1.509644e-01\n3   -1.683261e+00\n4    7.764901e-02\n5   -1.599412e-01\n6   -1.073313e+00\n7    1.726612e+00\n8   -3.448896e-02\n9   -1.183999e+00\n10  -5.837230e-01\n11  -7.476412e-01\n12   8.903634e-01\n13  -8.349841e-01\n14   1.413517e+00\n15   9.654870e-01\n16  -1.234684e+00\n17   3.240069e+00\n18   1.226612e+00\n19  -2.336475e-02\n20   1.602421e+00\n21   1.889983e+00\n22   1.225707e+00\n23   2.428997e+00\n24  -3.590654e-01\n25   6.135689e-01\n26  -1.684356e+00\n27   4.149922e-01\n28   6.141164e-01\n29   1.865188e+00\n30   6.152114e-01\n31   3.175522e+00\n32   2.076888e+00\n33   6.775683e-01\n34   8.755975e-01\n35   4.399781e-01\n36   1.625955e+00\n37   2.438122e+00\n38   1.013816e+00\n39   3.387735e-01\n40  -1.898055e+00\n41  -8.466558e-01\n42   3.774303e-01\n43   2.900875e-01\n44  -4.478599e-01\n45   4.531015e-01\n46   9.386928e-01\n47   8.841658e-02\n48  -3.351744e-01\n49   7.619736e-02\n50  -8.847652e-01\n51   6.648016e-01\n52   1.702530e+00\n53   3.998696e-01\n54  -1.997618e+00\n55  -1.622903e+00\n56  -1.134218e+00\n57  -4.361882e-01\n58   1.276209e-01\n59  -8.973410e-01\n60  -2.600508e-01\n61   2.012532e+00\n62   1.901298e+00\n63   9.275686e-01\n64   1.650750e+00\n65  -1.572027e+00\n66   2.713274e+00\n67   6.637066e-01\n68   1.477516e+00\n69   1.640877e-01\n70  -6.967652e-01\n71  -1.229268e-01\n72  -3.468462e-01\n73  -3.558397e-02\n74   6.400065e-01\n75   4.160872e-01\n76   1.826531e+00\n77   1.449627e+00\n78   5.388019e-01\n79  -1.209342e+00\n80  -2.717225e-01\n81  -6.734217e-01\n82   6.764733e-01\n83   1.600969e+00\n84   5.393494e-01\n85  -2.981601e-01\n86  -1.696932e+00\n87  -9.752022e-01\n88  -4.780320e-02\n89  -4.967368e-01\n90   8.644733e-01\n91  -7.401587e-01\n92  -3.368169e-01\n93  -7.350244e-02\n94   6.511307e-01\n95  -4.611741e-01\n96  -2.352557e-01\n97   2.011021e-01\n98  -1.462436e+00\n99   6.628024e-01\n100 -8.333416e-01\n101 -3.123785e-01\n102 -1.101601e-01\n103  1.000146e+00\n104 -1.488170e-01\n105 -5.866264e-01\n106 -1.334794e+00\n107  5.760070e-01\n108 -2.600083e+00\n109 -1.869263e-01\n110 -1.862493e+00\n111 -7.487362e-01\n112 -1.225559e+00\n113 -1.411560e+00\n114 -1.261669e+00\n115 -4.861601e-01\n116 -1.935261e+00\n117 -9.737506e-01\n118 -6.876401e-01\n119 -4.602700e-01\n120 -1.634932e+00\n121 -1.286108e+00\n122  6.288312e-02\n123  9.518162e-01\n124  7.501453e-01\n125 -1.184903e+00\n126  6.126647e-01\n127 -8.255024e-01\n128 -1.712793e+00\n129 -7.609555e-01\n130 -1.086436e+00\n131 -1.385670e+00\n132 -4.358315e-01\n133 -8.477508e-01\n134 -1.584794e+00\n135 -1.991455e-01\n136 -9.958341e-02\n137 -4.742975e-01\n138 -6.353124e-01\n139 -1.399341e+00\n140 -7.487362e-01\n141 -3.357219e-01\n142 -8.101890e-01\n143 -1.248546e+00\n144  2.884449e-01\n145 -8.980794e-01\n146  5.876788e-01\n147 -2.853934e-01\n148 -1.150079e+00\n149 -1.311094e+00\n150  8.624741e-01\n151 -2.484851e+00\n152 -2.135289e+00\n153 -8.594226e-01\n154 -1.436355e+00\n155  1.250743e-01\n156 -7.731747e-01\n157 -1.298874e+00\n158 -3.991739e-01\n159 -1.349012e+00\n160 -1.537560e+00\n161 -9.132019e-01\n162  1.006358e-01\n163 -1.649341e+00\n164 -9.863264e-01\n165 -1.475012e+00\n166 -1.497451e+00\n167 -8.619692e-01\n168 -1.820742e+00\n169 -1.199860e+00\n170 -3.493928e-01\n171 -9.598888e-01\n172  2.122263e-01\n173 -4.733934e-01\n174 -5.251736e-01\n175 -4.987360e-01\n176 -2.298850e+00\n177 -2.239406e-01\n178 -9.243262e-01\n179 -2.002405e-01\n180 -2.130522e-05\n181  2.199793e+00\n182  1.100064e+00\n183 -7.350244e-02\n184  3.362268e-01\n185  6.855983e-01\n186  8.252689e-01\n187 -4.998310e-01\n188  8.677407e-02\n189  1.036065e+00\n190 -1.012217e+00\n191 -1.637122e+00\n192  7.114885e-01\n193 -7.514496e-02\n194  3.484461e-01\n195  1.312503e+00\n196  5.749120e-01\n197 -4.252548e-01\n198  7.754879e-01\n199  1.523655e+00\n200  4.616790e-01\n201  8.498982e-01\n202  6.113788e-01\n203  2.372480e+00\n204  2.048808e+00\n205  5.131026e-01\n206 -2.631449e-01\n207  8.360365e-01\n208  1.239792e-01\n209  8.858176e-01\n210 -3.007068e-01\n211 -3.867814e-02\n212  3.086727e+00\n213  1.410970e+00\n214  1.354601e-01\n215  7.999263e-01\n216  1.561956e+00\n217  2.210370e+00\n218  1.526393e+00\n219  2.880883e-01\n220 -1.123501e-01\n221  1.351035e-01\n222 -7.882972e-01\n223  4.890208e-01\n224 -1.750355e+00\n225  6.850508e-01\n226  2.007454e-01\n227  1.274750e+00\n228 -4.505974e-01\n229  9.250219e-01\n230  1.254309e-01\n231 -1.237231e+00\n232 -2.616933e-01\n233 -6.402074e-02\n234 -3.118310e-01\n235  3.535661e+00\n236  1.746645e-01\n237  8.741458e-01\n238  6.744741e-01\n239  4.811723e-02\n240 -1.663819e-03\n241 -2.391225e-02\n242  2.348589e+00\n243  7.364744e-01\n244 -5.490645e-01\n```\n\n\n:::\n:::\n\nFollowing my suggestion, plot these the standard way with `colour`\ndistinguishing the jobs:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = x.LD1, y = x.LD2, colour = jobname)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/jobs-8-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# ggplot(d, aes(x = x.LD1, y = x.LD2, colour = class)) + geom_point()\n# ggplot(d, aes(x = job, y = x.LD1)) + geom_boxplot()\n```\n:::\n\nI was mostly right about the customer service people: small `LD1`\ndefinitely, large `LD2` kinda. I wasn't more right because the group\nmeans don't tell the whole story: evidently, the customer service people\nvary quite a bit on `outdoor`, so the red dots are all over the left\nside of the plot.\n\nThere is quite a bit of intermingling of the three employee groups on\nthe plot, but the point of the MANOVA is that the groups are (way) more\nseparated than you'd expect by chance, that is if the employees were\njust randomly scattered across the plot.\n\nTo think back to that `trace` thing: here, it seems that `LD1` mainly\nseparates customer service (left) from dispatchers (right); the\nmechanics are all over the place on `LD1`, but they tend to be low on\n`LD2`. So `LD2` *does* have something to say.\n\n$\\blacksquare$\n\n(i) <a name=\"part:predjob\">\\*</a> Obtain predicted job allocations for\n    each individual (based on their scores on the three scales), and\n    tabulate the true jobs against the predicted jobs. How would you\n    describe the quality of the classification? Is that in line with\n    what the plot would suggest?\n\nSolution\n\nUse the predictions that you got before and saved in `d`:\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(d, table(obs = jobname, pred = class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            pred\nobs          custserv dispatcher mechanic\n  custserv         68          4       13\n  dispatcher        3         50       13\n  mechanic         16         10       67\n```\n\n\n:::\n:::\n\nOr, the `tidyverse` way:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% count(job, class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  job      class  n\n1   1   custserv 68\n2   1 dispatcher  4\n3   1   mechanic 13\n4   2   custserv 16\n5   2 dispatcher 10\n6   2   mechanic 67\n7   3   custserv  3\n8   3 dispatcher 50\n9   3   mechanic 13\n```\n\n\n:::\n:::\n\nor:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(job, class) %>%\n  pivot_wider(names_from=class, values_from=n, values_fill = list(n=0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 4\n    job custserv dispatcher mechanic\n  <dbl>    <int>      <int>    <int>\n1     1       68          4       13\n2     2       16         10       67\n3     3        3         50       13\n```\n\n\n:::\n:::\n\nI didn't really need the `values_fill` since there are no missing\nfrequencies, but I've gotten used to putting it in. There are a lot of\nmisclassifications, but there are a lot of people, so a large fraction\nof people actually got classified correctly. The biggest frequencies are\nof people who got classified correctly. I think this is about what I was\nexpecting, looking at the plot: the people top left are obviously\ncustomer service, the ones top right are in dispatch, and most of the\nones at the bottom are mechanics. So there will be some errors, but the\nmajority of people should be gotten right. The easiest pairing to get\nconfused is customer service and mechanics, which you might guess from\nthe plot: those customer service people with a middling `LD1` score and\na low `LD2` score (that is, high on `outdoor`) could easily be confused\nwith the mechanics. The easiest pairing to distinguish is customer\nservice and dispatchers: on the plot, left and right, that is, low and\nhigh respectively on `LD1`.\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% \n  filter(jobname != class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    outdoor social conservative job id   X6    jobname      class\n2        14     17            6   1  2 <NA>   custserv   mechanic\n6        20     25           12   1  6 <NA>   custserv   mechanic\n16       18     25            5   1 16 <NA>   custserv   mechanic\n26       21     31           11   1 26 <NA>   custserv   mechanic\n34       13     21           14   1 34 <NA>   custserv dispatcher\n40       22     22            6   1 40 <NA>   custserv   mechanic\n47       15     19            9   1 47 <NA>   custserv   mechanic\n49       15     21           10   1 49 <NA>   custserv   mechanic\n53       14     12            6   1 53 <NA>   custserv dispatcher\n54       22     24            6   1 54 <NA>   custserv   mechanic\n55       22     27           12   1 55 <NA>   custserv   mechanic\n57       16     18            5   1 57 <NA>   custserv   mechanic\n61        7     13            7   1 61 <NA>   custserv dispatcher\n65       20     28            8   1 65 <NA>   custserv   mechanic\n77       11     15           12   1 77 <NA>   custserv dispatcher\n81       17     20            6   1 81 <NA>   custserv   mechanic\n85       16     24           10   1 85 <NA>   custserv   mechanic\n88       15     27           12   2  3 <NA>   mechanic   custserv\n89       15     29            8   2  4 <NA>   mechanic   custserv\n90       11     25           11   2  5 <NA>   mechanic   custserv\n91       24      9           17   2  6 <NA>   mechanic dispatcher\n93       14     18            4   2  8 <NA>   mechanic   custserv\n94       13     22           12   2  9 <NA>   mechanic   custserv\n96       16     28           13   2 11 <NA>   mechanic   custserv\n99       14     19           13   2 14 <NA>   mechanic dispatcher\n100      14     28            1   2 15 <NA>   mechanic   custserv\n102      14     24            7   2 17 <NA>   mechanic   custserv\n103      12     16           10   2 18 <NA>   mechanic dispatcher\n107      13     20           10   2 22 <NA>   mechanic   custserv\n119      17     28           13   2 34 <NA>   mechanic   custserv\n123      11     25           12   2 38 <NA>   mechanic   custserv\n124      14     19           14   2 39 <NA>   mechanic dispatcher\n126      13     14            7   2 41 <NA>   mechanic dispatcher\n127      22     18           16   2 42 <NA>   mechanic dispatcher\n138      16     22            5   2 53 <NA>   mechanic   custserv\n146      14     17           11   2 61 <NA>   mechanic dispatcher\n147      15     23            7   2 62 <NA>   mechanic   custserv\n150      13     16           11   2 65 <NA>   mechanic dispatcher\n153      17     29            9   2 68 <NA>   mechanic   custserv\n161      21     12            9   2 76 <NA>   mechanic dispatcher\n168      18     28            0   2 83 <NA>   mechanic   custserv\n171      17     24            5   2 86 <NA>   mechanic   custserv\n172      17     18           15   2 87 <NA>   mechanic dispatcher\n183      14     18            4   3  5 <NA> dispatcher   custserv\n187      19     18           12   3  9 <NA> dispatcher   mechanic\n190      20     15            7   3 12 <NA> dispatcher   mechanic\n191      24     20           13   3 13 <NA> dispatcher   mechanic\n205      13     16            7   3 27 <NA> dispatcher   mechanic\n218       8     20            8   3 40 <NA> dispatcher   custserv\n219      14     16            7   3 41 <NA> dispatcher   mechanic\n223      14     26           15   3 45 <NA> dispatcher   custserv\n224      25     16           12   3 46 <NA> dispatcher   mechanic\n226      14     16            6   3 48 <NA> dispatcher   mechanic\n230      16     19           12   3 52 <NA> dispatcher   mechanic\n231      21     15            7   3 53 <NA> dispatcher   mechanic\n232      18     23           15   3 54 <NA> dispatcher   mechanic\n234      17     18            9   3 56 <NA> dispatcher   mechanic\n241      16     22           12   3 63 <NA> dispatcher   mechanic\n244      18     20           10   3 66 <NA> dispatcher   mechanic\n    posterior.custserv posterior.dispatcher posterior.mechanic        x.LD1\n2         3.677743e-01         0.1432467601         0.48897890  0.148030225\n6         1.682521e-01         0.0469230463         0.78482488  0.220387584\n16        4.887584e-01         0.0047006562         0.50654091  1.489292781\n26        3.564096e-01         0.0036166753         0.63997368  1.449043794\n34        3.220516e-01         0.3632645747         0.31468387 -0.222828431\n40        9.523627e-02         0.0156806918         0.88908304  0.383555765\n47        2.955104e-01         0.1781956255         0.52629396 -0.020378089\n49        3.774214e-01         0.1227084023         0.49987015  0.213178216\n53        1.013578e-01         0.5274616137         0.37118058 -0.823340502\n54        1.373009e-01         0.0080150113         0.85468408  0.772104056\n55        1.302936e-01         0.0198604496         0.84984598  0.424974578\n57        3.052008e-01         0.0717529538         0.62304629  0.313335060\n61        4.375162e-01         0.4532249345         0.10925889 -0.140193803\n65        3.769863e-01         0.0041498345         0.61886390  1.423177963\n77        1.068060e-01         0.7582387752         0.13495523 -0.894528035\n81        2.979887e-01         0.0468990011         0.65511235  0.454910717\n85        4.695896e-01         0.0400178962         0.49039250  0.704020004\n88        6.570708e-01         0.0220178411         0.32091138  1.068839117\n89        8.245977e-01         0.0018316827         0.17357064  2.077355351\n90        8.507090e-01         0.0222471944         0.12704385  1.203205405\n91        4.444566e-05         0.9596984304         0.04025712 -4.030881267\n93        5.068084e-01         0.0508241360         0.44236742  0.652288343\n94        5.176276e-01         0.1503272946         0.33204511  0.281429686\n96        5.946078e-01         0.0224010768         0.38299117  1.016140628\n99        1.818092e-01         0.4722545433         0.34593630 -0.548365384\n100       9.232779e-01         0.0001474794         0.07657460  3.060005755\n102       7.274351e-01         0.0114338064         0.26113112  1.352957258\n103       2.236793e-01         0.5076274641         0.26869326 -0.482250566\n107       4.700075e-01         0.1664738951         0.36351861  0.202865367\n119       5.051718e-01         0.0236299959         0.47119820  0.924159979\n123       8.289650e-01         0.0329674992         0.13806754  1.048213420\n124       1.413975e-01         0.5585445851         0.30005787 -0.703357370\n126       1.945372e-01         0.4396159935         0.36584685 -0.497803548\n127       6.659222e-03         0.5770406009         0.41630018 -1.943460675\n138       5.162079e-01         0.0152550846         0.46853700  1.090431642\n146       1.547478e-01         0.4902367972         0.35501536 -0.626929704\n147       5.990222e-01         0.0196332324         0.38134453  1.066702464\n150       1.384048e-01         0.5930785867         0.26851663 -0.729223201\n153       6.677764e-01         0.0034775005         0.32874611  1.738402068\n161       6.574656e-03         0.5473649373         0.44606041 -1.932180998\n168       7.534177e-01         0.0001880719         0.24639427  2.847075147\n171       5.292573e-01         0.0068851106         0.46385759  1.386999284\n172       3.434966e-02         0.6633133705         0.30233696 -1.328565447\n183       5.068084e-01         0.0508241360         0.44236742  0.652288343\n187       4.391730e-02         0.3717385205         0.58434418 -1.047550786\n190       3.619326e-02         0.2215317792         0.74227496 -0.947393942\n191       1.290640e-02         0.1738047250         0.81328888 -1.273897723\n205       3.273139e-01         0.2622425873         0.41044354 -0.109255257\n218       8.699108e-01         0.0451516733         0.08493749  0.972752581\n219       2.624197e-01         0.2610486472         0.47653166 -0.201235906\n223       5.772471e-01         0.0920176042         0.33073532  0.501569662\n224       3.669802e-03         0.3209918356         0.67533836 -1.987982968\n226       3.046592e-01         0.1992891879         0.49605161 -0.046243920\n230       1.386787e-01         0.3651632252         0.49615806 -0.577334695\n231       2.611049e-02         0.1984310578         0.77545845 -1.039374590\n232       1.347321e-01         0.2417816494         0.62348623 -0.449175368\n234       1.401549e-01         0.2188119721         0.64103311 -0.398613532\n241       2.853946e-01         0.1586444090         0.55596102  0.005487741\n244       1.434582e-01         0.1499332152         0.70660857 -0.257037875\n          x.LD2\n2    0.15096436\n6   -1.07331266\n16  -1.23468418\n26  -1.68435616\n34   0.87559750\n40  -1.89805540\n47   0.08841658\n49   0.07619736\n53   0.39986962\n54  -1.99761750\n55  -1.62290339\n57  -0.43618819\n61   2.01253164\n65  -1.57202733\n77   1.44962668\n81  -0.67342173\n85  -0.29816011\n88  -0.04780320\n89  -0.49673681\n90   0.86447328\n91  -0.74015869\n93  -0.07350244\n94   0.65113069\n96  -0.23525568\n99   0.66280241\n100 -0.83334160\n102 -0.11016012\n103  1.00014556\n107  0.57600704\n119 -0.46026999\n123  0.95181616\n124  0.75014529\n126  0.61266471\n127 -0.82550241\n138 -0.63531240\n146  0.58767876\n147 -0.28539338\n150  0.86247412\n153 -0.85942256\n161 -0.91320194\n168 -1.82074173\n171 -0.95988882\n172  0.21222628\n183 -0.07350244\n187 -0.49983098\n190 -1.01221653\n191 -1.63712177\n205  0.51310261\n218  1.52639284\n219  0.28808829\n223  0.48902081\n224 -1.75035476\n226  0.20074542\n230  0.12543090\n231 -1.23723085\n232 -0.26169329\n234 -0.31183099\n241 -0.02391225\n244 -0.54906453\n```\n\n\n:::\n:::\n\n\nWhat fraction of people actually got misclassified? You could just pull\nout the numbers and add them up, but you know me: I'm too lazy to do\nthat.\n\nWe can work out the total number and fraction who got misclassified.\nThere are different ways you might do this, but the `tidyverse` way\nprovides the easiest starting point. For example, we can make a new\ncolumn that indicates whether a group is the correct or wrong\nclassification:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(job, class) %>%\n  mutate(job_stat = ifelse(job == class, \"correct\", \"wrong\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  job      class  n job_stat\n1   1   custserv 68    wrong\n2   1 dispatcher  4    wrong\n3   1   mechanic 13    wrong\n4   2   custserv 16    wrong\n5   2 dispatcher 10    wrong\n6   2   mechanic 67    wrong\n7   3   custserv  3    wrong\n8   3 dispatcher 50    wrong\n9   3   mechanic 13    wrong\n```\n\n\n:::\n:::\n\nFrom there, we count up the correct and wrong ones, recognizing that we\nwant to total up the *frequencies* in `n`, not just count the number of\nrows:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(job, class) %>%\n  mutate(job_stat = ifelse(job == class, \"correct\", \"wrong\")) %>%\n  count(job_stat, wt = n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  job_stat   n\n1    wrong 244\n```\n\n\n:::\n:::\n\nand turn these into proportions:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(job, class) %>%\n  mutate(job_stat = ifelse(job == class, \"correct\", \"wrong\")) %>%\n  count(job_stat, wt = n) %>%\n  mutate(proportion = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  job_stat   n proportion\n1    wrong 244          1\n```\n\n\n:::\n:::\n\nThere is a `count` followed by another `count` of the first lot of\ncounts, so the second count column has taken over the name `n`.\n\n24% of all the employees got classified into the wrong job, based on\ntheir scores on `outdoor`, `social` and `conservative`.\n\nThis is actually not bad, from one point of view: if you just guessed\nwhich job each person did, without looking at their scores on the scales\nat all, you would get ${1\\over 3}=33\\%$ of them right, just by luck, and\n${2\\over3}=67\\%$ of them wrong. From 67% to 24% error is a big\nimprovement, and *that* is what the MANOVA is reacting to.\n\nTo figure out whether some of the groups were harder to classify than\nothers, squeeze a `group_by` in early to do the counts and proportions\nfor each (true) job:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(job, class) %>%\n  mutate(job_stat = ifelse(job == class, \"correct\", \"wrong\")) %>%\n  group_by(job) %>%\n  count(job_stat, wt = n) %>%\n  mutate(proportion = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 4\n# Groups:   job [3]\n    job job_stat     n proportion\n  <dbl> <chr>    <int>      <dbl>\n1     1 wrong       85          1\n2     2 wrong       93          1\n3     3 wrong       66          1\n```\n\n\n:::\n:::\n\nor even split out the correct and wrong ones into their own columns:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(job, class) %>%\n  mutate(job_stat = ifelse(job == class, \"correct\", \"wrong\")) %>%\n  group_by(job) %>%\n  count(job_stat, wt = n) %>%\n  mutate(proportion = n / sum(n)) %>%\n  select(-n) %>%\n  pivot_wider(names_from=job_stat, values_from=proportion)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 2\n# Groups:   job [3]\n    job wrong\n  <dbl> <dbl>\n1     1     1\n2     2     1\n3     3     1\n```\n\n\n:::\n:::\n\nThe mechanics were hardest to get right and easiest to get wrong, though\nthere isn't much in it. I think the reason is that the mechanics were\nsort of \"in the middle\" in that a mechanic could be mistaken for either\na dispatcher or a customer service representative, but but customer\nservice and dispatchers were more or less distinct from each other.\n\nIt's up to you whether you prefer to do this kind of thing by learning\nenough about `table` to get it to work, or whether you want to use\ntidy-data mechanisms to do it in a larger number of smaller steps. I\nimmediately thought of `table` because I knew about it, but the\ntidy-data way is more consistent with the way we have been doing things.\n\n$\\blacksquare$\n\n(j) Consider an employee with these scores: 20 on `outdoor`, 17 on\n    `social` and 8 on `conservative` What job do you think they do, and\n    how certain are you about that? Use `predict`, first making a data\n    frame out of the values to predict for.\n\nSolution\n\nThis is in fact exactly the same idea as the data frame that I generally\ncalled `new` when doing predictions for other models. I think the\nclearest way to make one of these is with `tribble`:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- tribble(\n  ~outdoor, ~social, ~conservative,\n  20, 17, 8\n)\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 3\n  outdoor social conservative\n    <dbl>  <dbl>        <dbl>\n1      20     17            8\n```\n\n\n:::\n:::\n\nThere's no need for `datagrid` or `crossing` here because I'm not doing\ncombinations of things. (I might have done that here, to get a sense for\nexample of \"what effect does a higher score on `outdoor` have on the\nlikelihood of a person doing each job?\". But I didn't.\n\n::: {.cell}\n\n:::\n\nThen feed this into `predict` as the *second* thing:\n\n::: {.cell}\n\n```{.r .cell-code}\npp1 <- predict(job.1, new)\n```\n:::\n\nOur predictions are these:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(new, pp1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  outdoor social conservative    class posterior.custserv posterior.dispatcher\n1      20     17            8 mechanic         0.05114665            0.1687909\n  posterior.mechanic      x.LD1     x.LD2\n1          0.7800624 -0.7138376 -1.024436\n```\n\n\n:::\n:::\n\nThe `class` thing gives our predicted job, and the `posterior`\nprobabilities say how sure we are about that. So we reckon there's a 78%\nchance that this person is a mechanic; they might be a dispatcher but\nthey are unlikely to be in customer service. Our best guess is that they\nare a mechanic.[^_jobs-4]\n\n[^_jobs-4]: I discovered that I used *pp* twice, and I want to use the\n    first one again later, so I had to rename this one.\n\nDoes this pass the sanity-check test? First figure out where our new\nemployee stands compared to the others:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(jobs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    outdoor          social       conservative        job       \n Min.   : 0.00   Min.   : 7.00   Min.   : 0.00   Min.   :1.000  \n 1st Qu.:13.00   1st Qu.:17.00   1st Qu.: 8.00   1st Qu.:1.000  \n Median :16.00   Median :21.00   Median :11.00   Median :2.000  \n Mean   :15.64   Mean   :20.68   Mean   :10.59   Mean   :1.922  \n 3rd Qu.:19.00   3rd Qu.:25.00   3rd Qu.:13.00   3rd Qu.:3.000  \n Max.   :28.00   Max.   :35.00   Max.   :20.00   Max.   :3.000  \n       id             X6              jobname         \n Min.   : 1.00   Length:244         Length:244        \n 1st Qu.:21.00   Class :character   Class :character  \n Median :41.00   Mode  :character   Mode  :character  \n Mean   :41.95                                        \n 3rd Qu.:61.25                                        \n Max.   :93.00                                        \n```\n\n\n:::\n:::\n\nTheir score on `outdoor` is above average, but their scores on the other\ntwo scales are below average (right on the 1st quartile in each case).\n\nGo back to the table of means from the discriminant analysis output. The\nmechanics have the highest average for `outdoor`, they're in the middle\non `social` and they are lowish on `conservative`. Our new employee is\nat least somewhat like that.\n\nOr, we can figure out where our new employee sits on the plot. The\noutput from `predict` gives the predicted `LD1` and `LD2`, which are\n0.71 and $-1.02$ respectively. This employee would sit to the right of\nand below the middle of the plot: in the greens, but with a few blues\nnearby: most likely a mechanic, possibly a dispatcher, but likely not\ncustomer service, as the posterior probabilities suggest.\n\nExtra: I can use the same mechanism to predict for a combination of\nvalues. This would allow for the variability of each of the original\nvariables to differ, and enable us to assess the effect of, say, a\nchange in `conservative` over its \"typical range\", which we found out\nabove with `summary(jobs)`. I'll take the quartiles, in my usual\nfashion:\n\n::: {.cell}\n\n```{.r .cell-code}\noutdoors <- c(13, 19)\nsocials <- c(17, 25)\nconservatives <- c(8, 13)\n```\n:::\n\nThe IQRs are not that different, which says that what we get here will\nnot be that different from the \\`\\`coefficients of linear\ndiscriminants'' above:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- crossing(\n  outdoor = outdoors, social = socials,\n  conservative = conservatives\n)\npp2 <- predict(job.1, new)\npx <- round(pp2$x, 2)\ncbind(new, pp2$class, px)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  outdoor social conservative  pp2$class   LD1   LD2\n1      13     17            8   mechanic -0.07  0.55\n2      13     17           13 dispatcher -0.84  0.99\n3      13     25            8   custserv  1.48  0.15\n4      13     25           13   custserv  0.71  0.59\n5      19     17            8   mechanic -0.62 -0.80\n6      19     17           13 dispatcher -1.40 -0.36\n7      19     25            8   mechanic  0.93 -1.20\n8      19     25           13   mechanic  0.16 -0.76\n```\n\n\n:::\n:::\n\nThe highest (most positive) LD1 score goes with high outdoor, low\nsocial, high conservative (and being a dispatcher). It is often\ninteresting to look at the *second*-highest one as well: here that is\n*low* outdoor, and the same low social and high conservative as before.\nThat means that `outdoor` has nothing much to do with `LD1` score. Being\nlow `social` is strongly associated with `LD1` being positive, so that's\nthe important part of `LD1`.\n\nWhat about `LD2`? The most positive LD2 are these:\n\n```         \n\nLD2    outdoor  social  conservative\n====================================\n0.99   low      low     high\n0.59   low      high    high\n0.55   low      low     low\n```\n\nThese most consistently go with `outdoor` being low.\n\nIs that consistent with the \"coefficients of linear discriminants\"?\n\n::: {.cell}\n\n```{.r .cell-code}\njob.1$scaling\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     LD1         LD2\nsocial        0.19427415 -0.04978105\noutdoor      -0.09198065 -0.22501431\nconservative -0.15499199  0.08734288\n```\n\n\n:::\n:::\n\nVery much so: `outdoor` has nothing much to do with `LD1` and everything\nto do with `LD2`.\n\n$\\blacksquare$\n\n\n\n##  Observing children with ADHD\n\n\n A number of children with ADHD were observed by their mother\nor their father (only one parent observed each child). Each parent was\nasked to rate occurrences of behaviours of four different types,\nlabelled `q1` through `q4` in the data set. Also\nrecorded was the identity of the parent doing the observation for each\nchild: 1 is father, 2 is mother.\n\nCan we tell (without looking at the `parent` column) which\nparent is doing the observation? Research suggests that rating the\ndegree of impairment in different categories depends on who is doing\nthe rating: for example, mothers may feel that a  child has difficulty\nsitting still, while fathers, who might do more observing of a child\nat play, might think of such a child as simply being \"active\" or\n\"just being a kid\". The data are in\n[link](http://ritsokiguess.site/datafiles/adhd-parents.txt). \n\n\n\n(a) Read in the data and confirm that you have four ratings and\na column labelling the parent who made each observation.\n\n\nSolution\n\n\nAs ever:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"https://ritsokiguess.site/datafiles/adhd-parents.txt\"\nadhd <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 29 Columns: 5\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (1): parent\ndbl (4): q1, q2, q3, q4\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nadhd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 29 x 5\n   parent    q1    q2    q3    q4\n   <chr>  <dbl> <dbl> <dbl> <dbl>\n 1 father     2     1     3     1\n 2 mother     1     3     1     1\n 3 father     2     1     3     1\n 4 mother     3     2     3     3\n 5 mother     3     3     2     1\n 6 mother     1     3     3     1\n 7 mother     3     3     1     1\n 8 mother     2     3     1     1\n 9 mother     1     3     3     1\n10 mother     3     3     3     3\n# i 19 more rows\n```\n\n\n:::\n:::\n\n     \n\nYes, exactly that.\n    \n\n$\\blacksquare$\n\n(b) Run a suitable discriminant analysis and display the output.\n\n\nSolution\n\n\nThis is as before:\n\n::: {.cell}\n\n```{.r .cell-code}\nadhd.1 <- lda(parent ~ q1 + q2 + q3 + q4, data = adhd)\nadhd.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(parent ~ q1 + q2 + q3 + q4, data = adhd)\n\nPrior probabilities of groups:\n   father    mother \n0.1724138 0.8275862 \n\nGroup means:\n          q1       q2       q3    q4\nfather 1.800 1.000000 1.800000 1.800\nmother 2.375 2.791667 1.958333 1.625\n\nCoefficients of linear discriminants:\n          LD1\nq1 -0.3223454\nq2  2.3219448\nq3  0.1411360\nq4  0.1884613\n```\n\n\n:::\n:::\n\n     \n    \n\n$\\blacksquare$\n\n(c) Which behaviour item or items seem to be most helpful at\ndistinguishing the parent making the observations? Explain briefly.\n\n\nSolution\n\n\nLook at the Coefficients of Linear Discriminants. The coefficient\nof `q2`, 2.32, is much larger in size than the others, so\nit's really `q2` that distinguishes mothers and fathers.\nNote also that the group means for fathers and mothers are fairly\nclose on all the items except for `q2`, which are a long\nway apart. So that's another hint that it might be `q2`\nthat makes the difference. But that might be deceiving: one of the\nother `q`s, even though the means are close for mothers and\nfathers, might actually do a good job of distinguishing mothers\nfrom fathers, because it has a small SD overall.\n    \n\n$\\blacksquare$\n\n(d) Obtain the predictions from the `lda`, and make a\nsuitable plot of the discriminant scores, bearing in mind that you\nonly have one `LD`.  Do you think there will be any\nmisclassifications? Explain briefly.\n\n\nSolution\n\n\nThe prediction is the obvious thing. I take a quick look at it\n(using `glimpse`), but only because I feel like it:\n\n::: {.cell}\n\n```{.r .cell-code}\nadhd.2 <- predict(adhd.1)\nglimpse(adhd.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 3\n $ class    : Factor w/ 2 levels \"father\",\"mother\": 1 2 1 2 2 2 2 2 2 2 ...\n $ posterior: num [1:29, 1:2] 9.98e-01 5.57e-06 9.98e-01 4.97e-02 4.10e-05 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:29] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:2] \"father\" \"mother\"\n $ x        : num [1:29, 1] -3.327 1.357 -3.327 -0.95 0.854 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:29] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr \"LD1\"\n```\n\n\n:::\n:::\n\n     \n\nThe discriminant scores are in the thing called `x` in\nthere. There is only `LD1` (only two groups, mothers and\nfathers), so the right way to plot it is against the true groups, eg.\nby a boxplot, first making a data frame, using `data.frame`,\ncontaining what you need:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- cbind(adhd, adhd.2)\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  parent q1 q2 q3 q4  class posterior.father posterior.mother        LD1\n1 father  2  1  3  1 father     9.984540e-01      0.001545972 -3.3265660\n2 mother  1  3  1  1 mother     5.573608e-06      0.999994426  1.3573971\n3 father  2  1  3  1 father     9.984540e-01      0.001545972 -3.3265660\n4 mother  3  2  3  3 mother     4.971864e-02      0.950281356 -0.9500439\n5 mother  3  3  2  1 mother     4.102507e-05      0.999958975  0.8538422\n6 mother  1  3  3  1 mother     1.820430e-06      0.999998180  1.6396690\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(d, aes(x = parent, y = LD1)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/adhd-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n \nThe fathers look to be a very compact group with `LD1` score\naround $-3$, so I don't foresee any problems there. The mothers, on\nthe other hand, have outliers: there is one with `LD1` score\nbeyond $-3$ that will certainly be mistaken for a father. There are a\ncouple of other unusual `LD1` scores among the mothers, but a\nrule like \n\"anything above $-2$ is called a mother, anything below is called a father\" \nwill get these two right. So I expect that the one\nvery low mother will get misclassified, but that's the only one.\n    \n\n$\\blacksquare$\n\n(e) Obtain the predicted group memberships and make a table of\nactual vs.\\ predicted. Were there any misclassifications? Explain\nbriefly. \n\n\nSolution\n\n\nUse the predictions from the previous part, and the observed\n`parent` values from the original data frame. Then use\neither `table` or `tidyverse` to summarize.\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(d, table(obs = parent, pred = class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        pred\nobs      father mother\n  father      5      0\n  mother      1     23\n```\n\n\n:::\n:::\n\n     \n\nOr,\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% count(parent, class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  parent  class  n\n1 father father  5\n2 mother father  1\n3 mother mother 23\n```\n\n\n:::\n:::\n\n \n\nor\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(parent, class) %>%\n  pivot_wider(names_from=class, values_from=n, values_fill = list(n=0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 3\n  parent father mother\n  <chr>   <int>  <int>\n1 father      5      0\n2 mother      1     23\n```\n\n\n:::\n:::\n\n \nOne of the mothers got classified as a father (evidently that one with\na very negative `LD1` score), but everything else is correct.\n\nThis time, by \"explain briefly\" I mean something like \n\"tell me how you know there are or are not misclassifications\", or \n\"describe any misclassifications that occur\" or something like that. \n\nExtra: I was curious --- what is it about that one mother that caused\nher to get misclassified? (I didn't ask you to think further about\nthis, but in case you are curious as well.) \n\nFirst, which mother *was* it? Let's begin by adding the predicted\nclassification to the data frame, and then we can query it by asking\nto see only the rows where the actual parent and the predicted parent\nwere different. I'm also going to create a column `id` that\nwill give us the row of the *original* data frame:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  mutate(id = row_number()) %>%\n  filter(parent != class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   parent q1 q2 q3 q4  class posterior.father posterior.mother       LD1 id\n17 mother  1  1  2  1 father        0.9968343      0.003165699 -3.145357 17\n```\n\n\n:::\n:::\n\n \n\nIt was the original row 17. So what was unusual about this? We know\nfrom earlier\nthat behaviour `q2` was the one that generally distinguished\nmothers from fathers, so maybe we should find the mean and SD of scores for\nmothers and fathers on `q2`:\n\n::: {.cell}\n\n```{.r .cell-code}\nadhd %>% group_by(parent) %>% summarize(m2 = mean(q2), s2 = sd(q2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 3\n  parent    m2    s2\n  <chr>  <dbl> <dbl>\n1 father  1    0    \n2 mother  2.79 0.509\n```\n\n\n:::\n:::\n\n \n\nThe fathers' scores on `q2` were *all* 1, but the mothers'\nscores on `q2` were on average much higher. So it's not really\na surprise  that this mother was mistaken for a father.\n    \n\n$\\blacksquare$\n\n(f) Re-run the discriminant analysis using cross-validation,\nand again obtain a table of actual and predicted parents. Is the\npattern of misclassification different from before? Hints: (i) Bear in mind\nthat there is no `predict` step this time, because the\ncross-validation output includes predictions; (ii) use a different name\nfor the predictions this time because we are going to do a\ncomparison in a moment.\n\n\nSolution\n\n\nSo, this, with different name:\n\n::: {.cell}\n\n```{.r .cell-code}\nadhd.3 <- lda(parent ~ q1 + q2 + q3 + q4, data = adhd, CV = T)\ndd <- cbind(adhd, class = adhd.3$class, posterior = adhd.3$posterior)\nwith(dd, table(parent, class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        class\nparent   father mother\n  father      5      0\n  mother      1     23\n```\n\n\n:::\n:::\n\n     \n\nIt's exactly the same pattern of misclassification. (In fact, it's\nexactly the same mother being misclassified as a father.)\n\nThis one is the same *not* because of having lots of data. In\nfact, as you see below, having a small data set makes quite a bit of\ndifference to the posterior probabilities (where they are not close to\n1 or 0), but the decisions about whether the parents are a mother or a\nfather are clear-cut enough that none of *those* change. Even\nthough (some of) the posterior probabilities are noticeably changed,\nwhich one is the bigger has not changed at all.\n    \n\n$\\blacksquare$\n\n(g) Display the original data (that you read in from the data\nfile) side by side with two sets of posterior probabilities: the\nones that you obtained with `predict` before, and the ones\nfrom the cross-validated analysis. Comment briefly on whether the\ntwo sets of posterior probabilities are similar. Hints: (i) use\n`data.frame` rather than `cbind`, for reasons that I\nexplain elsewhere; (ii) round the posterior probabilities to 3\ndecimals before you display them.\nThere are only 29 rows, so look at them all. I am going to add the\n`LD1` scores to my output and sort by that, but you don't\nneed to. (This is for something I am going to add later.)\n\n\nSolution\n\n\n\nWe have two data frames, `d` and \n`dd`^[I have to learn to come up with better names.]  \nthat respectively\ncontain everything from the (original) `lda` output and the\ncross-validated output. Let's glue them together, look at what we\nhave, and then pull out what we need:\n\n::: {.cell}\n\n```{.r .cell-code}\nall <- data.frame(d, dd)\nhead(all)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  parent q1 q2 q3 q4  class posterior.father posterior.mother        LD1\n1 father  2  1  3  1 father     9.984540e-01      0.001545972 -3.3265660\n2 mother  1  3  1  1 mother     5.573608e-06      0.999994426  1.3573971\n3 father  2  1  3  1 father     9.984540e-01      0.001545972 -3.3265660\n4 mother  3  2  3  3 mother     4.971864e-02      0.950281356 -0.9500439\n5 mother  3  3  2  1 mother     4.102507e-05      0.999958975  0.8538422\n6 mother  1  3  3  1 mother     1.820430e-06      0.999998180  1.6396690\n  parent.1 q1.1 q2.1 q3.1 q4.1 class.1 posterior.father.1 posterior.mother.1\n1   father    2    1    3    1  father       9.958418e-01        0.004158233\n2   mother    1    3    1    1  mother       5.036602e-06        0.999994963\n3   father    2    1    3    1  father       9.958418e-01        0.004158233\n4   mother    3    2    3    3  mother       2.359247e-01        0.764075258\n5   mother    3    3    2    1  mother       5.702541e-05        0.999942975\n6   mother    1    3    3    1  mother       8.430421e-07        0.999999157\n```\n\n\n:::\n:::\n\n \nThe ones with a 1 on the end are the cross-validated ones. We need the posterior probabilities, rounded, and they need to  have shorter names:\n\n::: {.cell}\n\n```{.r .cell-code}\nall %>%\n  select(parent, starts_with(\"posterior\"), LD1) %>%\n  mutate(across(starts_with(\"posterior\"), \\(x) round(x, 3))) %>%\n  rename_with(\n    ~ str_replace(., \"posterior\", \"p\"),\n    starts_with(\"posterior\"),\n  ) %>%\n  arrange(LD1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   parent p.father p.mother p.father.1 p.mother.1        LD1\n19 father    0.999    0.001      0.999      0.001 -3.6088379\n1  father    0.998    0.002      0.996      0.004 -3.3265660\n3  father    0.998    0.002      0.996      0.004 -3.3265660\n27 father    0.998    0.002      0.994      0.006 -3.2319152\n17 mother    0.997    0.003      1.000      0.000 -3.1453565\n16 father    0.992    0.008      0.958      0.042 -2.9095698\n4  mother    0.050    0.950      0.236      0.764 -0.9500439\n23 mother    0.043    0.957      0.107      0.893 -0.9099704\n13 mother    0.015    0.985      0.030      0.970 -0.6349504\n7  mother    0.000    1.000      0.000      1.000  0.7127063\n20 mother    0.000    1.000      0.000      1.000  0.7127063\n21 mother    0.000    1.000      0.000      1.000  0.7127063\n5  mother    0.000    1.000      0.000      1.000  0.8538422\n14 mother    0.000    1.000      0.000      1.000  0.8538422\n12 mother    0.000    1.000      0.000      1.000  0.9011676\n15 mother    0.000    1.000      0.000      1.000  0.9949782\n18 mother    0.000    1.000      0.000      1.000  0.9949782\n22 mother    0.000    1.000      0.000      1.000  0.9949782\n28 mother    0.000    1.000      0.000      1.000  0.9949782\n8  mother    0.000    1.000      0.000      1.000  1.0350517\n29 mother    0.000    1.000      0.000      1.000  1.0350517\n11 mother    0.000    1.000      0.000      1.000  1.2307649\n24 mother    0.000    1.000      0.000      1.000  1.2307649\n25 mother    0.000    1.000      0.000      1.000  1.2307649\n2  mother    0.000    1.000      0.000      1.000  1.3573971\n10 mother    0.000    1.000      0.000      1.000  1.3719009\n26 mother    0.000    1.000      0.000      1.000  1.5458584\n6  mother    0.000    1.000      0.000      1.000  1.6396690\n9  mother    0.000    1.000      0.000      1.000  1.6396690\n```\n\n\n:::\n:::\n\n \n\nThe `rename_with` changes the names of the columns that start\nwith `posterior` to start with `p` instead (shorter). I\nlearned about this today (having wondered whether it existed or not),\nand it took about three goes for me to get it right.^[`str-replace` is from `stringr`, and takes three inputs: a piece of text, the text to look for, and the text to replace it with. The piece of text in this case is one of the columns whose name starts with `posterior`; the dot represents *it*, that is to say \"one of the columns whose name starts with `posterior`.]\nThe first column is the actual parent; the other five columns are: the\nposterior probabilities from before, for father and for mother (two\ncolumns), and the posterior probabilities from cross-validation for\nfather and for mother (two more columns), and the LD1 scores from\nbefore, sorted into order.  You might have these the other way around\nfrom me, but in any case you ought to make it clear which is which. I\nincluded the `LD1` scores for my discussion below; you don't\nneed to.\nAre the two sets of posterior probabilities similar? Only kinda. The\nones at the top and bottom of the list are without doubt respectively\nfathers at the top of the list (top 5 rows on my sorted output, except that\none of those is actually a mother), or mothers at the bottom, from row\n10 down. But for rows 6 through 9, the posterior probabilities are not\nthat similar.  The most dissimilar ones are in row 4, where the\nregular `lda` gives a posterior probability of 0.050 that the\nparent is a father, but under cross-validation that goes all the way\nup to 0.236. I think this is one of those mothers that is a bit like a\nfather: her score on `q2` was only 2, compared to 3 for most of\nthe mothers. If you take out this mother, as cross-validation does,\nthere are noticeably fewer `q2=2` mothers left, so the\nobservation looks more like a father than it would otherwise.\n    \n\n$\\blacksquare$\n\n(h) Row 17 of your (original) data frame above, row 5 of the\noutput in the previous part, is the mother that was\nmisclassified as a father. Why is it that the cross-validated\nposterior probabilities are 1 and 0, while the previous posterior\nprobabilities are a bit less than 1 and a bit more than 0?\n\n\nSolution\n\n\nIn making the classification, the non-cross-validated procedure\nuses all the data, so that parent \\#17 suggests that the mothers are\nvery variable on `q2`, so it is conceivable (though still\nunlikely) that this parent actually is a mother. \nUnder cross-validation, however, parent \\#17 is\n*omitted*. This mother is nothing like any of the other\nmothers, or, to put it another way, the remaining mothers as a\ngroup are very far away from this one, so \\#17 doesn't look like a\nmother *at all*.\n    \n\n$\\blacksquare$\n\n(i) Find the parents where the cross-validated posterior\nprobability of being a father is \"non-trivial\": that is, not\nclose to zero and not close to 1. (You will have to make a judgement\nabout what \"close to zero or 1\" means for you.) What do these\nparents have in common, all of them or most of them?\n\n\nSolution\n\n\nLet's add something to the output we had before: the original\nscores on `q1` through `q4`:\n\n::: {.cell}\n\n```{.r .cell-code}\nall %>%\n  select(q1:q4, parent, starts_with(\"posterior\"), LD1) %>%\n  mutate(across(starts_with(\"posterior\"), \\(x) round(x, 3))) %>%\n  rename_with(\n    \\(y) str_replace(y, \"posterior\", \"p\"),\n    starts_with(\"posterior\")) %>%\n  arrange(LD1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   q1 q2 q3 q4 parent p.father p.mother p.father.1 p.mother.1        LD1\n19  2  1  1  1 father    0.999    0.001      0.999      0.001 -3.6088379\n1   2  1  3  1 father    0.998    0.002      0.996      0.004 -3.3265660\n3   2  1  3  1 father    0.998    0.002      0.996      0.004 -3.3265660\n27  2  1  1  3 father    0.998    0.002      0.994      0.006 -3.2319152\n17  1  1  2  1 mother    0.997    0.003      1.000      0.000 -3.1453565\n16  1  1  1  3 father    0.992    0.008      0.958      0.042 -2.9095698\n4   3  2  3  3 mother    0.050    0.950      0.236      0.764 -0.9500439\n23  2  2  1  3 mother    0.043    0.957      0.107      0.893 -0.9099704\n13  1  2  2  2 mother    0.015    0.985      0.030      0.970 -0.6349504\n7   3  3  1  1 mother    0.000    1.000      0.000      1.000  0.7127063\n20  3  3  1  1 mother    0.000    1.000      0.000      1.000  0.7127063\n21  3  3  1  1 mother    0.000    1.000      0.000      1.000  0.7127063\n5   3  3  2  1 mother    0.000    1.000      0.000      1.000  0.8538422\n14  3  3  2  1 mother    0.000    1.000      0.000      1.000  0.8538422\n12  3  3  1  2 mother    0.000    1.000      0.000      1.000  0.9011676\n15  3  3  3  1 mother    0.000    1.000      0.000      1.000  0.9949782\n18  3  3  3  1 mother    0.000    1.000      0.000      1.000  0.9949782\n22  3  3  3  1 mother    0.000    1.000      0.000      1.000  0.9949782\n28  3  3  3  1 mother    0.000    1.000      0.000      1.000  0.9949782\n8   2  3  1  1 mother    0.000    1.000      0.000      1.000  1.0350517\n29  2  3  1  1 mother    0.000    1.000      0.000      1.000  1.0350517\n11  3  3  2  3 mother    0.000    1.000      0.000      1.000  1.2307649\n24  3  3  2  3 mother    0.000    1.000      0.000      1.000  1.2307649\n25  3  3  2  3 mother    0.000    1.000      0.000      1.000  1.2307649\n2   1  3  1  1 mother    0.000    1.000      0.000      1.000  1.3573971\n10  3  3  3  3 mother    0.000    1.000      0.000      1.000  1.3719009\n26  1  3  1  2 mother    0.000    1.000      0.000      1.000  1.5458584\n6   1  3  3  1 mother    0.000    1.000      0.000      1.000  1.6396690\n9   1  3  3  1 mother    0.000    1.000      0.000      1.000  1.6396690\n```\n\n\n:::\n:::\n\n     \nTo my mind, the \"non-trivial\" posterior probabilities are in rows 5\nthrough 9. (You might have drawn the line in a different place.) These\nare the ones where there was some doubt, though maybe only a little,\nabout which parent actually gave the ratings. For three of these,\nthe parent (that was actually a mother) gave a rating of\n2 on `q2`. These were the only 2's on `q2`. The others\nwere easy to call: \"mother\" if 3 and \"father\" if 1, and you'd get\nthem all right except for that outlying mother.\nThe clue in looking at `q2` was that we found earlier that\n`LD1` contained mostly `q2`, so that it was mainly\n`q2` that separated the fathers and mothers. If you found\nsomething else that the \"non-trivial\" rows had in common, that is\ngood too, but I think looking at `q2` is your quickest route to\nan answer. (`q1=1` picks out some of these, but not all of\nthem.)\nThis is really the same kind of issue as we discussed when\ncomparing the posterior probabilities for `lda` and\ncross-validation above: there were only a few parents with\n`q2=2`, so the effect there is that under cross-validation,\nthere are even fewer when you take one of them out.\n    \n$\\blacksquare$\n\n\n\n\n\n\n\n##  Growing corn\n\n\nA new type of corn seed has been developed.\nThe people developing it want to know if the type of soil the seed\nis planted in has an impact on how well the seed performs, and if so,\nwhat kind of impact. Three\noutcome measures were used: the yield of corn produced (from a fixed\namount of seed), the amount of water needed, and the amount of\nherbicide needed. The data are in\n[link](http://ritsokiguess.site/datafiles/cornseed.csv). 32 fields\nwere planted with the seed, 8 fields with each soil type.\n\n\n\n(a) Read in the data and verify that you have 32 observations\nwith the correct variables.\n\n\nSolution\n\n\nThe usual:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"https://ritsokiguess.site/datafiles/cornseed.csv\"\ncornseed <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 32 Columns: 5\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): soil\ndbl (4): field, yield, water, herbicide\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ncornseed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 32 x 5\n   field soil  yield water herbicide\n   <dbl> <chr> <dbl> <dbl>     <dbl>\n 1     1 loam   76.7  29.5       7.5\n 2     2 loam   60.5  32.1       6.3\n 3     3 loam   96.1  40.7       4.2\n 4     4 loam   88.1  45.1       4.9\n 5     5 loam   50.2  34.1      11.7\n 6     6 loam   55    31.1       6.9\n 7     7 loam   65.4  21.6       4.3\n 8     8 loam   65.7  27.7       5.3\n 9     9 sandy  67.3  48.3       5.5\n10    10 sandy  61.3  28.9       6.9\n# i 22 more rows\n```\n\n\n:::\n:::\n\n     \n\nWe have 32 rows; we have a categorical soil type, three\nnumerical columns containing the yield, water and herbicide values,\nand we also have a label for each of the 32 fields (which is actually\na number, but we don't have to worry about that, since we won't be\nusing `field` for anything).\n    \n$\\blacksquare$\n\n(b) Run a multivariate analysis of variance to see whether\nthe type of soil has any effect on any of the variables. What do you\nconclude from it?\n\n\nSolution\n\n\nThe usual thing: create the response, use `manova` (or\n`Manova` from `car` if you like, but it's not necessary):\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(cornseed, cbind(yield, water, herbicide))\ncornseed.1 <- manova(response ~ soil, data = cornseed)\nsummary(cornseed.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Df Pillai approx F num Df den Df  Pr(>F)  \nsoil       3 0.5345   2.0234      9     84 0.04641 *\nResiduals 28                                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n     \n\nWith a P-value (just) less than 0.05, soil type has some effect on the\nresponse variables: that is, it affects one or more of the three\nresponses, or some combination of them. ANOVA conclusions are usually\nvague, and MANOVA conclusions are vaguer than most. We will try to\nimprove on this. But with an only-just-significant P-value, we should\nnot be expecting miracles.\n\nWe ought to check Box's M test:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(BoxM(response, cornseed$soil))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Box's M Test \n\nChi-Squared Value = 12.07432 , df = 18  and p-value: 0.843 \n```\n\n\n:::\n:::\n\nNo problems with unequal variances or correlations, at least.\n\nHere and below, `field` is neither a response variable nor an\nexplanatory variable; it is an experimental unit, so `field`\nacts as an ID rather than anything else. So `field` should not\nbe part of any of the analyses; if it did appear, the only way it\ncould is as a factor, for example if this was somehow a repeated\nmeasures analysis over the three response variables. In that case,\n`lmer`, if you were going that way, would use `field` as\na random effect.\n\nThe variables to include are the\nyield, water and herbicide as measured response variables, and soil\ntype, as the categorical explanatory variable. (For the discriminant\nanalysis, these get turned around: the grouping variable `soil`\nacts like a response and the others act as explanatory.)\n    \n$\\blacksquare$\n\n(c) Run a discriminant analysis on these data, \"predicting\"\nsoil type from the three response variables. Display the results.\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncornseed.2 <- lda(soil ~ yield + water + herbicide, data = cornseed)\ncornseed.2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(soil ~ yield + water + herbicide, data = cornseed)\n\nPrior probabilities of groups:\n clay  loam salty sandy \n 0.25  0.25  0.25  0.25 \n\nGroup means:\n        yield   water herbicide\nclay  58.8375 33.0875    4.0875\nloam  69.7125 32.7375    6.3875\nsalty 55.3125 30.6375    3.8625\nsandy 62.5750 28.2000    4.3500\n\nCoefficients of linear discriminants:\n                  LD1         LD2         LD3\nyield     -0.08074845 -0.02081174 -0.04822432\nwater      0.03759961  0.09598577 -0.03231897\nherbicide -0.50654017  0.06979662  0.27281743\n\nProportion of trace:\n   LD1    LD2    LD3 \n0.9487 0.0456 0.0057 \n```\n\n\n:::\n:::\n\n     \n\nNo `field` in here, for reasons discussed above. (I'm not even\nsure how you *can* run a discriminant analysis with a factor\nvariable on the right of the squiggle.) The fields were numbered by\nsoil type:\n\n::: {.cell}\n\n```{.r .cell-code}\ncornseed %>% select(field, soil)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 32 x 2\n   field soil \n   <dbl> <chr>\n 1     1 loam \n 2     2 loam \n 3     3 loam \n 4     4 loam \n 5     5 loam \n 6     6 loam \n 7     7 loam \n 8     8 loam \n 9     9 sandy\n10    10 sandy\n# i 22 more rows\n```\n\n\n:::\n:::\n\n \n\nso evidently if you know the field number you can guess the field\ntype, but we didn't care about that: we cared about whether you can\ndistinguish the fields by yield, water, herbicide or combination\nthereof. \n    \n$\\blacksquare$\n\n(d) <a name=\"part:corn-svd\">*</a> \nWhich linear discriminants seem to be worth paying attention to?\nWhy did you get three linear discriminants? Explain briefly.\n\n\nSolution\n\n\nLook for  \"proportion of trace\" in the output.\n\nThe first one is *way* bigger than the others, which says that\nthe first linear discriminant is way more important (at separating the\ngroups) than either of the other two.\n\nAs to why we got three: there are 3 variables and 4 groups (soil\ntypes), and the smaller of 3 and $4-1$ is 3.\n    \n$\\blacksquare$\n\n(e) Which response variables do the important linear\ndiscriminants depend on? Answer this by extracting something from\nyour discriminant analysis output.\n\n\nSolution\n\n\nThe table \"coefficients of linear discriminants\".\nWe said earlier that the only important discriminant is\n`LD1`. On that, the only notably non-zero coefficient is for\n`herbicide`; the ones for `yield` and `water` are\nclose to zero. That is to say, the effects of the soil types play out\nthrough herbicide and not either of the other two variables.\n\nI didn't ask you to, but you could check this by seeing how\n`herbicide` differs according to soil type:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(cornseed, aes(x = soil, y = herbicide)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/cornseed-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThe fields in `loam` soil needed more herbicide than the others.\n\nOr by `water`:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(cornseed, aes(x = soil, y = water)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/cornseed-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThere isn't much difference in the amount of water needed between any\nof the fields, no matter what soil type. \n\nThis confirms that `water` is not distinguished by soil type,\nwhile `herbicide` is (at least to some extent).\n    \n$\\blacksquare$\n\n(f) Obtain predictions for the discriminant analysis. (You\ndon't need to do anything with them yet.)\n\n\nSolution\n\n\nJust this, therefore:\n\n::: {.cell}\n\n```{.r .cell-code}\ncornseed.pred <- predict(cornseed.2)\n```\n:::\n\n     \n    \n$\\blacksquare$\n\n(g) Plot the first two discriminant scores against each other,\ncoloured by soil type. You'll have to start by making a data frame\ncontaining what you need.\n\n\nSolution\n\n\nI changed my mind from the past about how to do this. I make a big data frame out of the data and predictions (with `cbind`) and go from there:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- cbind(cornseed, cornseed.pred)\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  field soil yield water herbicide class posterior.clay posterior.loam\n1     1 loam  76.7  29.5       7.5  loam    0.008122562      0.9303136\n2     2 loam  60.5  32.1       6.3  loam    0.195608997      0.3536733\n3     3 loam  96.1  40.7       4.2  loam    0.029529543      0.8533003\n4     4 loam  88.1  45.1       4.9  loam    0.069082256      0.7696194\n5     5 loam  50.2  34.1      11.7  loam    0.010588934      0.9457005\n6     6 loam  55.0  31.1       6.9  loam    0.208208691      0.3194421\n  posterior.salty posterior.sandy      x.LD1      x.LD2       x.LD3\n1     0.003067182      0.05849665 -2.7137304 -0.2765450  0.09765792\n2     0.134234919      0.31648283 -0.6999983  0.2264123  0.46748170\n3     0.008018680      0.10915147 -2.1875521  0.1644190 -2.10016387\n4     0.020907569      0.14039076 -1.7307043  0.8021079 -1.66560056\n5     0.005163698      0.03854692 -2.5284069  1.0096466  2.37276838\n6     0.159072574      0.31327660 -0.5974055  0.2867691  0.92872489\n```\n\n\n:::\n:::\n\n     \nThen we use this as input to `ggplot`:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = x.LD1, y = x.LD2, colour = soil)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/cornseed-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\n    \n$\\blacksquare$\n\n(h) On your plot that you just made, explain briefly how `LD1`\ndistinguishes at least one of the soil types.\n\n\n\nSolution\n\n\nFind a soil type that is typically high (or low or average) on\nLD1. Any one or more of these will do: loam soils are typically high on LD1,\nclay soils or salty soils are typically low on LD1; sandy soils are\ntypically average on LD1. (There are exceptions, but I'm looking for\n\"typically\".)\n  \n$\\blacksquare$\n\n(i) On your plot, does `LD2` appear to do anything to\nseparate the groups? Is this surprising given your earlier findings?\nExplain briefly.\n\n\n\nSolution\n\n\nAll the soil groups appear go to about the full height of the plot:\nthat is to say, none of the groups appear to be especially at the\ntop or the bottom. That means that `LD2` does not separate\nthe groups at all. \n\nBack in part (<a href=\"#part:corn-svd\">here</a>), we said that\nthe first linear discriminant is way more important than either of\nthe other two, and here we see what that means: `LD2` does nothing to\nseparate the groups. So it's not a surprising finding at all.\n\nI thought earlier about asking you to plot only the first linear\ndiscriminant, and now we see  why: only the first one separates the\ngroups. If you wanted to do that, you could  make a boxplot of the\ndiscriminant scores by `soil` group, thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = soil, y = x.LD1)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/cornseed-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n   \nThis says more or less the same thing as your plot of `LD1` and\n`LD2`: `loam` has the highest `LD1` score,\n`sandy` is about in the middle, and `clay` and\n`salty` have typically negative `LD1` scores, similar to\neach other, though there is one outlying `salty` that looks a\nlot more like a `loam`.\n  \n\n$\\blacksquare$\n\n(j) Make a table of actual and predicted `soil`\ngroup. Which soil type was classified correctly the most often? \n\n\n\nSolution\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(d, table(obs = soil, pred = class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       pred\nobs     clay loam salty sandy\n  clay     3    0     3     2\n  loam     0    6     0     2\n  salty    1    1     5     1\n  sandy    2    1     1     4\n```\n\n\n:::\n:::\n\n   \n\nOr, the `tidyverse` way, which is below.\n\nThere were 8 fields of each soil type. The soil type that has the most\nof its fields classified correctly (based on the values of the\nresponse variables) has the biggest number down the diagonal of the\ntable: looking at 3, 6, 5 and 4, we see that the `loam` soil\ntype had the most of its fields classified correctly, so this was the\nmost distinct from the others. (We also saw this on the plot of\n`LD1` vs. `LD2`: the `loam` fields were all over\non the right.)\n\nThis was easier because we had the same number of fields of each\ntype. If we didn't have that, the right way to go then would be to work out\n*row* percentages: \n\"out of the fields that were actually sandy, what percent of them got classified as sandy\", \nand so on.\n\nThis is not a perfect classification, though, which is about what you\nwould expect from the soil types being intermingled on the plot of\n`LD1` vs. `LD2`. If you look at the table,\n`salty` and `sandy` are fairly distinct also, but\n`clay` is often confused with both of them. On the plot of\n`LD1` and `LD2`, `salty` is generally to the left\nof `sandy`, but `clay` is mixed up with them both.\nThe tidyverse way of doing this is equally good. This is the tidied-up way:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% count(soil, class) %>% \n  pivot_wider(names_from = class, values_from = n, values_fill = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 5\n  soil   clay salty sandy  loam\n  <chr> <int> <int> <int> <int>\n1 clay      3     3     2     0\n2 loam      0     0     2     6\n3 salty     1     5     1     1\n4 sandy     2     1     4     1\n```\n\n\n:::\n:::\n\n \nSix out of eight `loam`s were correctly classified, which is\nbetter than anything else.\n\nExtra: we can calculate misclassification rates, first overall, which is easier:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(soil, class) %>%\n  mutate(soil_stat = ifelse(soil == class, \"correct\", \"wrong\")) %>%\n  count(soil_stat, wt = n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  soil_stat  n\n1   correct 18\n2     wrong 14\n```\n\n\n:::\n:::\n\n \n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(soil, class) %>%\n  mutate(soil_stat = ifelse(soil == class, \"correct\", \"wrong\")) %>%\n  count(soil_stat, wt = n) %>%\n  mutate(prop = nn / sum(nn))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in `mutate()`:\ni In argument: `prop = nn/sum(nn)`.\nCaused by error:\n! object 'nn' not found\n```\n\n\n:::\n:::\n\n\n\nNote the use of `wt` on the second `count` to count the\nnumber of *observations* from the first `count`, not the\nnumber of *rows*.\n\nThis shows that 44\\% of the soil types were misclassified, which\nsounds awful, but is actually not so bad, considering. Bear in mind\nthat if you were just guessing, you'd get 75\\% of them wrong, so\ngetting 44\\% wrong is quite a bit better than that. The variables\n(especially `herbicide`) are at least somewhat informative\nabout soil type; it's better to know them than not to.\n\nOr do it by actual soil type:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(soil, class) %>%\n  group_by(soil) %>%\n  mutate(soil_stat = ifelse(soil == class, \"correct\", \"wrong\")) %>%\n  count(soil_stat, wt = n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 3\n# Groups:   soil [4]\n  soil  soil_stat     n\n  <chr> <chr>     <int>\n1 clay  correct       3\n2 clay  wrong         5\n3 loam  correct       6\n4 loam  wrong         2\n5 salty correct       5\n6 salty wrong         3\n7 sandy correct       4\n8 sandy wrong         4\n```\n\n\n:::\n:::\n\n \n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(soil, class) %>%\n  group_by(soil) %>%\n  mutate(soil_stat = ifelse(soil == class, \"correct\", \"wrong\")) %>%\n  count(soil_stat, wt = n) %>%\n  mutate(prop = n / sum(n)) %>%\n  select(-n) %>%\n  pivot_wider(names_from=soil_stat, values_from=prop)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 3\n# Groups:   soil [4]\n  soil  correct wrong\n  <chr>   <dbl> <dbl>\n1 clay    0.375 0.625\n2 loam    0.75  0.25 \n3 salty   0.625 0.375\n4 sandy   0.5   0.5  \n```\n\n\n:::\n:::\n\n \n\nLoam soil was the easiest to get right, and clay was easiest to get\nwrong. However, these proportions were each based on only eight\nobservations, so it's probably wise *not* to say that loam is\n*always* easiest to get right.\n\nI didn't have you look at posterior probabilities here.^[Rest assured that I did on the final exam!] With 32 fields, this is rather a lot\nto list them all, but what we can do is to look at the ones that were\nmisclassified (the true soil type differs from the predicted soil\ntype). Before that, though, we need to make a data frame with the stuff in\nit that we want to look at. And before *that*, I want to round\nthe posterior probabilities to a small number of decimals.\n\nThen, we can fire away with this:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  mutate(across(starts_with(\"posterior\"), \n                \\(post) round(post, 3))) %>%\n  mutate(row = row_number()) -> dd\ndd %>% filter(soil != class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   field  soil yield water herbicide class posterior.clay posterior.loam\n7      7  loam  65.4  21.6       4.3 sandy          0.174          0.214\n8      8  loam  65.7  27.7       5.3 sandy          0.163          0.352\n9      9 sandy  67.3  48.3       5.5  clay          0.384          0.206\n10    10 sandy  61.3  28.9       6.9  loam          0.106          0.553\n11    11 sandy  58.2  42.5       4.8  clay          0.436          0.043\n13    13 sandy  66.9  23.9       1.1 salty          0.317          0.013\n17    17 salty  62.8  25.9       2.9 sandy          0.308          0.038\n20    20 salty  75.6  27.7       6.3  loam          0.026          0.819\n24    24 salty  68.4  35.3       1.9  clay          0.403          0.018\n25    25  clay  52.5  39.0       3.1 salty          0.414          0.004\n28    28  clay  63.5  25.6       3.0 sandy          0.298          0.047\n30    30  clay  61.5  16.8       1.9 sandy          0.255          0.020\n31    31  clay  62.9  25.8       2.4 salty          0.320          0.024\n32    32  clay  49.3  39.4       5.2 salty          0.416          0.019\n   posterior.salty posterior.sandy      x.LD1       x.LD2       x.LD3 row\n7            0.147           0.465 -0.4773813 -1.02300901  0.02489683   7\n8            0.113           0.373 -0.7787883 -0.37394272  0.08610126   8\n9            0.195           0.215 -0.2347419  1.58402466 -0.60226491   9\n10           0.069           0.272 -1.1888399 -0.05551354  0.69601339  10\n11           0.339           0.182  0.6365694  1.16783644 -0.16694577  11\n13           0.362           0.307  1.1089037 -1.05680853 -0.99478905  13\n17           0.315           0.340  0.6033993 -0.65387493 -0.37063590  17\n20           0.012           0.143 -2.0847382 -0.51018238 -0.11850210  20\n24           0.351           0.227  1.0111845  0.06204891 -1.21730783  24\n25           0.484           0.098  1.8263552  0.83185894 -0.24274038  25\n28           0.295           0.360  0.4849415 -0.69025922 -0.36741549  28\n30           0.338           0.388  0.8727560 -1.57008678 -0.28665910  30\n31           0.346           0.310  0.8448346 -0.70045299 -0.50863515  31\n32           0.418           0.146  1.0360558  1.08342373  0.47156646  32\n```\n\n\n:::\n:::\n\n \nMost of the posterior probabilities are neither especially small nor\nespecially large, which adds to the impression that things are really\nrather uncertain. For example, field 8 could have been either loam\n(0.352) or sandy (0.373). There was one field that was actually salty\nbut looked like a loam one (with `LD1` score around 2); this is\nfield 20, that needed a lot of herbicide; it was rated to have an 82\\%\nchance of being loam and only 1\\% chance of salty.\n\nLet's remind ourselves of why we were doing this: the MANOVA was\nsignificant, so at least some of the fields were different on some of\nthe variables from some of the others. What we found by doing the\ndiscriminant analysis was that only the first discriminant was of any\nvalue in distinguishing the soil types by the variables we measured,\nand *that* was mostly `herbicide`. So the major effect\nthat soil type had was on the amount of herbicide needed, with the\nloam soils needing most.\n\nI wanted to finish with one more thing, which was to look again at the\nsoils that were actually loam:\n\n::: {.cell}\n\n```{.r .cell-code}\ndd %>%\n  filter(soil == \"loam\") %>%\n  select(soil, yield, water, herbicide, class, starts_with(\"posterior\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  soil yield water herbicide class posterior.clay posterior.loam\n1 loam  76.7  29.5       7.5  loam          0.008          0.930\n2 loam  60.5  32.1       6.3  loam          0.196          0.354\n3 loam  96.1  40.7       4.2  loam          0.030          0.853\n4 loam  88.1  45.1       4.9  loam          0.069          0.770\n5 loam  50.2  34.1      11.7  loam          0.011          0.946\n6 loam  55.0  31.1       6.9  loam          0.208          0.319\n7 loam  65.4  21.6       4.3 sandy          0.174          0.214\n8 loam  65.7  27.7       5.3 sandy          0.163          0.352\n  posterior.salty posterior.sandy\n1           0.003           0.058\n2           0.134           0.316\n3           0.008           0.109\n4           0.021           0.140\n5           0.005           0.039\n6           0.159           0.313\n7           0.147           0.465\n8           0.113           0.373\n```\n\n\n:::\n:::\n\n \nFields 7 and 8 could have been pretty much any type of soil;\n`sandy` came out with the highest posterior probability, so\nthat's what they were predicted (wrongly) to be. Some of the fields,\n1, 3 and 5, were clearly (and correctly) loam. For 1 and 5, you can clearly\nsee that this is because `herbicide` was high, but field 3 is\nmore of a mystery. For this field, `herbicide` is *not*\nhigh, so one or more of the other variables must be pointing towards\n`loam`. \n\nWe can obtain predicted\n`LD1` scores for various combinations of \"typical\" values of\nthe response variables and see what has what effect on `LD1`:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(cornseed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     field           soil               yield           water      \n Min.   : 1.00   Length:32          Min.   :45.00   Min.   :14.50  \n 1st Qu.: 8.75   Class :character   1st Qu.:50.58   1st Qu.:25.75  \n Median :16.50   Mode  :character   Median :61.40   Median :29.60  \n Mean   :16.50                      Mean   :61.61   Mean   :31.17  \n 3rd Qu.:24.25                      3rd Qu.:67.00   3rd Qu.:36.83  \n Max.   :32.00                      Max.   :96.10   Max.   :54.20  \n   herbicide     \n Min.   : 1.100  \n 1st Qu.: 3.075  \n Median : 4.750  \n Mean   : 4.672  \n 3rd Qu.: 5.825  \n Max.   :11.700  \n```\n\n\n:::\n:::\n\n \n\nThe problem is that the variables have different spreads. Let's do\nsome predictions (ie.\\ calculations) of LD1 score for combinations of\nquartiles of our response variables. I like quartiles because these\nare \"representative\" values of the variables, typical of how far up\nand down they go. This process is one you've seen before:\n\n::: {.cell}\n\n```{.r .cell-code}\nyields <- c(51, 67)\nwaters <- c(26, 37)\nherbicides <- c(3, 6)\nnew <- datagrid(model = cornseed.2, yield = yields, \n                water = waters, herbicide = herbicides)\npred <- predict(cornseed.2, new)\ncbind(new, pred$x) %>% arrange(desc(LD1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  yield water herbicide rowid         LD1        LD2         LD3\n3    51    37         3     3  1.92293271  0.6641254 -0.13304771\n1    51    26         3     1  1.50933696 -0.3917181  0.22246094\n7    67    37         3     7  0.63095747  0.3311374 -0.90463685\n4    51    37         6     4  0.40331219  0.8735152  0.68540458\n5    67    26         3     5  0.21736172 -0.7247060 -0.54912820\n2    51    26         6     2 -0.01028356 -0.1823283  1.04091323\n8    67    37         6     8 -0.88866305  0.5405273 -0.08618456\n6    67    26         6     6 -1.30225880 -0.5153162  0.26932408\n```\n\n\n:::\n:::\n\n\nI arranged the predicted LD1 scores in descending order, so the most\nloam-like combinations are at the top. The top two combinations look\nlike loam; they both have high `herbicide`, as we figured\nbefore. But they also have high `yield`. That might go some way\ntowards explaining why field 3, with its non-high `herbicide`,\nwas confidently predicted to be `loam`:\n\n::: {.cell}\n\n```{.r .cell-code}\ncornseed %>% filter(field == 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 5\n  field soil  yield water herbicide\n  <dbl> <chr> <dbl> <dbl>     <dbl>\n1     3 loam   96.1  40.7       4.2\n```\n\n\n:::\n:::\n\n \n\nThis has a very high `yield`, and *that* is what is making\nus (correctly) think it is `loam`.\n\nI suddenly remembered that I hadn't done a biplot of this one, which I\ncould, since it's a discriminant analysis:\n\n::: {.cell}\n\n```{.r .cell-code}\nggbiplot(cornseed.2, groups = cornseed$soil)\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/cornseed-22-1.pdf){fig-pos='H'}\n:::\n:::\n\n \n\nThis shows the dominant influence of `herbicide` on LD1 score\n(more herbicide is more positive), and that `water` has nothing\nto say (in terms of distinguishing soil types) and `yield` has\nnot much to say, their arrows being short. That observation with a\nnon-high `herbicide` that was predicted to be \\textrm{loam} had\nthe highest `yield` of all, so even the small influence of\n`yield` on `LD1` made a big difference here.\n  \n\n$\\blacksquare$\n\n\n\n\n\n##  Understanding athletes' height, weight, sport and gender\n\n\nOn a previous assignment, we used MANOVA on the athletes\ndata to demonstrate that there was a significant relationship between\nthe combination of the athletes' height and weight, with the sport they\nplay and the athlete's gender. The problem with MANOVA is that it\ndoesn't give any information about the *kind* of relationship. To\nunderstand that, we need to do discriminant analysis, which is the\npurpose of this question.\n\nThe data can be found at\n[link](https://ritsokiguess.site/datafiles/ais.txt). \n\n\n\n(a) Once again, read in and display (some of) the data, bearing\nin mind that the data values are separated by *tabs*. (This\nought to be a free two marks.)\n\nSolution\n\n\nNothing new here:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"https://ritsokiguess.site/datafiles/ais.txt\"\nathletes <- read_tsv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 202 Columns: 13\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nathletes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 202 x 13\n   Sex    Sport     RCC   WCC    Hc    Hg  Ferr   BMI   SSF `%Bfat`   LBM    Ht\n   <chr>  <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>\n 1 female Netball  4.56  13.3  42.2  13.6    20  19.2  49      11.3  53.1  177.\n 2 female Netball  4.15   6    38    12.7    59  21.2 110.     25.3  47.1  173.\n 3 female Netball  4.16   7.6  37.5  12.3    22  21.4  89      19.4  53.4  176 \n 4 female Netball  4.32   6.4  37.7  12.3    30  21.0  98.3    19.6  48.8  170.\n 5 female Netball  4.06   5.8  38.7  12.8    78  21.8 122.     23.1  56.0  183 \n 6 female Netball  4.12   6.1  36.6  11.8    21  21.4  90.4    16.9  56.4  178.\n 7 female Netball  4.17   5    37.4  12.7   109  21.5 107.     21.3  53.1  177.\n 8 female Netball  3.8    6.6  36.5  12.4   102  24.4 157.     26.6  54.4  174.\n 9 female Netball  3.96   5.5  36.3  12.4    71  22.6 101.     17.9  56.0  174.\n10 female Netball  4.44   9.7  41.4  14.1    64  22.8 126.     25.0  51.6  174.\n# i 192 more rows\n# i 1 more variable: Wt <dbl>\n```\n\n\n:::\n:::\n\n \n\n\n$\\blacksquare$\n\n(b) Use `unite` to make a new column in your data frame\nwhich contains the sport-gender *combination*. Display it. (You\nmight like to display only a few columns so that it is clear that\nyou did the right thing.) Hint: you've seen `unite` in the\npeanuts example in class.\n\nSolution\n\n\nThe columns to combine are called `Sport` and `Sex`,\nwith Capital Letters. The syntax for `unite` is that you\ngive the name of the new combo column first, and then the names of\nthe columns you want to combine, either by listing them or by\nusing a select-helper. They will be separated by an underscore by\ndefault, which is usually easiest to handle.^[The opposite      of `unite` is `separate`, which splits a combined      column like my `combo` into separate columns; it too uses    underscore as the default separator.] \nIn `unite`, you can\ngroup the columns to \"unite\" with `c()`, as in class, or\nnot, as here. Either way is good.^[You used to have to group    them, but you don't any more. Hence my old code has them grouped,    but my new code does not.]\nWe'll be using height and weight in the\nanalysis to come, so I decided to display just those:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes %>%\n  unite(combo, Sport, Sex) -> athletesc\nathletesc %>% select(combo, Ht, Wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 202 x 3\n   combo             Ht    Wt\n   <chr>          <dbl> <dbl>\n 1 Netball_female  177.  59.9\n 2 Netball_female  173.  63  \n 3 Netball_female  176   66.3\n 4 Netball_female  170.  60.7\n 5 Netball_female  183   72.9\n 6 Netball_female  178.  67.9\n 7 Netball_female  177.  67.5\n 8 Netball_female  174.  74.1\n 9 Netball_female  174.  68.2\n10 Netball_female  174.  68.8\n# i 192 more rows\n```\n\n\n:::\n:::\n\n     \n\nI gave the data frame a new name, since I might want to come back to\nthe original later. Also, displaying only those columns gives more\nwidth for the display of my `combo`, so that I can be sure I\ngot it right.\n\nExtra: there is another column, `SSF`, that begins with S, so the\nselect-helper thing is not so obviously helpful here. But the two\ncolumns we want start with S followed by either e or p, so we could do this:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes %>%\n  unite(combo, matches(\"^S(e|p)\")) %>%\n  select(combo, Ht, Wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 202 x 3\n   combo             Ht    Wt\n   <chr>          <dbl> <dbl>\n 1 female_Netball  177.  59.9\n 2 female_Netball  173.  63  \n 3 female_Netball  176   66.3\n 4 female_Netball  170.  60.7\n 5 female_Netball  183   72.9\n 6 female_Netball  178.  67.9\n 7 female_Netball  177.  67.5\n 8 female_Netball  174.  74.1\n 9 female_Netball  174.  68.2\n10 female_Netball  174.  68.8\n# i 192 more rows\n```\n\n\n:::\n:::\n\n \n\nThe `matches` takes a so-called regular expression. This one\nsays ``starting at the beginning of the column name, find an uppercase\nS followed by either a lowercase e or a lowercase p''. This picks out\nthe columns and only the columns we want. In the opposite order,\nthough (either order is fine).\n\nI have a feeling we can also take advantage of the fact that the two\ncolumns we want to `unite` are the only two text ones:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes %>% \n  unite(combo, where(is.character))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 202 x 12\n   combo       RCC   WCC    Hc    Hg  Ferr   BMI   SSF `%Bfat`   LBM    Ht    Wt\n   <chr>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>\n 1 female_N~  4.56  13.3  42.2  13.6    20  19.2  49      11.3  53.1  177.  59.9\n 2 female_N~  4.15   6    38    12.7    59  21.2 110.     25.3  47.1  173.  63  \n 3 female_N~  4.16   7.6  37.5  12.3    22  21.4  89      19.4  53.4  176   66.3\n 4 female_N~  4.32   6.4  37.7  12.3    30  21.0  98.3    19.6  48.8  170.  60.7\n 5 female_N~  4.06   5.8  38.7  12.8    78  21.8 122.     23.1  56.0  183   72.9\n 6 female_N~  4.12   6.1  36.6  11.8    21  21.4  90.4    16.9  56.4  178.  67.9\n 7 female_N~  4.17   5    37.4  12.7   109  21.5 107.     21.3  53.1  177.  67.5\n 8 female_N~  3.8    6.6  36.5  12.4   102  24.4 157.     26.6  54.4  174.  74.1\n 9 female_N~  3.96   5.5  36.3  12.4    71  22.6 101.     17.9  56.0  174.  68.2\n10 female_N~  4.44   9.7  41.4  14.1    64  22.8 126.     25.0  51.6  174.  68.8\n# i 192 more rows\n```\n\n\n:::\n:::\n\nI wasn't expecting that to work! \n\n$\\blacksquare$\n\n(c) Run a discriminant analysis \"predicting\" sport-gender\ncombo from height and weight. Display the results. (No comment\nneeded yet.)\n\nSolution\n\n\nThat would be this. I'm having my familiar trouble with names:\n\n::: {.cell}\n\n```{.r .cell-code}\ncombo.1 <- lda(combo ~ Ht + Wt, data = athletesc)\n```\n:::\n\n     \n\nIf you used a new name for the data frame with the sport-gender\ncombinations in it, use that new name here.\n\nThe output:\n\n::: {.cell}\n\n```{.r .cell-code}\ncombo.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(combo ~ Ht + Wt, data = athletesc)\n\nPrior probabilities of groups:\n  BBall_female     BBall_male   Field_female     Field_male     Gym_female \n    0.06435644     0.05940594     0.03465347     0.05940594     0.01980198 \nNetball_female     Row_female       Row_male    Swim_female      Swim_male \n    0.11386139     0.10891089     0.07425743     0.04455446     0.06435644 \n  T400m_female     T400m_male  Tennis_female    Tennis_male  TSprnt_female \n    0.05445545     0.08910891     0.03465347     0.01980198     0.01980198 \n   TSprnt_male     WPolo_male \n    0.05445545     0.08415842 \n\nGroup means:\n                     Ht       Wt\nBBall_female   182.2692 71.33077\nBBall_male     195.5833 88.92500\nField_female   172.5857 80.04286\nField_male     185.2750 95.76250\nGym_female     153.4250 43.62500\nNetball_female 176.0870 69.59348\nRow_female     178.8591 72.90000\nRow_male       187.5333 86.80667\nSwim_female    173.1778 65.73333\nSwim_male      185.6462 81.66154\nT400m_female   169.3364 57.23636\nT400m_male     179.1889 68.20833\nTennis_female  168.5714 58.22857\nTennis_male    183.9500 75.40000\nTSprnt_female  170.4750 59.72500\nTSprnt_male    178.5364 75.79091\nWPolo_male     188.2235 86.72941\n\nCoefficients of linear discriminants:\n          LD1        LD2\nHt 0.08898971 -0.1888615\nWt 0.06825230  0.1305246\n\nProportion of trace:\n   LD1    LD2 \n0.7877 0.2123 \n```\n\n\n:::\n:::\n\n \n\nI comment here that there are two linear discriminants because there\nare two variables (height and weight) and actually 17 groups (not\nquite $2\\times 10$ because some sports are played by athletes of only\none gender). The smaller of 2 and $17-1$ is 2. (I often ask about\nthis, but am choosing not to here.)\n\n$\\blacksquare$\n\n(d) What kind of height and weight would make an athlete have a\nlarge (positive) score on `LD1`? Explain briefly.\n\nSolution\n\n\nThe Coefficients of Linear Discriminants for `LD1` are both\npositive, so an athlete with a large positive score on\n`LD1` has a large height and weight: that is to say, they\nare tall and heavy.\n\n$\\blacksquare$\n\n(e) Make a guess at the sport-gender combination that has the\n*highest* score on LD1. Why did you choose the combination you did?\n\nSolution\n\n\nI could have made you guess the *smallest* score on LD1, but\nthat would have been too easy (female gymnasts). \nFor this one, you want a sport-gender combination that is typically\ntall and heavy, and you can look in the table of Group Means to\nhelp you find a candidate group.\nI think the two best guesses are male basketball players (tallest\nand nearly the heaviest) and male field athletes (heaviest and\namong the group of athletes that are second-tallest behind the\nmale basketball players). I don't so much mind what you guess, as\nlong as you make a sensible call about a group that is reasonably\ntall and reasonably heavy (or, I suppose, that matches with what\nyou said in the previous part, whatever that was).\n\n$\\blacksquare$\n\n(f) <a name=\"part:ld2\">*</a> What combination of height and weight would make an athlete have a\n*small* (that is, very negative) score on LD2? Explain briefly.\n\nSolution\n\n\nThe italics in the question are something to do with questions\nthat have a link to them in Bookdown. I don't know how to fix\nthat.\nGoing back to the Coefficients of Linear Discriminants, the\ncoefficient for Height is negative, and the one for Weight is\npositive. What will make an athlete come out small (very\nnegative) on this is if they have a *large* height and a\n*small* weight. \nTo clarify your thinking on this, think of\nthe heights and weights as being standardized, so that a big one\nwill be positive and a small one will be negative. To make\n`LD2` very negative, you want a \"plus\" height to multiply\nthe minus sign, and a \"minus\" weight multiplying the plus sign.\nExtra: what is happening here is that `LD1` gives the most\nimportant way in which the groups differ, and `LD2` the\nnext-most important. There is generally a positive correlation\nbetween height and weight (taller athletes are generally heavier),\nso the most important \"dimension\" is the big-small one with tall\nheavy athletes at one end and short light athletes at the other.\nThe `Proportion of trace` in the output says that\n`LD1` is definitely more important, in terms of separating\nthe groups, than `LD2` is, but the latter still has\n*some* value.\n\n$\\blacksquare$\n\n(g) Obtain predictions for the discriminant analysis, and use\nthese to make a plot of `LD1` score against `LD2`\nscore, with the individual athletes distinguished by what sport they play\nand gender they are. (You can use colour to distinguish them, or you\ncan use shapes. If you want to go the latter way, there are clues in\nmy solutions to the MANOVA question about these athletes.)\n\nSolution\n\n\nThe prediction part is only one step:\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(combo.1)\n```\n:::\n\n     \n\nOne point for this.\n\nThis, in case you are wondering, is obtaining predicted group\nmembership and LD scores for the original data, that is, for our 202\nathletes. \n\nI prefer (no obligation) to take a look at what I have. My `p`\nis actually a `list`:\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"list\"\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 3\n $ class    : Factor w/ 17 levels \"BBall_female\",..: 12 6 6 6 7 6 6 6 6 6 ...\n $ posterior: num [1:202, 1:17] 0.1235 0.0493 0.084 0.0282 0.1538 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:202] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:17] \"BBall_female\" \"BBall_male\" \"Field_female\" \"Field_male\" ...\n $ x        : num [1:202, 1:2] -1.325 -1.487 -0.96 -1.885 0.114 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:202] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:2] \"LD1\" \"LD2\"\n```\n\n\n:::\n:::\n\n \n\nOur standard procedure is to `cbind` the predictions together with the original data (including the combo), and get a huge data frame (in this case):\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- cbind(athletesc, p)\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           combo  RCC  WCC   Hc   Hg Ferr   BMI   SSF %Bfat   LBM    Ht   Wt\n1 Netball_female 4.56 13.3 42.2 13.6   20 19.16  49.0 11.29 53.14 176.8 59.9\n2 Netball_female 4.15  6.0 38.0 12.7   59 21.15 110.2 25.26 47.09 172.6 63.0\n3 Netball_female 4.16  7.6 37.5 12.3   22 21.40  89.0 19.39 53.44 176.0 66.3\n4 Netball_female 4.32  6.4 37.7 12.3   30 21.03  98.3 19.63 48.78 169.9 60.7\n5 Netball_female 4.06  5.8 38.7 12.8   78 21.77 122.1 23.11 56.05 183.0 72.9\n6 Netball_female 4.12  6.1 36.6 11.8   21 21.38  90.4 16.86 56.45 178.2 67.9\n           class posterior.BBall_female posterior.BBall_male\n1     T400m_male             0.12348360         3.479619e-04\n2 Netball_female             0.04927852         7.263143e-05\n3 Netball_female             0.08402197         4.567927e-04\n4 Netball_female             0.02820743         1.539520e-05\n5     Row_female             0.15383834         1.197089e-02\n6 Netball_female             0.11219817         1.320889e-03\n  posterior.Field_female posterior.Field_male posterior.Gym_female\n1           0.0002835604         1.460578e-05         4.206308e-05\n2           0.0041253799         9.838207e-05         3.101597e-04\n3           0.0032633771         2.676308e-04         3.414854e-05\n4           0.0048909758         4.524089e-05         1.531681e-03\n5           0.0011443415         1.169322e-03         2.247239e-07\n6           0.0021761290         3.751161e-04         8.019783e-06\n  posterior.Netball_female posterior.Row_female posterior.Row_male\n1                0.1699941            0.1241779       0.0023825007\n2                0.2333569            0.1414225       0.0025370630\n3                0.2291353            0.1816810       0.0077872436\n4                0.2122221            0.1045723       0.0009826883\n5                0.1326885            0.1822427       0.0456717871\n6                0.2054332            0.1917380       0.0133925352\n  posterior.Swim_female posterior.Swim_male posterior.T400m_female\n1            0.07434038         0.011678465            0.103051973\n2            0.11730520         0.009274681            0.119270442\n3            0.08659049         0.023136399            0.058696177\n4            0.13254329         0.004132741            0.179336337\n5            0.02802782         0.089868173            0.008428382\n6            0.06557996         0.036249576            0.036328215\n  posterior.T400m_male posterior.Tennis_female posterior.Tennis_male\n1           0.25594274             0.047204095           0.017883433\n2           0.13618567             0.075858992           0.008601514\n3           0.17305732             0.035224944           0.017564554\n4           0.09812128             0.120824963           0.004345342\n5           0.17333438             0.004456769           0.046106286\n6           0.19213811             0.020599135           0.025565109\n  posterior.TSprnt_female posterior.TSprnt_male posterior.WPolo_male      x.LD1\n1             0.040192120            0.02616911         0.0028113441 -1.3251857\n2             0.050772549            0.04902216         0.0025072687 -1.4873604\n3             0.028170015            0.06274649         0.0081661381 -0.9595628\n4             0.070141970            0.03716409         0.0009221631 -1.8846129\n5             0.005144923            0.06163897         0.0542681927  0.1138304\n6             0.018513425            0.06367698         0.0147074229 -0.6545817\n        x.LD2\n1 -1.34799600\n2 -0.15015145\n3 -0.36154960\n4  0.05956819\n5 -0.82211820\n6 -0.56820566\n```\n\n\n:::\n:::\n\n \nAnd so, to the graph:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = x.LD1, y = x.LD2, colour = combo)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/athletes-d-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n \nIf you can distinguish seventeen different colours, your eyes are\nbetter than mine! You might prefer to use seventeen different shapes,\nalthough I wonder how much better that will be:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = x.LD1, y = x.LD2, shape = combo)) + geom_point() +\n  scale_shape_manual(values = 1:17)\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/athletes-d-11-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nYou have to do something special to get as many as seventeen\nshapes. This idea came from the MANOVA question in the last\nassignment. \n\nOr even this:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = x.LD1, y = x.LD2, shape = combo, colour = combo)) + geom_point() +\n  scale_shape_manual(values = 1:17)\n```\n\n::: {.cell-output-display}\n![](discriminant-analysis_files/figure-pdf/athletes-d-12-1.pdf){fig-pos='H'}\n:::\n:::\n\n \nPerhaps having colours *and* shapes makes the combos easier to\ndistinguish. We're beginning to stray onto the boundary between\nstatistics and aesthetics here!\n\nExtra: earlier, I asked you to guess which group(s) of athletes had a\nhigh (positive) score on LD1. These are the ones on the right side of\nthis plot: male basketball players bottom right and male field\nathletes top right. Was that what you guessed? What about the other\nguesses you might have made?\n\n$\\blacksquare$\n\n(h) Look on your graph for the four athletes with the smallest\n(most negative) scores on `LD2`. What do they have in common?\nDoes this make sense, given your answer to part (<a href=\"#part:ld2\">here</a>)?\nExplain briefly.\n\nSolution\n\n\nThese are the four athletes at the bottom of the plot. If you can\ndistinguish the colours, two of these are red and two of them are\norange, so they are all basketball players (two male and two\nfemale). If you plotted the shapes, and you used the same shapes I\ndid, two of them are circles and the other two are upward-facing\ntriangles, leading you to the same conclusion. (You could also\ndenote each combo by a letter and plot with those letters, as per\nthe solutions to the last assignment.)\nBack in part (<a href=\"#part:ld2\">here</a>), I said that what would make an\nathlete come out very negative on `LD2` is if they were\n*tall* and *not heavy*. This is the stereotypical\ndescription of a basketball player, so it makes perfect sense to\nme. \n\nExtra: some basketball players are tall and *heavier*; these\nare the ones on the right of the plot, with a larger `LD1`\nscore, to reflect that they are both tall and heavy, but with an\n`LD2` score closer to zero, reflecting that, given how tall\nthey are, their weight is about what you'd expect. LD2 is really\nsaying something like \"weight relative to height\", with someone\nat the top of the picture being unusually heavy and someone at the\nbottom unusually light.\n\n$\\blacksquare$\n\n(i) Obtain a (very large) square table, or a (very long) table\nwith frequencies, of actual and predicted sport-gender\ncombinations. You will probably have to make the square table very\nsmall to fit it on the page. For that, displaying the columns in two\nor more sets is OK (for example, six columns and all the rows, six\nmore columns and all the rows, then the last five columns for all\nthe rows).  Are there any sport-gender combinations that\nseem relatively easy to classify correctly?  Explain briefly.\n\nSolution\n\n\nLet's see what happens:\n\n::: {.cell}\n\n```{.r .cell-code}\ntab <- with(d, table(combo, class))\ntab\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                class\ncombo            BBall_female BBall_male Field_female Field_male Gym_female\n  BBall_female              3          1            0          0          0\n  BBall_male                0          9            0          0          0\n  Field_female              0          0            5          0          0\n  Field_male                0          1            0          7          0\n  Gym_female                0          0            0          0          4\n  Netball_female            0          0            1          0          0\n  Row_female                0          0            0          0          1\n  Row_male                  0          2            0          1          0\n  Swim_female               0          0            0          0          0\n  Swim_male                 0          4            0          0          0\n  T400m_female              0          0            0          0          0\n  T400m_male                3          1            0          0          0\n  Tennis_female             0          0            1          0          1\n  Tennis_male               1          0            0          0          0\n  TSprnt_female             0          0            0          0          0\n  TSprnt_male               0          0            0          0          0\n  WPolo_male                1          3            0          2          0\n                class\ncombo            Netball_female Row_female Row_male Swim_female Swim_male\n  BBall_female                5          1        0           0         0\n  BBall_male                  0          0        0           0         2\n  Field_female                1          0        0           0         0\n  Field_male                  0          2        0           0         0\n  Gym_female                  0          0        0           0         0\n  Netball_female             13          4        0           0         0\n  Row_female                  5         10        0           0         1\n  Row_male                    0          0        1           0         0\n  Swim_female                 4          1        0           0         0\n  Swim_male                   2          3        0           0         0\n  T400m_female                3          0        0           0         0\n  T400m_male                  5          3        0           0         0\n  Tennis_female               2          0        0           0         0\n  Tennis_male                 0          3        0           0         0\n  TSprnt_female               1          0        0           0         0\n  TSprnt_male                 6          3        0           0         0\n  WPolo_male                  0          3        1           0         0\n                class\ncombo            T400m_female T400m_male Tennis_female Tennis_male\n  BBall_female              0          2             0           0\n  BBall_male                0          0             0           0\n  Field_female              1          0             0           0\n  Field_male                0          0             0           0\n  Gym_female                0          0             0           0\n  Netball_female            1          4             0           0\n  Row_female                0          4             0           0\n  Row_male                  1          0             0           0\n  Swim_female               3          1             0           0\n  Swim_male                 0          1             0           0\n  T400m_female              6          2             0           0\n  T400m_male                1          5             0           0\n  Tennis_female             2          1             0           0\n  Tennis_male               0          0             0           0\n  TSprnt_female             2          1             0           0\n  TSprnt_male               0          0             0           0\n  WPolo_male                0          0             0           0\n                class\ncombo            TSprnt_female TSprnt_male WPolo_male\n  BBall_female               0           0          1\n  BBall_male                 0           0          1\n  Field_female               0           0          0\n  Field_male                 0           0          2\n  Gym_female                 0           0          0\n  Netball_female             0           0          0\n  Row_female                 0           0          1\n  Row_male                   0           0         10\n  Swim_female                0           0          0\n  Swim_male                  0           0          3\n  T400m_female               0           0          0\n  T400m_male                 0           0          0\n  Tennis_female              0           0          0\n  Tennis_male                0           0          0\n  TSprnt_female              0           0          0\n  TSprnt_male                0           0          2\n  WPolo_male                 0           0          7\n```\n\n\n:::\n:::\n\n     \n\nThat's kind of long.\n\nFor combos that are easy to classify, you're looking for a largish\nnumber on the diagonal of the table (classified correctly), bearing in\nmind that you only see about four columns of the table at once, and\n(much) smaller numbers in the rest of the row and column. I don't mind\nwhich ones you pick out, but see if you can find a few:\n\n\n\n* Male basketball players (9 out of 12 classified correctly)\n\n* Male field athletes (7 out of 10 classified correctly)\n\n* Female netball players (13 out of about 23)\n\n* Female rowers (10 out of about 22)\n\nOr you can turn it into a tibble:\n\n::: {.cell}\n\n```{.r .cell-code}\ntab %>% as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 289 x 3\n   combo          class            n\n   <chr>          <chr>        <int>\n 1 BBall_female   BBall_female     3\n 2 BBall_male     BBall_female     0\n 3 Field_female   BBall_female     0\n 4 Field_male     BBall_female     0\n 5 Gym_female     BBall_female     0\n 6 Netball_female BBall_female     0\n 7 Row_female     BBall_female     0\n 8 Row_male       BBall_female     0\n 9 Swim_female    BBall_female     0\n10 Swim_male      BBall_female     0\n# i 279 more rows\n```\n\n\n:::\n:::\n\n \nThis makes the `tidyverse` output, with frequencies. You\nprobably want to omit the zero ones:\n\n::: {.cell}\n\n```{.r .cell-code}\ntab %>% as_tibble() %>% filter(n > 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 70 x 3\n   combo        class            n\n   <chr>        <chr>        <int>\n 1 BBall_female BBall_female     3\n 2 T400m_male   BBall_female     3\n 3 Tennis_male  BBall_female     1\n 4 WPolo_male   BBall_female     1\n 5 BBall_female BBall_male       1\n 6 BBall_male   BBall_male       9\n 7 Field_male   BBall_male       1\n 8 Row_male     BBall_male       2\n 9 Swim_male    BBall_male       4\n10 T400m_male   BBall_male       1\n# i 60 more rows\n```\n\n\n:::\n:::\n\n \nThis is the same output as below. See there for comments.\n\nThe other, perhaps easier, way to tackle this one is the\n`tidyverse` way, making a \"long\" table of frequencies. Here is some of it. You'll be able to click to see more:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% count(combo, class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            combo          class  n\n1    BBall_female   BBall_female  3\n2    BBall_female     BBall_male  1\n3    BBall_female Netball_female  5\n4    BBall_female     Row_female  1\n5    BBall_female     T400m_male  2\n6    BBall_female     WPolo_male  1\n7      BBall_male     BBall_male  9\n8      BBall_male      Swim_male  2\n9      BBall_male     WPolo_male  1\n10   Field_female   Field_female  5\n11   Field_female Netball_female  1\n12   Field_female   T400m_female  1\n13     Field_male     BBall_male  1\n14     Field_male     Field_male  7\n15     Field_male     Row_female  2\n16     Field_male     WPolo_male  2\n17     Gym_female     Gym_female  4\n18 Netball_female   Field_female  1\n19 Netball_female Netball_female 13\n20 Netball_female     Row_female  4\n21 Netball_female   T400m_female  1\n22 Netball_female     T400m_male  4\n23     Row_female     Gym_female  1\n24     Row_female Netball_female  5\n25     Row_female     Row_female 10\n26     Row_female      Swim_male  1\n27     Row_female     T400m_male  4\n28     Row_female     WPolo_male  1\n29       Row_male     BBall_male  2\n30       Row_male     Field_male  1\n31       Row_male       Row_male  1\n32       Row_male   T400m_female  1\n33       Row_male     WPolo_male 10\n34    Swim_female Netball_female  4\n35    Swim_female     Row_female  1\n36    Swim_female   T400m_female  3\n37    Swim_female     T400m_male  1\n38      Swim_male     BBall_male  4\n39      Swim_male Netball_female  2\n40      Swim_male     Row_female  3\n41      Swim_male     T400m_male  1\n42      Swim_male     WPolo_male  3\n43   T400m_female Netball_female  3\n44   T400m_female   T400m_female  6\n45   T400m_female     T400m_male  2\n46     T400m_male   BBall_female  3\n47     T400m_male     BBall_male  1\n48     T400m_male Netball_female  5\n49     T400m_male     Row_female  3\n50     T400m_male   T400m_female  1\n51     T400m_male     T400m_male  5\n52  TSprnt_female Netball_female  1\n53  TSprnt_female   T400m_female  2\n54  TSprnt_female     T400m_male  1\n55    TSprnt_male Netball_female  6\n56    TSprnt_male     Row_female  3\n57    TSprnt_male     WPolo_male  2\n58  Tennis_female   Field_female  1\n59  Tennis_female     Gym_female  1\n60  Tennis_female Netball_female  2\n61  Tennis_female   T400m_female  2\n62  Tennis_female     T400m_male  1\n63    Tennis_male   BBall_female  1\n64    Tennis_male     Row_female  3\n65     WPolo_male   BBall_female  1\n66     WPolo_male     BBall_male  3\n67     WPolo_male     Field_male  2\n68     WPolo_male     Row_female  3\n69     WPolo_male       Row_male  1\n70     WPolo_male     WPolo_male  7\n```\n\n\n:::\n:::\n\n \n\nThe zeroes never show up here.\nThe `combo` column is the truth, and the `class` column\nis the prediction. Again, you can see where the big frequencies are; a\nlot of the female netball players were gotten right, but there were a\nlot of them to begin with.\n\nExtra: let's see if we can work out *proportions* correct. I've\nchanged my mind from how I originally wrote this. I still use\n`count`, but I start with the overall misclassification. Let's\ntake it in steps:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(combo, class) %>%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            combo          class  n    stat\n1    BBall_female   BBall_female  3 correct\n2    BBall_female     BBall_male  1   wrong\n3    BBall_female Netball_female  5   wrong\n4    BBall_female     Row_female  1   wrong\n5    BBall_female     T400m_male  2   wrong\n6    BBall_female     WPolo_male  1   wrong\n7      BBall_male     BBall_male  9 correct\n8      BBall_male      Swim_male  2   wrong\n9      BBall_male     WPolo_male  1   wrong\n10   Field_female   Field_female  5 correct\n11   Field_female Netball_female  1   wrong\n12   Field_female   T400m_female  1   wrong\n13     Field_male     BBall_male  1   wrong\n14     Field_male     Field_male  7 correct\n15     Field_male     Row_female  2   wrong\n16     Field_male     WPolo_male  2   wrong\n17     Gym_female     Gym_female  4 correct\n18 Netball_female   Field_female  1   wrong\n19 Netball_female Netball_female 13 correct\n20 Netball_female     Row_female  4   wrong\n21 Netball_female   T400m_female  1   wrong\n22 Netball_female     T400m_male  4   wrong\n23     Row_female     Gym_female  1   wrong\n24     Row_female Netball_female  5   wrong\n25     Row_female     Row_female 10 correct\n26     Row_female      Swim_male  1   wrong\n27     Row_female     T400m_male  4   wrong\n28     Row_female     WPolo_male  1   wrong\n29       Row_male     BBall_male  2   wrong\n30       Row_male     Field_male  1   wrong\n31       Row_male       Row_male  1 correct\n32       Row_male   T400m_female  1   wrong\n33       Row_male     WPolo_male 10   wrong\n34    Swim_female Netball_female  4   wrong\n35    Swim_female     Row_female  1   wrong\n36    Swim_female   T400m_female  3   wrong\n37    Swim_female     T400m_male  1   wrong\n38      Swim_male     BBall_male  4   wrong\n39      Swim_male Netball_female  2   wrong\n40      Swim_male     Row_female  3   wrong\n41      Swim_male     T400m_male  1   wrong\n42      Swim_male     WPolo_male  3   wrong\n43   T400m_female Netball_female  3   wrong\n44   T400m_female   T400m_female  6 correct\n45   T400m_female     T400m_male  2   wrong\n46     T400m_male   BBall_female  3   wrong\n47     T400m_male     BBall_male  1   wrong\n48     T400m_male Netball_female  5   wrong\n49     T400m_male     Row_female  3   wrong\n50     T400m_male   T400m_female  1   wrong\n51     T400m_male     T400m_male  5 correct\n52  TSprnt_female Netball_female  1   wrong\n53  TSprnt_female   T400m_female  2   wrong\n54  TSprnt_female     T400m_male  1   wrong\n55    TSprnt_male Netball_female  6   wrong\n56    TSprnt_male     Row_female  3   wrong\n57    TSprnt_male     WPolo_male  2   wrong\n58  Tennis_female   Field_female  1   wrong\n59  Tennis_female     Gym_female  1   wrong\n60  Tennis_female Netball_female  2   wrong\n61  Tennis_female   T400m_female  2   wrong\n62  Tennis_female     T400m_male  1   wrong\n63    Tennis_male   BBall_female  1   wrong\n64    Tennis_male     Row_female  3   wrong\n65     WPolo_male   BBall_female  1   wrong\n66     WPolo_male     BBall_male  3   wrong\n67     WPolo_male     Field_male  2   wrong\n68     WPolo_male     Row_female  3   wrong\n69     WPolo_male       Row_male  1   wrong\n70     WPolo_male     WPolo_male  7 correct\n```\n\n\n:::\n:::\n\n \n\nThat makes a new column `stat` that contains whether the\npredicted sport-gender combination was correct or wrong. For an\noverall misclassification rate we have to count these, but *not*\nsimply counting the number of rows; rather, we need to total up the\nthings in the `n` column:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(combo, class) %>%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %>%\n  count(stat, wt = n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     stat   n\n1 correct  70\n2   wrong 132\n```\n\n\n:::\n:::\n\n \n\nThis tells us how many predictions overall were right and how many\nwrong. \n\nTo make those into proportions, another `mutate`, dividing by\nthe total of `n`:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(combo, class) %>%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %>%\n  count(stat, wt = n) %>%\n  mutate(proportion = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     stat   n proportion\n1 correct  70  0.3465347\n2   wrong 132  0.6534653\n```\n\n\n:::\n:::\n\n \n\n65\\% of the sport-gender combinations were misclassified. This is\nawful, but is a lot better than guessing (we'd then get about 5\\% of\nthem right and about 95\\% wrong).\n\nThere's a subtlety here that will make sense when we do the\ncorresponding calculation by sport-gender combination. To do\n*that*, we put a `group_by(combo)` either before or after\nwe define `stat` (it doesn't matter which way):\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(combo, class) %>%\n  group_by(combo) %>%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %>%\n  count(stat, wt = n) %>%\n  mutate(proportion = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 27 x 4\n# Groups:   combo [17]\n   combo          stat        n proportion\n   <chr>          <chr>   <int>      <dbl>\n 1 BBall_female   correct     3      0.231\n 2 BBall_female   wrong      10      0.769\n 3 BBall_male     correct     9      0.75 \n 4 BBall_male     wrong       3      0.25 \n 5 Field_female   correct     5      0.714\n 6 Field_female   wrong       2      0.286\n 7 Field_male     correct     7      0.583\n 8 Field_male     wrong       5      0.417\n 9 Gym_female     correct     4      1    \n10 Netball_female correct    13      0.565\n# i 17 more rows\n```\n\n\n:::\n:::\n\n \n\nThat last `sum(n)`: what is it summing over? The answer is\n\"within `combo`\", since that is the `group_by`. You\nsee that the two `proportion` values within, say,\n`BBall_female`, add up to 1.\n\nWe don't actually see all the answers, because there are too many of\nthem. Let's try to get the proportion correct and wrong in their own\ncolumns. This almost works:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(combo, class) %>%\n  group_by(combo) %>%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %>%\n  count(stat, wt = n) %>%\n  mutate(proportion = n / sum(n)) %>%\n  pivot_wider(names_from=stat, values_from=proportion)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 27 x 4\n# Groups:   combo [17]\n   combo              n correct  wrong\n   <chr>          <int>   <dbl>  <dbl>\n 1 BBall_female       3   0.231 NA    \n 2 BBall_female      10  NA      0.769\n 3 BBall_male         9   0.75  NA    \n 4 BBall_male         3  NA      0.25 \n 5 Field_female       5   0.714 NA    \n 6 Field_female       2  NA      0.286\n 7 Field_male         7   0.583 NA    \n 8 Field_male         5  NA      0.417\n 9 Gym_female         4   1     NA    \n10 Netball_female    13   0.565 NA    \n# i 17 more rows\n```\n\n\n:::\n:::\n\n \n\nThis doesn't work because everything outside of the `pivot_wider` is\ntested for uniqueness; if it's unique, it gets its own row. Thus,\n`BBall_male` and 3 is different from `BBall_male` and\n9. But we only want one row of `BBall_male`. I think the\neasiest way around this is to get rid of `n`, since it has\nserved its purpose:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(combo, class) %>%\n  group_by(combo) %>%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %>%\n  count(stat, wt = n) %>%\n  mutate(proportion = n / sum(n)) %>%\n  select(-n) %>%\n  pivot_wider(names_from=stat, values_from=proportion, values_fill = list(proportion=0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 17 x 3\n# Groups:   combo [17]\n   combo          correct wrong\n   <chr>            <dbl> <dbl>\n 1 BBall_female    0.231  0.769\n 2 BBall_male      0.75   0.25 \n 3 Field_female    0.714  0.286\n 4 Field_male      0.583  0.417\n 5 Gym_female      1      0    \n 6 Netball_female  0.565  0.435\n 7 Row_female      0.455  0.545\n 8 Row_male        0.0667 0.933\n 9 Swim_female     0      1    \n10 Swim_male       0      1    \n11 T400m_female    0.545  0.455\n12 T400m_male      0.278  0.722\n13 TSprnt_female   0      1    \n14 TSprnt_male     0      1    \n15 Tennis_female   0      1    \n16 Tennis_male     0      1    \n17 WPolo_male      0.412  0.588\n```\n\n\n:::\n:::\n\nOne extra thing: some of the `proportion` values were missing, because there weren't any misclassified (or maybe correctly-classified!) athletes. The `values_fill` sets any missings in `proportion` to zero.\n \n\nWhile we're about it, let's arrange in\norder of misclassification probability:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(combo, class) %>%\n  group_by(combo) %>%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %>%\n  count(stat, wt = n) %>%\n  mutate(proportion = n / sum(n)) %>%\n  select(-n) %>%\n  pivot_wider(names_from=stat, values_from=proportion, values_fill = list(proportion=0)) %>% \n  replace_na(list(correct = 0, wrong = 0)) %>%\n  arrange(wrong)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 17 x 3\n# Groups:   combo [17]\n   combo          correct wrong\n   <chr>            <dbl> <dbl>\n 1 Gym_female      1      0    \n 2 BBall_male      0.75   0.25 \n 3 Field_female    0.714  0.286\n 4 Field_male      0.583  0.417\n 5 Netball_female  0.565  0.435\n 6 T400m_female    0.545  0.455\n 7 Row_female      0.455  0.545\n 8 WPolo_male      0.412  0.588\n 9 T400m_male      0.278  0.722\n10 BBall_female    0.231  0.769\n11 Row_male        0.0667 0.933\n12 Swim_female     0      1    \n13 Swim_male       0      1    \n14 TSprnt_female   0      1    \n15 TSprnt_male     0      1    \n16 Tennis_female   0      1    \n17 Tennis_male     0      1    \n```\n\n\n:::\n:::\n\n \n\nThe most distinctive athletes were the female gymnasts (tiny!),\nfollowed by the male basketball players (tall) and the female field\nathletes (heavy). These were easiest to predict from their height and\nweight. The ones at the bottom of the list were very confusible since\nthe discriminant analysis guessed them all wrong!\nSo what were the most common *misclassifications*? Let's go back\nto this:\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           combo  RCC  WCC   Hc   Hg Ferr   BMI   SSF %Bfat   LBM    Ht   Wt\n1 Netball_female 4.56 13.3 42.2 13.6   20 19.16  49.0 11.29 53.14 176.8 59.9\n2 Netball_female 4.15  6.0 38.0 12.7   59 21.15 110.2 25.26 47.09 172.6 63.0\n3 Netball_female 4.16  7.6 37.5 12.3   22 21.40  89.0 19.39 53.44 176.0 66.3\n4 Netball_female 4.32  6.4 37.7 12.3   30 21.03  98.3 19.63 48.78 169.9 60.7\n5 Netball_female 4.06  5.8 38.7 12.8   78 21.77 122.1 23.11 56.05 183.0 72.9\n6 Netball_female 4.12  6.1 36.6 11.8   21 21.38  90.4 16.86 56.45 178.2 67.9\n           class posterior.BBall_female posterior.BBall_male\n1     T400m_male             0.12348360         3.479619e-04\n2 Netball_female             0.04927852         7.263143e-05\n3 Netball_female             0.08402197         4.567927e-04\n4 Netball_female             0.02820743         1.539520e-05\n5     Row_female             0.15383834         1.197089e-02\n6 Netball_female             0.11219817         1.320889e-03\n  posterior.Field_female posterior.Field_male posterior.Gym_female\n1           0.0002835604         1.460578e-05         4.206308e-05\n2           0.0041253799         9.838207e-05         3.101597e-04\n3           0.0032633771         2.676308e-04         3.414854e-05\n4           0.0048909758         4.524089e-05         1.531681e-03\n5           0.0011443415         1.169322e-03         2.247239e-07\n6           0.0021761290         3.751161e-04         8.019783e-06\n  posterior.Netball_female posterior.Row_female posterior.Row_male\n1                0.1699941            0.1241779       0.0023825007\n2                0.2333569            0.1414225       0.0025370630\n3                0.2291353            0.1816810       0.0077872436\n4                0.2122221            0.1045723       0.0009826883\n5                0.1326885            0.1822427       0.0456717871\n6                0.2054332            0.1917380       0.0133925352\n  posterior.Swim_female posterior.Swim_male posterior.T400m_female\n1            0.07434038         0.011678465            0.103051973\n2            0.11730520         0.009274681            0.119270442\n3            0.08659049         0.023136399            0.058696177\n4            0.13254329         0.004132741            0.179336337\n5            0.02802782         0.089868173            0.008428382\n6            0.06557996         0.036249576            0.036328215\n  posterior.T400m_male posterior.Tennis_female posterior.Tennis_male\n1           0.25594274             0.047204095           0.017883433\n2           0.13618567             0.075858992           0.008601514\n3           0.17305732             0.035224944           0.017564554\n4           0.09812128             0.120824963           0.004345342\n5           0.17333438             0.004456769           0.046106286\n6           0.19213811             0.020599135           0.025565109\n  posterior.TSprnt_female posterior.TSprnt_male posterior.WPolo_male      x.LD1\n1             0.040192120            0.02616911         0.0028113441 -1.3251857\n2             0.050772549            0.04902216         0.0025072687 -1.4873604\n3             0.028170015            0.06274649         0.0081661381 -0.9595628\n4             0.070141970            0.03716409         0.0009221631 -1.8846129\n5             0.005144923            0.06163897         0.0542681927  0.1138304\n6             0.018513425            0.06367698         0.0147074229 -0.6545817\n        x.LD2\n1 -1.34799600\n2 -0.15015145\n3 -0.36154960\n4  0.05956819\n5 -0.82211820\n6 -0.56820566\n```\n\n\n:::\n\n```{.r .cell-code}\nd %>%\n  count(combo, class) %>%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            combo          class  n    stat\n1    BBall_female   BBall_female  3 correct\n2    BBall_female     BBall_male  1   wrong\n3    BBall_female Netball_female  5   wrong\n4    BBall_female     Row_female  1   wrong\n5    BBall_female     T400m_male  2   wrong\n6    BBall_female     WPolo_male  1   wrong\n7      BBall_male     BBall_male  9 correct\n8      BBall_male      Swim_male  2   wrong\n9      BBall_male     WPolo_male  1   wrong\n10   Field_female   Field_female  5 correct\n11   Field_female Netball_female  1   wrong\n12   Field_female   T400m_female  1   wrong\n13     Field_male     BBall_male  1   wrong\n14     Field_male     Field_male  7 correct\n15     Field_male     Row_female  2   wrong\n16     Field_male     WPolo_male  2   wrong\n17     Gym_female     Gym_female  4 correct\n18 Netball_female   Field_female  1   wrong\n19 Netball_female Netball_female 13 correct\n20 Netball_female     Row_female  4   wrong\n21 Netball_female   T400m_female  1   wrong\n22 Netball_female     T400m_male  4   wrong\n23     Row_female     Gym_female  1   wrong\n24     Row_female Netball_female  5   wrong\n25     Row_female     Row_female 10 correct\n26     Row_female      Swim_male  1   wrong\n27     Row_female     T400m_male  4   wrong\n28     Row_female     WPolo_male  1   wrong\n29       Row_male     BBall_male  2   wrong\n30       Row_male     Field_male  1   wrong\n31       Row_male       Row_male  1 correct\n32       Row_male   T400m_female  1   wrong\n33       Row_male     WPolo_male 10   wrong\n34    Swim_female Netball_female  4   wrong\n35    Swim_female     Row_female  1   wrong\n36    Swim_female   T400m_female  3   wrong\n37    Swim_female     T400m_male  1   wrong\n38      Swim_male     BBall_male  4   wrong\n39      Swim_male Netball_female  2   wrong\n40      Swim_male     Row_female  3   wrong\n41      Swim_male     T400m_male  1   wrong\n42      Swim_male     WPolo_male  3   wrong\n43   T400m_female Netball_female  3   wrong\n44   T400m_female   T400m_female  6 correct\n45   T400m_female     T400m_male  2   wrong\n46     T400m_male   BBall_female  3   wrong\n47     T400m_male     BBall_male  1   wrong\n48     T400m_male Netball_female  5   wrong\n49     T400m_male     Row_female  3   wrong\n50     T400m_male   T400m_female  1   wrong\n51     T400m_male     T400m_male  5 correct\n52  TSprnt_female Netball_female  1   wrong\n53  TSprnt_female   T400m_female  2   wrong\n54  TSprnt_female     T400m_male  1   wrong\n55    TSprnt_male Netball_female  6   wrong\n56    TSprnt_male     Row_female  3   wrong\n57    TSprnt_male     WPolo_male  2   wrong\n58  Tennis_female   Field_female  1   wrong\n59  Tennis_female     Gym_female  1   wrong\n60  Tennis_female Netball_female  2   wrong\n61  Tennis_female   T400m_female  2   wrong\n62  Tennis_female     T400m_male  1   wrong\n63    Tennis_male   BBall_female  1   wrong\n64    Tennis_male     Row_female  3   wrong\n65     WPolo_male   BBall_female  1   wrong\n66     WPolo_male     BBall_male  3   wrong\n67     WPolo_male     Field_male  2   wrong\n68     WPolo_male     Row_female  3   wrong\n69     WPolo_male       Row_male  1   wrong\n70     WPolo_male     WPolo_male  7 correct\n```\n\n\n:::\n:::\n\n \nWe want to express those `n` values as proportions out of their\nactual sport-gender combo, so we group by `combo` before\ndefining the proportions:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(combo, class) %>%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %>%\n  group_by(combo) %>%\n  mutate(proportion = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 70 x 5\n# Groups:   combo [17]\n   combo        class              n stat    proportion\n   <chr>        <fct>          <int> <chr>        <dbl>\n 1 BBall_female BBall_female       3 correct     0.231 \n 2 BBall_female BBall_male         1 wrong       0.0769\n 3 BBall_female Netball_female     5 wrong       0.385 \n 4 BBall_female Row_female         1 wrong       0.0769\n 5 BBall_female T400m_male         2 wrong       0.154 \n 6 BBall_female WPolo_male         1 wrong       0.0769\n 7 BBall_male   BBall_male         9 correct     0.75  \n 8 BBall_male   Swim_male          2 wrong       0.167 \n 9 BBall_male   WPolo_male         1 wrong       0.0833\n10 Field_female Field_female       5 correct     0.714 \n# i 60 more rows\n```\n\n\n:::\n:::\n\n \n\nOnly pick out the ones that were gotten wrong, and arrange the remaining\nproportions in descending order:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>%\n  count(combo, class) %>%\n  mutate(stat = ifelse(combo == class, \"correct\", \"wrong\")) %>%\n  group_by(combo) %>%\n  mutate(proportion = n / sum(n)) %>%\n  filter(stat == \"wrong\") %>%\n  arrange(desc(proportion))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 59 x 5\n# Groups:   combo [16]\n   combo         class              n stat  proportion\n   <chr>         <fct>          <int> <chr>      <dbl>\n 1 Tennis_male   Row_female         3 wrong      0.75 \n 2 Row_male      WPolo_male        10 wrong      0.667\n 3 TSprnt_male   Netball_female     6 wrong      0.545\n 4 TSprnt_female T400m_female       2 wrong      0.5  \n 5 Swim_female   Netball_female     4 wrong      0.444\n 6 BBall_female  Netball_female     5 wrong      0.385\n 7 Swim_female   T400m_female       3 wrong      0.333\n 8 Swim_male     BBall_male         4 wrong      0.308\n 9 Tennis_female Netball_female     2 wrong      0.286\n10 Tennis_female T400m_female       2 wrong      0.286\n# i 49 more rows\n```\n\n\n:::\n:::\n\n \n\nThe embarrassment champion is the three male tennis players that were\ntaken to be --- female rowers! Most of the other mistakes are more\nforgivable: the male rowers being taken for male water polo players,\nfor example. \n\n\n$\\blacksquare$\n\n\n",
    "supporting": [
      "discriminant-analysis_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}