{
  "hash": "d7a447f7d2fa8f723ab848c9c6ce88e3",
  "result": {
    "markdown": "# Logistic regression with nominal response\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nnet)\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n##  Finding non-missing values\n\n\n <a name=\"part:prepare-next\">*</a> This is to prepare you for something in the next\nquestion. It's meant to be easy.\n\nIn R, the code `NA` stands for \"missing value\" or\n\"value not known\". In R, `NA` should not have quotes around\nit. (It is a special code, not a piece of text.)\n\n\n(a) Create a vector `v` that contains some numbers and some\nmissing values, using `c()`. Put those values into a\none-column data frame.\n \n\n(b) Obtain a new column containing `is.na(v)`. When is this true and when is this false?\n \n\n(c) The symbol `!` means \"not\" in R (and other\nprogramming languages). What does `!is.na(v)` do? Create a\nnew column containing that.\n \n\n(d) Use `filter` to display just the\nrows of your data frame that have a non-missing value of `v`.\n\n \n\n\n\n\n\n\n##  European Social Survey and voting\n\n\n The European Social Survey is a giant survey carried out\nacross Europe covering demographic information, attitudes to and\namount of education, politics and so on. In this question, we will\ninvestigate what might make British people vote for a certain\npolitical party.\n\nThe information for this question is in a (large) spreadsheet at\n[link](http://ritsokiguess.site/datafiles/ess.csv). There is also a\n\"codebook\" at\n[link](http://ritsokiguess.site/datafiles/ess-codebook.pdf) that\ntells you what all the variables are. The ones we will use are the\nlast five columns of the spreadsheet, described on pages 11 onwards of\nthe codebook. (I could have given you more, but I didn't want to make\nthis any more complicated than it already was.)\n\n\n\n(a) Read in the `.csv` file, and verify that you have lots\nof rows and columns.\n \n\n(b) <a name=\"part:whatvar\">*</a> Use the codebook to find out what the columns\n`prtvtgb`, `gndr`, `agea`, `eduyrs` and\n`inwtm` are.  What do the values 1 and 2 for `gndr`\nmean? (You don't, at this point, have to worry about the values for\nthe other variables.)\n \n\n(c) The three major political parties in Britain are the\nConservative, Labour and Liberal Democrat. (These, for your\ninformation, correspond roughly to the Canadian Progressive\nConservative, NDP and Liberal parties.) For the variable that\ncorresponds to \"political party voted for at the last election\",\nwhich values correspond to these three parties?\n \n\n(d) Normally, I would give you a tidied-up\ndata set. But I figure you could use some practice tidying this one\nup. As the codebook shows, there are some numerical codes for\nmissing values, and we want to omit those.\nWe want just the columns `prtvtgb` through `inwtm`\nfrom the right side of the spreadsheet.  Use `dplyr` or\n`tidyr` tools to (i) select only these columns, (ii) include\nthe rows that correspond to people who voted for one of the three\nmajor parties, (iii) include the rows that have an age at interview\nless than 999, (iv) include the rows that have less than 40 years of\neducation, (v) include the rows that are not missing on\n`inwtm` (use the idea from Question~<a href=\"#part:prepare-next\">here</a>\nfor (v)).  The last four of those (the inclusion of rows) can be\ndone in one go.\n \n\n(e) Why is my response variable nominal rather than  ordinal? How can I tell?\nWhich R function should I use, therefore, to fit my model?\n \n\n(f) <a name=\"part:full\">*</a> Take the political party voted for, and turn it into a\nfactor, by feeding it into `factor`.\nFit an appropriate model to predict political party voted\nfor at the last election (as a factor) from all the other\nvariables. Gender is really a categorical variable too, but since\nthere are only two possible values it can be treated as a number.\n \n\n(g) We have a lot of explanatory variables. The standard way to\ntest whether we need all of them is to take one of them out at a time,\nand test which ones we can remove. This is a lot of work. We won't do\nthat. Instead,\nthe R function `step` does what you want. You feed `step`\ntwo things: a fitted model object, and the option `trace=0`\n(otherwise you get a lot of output). The final part of the output from\n`step` tells you which explanatory variables you need to keep.\nRun `step` on your fitted model. Which explanatory variables\nneed to stay in the model here?\n\n \n\n(h) Fit the model indicated by `step` (in the last part).\n\n \n\n(i) I didn't think that interview length could possibly be\nrelevant to which party a person voted for. Test whether interview\nlength can be removed from your model of the last part. What do you\nconclude? (Note that `step` and this test may disagree.)\n\n \n\n(j) Use your best model to obtain predictions from some\nsuitably chosen combinations of values of the explanatory variables\nthat remain. (If you have quantitative explanatory variables left,\nyou could use their first and third quartiles as values to predict\nfrom. Running `summary` on the data frame will get summaries\nof all the variables.)\n \n\n(k) What is the effect of increasing age? What is the effect of\nan increase in years of education?\n \n\n\n\n\n\n##  Alligator food\n\n\n What do alligators most like to eat? 219 alligators were captured\nin four Florida lakes. Each alligator's stomach contents were\nobserved, and the food that the alligator had eaten  was classified\ninto one of five categories: fish, invertebrates (such as snails or\ninsects), reptiles (such as turtles), birds, and \"other\" (such as\namphibians, plants or rocks). The researcher noted for each alligator\nwhat that alligator had most of in its stomach, as well as the gender\nof each alligator and whether it was \"large\" or \"small\" (greater\nor less than 2.3 metres in length). The data can be found in\n[link](http://ritsokiguess.site/datafiles/alligator.txt). The\nnumbers in the data set (apart from the first column) are all\nfrequencies. (You can ignore that first column \"profile\".)\n\nOur aim is to predict food type from the other variables.\n\n\n\n(a) Read in the data and display the first few lines. Describe\nhow the data are not \"tidy\".\n\n\n\n(b) Use `pivot_longer` to arrange the data\nsuitably for analysis (which will be using\n`multinom`). Demonstrate (by looking at the first few rows\nof your new data frame) that you now have something tidy.\n\n\n\n(c) What is different about this problem, compared to\nQuestion <a href=\"#q:abortion\">here</a>, that would make \n`multinom` the right tool to use?\n\n\n\n(d) Fit a suitable multinomial model predicting food type from\ngender, size and lake. Does each row represent one alligator or more\nthan one? If more than one, account for this in your modelling.\n\n\n\n(e) Do a test to see whether `Gender` should stay in\nthe model. (This will entail fitting another model.) What do you conclude?\n\n\n\n(f) Predict the probability that an alligator\nprefers each food type, given its size, gender (if necessary) and\nthe lake it was found \nin, using the more appropriate of the two models that you have\nfitted so far.  This means (i) \nmaking a data frame for prediction, and (ii) obtaining and\ndisplaying the predicted probabilities in a way that is easy to read.\n\n\n\n(g) What do you think is the most important way in which the\nlakes differ? (Hint: look at where the biggest predicted\nprobabilities are.)\n\n\n\n(h) How would you describe the major difference between the\ndiets of the small and large alligators?\n\n\n\n\n\n\n\n##  Crimes in San Francisco\n\n\n\nThe data in\n[link](http://ritsokiguess.site/datafiles/sfcrime1.csv) is a subset\nof a huge\ndataset of crimes committed in San Francisco between 2003 and\n2015. The variables are:\n\n\n\n* `Dates`: the date and time of the crime\n\n* `Category`: the category of crime, eg. \"larceny\" or\n\"vandalism\" (response).\n\n* `Descript`: detailed description of crime.\n\n* `DayOfWeek`: the day of the week of the crime.\n\n* `PdDistrict`: the name of the San Francisco Police\nDepartment district in which the crime took place.\n\n* `Resolution`: how the crime was resolved\n\n* `Address`: approximate street address of crime\n\n* `X`: longitude\n\n* `Y`: latitude\n\n\nOur aim is to see whether the category of crime depends on the day of\nthe week and the district in which it occurred. However, there are a\nlot of crime categories, so we will focus on the top four\n\"interesting\" ones, which are the ones included in this data file.\n\nSome of the model-fitting takes a while (you'll see why below). If\nyou're using R Markdown, you can wait for the models to fit each time\nyou re-run your document, or insert `cache=T` in the top line\nof your code chunk (the one with `r` in curly brackets in it,\nabove the actual code). Put a comma and the `cache=T` inside\nthe curly brackets.  What that does is to re-run that code chunk only\nif it changes; if it hasn't changed it will use the saved results from\nlast time it was run. That can save you a lot of waiting around.\n\n\n\n(a) Read in the data and display the dataset (or, at least,\npart of it).\n\n\n\n(b) Fit a multinomial logistic\nregression that predicts crime category from day of week and\ndistrict. (You don't need to look at it.) The model-fitting produces\nsome output. (If you're using R Markdown, that will\ncome with it.)\n\n\n\n\n(c) Fit a model that predicts Category from only the\ndistrict. Hand in the output from the fitting process as well. \n\n\n\n\n(d) Use `anova` to compare the two models you just\nobtained. What does the `anova` tell you?\n\n\n\n\n(e) Using your preferred model, obtain predicted probabilities\nthat a crime will be of each of these four categories for each day of\nthe week in the `TENDERLOIN` district (the name is ALL\nCAPS). This will mean constructing a data frame to predict from,\nobtaining the predictions and then displaying them suitably.\n\n\n\n\n(f) Describe briefly how the weekend days Saturday and Sunday\ndiffer from the rest.\n\n\n\n\n\n\n\n##  Crimes in San Francisco -- the data\n\n\n The data in [link](http://utsc.utoronto.ca/~butler/d29/sfcrime.csv) is a huge dataset of crimes committed in San\nFrancisco between 2003 and 2015. The variables are:\n\n\n\n* `Dates`: the date and time of the crime\n\n* `Category`: the category of crime, eg. \"larceny\" or\n\"vandalism\" (response).\n\n* `Descript`: detailed description of crime.\n\n* `DayOfWeek`: the day of the week of the crime.\n\n* `PdDistrict`: the name of the San Francisco Police\nDepartment district in which the crime took place.\n\n* `Resolution`: how the crime was resolved\n\n* `Address`: approximate street address of crime\n\n* `X`: longitude\n\n* `Y`: latitude\n\n\nOur aim is to see whether the category of crime depends on the day of\nthe week and the district in which it occurred. However, there are a\nlot of crime categories, so we will focus on the top four\n\"interesting\" ones, which we will have to discover.\n\n\n\n(a) Read in the data and verify that you have these columns and a\nlot of rows. (The data may take a moment to read in. You will see why.)\n\n\n\n(b) How is the response variable here different to the one in\nthe question about steak preferences (and therefore why would\n`multinom` from package `nnet` be the method of choice)?\n\n\n\n(c) Find out which crime categories there are, and \narrange them in order of how many crimes there were in each\ncategory. \n\n\n\n(d) Which are the four most frequent \"interesting\" crime\ncategories, that is to say, not including \"other offenses\" and\n\"non-criminal\"? Get them into a vector called\n`my.crimes`. See if you can find a way of doing this  that\ndoesn't involve typing them in (for full marks).\n\n\n\n(e) (Digression, but needed for the next part.) The R vector\n`letters` contains the lowercase letters from `a` to\n`z`. Consider the vector `('a','m',3,'Q')`. Some of\nthese are found amongst the lowercase letters, and some not. Type\nthese into a vector `v` and explain briefly why \n`v %in% letters` produces what it does.\n\n\n\n\n(f) We are going to `filter` only the rows of our data frame that\nhave one of the crimes in `my.crimes` as their\n`Category`. Also, `select` only the columns\n`Category`, `DayOfWeek` and `PdDistrict`. Save\nthe resulting data frame and display its structure. (You should have a\nlot fewer rows than you did before.)\n\n\n\n\n(g) Save these data in a file `sfcrime1.csv`.\n\n\n\n  \n\n\n\n\n\n##  What sports do these athletes play?\n\n\n The data at\n[link](http://ritsokiguess.site/datafiles/ais.txt) are physical\nand physiological measurements of 202 male and female Australian elite\nathletes. The data values are separated by *tabs*. We are going\nto see whether we can predict the sport an athlete plays from their\nheight and weight.\n\nThe sports, if you care, are respectively basketball, \n\"field athletics\" (eg. shot put, javelin throw, long jump etc.),\ngymnastics, netball, rowing, swimming, 400m running, tennis, sprinting\n(100m or 200m running), water polo.\n\n\n\n(a) Read in the data and display the first few rows.\n\n\n(b) Make a scatterplot of height vs.\\ weight, with the points\ncoloured by what sport the athlete plays. Put height on the $x$-axis\nand weight on the $y$-axis.\n\n\n\n(c) Explain briefly why a multinomial model (`multinom`\nfrom `nnet`) would be the best thing to use to predict sport\nplayed from the other variables.\n\n\n(d) Fit a suitable model for predicting sport played from\nheight and weight. (You don't need to look at the results.) 100\nsteps isn't quite enough, so set `maxit` equal to a larger\nnumber to allow the estimation to finish.\n\n\n(e) Demonstrate using `anova` that `Wt` should\nnot be removed from this model.\n\n\n(f) Make a data frame consisting of all combinations of\n`Ht` 160, 180 and 200 (cm), and `Wt` 50, 75, and 100\n(kg), and use it to obtain predicted probabilities of athletes of\nthose heights and weights playing each of the sports. Display the\nresults. You might have to display them smaller, or reduce the\nnumber of decimal places^[For this, use `round`.] \nto fit them on the page.\n\n\n(g) For an athlete who is 180 cm tall and weighs 100 kg, what\n sport would you guess they play? How sure are you that you are\n right? Explain briefly.\n\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n##  Finding non-missing values\n\n\n <a name=\"part:prepare-next\">*</a> This is to prepare you for something in the next\nquestion. It's meant to be easy.\n\nIn R, the code `NA` stands for \"missing value\" or\n\"value not known\". In R, `NA` should not have quotes around\nit. (It is a special code, not a piece of text.)\n\n\n(a) Create a vector `v` that contains some numbers and some\nmissing values, using `c()`. Put those values into a\none-column data frame.\n \nSolution\n\n\nLike this. The arrangement of numbers and missing values doesn't\nmatter, as long as you have some of each:\n::: {.cell}\n\n```{.r .cell-code}\nv <- c(1, 2, NA, 4, 5, 6, 9, NA, 11)\nmydata <- tibble(v)\nmydata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 1\n      v\n  <dbl>\n1     1\n2     2\n3    NA\n4     4\n5     5\n6     6\n7     9\n8    NA\n9    11\n```\n:::\n:::\n\n     \n\nThis has one column called `v`.\n \n$\\blacksquare$\n\n(b) Obtain a new column containing `is.na(v)`. When is this true and when is this false?\n \nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata <- mydata %>% mutate(isna = is.na(v))\nmydata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 2\n      v isna \n  <dbl> <lgl>\n1     1 FALSE\n2     2 FALSE\n3    NA TRUE \n4     4 FALSE\n5     5 FALSE\n6     6 FALSE\n7     9 FALSE\n8    NA TRUE \n9    11 FALSE\n```\n:::\n:::\n\n     \n\nThis is `TRUE` if the corresponding element of `v` is\nmissing (in my case, the third value and the second-last one), and\n`FALSE` otherwise (when there is an actual value there).\n \n$\\blacksquare$\n\n(c) The symbol `!` means \"not\" in R (and other\nprogramming languages). What does `!is.na(v)` do? Create a\nnew column containing that.\n \nSolution\n\n\nTry it and see. Give it whatever name you like. My name reflects\nthat I know what it's going to do:\n::: {.cell}\n\n```{.r .cell-code}\nmydata <- mydata %>% mutate(notisna = !is.na(v))\nmydata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 3\n      v isna  notisna\n  <dbl> <lgl> <lgl>  \n1     1 FALSE TRUE   \n2     2 FALSE TRUE   \n3    NA TRUE  FALSE  \n4     4 FALSE TRUE   \n5     5 FALSE TRUE   \n6     6 FALSE TRUE   \n7     9 FALSE TRUE   \n8    NA TRUE  FALSE  \n9    11 FALSE TRUE   \n```\n:::\n:::\n\n     \n\nThis is the logical opposite of `is.na`: it's true if there is\na value, and false if it's missing.\n \n$\\blacksquare$\n\n(d) Use `filter` to display just the\nrows of your data frame that have a non-missing value of `v`.\n\n \nSolution\n\n\n`filter` takes a column to say which rows to pick, in\nwhich case the column should contain something that either *is*\n`TRUE` or `FALSE`, or something that can be\ninterpreted that way:\n::: {.cell}\n\n```{.r .cell-code}\nmydata %>% filter(notisna)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 x 3\n      v isna  notisna\n  <dbl> <lgl> <lgl>  \n1     1 FALSE TRUE   \n2     2 FALSE TRUE   \n3     4 FALSE TRUE   \n4     5 FALSE TRUE   \n5     6 FALSE TRUE   \n6     9 FALSE TRUE   \n7    11 FALSE TRUE   \n```\n:::\n:::\n\n   \n\nor you can provide `filter` something that can be calculated\nfrom what's in the data frame, and also returns something that is\neither true or false:\n\n::: {.cell}\n\n```{.r .cell-code}\nmydata %>% filter(!is.na(v))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 x 3\n      v isna  notisna\n  <dbl> <lgl> <lgl>  \n1     1 FALSE TRUE   \n2     2 FALSE TRUE   \n3     4 FALSE TRUE   \n4     5 FALSE TRUE   \n5     6 FALSE TRUE   \n6     9 FALSE TRUE   \n7    11 FALSE TRUE   \n```\n:::\n:::\n\n \n\nIn either case, I only have non-missing values of `v`.\n \n$\\blacksquare$\n\n\n\n\n\n\n##  European Social Survey and voting\n\n\n The European Social Survey is a giant survey carried out\nacross Europe covering demographic information, attitudes to and\namount of education, politics and so on. In this question, we will\ninvestigate what might make British people vote for a certain\npolitical party.\n\nThe information for this question is in a (large) spreadsheet at\n[link](http://ritsokiguess.site/datafiles/ess.csv). There is also a\n\"codebook\" at\n[link](http://ritsokiguess.site/datafiles/ess-codebook.pdf) that\ntells you what all the variables are. The ones we will use are the\nlast five columns of the spreadsheet, described on pages 11 onwards of\nthe codebook. (I could have given you more, but I didn't want to make\nthis any more complicated than it already was.)\n\n\n\n(a) Read in the `.csv` file, and verify that you have lots\nof rows and columns.\n \nSolution\n\n\nThe obvious way. Printing it out will display some of the data\nand tell you how many rows and columns you have:\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/ess.csv\"\ness <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 2286 Columns: 17\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (4): cntry, cname, cproddat, name\ndbl (13): cedition, cseqno, essround, edition, idno, dweight, pspwght, pweig...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ness\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,286 x 17\n   cntry cname    cedition cproddat cseqno name  essround edition   idno dweight\n   <chr> <chr>       <dbl> <chr>     <dbl> <chr>    <dbl>   <dbl>  <dbl>   <dbl>\n 1 GB    ESS1-6e~        1 26.11.2~ 134168 ESS6~        6     2.1 101014   1.01 \n 2 GB    ESS1-6e~        1 26.11.2~ 134169 ESS6~        6     2.1 101048   2.02 \n 3 GB    ESS1-6e~        1 26.11.2~ 134170 ESS6~        6     2.1 101055   1.01 \n 4 GB    ESS1-6e~        1 26.11.2~ 134171 ESS6~        6     2.1 101089   0.505\n 5 GB    ESS1-6e~        1 26.11.2~ 134172 ESS6~        6     2.1 101097   0.505\n 6 GB    ESS1-6e~        1 26.11.2~ 134173 ESS6~        6     2.1 101113   1.01 \n 7 GB    ESS1-6e~        1 26.11.2~ 134174 ESS6~        6     2.1 101121   0.505\n 8 GB    ESS1-6e~        1 26.11.2~ 134175 ESS6~        6     2.1 101139   0.505\n 9 GB    ESS1-6e~        1 26.11.2~ 134176 ESS6~        6     2.1 101154   1.01 \n10 GB    ESS1-6e~        1 26.11.2~ 134177 ESS6~        6     2.1 101170   1.01 \n# i 2,276 more rows\n# i 7 more variables: pspwght <dbl>, pweight <dbl>, prtvtgb <dbl>, gndr <dbl>,\n#   agea <dbl>, eduyrs <dbl>, inwtm <dbl>\n```\n:::\n:::\n\n     \n\n2286 rows and 17 columns.\n \n$\\blacksquare$ \n\n(b) <a name=\"part:whatvar\">*</a> Use the codebook to find out what the columns\n`prtvtgb`, `gndr`, `agea`, `eduyrs` and\n`inwtm` are.  What do the values 1 and 2 for `gndr`\nmean? (You don't, at this point, have to worry about the values for\nthe other variables.)\n \nSolution\n\n\nRespectively, political party voted for at last election, gender\n(of respondent), age at interview, years of full-time education,\nlength of interview (in minutes). For `gndr`, male  is 1\nand female is 2.\n \n$\\blacksquare$\n\n(c) The three major political parties in Britain are the\nConservative, Labour and Liberal Democrat. (These, for your\ninformation, correspond roughly to the Canadian Progressive\nConservative, NDP and Liberal parties.) For the variable that\ncorresponds to \"political party voted for at the last election\",\nwhich values correspond to these three parties?\n \nSolution\n\n\n1, 2 and 3 respectively. (That was easy!)\n \n$\\blacksquare$\n\n(d) Normally, I would give you a tidied-up\ndata set. But I figure you could use some practice tidying this one\nup. As the codebook shows, there are some numerical codes for\nmissing values, and we want to omit those.\nWe want just the columns `prtvtgb` through `inwtm`\nfrom the right side of the spreadsheet.  Use `dplyr` or\n`tidyr` tools to (i) select only these columns, (ii) include\nthe rows that correspond to people who voted for one of the three\nmajor parties, (iii) include the rows that have an age at interview\nless than 999, (iv) include the rows that have less than 40 years of\neducation, (v) include the rows that are not missing on\n`inwtm` (use the idea from Question~<a href=\"#part:prepare-next\">here</a>\nfor (v)).  The last four of those (the inclusion of rows) can be\ndone in one go.\n \nSolution\n\n\nThe major parties are numbered 1,\n2 and 3, so we can select the ones less than 4 (or\n`<=3`). The reference back to the last question is a hint\nto use `!is.na()`. It also works to use `drop_na`, if you are familiar with that.\n\n::: {.cell}\n\n```{.r .cell-code}\ness %>%\n  select(prtvtgb:inwtm) %>%\n  filter(prtvtgb < 4, agea < 999, eduyrs < 40, !is.na(inwtm)) -> ess.major\n```\n:::\n\n     \nYou might get a weird error in `select`, something about\n\"unused argument\". If this happens to you, it's *not* because\nyou used `select` wrong, it's because you used the wrong\n`select`! There is one in `MASS`, and you need to make\nsure that this package is \"detached\" so that you use the\n`select` you want, namely the one in `dplyr`, loaded\nwith the `tidyverse`.  Use the instructions at the end of the\nmobile phones question or the abortion question to do this.\n\nThe other way around this is to say, instead of `select`,\n`dplyr::select` with two colons. This means \n\"the `select` that lives in `dplyr`, no other\", and is what\nWikipedia calls \"disambiguation\": out of several things with the\nsame name, you say which one you mean.\n\nIf you do the pipeline, you will probably *not* get it right the\nfirst time. (I didn't.) For debugging, try out one step at a time, and\nsummarize what you have so far, so that you can check it for\ncorrectness. A handy trick for that is to make the last piece of your\npipeline `summary()`, which produces a summary of the columns of\nthe resulting data frame. For example, I first did this (note that my\n`filter` is a lot simpler than the one above):\n\n::: {.cell}\n\n```{.r .cell-code}\ness %>%\n  select(prtvtgb:inwtm) %>%\n  filter(prtvtgb < 4, !is.na(inwtm)) %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    prtvtgb           gndr            agea            eduyrs     \n Min.   :1.000   Min.   :1.000   Min.   : 18.00   Min.   : 0.00  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.: 44.00   1st Qu.:11.00  \n Median :2.000   Median :2.000   Median : 58.00   Median :13.00  \n Mean   :1.803   Mean   :1.572   Mean   : 61.74   Mean   :14.23  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 71.00   3rd Qu.:16.00  \n Max.   :3.000   Max.   :2.000   Max.   :999.00   Max.   :88.00  \n     inwtm       \n Min.   :  7.00  \n 1st Qu.: 35.00  \n Median : 41.00  \n Mean   : 43.54  \n 3rd Qu.: 50.00  \n Max.   :160.00  \n```\n:::\n:::\n\n \n\nThe mean of a categorical variable like party voted for or gender\ndoesn't make much sense, but it looks as if all the values are sensible\nones (1 to 3 and 1, 2 respectively). However, the maximum values of\nage and years of education look like missing value codes, hence the\nother requirements I put in the question.^[If you do not take  out the `NA` values, they are shown separately on the end of  the `summary` for that column.]\n\nDisplaying as the last step of your pipeline also works, but the\nadvantage of `summary` is that you get to see whether there are\nany unusual values, in this case unusually *large* values that\nare missing value codes.\n \n$\\blacksquare$\n\n(e) Why is my response variable nominal rather than  ordinal? How can I tell?\nWhich R function should I use, therefore, to fit my model?\n \nSolution\n\n\nThe response variable is political party voted for. There is no\n(obvious) ordering to this (unless you want to try to place the parties\non a left-right spectrum), so this is nominal, and you'll need\n`multinom` from package `nnet`.\n\nIf I had included the minor parties and you were working on a\nleft-right spectrum, you would have had to decide where to put the\nsomewhat libertarian Greens^[The American Green party is more libertarian than Green parties elsewhere.] \nor the parties that exist only in Northern Ireland.^[Northern Ireland's political parties distinguish themselves by politics *and* religion. Northern Ireland has always had political tensions between its Protestants and its Catholics.]\n \n$\\blacksquare$\n\n(f) <a name=\"part:full\">*</a> Take the political party voted for, and turn it into a\nfactor, by feeding it into `factor`.\nFit an appropriate model to predict political party voted\nfor at the last election (as a factor) from all the other\nvariables. Gender is really a categorical variable too, but since\nthere are only two possible values it can be treated as a number.\n \nSolution\n\n\nThis, or something like it. `multinom` lives in package\n`nnet`, which you'll have to install first if you haven't already:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nnet)\ness.1 <- multinom(factor(prtvtgb) ~ gndr + agea + eduyrs + inwtm, data = ess.major)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  18 (10 variable)\ninitial  value 1343.602829 \niter  10 value 1256.123798\nfinal  value 1247.110080 \nconverged\n```\n:::\n:::\n\n     \n\nOr create a factor version of your response in the data frame first:\n\n::: {.cell}\n\n```{.r .cell-code}\ness.major <- ess.major %>% mutate(party = factor(prtvtgb))\n```\n:::\n\n \n\nand then:\n\n::: {.cell}\n\n```{.r .cell-code}\ness.1a <- multinom(party ~ gndr + agea + eduyrs + inwtm, data = ess.major)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  18 (10 variable)\ninitial  value 1343.602829 \niter  10 value 1256.123798\nfinal  value 1247.110080 \nconverged\n```\n:::\n:::\n\n \n\n \n$\\blacksquare$\n\n(g) We have a lot of explanatory variables. The standard way to\ntest whether we need all of them is to take one of them out at a time,\nand test which ones we can remove. This is a lot of work. We won't do\nthat. Instead,\nthe R function `step` does what you want. You feed `step`\ntwo things: a fitted model object, and the option `trace=0`\n(otherwise you get a lot of output). The final part of the output from\n`step` tells you which explanatory variables you need to keep.\nRun `step` on your fitted model. Which explanatory variables\nneed to stay in the model here?\n\n \nSolution\n\n\nI tried to give you lots of hints here:\n\n::: {.cell}\n\n```{.r .cell-code}\ness.2a <- step(ess.1, trace = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrying - gndr \ntrying - agea \ntrying - eduyrs \ntrying - inwtm \n# weights:  15 (8 variable)\ninitial  value 1343.602829 \niter  10 value 1248.343563\nfinal  value 1248.253638 \nconverged\ntrying - agea \ntrying - eduyrs \ntrying - inwtm \n```\n:::\n\n```{.r .cell-code}\ness.2a\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nmultinom(formula = factor(prtvtgb) ~ agea + eduyrs + inwtm, data = ess.major)\n\nCoefficients:\n  (Intercept)        agea     eduyrs       inwtm\n2    1.632266 -0.02153694 -0.0593757 0.009615167\n3   -1.281031 -0.01869263  0.0886487 0.009337084\n\nResidual Deviance: 2496.507 \nAIC: 2512.507 \n```\n:::\n:::\n\n   \n\nIf you didn't save your output in a variable, you'll get my last bit\nautomatically.\n\nThe end of the output gives us coefficients for (and thus tells us we\nneed to keep)  age, years of education and interview length.  \n\nThe actual numbers don't mean much; it's the indication that the\nvariable has stayed in the model that makes a\ndifference.^[There are three political parties; using the first  as a baseline, there are therefore $3-1=2$ coefficients for each variable.]\n\nIf you're wondering about the process: first `step` tries to\ntake out each explanatory variable, one at a time, from the starting\nmodel (the one that contains all the variables). Then it finds the\nbest model out of those and fits it. (It doesn't tell us which model\nthis is, but evidently it's the one without gender.) Then it takes\n*that* model and tries to remove its explanatory variables one at\na time (there are only three of them left). Having decided it cannot\nremove any of them, it stops, and shows us what's left.\n\nLeaving out the `trace=0` shows more output and more detail on\nthe process, but I figured this was enough (and this way, you don't\nhave to wade through all of that output). Try values like 1 or 2 for\n`trace` and see what you get.\n\n \n$\\blacksquare$\n\n(h) Fit the model indicated by `step` (in the last part).\n\n \nSolution\n\n Copy and paste, and take out the\nvariables you don't need. Or, better, save the output from\n`step` in a variable. This then becomes a fitted model\nobject and you can look at it any of the ways you can look at a\nmodel fit.  I found that gender needed to be removed, but if yours\nis different, follow through with whatever your `step` said\nto do.\n\n::: {.cell}\n\n```{.r .cell-code}\ness.2 <- multinom(party ~ agea + eduyrs + inwtm, data = ess.major)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  15 (8 variable)\ninitial  value 1343.602829 \niter  10 value 1248.343563\nfinal  value 1248.253638 \nconverged\n```\n:::\n:::\n\n   \n\nIf you saved the output from `step`, you'll already have this and you don't need to do it again:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(ess.2, ess.2a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of Multinomial Models\n\nResponse: party\nResponse: factor(prtvtgb)\n                  Model Resid. df Resid. Dev   Test    Df LR stat. Pr(Chi)\n1 agea + eduyrs + inwtm      2438   2496.507                              \n2 agea + eduyrs + inwtm      2438   2496.507 1 vs 2     0        0       1\n```\n:::\n:::\n\n \n\nSame model.\n \n$\\blacksquare$\n\n(i) I didn't think that interview length could possibly be\nrelevant to which party a person voted for. Test whether interview\nlength can be removed from your model of the last part. What do you\nconclude? (Note that `step` and this test may disagree.)\n\n \nSolution\n\n\nFit the model without `inwtm`:\n\n::: {.cell}\n\n```{.r .cell-code}\ness.3 <- multinom(party ~ agea + eduyrs, data = ess.major)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  12 (6 variable)\ninitial  value 1343.602829 \niter  10 value 1250.418281\nfinal  value 1250.417597 \nconverged\n```\n:::\n:::\n\n   \n\nand then use `anova` to compare them:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(ess.3, ess.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of Multinomial Models\n\nResponse: party\n                  Model Resid. df Resid. Dev   Test    Df LR stat.   Pr(Chi)\n1         agea + eduyrs      2440   2500.835                                \n2 agea + eduyrs + inwtm      2438   2496.507 1 vs 2     2 4.327917 0.1148695\n```\n:::\n:::\n\n \n\nThe P-value, 0.1149, is not small, which says that the smaller model\nis good, ie.\\ the one *without* interview length.\n\nI thought `drop1` would also work here, but it appears not to:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(ess.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrying - gndr \n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in if (trace) {: argument is not interpretable as logical\n```\n:::\n:::\n\n \n\nI think that's a bug in `multinom`, since normally if\n`step` works, then `drop1` will work too (normally\n`step` *uses* `drop1`).\n\nThe reason for the disagreement between `step` and\n`anova` is that `step` will tend to\nkeep marginal explanatory variables, that is, ones that are\n\"potentially interesting\" but whose P-values might not be less than\n0.05. There is still no substitute for your judgement in figuring out\nwhat to do! `step` uses a thing called AIC to decide what to\ndo, rather than actually doing a test. If you know about \n\"adjusted R-squared\" in choosing explanatory variables for a \nregression, it's the same idea: a variable can be not quite\nsignificant but still make the adjusted R-squared go up (typically\nonly a little).\n \n$\\blacksquare$\n\n(j) Use your best model to obtain predictions from some\nsuitably chosen combinations of values of the explanatory variables\nthat remain. (If you have quantitative explanatory variables left,\nyou could use their first and third quartiles as values to predict\nfrom. Running `summary` on the data frame will get summaries\nof all the variables.)\n \nSolution\n\n\nFirst make our new data frame of values to predict\nfrom. \nYou can use\n`quantile` or `summary` to find the quartiles. I\nonly had `agea` and `eduyrs` left, having decided\nthat interview time really ought to come out:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(ess.major)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    prtvtgb           gndr            agea           eduyrs     \n Min.   :1.000   Min.   :1.000   Min.   :18.00   Min.   : 0.00  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:44.00   1st Qu.:11.00  \n Median :2.000   Median :2.000   Median :58.00   Median :13.00  \n Mean   :1.803   Mean   :1.574   Mean   :57.19   Mean   :13.45  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:71.00   3rd Qu.:16.00  \n Max.   :3.000   Max.   :2.000   Max.   :94.00   Max.   :33.00  \n     inwtm       party  \n Min.   :  7.0   1:484  \n 1st Qu.: 35.0   2:496  \n Median : 41.0   3:243  \n Mean   : 43.7          \n 3rd Qu.: 50.0          \n Max.   :160.0          \n```\n:::\n:::\n\n     \n\nQuartiles for age are 44 and 71, and for years of education are 11 and 16.\n\nThis time, instead of predicting for variable values that `predictions` chooses for us (like a five-number summary), we are predicting for \"custom\" values, ones that *we* chose. To set that up, the `marginaleffects` way is to use `datagrid` like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = ess.3, agea = c(44, 71), eduyrs = c(11, 16))\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  party agea eduyrs\n1     2   44     11\n2     2   44     16\n3     2   71     11\n4     2   71     16\n```\n:::\n:::\n\nWhat `datagrid` does is to make all combinations of your variable values, and along with that, to use \"typical\" values for the others: the mean, in the case of quantitative variables like `inwtm`, and the most common category for categorical ones like `party`. If you feed `datagrid` a `model` first, it only includes variables in that model, which is easier to make sense of:\n\n::: {.cell}\n\n```{.r .cell-code}\ndatagrid(newdata = ess.major, agea = c(44, 71), eduyrs = c(11, 16))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   prtvtgb     gndr    inwtm party agea eduyrs\n1 1.802944 1.573998 43.69665     2   44     11\n2 1.802944 1.573998 43.69665     2   44     16\n3 1.802944 1.573998 43.69665     2   71     11\n4 1.802944 1.573998 43.69665     2   71     16\n```\n:::\n:::\n\n\nThe other variables don't make much sense, since they are really categorical but expressed as numbers, but they are not in the best model, so that doesn't do any harm. (In other cases, you might need to be more careful.)\n\nNext, we feed this into `predictions`, using the above dataframe as `newdata`, and with our best model, `ess.3` (the one *without* interview length). The results might confuse you at first, since you will probably get an error:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(ess.3, newdata = new))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   rowid group  estimate  std.error statistic       p.value  conf.low conf.high\n1      1     1 0.3331882 0.02156171 15.452770  7.226963e-54 0.2909280 0.3754484\n2      1     2 0.5093898 0.02325642 21.903192 2.422069e-106 0.4638081 0.5549715\n3      1     3 0.1574220 0.01579483  9.966679  2.132393e-23 0.1264647 0.1883793\n4      2     1 0.3471967 0.01864674 18.619703  2.224424e-77 0.3106498 0.3837436\n5      2     2 0.3955666 0.01916657 20.638356  1.242163e-94 0.3580008 0.4331324\n6      2     3 0.2572367 0.01678567 15.324782  5.222932e-53 0.2243374 0.2901360\n7      3     1 0.4551429 0.01925000 23.643788 1.367424e-123 0.4174136 0.4928722\n8      3     2 0.4085120 0.01905711 21.436196 6.143472e-102 0.3711607 0.4458633\n9      3     3 0.1363451 0.01323939 10.298442  7.161156e-25 0.1103964 0.1622939\n10     4     1 0.4675901 0.02385952 19.597634  1.619822e-85 0.4208263 0.5143539\n11     4     2 0.3127561 0.02208791 14.159604  1.629085e-45 0.2694646 0.3560476\n12     4     3 0.2196538 0.01956056 11.229426  2.923706e-29 0.1813158 0.2579918\n   party agea eduyrs\n1      2   44     11\n2      2   44     11\n3      2   44     11\n4      2   44     16\n5      2   44     16\n6      2   44     16\n7      2   71     11\n8      2   71     11\n9      2   71     11\n10     2   71     16\n11     2   71     16\n12     2   71     16\n```\n:::\n:::\n\nThe error message gives you a hint about what to do: add a `type = \"probs\"` to `predictions` (which is a consequence of how `multinom` works):\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(ess.3, newdata = new, type = \"probs\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   rowid group  estimate  std.error statistic       p.value  conf.low conf.high\n1      1     1 0.3331882 0.02156171 15.452770  7.226963e-54 0.2909280 0.3754484\n2      1     2 0.5093898 0.02325642 21.903192 2.422069e-106 0.4638081 0.5549715\n3      1     3 0.1574220 0.01579483  9.966679  2.132393e-23 0.1264647 0.1883793\n4      2     1 0.3471967 0.01864674 18.619703  2.224424e-77 0.3106498 0.3837436\n5      2     2 0.3955666 0.01916657 20.638356  1.242163e-94 0.3580008 0.4331324\n6      2     3 0.2572367 0.01678567 15.324782  5.222932e-53 0.2243374 0.2901360\n7      3     1 0.4551429 0.01925000 23.643788 1.367424e-123 0.4174136 0.4928722\n8      3     2 0.4085120 0.01905711 21.436196 6.143472e-102 0.3711607 0.4458633\n9      3     3 0.1363451 0.01323939 10.298442  7.161156e-25 0.1103964 0.1622939\n10     4     1 0.4675901 0.02385952 19.597634  1.619822e-85 0.4208263 0.5143539\n11     4     2 0.3127561 0.02208791 14.159604  1.629085e-45 0.2694646 0.3560476\n12     4     3 0.2196538 0.01956056 11.229426  2.923706e-29 0.1813158 0.2579918\n   party agea eduyrs\n1      2   44     11\n2      2   44     11\n3      2   44     11\n4      2   44     16\n5      2   44     16\n6      2   44     16\n7      2   71     11\n8      2   71     11\n9      2   71     11\n10     2   71     16\n11     2   71     16\n12     2   71     16\n```\n:::\n:::\n\nThere are twelve rows for our four predictions, because there are three predictions for each of our four \"people\": the probabilities of each one voting for each of the three parties. The party predicted for is in the column `group`, and the probability of each person (labelled by `rowid`) voting for that party is in `predicted`. Let's simplify things by keeping only those columns and the ones we are predicting for:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(ess.3, newdata = new, type = \"probs\")) %>% \n  select(rowid, group, estimate, agea, eduyrs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   rowid group  estimate agea eduyrs\n1      1     1 0.3331882   44     11\n2      1     2 0.5093898   44     11\n3      1     3 0.1574220   44     11\n4      2     1 0.3471967   44     16\n5      2     2 0.3955666   44     16\n6      2     3 0.2572367   44     16\n7      3     1 0.4551429   71     11\n8      3     2 0.4085120   71     11\n9      3     3 0.1363451   71     11\n10     4     1 0.4675901   71     16\n11     4     2 0.3127561   71     16\n12     4     3 0.2196538   71     16\n```\n:::\n:::\n\nand then pivot wider to get all three predictions for each person on one line:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(ess.3, newdata = new, type = \"probs\")) %>% \n  select(rowid, group, estimate, agea, eduyrs) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 6\n  rowid  agea eduyrs   `1`   `2`   `3`\n  <int> <dbl>  <dbl> <dbl> <dbl> <dbl>\n1     1    44     11 0.333 0.509 0.157\n2     2    44     16 0.347 0.396 0.257\n3     3    71     11 0.455 0.409 0.136\n4     4    71     16 0.468 0.313 0.220\n```\n:::\n:::\n\n\n$\\blacksquare$\n\n(k) What is the effect of increasing age? What is the effect of\nan increase in years of education?\n \nSolution\n\n\nTo assess the effect of age, hold years of education\nconstant. Thus, compare lines 1 and 3 (or 2 and 4):\nincreasing age tends to increase the chance that a person will\nvote Conservative (party 1), and decrease the chance that a person\nwill vote Labour (party 2). There doesn't seem to be much effect\nof age on the chance that a person will vote Liberal Democrat.\n\nTo assess education, hold age constant, and thus compare rows 1\nand 2 (or rows 3 and 4). This time, there isn't much effect on the\nchances of voting Conservative, but as education increases, the\nchance of voting Labour goes down, and the chance of voting\nLiberal Democrat goes up.\n\nA little history: back 150 or so years ago, Britain had two\npolitical parties, the Tories and the Whigs. The Tories became the\nConservative party (and hence, in Britain and in Canada, the\nConservatives are nicknamed Tories^[It amuses me that Toronto's current (2021) mayor, named Tory, is politically a Tory.]). \nThe Whigs became Liberals. At\nabout the same time as \nworking people got to vote (not women, yet, but working men) the\nLabour Party came into existence. The Labour Party has always been\naffiliated with working people and trades unions, like the NDP\nhere. But power has typically alternated between Conservative and\nLabour goverments, with the Liberals as a third party. In the\n1980s a new party called the Social Democrats came onto the scene,\nbut on realizing that they couldn't make much of a dent by\nthemselves, they merged with the Liberals to form the Liberal\nDemocrats, which became a slightly stronger third party.\n\nI was curious about what the effect of interview length would\nbe. Presumably, the effect is small, but I have no idea which way\nit would be. To assess this, this is `predictions` again, but this time we can let\n`predictions` pick some values for `inwtm` for us, and leave everything else at their mean. We have \nto remember to use the model `ess.2` that contained interview length, this time: \n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(ess.2, variables = \"inwtm\", type = \"probs\")) %>% \n  select(rowid, group, estimate, agea, eduyrs, inwtm) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6,115 x 7\n   rowid  agea eduyrs inwtm   `1`   `2`    `3`\n   <int> <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1     1    63     16     7 0.520 0.283 0.196 \n 2     2    51     13     7 0.454 0.382 0.164 \n 3     3    67     13     7 0.536 0.320 0.144 \n 4     4    80      9     7 0.581 0.333 0.0858\n 5     5    67     21     7 0.522 0.194 0.284 \n 6     6    50     13     7 0.448 0.386 0.165 \n 7     7    82     16     7 0.615 0.223 0.163 \n 8     8    78     11     7 0.584 0.310 0.107 \n 9     9    80     10     7 0.588 0.317 0.0948\n10    10    61     11     7 0.496 0.380 0.125 \n# i 6,105 more rows\n```\n:::\n:::\n\nAs interview length goes up (for a respondent with average age and years of education, though the pattern would be the same for people of different ages and different amounts of education), the respondent is less likely to vote Conservative (party 1), and more likely to vote for one of the other two parties.\n\nBut, as we \nsuspected, the effect is small (except for that very long interview length) and not really worth worrying about.\n \n$\\blacksquare$\n\n\n\n\n\n##  Alligator food\n\n\n What do alligators most like to eat? 219 alligators were captured\nin four Florida lakes. Each alligator's stomach contents were\nobserved, and the food that the alligator had eaten  was classified\ninto one of five categories: fish, invertebrates (such as snails or\ninsects), reptiles (such as turtles), birds, and \"other\" (such as\namphibians, plants or rocks). The researcher noted for each alligator\nwhat that alligator had most of in its stomach, as well as the gender\nof each alligator and whether it was \"large\" or \"small\" (greater\nor less than 2.3 metres in length). The data can be found in\n[link](http://ritsokiguess.site/datafiles/alligator.txt). The\nnumbers in the data set (apart from the first column) are all\nfrequencies. (You can ignore that first column \"profile\".)\n\nOur aim is to predict food type from the other variables.\n\n\n\n(a) Read in the data and display the first few lines. Describe\nhow the data are not \"tidy\".\n\n\nSolution\n\n\nSeparated by exactly one space:\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/alligator.txt\"\ngators.orig <- read_delim(my_url, \" \")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 16 Columns: 9\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (3): Gender, Size, Lake\ndbl (6): profile, Fish, Invertebrate, Reptile, Bird, Other\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ngators.orig\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 16 x 9\n   profile Gender Size  Lake      Fish Invertebrate Reptile  Bird Other\n     <dbl> <chr>  <chr> <chr>    <dbl>        <dbl>   <dbl> <dbl> <dbl>\n 1       1 f      <2.3  george       3            9       1     0     1\n 2       2 m      <2.3  george      13           10       0     2     2\n 3       3 f      >2.3  george       8            1       0     0     1\n 4       4 m      >2.3  george       9            0       0     1     2\n 5       5 f      <2.3  hancock     16            3       2     2     3\n 6       6 m      <2.3  hancock      7            1       0     0     5\n 7       7 f      >2.3  hancock      3            0       1     2     3\n 8       8 m      >2.3  hancock      4            0       0     1     2\n 9       9 f      <2.3  oklawaha     3            9       1     0     2\n10      10 m      <2.3  oklawaha     2            2       0     0     1\n11      11 f      >2.3  oklawaha     0            1       0     1     0\n12      12 m      >2.3  oklawaha    13            7       6     0     0\n13      13 f      <2.3  trafford     2            4       1     1     4\n14      14 m      <2.3  trafford     3            7       1     0     1\n15      15 f      >2.3  trafford     0            1       0     0     0\n16      16 m      >2.3  trafford     8            6       6     3     5\n```\n:::\n:::\n\n       \n\nThe last five columns are all frequencies. Or, one of the variables\n(food type) is spread over five columns instead of being contained in\none. Either is good.\n\nMy choice of \"temporary\" name reflects that I'm going to obtain a\n\"tidy\" data frame called `gators` in a moment.\n\n$\\blacksquare$\n\n(b) Use `pivot_longer` to arrange the data\nsuitably for analysis (which will be using\n`multinom`). Demonstrate (by looking at the first few rows\nof your new data frame) that you now have something tidy.\n\n\nSolution\n\n\nI'm creating my \"official\" data frame here:\n\n::: {.cell}\n\n```{.r .cell-code}\ngators.orig %>% \n  pivot_longer(Fish:Other, names_to = \"Food.type\", values_to = \"Frequency\") -> gators\ngators\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 80 x 6\n   profile Gender Size  Lake   Food.type    Frequency\n     <dbl> <chr>  <chr> <chr>  <chr>            <dbl>\n 1       1 f      <2.3  george Fish                 3\n 2       1 f      <2.3  george Invertebrate         9\n 3       1 f      <2.3  george Reptile              1\n 4       1 f      <2.3  george Bird                 0\n 5       1 f      <2.3  george Other                1\n 6       2 m      <2.3  george Fish                13\n 7       2 m      <2.3  george Invertebrate        10\n 8       2 m      <2.3  george Reptile              0\n 9       2 m      <2.3  george Bird                 2\n10       2 m      <2.3  george Other                2\n# i 70 more rows\n```\n:::\n:::\n\n\n       \nI gave my\ncolumn names Capital Letters to make them consistent with the others\n(and in an attempt to stop infesting my brain with annoying\nvariable-name errors when I fit models later).\n\nLooking at the first few lines reveals that I now have a column of\nfood types and one column of frequencies, both of which are what I\nwanted. I can check that I have all the different food types by\nfinding the distinct ones:\n\n::: {.cell}\n\n```{.r .cell-code}\ngators %>% distinct(Food.type)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 1\n  Food.type   \n  <chr>       \n1 Fish        \n2 Invertebrate\n3 Reptile     \n4 Bird        \n5 Other       \n```\n:::\n:::\n\n \n\n(Think about why `count` would be confusing here.)\n\nNote that `Food.type` is text (`chr`) rather than being a\nfactor. I'll hold my breath and see what happens when I fit a model\nwhere it is supposed to be a factor.\n\n$\\blacksquare$\n\n(c) What is different about this problem, compared to\nQuestion <a href=\"#q:abortion\">here</a>, that would make \n`multinom` the right tool to use?\n\n\nSolution\n\n\nLook at the response variable `Food.type` (or whatever\nyou called it): this has multiple categories, but they are\n*not ordered* in any logical way. Thus, in short, a nominal\nresponse. \n\n$\\blacksquare$\n\n(d) Fit a suitable multinomial model predicting food type from\ngender, size and lake. Does each row represent one alligator or more\nthan one? If more than one, account for this in your modelling.\n\n\nSolution\n\n\nEach row of the tidy `gators` represents as many\nalligators as are in the `Frequency` column. That is, if\nyou look at female small alligators in Lake George that ate\nmainly fish, there are three of those.^[When you have variables that are categories, you might have more than one individual with exactly the same categories; on the other hand, if they had measured *Size* as, say, length in centimetres, it would have been very unlikely to get two alligators of exactly the same size.]\nThis to remind you to include the `weights` piece,\notherwise `multinom` will assume that you have *one*\nobservation per line and not as many as the number in\n`Frequency`.\n\n*That* is the\nreason that `count` earlier would have been confusing:\nit would have told you how many *rows* contained each\nfood type, rather than how many *alligators*, and these\nwould have been different:\n::: {.cell}\n\n```{.r .cell-code}\ngators %>% count(Food.type)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 2\n  Food.type        n\n  <chr>        <int>\n1 Bird            16\n2 Fish            16\n3 Invertebrate    16\n4 Other           16\n5 Reptile         16\n```\n:::\n\n```{.r .cell-code}\ngators %>% count(Food.type, wt = Frequency)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 2\n  Food.type        n\n  <chr>        <dbl>\n1 Bird            13\n2 Fish            94\n3 Invertebrate    61\n4 Other           32\n5 Reptile         19\n```\n:::\n:::\n\n         \n\nEach food type appears on 16 rows, but is the favoured diet of very\ndifferent numbers of *alligators*. Note the use of `wt=`\nto specify a frequency variable.^[Discovered by me two minutes  ago.]\n\nYou ought to understand *why* those are different.\n\nAll right, back to modelling:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nnet)\ngators.1 <- multinom(Food.type ~ Gender + Size + Lake,\n  weights = Frequency, data = gators\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  35 (24 variable)\ninitial  value 352.466903 \niter  10 value 270.228588\niter  20 value 268.944257\nfinal  value 268.932741 \nconverged\n```\n:::\n:::\n\n       \n\nThis worked, even though `Food.type` was actually text. I guess\nit got converted to a factor. The ordering of the levels doesn't\nmatter here anyway, since this is not an ordinal model.\n\nNo need to look at it, since the output is kind of confusing anyway: \n::: {.cell}\n\n```{.r .cell-code}\nsummary(gators.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nmultinom(formula = Food.type ~ Gender + Size + Lake, data = gators, \n    weights = Frequency)\n\nCoefficients:\n             (Intercept)     Genderm   Size>2.3 Lakehancock Lakeoklawaha\nFish           2.4322304  0.60674971 -0.7308535  -0.5751295    0.5513785\nInvertebrate   2.6012531  0.14378459 -2.0671545  -2.3557377    1.4645820\nOther          1.0014505  0.35423803 -1.0214847   0.1914537    0.5775317\nReptile       -0.9829064 -0.02053375 -0.1741207   0.5534169    3.0807416\n             Laketrafford\nFish          -1.23681053\nInvertebrate  -0.08096493\nOther          0.32097943\nReptile        1.82333205\n\nStd. Errors:\n             (Intercept)   Genderm  Size>2.3 Lakehancock Lakeoklawaha\nFish           0.7706940 0.6888904 0.6523273   0.7952147     1.210229\nInvertebrate   0.7917210 0.7292510 0.7084028   0.9463640     1.232835\nOther          0.8747773 0.7623738 0.7250455   0.9072182     1.374545\nReptile        1.2827234 0.9088217 0.8555051   1.3797755     1.591542\n             Laketrafford\nFish            0.8661187\nInvertebrate    0.8814625\nOther           0.9589807\nReptile         1.3388017\n\nResidual Deviance: 537.8655 \nAIC: 585.8655 \n```\n:::\n:::\n\n \n\nYou get one coefficient for each variable (along the top) and for each\nresponse group (down the side), using the first group as a baseline\neverywhere. These numbers are hard to interpret; doing predictions is\nmuch easier.\n\n$\\blacksquare$\n\n(e) Do a test to see whether `Gender` should stay in\nthe model. (This will entail fitting another model.) What do you conclude?\n\n\nSolution\n\n\nThe other model to fit is the one *without* the variable\nyou're testing:\n::: {.cell}\n\n```{.r .cell-code}\ngators.2 <- update(gators.1, . ~ . - Gender)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  30 (20 variable)\ninitial  value 352.466903 \niter  10 value 272.246275\niter  20 value 270.046891\nfinal  value 270.040139 \nconverged\n```\n:::\n:::\n\n       \n\nI did `update` here to show you that it works, but of course\nthere's no problem in just writing out the whole model again and\ntaking out `Gender`, preferably by copying and pasting:\n\n::: {.cell}\n\n```{.r .cell-code}\ngators.2x <- multinom(Food.type ~ Size + Lake,\n  weights = Frequency, data = gators\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  30 (20 variable)\ninitial  value 352.466903 \niter  10 value 272.246275\niter  20 value 270.046891\nfinal  value 270.040139 \nconverged\n```\n:::\n:::\n\n \n\nand then you compare the models with and without `Gender` using `anova`:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(gators.2, gators.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of Multinomial Models\n\nResponse: Food.type\n                 Model Resid. df Resid. Dev   Test    Df LR stat.   Pr(Chi)\n1          Size + Lake       300   540.0803                                \n2 Gender + Size + Lake       296   537.8655 1 vs 2     4 2.214796 0.6963214\n```\n:::\n:::\n\n \n\nThe P-value is not small, so the two models fit equally well, and\ntherefore we should go with the smaller, simpler one: that is, the one\nwithout `Gender`.\n\nSometimes `drop1` works here too (and sometimes it doesn't, for\nreasons I haven't figured out):\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(gators.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrying - Gender \n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in if (trace) {: argument is not interpretable as logical\n```\n:::\n:::\n\n \n\nI don't even know what this error message means, never mind what to do\nabout it.\n\n$\\blacksquare$\n\n(f) Predict the probability that an alligator\nprefers each food type, given its size, gender (if necessary) and\nthe lake it was found \nin, using the more appropriate of the two models that you have\nfitted so far.  This means (i) \nmaking a data frame for prediction, and (ii) obtaining and\ndisplaying the predicted probabilities in a way that is easy to read.\n\n\nSolution\n\nOur best model `gators.2` contains size and lake, so we need to predict for all combinations of those. \n\nFirst, get hold of those combinations, which you can do this way:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = gators.2, \n                Size = levels(factor(gators$Size)),\n                Lake = levels(factor(gators$Lake)))\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Food.type Frequency Size     Lake\n1      Fish    2.7375 <2.3   george\n2      Fish    2.7375 <2.3  hancock\n3      Fish    2.7375 <2.3 oklawaha\n4      Fish    2.7375 <2.3 trafford\n5      Fish    2.7375 >2.3   george\n6      Fish    2.7375 >2.3  hancock\n7      Fish    2.7375 >2.3 oklawaha\n8      Fish    2.7375 >2.3 trafford\n```\n:::\n:::\n\nThere are four lakes and two sizes, so we should (and do) have eight rows.\n\nNext, use this to make predictions, not forgetting the `type = \"probs\"` that you need for this kind of model. The last step puts the \"long\" predictions \"wider\" so that you can eyeball them:\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(gators.2, newdata = new)) %>% \n  select(group, estimate, Size, Lake) %>% \n  pivot_wider(names_from = group, values_from = estimate) -> preds1\npreds1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 7\n  Size  Lake        Bird  Fish Invertebrate  Other Reptile\n  <chr> <chr>      <dbl> <dbl>        <dbl>  <dbl>   <dbl>\n1 <2.3  george   0.0297  0.452       0.413  0.0938  0.0116\n2 <2.3  hancock  0.0704  0.535       0.0931 0.254   0.0475\n3 <2.3  oklawaha 0.00882 0.258       0.602  0.0539  0.0772\n4 <2.3  trafford 0.0359  0.184       0.517  0.174   0.0888\n5 >2.3  george   0.0811  0.657       0.140  0.0979  0.0239\n6 >2.3  hancock  0.141   0.570       0.0231 0.194   0.0718\n7 >2.3  oklawaha 0.0294  0.458       0.249  0.0687  0.195 \n8 >2.3  trafford 0.108   0.296       0.193  0.201   0.202 \n```\n:::\n:::\n\nI saved these to look at again later (you don't need to).\n\nIf you thought that the better model was the one with `Gender`\nin it, or you otherwise forgot that you didn't need `Gender`\nthen you needed to include `Gender` in `new` also.\n\n\n$\\blacksquare$\n\n(g) What do you think is the most important way in which the\nlakes differ? (Hint: look at where the biggest predicted\nprobabilities are.)\n\n\nSolution\n\n\nHere are the predictions again:\n::: {.cell}\n\n```{.r .cell-code}\npreds1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 7\n  Size  Lake        Bird  Fish Invertebrate  Other Reptile\n  <chr> <chr>      <dbl> <dbl>        <dbl>  <dbl>   <dbl>\n1 <2.3  george   0.0297  0.452       0.413  0.0938  0.0116\n2 <2.3  hancock  0.0704  0.535       0.0931 0.254   0.0475\n3 <2.3  oklawaha 0.00882 0.258       0.602  0.0539  0.0772\n4 <2.3  trafford 0.0359  0.184       0.517  0.174   0.0888\n5 >2.3  george   0.0811  0.657       0.140  0.0979  0.0239\n6 >2.3  hancock  0.141   0.570       0.0231 0.194   0.0718\n7 >2.3  oklawaha 0.0294  0.458       0.249  0.0687  0.195 \n8 >2.3  trafford 0.108   0.296       0.193  0.201   0.202 \n```\n:::\n:::\n\n       \nFollowing my own hint: the preferred diet in George and Hancock lakes\nis fish, but the preferred diet in Oklawaha and Trafford lakes is (at\nleast sometimes) invertebrates. That is to say, the preferred diet in\nthose last two lakes is less likely to be invertebrates than it is in\nthe first two (comparing for alligators of the same size).  This is\ntrue for both large and small alligators, as it should be, since there\nis no interaction in the model.\n\nThat will do, though you can also note that reptiles are more\ncommonly found in the last two lakes, and birds sometimes appear\nin the diet in Hancock and Trafford but rarely in the other two\nlakes. \n\nAnother way to think about this is to hold size constant and\ncompare lakes (and then check that it applies to the other size\ntoo). In this case, you'd find the biggest predictions among the\nfirst four rows, and then check that the pattern persists in the\nsecond four rows. (It does.)\n\nI think looking at predicted probabilities like this is the\neasiest way to see what the model is telling you. \n\n$\\blacksquare$\n\n(h) How would you describe the major difference between the\ndiets of the small and large alligators?\n\n\nSolution\n\n\nSame idea: hold lake constant, and compare small and large, then\ncheck that your conclusion holds for the other lakes as it should.\nFor example, in George Lake, the large alligators are more\nlikely to eat fish, and less likely to eat invertebrates,\ncompared to the small ones. The other food types are not that\nmuch different, though you might also note that birds appear\nmore in the diets of large alligators than small ones. \nDoes that hold in the other lakes? I think so, though there is\nless difference for fish in Hancock lake than the others (where\ninvertebrates are rare for both sizes). Birds don't commonly\nappear in any alligator's diets, but where they do, they are\ncommoner for large alligators than small ones.\n    \n$\\blacksquare$\n\n\n\n\n\n##  Crimes in San Francisco\n\n\n\nThe data in\n[link](http://ritsokiguess.site/datafiles/sfcrime1.csv) is a subset\nof a huge\ndataset of crimes committed in San Francisco between 2003 and\n2015. The variables are:\n\n\n\n* `Dates`: the date and time of the crime\n\n* `Category`: the category of crime, eg. \"larceny\" or\n\"vandalism\" (response).\n\n* `Descript`: detailed description of crime.\n\n* `DayOfWeek`: the day of the week of the crime.\n\n* `PdDistrict`: the name of the San Francisco Police\nDepartment district in which the crime took place.\n\n* `Resolution`: how the crime was resolved\n\n* `Address`: approximate street address of crime\n\n* `X`: longitude\n\n* `Y`: latitude\n\n\nOur aim is to see whether the category of crime depends on the day of\nthe week and the district in which it occurred. However, there are a\nlot of crime categories, so we will focus on the top four\n\"interesting\" ones, which are the ones included in this data file.\n\nSome of the model-fitting takes a while (you'll see why below). If\nyou're using R Markdown, you can wait for the models to fit each time\nyou re-run your document, or insert `cache=T` in the top line\nof your code chunk (the one with `r` in curly brackets in it,\nabove the actual code). Put a comma and the `cache=T` inside\nthe curly brackets.  What that does is to re-run that code chunk only\nif it changes; if it hasn't changed it will use the saved results from\nlast time it was run. That can save you a lot of waiting around.\n\n\n\n(a) Read in the data and display the dataset (or, at least,\npart of it).\n\n\nSolution\n\n\nThe usual:\n::: {.cell hash='nominal-response_cache/pdf/napoli_a43198b163b1d9c03c0862450728f96e'}\n\n```{.r .cell-code}\nmy_url <- \"http://utsc.utoronto.ca/~butler/d29/sfcrime1.csv\"\nsfcrime <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 359528 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (3): Category, DayOfWeek, PdDistrict\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nsfcrime\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 359,528 x 3\n   Category      DayOfWeek PdDistrict\n   <chr>         <chr>     <chr>     \n 1 LARCENY/THEFT Wednesday NORTHERN  \n 2 LARCENY/THEFT Wednesday PARK      \n 3 LARCENY/THEFT Wednesday INGLESIDE \n 4 VEHICLE THEFT Wednesday INGLESIDE \n 5 VEHICLE THEFT Wednesday BAYVIEW   \n 6 LARCENY/THEFT Wednesday RICHMOND  \n 7 LARCENY/THEFT Wednesday CENTRAL   \n 8 LARCENY/THEFT Wednesday CENTRAL   \n 9 LARCENY/THEFT Wednesday NORTHERN  \n10 ASSAULT       Wednesday INGLESIDE \n# i 359,518 more rows\n```\n:::\n:::\n\n     \n\nThis is a tidied-up version of the data, with only the variables we'll\nlook at, and only the observations from one of the \"big four\"\ncrimes, a mere 300,000 of them. This is the data set we created earlier.\n    \n$\\blacksquare$\n\n(b) Fit a multinomial logistic\nregression that predicts crime category from day of week and\ndistrict. (You don't need to look at it.) The model-fitting produces\nsome output, which will at least convince you that it is working, since it takes some time.\n\n\n\nSolution\n\n\nThe modelling part is easy enough, as long as you can get the\nuppercase letters in the right places:\n\n::: {.cell}\n\n```{.r .cell-code}\nsfcrime.1 <- multinom(Category ~ DayOfWeek + PdDistrict, data=sfcrime)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  68 (48 variable)\ninitial  value 498411.639069 \niter  10 value 430758.073422\niter  20 value 430314.270403\niter  30 value 423303.587698\niter  40 value 420883.528523\niter  50 value 418355.242764\nfinal  value 418149.979622 \nconverged\n```\n:::\n:::\n\n \n  \n$\\blacksquare$\n\n(c) Fit a model that predicts Category from only the\ndistrict. \n\n\n\nSolution\n\n\nSame idea. Write it out, or use `update`:\n::: {.cell}\n\n```{.r .cell-code}\nsfcrime.2 <- update(sfcrime.1, . ~ . - DayOfWeek)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  44 (30 variable)\ninitial  value 498411.639069 \niter  10 value 426003.543845\niter  20 value 425542.806828\niter  30 value 421715.787609\nfinal  value 418858.235297 \nconverged\n```\n:::\n:::\n\n   \n  \n$\\blacksquare$\n\n(d) Use `anova` to compare the two models you just\nobtained. What does the `anova` tell you?\n\n\n\nSolution\n\n\nThis:\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(sfcrime.2, sfcrime.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of Multinomial Models\n\nResponse: Category\n                   Model Resid. df Resid. Dev   Test    Df LR stat. Pr(Chi)\n1             PdDistrict   1078554   837716.5                              \n2 DayOfWeek + PdDistrict   1078536   836300.0 1 vs 2    18 1416.511       0\n```\n:::\n:::\n\n \n\nThis is a very small P-value. The null hypothesis is that the two\nmodels are equally good, and this is clearly rejected. We need the\nbigger model: that is, we need to keep `DayOfWeek` in there,\nbecause the pattern of crimes (in each district) differs over day of\nweek. \n\nOne reason the P-value came out so small is that we have a ton of\ndata, so that even a very small difference between days of the week\ncould come out very strongly significant. The Machine Learning people\n(this is a machine learning dataset) don't worry so much about tests\nfor that reason: they are more concerned with predicting things well,\nso they just throw everything into the model and see what comes out.\n  \n\n$\\blacksquare$\n\n(e) Using your preferred model, obtain predicted probabilities\nthat a crime will be of each of these four categories for each day of\nthe week in the `TENDERLOIN` district (the name is ALL\nCAPS). This will mean constructing a data frame to predict from,\nobtaining the predictions and then displaying them suitably.\n\n\n\nSolution\n\n\nUse `datagrid` first to get the combinations you want (and only those), namely all the days of the week, but only the district called TENDERLOIN.\n\nSo, let's get the days of the week. The easiest way is to count them and ignore the counts:^[I know I just said not to do this kind of thing, but counting is really fast, while unnecessary predictions are really slow.]\n\n::: {.cell}\n\n```{.r .cell-code}\nsfcrime %>% count(DayOfWeek) %>% \n  pull(DayOfWeek) -> daysofweek\ndaysofweek\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Friday\"    \"Monday\"    \"Saturday\"  \"Sunday\"    \"Thursday\"  \"Tuesday\"  \n[7] \"Wednesday\"\n```\n:::\n:::\n\nAnother way is the `levels(factor())` thing you may have seen before:\n\n::: {.cell}\n\n```{.r .cell-code}\nlevels(factor(sfcrime$DayOfWeek))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Friday\"    \"Monday\"    \"Saturday\"  \"Sunday\"    \"Thursday\"  \"Tuesday\"  \n[7] \"Wednesday\"\n```\n:::\n:::\n\n\nNow we can use these in `datagrid`:^[There is only one district, so we can just put that in here.]\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = sfcrime.1, DayOfWeek = daysofweek, PdDistrict = \"TENDERLOIN\")\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Category DayOfWeek PdDistrict\n1 LARCENY/THEFT    Friday TENDERLOIN\n2 LARCENY/THEFT    Monday TENDERLOIN\n3 LARCENY/THEFT  Saturday TENDERLOIN\n4 LARCENY/THEFT    Sunday TENDERLOIN\n5 LARCENY/THEFT  Thursday TENDERLOIN\n6 LARCENY/THEFT   Tuesday TENDERLOIN\n7 LARCENY/THEFT Wednesday TENDERLOIN\n```\n:::\n:::\n\nGood. And then predict for just these. This is slow, but not as slow as predicting for all districts. I'm saving the result of this slow part, so that it doesn't matter if I change my mind later about what to do with it. I want to make sure that I don't have to do it again, is all:^[The same thing applies if you are doing something like webscraping, that is to say downloading stuff from the web that you then want to do something with. Download it *once* and save it, then you can take as long as you need to decide what you're doing with it.]\n\n::: {.cell}\n\n```{.r .cell-code}\np <- cbind(predictions(sfcrime.1, newdata = new, type = \"probs\"))\np\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   rowid         group   estimate    std.error statistic       p.value\n1      1       ASSAULT 0.21259967 0.0028673349  74.14539  0.000000e+00\n2      1 DRUG/NARCOTIC 0.46581221 0.0040502749 115.00755  0.000000e+00\n3      1 LARCENY/THEFT 0.29077933 0.0031835632  91.33770  0.000000e+00\n4      1 VEHICLE THEFT 0.03080879 0.0010194200  30.22189 1.221864e-200\n5      2       ASSAULT 0.20724299 0.0028411642  72.94298  0.000000e+00\n6      2 DRUG/NARCOTIC 0.50014661 0.0040370454 123.88927  0.000000e+00\n7      2 LARCENY/THEFT 0.26560692 0.0030484708  87.12792  0.000000e+00\n8      2 VEHICLE THEFT 0.02700348 0.0009049965  29.83822 1.248322e-195\n9      3       ASSAULT 0.24259978 0.0031132688  77.92446  0.000000e+00\n10     3 DRUG/NARCOTIC 0.42007509 0.0040967405 102.53886  0.000000e+00\n11     3 LARCENY/THEFT 0.30606108 0.0032828100  93.23143  0.000000e+00\n12     3 VEHICLE THEFT 0.03126405 0.0010360034  30.17756 4.667404e-200\n13     4       ASSAULT 0.25485968 0.0032526073  78.35550  0.000000e+00\n14     4 DRUG/NARCOTIC 0.42872910 0.0041997370 102.08475  0.000000e+00\n15     4 LARCENY/THEFT 0.28686823 0.0032231643  89.00205  0.000000e+00\n16     4 VEHICLE THEFT 0.02954299 0.0009878381  29.90671 1.609455e-196\n17     5       ASSAULT 0.19387537 0.0027041710  71.69494  0.000000e+00\n18     5 DRUG/NARCOTIC 0.51795000 0.0039560503 130.92604  0.000000e+00\n19     5 LARCENY/THEFT 0.26176575 0.0029981098  87.31026  0.000000e+00\n20     5 VEHICLE THEFT 0.02640888 0.0008846650  29.85184 8.309417e-196\n21     6       ASSAULT 0.19424470 0.0027095476  71.68898  0.000000e+00\n22     6 DRUG/NARCOTIC 0.52086682 0.0039563543 131.65323  0.000000e+00\n23     6 LARCENY/THEFT 0.25933313 0.0029864954  86.83527  0.000000e+00\n24     6 VEHICLE THEFT 0.02555535 0.0008585414  29.76600 1.076500e-194\n25     7       ASSAULT 0.18748667 0.0026233949  71.46719  0.000000e+00\n26     7 DRUG/NARCOTIC 0.54002868 0.0038638472 139.76450  0.000000e+00\n27     7 LARCENY/THEFT 0.24794975 0.0028805071  86.07851  0.000000e+00\n28     7 VEHICLE THEFT 0.02453490 0.0008243000  29.76453 1.124893e-194\n     conf.low  conf.high      Category DayOfWeek PdDistrict\n1  0.20697980 0.21821954 LARCENY/THEFT    Friday TENDERLOIN\n2  0.45787382 0.47375060 LARCENY/THEFT    Friday TENDERLOIN\n3  0.28453966 0.29701900 LARCENY/THEFT    Friday TENDERLOIN\n4  0.02881077 0.03280682 LARCENY/THEFT    Friday TENDERLOIN\n5  0.20167441 0.21281157 LARCENY/THEFT    Monday TENDERLOIN\n6  0.49223414 0.50805907 LARCENY/THEFT    Monday TENDERLOIN\n7  0.25963203 0.27158182 LARCENY/THEFT    Monday TENDERLOIN\n8  0.02522972 0.02877724 LARCENY/THEFT    Monday TENDERLOIN\n9  0.23649789 0.24870167 LARCENY/THEFT  Saturday TENDERLOIN\n10 0.41204562 0.42810455 LARCENY/THEFT  Saturday TENDERLOIN\n11 0.29962689 0.31249527 LARCENY/THEFT  Saturday TENDERLOIN\n12 0.02923352 0.03329458 LARCENY/THEFT  Saturday TENDERLOIN\n13 0.24848469 0.26123467 LARCENY/THEFT    Sunday TENDERLOIN\n14 0.42049776 0.43696043 LARCENY/THEFT    Sunday TENDERLOIN\n15 0.28055094 0.29318552 LARCENY/THEFT    Sunday TENDERLOIN\n16 0.02760687 0.03147912 LARCENY/THEFT    Sunday TENDERLOIN\n17 0.18857529 0.19917545 LARCENY/THEFT  Thursday TENDERLOIN\n18 0.51019628 0.52570371 LARCENY/THEFT  Thursday TENDERLOIN\n19 0.25588957 0.26764194 LARCENY/THEFT  Thursday TENDERLOIN\n20 0.02467497 0.02814279 LARCENY/THEFT  Thursday TENDERLOIN\n21 0.18893409 0.19955532 LARCENY/THEFT   Tuesday TENDERLOIN\n22 0.51311250 0.52862113 LARCENY/THEFT   Tuesday TENDERLOIN\n23 0.25347971 0.26518656 LARCENY/THEFT   Tuesday TENDERLOIN\n24 0.02387264 0.02723806 LARCENY/THEFT   Tuesday TENDERLOIN\n25 0.18234491 0.19262843 LARCENY/THEFT Wednesday TENDERLOIN\n26 0.53245568 0.54760168 LARCENY/THEFT Wednesday TENDERLOIN\n27 0.24230406 0.25359544 LARCENY/THEFT Wednesday TENDERLOIN\n28 0.02291930 0.02615050 LARCENY/THEFT Wednesday TENDERLOIN\n```\n:::\n:::\n\nThis, as you remember, is long format, so grab the columns you need from it and pivot wider. The columns you want to make sure you have are `estimate`, `group` (the type of crime), and the day of week:\n\n::: {.cell}\n\n```{.r .cell-code}\np %>% \n  select(group, estimate, DayOfWeek) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 x 5\n  DayOfWeek ASSAULT `DRUG/NARCOTIC` `LARCENY/THEFT` `VEHICLE THEFT`\n  <chr>       <dbl>           <dbl>           <dbl>           <dbl>\n1 Friday      0.213           0.466           0.291          0.0308\n2 Monday      0.207           0.500           0.266          0.0270\n3 Saturday    0.243           0.420           0.306          0.0313\n4 Sunday      0.255           0.429           0.287          0.0295\n5 Thursday    0.194           0.518           0.262          0.0264\n6 Tuesday     0.194           0.521           0.259          0.0256\n7 Wednesday   0.187           0.540           0.248          0.0245\n```\n:::\n:::\n\nSuccess. (If you don't get rid of enough, you still have 28 rows and a bunch of missing values; in that case, `pivot_wider` will infer that everything should be in its own row.)\n\n  \n$\\blacksquare$\n\n(f) Describe briefly how the weekend days Saturday and Sunday\ndiffer from the rest.\n\n\n\nSolution\n\n\nThe days ended up in some quasi-random order, but Saturday and Sunday are\nstill together, so we can still easily compare them with the rest.\nMy take is that the last two columns don't differ much between\nweekday and weekend, but the first two columns do: the probability\nof a crime being an assault is a bit higher on the weekend, and the\nprobability of a crime being drug-related is a bit lower.\nI will accept anything reasonable supported by the predictions you got.\nWe said there was a strongly significant day-of-week effect, but the\nchanges from weekday to weekend are actually pretty small (but the\nchanges from one weekday to another are even smaller). This supports\nwhat I guessed before, that with this much data even a small effect\n(the one shown here) is statistically\nsignificant.^[Statistical significance as an idea grew up in    the days before \"big data\".]\n\n\nExtra: I want to compare another district. What districts do we have?\n\n::: {.cell}\n\n```{.r .cell-code}\nsfcrime %>% count(PdDistrict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 x 2\n   PdDistrict     n\n   <chr>      <int>\n 1 BAYVIEW    31693\n 2 CENTRAL    38052\n 3 INGLESIDE  30102\n 4 MISSION    45277\n 5 NORTHERN   47750\n 6 PARK       19197\n 7 RICHMOND   18211\n 8 SOUTHERN   67981\n 9 TARAVAL    24981\n10 TENDERLOIN 36284\n```\n:::\n:::\n\n   \n\nThis is the number of our \"big four\" crimes committed in each\ndistrict. Let's look at the lowest-crime district `RICHMOND`. I\ncopy and paste my code. Since I want to compare two districts, I\ninclude both:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = sfcrime.1, PdDistrict = c(\"TENDERLOIN\", \"RICHMOND\"), DayOfWeek = daysofweek)\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Category PdDistrict DayOfWeek\n1  LARCENY/THEFT TENDERLOIN    Friday\n2  LARCENY/THEFT TENDERLOIN    Monday\n3  LARCENY/THEFT TENDERLOIN  Saturday\n4  LARCENY/THEFT TENDERLOIN    Sunday\n5  LARCENY/THEFT TENDERLOIN  Thursday\n6  LARCENY/THEFT TENDERLOIN   Tuesday\n7  LARCENY/THEFT TENDERLOIN Wednesday\n8  LARCENY/THEFT   RICHMOND    Friday\n9  LARCENY/THEFT   RICHMOND    Monday\n10 LARCENY/THEFT   RICHMOND  Saturday\n11 LARCENY/THEFT   RICHMOND    Sunday\n12 LARCENY/THEFT   RICHMOND  Thursday\n13 LARCENY/THEFT   RICHMOND   Tuesday\n14 LARCENY/THEFT   RICHMOND Wednesday\n```\n:::\n:::\n\nand then as we just did. I'm going to be a bit more selective about the columns I keep this time, since the display will be a bit wider and I don't want it to be too big for the page:\n\n::: {.cell}\n\n```{.r .cell-code}\np <- cbind(predictions(sfcrime.1, newdata = new, type = \"probs\"))\np\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   rowid         group   estimate    std.error statistic       p.value\n1      1       ASSAULT 0.21259967 0.0028673349  74.14539  0.000000e+00\n2      1 DRUG/NARCOTIC 0.46581221 0.0040502749 115.00755  0.000000e+00\n3      1 LARCENY/THEFT 0.29077933 0.0031835632  91.33770  0.000000e+00\n4      1 VEHICLE THEFT 0.03080879 0.0010194200  30.22189 1.221864e-200\n5      2       ASSAULT 0.20724299 0.0028411642  72.94298  0.000000e+00\n6      2 DRUG/NARCOTIC 0.50014661 0.0040370454 123.88927  0.000000e+00\n7      2 LARCENY/THEFT 0.26560692 0.0030484708  87.12792  0.000000e+00\n8      2 VEHICLE THEFT 0.02700348 0.0009049965  29.83822 1.248322e-195\n9      3       ASSAULT 0.24259978 0.0031132688  77.92446  0.000000e+00\n10     3 DRUG/NARCOTIC 0.42007509 0.0040967405 102.53886  0.000000e+00\n11     3 LARCENY/THEFT 0.30606108 0.0032828100  93.23143  0.000000e+00\n12     3 VEHICLE THEFT 0.03126405 0.0010360034  30.17756 4.667404e-200\n13     4       ASSAULT 0.25485968 0.0032526073  78.35550  0.000000e+00\n14     4 DRUG/NARCOTIC 0.42872910 0.0041997370 102.08475  0.000000e+00\n15     4 LARCENY/THEFT 0.28686823 0.0032231643  89.00205  0.000000e+00\n16     4 VEHICLE THEFT 0.02954299 0.0009878381  29.90671 1.609455e-196\n17     5       ASSAULT 0.19387537 0.0027041710  71.69494  0.000000e+00\n18     5 DRUG/NARCOTIC 0.51795000 0.0039560503 130.92604  0.000000e+00\n19     5 LARCENY/THEFT 0.26176575 0.0029981098  87.31026  0.000000e+00\n20     5 VEHICLE THEFT 0.02640888 0.0008846650  29.85184 8.309417e-196\n21     6       ASSAULT 0.19424470 0.0027095476  71.68898  0.000000e+00\n22     6 DRUG/NARCOTIC 0.52086682 0.0039563543 131.65323  0.000000e+00\n23     6 LARCENY/THEFT 0.25933313 0.0029864954  86.83527  0.000000e+00\n24     6 VEHICLE THEFT 0.02555535 0.0008585414  29.76600 1.076500e-194\n25     7       ASSAULT 0.18748667 0.0026233949  71.46719  0.000000e+00\n26     7 DRUG/NARCOTIC 0.54002868 0.0038638472 139.76450  0.000000e+00\n27     7 LARCENY/THEFT 0.24794975 0.0028805071  86.07851  0.000000e+00\n28     7 VEHICLE THEFT 0.02453490 0.0008243000  29.76453 1.124893e-194\n29     8       ASSAULT 0.16663037 0.0030461500  54.70196  0.000000e+00\n30     8 DRUG/NARCOTIC 0.04944552 0.0016390829  30.16658 6.502980e-200\n31     8 LARCENY/THEFT 0.54666349 0.0042749375 127.87637  0.000000e+00\n32     8 VEHICLE THEFT 0.23726062 0.0037982590  62.46562  0.000000e+00\n33     9       ASSAULT 0.17601746 0.0031981340  55.03755  0.000000e+00\n34     9 DRUG/NARCOTIC 0.05753044 0.0018846976  30.52503 1.213281e-204\n35     9 LARCENY/THEFT 0.54110340 0.0043282197 125.01755  0.000000e+00\n36     9 VEHICLE THEFT 0.22534869 0.0037372125  60.29860  0.000000e+00\n37    10       ASSAULT 0.18093521 0.0032325815  55.97236  0.000000e+00\n38    10 DRUG/NARCOTIC 0.04243108 0.0014310717  29.64986 3.405338e-193\n39    10 LARCENY/THEFT 0.54752727 0.0042817934 127.87335  0.000000e+00\n40    10 VEHICLE THEFT 0.22910644 0.0037350522  61.33955  0.000000e+00\n41    11       ASSAULT 0.19736750 0.0034530881  57.15681  0.000000e+00\n42    11 DRUG/NARCOTIC 0.04496576 0.0015153103  29.67429 1.648472e-193\n43    11 LARCENY/THEFT 0.53287082 0.0043336927 122.95999  0.000000e+00\n44    11 VEHICLE THEFT 0.22479591 0.0037250167  60.34762  0.000000e+00\n45    12       ASSAULT 0.16838408 0.0030970193  54.36972  0.000000e+00\n46    12 DRUG/NARCOTIC 0.06092432 0.0019808199  30.75712 9.820189e-208\n47    12 LARCENY/THEFT 0.54532596 0.0043170541 126.31900  0.000000e+00\n48    12 VEHICLE THEFT 0.22536564 0.0037349645  60.33943  0.000000e+00\n49    13       ASSAULT 0.17069994 0.0031277500  54.57595  0.000000e+00\n50    13 DRUG/NARCOTIC 0.06199196 0.0020116801  30.81601 1.599259e-208\n51    13 LARCENY/THEFT 0.54664723 0.0043163110 126.64686  0.000000e+00\n52    13 VEHICLE THEFT 0.22066087 0.0036892519  59.81182  0.000000e+00\n53    14       ASSAULT 0.17099635 0.0031257853  54.70508  0.000000e+00\n54    14 DRUG/NARCOTIC 0.06670490 0.0021439056  31.11373 1.570568e-212\n55    14 LARCENY/THEFT 0.54243172 0.0043128866 125.76999  0.000000e+00\n56    14 VEHICLE THEFT 0.21986703 0.0036721244  59.87461  0.000000e+00\n     conf.low  conf.high      Category PdDistrict DayOfWeek\n1  0.20697980 0.21821954 LARCENY/THEFT TENDERLOIN    Friday\n2  0.45787382 0.47375060 LARCENY/THEFT TENDERLOIN    Friday\n3  0.28453966 0.29701900 LARCENY/THEFT TENDERLOIN    Friday\n4  0.02881077 0.03280682 LARCENY/THEFT TENDERLOIN    Friday\n5  0.20167441 0.21281157 LARCENY/THEFT TENDERLOIN    Monday\n6  0.49223414 0.50805907 LARCENY/THEFT TENDERLOIN    Monday\n7  0.25963203 0.27158182 LARCENY/THEFT TENDERLOIN    Monday\n8  0.02522972 0.02877724 LARCENY/THEFT TENDERLOIN    Monday\n9  0.23649789 0.24870167 LARCENY/THEFT TENDERLOIN  Saturday\n10 0.41204562 0.42810455 LARCENY/THEFT TENDERLOIN  Saturday\n11 0.29962689 0.31249527 LARCENY/THEFT TENDERLOIN  Saturday\n12 0.02923352 0.03329458 LARCENY/THEFT TENDERLOIN  Saturday\n13 0.24848469 0.26123467 LARCENY/THEFT TENDERLOIN    Sunday\n14 0.42049776 0.43696043 LARCENY/THEFT TENDERLOIN    Sunday\n15 0.28055094 0.29318552 LARCENY/THEFT TENDERLOIN    Sunday\n16 0.02760687 0.03147912 LARCENY/THEFT TENDERLOIN    Sunday\n17 0.18857529 0.19917545 LARCENY/THEFT TENDERLOIN  Thursday\n18 0.51019628 0.52570371 LARCENY/THEFT TENDERLOIN  Thursday\n19 0.25588957 0.26764194 LARCENY/THEFT TENDERLOIN  Thursday\n20 0.02467497 0.02814279 LARCENY/THEFT TENDERLOIN  Thursday\n21 0.18893409 0.19955532 LARCENY/THEFT TENDERLOIN   Tuesday\n22 0.51311250 0.52862113 LARCENY/THEFT TENDERLOIN   Tuesday\n23 0.25347971 0.26518656 LARCENY/THEFT TENDERLOIN   Tuesday\n24 0.02387264 0.02723806 LARCENY/THEFT TENDERLOIN   Tuesday\n25 0.18234491 0.19262843 LARCENY/THEFT TENDERLOIN Wednesday\n26 0.53245568 0.54760168 LARCENY/THEFT TENDERLOIN Wednesday\n27 0.24230406 0.25359544 LARCENY/THEFT TENDERLOIN Wednesday\n28 0.02291930 0.02615050 LARCENY/THEFT TENDERLOIN Wednesday\n29 0.16066003 0.17260071 LARCENY/THEFT   RICHMOND    Friday\n30 0.04623297 0.05265806 LARCENY/THEFT   RICHMOND    Friday\n31 0.53828477 0.55504222 LARCENY/THEFT   RICHMOND    Friday\n32 0.22981617 0.24470507 LARCENY/THEFT   RICHMOND    Friday\n33 0.16974923 0.18228569 LARCENY/THEFT   RICHMOND    Monday\n34 0.05383650 0.06122438 LARCENY/THEFT   RICHMOND    Monday\n35 0.53262025 0.54958656 LARCENY/THEFT   RICHMOND    Monday\n36 0.21802389 0.23267349 LARCENY/THEFT   RICHMOND    Monday\n37 0.17459946 0.18727095 LARCENY/THEFT   RICHMOND  Saturday\n38 0.03962623 0.04523593 LARCENY/THEFT   RICHMOND  Saturday\n39 0.53913511 0.55591943 LARCENY/THEFT   RICHMOND  Saturday\n40 0.22178587 0.23642701 LARCENY/THEFT   RICHMOND  Saturday\n41 0.19059958 0.20413543 LARCENY/THEFT   RICHMOND    Sunday\n42 0.04199581 0.04793572 LARCENY/THEFT   RICHMOND    Sunday\n43 0.52437694 0.54136471 LARCENY/THEFT   RICHMOND    Sunday\n44 0.21749501 0.23209681 LARCENY/THEFT   RICHMOND    Sunday\n45 0.16231403 0.17445412 LARCENY/THEFT   RICHMOND  Thursday\n46 0.05704199 0.06480666 LARCENY/THEFT   RICHMOND  Thursday\n47 0.53686469 0.55378723 LARCENY/THEFT   RICHMOND  Thursday\n48 0.21804525 0.23268604 LARCENY/THEFT   RICHMOND  Thursday\n49 0.16456966 0.17683021 LARCENY/THEFT   RICHMOND   Tuesday\n50 0.05804914 0.06593478 LARCENY/THEFT   RICHMOND   Tuesday\n51 0.53818742 0.55510704 LARCENY/THEFT   RICHMOND   Tuesday\n52 0.21343007 0.22789167 LARCENY/THEFT   RICHMOND   Tuesday\n53 0.16486992 0.17712277 LARCENY/THEFT   RICHMOND Wednesday\n54 0.06250292 0.07090688 LARCENY/THEFT   RICHMOND Wednesday\n55 0.53397862 0.55088483 LARCENY/THEFT   RICHMOND Wednesday\n56 0.21266980 0.22706426 LARCENY/THEFT   RICHMOND Wednesday\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np %>% \n  select(group, estimate, DayOfWeek, PdDistrict) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 14 x 6\n   DayOfWeek PdDistrict ASSAULT `DRUG/NARCOTIC` `LARCENY/THEFT` `VEHICLE THEFT`\n   <chr>     <chr>        <dbl>           <dbl>           <dbl>           <dbl>\n 1 Friday    TENDERLOIN   0.213          0.466            0.291          0.0308\n 2 Monday    TENDERLOIN   0.207          0.500            0.266          0.0270\n 3 Saturday  TENDERLOIN   0.243          0.420            0.306          0.0313\n 4 Sunday    TENDERLOIN   0.255          0.429            0.287          0.0295\n 5 Thursday  TENDERLOIN   0.194          0.518            0.262          0.0264\n 6 Tuesday   TENDERLOIN   0.194          0.521            0.259          0.0256\n 7 Wednesday TENDERLOIN   0.187          0.540            0.248          0.0245\n 8 Friday    RICHMOND     0.167          0.0494           0.547          0.237 \n 9 Monday    RICHMOND     0.176          0.0575           0.541          0.225 \n10 Saturday  RICHMOND     0.181          0.0424           0.548          0.229 \n11 Sunday    RICHMOND     0.197          0.0450           0.533          0.225 \n12 Thursday  RICHMOND     0.168          0.0609           0.545          0.225 \n13 Tuesday   RICHMOND     0.171          0.0620           0.547          0.221 \n14 Wednesday RICHMOND     0.171          0.0667           0.542          0.220 \n```\n:::\n:::\n\n\n\nRichmond is obviously not a drug-dealing kind of place; most of its\ncrimes are theft of one kind or another. But the predicted effect of\nweekday vs. weekend is the same: Richmond doesn't have many assaults\nor drug crimes, but it also has more assaults and fewer drug crimes on\nthe weekend than during the week. There is not much effect of day of\nthe week on the other two crime types in either place.\n\nThe consistency of *pattern*, even though the prevalence of the\ndifferent crime types differs by location, is a feature of the model:\nwe fitted an additive model, that says there is an effect of weekday,\nand *independently* there is an effect of location. The\n*pattern* over weekday is the same for each location, implied by\nthe model. This may or may not be supported by the actual data.\n\nThe way to assess this is to fit a model with interaction (we will see\nmore of this when we revisit ANOVA later), and compare the fit. This one takes longer to fit:\n\n::: {.cell}\n\n```{.r .cell-code}\nsfcrime.3 <- update(sfcrime.1, . ~ . + DayOfWeek * PdDistrict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  284 (210 variable)\ninitial  value 498411.639069 \niter  10 value 429631.807781\niter  20 value 429261.427210\niter  30 value 428111.625547\niter  40 value 423807.031450\niter  50 value 421129.496196\niter  60 value 420475.833895\niter  70 value 419523.235916\niter  80 value 418621.612920\niter  90 value 418147.629782\niter 100 value 418036.670485\nfinal  value 418036.670485 \nstopped after 100 iterations\n```\n:::\n\n```{.r .cell-code}\n# anova(sfcrime.1,sfcrime.3)\n```\n:::\n\n \n\nThis one didn't actually complete the fitting process: it got to 100\ntimes around and stopped (since that's the default limit). We can make\nit go a bit further thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nsfcrime.3 <- update(sfcrime.1, .~.+DayOfWeek*PdDistrict, maxit=300)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  284 (210 variable)\ninitial  value 498411.639069 \niter  10 value 429631.807781\niter  20 value 429261.427210\niter  30 value 428111.625547\niter  40 value 423807.031450\niter  50 value 421129.496196\niter  60 value 420475.833895\niter  70 value 419523.235916\niter  80 value 418621.612920\niter  90 value 418147.629782\niter 100 value 418036.670485\niter 110 value 417957.337016\niter 120 value 417908.465189\niter 130 value 417890.580843\niter 140 value 417874.839492\niter 150 value 417867.449342\niter 160 value 417862.626213\niter 170 value 417858.654628\nfinal  value 417858.031854 \nconverged\n```\n:::\n\n```{.r .cell-code}\nanova(sfcrime.1, sfcrime.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of Multinomial Models\n\nResponse: Category\n                                          Model Resid. df Resid. Dev   Test\n1                        DayOfWeek + PdDistrict   1078536   836300.0       \n2 DayOfWeek + PdDistrict + DayOfWeek:PdDistrict   1078374   835716.1 1 vs 2\n     Df LR stat. Pr(Chi)\n1                       \n2   162 583.8955       0\n```\n:::\n:::\n\n \n\nThis time, we got to the end. (The `maxit=300` gets passed on\nto `multinom`, and says \"go around up to 300 times if needed\".) \nAs you will see if you try it, this takes a bit of time to\nrun. \n\nThis `anova` is also strongly significant, but in the light of\nthe previous discussion, the differential effect of day of week in\ndifferent districts might not be very big. We can even assess that; we\nhave all the machinery for the predictions, and we just have to apply\nthem to this model. The only thing is waiting for it to finish!\n\n::: {.cell}\n\n```{.r .cell-code}\np <- cbind(predictions(sfcrime.3, newdata = new, type = \"probs\"))\np\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   rowid         group   estimate   std.error statistic       p.value\n1      1       ASSAULT 0.21635131 0.005833041  37.09065 3.974590e-301\n2      1 DRUG/NARCOTIC 0.44958359 0.007047019  63.79769  0.000000e+00\n3      1 LARCENY/THEFT 0.30069679 0.006496085  46.28892  0.000000e+00\n4      1 VEHICLE THEFT 0.03336831 0.002544206  13.11541  2.687128e-39\n5      2       ASSAULT 0.20613374 0.005592608  36.85825 2.157188e-297\n6      2 DRUG/NARCOTIC 0.51256922 0.006910331  74.17433  0.000000e+00\n7      2 LARCENY/THEFT 0.25699538 0.006041219  42.54032  0.000000e+00\n8      2 VEHICLE THEFT 0.02430166 0.002128835  11.41548  3.499807e-30\n9      3       ASSAULT 0.23685611 0.006050816  39.14449  0.000000e+00\n10     3 DRUG/NARCOTIC 0.41026767 0.007000508  58.60541  0.000000e+00\n11     3 LARCENY/THEFT 0.32218205 0.006650827  48.44240  0.000000e+00\n12     3 VEHICLE THEFT 0.03069417 0.002454861  12.50343  7.150050e-36\n13     4       ASSAULT 0.25703306 0.006511485  39.47380  0.000000e+00\n14     4 DRUG/NARCOTIC 0.40019234 0.007300308  54.81855  0.000000e+00\n15     4 LARCENY/THEFT 0.31011835 0.006892104  44.99618  0.000000e+00\n16     4 VEHICLE THEFT 0.03265625 0.002648344  12.33082  6.181634e-35\n17     5       ASSAULT 0.19518511 0.005344290  36.52218 4.931453e-292\n18     5 DRUG/NARCOTIC 0.52867900 0.006730899  78.54508  0.000000e+00\n19     5 LARCENY/THEFT 0.25284647 0.005860734  43.14245  0.000000e+00\n20     5 VEHICLE THEFT 0.02328943 0.002033673  11.45191  2.300307e-30\n21     6       ASSAULT 0.19071370 0.005303163  35.96225 3.256615e-283\n22     6 DRUG/NARCOTIC 0.53955543 0.006728212  80.19299  0.000000e+00\n23     6 LARCENY/THEFT 0.24197567 0.005781232  41.85538  0.000000e+00\n24     6 VEHICLE THEFT 0.02775520 0.002217445  12.51674  6.046554e-36\n25     7       ASSAULT 0.19063090 0.005230346  36.44709 7.649130e-291\n26     7 DRUG/NARCOTIC 0.54577677 0.006629834  82.32134  0.000000e+00\n27     7 LARCENY/THEFT 0.24015853 0.005688149  42.22086  0.000000e+00\n28     7 VEHICLE THEFT 0.02343381 0.002014340  11.63349  2.784683e-31\n29     8       ASSAULT 0.17560619 0.007417830  23.67353 6.757676e-124\n30     8 DRUG/NARCOTIC 0.05241934 0.004345037  12.06419  1.632662e-33\n31     8 LARCENY/THEFT 0.54477973 0.009708695  56.11256  0.000000e+00\n32     8 VEHICLE THEFT 0.22719474 0.008169092  27.81151 3.148774e-170\n33     9       ASSAULT 0.16688547 0.007318284  22.80391 4.193666e-115\n34     9 DRUG/NARCOTIC 0.06038366 0.004675011  12.91626  3.644344e-38\n35     9 LARCENY/THEFT 0.55376000 0.009756470  56.75824  0.000000e+00\n36     9 VEHICLE THEFT 0.21897087 0.008116606  26.97813 2.668751e-160\n37    10       ASSAULT 0.18595137 0.007631708  24.36563 3.959202e-131\n38    10 DRUG/NARCOTIC 0.04414673 0.004029413  10.95612  6.210541e-28\n39    10 LARCENY/THEFT 0.52442616 0.009795983  53.53482  0.000000e+00\n40    10 VEHICLE THEFT 0.24547574 0.008441853  29.07842 6.730205e-186\n41    11       ASSAULT 0.18514063 0.007643884  24.22075 1.344910e-129\n42    11 DRUG/NARCOTIC 0.05211970 0.004374210  11.91523  9.859865e-33\n43    11 LARCENY/THEFT 0.54039667 0.009807759  55.09889  0.000000e+00\n44    11 VEHICLE THEFT 0.22234300 0.008183285  27.17038 1.454437e-162\n45    12       ASSAULT 0.18111853 0.007589328  23.86489 7.093421e-126\n46    12 DRUG/NARCOTIC 0.05621201 0.004539032  12.38414  3.184725e-35\n47    12 LARCENY/THEFT 0.53944839 0.009822578  54.91923  0.000000e+00\n48    12 VEHICLE THEFT 0.22322107 0.008205928  27.20242 6.081322e-163\n49    13       ASSAULT 0.17581107 0.007419823  23.69478 4.081512e-124\n50    13 DRUG/NARCOTIC 0.05587178 0.004476812  12.48026  9.567217e-36\n51    13 LARCENY/THEFT 0.54714052 0.009702602  56.39111  0.000000e+00\n52    13 VEHICLE THEFT 0.22117663 0.008089956  27.33966 1.433443e-164\n53    14       ASSAULT 0.16037957 0.007202175  22.26821 7.513935e-110\n54    14 DRUG/NARCOTIC 0.06279956 0.004761483  13.18908  1.014210e-39\n55    14 LARCENY/THEFT 0.55255651 0.009758995  56.62023  0.000000e+00\n56    14 VEHICLE THEFT 0.22426437 0.008186244  27.39527 3.122467e-165\n     conf.low  conf.high      Category PdDistrict DayOfWeek\n1  0.20491876 0.22778386 LARCENY/THEFT TENDERLOIN    Friday\n2  0.43577169 0.46339550 LARCENY/THEFT TENDERLOIN    Friday\n3  0.28796470 0.31342888 LARCENY/THEFT TENDERLOIN    Friday\n4  0.02838176 0.03835486 LARCENY/THEFT TENDERLOIN    Friday\n5  0.19517243 0.21709505 LARCENY/THEFT TENDERLOIN    Monday\n6  0.49902522 0.52611322 LARCENY/THEFT TENDERLOIN    Monday\n7  0.24515480 0.26883595 LARCENY/THEFT TENDERLOIN    Monday\n8  0.02012922 0.02847410 LARCENY/THEFT TENDERLOIN    Monday\n9  0.22499673 0.24871549 LARCENY/THEFT TENDERLOIN  Saturday\n10 0.39654693 0.42398842 LARCENY/THEFT TENDERLOIN  Saturday\n11 0.30914666 0.33521743 LARCENY/THEFT TENDERLOIN  Saturday\n12 0.02588273 0.03550561 LARCENY/THEFT TENDERLOIN  Saturday\n13 0.24427079 0.26979534 LARCENY/THEFT TENDERLOIN    Sunday\n14 0.38588400 0.41450068 LARCENY/THEFT TENDERLOIN    Sunday\n15 0.29661007 0.32362663 LARCENY/THEFT TENDERLOIN    Sunday\n16 0.02746559 0.03784690 LARCENY/THEFT TENDERLOIN    Sunday\n17 0.18471049 0.20565972 LARCENY/THEFT TENDERLOIN  Thursday\n18 0.51548668 0.54187131 LARCENY/THEFT TENDERLOIN  Thursday\n19 0.24135964 0.26433330 LARCENY/THEFT TENDERLOIN  Thursday\n20 0.01930350 0.02727535 LARCENY/THEFT TENDERLOIN  Thursday\n21 0.18031970 0.20110771 LARCENY/THEFT TENDERLOIN   Tuesday\n22 0.52636838 0.55274248 LARCENY/THEFT TENDERLOIN   Tuesday\n23 0.23064466 0.25330668 LARCENY/THEFT TENDERLOIN   Tuesday\n24 0.02340908 0.03210131 LARCENY/THEFT TENDERLOIN   Tuesday\n25 0.18037961 0.20088219 LARCENY/THEFT TENDERLOIN Wednesday\n26 0.53278253 0.55877100 LARCENY/THEFT TENDERLOIN Wednesday\n27 0.22900996 0.25130709 LARCENY/THEFT TENDERLOIN Wednesday\n28 0.01948577 0.02738184 LARCENY/THEFT TENDERLOIN Wednesday\n29 0.16106751 0.19014487 LARCENY/THEFT   RICHMOND    Friday\n30 0.04390322 0.06093546 LARCENY/THEFT   RICHMOND    Friday\n31 0.52575104 0.56380843 LARCENY/THEFT   RICHMOND    Friday\n32 0.21118361 0.24320586 LARCENY/THEFT   RICHMOND    Friday\n33 0.15254190 0.18122905 LARCENY/THEFT   RICHMOND    Monday\n34 0.05122080 0.06954651 LARCENY/THEFT   RICHMOND    Monday\n35 0.53463767 0.57288233 LARCENY/THEFT   RICHMOND    Monday\n36 0.20306262 0.23487913 LARCENY/THEFT   RICHMOND    Monday\n37 0.17099349 0.20090924 LARCENY/THEFT   RICHMOND  Saturday\n38 0.03624923 0.05204424 LARCENY/THEFT   RICHMOND  Saturday\n39 0.50522639 0.54362594 LARCENY/THEFT   RICHMOND  Saturday\n40 0.22893001 0.26202147 LARCENY/THEFT   RICHMOND  Saturday\n41 0.17015889 0.20012237 LARCENY/THEFT   RICHMOND    Sunday\n42 0.04354640 0.06069299 LARCENY/THEFT   RICHMOND    Sunday\n43 0.52117382 0.55961953 LARCENY/THEFT   RICHMOND    Sunday\n44 0.20630405 0.23838194 LARCENY/THEFT   RICHMOND    Sunday\n45 0.16624372 0.19599334 LARCENY/THEFT   RICHMOND  Thursday\n46 0.04731567 0.06510835 LARCENY/THEFT   RICHMOND  Thursday\n47 0.52019649 0.55870029 LARCENY/THEFT   RICHMOND  Thursday\n48 0.20713774 0.23930439 LARCENY/THEFT   RICHMOND  Thursday\n49 0.16126848 0.19035365 LARCENY/THEFT   RICHMOND   Tuesday\n50 0.04709739 0.06464617 LARCENY/THEFT   RICHMOND   Tuesday\n51 0.52812377 0.56615727 LARCENY/THEFT   RICHMOND   Tuesday\n52 0.20532061 0.23703265 LARCENY/THEFT   RICHMOND   Tuesday\n53 0.14626356 0.17449557 LARCENY/THEFT   RICHMOND Wednesday\n54 0.05346722 0.07213189 LARCENY/THEFT   RICHMOND Wednesday\n55 0.53342923 0.57168379 LARCENY/THEFT   RICHMOND Wednesday\n56 0.20821963 0.24030911 LARCENY/THEFT   RICHMOND Wednesday\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np %>% \n  select(group, estimate, DayOfWeek, PdDistrict) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 14 x 6\n   DayOfWeek PdDistrict ASSAULT `DRUG/NARCOTIC` `LARCENY/THEFT` `VEHICLE THEFT`\n   <chr>     <chr>        <dbl>           <dbl>           <dbl>           <dbl>\n 1 Friday    TENDERLOIN   0.216          0.450            0.301          0.0334\n 2 Monday    TENDERLOIN   0.206          0.513            0.257          0.0243\n 3 Saturday  TENDERLOIN   0.237          0.410            0.322          0.0307\n 4 Sunday    TENDERLOIN   0.257          0.400            0.310          0.0327\n 5 Thursday  TENDERLOIN   0.195          0.529            0.253          0.0233\n 6 Tuesday   TENDERLOIN   0.191          0.540            0.242          0.0278\n 7 Wednesday TENDERLOIN   0.191          0.546            0.240          0.0234\n 8 Friday    RICHMOND     0.176          0.0524           0.545          0.227 \n 9 Monday    RICHMOND     0.167          0.0604           0.554          0.219 \n10 Saturday  RICHMOND     0.186          0.0441           0.524          0.245 \n11 Sunday    RICHMOND     0.185          0.0521           0.540          0.222 \n12 Thursday  RICHMOND     0.181          0.0562           0.539          0.223 \n13 Tuesday   RICHMOND     0.176          0.0559           0.547          0.221 \n14 Wednesday RICHMOND     0.160          0.0628           0.553          0.224 \n```\n:::\n:::\n\nIt doesn't *look* much different. Maybe the Tenderloin has a\nlarger weekend increase in assaults than Richmond does. \n\n$\\blacksquare$\n\n\n\n\n##  Crimes in San Francisco -- the data\n\n\n The data in [link](http://utsc.utoronto.ca/~butler/d29/sfcrime.csv) is a huge dataset of crimes committed in San\nFrancisco between 2003 and 2015. The variables are:\n\n\n\n* `Dates`: the date and time of the crime\n\n* `Category`: the category of crime, eg. \"larceny\" or\n\"vandalism\" (response).\n\n* `Descript`: detailed description of crime.\n\n* `DayOfWeek`: the day of the week of the crime.\n\n* `PdDistrict`: the name of the San Francisco Police\nDepartment district in which the crime took place.\n\n* `Resolution`: how the crime was resolved\n\n* `Address`: approximate street address of crime\n\n* `X`: longitude\n\n* `Y`: latitude\n\n\nOur aim is to see whether the category of crime depends on the day of\nthe week and the district in which it occurred. However, there are a\nlot of crime categories, so we will focus on the top four\n\"interesting\" ones, which we will have to discover.\n\n\n\n(a) Read in the data and verify that you have these columns and a\nlot of rows. (The data may take a moment to read in. You will see why.)\n\n\nSolution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://utsc.utoronto.ca/~butler/d29/sfcrime.csv\"\nsfcrime <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 878049 Columns: 9\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (6): Category, Descript, DayOfWeek, PdDistrict, Resolution, Address\ndbl  (2): X, Y\ndttm (1): Dates\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nsfcrime\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 878,049 x 9\n   Dates               Category Descript DayOfWeek PdDistrict Resolution Address\n   <dttm>              <chr>    <chr>    <chr>     <chr>      <chr>      <chr>  \n 1 2015-05-13 23:53:00 WARRANTS WARRANT~ Wednesday NORTHERN   ARREST, B~ OAK ST~\n 2 2015-05-13 23:53:00 OTHER O~ TRAFFIC~ Wednesday NORTHERN   ARREST, B~ OAK ST~\n 3 2015-05-13 23:33:00 OTHER O~ TRAFFIC~ Wednesday NORTHERN   ARREST, B~ VANNES~\n 4 2015-05-13 23:30:00 LARCENY~ GRAND T~ Wednesday NORTHERN   NONE       1500 B~\n 5 2015-05-13 23:30:00 LARCENY~ GRAND T~ Wednesday PARK       NONE       100 Bl~\n 6 2015-05-13 23:30:00 LARCENY~ GRAND T~ Wednesday INGLESIDE  NONE       0 Bloc~\n 7 2015-05-13 23:30:00 VEHICLE~ STOLEN ~ Wednesday INGLESIDE  NONE       AVALON~\n 8 2015-05-13 23:30:00 VEHICLE~ STOLEN ~ Wednesday BAYVIEW    NONE       KIRKWO~\n 9 2015-05-13 23:00:00 LARCENY~ GRAND T~ Wednesday RICHMOND   NONE       600 Bl~\n10 2015-05-13 23:00:00 LARCENY~ GRAND T~ Wednesday CENTRAL    NONE       JEFFER~\n# i 878,039 more rows\n# i 2 more variables: X <dbl>, Y <dbl>\n```\n:::\n:::\n\n     \n\nThose columns indeed, and pushing a million rows! That's why it took\nso long!\n\nThere are also 39 categories of crime, so we need to cut that down\nsome. There are only ten districts, however, so we should be able to\nuse that as is.\n    \n$\\blacksquare$\n\n(b) How is the response variable here different to the one in\nthe question about steak preferences (and therefore why would\n`multinom` from package `nnet` be the method of choice)?\n\n\nSolution\n\n\nSteak preferences have a natural order, while crime categories do\nnot. Since they are unordered, `multinom` is better than\n`polr`. \n    \n$\\blacksquare$\n\n(c) Find out which crime categories there are, and \narrange them in order of how many crimes there were in each\ncategory. \n\n\nSolution\n\n\nThis can be the one you know, a group-by and summarize, followed\nby `arrange` to sort:\n\n::: {.cell}\n\n```{.r .cell-code}\nsfcrime %>% group_by(Category) %>%\nsummarize(count=n()) %>%\narrange(desc(count))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 39 x 2\n   Category        count\n   <chr>           <int>\n 1 LARCENY/THEFT  174900\n 2 OTHER OFFENSES 126182\n 3 NON-CRIMINAL    92304\n 4 ASSAULT         76876\n 5 DRUG/NARCOTIC   53971\n 6 VEHICLE THEFT   53781\n 7 VANDALISM       44725\n 8 WARRANTS        42214\n 9 BURGLARY        36755\n10 SUSPICIOUS OCC  31414\n# i 29 more rows\n```\n:::\n:::\n\n \n\nor this one does the same thing and saves a step:\n\n::: {.cell}\n\n```{.r .cell-code}\nsfcrime %>% count(Category) %>%\narrange(desc(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 39 x 2\n   Category            n\n   <chr>           <int>\n 1 LARCENY/THEFT  174900\n 2 OTHER OFFENSES 126182\n 3 NON-CRIMINAL    92304\n 4 ASSAULT         76876\n 5 DRUG/NARCOTIC   53971\n 6 VEHICLE THEFT   53781\n 7 VANDALISM       44725\n 8 WARRANTS        42214\n 9 BURGLARY        36755\n10 SUSPICIOUS OCC  31414\n# i 29 more rows\n```\n:::\n:::\n\n \n\nFor this one, do the `count` step first, to see what you\nget. It produces a two-column data frame with the column of counts\ncalled `n`. So now you know that the second line has to be\n`arrange(desc(n))`, whereas before you tried `count`,\nall you knew is that it was arrange-desc-something.\n\nYou need to sort in descending order so that the categories you want\nto see actually do appear at the top.\n    \n$\\blacksquare$\n\n(d) Which are the four most frequent \"interesting\" crime\ncategories, that is to say, not including \"other offenses\" and\n\"non-criminal\"? Get them into a vector called\n`my.crimes`. See if you can find a way of doing this  that\ndoesn't involve typing them in (for full marks).\n\n\nSolution\n\n\nThe most frequent interesting ones are, in order, larceny-theft,\nassault, drug-narcotic and vehicle theft. \nThe fact that \"other offenses\" is so big indicates that there\nare a lot of possible crimes out there, and even 39 categories of\ncrime isn't enough. \"Non-criminal\" means, I think, that the\npolice were called, but on arriving at the scene, they found that\nno law had been broken.\n\nI think the easy way to get the \"top four\" crimes out is to pull\nthem out of the data frame that `count` produces. They are\nrows 1, 4, 5 and 6, so add a `slice` to your pipeline:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy.rows <- c(1,4,5,6)\nsfcrime %>% count(Category) %>%\n  arrange(desc(n)) %>%\n  slice(my.rows) %>% pull(Category) -> my.crimes\nmy.crimes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"LARCENY/THEFT\" \"ASSAULT\"       \"DRUG/NARCOTIC\" \"VEHICLE THEFT\"\n```\n:::\n:::\n\n     \n\nI just want the `Category` column (as a vector), and\n`pull` is the way to get that. (If I don't do `pull`, I\nget a data frame.)\n\nIf you can't think of anything, just type them into a vector with\n`c`, or better, copy-paste them from the console. But that's a\nlast resort, and would cost you a point. If you do this, they have to\nmatch exactly, UPPERCASE and all.\n    \n$\\blacksquare$\n\n(e) (Digression, but needed for the next part.) The R vector\n`letters` contains the lowercase letters from `a` to\n`z`. Consider the vector `('a','m',3,'Q')`. Some of\nthese are found amongst the lowercase letters, and some not. Type\nthese into a vector `v` and explain briefly why \n`v %in% letters` produces what it does.\n\n\n\nSolution\n\n\nThis is the ultimate \"try it and see\":\n\n::: {.cell}\n\n```{.r .cell-code}\nv=c('a','m',3,'Q')\nv %in% letters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  TRUE  TRUE FALSE FALSE\n```\n:::\n:::\n\n   \n\nThe first two elements of the answer are `TRUE` because\nlowercase-a and lowercase-m can be found in the lowercase letters\nsomewhere. The other two are false because the number 3 and the\nuppercase-Q cannot be found anywhere in the lowercase letters.\n\nThe name is `%in%` because it's asking whether each element of\nthe first vector (one at a time) is *in* the set defined by the\nsecond thing: \"is `a` a lowercase letter?\" ... \nis \"`Q` a lowercase letter?\" \nand getting the answers \"yes, yes, no, no\".\n  \n$\\blacksquare$\n\n(f) We are going to `filter` only the rows of our data frame that\nhave one of the crimes in `my.crimes` as their\n`Category`. Also, `select` only the columns\n`Category`, `DayOfWeek` and `PdDistrict`. Save\nthe resulting data frame and display its structure. (You should have a\nlot fewer rows than you did before.)\n\n\n\nSolution\n\n\nThe hard part about this is to get the inputs to `%in%` the\nright way around. We are testing the things in `Category` one\nat a time for membership in the set in `my.crimes`, so this:\n\n::: {.cell}\n\n```{.r .cell-code}\nsfcrime %>% filter(Category %in% my.crimes) %>%\nselect(c(Category,DayOfWeek,PdDistrict)) -> sfcrimea\nsfcrimea\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 359,528 x 3\n   Category      DayOfWeek PdDistrict\n   <chr>         <chr>     <chr>     \n 1 LARCENY/THEFT Wednesday NORTHERN  \n 2 LARCENY/THEFT Wednesday PARK      \n 3 LARCENY/THEFT Wednesday INGLESIDE \n 4 VEHICLE THEFT Wednesday INGLESIDE \n 5 VEHICLE THEFT Wednesday BAYVIEW   \n 6 LARCENY/THEFT Wednesday RICHMOND  \n 7 LARCENY/THEFT Wednesday CENTRAL   \n 8 LARCENY/THEFT Wednesday CENTRAL   \n 9 LARCENY/THEFT Wednesday NORTHERN  \n10 ASSAULT       Wednesday INGLESIDE \n# i 359,518 more rows\n```\n:::\n:::\n\n\n   \nI had trouble thinking of a good name for this one, so I put an \"a\"\non the end. (I would have used a number, but I prefer those for\nmodels.) \n\nDown to a \"mere\" 359,000 rows. Don't be stressed that the\n`Category` factor still has 39 levels (the original 39 crime\ncategories); only four of them have any data in them:\n\n::: {.cell}\n\n```{.r .cell-code}\nsfcrimea %>% count(Category)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 2\n  Category           n\n  <chr>          <int>\n1 ASSAULT        76876\n2 DRUG/NARCOTIC  53971\n3 LARCENY/THEFT 174900\n4 VEHICLE THEFT  53781\n```\n:::\n:::\n\n \n\nSo all of the crimes that are left are one of the four Categories we\nwant to look at.\n  \n$\\blacksquare$\n\n(g) Save these data in a file `sfcrime1.csv`.\n\n\n\nSolution\n\n\nThis is `write_csv` again:\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(sfcrimea,\"sfcrime1.csv\")\n```\n:::\n\n$\\blacksquare$   \n  \n\n\n\n\n\n##  What sports do these athletes play?\n\n\n The data at\n[link](http://ritsokiguess.site/datafiles/ais.txt) are physical\nand physiological measurements of 202 male and female Australian elite\nathletes. The data values are separated by *tabs*. We are going\nto see whether we can predict the sport an athlete plays from their\nheight and weight.\n\nThe sports, if you care, are respectively basketball, \n\"field athletics\" (eg. shot put, javelin throw, long jump etc.),\ngymnastics, netball, rowing, swimming, 400m running, tennis, sprinting\n(100m or 200m running), water polo.\n\n\n\n(a) Read in the data and display the first few rows.\n\nSolution\n\n\nThe data values are separated by tabs, so `read_tsv` is\nthe thing:\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/ais.txt\"\nathletes <- read_tsv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 202 Columns: 13\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nathletes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 202 x 13\n   Sex    Sport     RCC   WCC    Hc    Hg  Ferr   BMI   SSF `%Bfat`   LBM    Ht\n   <chr>  <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>\n 1 female Netball  4.56  13.3  42.2  13.6    20  19.2  49      11.3  53.1  177.\n 2 female Netball  4.15   6    38    12.7    59  21.2 110.     25.3  47.1  173.\n 3 female Netball  4.16   7.6  37.5  12.3    22  21.4  89      19.4  53.4  176 \n 4 female Netball  4.32   6.4  37.7  12.3    30  21.0  98.3    19.6  48.8  170.\n 5 female Netball  4.06   5.8  38.7  12.8    78  21.8 122.     23.1  56.0  183 \n 6 female Netball  4.12   6.1  36.6  11.8    21  21.4  90.4    16.9  56.4  178.\n 7 female Netball  4.17   5    37.4  12.7   109  21.5 107.     21.3  53.1  177.\n 8 female Netball  3.8    6.6  36.5  12.4   102  24.4 157.     26.6  54.4  174.\n 9 female Netball  3.96   5.5  36.3  12.4    71  22.6 101.     17.9  56.0  174.\n10 female Netball  4.44   9.7  41.4  14.1    64  22.8 126.     25.0  51.6  174.\n# i 192 more rows\n# i 1 more variable: Wt <dbl>\n```\n:::\n:::\n\n     \n\nIf you didn't remember that, this also works:\n\n::: {.cell}\n\n```{.r .cell-code}\nathletes <- read_delim(my_url, \"\\t\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 202 Columns: 13\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr  (2): Sex, Sport\ndbl (11): RCC, WCC, Hc, Hg, Ferr, BMI, SSF, %Bfat, LBM, Ht, Wt\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n \n\n(this is the R way of expressing \"tab\".)\n\n$\\blacksquare$\n\n(b) Make a scatterplot of height vs.\\ weight, with the points\ncoloured by what sport the athlete plays. Put height on the $x$-axis\nand weight on the $y$-axis.\n\nSolution\n\n\nI'm doing this to give you a little intuition for later:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(athletes, aes(x = Ht, y = Wt, colour = Sport)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](nominal-response_files/figure-pdf/oz-multi-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n     \n\nThe reason for giving you the axes to use is (i) neither variable is\nreally a response, so it doesn't actually matter which one goes on\nwhich axis, and (ii) I wanted to give the grader something consistent\nto look at.\n\n\n$\\blacksquare$\n\n\n(c) Explain briefly why a multinomial model (`multinom`\nfrom `nnet`) would be the best thing to use to predict sport\nplayed from the other variables.\n\nSolution\n\n\nThe categories of `Sport` are not in any kind of order, and\nthere are more than two of them.\nThat's really all you needed, for which two marks was kind of\ngenerous. \n\n$\\blacksquare$\n\n(d) Fit a suitable model for predicting sport played from\nheight and weight. (You don't need to look at the results.) 100\nsteps isn't quite enough, so set `maxit` equal to a larger\nnumber to allow the estimation to finish.\n\nSolution\n\n\n120 steps is actually enough, but any number larger than 110 is\nfine. It doesn't matter if your guess is way too high. Like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nnet)\nsport.1 <- multinom(Sport ~ Ht + Wt, data = athletes, maxit = 200)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  40 (27 variable)\ninitial  value 465.122189 \niter  10 value 410.089598\niter  20 value 391.426721\niter  30 value 365.829150\niter  40 value 355.326457\niter  50 value 351.255395\niter  60 value 350.876479\niter  70 value 350.729699\niter  80 value 350.532323\niter  90 value 350.480130\niter 100 value 350.349271\niter 110 value 350.312029\nfinal  value 350.311949 \nconverged\n```\n:::\n:::\n\n \n\nAs long as you see the word `converged` at the end, you're\ngood. \n\n$\\blacksquare$\n\n(e) Demonstrate using `anova` that `Wt` should\nnot be removed from this model.\n\nSolution\n\n\nThe idea is to fit a model without `Wt`, and then show that\nit fits significantly worse. This converges in less than 100 iterations, so you can have `maxit` or not as you prefer:\n\n::: {.cell}\n\n```{.r .cell-code}\nsport.2 <- update(sport.1, . ~ . - Wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  30 (18 variable)\ninitial  value 465.122189 \niter  10 value 447.375728\niter  20 value 413.597441\niter  30 value 396.685596\niter  40 value 394.121380\niter  50 value 394.116993\niter  60 value 394.116434\nfinal  value 394.116429 \nconverged\n```\n:::\n\n```{.r .cell-code}\nanova(sport.2, sport.1, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio tests of Multinomial Models\n\nResponse: Sport\n    Model Resid. df Resid. Dev   Test    Df LR stat.      Pr(Chi)\n1      Ht      1800   788.2329                                   \n2 Ht + Wt      1791   700.6239 1 vs 2     9 87.60896 4.884981e-15\n```\n:::\n:::\n\n     \n\nThe P-value is very small indeed, so the bigger model `sport.1`\nis definitely better (or the smaller model `sport.2` is\nsignificantly worse, however you want to say it). So taking\n`Wt` out is definitely a mistake. \n\nThis is what I would have guessed (I actually wrote the question in\nanticipation of this being the answer) because weight certainly seems\nto help in distinguishing the sports. For example, the field athletes\nseem to be heavy for their height compared to the other athletes (look\nback at the graph you made). \n\n`drop1`, the obvious thing, doesn't work here:\n\n::: {.cell}\n\n```{.r .cell-code}\ndrop1(sport.1, test = \"Chisq\", trace = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrying - Ht \n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in if (trace) {: argument is not interpretable as logical\n```\n:::\n:::\n\n \n\nI gotta figure out what that error is.\nDoes `step`?\n\n::: {.cell}\n\n```{.r .cell-code}\nstep(sport.1, direction = \"backward\", test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=754.62\nSport ~ Ht + Wt\n\ntrying - Ht \n# weights:  30 (18 variable)\ninitial  value 465.122189 \niter  10 value 441.367394\niter  20 value 381.021649\niter  30 value 380.326030\nfinal  value 380.305003 \nconverged\ntrying - Wt \n# weights:  30 (18 variable)\ninitial  value 465.122189 \niter  10 value 447.375728\niter  20 value 413.597441\niter  30 value 396.685596\niter  40 value 394.121380\niter  50 value 394.116993\niter  60 value 394.116434\nfinal  value 394.116429 \nconverged\n       Df      AIC\n<none> 27 754.6239\n- Ht   18 796.6100\n- Wt   18 824.2329\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nmultinom(formula = Sport ~ Ht + Wt, data = athletes, maxit = 200)\n\nCoefficients:\n        (Intercept)         Ht          Wt\nField      59.98535 -0.4671650  0.31466413\nGym       112.49889 -0.5027056 -0.57087657\nNetball    47.70209 -0.2947852  0.07992763\nRow        35.90829 -0.2517942  0.14164007\nSwim       36.82832 -0.2444077  0.10544986\nT400m      32.73554 -0.1482589 -0.07830622\nTennis     41.92855 -0.2278949 -0.01979877\nTSprnt     51.43723 -0.3359534  0.12378285\nWPolo      23.35291 -0.2089807  0.18819526\n\nResidual Deviance: 700.6239 \nAIC: 754.6239 \n```\n:::\n:::\n\n \n\nCuriously enough, it does. The final model is the same as the initial\none, telling us that neither variable should be removed.\n\n$\\blacksquare$\n\n(f) Make a data frame consisting of all combinations of\n`Ht` 160, 180 and 200 (cm), and `Wt` 50, 75, and 100\n(kg), and use it to obtain predicted probabilities of athletes of\nthose heights and weights playing each of the sports. Display the\nresults. You might have to display them smaller, or reduce the\nnumber of decimal places^[For this, use `round`.] \nto fit them on the page.\n\nSolution\n\nTo get all combinations of those heights and weights, use `datagrid`:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = sport.1, Ht = c(160, 180, 200), Wt = c(50, 75, 100))\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sport  Ht  Wt\n1   Row 160  50\n2   Row 160  75\n3   Row 160 100\n4   Row 180  50\n5   Row 180  75\n6   Row 180 100\n7   Row 200  50\n8   Row 200  75\n9   Row 200 100\n```\n:::\n:::\n\n(check: $3 \\times 3 = 9$ rows.)\n\nThen predict:\n\n::: {.cell}\n\n```{.r .cell-code}\np <- cbind(predictions(sport.1, newdata = new, type = \"probs\"))\np\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   rowid   group     estimate    std.error    statistic      p.value\n1      1   BBall 2.147109e-03 1.391946e-03   1.54252381 1.229464e-01\n2      1   Field 5.676191e-03 5.296703e-03   1.07164604 2.838790e-01\n3      1     Gym 7.269646e-02 1.063692e-01   0.68343558 4.943316e-01\n4      1 Netball 1.997276e-01 9.265931e-02   2.15550532 3.112231e-02\n5      1     Row 3.205118e-02 1.961748e-02   1.63380705 1.022994e-01\n6      1    Swim 4.293546e-02 2.847074e-02   1.50805577 1.315403e-01\n7      1   T400m 3.517558e-01 1.232516e-01   2.85396456 4.317735e-03\n8      1  TSprnt 1.033315e-01 5.947166e-02   1.73749076 8.230058e-02\n9      1  Tennis 1.885847e-01 8.821917e-02   2.13768358 3.254243e-02\n10     1   WPolo 1.094045e-03 1.054567e-03   1.03743513 2.995331e-01\n11     2   BBall 1.044581e-04 3.708376e-05   2.81681454 4.850252e-03\n12     2   Field 7.203893e-01 7.941566e-02   9.07112401 1.177955e-19\n13     2     Gym 2.240720e-09 3.041901e-08   0.07366183 9.412795e-01\n14     2 Netball 7.166865e-02 6.173438e-02   1.16091948 2.456746e-01\n15     2     Row 5.379841e-02 3.967176e-02   1.35608823 1.750711e-01\n16     2    Swim 2.916160e-02 2.642949e-02   1.10337366 2.698649e-01\n17     2   T400m 2.416185e-03 2.584968e-03   0.93470579 3.499399e-01\n18     2  TSprnt 1.109879e-01 6.565757e-02   1.69040519 9.095046e-02\n19     2  Tennis 5.592833e-03 4.602978e-03   1.21504666 2.243483e-01\n20     2   WPolo 5.880674e-03 4.117091e-03   1.42835638 1.531893e-01\n21     3   BBall 5.541050e-08 7.465350e-08   0.74223586 4.579444e-01\n22     3   Field 9.968725e-01 3.547634e-03 280.99640418 0.000000e+00\n23     3     Gym 7.530504e-19 1.957407e-17   0.03847183 9.693115e-01\n24     3 Netball 2.804029e-04 5.518061e-04   0.50815471 6.113449e-01\n25     3     Row 9.845934e-04 1.517126e-03   0.64898611 5.163474e-01\n26     3    Swim 2.159577e-04 4.066368e-04   0.53108250 5.953616e-01\n27     3   T400m 1.809594e-07 4.080955e-07   0.44342428 6.574589e-01\n28     3  TSprnt 1.299813e-03 2.161171e-03   0.60143906 5.475476e-01\n29     3  Tennis 1.808504e-06 3.970856e-06   0.45544439 6.487896e-01\n30     3   WPolo 3.446523e-04 4.952096e-04   0.69597260 4.864460e-01\n31     4   BBall 9.142760e-02 8.656711e-02   1.05614709 2.909010e-01\n32     4   Field 2.116103e-05 2.963329e-05   0.71409643 4.751676e-01\n33     4     Gym 1.331347e-04 3.042400e-04   0.43759751 6.616781e-01\n34     4 Netball 2.339857e-02 2.450012e-02   0.95503881 3.395581e-01\n35     4     Row 8.871768e-03 8.714721e-03   1.01802087 3.086680e-01\n36     4    Swim 1.377656e-02 1.483001e-02   0.92896454 3.529075e-01\n37     4   T400m 7.721547e-01 1.575275e-01   4.90171386 9.500415e-07\n38     4  TSprnt 5.313751e-03 6.034917e-03   0.88050101 3.785880e-01\n39     4  Tennis 8.418976e-02 8.263889e-02   1.01876685 3.083137e-01\n40     4   WPolo 7.129763e-04 9.521010e-04   0.74884520 4.539505e-01\n41     5   BBall 7.787300e-02 1.749188e-02   4.45195200 8.509319e-06\n42     5   Field 4.701849e-02 1.854252e-02   2.53571267 1.122188e-02\n43     5     Gym 7.184344e-11 7.642704e-10   0.09400266 9.251070e-01\n44     5 Netball 1.469948e-01 3.239439e-02   4.53766198 5.688134e-06\n45     5     Row 2.607097e-01 3.898031e-02   6.68824113 2.258688e-11\n46     5    Swim 1.638165e-01 3.230338e-02   5.07118714 3.953418e-07\n47     5   T400m 9.285706e-02 2.900349e-02   3.20158252 1.366749e-03\n48     5  TSprnt 9.992310e-02 2.644361e-02   3.77872442 1.576338e-04\n49     5  Tennis 4.371258e-02 1.996803e-02   2.18912836 2.858751e-02\n50     5   WPolo 6.709478e-02 2.303076e-02   2.91326854 3.576669e-03\n51     6   BBall 5.379479e-04 4.999131e-04   1.07608273 2.818903e-01\n52     6   Field 8.473139e-01 8.688783e-02   9.75181434 1.812004e-22\n53     6     Gym 3.144321e-19 7.239834e-18   0.04343084 9.653581e-01\n54     6 Netball 7.489597e-03 8.598742e-03   0.87101082 3.837482e-01\n55     6     Row 6.213668e-02 4.800951e-02   1.29425775 1.955764e-01\n56     6    Swim 1.579859e-02 1.639416e-02   0.96367168 3.352106e-01\n57     6   T400m 9.056685e-05 1.335476e-04   0.67816148 4.976693e-01\n58     6  TSprnt 1.523963e-02 1.658782e-02   0.91872404 3.582399e-01\n59     6  Tennis 1.840761e-04 3.082223e-04   0.59721856 5.503615e-01\n60     6   WPolo 5.120898e-02 4.087116e-02   1.25293690 2.102287e-01\n61     7   BBall 6.907544e-01 3.885716e-01   1.77767622 7.545705e-02\n62     7   Field 1.399715e-08 2.477095e-08   0.56506338 5.720307e-01\n63     7     Gym 4.326057e-08 2.205756e-07   0.19612585 8.445117e-01\n64     7 Netball 4.863664e-04 9.195664e-04   0.52890838 5.968690e-01\n65     7     Row 4.357120e-04 6.857758e-04   0.63535626 5.251961e-01\n66     7    Swim 7.843113e-04 1.429732e-03   0.54857209 5.832991e-01\n67     7   T400m 3.007396e-01 3.894091e-01   0.77229728 4.399384e-01\n68     7  TSprnt 4.848339e-05 8.602477e-05   0.56359796 5.730278e-01\n69     7  Tennis 6.668610e-03 1.092172e-02   0.61058257 5.414760e-01\n70     7   WPolo 8.244005e-05 1.527648e-04   0.53965331 5.894361e-01\n71     8   BBall 8.889269e-01 4.840250e-02  18.36530816 2.490322e-75\n72     8   Field 4.698989e-05 3.649151e-05   1.28769392 1.978525e-01\n73     8     Gym 3.527126e-14 2.729420e-13   0.12922623 8.971786e-01\n74     8 Netball 4.616460e-03 4.544240e-03   1.01589263 3.096805e-01\n75     8     Row 1.934547e-02 1.442788e-02   1.34083956 1.799725e-01\n76     8    Swim 1.409088e-02 1.263807e-02   1.11495505 2.648697e-01\n77     8   T400m 5.464293e-02 4.388447e-02   1.24515426 2.130751e-01\n78     8  TSprnt 1.377496e-03 1.056772e-03   1.30349387 1.924062e-01\n79     8  Tennis 5.231368e-03 3.100760e-03   1.68712425 9.157946e-02\n80     8   WPolo 1.172154e-02 9.701736e-03   1.20819000 2.269742e-01\n81     9   BBall 2.738448e-01 1.066147e-01   2.56854635 1.021261e-02\n82     9   Field 3.776290e-02 1.804311e-02   2.09292626 3.635574e-02\n83     9     Gym 6.884080e-21 1.381242e-19   0.04983977 9.602501e-01\n84     9 Netball 1.048940e-02 8.439252e-03   1.24293033 2.138935e-01\n85     9     Row 2.056153e-01 8.214542e-02   2.50306407 1.231233e-02\n86     9    Swim 6.060159e-02 3.785495e-02   1.60088919 1.094015e-01\n87     9   T400m 2.376695e-03 2.270886e-03   1.04659406 2.952868e-01\n88     9  TSprnt 9.368804e-03 7.032916e-03   1.33213639 1.828154e-01\n89     9  Tennis 9.824069e-04 1.222494e-03   0.80360885 4.216229e-01\n90     9   WPolo 3.989581e-01 1.174417e-01   3.39707492 6.811032e-04\n        conf.low    conf.high Sport  Ht  Wt\n1  -5.810540e-04 4.875273e-03   Row 160  50\n2  -4.705157e-03 1.605754e-02   Row 160  50\n3  -1.357832e-01 2.811762e-01   Row 160  50\n4   1.811873e-02 3.813365e-01   Row 160  50\n5  -6.398378e-03 7.050074e-02   Row 160  50\n6  -1.286616e-02 9.873709e-02   Row 160  50\n7   1.101870e-01 5.933245e-01   Row 160  50\n8  -1.323085e-02 2.198938e-01   Row 160  50\n9   1.567827e-02 3.614911e-01   Row 160  50\n10 -9.728684e-04 3.160958e-03   Row 160  50\n11  3.177524e-05 1.771409e-04   Row 160  75\n12  5.647375e-01 8.760411e-01   Row 160  75\n13 -5.737944e-08 6.186088e-08   Row 160  75\n14 -4.932852e-02 1.926658e-01   Row 160  75\n15 -2.395681e-02 1.315536e-01   Row 160  75\n16 -2.263924e-02 8.096244e-02   Row 160  75\n17 -2.650260e-03 7.482630e-03   Row 160  75\n18 -1.769858e-02 2.396744e-01   Row 160  75\n19 -3.428838e-03 1.461450e-02   Row 160  75\n20 -2.188677e-03 1.395002e-02   Row 160  75\n21 -9.090766e-08 2.017287e-07   Row 160 100\n22  9.899193e-01 1.003826e+00   Row 160 100\n23 -3.761143e-17 3.911753e-17   Row 160 100\n24 -8.011172e-04 1.361923e-03   Row 160 100\n25 -1.988918e-03 3.958105e-03   Row 160 100\n26 -5.810359e-04 1.012951e-03   Row 160 100\n27 -6.188930e-07 9.808119e-07   Row 160 100\n28 -2.936005e-03 5.535631e-03   Row 160 100\n29 -5.974231e-06 9.591239e-06   Row 160 100\n30 -6.259407e-04 1.315245e-03   Row 160 100\n31 -7.824082e-02 2.610960e-01   Row 180  50\n32 -3.691916e-05 7.924122e-05   Row 180  50\n33 -4.631648e-04 7.294342e-04   Row 180  50\n34 -2.462079e-02 7.141792e-02   Row 180  50\n35 -8.208772e-03 2.595231e-02   Row 180  50\n36 -1.528974e-02 4.284285e-02   Row 180  50\n37  4.634065e-01 1.080903e+00   Row 180  50\n38 -6.514469e-03 1.714197e-02   Row 180  50\n39 -7.777949e-02 2.461590e-01   Row 180  50\n40 -1.153107e-03 2.579060e-03   Row 180  50\n41  4.358955e-02 1.121564e-01   Row 180  75\n42  1.067583e-02 8.336116e-02   Row 180  75\n43 -1.426099e-09 1.569786e-09   Row 180  75\n44  8.350296e-02 2.104866e-01   Row 180  75\n45  1.843097e-01 3.371097e-01   Row 180  75\n46  1.005030e-01 2.271299e-01   Row 180  75\n47  3.601127e-02 1.497028e-01   Row 180  75\n48  4.809458e-02 1.517516e-01   Row 180  75\n49  4.575961e-03 8.284919e-02   Row 180  75\n50  2.195533e-02 1.122342e-01   Row 180  75\n51 -4.418638e-04 1.517760e-03   Row 180 100\n52  6.770169e-01 1.017611e+00   Row 180 100\n53 -1.387538e-17 1.450425e-17   Row 180 100\n54 -9.363627e-03 2.434282e-02   Row 180 100\n55 -3.196023e-02 1.562336e-01   Row 180 100\n56 -1.633337e-02 4.793055e-02   Row 180 100\n57 -1.711817e-04 3.523154e-04   Row 180 100\n58 -1.727190e-02 4.775116e-02   Row 180 100\n59 -4.200285e-04 7.881806e-04   Row 180 100\n60 -2.889701e-02 1.313150e-01   Row 180 100\n61 -7.083184e-02 1.452341e+00   Row 200  50\n62 -3.455301e-08 6.254732e-08   Row 200  50\n63 -3.890596e-07 4.755808e-07   Row 200  50\n64 -1.315951e-03 2.288683e-03   Row 200  50\n65 -9.083840e-04 1.779808e-03   Row 200  50\n66 -2.017913e-03 3.586535e-03   Row 200  50\n67 -4.624882e-01 1.063967e+00   Row 200  50\n68 -1.201221e-04 2.170888e-04   Row 200  50\n69 -1.473756e-02 2.807478e-02   Row 200  50\n70 -2.169735e-04 3.818536e-04   Row 200  50\n71  7.940597e-01 9.837940e-01   Row 200  75\n72 -2.453215e-05 1.185119e-04   Row 200  75\n73 -4.996852e-13 5.702277e-13   Row 200  75\n74 -4.290087e-03 1.352301e-02   Row 200  75\n75 -8.932653e-03 4.762360e-02   Row 200  75\n76 -1.067928e-02 3.886105e-02   Row 200  75\n77 -3.136904e-02 1.406549e-01   Row 200  75\n78 -6.937395e-04 3.448732e-03   Row 200  75\n79 -8.460107e-04 1.130875e-02   Row 200  75\n80 -7.293513e-03 3.073659e-02   Row 200  75\n81  6.488384e-02 4.828058e-01   Row 200 100\n82  2.399053e-03 7.312674e-02   Row 200 100\n83 -2.638344e-19 2.776026e-19   Row 200 100\n84 -6.051228e-03 2.703003e-02   Row 200 100\n85  4.461319e-02 3.666173e-01   Row 200 100\n86 -1.359276e-02 1.347959e-01   Row 200 100\n87 -2.074159e-03 6.827550e-03   Row 200 100\n88 -4.415459e-03 2.315307e-02   Row 200 100\n89 -1.413637e-03 3.378451e-03   Row 200 100\n90  1.687767e-01 6.291395e-01   Row 200 100\n```\n:::\n:::\n\nI saved these, because I want to talk about what to do next. The normal procedure is to say that there is a prediction for each `group` (sport), so we want to grab only the columns we need and pivot wider. But, this time there are 10 sports (see how there are $9 \\times 10 = 90$ rows, with a predicted probability for each height-weight combo *for each sport*). I suggested reducing the number of decimal places in the predictions; the time to do that is *now*, while they are all in one column (rather than trying to round a bunch of columns all at once, which is doable but more work). Hence:\n\n::: {.cell}\n\n```{.r .cell-code}\np %>% \n  mutate(estimate = round(estimate, 2)) %>% \n  select(group, estimate, Ht, Wt) %>% \n  pivot_wider(names_from = group, values_from = estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 12\n     Ht    Wt BBall Field   Gym Netball   Row  Swim T400m TSprnt Tennis WPolo\n  <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n1   160    50  0     0.01  0.07    0.2   0.03  0.04  0.35   0.1    0.19  0   \n2   160    75  0     0.72  0       0.07  0.05  0.03  0      0.11   0.01  0.01\n3   160   100  0     1     0       0     0     0     0      0      0     0   \n4   180    50  0.09  0     0       0.02  0.01  0.01  0.77   0.01   0.08  0   \n5   180    75  0.08  0.05  0       0.15  0.26  0.16  0.09   0.1    0.04  0.07\n6   180   100  0     0.85  0       0.01  0.06  0.02  0      0.02   0     0.05\n7   200    50  0.69  0     0       0     0     0     0.3    0      0.01  0   \n8   200    75  0.89  0     0       0     0.02  0.01  0.05   0      0.01  0.01\n9   200   100  0.27  0.04  0       0.01  0.21  0.06  0      0.01   0     0.4 \n```\n:::\n:::\n\nThis works (although even then it might not all fit onto the page and you'll have to scroll left and right to see everything).\n\nIf you forget to round until the end, you'll have to do something like this:\n\n::: {.cell}\n\n```{.r .cell-code}\np %>% \n  select(group, estimate, Ht, Wt) %>% \n  pivot_wider(names_from = group, values_from = estimate) %>% \n  mutate(across(BBall:WPolo, \\(x) round(x, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 12\n     Ht    Wt BBall Field   Gym Netball   Row  Swim T400m TSprnt Tennis WPolo\n  <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n1   160    50  0     0.01  0.07    0.2   0.03  0.04  0.35   0.1    0.19  0   \n2   160    75  0     0.72  0       0.07  0.05  0.03  0      0.11   0.01  0.01\n3   160   100  0     1     0       0     0     0     0      0      0     0   \n4   180    50  0.09  0     0       0.02  0.01  0.01  0.77   0.01   0.08  0   \n5   180    75  0.08  0.05  0       0.15  0.26  0.16  0.09   0.1    0.04  0.07\n6   180   100  0     0.85  0       0.01  0.06  0.02  0      0.02   0     0.05\n7   200    50  0.69  0     0       0     0     0     0.3    0      0.01  0   \n8   200    75  0.89  0     0       0     0.02  0.01  0.05   0      0.01  0.01\n9   200   100  0.27  0.04  0       0.01  0.21  0.06  0      0.01   0     0.4 \n```\n:::\n:::\n\nIn words, \"for each column from `BBall` through `WPolo`, replace it by itself rounded to 2 decimals\".\n\nThere's nothing magic about two decimals; three or maybe even four would be fine. A small enough number that you can see most or all of the columns at once, and easily compare the numbers for size (which is hard when some of them are in scientific notation).\n\nExtra: you can even abbreviate the sport names, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\np %>% \n  mutate(estimate = round(estimate, 2),\n         group = abbreviate(group, 3)) %>% \n  select(group, estimate, Ht, Wt) %>% \n  pivot_wider(names_from = group, values_from = estimate) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 12\n     Ht    Wt   BBl   Fld   Gym   Ntb   Row   Swm   T40   TSp   Tnn   WPl\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1   160    50  0     0.01  0.07  0.2   0.03  0.04  0.35  0.1   0.19  0   \n2   160    75  0     0.72  0     0.07  0.05  0.03  0     0.11  0.01  0.01\n3   160   100  0     1     0     0     0     0     0     0     0     0   \n4   180    50  0.09  0     0     0.02  0.01  0.01  0.77  0.01  0.08  0   \n5   180    75  0.08  0.05  0     0.15  0.26  0.16  0.09  0.1   0.04  0.07\n6   180   100  0     0.85  0     0.01  0.06  0.02  0     0.02  0     0.05\n7   200    50  0.69  0     0     0     0     0     0.3   0     0.01  0   \n8   200    75  0.89  0     0     0     0.02  0.01  0.05  0     0.01  0.01\n9   200   100  0.27  0.04  0     0.01  0.21  0.06  0     0.01  0     0.4 \n```\n:::\n:::\n\nOnce again, the time to do this is while everything is long, so that you only have one column to work with.^[`abbreviate` comes from base R; it returns an abbreviated version of its (text) input, with a minimum length as its second input. If two abbreviations come out the same, it makes them longer until they are different. It has some heuristics to get things short enough: remove vowels, then remove letters on the end.] Again, you can do this at the end, but it's more work, because you are now renaming columns:\n\n::: {.cell}\n\n```{.r .cell-code}\np %>% \n  mutate(estimate = round(estimate, 2)) %>% \n  select(group, estimate, Ht, Wt) %>% \n  pivot_wider(names_from = group, values_from = estimate) %>% \n  rename_with(\\(x) abbreviate(x, 3), everything())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 x 12\n     Ht    Wt   BBl   Fld   Gym   Ntb   Row   Swm   T40   TSp   Tnn   WPl\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1   160    50  0     0.01  0.07  0.2   0.03  0.04  0.35  0.1   0.19  0   \n2   160    75  0     0.72  0     0.07  0.05  0.03  0     0.11  0.01  0.01\n3   160   100  0     1     0     0     0     0     0     0     0     0   \n4   180    50  0.09  0     0     0.02  0.01  0.01  0.77  0.01  0.08  0   \n5   180    75  0.08  0.05  0     0.15  0.26  0.16  0.09  0.1   0.04  0.07\n6   180   100  0     0.85  0     0.01  0.06  0.02  0     0.02  0     0.05\n7   200    50  0.69  0     0     0     0     0     0.3   0     0.01  0   \n8   200    75  0.89  0     0     0     0.02  0.01  0.05  0     0.01  0.01\n9   200   100  0.27  0.04  0     0.01  0.21  0.06  0     0.01  0     0.4 \n```\n:::\n:::\n\n`rename` doesn't work with `across` (you will get an error if you try it), so you need to use `rename_with`^[Which I learned about today.]. This has syntax \"how to rename\" first (here, with an abbreviated version of itself), and \"what to rename\" second (here, everything, or `BBall` through `WPolo` if you prefer). \n\n    \n$\\blacksquare$\n\n(g) For an athlete who is 180 cm tall and weighs 100 kg, what\n sport would you guess they play? How sure are you that you are\n right? Explain briefly.\n\nSolution\n\n\nFind this height and weight in your predictions (it's row 6). Look\nalong the line for the highest probability, which is 0.85 for\n`Field` (that is, field athletics). All the other\nprobabilities are much smaller (the biggest of the others is\n0.06). So this means we would guess the athlete to be a field\nathlete, and because the predicted probability is so big, we are\nvery likely to be right.\nThis kind of thought process is characteristic of discriminant\nanalysis, which we'll see more of later in the course.\nCompare that with the scatterplot you drew earlier: the field\nathletes do seem to be distinct from the rest in terms of\nheight-weight combination. \nSome of the other height-weight combinations are almost equally\nobvious: for example, very tall people who are not very heavy are\nlikely to play basketball. 400m runners are likely to be of\nmoderate height but light weight. Some of the other sports, or\nheight-weight combinations, are difficult to judge. Consider also\nthat we have mixed up males and females in this data set. We might\ngain some clarity by including `Sex` in the model, and also\nin the predictions. But I wanted to keep this question relatively\nsimple for you, and I wanted to stop from getting unreasonably\nlong. (You can decide whether you think it's already too long.)\n \n$\\blacksquare$\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}