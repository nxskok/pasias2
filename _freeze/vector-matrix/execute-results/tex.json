{
  "hash": "9647191a053139d261f051791d98b45b",
  "result": {
    "engine": "knitr",
    "markdown": "# Vector and matrix algebra\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n\n## Heights and foot lengths again\n\n Earlier, we investigated some data on predicting the height of a person from the length of their foot. The data were in [http://ritsokiguess.site/datafiles/heightfoot.csv](http://ritsokiguess.site/datafiles/heightfoot.csv).\n\n\n\n(a) Read in and display (some of) the data.\n\n\n\n(b) In your regression course, you learned (or will learn) the matrix formulation of the least squares estimates of intercept and slope. This produces a vector $\\hat\\beta$ containing estimates of both the intercept and the slope, from the formula\n\n$$ \\hat\\beta = (X^T X)^{-1} X^T y, $$\n\nwhere:\n\n- $X$ is a matrix containing a column of 1s followed by all the columns of explanatory variables\n- $X^T$ denotes the (matrix) transpose of $X$\n- $M^{-1}$ denotes the inverse of the matrix $M$\n- $y$ denotes the column of response variable values.\n\nUse the formula above to obtain the least squares estimates of intercepts and slope for this regression, using R's vector-matrix algebra. Hint: you are advised to do the calculation in steps, or else it will be very hard to read, and hard for the grader to check that it is correct.\n\n\n\n(c) Verify that your calculation is correct by running the regression.\n\n\n\n\n\n\n\n\nMy solutions follow:\n\n\n\n\n## Heights and foot lengths again\n\n Earlier, we investigated some data on predicting the height of a person from the length of their foot. The data were in [http://ritsokiguess.site/datafiles/heightfoot.csv](http://ritsokiguess.site/datafiles/heightfoot.csv).\n\n\n\n(a) Read in and display (some of) the data.\n\nSolution\n\n\nCopy what you did before:\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/heightfoot.csv\"\nhf <- read_csv(my_url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 33 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (2): height, foot\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nhf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 33 x 2\n   height  foot\n    <dbl> <dbl>\n 1   66.5  27  \n 2   73.5  29  \n 3   70    25.5\n 4   71    27.9\n 5   73    27  \n 6   71    26  \n 7   71    29  \n 8   69.5  27  \n 9   73    29  \n10   71    27  \n# i 23 more rows\n```\n\n\n:::\n:::\n\n\n$\\blacksquare$\n\n\n(b) In your regression course, you learned (or will learn) the matrix formulation of the least squares estimates of intercept and slope. This produces a vector $\\hat\\beta$ containing estimates of both the intercept and the slope, from the formula\n\n$$ \\hat\\beta = (X^T X)^{-1} X^T y, $$\n\nwhere:\n\n- $X$ is a matrix containing a column of 1s followed by all the columns of explanatory variables\n- $X^T$ denotes the (matrix) transpose of $X$\n- $M^{-1}$ denotes the inverse of the matrix $M$\n- $y$ denotes the column of response variable values.\n\nUse the formula above to obtain the least squares estimates of intercepts and slope for this regression, using R's vector-matrix algebra. Hint: you are advised to do the calculation in steps, or else it will be very hard to read, and hard for the grader to check that it is correct.\n\nSolution\n\n\nThere is some setup first: we have to get hold of $X$ and $y$ from the data as a matrix and a vector respectively. I would use tidyverse ideas to do this, and then turn them into a matrix at the end, which I think is best. Don't forget to create a column of 1s to make the first column of $X$\n\n::: {.cell}\n\n```{.r .cell-code}\nhf %>% mutate(one=1) %>% \nselect(one, foot) %>% \nas.matrix() -> X\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     one foot\n[1,]   1 27.0\n[2,]   1 29.0\n[3,]   1 25.5\n[4,]   1 27.9\n[5,]   1 27.0\n[6,]   1 26.0\n```\n\n\n:::\n:::\n\n(`head` displays the first six rows, or else you'll be displaying all 33, which is too many.)\n\nAnother approach is this:\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- cbind(1, hf$foot)\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1 27.0\n[2,]    1 29.0\n[3,]    1 25.5\n[4,]    1 27.9\n[5,]    1 27.0\n[6,]    1 26.0\n```\n\n\n:::\n:::\n\nNote that the recycling rules mean that a column with only one value in it will be repeated to the length of the other one, and so this is better than working out how many observations there are and repeating `1` that many times.\n\nThe choice here is whether to use tidyverse stuff and turn into a matrix at the end, or make a matrix at the start (which is what `cbind` from base R is doing). I don't believe you've seen that in this course, so you ought to cite your source if you go that way.\n\nThe simplest choice for making $y$ is this:\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- hf$height\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 66.5 73.5 70.0 71.0 73.0 71.0 71.0 69.5 73.0 71.0 69.0 69.0 73.0 75.0 73.0\n[16] 72.0 69.0 68.0 72.5 78.0 79.0 71.0 74.0 66.0 71.0 71.0 71.0 84.0 77.0 72.0\n[31] 70.0 76.0 68.0\n```\n\n\n:::\n:::\n\nThis also works:\n\n::: {.cell}\n\n```{.r .cell-code}\nhf %>% select(height) %>% pull(height)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 66.5 73.5 70.0 71.0 73.0 71.0 71.0 69.5 73.0 71.0 69.0 69.0 73.0 75.0 73.0\n[16] 72.0 69.0 68.0 72.5 78.0 79.0 71.0 74.0 66.0 71.0 71.0 71.0 84.0 77.0 72.0\n[31] 70.0 76.0 68.0\n```\n\n\n:::\n:::\n\n(remembering that you don't want to have anything that's a dataframe), or this:\n\n::: {.cell}\n\n```{.r .cell-code}\nhf %>% select(height) %>% as.matrix() -> yy\nhead(yy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     height\n[1,]   66.5\n[2,]   73.5\n[3,]   70.0\n[4,]   71.0\n[5,]   73.0\n[6,]   71.0\n```\n\n\n:::\n:::\n\nremembering that a (column) vector and a 1-column matrix are the same thing as R is concerned.\n\nNow we want to construct some things. I would go at it this way, rather than trying to do everything at once (if you do, you will either get lost now, or in six months when you try to figure out what you did):\n\n::: {.cell}\n\n```{.r .cell-code}\nXt <- t(X) # X-transpose\nXtX <- Xt %*% X \nXtXi <- solve(XtX)\nXty <- Xt %*% y\nXtXi %*% Xty\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]\n[1,] 34.336335\n[2,]  1.359062\n```\n\n\n:::\n:::\n\nThe intercept is 34.33 and the slope is 1.36.\n\nThese compute, respectively, $X^T$, $X^T X$, the inverse of that, $X^T y$ and $\\hat\\beta$. Expect credit for laying out your calculation clearly.\n\nExtra: the value of this formula is that it applies no matter whether you have one $x$-variable, as here (or in the windmill data), or whether you have a lot (as in the asphalt data). In either case, $\\hat\\beta$ contains the estimate of the intercept followed by all the slope estimates, however many there are. There are also matrix formulas that tell you how the slopes or the residuals will change if you remove one observation or one explanatory variable, so that something like `step` will work very efficiently, and calculations for leverages likewise.\n\n\n$\\blacksquare$\n\n\n(c) Verify that your calculation is correct by running the regression.\n\nSolution\n\n\nThe usual `lm`:\n\n::: {.cell}\n\n```{.r .cell-code}\nhf.1 <- lm(height ~ foot, data = hf)\nsummary(hf.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = height ~ foot, data = hf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7491 -1.3901 -0.0310  0.8918 12.9690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  34.3363     9.9541   3.449 0.001640 ** \nfoot          1.3591     0.3581   3.795 0.000643 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.102 on 31 degrees of freedom\nMultiple R-squared:  0.3173,\tAdjusted R-squared:  0.2952 \nF-statistic: 14.41 on 1 and 31 DF,  p-value: 0.0006428\n```\n\n\n:::\n:::\nThe same.\n\nExtra: \nin this \"well-conditioned\" case,^[Which in this case means that the column of 1s and the actual x values are not strongly correlated, which means that the x-values vary enough.]  it makes no difference, but if $X^T X$ is almost singular, so that it almost doesn't have an inverse (for example, some of your explanatory variables are highly correlated with each other), you can get into trouble. Regression calculations in practice use something more sophisticated like the \n[singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) of $X^TX$ to diagnose whether $X^TX$ is actually singular or almost so, which from a numerical point of view is almost as bad, and to produce a sensible answer in that case.\n\nI guess I should try to make up one where it struggles. Let me do one with two $x$'s that are strongly correlated:\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tribble(\n~x1, ~x2, ~y,\n10, 20, 55,\n11, 19.0001, 60,\n12, 17.9999, 61,\n13, 17.0001, 64,\n14, 15.9998, 66,\n15, 15.0001, 67\n)\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 3\n     x1    x2     y\n  <dbl> <dbl> <dbl>\n1    10  20      55\n2    11  19.0    60\n3    12  18.0    61\n4    13  17.0    64\n5    14  16.0    66\n6    15  15.0    67\n```\n\n\n:::\n:::\n\n`x2` is almost exactly equal to 30 minus `x1`. What's the right answer?\n\n::: {.cell}\n\n```{.r .cell-code}\nd.1 <- lm(y ~ x1 + x2, data = d)\nsummary(d.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = d)\n\nResiduals:\n       1        2        3        4        5        6 \n-1.37530  1.26859  0.03118  0.63549  0.43765 -0.99760 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -11837.3   138685.6  -0.085    0.937\nx1             398.0     4622.9   0.086    0.937\nx2             395.7     4622.8   0.086    0.937\n\nResidual standard error: 1.303 on 3 degrees of freedom\nMultiple R-squared:  0.9485,\tAdjusted R-squared:  0.9141 \nF-statistic: 27.61 on 2 and 3 DF,  p-value: 0.0117\n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(d.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          x1          x2 \n-11837.2938    398.0000    395.6835 \n```\n\n\n:::\n:::\n\nYou should be right away suspicious here: the R-squared is high, but neither of the explanatory variables are significant! (This actually means that you can remove one of them, either one.) The standard errors are also suspiciously large, never a good sign. If you've done C67, you might be thinking about variance inflation factors here:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nvif(d.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x1        x2 \n220326271 220326271 \n```\n\n\n:::\n:::\n\nThese are both huge (greater than 5 or 10 or whatever guideline you use), indicating that they are highly correlated with each other (as we know they are).\n\nAll right, how does the matrix algebra work? This is just the same as before:\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% mutate(one=1) %>% \nselect(one, starts_with(\"x\")) %>% \nas.matrix() -> X\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     one x1      x2\n[1,]   1 10 20.0000\n[2,]   1 11 19.0001\n[3,]   1 12 17.9999\n[4,]   1 13 17.0001\n[5,]   1 14 15.9998\n[6,]   1 15 15.0001\n```\n\n\n:::\n:::\n\nand then\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- d$y\nXt <- t(X) # X-transpose\nXtX <- Xt %*% X \nXtXi <- solve(XtX)\nXty <- Xt %*% y\nXtXi %*% Xty\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]\none -11837.1777\nx1     397.9962\nx2     395.6796\n```\n\n\n:::\n:::\n\nThese answers are actually noticeably different from the right answers (with a few more decimals here):\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(d.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          x1          x2 \n-11837.2938    398.0000    395.6835 \n```\n\n\n:::\n:::\n\nOne way of finding out how nearly singular $X^TX$ is is to look at its eigenvalues. You'll recall that a singular matrix has one or more zero eigenvalues:\n\n::: {.cell}\n\n```{.r .cell-code}\neigen(XtX)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neigen() decomposition\n$values\n[1] 2.781956e+03 3.404456e+01 8.805965e-11\n\n$vectors\n            [,1]        [,2]        [,3]\n[1,] -0.04643281 -0.00782885  0.99889074\n[2,] -0.57893109 -0.81469635 -0.03329647\n[3,] -0.81405331  0.57983495 -0.03329628\n```\n\n\n:::\n:::\n\nThe third eigenvalue is $8.8 \\times 10^{-11}$, which is very close to zero, especially compared to the other two, which are 34 and over two *thousand*. This is a very nearly singular matrix, and hence $(X^TX)^{-1}$ is very close to not existing at all, and *that* would mean that you couldn't even compute the intercept and slope estimates, never mind hope to get close to the right answer.\n\n\n$\\blacksquare$\n\n\n\n",
    "supporting": [
      "vector-matrix_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}