---
editor: 
  markdown: 
    wrap: 72
---

## What distinguishes people who do different jobs?

244 people work at a certain company. They each have one of
three jobs: customer service, mechanic, dispatcher. In the data set,
these are labelled 1, 2 and 3 respectively. In addition, they each are
rated on scales called `outdoor`, `social` and `conservative`. Do people
with different jobs tend to have different scores on these scales, or,
to put it another way, if you knew a person's scores on `outdoor`,
`social` and `conservative`, could you say something about what kind of
job they were likely to hold? The data are in
[link](http://ritsokiguess.site/datafiles/jobs.txt).


(a) Read in the data and display some of it.

Solution

The usual. This one is aligned columns. I'm using a "temporary" name for
my read-in data frame, since I'm going to create the proper one in a
moment.

```{r jobs-1 }
my_url <- "http://ritsokiguess.site/datafiles/jobs.txt"
jobs0 <- read_table(my_url)
jobs0
```

We got all that was promised, plus a label `id` for each employee, which
we will from here on ignore.[^_jobs-2]

[^_jobs-2]: Until much later.

$\blacksquare$

(b) Note the types of each of the variables, and create any new
    variables that you need to.

Solution

These are all `int` or whole numbers. But, the job ought to be a
`factor`: the labels 1, 2 and 3 have no meaning as such, they just label
the three different jobs. (I gave you a hint of this above.) So we need
to turn `job` into a factor. I think the best way to do that is via
`mutate`, and then we save the new data frame into one called `jobs`
that we actually use for the analysis below:

```{r jobs-2 }
job_labels <- c("custserv", "mechanic", "dispatcher")
jobs0 %>%
  mutate(job = factor(job, labels = job_labels)) -> jobs
```

I lived on the edge and saved my factor `job` into a variable with the
same name as the numeric one. I should check that I now have the right
thing:

```{r jobs-3 }
jobs
```

I like this better because you see the actual factor levels rather than
the underlying numeric values by which they are stored.

All is good here. If you forget the `labels` thing, you'll get a factor,
but its levels will be 1, 2, and 3, and you will have to remember which
jobs they go with. I'm a fan of giving factors named levels, so that you
can remember what stands for what.[^_jobs-3]

[^_jobs-3]: When you're *recording* the data, you may find it convenient
    to use short codes to represent the possibly long factor levels, but
    in that case you should also use a
    [**codebook**](https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html)
    so that you know what the codes represent. When I read the data into
    R, I would create a factor with named levels, like I did here, if I
    don't already have one.

Extra: another way of doing this is to make a lookup table, that is, a
little table that shows which job goes with which number:

```{r jobs-4 }
lookup_tab <- tribble(
  ~job, ~jobname,
  1, "custserv",
  2, "mechanic",
  3, "dispatcher"
)
lookup_tab
```

I carefully put the numbers in a column called `job` because I want to
match these with the column called `job` in `jobs0`:

```{r jobs-5 }
jobs0 %>%
  left_join(lookup_tab) -> jobs
jobs %>% slice_sample(n = 20)
```

You see that each row has the *name* of the job that employee has, in
the column `jobname`, because the job `id` was looked up in our lookup
table. (I displayed some random rows so you could see that it worked.)

$\blacksquare$

(c) Run a multivariate analysis of variance to convince yourself that
    there are some differences in scale scores among the jobs.

Solution

You know how to do this, right? This one is the easy way:

```{r vuggy.a}
response <- with(jobs, cbind(social, outdoor, conservative))
response.1 <- manova(response ~ jobname, data = jobs)
summary(response.1)
```

Or you can use `Manova`. That is mostly for practice here, since there
is no reason to make things difficult for yourself:

```{r kazoo}
library(car)
response.2 <- lm(response ~ job, data = jobs)
summary(Manova(response.2))
```

This version gives the four different versions of the test (rather than
just the Pillai test that `manova` gives), but the results are in this
case identical for all of them.

So: oh yes, there are differences (on some or all of the variables, for
some or all of the groups). So we need something like discriminant
analysis to understand the differences.

We really ought to follow this up with Box's M test, to be sure that the
variances and correlations for each variable are equal enough across the
groups, but we note off the top that the P-values (all of them) are
really small, so there ought not to be much doubt about the conclusion
anyway:

```{r}
summary(BoxM(response, jobs$job))
```

This is small, but not (for this test) small enough to worry about (it's
not less than 0.001).

This, and the `lda` below, actually works perfectly well if you use the
original (integer) job, but then you have to remember which job number
is which.

$\blacksquare$

(d) Run a discriminant analysis and display the output.

Solution

Now `jobname` is the "response":

```{r jobs-6 }
job.1 <- lda(jobname ~ social + outdoor + conservative, data = jobs)
job.1
```

$\blacksquare$

(e) Which is the more important, `LD1` or `LD2`? How much more
    important? Justify your answer briefly.

Solution

Look at the "proportion of trace" at the bottom. The value for `LD1` is
quite a bit higher, so `LD1` is quite a bit more important when it comes
to separating the groups. `LD2` is, as I said, less important, but is
not completely worthless, so it will be worth taking a look at it.

$\blacksquare$

(f) Describe what values for an individual on the scales will make each
    of `LD1` and `LD2` high.

Solution

This is a two-parter: decide whether each scale makes a positive,
negative or zero contribution to the linear discriminant (looking at the
"coefficients of linear discriminants"), and then translate that into
what would make each `LD` high. Let's start with `LD1`:

Its coefficients on the three scales are respectively negative
($-0.19$), zero (0.09; my call) and positive (0.15). Where you draw the
line is up to you: if you want to say that `outdoor`'s contribution is
positive, go ahead. This means that `LD1` will be high if `social` is
*low* and if `conservative` is *high*. (If you thought that `outdoor`'s
coefficient was positive rather than zero, if `outdoor` is high as
well.)

Now for `LD2`: I'm going to call `outdoor`'s coefficient of $-0.22$
negative and the other two zero, so that `LD2` is high if `outdoor` is
*low*. Again, if you made a different judgement call, adapt your answer
accordingly.

$\blacksquare$

(g) The first group of employees, customer service, have the highest
    mean on `social` and the lowest mean on both of the other two
    scales. Would you expect the customer service employees to score
    high or low on `LD1`? What about `LD2`?

Solution

In the light of what we said in the previous part, the customer service
employees, who are high on `social` and low on `conservative`, should be
*low* (negative) on `LD1`, since both of these means are pointing that
way. As I called it, the only thing that matters to `LD2` is `outdoor`,
which is *low* for the customer service employees, and thus `LD2` for
them will be *high* (negative coefficient).

$\blacksquare$

(h) Plot your discriminant scores (which you will have to obtain first),
    and see if you were right about the customer service employees in
    terms of `LD1` and `LD2`. The job names are rather long, and there
    are a lot of individuals, so it is probably best to plot the scores
    as coloured circles with a legend saying which colour goes with
    which job (rather than labelling each individual with the job they
    have).

Solution

Predictions first, then make a data frame combining the predictions with
the original data:

```{r jobs-7 }
p <- predict(job.1)
as.data.frame(p)
d <- cbind(jobs, p)
d
```

Following my suggestion, plot these the standard way with `colour`
distinguishing the jobs:

```{r jobs-8 }
#| fig-height: 8
ggplot(d, aes(x = x.LD1, y = x.LD2, colour = jobname)) + geom_point()
# ggplot(d, aes(x = x.LD1, y = x.LD2, colour = class)) + geom_point()
# ggplot(d, aes(x = job, y = x.LD1)) + geom_boxplot()
```

I was mostly right about the customer service people: small `LD1`
definitely, large `LD2` kinda. I wasn't more right because the group
means don't tell the whole story: evidently, the customer service people
vary quite a bit on `outdoor`, so the red dots are all over the left
side of the plot.

There is quite a bit of intermingling of the three employee groups on
the plot, but the point of the MANOVA is that the groups are (way) more
separated than you'd expect by chance, that is if the employees were
just randomly scattered across the plot.

To think back to that `trace` thing: here, it seems that `LD1` mainly
separates customer service (left) from dispatchers (right); the
mechanics are all over the place on `LD1`, but they tend to be low on
`LD2`. So `LD2` *does* have something to say.

$\blacksquare$

(i) <a name="part:predjob">\*</a> Obtain predicted job allocations for
    each individual (based on their scores on the three scales), and
    tabulate the true jobs against the predicted jobs. How would you
    describe the quality of the classification? Is that in line with
    what the plot would suggest?

Solution

Use the predictions that you got before and saved in `d`:

```{r cabteloupt}
with(d, table(obs = jobname, pred = class))
```

Or, the `tidyverse` way:

```{r jobs-9 }
d %>% count(job, class)
```

or:

```{r jobs-10 }
d %>%
  count(job, class) %>%
  pivot_wider(names_from=class, values_from=n, values_fill = list(n=0))
```

I didn't really need the `values_fill` since there are no missing
frequencies, but I've gotten used to putting it in. There are a lot of
misclassifications, but there are a lot of people, so a large fraction
of people actually got classified correctly. The biggest frequencies are
of people who got classified correctly. I think this is about what I was
expecting, looking at the plot: the people top left are obviously
customer service, the ones top right are in dispatch, and most of the
ones at the bottom are mechanics. So there will be some errors, but the
majority of people should be gotten right. The easiest pairing to get
confused is customer service and mechanics, which you might guess from
the plot: those customer service people with a middling `LD1` score and
a low `LD2` score (that is, high on `outdoor`) could easily be confused
with the mechanics. The easiest pairing to distinguish is customer
service and dispatchers: on the plot, left and right, that is, low and
high respectively on `LD1`.

```{r}
d %>% 
  filter(jobname != class)
```


What fraction of people actually got misclassified? You could just pull
out the numbers and add them up, but you know me: I'm too lazy to do
that.

We can work out the total number and fraction who got misclassified.
There are different ways you might do this, but the `tidyverse` way
provides the easiest starting point. For example, we can make a new
column that indicates whether a group is the correct or wrong
classification:

```{r jobs-11 }
d %>%
  count(job, class) %>%
  mutate(job_stat = ifelse(job == class, "correct", "wrong"))
```

From there, we count up the correct and wrong ones, recognizing that we
want to total up the *frequencies* in `n`, not just count the number of
rows:

```{r jobs-12 }
d %>%
  count(job, class) %>%
  mutate(job_stat = ifelse(job == class, "correct", "wrong")) %>%
  count(job_stat, wt = n)
```

and turn these into proportions:

```{r jobs-13, error=T}
d %>%
  count(job, class) %>%
  mutate(job_stat = ifelse(job == class, "correct", "wrong")) %>%
  count(job_stat, wt = n) %>%
  mutate(proportion = n / sum(n))
```

There is a `count` followed by another `count` of the first lot of
counts, so the second count column has taken over the name `n`.

24% of all the employees got classified into the wrong job, based on
their scores on `outdoor`, `social` and `conservative`.

This is actually not bad, from one point of view: if you just guessed
which job each person did, without looking at their scores on the scales
at all, you would get ${1\over 3}=33\%$ of them right, just by luck, and
${2\over3}=67\%$ of them wrong. From 67% to 24% error is a big
improvement, and *that* is what the MANOVA is reacting to.

To figure out whether some of the groups were harder to classify than
others, squeeze a `group_by` in early to do the counts and proportions
for each (true) job:

```{r jobs-14, error=T}
d %>%
  count(job, class) %>%
  mutate(job_stat = ifelse(job == class, "correct", "wrong")) %>%
  group_by(job) %>%
  count(job_stat, wt = n) %>%
  mutate(proportion = n / sum(n))
```

or even split out the correct and wrong ones into their own columns:

```{r jobs-15, error=T}
d %>%
  count(job, class) %>%
  mutate(job_stat = ifelse(job == class, "correct", "wrong")) %>%
  group_by(job) %>%
  count(job_stat, wt = n) %>%
  mutate(proportion = n / sum(n)) %>%
  select(-n) %>%
  pivot_wider(names_from=job_stat, values_from=proportion)
```

The mechanics were hardest to get right and easiest to get wrong, though
there isn't much in it. I think the reason is that the mechanics were
sort of "in the middle" in that a mechanic could be mistaken for either
a dispatcher or a customer service representative, but but customer
service and dispatchers were more or less distinct from each other.

It's up to you whether you prefer to do this kind of thing by learning
enough about `table` to get it to work, or whether you want to use
tidy-data mechanisms to do it in a larger number of smaller steps. I
immediately thought of `table` because I knew about it, but the
tidy-data way is more consistent with the way we have been doing things.

$\blacksquare$

(j) Consider an employee with these scores: 20 on `outdoor`, 17 on
    `social` and 8 on `conservative` What job do you think they do, and
    how certain are you about that? Use `predict`, first making a data
    frame out of the values to predict for.

Solution

This is in fact exactly the same idea as the data frame that I generally
called `new` when doing predictions for other models. I think the
clearest way to make one of these is with `tribble`:

```{r jobs-16 }
new <- tribble(
  ~outdoor, ~social, ~conservative,
  20, 17, 8
)
new
```

There's no need for `datagrid` or `crossing` here because I'm not doing
combinations of things. (I might have done that here, to get a sense for
example of "what effect does a higher score on `outdoor` have on the
likelihood of a person doing each job?". But I didn't.

```{r}

```

Then feed this into `predict` as the *second* thing:

```{r jobs-17 }
pp1 <- predict(job.1, new)
```

Our predictions are these:

```{r jobs-18 }
cbind(new, pp1)
```

The `class` thing gives our predicted job, and the `posterior`
probabilities say how sure we are about that. So we reckon there's a 78%
chance that this person is a mechanic; they might be a dispatcher but
they are unlikely to be in customer service. Our best guess is that they
are a mechanic.[^_jobs-4]

[^_jobs-4]: I discovered that I used *pp* twice, and I want to use the
    first one again later, so I had to rename this one.

Does this pass the sanity-check test? First figure out where our new
employee stands compared to the others:

```{r jobs-19 }
summary(jobs)
```

Their score on `outdoor` is above average, but their scores on the other
two scales are below average (right on the 1st quartile in each case).

Go back to the table of means from the discriminant analysis output. The
mechanics have the highest average for `outdoor`, they're in the middle
on `social` and they are lowish on `conservative`. Our new employee is
at least somewhat like that.

Or, we can figure out where our new employee sits on the plot. The
output from `predict` gives the predicted `LD1` and `LD2`, which are
0.71 and $-1.02$ respectively. This employee would sit to the right of
and below the middle of the plot: in the greens, but with a few blues
nearby: most likely a mechanic, possibly a dispatcher, but likely not
customer service, as the posterior probabilities suggest.

Extra: I can use the same mechanism to predict for a combination of
values. This would allow for the variability of each of the original
variables to differ, and enable us to assess the effect of, say, a
change in `conservative` over its "typical range", which we found out
above with `summary(jobs)`. I'll take the quartiles, in my usual
fashion:

```{r jobs-20 }
outdoors <- c(13, 19)
socials <- c(17, 25)
conservatives <- c(8, 13)
```

The IQRs are not that different, which says that what we get here will
not be that different from the \`\`coefficients of linear
discriminants'' above:

```{r jobs-21 }
new <- crossing(
  outdoor = outdoors, social = socials,
  conservative = conservatives
)
pp2 <- predict(job.1, new)
px <- round(pp2$x, 2)
cbind(new, pp2$class, px)
```

The highest (most positive) LD1 score goes with high outdoor, low
social, high conservative (and being a dispatcher). It is often
interesting to look at the *second*-highest one as well: here that is
*low* outdoor, and the same low social and high conservative as before.
That means that `outdoor` has nothing much to do with `LD1` score. Being
low `social` is strongly associated with `LD1` being positive, so that's
the important part of `LD1`.

What about `LD2`? The most positive LD2 are these:

```         

LD2    outdoor  social  conservative
====================================
0.99   low      low     high
0.59   low      high    high
0.55   low      low     low
```

These most consistently go with `outdoor` being low.

Is that consistent with the "coefficients of linear discriminants"?

```{r jobs-22 }
job.1$scaling
```

Very much so: `outdoor` has nothing much to do with `LD1` and everything
to do with `LD2`.

$\blacksquare$

