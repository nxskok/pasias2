##  Crimes in San Francisco



The data in
[link](http://ritsokiguess.site/datafiles/sfcrime1.csv) is a subset
of a huge
dataset of crimes committed in San Francisco between 2003 and
2015. The variables are:



* `Dates`: the date and time of the crime

* `Category`: the category of crime, eg. "larceny" or
"vandalism" (response).

* `Descript`: detailed description of crime.

* `DayOfWeek`: the day of the week of the crime.

* `PdDistrict`: the name of the San Francisco Police
Department district in which the crime took place.

* `Resolution`: how the crime was resolved

* `Address`: approximate street address of crime

* `X`: longitude

* `Y`: latitude


Our aim is to see whether the category of crime depends on the day of
the week and the district in which it occurred. However, there are a
lot of crime categories, so we will focus on the top four
"interesting" ones, which are the ones included in this data file.

Some of the model-fitting takes a while (you'll see why below). In your Quarto document, you can wait for the models to fit each time
you re-render your document, or you can insert `#| cache: true` below the top line
of your code chunk (above the first line of actual code).   What that does is to re-run that code chunk only
if it changes; if it hasn't changed it will use the saved results from
last time it was run. That can save you a lot of waiting around.^[The first render after you put this in, it will have to run it again, so you won't save any time there: it has to create a saved version to use in the future. But the *second* time, if you haven't changed this code chunk, you will definitely see a speedup.]



(a) Read in the data and display the dataset (or, at least,
part of it).


Solution


The usual:

```{r napoli}
#| cache: true
my_url <- "http://utsc.utoronto.ca/~butler/d29/sfcrime1.csv"
sfcrime <- read_csv(my_url)
sfcrime
```

     

This is a tidied-up version of the data, with only the variables we'll
look at, and only the observations from one of the "big four"
crimes, a mere 300,000 of them. This is the data set we created earlier.
    
$\blacksquare$

(b) Fit a multinomial logistic
regression that predicts crime category from day of week and
district. (You don't need to look at it.) The model-fitting produces
some output, which will at least convince you that it is working, since it takes some time.



Solution


The modelling part is easy enough, as long as you can get the
uppercase letters in the right places:

```{r roma}
#| cache: true
sfcrime.1 <- multinom(Category ~ DayOfWeek + PdDistrict, data=sfcrime)
```

 
  
$\blacksquare$

(c) Fit a model that predicts Category from only the
district. 



Solution


Same idea. Write it out, or use `update`:

```{r sfcrime-1 }
#| cache: true
sfcrime.2 <- update(sfcrime.1, . ~ . - DayOfWeek)
```

   
  
$\blacksquare$

(d) Use `anova` to compare the two models you just
obtained. What does the `anova` tell you?



Solution


This:

```{r sfcrime-2 }
anova(sfcrime.2, sfcrime.1)
```

 

This is a very small P-value. The null hypothesis is that the two
models are equally good, and this is clearly rejected. We need the
bigger model: that is, we need to keep `DayOfWeek` in there,
because the pattern of crimes (in each district) differs over day of
week. 

One reason the P-value came out so small is that we have a ton of
data, so that even a very small difference between days of the week
could come out very strongly significant. The Machine Learning people
(this is a machine learning dataset) don't worry so much about tests
for that reason: they are more concerned with predicting things well,
so they just throw everything into the model and see what comes out.
  

$\blacksquare$

(e) Using your preferred model, obtain predicted probabilities
that a crime will be of each of these four categories for each day of
the week in the `TENDERLOIN` district (the name is ALL
CAPS). This will mean constructing a data frame to predict from,
obtaining the predictions and then displaying them suitably.



Solution


Use `datagrid` first to get the combinations you want (and only those), namely all the days of the week, but only the district called TENDERLOIN.

So, let's get the days of the week. The easiest way is to count them and ignore the counts:^[I know I just said not to do this kind of thing, but counting is really fast, while unnecessary predictions are really slow.]

```{r}
sfcrime %>% count(DayOfWeek) %>% 
  pull(DayOfWeek) -> daysofweek
daysofweek
```

Another way is the `levels(factor())` thing you may have seen before:

```{r}
levels(factor(sfcrime$DayOfWeek))
```


Now we can use these in `datagrid`:^[There is only one district, so we can just put that in here.]

```{r}
new <- datagrid(model = sfcrime.1, DayOfWeek = daysofweek, PdDistrict = "TENDERLOIN")
new
```

Good. And then predict for just these. This is slow, but not as slow as predicting for all districts. I'm saving the result of this slow part, so that it doesn't matter if I change my mind later about what to do with it. I want to make sure that I don't have to do it again, is all:^[The same thing applies if you are doing something like webscraping, that is to say downloading stuff from the web that you then want to do something with. Download it *once* and save it, then you can take as long as you need to decide what you're doing with it.]

```{r pred1}
#| cache: true
p <- cbind(predictions(sfcrime.1, newdata = new, type = "probs"))
p
```

This, as you remember, is long format, so grab the columns you need from it and pivot wider. The columns you want to make sure you have are `estimate`, `group` (the type of crime), and the day of week:

```{r}
p %>% 
  select(group, estimate, DayOfWeek) %>% 
  pivot_wider(names_from = group, values_from = estimate)
```

Success. (If you don't get rid of enough, you still have 28 rows and a bunch of missing values; in that case, `pivot_wider` will infer that everything should be in its own row.)

  
$\blacksquare$

(f) Describe briefly how the weekend days Saturday and Sunday
differ from the rest.



Solution


The days ended up in some quasi-random order, but Saturday and Sunday are
still together, so we can still easily compare them with the rest.
My take is that the last two columns don't differ much between
weekday and weekend, but the first two columns do: the probability
of a crime being an assault is a bit higher on the weekend, and the
probability of a crime being drug-related is a bit lower.
I will accept anything reasonable supported by the predictions you got.
We said there was a strongly significant day-of-week effect, but the
changes from weekday to weekend are actually pretty small (but the
changes from one weekday to another are even smaller). This supports
what I guessed before, that with this much data even a small effect
(the one shown here) is statistically
significant.^[Statistical significance as an idea grew up in    the days before "big data".]


Extra: I want to compare another district. What districts do we have?

```{r sfcrime-8 }
sfcrime %>% count(PdDistrict)
```

   

This is the number of our "big four" crimes committed in each
district. Let's look at the lowest-crime district `RICHMOND`. I
copy and paste my code. Since I want to compare two districts, I
include both:


```{r}
new <- datagrid(model = sfcrime.1, PdDistrict = c("TENDERLOIN", "RICHMOND"), DayOfWeek = daysofweek)
new
```

and then as we just did. I'm going to be a bit more selective about the columns I keep this time, since the display will be a bit wider and I don't want it to be too big for the page:

```{r pred2}
#| cache: true
p <- cbind(predictions(sfcrime.1, newdata = new, type = "probs"))
p
```

```{r}
p %>% 
  select(group, estimate, DayOfWeek, PdDistrict) %>% 
  pivot_wider(names_from = group, values_from = estimate)
```



Richmond is obviously not a drug-dealing kind of place; most of its
crimes are theft of one kind or another. But the predicted effect of
weekday vs. weekend is the same: Richmond doesn't have many assaults
or drug crimes, but it also has more assaults and fewer drug crimes on
the weekend than during the week. There is not much effect of day of
the week on the other two crime types in either place.

The consistency of *pattern*, even though the prevalence of the
different crime types differs by location, is a feature of the model:
we fitted an additive model, that says there is an effect of weekday,
and *independently* there is an effect of location. The
*pattern* over weekday is the same for each location, implied by
the model. This may or may not be supported by the actual data.

The way to assess this is to fit a model with interaction (we will see
more of this when we revisit ANOVA later), and compare the fit. This one takes longer to fit:

```{r model-three-one} 
#| cache: true
sfcrime.3 <- update(sfcrime.1, . ~ . + DayOfWeek * PdDistrict)
```

 

This one didn't actually complete the fitting process: it got to 100
times around and stopped (since that's the default limit). We can make
it go a bit further thus:

```{r model-three-two}
#| cache: true
sfcrime.3 <- update(sfcrime.1, .~.+DayOfWeek*PdDistrict, maxit=300)
anova(sfcrime.1, sfcrime.3)
```

 

This time, we got to the end. (The `maxit=300` gets passed on
to `multinom`, and says "go around up to 300 times if needed".) 
As you will see if you try it, this takes a bit of time to
run. 

This `anova` is also strongly significant, but in the light of
the previous discussion, the differential effect of day of week in
different districts might not be very big. We can even assess that; we
have all the machinery for the predictions, and we just have to apply
them to this model. The only thing is waiting for it to finish!

```{r preds3}
#| cache: true
p <- cbind(predictions(sfcrime.3, newdata = new, type = "probs"))
p
```

```{r}
p %>% 
  select(group, estimate, DayOfWeek, PdDistrict) %>% 
  pivot_wider(names_from = group, values_from = estimate)
```

It doesn't *look* much different. Maybe the Tenderloin has a
larger weekend increase in assaults than Richmond does. 

$\blacksquare$


